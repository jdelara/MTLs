[
    {
        "title": "Fully Verifying Transformation Contracts for Declarative ATL",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The Atlas Transformation Language (ATL) is today a de-facto standard in model-driven development. It is understood by the community that methods for exhaustively verifying such transformations provide an important pillar for achieving a stronger adoption of model-driven development in industry. In this paper we propose a method for verifying ATL model transformations by translating them into DSLTrans, a transformation language with limited expressiveness. Pre-/post-condition contracts are then veriﬁed on the resulting DSLTrans speciﬁcation using a symbolic-execution property prover. The technique we present in this paper is exhaustive for the declarative ATL subset, meaning that if a contract holds, it will hold when any input model is passed to the ATL transformation being checked. We explore the scalability of our technique using a set of examples, including a model transformation developed in collaboration with our industrial partner.",
        "keywords": [
            "Model transformation",
            "Formal veriﬁcation",
            "ATL",
            "Contracts",
            "Symbolic execution",
            "Pre-/Post-conditions"
        ],
        "authors": [
            "Bentley James Oakes",
            "Javier Troya",
            "Levi L´ucio",
            "and Manuel Wimmer"
        ],
        "file_path": "data/models/models15/Fully verifying transformation contracts for declarative ATL.pdf"
    },
    {
        "title": "Modeling User Intentions for In-Car Infotainment Systems using Bayesian Networks",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "To support users in operating a computer system with a varying set of functions, it is fundamental to understand their intentions, e.g., within an in-car infotainment system. Although the development of current in-car infotainment systems is already model-based, explicitly gathering and modeling user intentions is currently not supported. However, manually creating software that predicts user intentions is complex, error-prone and expensive. Model-based development can help in overcoming these issues. In this paper, we present an approach for modeling a user’s intention based on Bayesian networks. We support developers of in-car infotainment systems by providing means to model possible intentions of users according to the current situation. We further allow modeling of user preferences and show how the modeled intentions may change during run-time as a result of the user’s behavior. We demonstrate feasibility of our approach using an industrial example of an intention-aware in-car infotainment system.",
        "keywords": [],
        "authors": [
            "Daniel L¨uddecke",
            "Christoph Seidl",
            "Jens Schneider",
            "Ina Schaefer"
        ],
        "file_path": "data/models/models15/Modeling user intentions for in-car infotainment systems using Bayesian networks.pdf"
    },
    {
        "title": "SoSPa: A System of Security Design Patterns for Systematically Engineering Secure Systems",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model-Driven Security (MDS) for secure systems development still has limitations to be more applicable in practice. A recent systematic review of MDS shows that current MDS approaches have not dealt with multiple security concerns systematically. Besides, catalogs of security patterns which can address multiple security concerns have not been applied eﬃciently. This paper presents an MDS approach based on a uniﬁed System of Security design Patterns (SoSPa). In SoSPa, security design patterns are collected, speciﬁed as reusable aspect models to form a coherent system of them that guides developers in systematically addressing multiple security concerns. SoSPa consists of not only interrelated security design patterns but also a reﬁnement process towards their application. We applied SoSPa to design the security of crisis management systems. The result shows that multiple security concerns in the case study have been addressed by systematically integrating diﬀerent security solutions.",
        "keywords": [],
        "authors": [
            "Phu H. Nguyen",
            "Koen Yskout",
            "Thomas Heyman",
            "Jacques Klein",
            "Riccardo Scandariato",
            "Yves Le Traon"
        ],
        "file_path": "data/models/models15/SoSPa A system of Security design Patterns for systematically engineering secure systems.pdf"
    },
    {
        "title": "Performance Prediction upon Toolchain Migration in Model-Based Software",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Changing the development environment can have severe impacts on the system behavior such as the execution-time performance. Since it can be costly to migrate a software application, engineers would like to predict the performance parameters of the application under the new environment with as little effort as possible.\n\nIn this paper, we concentrate on model-driven development and provide a methodology to estimate the execution-time performance of application models under different toolchains. Our approach has low cost compared to the migration effort of an entire application. As part of the approach, we provide methods for characterizing model-driven applications, an algorithm for generating application-speciﬁc microbenchmarks, and results on using different methods for estimating the performance. In the work, we focus on SCADE as the development toolchain and use a Cruise Control and a Water Level application as case studies to conﬁrm the technical feasibility and viability of our technique.",
        "keywords": [
            "Model-based development",
            "Migration",
            "Automated Code Generation",
            "Estimation",
            "Prediction"
        ],
        "authors": [
            "Aymen Ketata",
            "Carlos Moreno",
            "Sebastian Fischmeister",
            "Jia Liang",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/models/models15/Performance prediction upon toolchain migration in model-based software.pdf"
    },
    {
        "title": "Engineering Tagging Languages for DSLs",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "To keep a DSL clean, readable and reusable in different contexts, it is useful to deﬁne a separate tagging language. A tag model logically adds information to the tagged DSL model while technically keeping the artifacts separated. Using a generic tagging language leads to promiscuous tag models, whereas deﬁning a target DSL-speciﬁc tag language has a high initial overhead. This paper presents a systematic approach to deﬁne a DSL-speciﬁc tag language and a corresponding schema language, combining the advantages of both worlds: (a) the tag language speciﬁcally ﬁts to the DSL, (b) the artifacts are kept separated and enabling reuse with different tag decorations, (c) the tag language follows a deﬁned type schema, and (d) systematic derivation considerably reduces the effort necessary to implement the tag language. An example shows that it can at least partially be realized by a generator and applied for any kind of DSL.",
        "keywords": [
            "Software Engineering",
            "Modeling",
            "MDE",
            "GSE"
        ],
        "authors": [
            "Timo Greifenberg",
            "Markus Look",
            "Sebastian Roidl",
            "Bernhard Rumpe"
        ],
        "file_path": "data/models/models15/Engineering tagging languages for DSLs.pdf"
    },
    {
        "title": "Extracting Frame Conditions from Operation Contracts",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "In behavioral modeling, operation contracts deﬁned by pre- and postconditions describe the effects on model properties (i.e., model elements such as attributes, links, etc.) that are enforced by an operation. However, it is usually omitted which model properties should not be modiﬁed. Deﬁning so-called frame conditions can ﬁll this gap. But, thus far, these have to be deﬁned manually – a time-consuming task. In this work, we propose a methodology which aims to support the modeler in the deﬁnition of the frame conditions by extracting suggestions based on an automatic analysis of operation contracts provided in OCL. More precisely, the proposed approach performs a structural analysis of pre- and postconditions together with invariants in order to categorize which class and object properties are clearly “variable” or “unaffected” – and which are “ambiguous”, i.e. indeed require a more thorough inspection. The developed concepts are implemented as a prototype and evaluated by means of several example models known from the literature.",
        "keywords": [],
        "authors": [
            "Philipp Niemann",
            "Frank Hilken",
            "Martin Gogolla",
            "Robert Wille"
        ],
        "file_path": "data/models/models15/Extracting frame conditions from operation contracts.pdf"
    },
    {
        "title": "State Machine Antipatterns for UML-RT",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Software development guidelines are a set of rules which can help improve the quality of software. These rules are deﬁned on the basis of experience gained by the software development community over time. Software antipatterns are a powerful and effective form of guidelines used for the identiﬁcation of bad design choices and development practices that often lead to poor-quality software. This paper introduces a set of seven state machine antipatterns for the model-based development of real time embedded software systems. Each of these antipatterns is described with a pair of examples: one for the antipattern itself and a second one for improved, refactored solution.",
        "keywords": [],
        "authors": [
            "Tuhin Kanti Das",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models15/State machine antipatterns for UML-RT.pdf"
    },
    {
        "title": "Textual Diagram Layout Language and Visualization Algorithm",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Graphical diagrams are an excellent source of information for understanding models. On the other hand, editing, storing and versioning models are more efficient in textual representations. In order to combine the advantages of these two representations, diagrams have to be generated from models defined in text. The generated diagrams are usually created by autolayout algorithms based on heuristics.\n\nIn this paper we argue that automatically laid out diagrams are not ideal. Instead, we propose a textual layout description language that allows users to define the arrangement of those diagram elements they consider important. The paper also presents algorithms that create diagrams according to the layout description and arrange the underspecified elements automatically.\n\nThe paper reports on the implementation of the proposed layout description language as an embedded language in Java. It is used to generate class and state machine diagrams compatible with the Papyrus UML editor.",
        "keywords": [],
        "authors": [
            "Balázs Gregorics",
            "Tibor Gregorics",
            "Gábor Ferenc Kovács",
            "András Dobreff",
            "Gergely Dévai"
        ],
        "file_path": "data/models/models15/Textual diagram layout language and visualization algorithm.pdf"
    },
    {
        "title": "MODELS 2015 Organization",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models15/Contents.pdf"
    },
    {
        "title": "Reusable Event Types for Models at Runtime to Support the Examination of Runtime Phenomena",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Today’s software is getting more and more complex and harder to understand. Models help to organize knowledge and emphasize the structure of a software at a higher abstraction level. While the usage of model-driven techniques is widely adopted during software construction, it is still an open research topic if models can also be used to make runtime phenomena more comprehensible as well. It is not obvious which models are suitable for manual analysis and which model elements can be related to what type of runtime events. This paper proposes a collection of runtime event types that can be reused for various systems and meta-models. Based on these event types, information can be derived which help human observers to assess the current system state. Our approach is applied in a case study and evaluated regarding generalisability and completeness by relating it to two different meta-models.",
        "keywords": [
            "events",
            "examination",
            "models",
            "runtime"
        ],
        "authors": [
            "Michael Szvetits",
            "Uwe Zdun"
        ],
        "file_path": "data/models/models15/Reusable event types for models at runtime to support the examination of runtime phenomena.pdf"
    },
    {
        "title": "Consistent Co-Evolution of Models and Transformations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Evolving metamodels are in the center of Model-Driven Engineering, necessitating the co-evolution of dependent artifacts like models and transformations. While model co-evolution has been extensively studied, transformation co-evolution has received less attention up to now. Current approaches for transformation co-evolution provide a ﬁxed, restricted set of metamodel (MM) changes, only. Furthermore, composite changes are treated as monolithic units, which may lead to inconsistent co-evolution for overlapping atomic changes and prohibits extensibility. Finally, transformation co-evolution is considered in isolation, possibly inducing inconsistencies between model and transformation co-evolution. To overcome these limitations, we propose a complete set of atomic MM changes being able to describe arbitrary MM evolutions. Reusability and extensibility are supported by means of change composition, ensuring an intra-artifact consistent co-evolution. Furthermore, each change provides resolution actions for both, models and transformations, ensuring an inter-artifact consistent co-evolution. Based on our conceptual approach, a prototypical implementation is presented.",
        "keywords": [],
        "authors": [
            "Angelika Kusel",
            "Jürgen Etzlstorfer",
            "Elisabeth Kapsammer",
            "Werner Retschitzegger",
            "Wieland Schwinger",
            "Johannes Schönböck"
        ],
        "file_path": "data/models/models15/Consistent co-evolution of models and transformations.pdf"
    },
    {
        "title": "A Model-Based Framework for Probabilistic Simulation of Legal Policies",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Legal policy simulation is an important decision-support tool in domains such as taxation. The primary goal of legal policy simulation is predicting how changes in the law affect measures of interest, e.g., revenue. Currently, legal policies are simulated via a combination of spreadsheets and software code. This poses a validation challenge both due to complexity reasons and due to legal experts lacking the expertise to understand software code. A further challenge is that representative data for simulation may be unavailable, thus necessitating a data generator. We develop a framework for legal policy simulation that is aimed at addressing these challenges. The framework uses models for specifying both legal policies and the probabilistic characteristics of the underlying population. We devise an automated algorithm for simulation data generation. We evaluate our framework through a case study on Luxembourg’s Tax Law.",
        "keywords": [
            "Legal Policies",
            "Simulation",
            "UML Proﬁles",
            "Model-Driven Code Generation",
            "Probabilistic Data Generation"
        ],
        "authors": [
            "Ghanem Soltana",
            "Nicolas Sannier",
            "Mehrdad Sabetzadeh",
            "and Lionel C. Briand"
        ],
        "file_path": "data/models/models15/A model-based framework for probabilistic simulation of legal policies.pdf"
    },
    {
        "title": "A Statistical Analysis Approach to Assist Model Transformation Evolution",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Model Driven Engineering (MDE) is essentially based in metamodel deﬁnition, model edition and the speciﬁcation of model transformations (MT) among these. In many cases the development, evolution and adaptation of these transformations is still carried out without the support of proper methods and tools to reduce the effort and related costs to these activities. In this work, a novel model testing approach speciﬁcally designed to assist the engineer in model transformation evolution is presented. A statistical analysis of the actual behavior of the transformations is performed by means of the computation of well-known information extraction metrics. In order to assist the MT adaptation, a detailed interpretation of the possible results of those metrics is also presented. And ﬁnally, the results of applying this approach on a Model-Driven Reverse Engineering (MDRE) scenario deﬁned in the context of the MIGRARIA project are discussed.",
        "keywords": [
            "Model Transformation",
            "Model Transformation Evolution",
            "Model Transformation Testing",
            "Testing Oracle"
        ],
        "authors": [
            "Roberto Rodriguez-Echeverria",
            "Fernando Macias"
        ],
        "file_path": "data/models/models15/A statistical analysis approach to assist model transformation evolution.pdf"
    },
    {
        "title": "Quick Fixing ATL Model Transformations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Abstract—The correctness of model transformations is key to\nobtain reliable MDE solutions. However, current transformation\ntools provide limited support to statically detect and correct\nerrors. This way, the identiﬁcation of errors and their correction\nare mostly manual activities. Our aim is to improve this situation.\nBased on a static analyser for ATL model transformations\nwhich we have previously built, we present a method and a system\nto propose quick ﬁxes for transformation errors. The analyser\nis based on a combination of program analysis and constraint\nsolving, and our quick ﬁx generation technique makes use of the\nanalyser features to provide a range of ﬁxes, notably some non-\ntrivial, transformation-speciﬁc ones. Our approach integrates\nseamlessly with the ATL editor. We provide an evaluation based\non an existing faulty transformation, and automatically generated\ntransformation mutants, showing overall good results.",
        "keywords": [
            "Model Transformation",
            "Transformation Static Analysis",
            "ATL",
            "Quick ﬁxes",
            "Veriﬁcation and Testing"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models15/Quick fixing ATL model transformations.pdf"
    },
    {
        "title": "A-posteriori Typing for Model-Driven Engineering",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model-Driven Engineering is founded on the ability to create and process models conformant to a meta-model. Hence, meta-model classes are used in two ways: as templates to create objects, and as classiﬁers for them. While these two aspects are inherently tied in most meta-modelling approaches, in this paper, we discuss the beneﬁts of their decoupling. Thus, we rely on standard mechanisms for object creation and propose a-posteriori typing as a means to reclassify objects and enable multiple, partial, dynamic typings. This approach enhances ﬂexibility, permitting unanticipated reutilization (as existing model management operations deﬁned for a meta-model can be reused with other models once they get reclassiﬁed), as well as model transformation by reclassiﬁcation. We show the underlying theory behind the introduced concepts, and illustrate its applicability using our METADEPTH meta-modelling tool.",
        "keywords": [
            "A-posteriori typing",
            "Model typing",
            "Partial typing",
            "Dynamic typing",
            "Flexible MDE"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/models/models15/A-posteriori typing for Model-Driven Engineering.pdf"
    },
    {
        "title": "A Situational Method for Semi-automated Enterprise Architecture Documentation (SoSyM Abstract)",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Large organizations critically rely on their IT infrastructure. So called Enterprise Architecture (EA) models are often created to understand how the IT supports the business and used to optimize the IT and align it with the business. However, the models grow very large and are hard to keep up-to-date. Current approaches focus on automated data collection to tackle this problem, which is not feasible in many situations. In this paper we present a semi-automated EA documentation method and tool support that tackles this problem and takes the organizational contexts into account.",
        "keywords": [],
        "authors": [
            "Matthias Farwick",
            "Christian M. Schweda",
            "Ruth Breu",
            "Inge Hanschke"
        ],
        "file_path": "data/models/models15/A situational method for semi-automated enterprise architecture documentation -SoSyM abstract-.pdf"
    },
    {
        "title": "Identiﬁcation of Simulink Model Antipattern Instances using Model Clone Detection",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "One challenge facing the Model-Driven Engineering community is the need for model quality assurance. Speciﬁcally, there should be better facilities for analyzing models automat-ically. One measure of quality is the presence or absence of good and bad properties, such as patterns and antipatterns, respectively. We elaborate on and validate our earlier idea of detecting patterns in model-based systems using model clone detection by devising a Simulink antipattern instance detector. We chose Simulink because it is prevalent in industry, has mature model clone detection techniques, and interests our industrial partners. We demonstrate our technique using near-miss cross-clone detection to ﬁnd instances of Simulink antipatterns derived from the literature in four sets of public Simulink projects. We present our detection results, highlight interesting examples, and discuss potential improvements to our approach. We hope this work provides a ﬁrst step in helping practitioners improve Simulink model quality and further research in the area.",
        "keywords": [],
        "authors": [
            "Matthew Stephan",
            "James R. Cordy"
        ],
        "file_path": "data/models/models15/Identification of Simulink model antipattern instances using model clone detection.pdf"
    },
    {
        "title": "Automobile: Aircraft or Smartphone? Modeling Challenges and Opportunities in Automotive Systems",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Automotive systems are turning out to be one of the most complex consumer electronic systems being ever built. For the modern day users, they are products like smartphones and tablets but in size, complexity and quality and safety requirements they match if not exceed aircraft, and similar high integrity systems. Many of the major advances in Software engineering like model based development, platform based design and product line engineering have been introduced in the development of automotive electronic and software subsystems, which involve million lines of code and tens of electronic control units interconnected with multiple communication buses. This talk will highlight the challenges, current practices and new developments in the industry in building next generation automotive software from the modeling and analysis perspective. The challenges include traditional issues like system integration and feature interaction arising out of the federated development model, heterogeneity in subsystem behavior, time and space distributed development of software and the recent and rapidly increasing demand for advanced driver assistance features and system level requirements like fault tolerance and security. The talk attempts to outline a set of requirements for modeling from the perspective of system design and analysis. The talk will also touch upon some of the research and developments efforts currently ongoing within and with our external partners to meet these challenges.",
        "keywords": [],
        "authors": [
            "Ramesh S"
        ],
        "file_path": "data/models/models15/Automobile Aircraft or smartphone- Modeling challenges and opportunities in Automotive Systems -keynote-.pdf"
    },
    {
        "title": "Enriching Megamodel Management with Collection-Based Operators",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Megamodels are often used in MDE to describe collections of models and relationships between them. Typical collection-based operations – map, reduce, ﬁlter – cannot be applied directly to megamodels since these operators need to take relationships between models into consideration. In this paper, we propose adapted versions of these operators, demonstrating them on four megamodeling scenarios. We then analyze their applicability for handling industrial-sized megamodels. Finally, we report on a reference implementation of the operators and experimental results using it.",
        "keywords": [],
        "authors": [
            "Rick Salay",
            "Sahar Kokaly",
            "Alessio Di Sandro",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models15/Enriching megamodel management with collection-based operators.pdf"
    },
    {
        "title": "Modelling the Climate System: Is Model-Based Science Like Model-Based Engineering?",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Modern computational science is largely a model-building activity. At first sight, the models that scientists construct seem to differ radically from those used in model-based engineering. Scientists tend to build indicative ('how things are') models of the world using sets of continuous equations, while engineers tend to build optative ('how things should be') models of the world using structural and procedural abstractions. But a closer look reveals many fascinating similarities. In this talk, I will explore the relationship between the two types of modelling, drawing on my field studies of how climate modellers work. I'll begin with an overview of what a climate model is and how it is used. I'll then dive deeper into the engineering challenges of constructing a climate model, including the challenges of coupling disparate model components, dealing with model version-ing and model management issues, and the role that climate models play in enabling collaborative work. In the process, I hope to inspire people to explore how ideas from model-based software engineering might contribute to scientific mod-elling in general, and, more specifically, to the societal grand challenge of climate change.",
        "keywords": [],
        "authors": [
            "Steve Easterbrook"
        ],
        "file_path": "data/models/models15/Modelling the climate system Is model-based science like model-based engineering- -Keynote-.pdf"
    },
    {
        "title": "Formalizing the ISO/IEC/IEEE 29119 Software Testing Standard",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in the literature and in the practice. However, all of the techniques have been developed using their own concepts and terminology of MBT, which are very often different than other techniques and at times have conflicting semantics. Moreover, while working on MBT projects with our industrial partners in the last several years, we were unable to find a unified way of defining MBT techniques based on standard terminology. To precisely define MBT concepts with the aim of providing common understanding of MBT terminology across techniques, we formalize a small subset of the recently released ISO/IEC/IEEE 29119 Software Testing Standard as a conceptual model (UML class diagrams) together with OCL constraints. The conceptual model captures all the necessary concepts based on the standard terminology that are mandatory or optional in the context of MBT techniques and can be used to define new MBT tools and techniques. To validate the conceptual model, we instantiated its concepts for various MBT techniques previously developed in the context of our industrial partners. Such instantiation automatically enforces the specified OCL constraints. This type of validation provided us feedback to further refine the conceptual model. Finally, we also provide our experiences and lessons learnt for such formalization and validation.",
        "keywords": [
            "Model-Based Testing",
            "ISO/IEC/IEEE 29119",
            "UML",
            "Test Case Generation",
            "Modeling Methodology"
        ],
        "authors": [
            "Shaukat Ali",
            "Tao Yue"
        ],
        "file_path": "data/models/models15/Formalizing the ISO-IEC-IEEE 29119 Software Testing Standard.pdf"
    },
    {
        "title": "On the Use of UML Documentation in Software Maintenance: Results from a Survey in Industry",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "This paper presents the findings of a survey on the use of UML in software maintenance, carried out with 178 professionals working on software maintenance projects in 12 different countries. As part of long-term research we are carrying out to investigate the benefits of using UML in software maintenance, the main objectives of this survey are: 1) to explore whether UML diagrams are being used in software industry maintenance projects; 2) to see what UML diagrams are the most effective for software maintenance; 3) to find out what the perceived benefits of using UML diagrams are; and 4) to contextualize the kind of companies that use UML documentation in software maintenance. Some complementary results based on the way the documentation is used (whether it is UML-based or not) during software maintenance are also presented.",
        "keywords": [
            "UML",
            "Software Maintenance",
            "Survey"
        ],
        "authors": [
            "Ana M. Fernández-Sáez",
            "DaniloCaivano",
            "Marcela Genero",
            "Michel R.V. Chaudron"
        ],
        "file_path": "data/models/models15/On the use of UML documentation in software maintenance Results from a survey in industry.pdf"
    },
    {
        "title": "Incremental Symbolic Execution of Evolving State Machines",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "This paper introduces two complementary techniques, memoization-based and dependency-based incremental symbolic execution, that aim to optimize the analysis of state machine models that undergo change. We implement the two proposed techniques on IBM Rhapsody Statecharts and present some evaluation results.",
        "keywords": [],
        "authors": [
            "Amal Khalil",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models15/Incremental symbolic execution of evolving state machines.pdf"
    },
    {
        "title": "A Framework for Relating Syntactic and Semantic Model Differences",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model differencing is an important activity in model-based development processes. Differences need to be detected, analyzed, and understood to evolve systems and explore alternatives.\n\nTwo distinct approaches have been studied in the literature: syntactic differencing, which compares the concrete or abstract syntax of models, and semantic differencing, which compares models in terms of their meaning. Syntactic differencing identifies change operations that transform the syntactical representation of one model to the syntactical representation of the other. However, it does not explain their impact on the meaning of the model. Semantic model differencing is independent of syntactic changes and presents differences as elements in the semantics of one model but not the other. However, it does not reveal the syntactic changes causing these semantic differences.\n\nWe define a language independent, abstract framework, which relates syntactic change operations and semantic difference witnesses. We formalize fundamental relations of necessary and sufficient sets of change operations and analyze their properties. We further demonstrate concrete instances of the framework for three different popular modeling languages, namely, class diagrams, activity diagrams, and feature models. The framework provides a novel foundation for combining syntactic and semantic differencing.",
        "keywords": [],
        "authors": [
            "Shahar Maoz and Jan Oliver Ringert"
        ],
        "file_path": "data/models/models15/A framework for relating syntactic and semantic model differences.pdf"
    },
    {
        "title": "Synthesizing Tests for Combinatorial Coverage of\nModal Scenario Speciﬁcations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Software-intensive systems often consist of many components that interact to fulﬁll complex functionality. Testing these systems is vital, preferably by a minimal set of tests that covers all relevant cases. The behavior is typically speciﬁed by scenarios that describe what the system may, must, or must not do. When designing tests, as in the design of the system itself, the challenge is to consider interactions of scenarios. When doing this manually, critical interactions are easily overlooked. Inspired by Combinatorial Test Design, which exploits that bugs are typically found by regarding the interaction of a small set of parameters, we propose a new test coverage criterion based on scenario interactions. Furthermore, we present a novel technique for automatically synthesizing from Modal Sequence Diagram speciﬁcations a minimal set of tests that ensures a maximal coverage of possible t-wise scenario interactions. The technique is evaluated on an example speciﬁcation from an industrial project.",
        "keywords": [],
        "authors": [
            "Valerio Panzica La Manna",
            "Itai Segall",
            "Joel Greenyer"
        ],
        "file_path": "data/models/models15/Synthesizing tests for combinatorial coverage of modal scenario specifications.pdf"
    },
    {
        "title": "Toward Overcoming Accidental Complexity in Organisational Decision-Making",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "This paper takes a practitioner’s perspective on the problem of organisational decision-making. Industry practice follows a refinement based iterative method for organizational decision-making. However, existing enterprise modelling tools are not complete with respect to the needs of organizational decision-making. As a result, today, a decision maker is forced to use a chain of non-interoperable tools supporting paradigmatically diverse modelling languages with the onus of their co-ordinated use lying entirely on the decision maker. This paper argues the case for a model-based approach to overcome this accidental complexity. A bridge meta-model, specifying relationships across models created by individual tools, ensures integration and a method, describing what should be done when and how, and ensures better tool integration. Validation of the proposed solution using a case study is presented with current limitations and possible means of overcoming them outlined.",
        "keywords": [
            "Organizational decision making",
            "Enterprise modeling tools",
            "Meta modelling",
            "Method"
        ],
        "authors": [
            "Vinay Kulkarni",
            "Souvik Barat",
            "Tony Clark",
            "Balbir Barn"
        ],
        "file_path": "data/models/models15/Toward overcoming accidental complexity in organisational decision-making.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models15/Sponsors.pdf"
    },
    {
        "title": "FRAGMENTA: A Theory of Fragmentation for MDE",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model-Driven Engineering (MDE) promotes models throughout development. However, models may become large and unwieldy even for small to medium-sized systems. This paper tackles the MDE challenges of model complexity and scalability. It proposes FRAGMENTA, a theory of modular design that breaks down overall models into fragments that can be put together to build meaningful wholes, in contrast to classical MDE approaches that are essentially monolithic. The theory is based on an algebraic description of models, fragments and clusters based on graphs and morphisms. The paper’s novelties include: (i) a mathematical treatment of fragments and a seaming mechanism of proxies to enable inter-fragment referencing, (ii) fragmentation strategies, which prescribe a fragmentation structure to model instances, (iii) FRAGMENTA’s support for both top-down and bottom-up design, and (iv) our formally proved result that shows that inheritance hierarchies remain well-formed (acyclic) globally when fragments are composed provided some local fragment constraints are met.",
        "keywords": [
            "Model-driven engineering",
            "meta-modelling",
            "modularity",
            "graphs",
            "scalability",
            "model composition"
        ],
        "authors": [
            "Nuno Amálio",
            "Juan de Lara",
            "Esther Guerra"
        ],
        "file_path": "data/models/models15/Fragmenta A theory of fragmentation for MDE.pdf"
    },
    {
        "title": "An Automated Model Based Testing Approach for Platform Games",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Game development has recently gained a lot of momentum and is now a major software development industry. Platform games are being revived with both their 2D and 3D versions being developed. A major challenge faced by the industry is a lack of automated system-level approaches for game testing. Currently in most game development organizations, games are tested manually or using semi-automated techniques. Such testing techniques do not scale to the industry requirements where more systematic and repeatable approaches are required. In this paper we propose a model-based testing approach for automated black box functional testing of platform games. The paper provides a detailed modeling methodology to support automated system-level game testing. As part of the methodology, we provide guidelines for modeling the platform games for testing using our proposed game test modeling profile. We use domain modeling for representing the game structure and UML state machines for behavioral modeling. We present the details related to automated test case generation, execution, and oracle generation. We demonstrate our model-based testing approach by applying it on two cases studies, a widely referenced and open source implementation of Mario brothers game and an industrial case study of an endless runner game. The proposed approach was able to identify major faults in the open source game implementation. Our results showed that the proposed approach is practical and can be applied successfully on industrial games.",
        "keywords": [
            "Model based testing (MBT)",
            "game testing",
            "black box testing",
            "functional testing",
            "system-level testing",
            "and unified modeling language (UML)"
        ],
        "authors": [
            "Sidra Iftikhar",
            "Muhammad Zohaib Iqbal",
            "Muhammad Uzair Khan",
            "Wardah Mahmood"
        ],
        "file_path": "data/models/models15/An automated model based testing approach for platform games.pdf"
    },
    {
        "title": "Pattern-Based Debugging of Declarative Models",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Pattern-based debugging compares the engineer’s model to a pre-computed library of patterns, and generates discriminating examples that help the engineer decide if the model’s constraints need to be strengthened or weakened. A number of tactics are used to help connect the generated examples to the text of the model. This technique augments existing example/counter-example generators and unsatisﬁable core analysis tools, to help the engineer better localize and understand defects caused by complete overconstraint, partial overconstraint, and underconstraint. The technique is applied to localizing, understanding, and ﬁxing a defect in an Alloy model of Dijkstra’s Dining Philosopher’s problem. Automating the search procedure remains as essential future work.",
        "keywords": [],
        "authors": [
            "Vajih Montaghami and Derek Rayside"
        ],
        "file_path": "data/models/models15/Pattern-based debugging of declarative models.pdf"
    },
    {
        "title": "Enhancing the Communication Value of UML Models with Graphical Layers",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "UML is defined as a visual modeling language for specifying, constructing, and documenting software intensive systems. In that context, UML diagrams play a central role in the whole software engineering process, starting from early analysis, through implementation, to maintenance. Recent surveys of UML use in industry showed that software practitioners use it on a regular basis, and particularly for communication and as a mental-assist tool. However, they also pointed out the following weaknesses: the lack of context, graphical layout problems, and the language’s inadequacy as a facility for communication between technical teams and their clients. In this paper, we present a general approach that addresses these problems by enhancing the effectiveness of UML models as a communication vehicle. Our approach is based on expressing stakeholder-specific viewpoints through the use of secondary notations. This involves the use of auxiliary visual variables (e.g., color, position, size) that are not formally specified in UML. To that end, we extend the traditional concept of layer found in many graphical editors to UML diagram editors. FlipLayers is an implementation of our approach. It is in the form of a plugin for the Papyrus modeling environment. One scenario with several case studies is presented in the paper to demonstrate the benefits of our approach and also to illustrate how to express viewpoints with FlipLayers.",
        "keywords": [],
        "authors": [
            "Yosser El Ahmar",
            "Sébastien Gérard",
            "Cédric Dumoulin",
            "Xavier Le Pallec"
        ],
        "file_path": "data/models/models15/Enhancing the communication value of UML models with graphical layers.pdf"
    },
    {
        "title": "Systematically Deriving Domain-Speciﬁc Transformation Languages",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model transformations are helpful to evolve, refactor, refine and maintain models. While domain-specific languages are normally intuitive for modelers, common model transformation approaches (regardless of whether they transform graphical or textual models) are based on the modeling language’s abstract syntax requiring the modeler to learn the internal representation of the model to describe transformations. This paper presents a process that allows to systematically derive a textual domain-specific transformation language from the grammar of a given textual modeling language. As example, we apply this systematic derivation to UML class diagrams to obtain a domain-specific transformation language called CDTrans. CDTrans incorporates the concrete syntax of class diagrams which is already familiar to the modeler and extends it with a few transformation operators. To demonstrate the usefulness of the derived transformation language, we describe several refactoring transformations.",
        "keywords": [
            "Model transformation",
            "concrete syntax",
            "domain-specific",
            "language-specific",
            "systematic derivation",
            "generation"
        ],
        "authors": [
            "Katrin Hölldobler",
            "Bernhard Rumpe",
            "Ingo Weisemöller"
        ],
        "file_path": "data/models/models15/Systematically deriving domain-specific transformation languages.pdf"
    },
    {
        "title": "Process Mining in Software Systems\nDiscovering Real-Life Business Transactions and Process Models from Distributed Systems",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "This paper presents a novel reverse engineering technique for obtaining real-life event logs from distributed systems. This allows us to analyze the operational processes of software systems under real-life conditions, and use process mining techniques to obtain precise and formal models. Hence, the work can be positioned in-between reverse engineering and process mining. We present a formal deﬁnition, implementation and an instrumentation strategy based the joinpoint-pointcut model. Two case studies are used to evaluate our approach. These concrete exam- ples demonstrate the feasibility and usefulness of our approach.",
        "keywords": [
            "Reverse Engineering",
            "Process Mining",
            "Event Log",
            "Distributed Systems",
            "Performance Analysis",
            "Process Discovery",
            "Joinpoint-Pointcut Model",
            "Aspect-Oriented Programming"
        ],
        "authors": [
            "Maikel Leemans (m.leemans@tue.nl) and Wil M. P. van der Aalst (w.m.p.v.d.aalst@tue.nl)"
        ],
        "file_path": "data/models/models15/Process mining in software systems Discovering real-life business transactions and process models from distributed systems.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Aalst",
            "Wil M. P. van der",
            "Ali",
            "Shaukat",
            "Almorsy",
            "Mohamed",
            "Amálio",
            "Nuno",
            "Atkinson",
            "Colin",
            "Auguston",
            "Mikhail",
            "Babau",
            "Jean-Philippe",
            "Barat",
            "Souvik",
            "Barn",
            "Balbir",
            "Bastarrica",
            "María Cecilia",
            "Basten",
            "Twan",
            "Breu",
            "Ruth",
            "Briand",
            "Lionel C.",
            "Burgueño",
            "Loli",
            "Caivano",
            "Danilo",
            "Chaudron",
            "Michel R. V.",
            "Chechik",
            "Marsha",
            "Chen",
            "Tieming",
            "Clark",
            "Tony",
            "Combemale",
            "Benoit",
            "Conte",
            "Tayana",
            "Cordy",
            "James R.",
            "Czarnecki",
            "Krzysztof",
            "Das",
            "Tuhin Kanti",
            "DeAntoni",
            "Julien",
            "De Lara",
            "Juan",
            "Dévai",
            "Gergely",
            "Diaz-Pace",
            "J. Andrés",
            "Dingel",
            "Juergen",
            "Di Sandro",
            "Alessio",
            "Dobreff",
            "András",
            "Dong",
            "Jin Song",
            "Drechsler",
            "Rolf",
            "Drira",
            "Khalil",
            "Dumoulin",
            "Cédric",
            "Easterbrook",
            "Steve",
            "Eder",
            "Klaus",
            "Eichler",
            "Cédric",
            "Elaasar",
            "Maged",
            "El Ahmar",
            "Yosser",
            "Etzlstorfer",
            "Jürgen",
            "Farwick",
            "Matthias",
            "Fernández-Sáez",
            "Ana M.",
            "Fischmeister",
            "Sebastian",
            "Fouché",
            "Alexis",
            "Fouquet",
            "Francois",
            "Garmendia",
            "Antonio",
            "Geilen",
            "Marc",
            "Genero",
            "Marcela",
            "Gérard",
            "Sébastien",
            "Gerbig",
            "Ralph",
            "Gogolla",
            "Martin",
            "Goknil",
            "Arda",
            "Greenyer",
            "Joel",
            "Gregorics",
            "Balázs",
            "Gregorics",
            "Tibor",
            "Greifenberg",
            "Timo",
            "Grieco",
            "Alfredo",
            "Grünbacher",
            "Paul",
            "Grundy",
            "John",
            "Guerra",
            "Esther",
            "Hajri",
            "Ines",
            "Hanschke",
            "Inge",
            "Hartmann",
            "Thomas",
            "Heyman",
            "Thomas",
            "Hilken",
            "Christoph",
            "Hilken",
            "Frank",
            "Hölldobler",
            "Katrin",
            "Iftikhar",
            "Sidra",
            "Iqbal",
            "Muhammad Zohaib",
            "Jacobs",
            "Johan",
            "Kapsammer",
            "Elisabeth",
            "Kerboeuf",
            "Mickaël",
            "Ketata",
            "Aymen",
            "Khalil",
            "Amal",
            "Khan",
            "Muhammad Uzair",
            "Kholkar",
            "Deepali",
            "Kienzle",
            "Jörg",
            "Klein",
            "Jacques",
            "Kokaly",
            "Sahar",
            "Kovács",
            "Gábor Ferenc",
            "Kˇrikava",
            "Filip",
            "Kühne",
            "Thomas",
            "Kulkarni",
            "Vinay",
            "Kusel",
            "Angelika",
            "Leemans",
            "Maikel",
            "Le Pallec",
            "Xavier",
            "Le Traon",
            "Yves",
            "Lettner",
            "Daniela",
            "Liang",
            "Jia",
            "Liu",
            "Yang",
            "Look",
            "Markus",
            "Lúcio",
            "Levi",
            "Lüddecke",
            "Daniel",
            "Macias",
            "Fernando",
            "Mahmood",
            "Wardah",
            "Mallet",
            "Frédéric",
            "Maoz",
            "Shahar",
            "Marcos",
            "Claudia",
            "Marczak",
            "Sabrina",
            "Martin",
            "Kevin J. M.",
            "Moawad",
            "Assaad",
            "Montaghami",
            "Vajih",
            "Monteil",
            "Thierry",
            "Moreno",
            "Carlos",
            "Murphy",
            "Gail",
            "Nain",
            "Gregory",
            "Nguyen",
            "Phu H.",
            "Nguyen",
            "Tuong Huan",
            "Niemann",
            "Philipp",
            "Noyrit",
            "Florian",
            "Oakes",
            "Bentley James",
            "Oran",
            "Ana Carolina",
            "Panzica La Manna",
            "Valerio",
            "Peleska",
            "Jan",
            "Perovich",
            "Daniel",
            "Pescador",
            "Ana",
            "Prähofer",
            "Herbert",
            "Przigoda",
            "Nils",
            "Rabelo",
            "Jacilane",
            "Rago",
            "Alejandro",
            "Rayside",
            "Derek",
            "Reniers",
            "Michel",
            "Retschitzegger",
            "Werner",
            "Ringert",
            "Jan Oliver",
            "Rodriguez-Echeverria",
            "Roberto",
            "Roidl",
            "Sebastian",
            "Rouvoy",
            "Romain",
            "Rumpe",
            "Bernhard",
            "S",
            "Ramesh",
            "Sabetzadeh",
            "Mehrdad",
            "Salay",
            "Rick",
            "Sánchez Cuadrado",
            "Jesús",
            "Sanden",
            "Bram van der",
            "Sannier",
            "Nicolas",
            "Scandariato",
            "Riccardo",
            "Schaefer",
            "Ina",
            "Schiffelers",
            "Ramon",
            "Schneider",
            "Jens",
            "Schönböck",
            "Johannes",
            "Schöttle",
            "Matthias",
            "Schweda",
            "Christian M.",
            "Schwinger",
            "Wieland",
            "Segall",
            "Itai",
            "Seidl",
            "Christoph",
            "Seinturier",
            "Lionel",
            "Silvestre",
            "Luis",
            "Simmonds",
            "Jocelyn",
            "Soltana",
            "Ghanem",
            "Song",
            "Songzheng",
            "Stephan",
            "Matthew",
            "Stephany",
            "Thierry",
            "Stolf",
            "Patricia",
            "Sun",
            "Jun",
            "Sunkle",
            "Sagar",
            "Szvetits",
            "Michael",
            "Troya",
            "Javier",
            "Valentim",
            "Natasha M. Costa",
            "Vallecillo",
            "Antonio",
            "Vallejo",
            "Paola",
            "Vara Larsen",
            "Matias Ezequiel",
            "Voeten",
            "Jeroen",
            "Weisemöller",
            "Ingo",
            "Wille",
            "Robert",
            "Wimmer",
            "Manuel",
            "Yskout",
            "Koen",
            "Yue",
            "Tao",
            "Zdun",
            "Uwe"
        ],
        "file_path": "data/models/models15/Author index.pdf"
    },
    {
        "title": "Stream my Models: Reactive Peer-to-Peer Distributed Models@run.time",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The models@run.time paradigm promotes the use of models during the execution of cyber-physical systems to represent their context and to reason about their runtime behaviour. However, current modeling techniques do not allow to cope at the same time with the large-scale, distributed, and constantly changing nature of these systems. In this paper, we introduce a distributed models@run.time approach, combining ideas from reactive programming, peer-to-peer distribution, and large-scale models@run.time. We deﬁne distributed models as observable streams of chunks that are exchanged between nodes in a peer-to-peer manner. A lazy loading strategy allows to transparently access the complete virtual model from every node, although chunks are actually distributed across nodes. Observers and automatic reloading of chunks enable a reactive programming style. We integrated our approach into the Kevoree Modeling Framework and demonstrate that it enables frequently changing, reactive distributed models that can scale to millions of elements and several thousand nodes.",
        "keywords": [
            "Models@run.time",
            "Distributed models",
            "Reactive programming",
            "Asynchronous programming",
            "Peer-to-peer"
        ],
        "authors": [
            "Thomas Hartmann",
            "Assaad Moawad",
            "Francois Fouquet",
            "Gregory Nain",
            "Jacques Klein",
            "and Yves Le Traon"
        ],
        "file_path": "data/models/models15/Stream my models Reactive peer-to-peer distributed models-run.time.pdf"
    },
    {
        "title": "Concern-Oriented Interfaces for Model-Based Reuse of APIs",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Reuse is essential in modern software engineering, but limited in the context of MDE by the poor availability of reusable models. On the other hand, reusable code artifacts such as frameworks and libraries are abundant. This paper presents an approach to raise reusable code artifacts to the modelling level by modelling their API using concern-oriented techniques, thus enabling their use in the context of MDE. Our API interface models contain additional information, such as the encapsulated features and their impacts, to assist the developer in the reuse process. Once he has speciﬁed his needs, the model interface exposes only the API elements relevant for this speciﬁc reuse at the model level, together with the required usage protocol. We show how this approach is applied by hand to model the interface of a small GUI framework and outline how we envision this process to be performed semi-automatically.",
        "keywords": [],
        "authors": [
            "Matthias Schöttle and Jörg Kienzle"
        ],
        "file_path": "data/models/models15/Concern-oriented interfaces for model-based reuse of APIs.pdf"
    },
    {
        "title": "Applying Product Line Use Case Modeling in an Industrial Automotive Embedded System: Lessons Learned and a Reﬁned Approach",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "In this paper, we propose, apply, and assess Product line Use case modeling Method (PUM), an approach that supports modeling variability at different levels of granularity in use cases and domain models. Our motivation is that, in many software development environments, use case modeling drives interactions among stakeholders and, therefore, use cases and domain models are common practice for requirements elicitation and analysis. In PUM, we integrate and adapt existing product line extensions for use cases and introduce some template extensions for use case speciﬁcations. Variability is captured in use case diagrams while it is reﬂected at a greater level of detail in use case speciﬁcations. Variability in domain concepts is captured in domain models. PUM is supported by a tool relying on Natural Language Processing (NLP). We applied PUM to an industrial automotive embedded system and report lessons learned and results from structured interviews with experienced engineers.",
        "keywords": [],
        "authors": [
            "Ines Hajri",
            "Arda Goknil",
            "Lionel C. Briand",
            "Thierry Stephany"
        ],
        "file_path": "data/models/models15/Applying product line Use case modeling in an industrial automotive embedded system Lessons learned and a refined approach.pdf"
    },
    {
        "title": "Beyond Discrete Modeling: A Continuous and Efﬁcient Model for IoT",
        "submission-date": "2015/MM",
        "publication-date": "2015/MM",
        "abstract": "Internet of Things applications analyze our past habits through sensor measures to anticipate future trends. To yield accurate predictions, intelligent systems not only rely on single numerical values, but also on structured models aggregated from different sensors. Computation theory, based on the discretization of observable data into timed events, can easily lead to millions of values. Time series and similar database structures can efﬁciently index the mere data, but quickly reach computation and storage limits when it comes to structuring and processing IoT data. We propose a concept of continuous models that can handle high-volatile IoT data by deﬁning a new type of meta attribute, which represents the continuous nature of IoT data. On top of traditional discrete object-oriented modeling APIs, we enable models to represent very large sequences of sensor values by using mathematical polynomials. We show on various IoT datasets that this signiﬁcantly improves storage and reasoning efﬁciency.",
        "keywords": [
            "IoT",
            "Continuous modeling",
            "Discrete modeling",
            "Polynomial",
            "Extrapolation",
            "Big Data"
        ],
        "authors": [
            "Assaad Moawad",
            "Thomas Hartmann",
            "Francois Fouquet",
            "Gregory Nain",
            "Jacques Klein",
            "and Yves Le Traon"
        ],
        "file_path": "data/models/models15/Beyond discrete modeling A continuous and efficient model for IoT.pdf"
    },
    {
        "title": "A Megamodel for Software Process Line Modeling and Evolution",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Companies formalize software processes as a way of organizing development projects. Since there are differences in project contexts, a one-size-fits-all approach does not work well in practice. Some companies use a family of a predeﬁned processes, but this approach has a high process maintenance cost. Instead, we deﬁne Software Process Lines (SPrL), where a general process with variability is tailored to project contexts. Model-Driven Engineering (MDE) provides a formal framework for deﬁning the models and transformations required for automated SPrL tailoring. However, this approach requires the deﬁnition and co-evolution of various types of models and tool support beyond the skills of process engineers, making the industrial adoption challenging. This paper shares our experience using a megamodeling approach to the development of the back-end of our toolset. The megamodel provides a uniform mechanism for process deﬁnition, variability, tailoring and evolution, and we hide the MDE complexity through a user-friendly front-end. We report the application of our approach at Mobius, a small Chilean software enterprise.",
        "keywords": [
            "Megamodel",
            "Software Process Line",
            "Variability"
        ],
        "authors": [
            "Jocelyn Simmonds",
            "Daniel Perovich",
            "Mar´ıa Cecilia Bastarrica and Luis Silvestre"
        ],
        "file_path": "data/models/models15/A megamodel for Software Process Line modeling and evolution.pdf"
    },
    {
        "title": "Feature Modeling of Two Large-Scale Industrial Software Systems: Experiences and Lessons Learned",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Feature models are frequently used to capture the knowledge about conﬁgurable software systems and product lines. However, feature modeling of large-scale systems is challenging as many models are needed for diverse purposes. For instance, feature models can be used to reﬂect the perspectives of product management, technical solution architecture, or product conﬁg-uration. Furthermore, models are required at different levels of granularity. Although numerous approaches and tools are available, it remains hard to deﬁne the purpose, scope, and granularity of feature models. In this paper we thus present experiences of developing feature models for two large-scale industrial automation software systems. Speciﬁcally, we extended an existing feature modeling tool to support models for different purposes and at multiple levels. We report results on the characteristics and modularity of the feature models, including metrics about model dependencies. We further discuss lessons learned during the modeling process.",
        "keywords": [
            "feature modeling",
            "industrial software systems",
            "experience report"
        ],
        "authors": [
            "Daniela Lettner",
            "Klaus Eder",
            "Paul Grünbacher",
            "Herbert Prähofer"
        ],
        "file_path": "data/models/models15/Feature modeling of two large-scale industrial software systems Experiences and lessons learned.pdf"
    },
    {
        "title": "Systematic Generation of Standard Compliant Tool Support of Diagrammatic Modeling Languages",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "In the Model-Driven Engineering community, the abstract syntax of modeling languages is usually defined and implemented using metamodeling techniques. However, it is not the case for the concrete syntax of graphical modeling languages. Indeed, this concern is mostly specified by informal means. This practice leaves considerable leeway in the implementation and raises several standards compliance issues. Hence, toolsmiths can only rely on their interpretation of the standard and lack of systematic way to build conforming tool support. In this context, a first normative specification of the concrete syntax of UML 2.5 has been recently released using Diagram Definition. In this paper, we propose an approach that uses those formal specifications to systematically generate modeling language tool support that guarantees compliance to standard notation. We assess the approach on a subset of the UML class diagram implemented within the open-source Papyrus tool.",
        "keywords": [
            "Diagrammatic languages",
            "standard-compliance",
            "tooling support",
            "MDE",
            "UML"
        ],
        "authors": [
            "Alexis Fouché",
            "Florian Noyrit",
            "Sébastien Gérard",
            "Maged Elaasar"
        ],
        "file_path": "data/models/models15/Systematic generation of standard compliant tool support of diagrammatic modeling languages.pdf"
    },
    {
        "title": "Pattern-Based Development of Domain-Speciﬁc Modelling Languages",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Model-Driven Engineering (MDE) promotes the use of models to conduct all phases of software development in an automated way. Models are frequently deﬁned using Domain-Speciﬁc Modelling Languages (DSMLs), which many times need to be developed for the domain at hand. However, while constructing DSMLs is a recurring activity in MDE, there is scarce support for gathering, reusing and enacting knowledge for their design and implementation. This forces the development of every new DSML to start from scratch.\n\nTo alleviate this problem, we propose the construction of DSMLs and their modelling environments aided by patterns which gather knowledge of speciﬁc domains, design alternatives, concrete syntax, dynamic semantics and functionality for the modelling environment. They may have associated services, realized via components. Our approach is supported by a tool that enables the construction of DSMLs through the application of patterns, and synthesizes a graphical modelling environment according to them.",
        "keywords": [
            "Domain-Speciﬁc Modelling Languages",
            "Meta-Modelling",
            "Meta-Modelling Patterns",
            "Modelling Environments"
        ],
        "authors": [
            "Ana Pescador",
            "Antonio Garmendia",
            "Esther Guerra",
            "Jes´us S´anchez Cuadrado",
            "Juan de Lara"
        ],
        "file_path": "data/models/models15/Pattern-based development of Domain-Specific Modelling Languages.pdf"
    },
    {
        "title": "A Controlled Experiment with Usability Inspection Techniques Applied to Use Case Specifications: Comparing the MIT 1 and the UCE Techniques",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "A Use Case Model is composed of use cases that describe software functionalities through Use Case Specifications. The evaluation of the specifications that compose such a model can allow for an early identification of usability defects. We previously proposed MIT 1—Model Inspection Technique for Usability Evaluation that aims to support the identification of usability defects through the evaluation of use cases specifications. In this paper, we present the evaluation of this technique through a controlled experiment that measured its efficiency, effectiveness, perceived ease of use, and perceived usefulness when compared to the Use Case Evaluation (UCE) method. Our quantitative findings indicate that MIT 1 allows users to find more usability defects in less time than UCE. However, UCE was considered easiest to use and more useful than MIT 1, highlighting improvement needs for MIT 1.",
        "keywords": [
            "controlled experiment",
            "use case model",
            "use case specification",
            "early usability",
            "inspection",
            "empirical study"
        ],
        "authors": [
            "Natasha M. Costa Valentim",
            "Jacilane Rabelo",
            "Ana Carolina Oran",
            "Tayana Conte",
            "Sabrina Marczak"
        ],
        "file_path": "data/models/models15/A controlled experiment with Usability Inspection Techniques applied to Use Case Specifications comparing the MIT 1 and the UCE techniques.pdf"
    },
    {
        "title": "Proceedings",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Timothy Lethbridge",
            "Jordi Cabot",
            "and Alexander Egyed"
        ],
        "file_path": "data/models/models15/Front cover.pdf"
    },
    {
        "title": "Checking Concurrent Behavior in UML/OCL Models",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The Uniﬁed Modeling Language (UML) is a de-facto standard for software development and, together with the Object Constraint Language (OCL), allows for a precise description of a system prior to its implementation. At the same time, these descriptions can be employed to check the consistency and, hence, the correctness of a given UML/OCL model. In the recent past, numerous (automated) approaches have been proposed for this purpose. The behavior of the systems has usually been considered by means of sequence diagrams, state machines, and activity diagrams. But with the increasing popularity of design by contract, also composite structures, classes, and operations are frequently used to describe behavior in UML/OCL. However, for these description means no solution for the validation and veriﬁcation of concurrent behavior is available yet. In this work, we propose such a solution. To this end, we discuss the possible interpretations of “concurrency” which are admissible according to the common UML/OCL interpretation and, afterwards, propose a methodology which exploits solvers for SAT Modulo Theories (i. e., SMT solvers) in order to check the concurrent behavior of UML/OCL models. How to address the resulting problems is described and illustrated by means of a running example. Finally, the application of the proposed method is demonstrated.",
        "keywords": [],
        "authors": [
            "Nils Przigoda",
            "Christoph Hilken",
            "Robert Wille",
            "Jan Peleska",
            "Rolf Drechsler"
        ],
        "file_path": "data/models/models15/Checking concurrent behavior in UML-OCL models.pdf"
    },
    {
        "title": "Integrating Goal-Oriented and Use Case-Based Requirements Engineering: The Missing Link",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Combining goal-oriented and use case modeling has been shown as an effective method of requirements engineering. To ensure the quality of such modeled artifacts, a conceptual foundation is needed to govern the process of determining what types of artifacts to be modeled, and how they should be specified and analyzed for 3Cs problems (completeness, consistency and correctness). However, such a foundation is missing in current goal-use case integration approaches. In this paper, we present GUIMeta, a meta-model, to address this problem. GUIMeta consists of three layers. The artifact layer defines the semantics and classification of artifacts and their relationships. The specification layer offers specification rules for each artifact class. The ontology layer allows semantics to be integrated into the entire model. Our promising evaluation shows the suitability of GUIMeta in modeling goals and use cases.",
        "keywords": [
            "Goal and Use Case",
            "Meta-model",
            "Functional Grammar",
            "Ontology"
        ],
        "authors": [
            "Tuong Huan Nguyen",
            "John Grundy",
            "and Mohamed Almorsy"
        ],
        "file_path": "data/models/models15/Integrating goal-oriented and use case-based requirements engineering The missing link.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Timothy Lethbridge",
            "Jordi Cabot",
            "Alexander Egyed",
            "Yvan Labiche",
            "Gunter Mussbacher",
            "Ana Moreira",
            "Abdelwahab Hamou-Lhadj",
            "Emilio Insfran",
            "Vinay Kulkarni",
            "Omar Badreddin",
            "Benoit Baudry",
            "Benoit Combemale",
            "Tony Clark",
            "Arnon Sturm",
            "Marsha Chechik",
            "Dimitris Kolovos",
            "Martin Gogolla",
            "Mira Balaban",
            "Stéphane Somé",
            "Sahar Kokaly",
            "Michalis Famelis",
            "Manuel Wimmer",
            "Tian Zhang"
        ],
        "file_path": "data/models/models15/MODELS 2015 organization.pdf"
    },
    {
        "title": "A Behavioral Coordination Operator Language (BCOoL)",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "The design of complex systems involves various, possibly heterogeneous, structural and behavioral models. In model-driven engineering, the coordination of behavioral models to produce a single integrated model is necessary to provide support for validation and veriﬁcation. Indeed, it allows system designers to understand and validate the global and emerging behavior of the system. However, the manual coordination of models is tedious and error-prone, and current approaches to automate the coordination are bound to a ﬁxed set of coordination patterns. In this paper, we propose a Behavioral Coordination Operator Language (B-COOL) to reify coordination patterns between speciﬁc domains by using coordination operators between the Domain-Speciﬁc Modeling Languages used in these domains. Those operators are then used to automate the coordination of models conforming to these languages. We illustrate the use of B-COOL with the deﬁnition of coordination operators between timed ﬁnite state machines and activity diagrams.",
        "keywords": [
            "Heterogeneous Modeling",
            "Coordination Languages",
            "DSMLs"
        ],
        "authors": [
            "Matias Ezequiel Vara Larsen",
            "Julien DeAntoni",
            "Benoit Combemale",
            "and Fr´ed´eric Mallet"
        ],
        "file_path": "data/models/models15/A Behavioral Coordination Operator Language -BCOoL-.pdf"
    },
    {
        "title": "Models 2015 – the 18th instance of the International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "This volume contains the proceedings of Models 2015 – the 18th instance of the International Conference on Model Driven Engineering Languages and Systems. This year we received 216 abstract submissions that materialized in 172 papers, consisting of 132 technical papers (including 22 new ideas papers) and 40 in-practice papers. Of these, the Program Committee and Program Board accepted 35 foundations papers (26.5% acceptance rate) and 11 in-practice papers (28%).",
        "keywords": [],
        "authors": [
            "Tim Lethbridge",
            "Jordi Cabot and Alexander Egyed"
        ],
        "file_path": "data/models/models15/Message from the chairs.pdf"
    },
    {
        "title": "Modular Model-Based Supervisory Controller Design for Wafer Logistics in Lithography Machines",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Development of high-level supervisory controllers is an important challenge in the design of high-tech systems. It has become a significant issue due to increased complexity, combined with demands for verified quality, time to market, ease of development, and integration of new functionality. To deal with these challenges, model-based engineering approaches are suggested as a cost-effective way to support easy adaptation, validation, synthesis, and verification of controllers. This paper presents an industrial case study on modular design of a supervisory controller for wafer logistics in lithography machines. The uncontrolled system and control requirements are modeled independently in a modular way, using small, loosely coupled and minimally restrictive extended finite automata. The multiparty synchronization mechanism that is part of the specification formalism provides clear advantages in terms of modularity, traceability, and adaptability of the model. We show that being able to refer to variables and states of automata in guard expressions and state-based requirements, enabled by the use of extended finite automata, provides concise models. Additionally, we show how modular synthesis allows construction of local supervisors that ensure safety of parts of the system, since monolithic synthesis is not feasible for our industrial case.",
        "keywords": [],
        "authors": [
            "Bram van der Sanden",
            "Michel Reniers",
            "Marc Geilen",
            "Twan Basten",
            "Johan Jacobs",
            "Jeroen Voeten",
            "Ramon Schiffelers"
        ],
        "file_path": "data/models/models15/Modular model-based supervisory controller design for wafer logistics in lithography machines.pdf"
    },
    {
        "title": "A Unifying Approach to Connections for Multi-Level Modeling",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Capturing relationships between concepts in a domain is as important as capturing the concepts themselves. Modeling languages reﬂect this by providing connections with rich semantics, such as associations and links, thus providing a key advantage over approaches that support relationships with simple references only. While connections for two-level modeling (e.g. in the UML) have enjoyed a stable design for a considerable time, the same cannot be said for connections in multi-level modeling languages. As interest in multi-level modeling grows, it is important to provide a comprehensive design for connections that not only adheres to multi-level principles such as level-agnosticism and explicit level organization, but also supports deep characterization, i.e., the ability to specify level content beyond one level boundary. In this paper we propose a unifying conceptual model for connections whose expressiveness and scalability does not come at the cost of concept proliferation.",
        "keywords": [],
        "authors": [
            "Colin Atkinson",
            "Ralph Gerbig",
            "Thomas K¨uhne"
        ],
        "file_path": "data/models/models15/A unifying approach to connections for multi-level modeling.pdf"
    },
    {
        "title": "Employing Classifying Terms for Testing Model Transformations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "This contribution proposes a new technique for developing test cases for UML and OCL models. The technique is based on an approach that automatically constructs object models for class models enriched by OCL constraints. By guiding the construction process through so-called classifying terms, the built test cases in form of object models are classified into equivalence classes. A classifying term can be an arbitrary OCL term on the class model that calculates for an object model a characteristic value. From each equivalence class of object models with identical characteristic values one representative is chosen. The constructed test cases behave significantly different with regard to the selected classifying term. By building few diverse object models, properties of the UML and OCL model can be explored effectively. The technique is applied for automatically constructing relevant source model test cases for model transformations between a source and target metamodel.",
        "keywords": [],
        "authors": [
            "Martin Gogolla",
            "Antonio Vallecillo",
            "Loli Burgueño",
            "Frank Hilken"
        ],
        "file_path": "data/models/models15/Employing classifying terms for testing model transformations.pdf"
    },
    {
        "title": "Identifying Duplicate Functionality in Textual Use Cases by Aligning Semantic Actions",
        "submission-date": "2014/08",
        "publication-date": "2015/00",
        "abstract": "Developing high-quality requirements specifications often demands a thoughtful analysis and an adequate level of expertise from analysts. Although requirements modeling techniques provide mechanisms for abstraction and clarity, fostering the reuse of shared functionality (e.g., via UML relationships for use cases), they are seldom employed in practice. A particular quality problem of textual requirements, such as use cases, is that of having duplicate pieces of functionality scattered across the specifications. Duplicate functionality can sometimes improve readability for end users, but hinders development-related tasks such as effort estimation, feature prioritization and maintenance, among others. Unfortunately, inspecting textual requirements by hand in order to deal with redundant functionality can be an arduous, time-consuming and error-prone activity for analysts. In this context, we introduce a novel approach called ReqAligner that aids analysts to spot signs of duplication in use cases in an automated fashion. To do so, ReqAligner combines several text processing techniques, such as a use-case-aware classifier and a customized algorithm for sequence alignment. Essentially, the classifier converts the use cases into an abstract representation that consists of sequences of semantic actions, and then these sequences are compared pairwise in order to identify action matches, which become possible duplications. We have applied our technique to five real-world specifications, achieving promising results and identifying many sources of duplication in the use cases.",
        "keywords": [],
        "authors": [
            "Alejandro Rago",
            "Claudia Marcos",
            "J. Andrés Diaz-Pace"
        ],
        "file_path": "data/models/models15/Identifying duplicate functionality in textual use cases by aligning semantic actions -SoSyM abstract-.pdf"
    },
    {
        "title": "Model-Driven Regulatory Compliance: A Case Study of “Know Your Customer” Regulations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Modern enterprises face an unprecedented regulatory regime. Industry governance, risk, and compliance (GRC) solutions are document-oriented and expert-driven. Formal compliance checking techniques in contrast attempt to provide ways for rigorous modeling and analysis of regulatory compliance but miss out on holistic GRC perspective due to missing integration between diverse set of (semi-) formal models. We show that streamlining regulatory compliance using multiple purposive models of various aspects of regulations, it is possible to leverage both the rigor of formal techniques and the holistic enterprise GRC perspective. Our contributions are twofold. First, we present a model-driven architecture based on a conceptual model of integrated GRC that is capable of addressing key challenges of regulatory compliance. Second, using Know Your Customer regulations in Indian context as a case study, we demonstrate the utility of this architecture. Initial results with KYC regulations are promising and point to further work in model-driven regulatory compliance.",
        "keywords": [],
        "authors": [
            "Sagar Sunkle",
            "Deepali Kholkar",
            "and Vinay Kulkarni"
        ],
        "file_path": "data/models/models15/Model-driven regulatory compliance A case study of -Know Your Customer- regulations.pdf"
    },
    {
        "title": "Improving Reuse by means of Asymmetrical Model Migrations: An Application to the Orcc Case Study",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The legacy code of a tool handling domain speciﬁc data gathers valuable expertise. However in many cases, it must be rewritten to make it apply to structurally incompatible data. We investigate a co-evolution approach to avoid this update by making the call context meet the a legacy tool deﬁnition domain. The data conforming to the call context co-evolve into data conforming to the deﬁnition domain. Once processed by the tool, they can be put back into their original context thanks to a speciﬁc reverse transformation which enables the recovery of elements that had been initially removed. This approach is applied to Orcc, a compiler for dataﬂow applications. Orcc requires many common functions that are expected to be adapted to its own context. Our approach is an effective way to reuse them instead of rewriting them.",
        "keywords": [
            "TMM",
            "Refactoring",
            "Migration",
            "Application domain",
            "Legacy tool domain",
            "Co-evolution",
            "Reverse Migration"
        ],
        "authors": [
            "Paola Vallejo",
            "Mickaël Kerboeuf",
            "Kevin J. M. Martin",
            "Jean-Philippe Babau"
        ],
        "file_path": "data/models/models15/Improving reuse by means of asymmetrical model migrations An application to the Orcc case study.pdf"
    },
    {
        "title": "Enhanced Graph Rewriting Systems for Complex Software Domains",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Methodologies for correct by construction reconﬁgurations can efﬁciently solve consistency issues in dynamic software architecture. Graph-based models are appropriate for designing such architectures and methods. At the same time, they may be unﬁt to characterize a system from a non functional perspective. This stems from efﬁciency and applicability limitations in handling time-varying characteristics and their related dependencies. In order to lift these restrictions, an extension to graph rewriting systems is proposed herein. The suitability of this approach, as well as the restraints of currently available ones, are illustrated, analysed and experimentally evaluated with reference to a concrete example. This investigation demonstrates that the conceived solution can: (i) express any kind of algebraic dependencies between evolving requirements and properties; (ii) signiﬁcantly ameliorate the efﬁciency and scalability of system modiﬁcations with respect to classic methodologies; (iii) provide an efﬁcient access to attribute values; (iv) be fruitfully exploited in software management systems; (v) guarantee theoretical properties of a grammar, like its termination.",
        "keywords": [],
        "authors": [
            "Cédric Eichler",
            "Thierry Monteil",
            "Patricia Stolf",
            "Alfredo Grieco",
            "Khalil Drira"
        ],
        "file_path": "data/models/models15/Enhanced graph rewriting systems for complex software domains -SoSyM abstract-.pdf"
    },
    {
        "title": "Formalizing and Verifying Stochastic System Architectures Using Monterey Phoenix",
        "submission-date": "2014/04",
        "publication-date": "2015/00",
        "abstract": "The analysis of software architecture plays an important role in understanding the system structures and facilitate proper implementation of user requirements. Despite its importance in the software engineering practice, the lack of formal description and veriﬁcation support in this domain hinders the development of quality architectural models. To tackle this problem, in this work, we develop an approach for modeling and verifying software architectures speciﬁed using Monterey Phoenix (MP) architecture description language. MP is capable of modeling system and environment behaviors based on event traces, as well as supporting different architecture composition operations and views. First, we formalize the syntax and operational semantics for MP; therefore, formal veriﬁcation of MP models is feasible. Second, we extend MP to support shared variables and stochastic characteristics, which not only increases the expressiveness of MP, but also widens the properties MP can check, such as quantitative requirements. Third, a dedicated model checker for MP has been implemented, so that automatic veriﬁcation of MP models is supported. Finally, several experiments are conducted to evaluate the applicability and efﬁciency of our approach.",
        "keywords": [],
        "authors": [
            "Songzheng Song",
            "Yang Liu",
            "Mikhail Auguston",
            "Jun Sun",
            "Jin Song Dong",
            "Tieming Chen"
        ],
        "file_path": "data/models/models15/Formalizing and verifying stochastic system architectures using Monterey Phoenix -SoSyM abstract-.pdf"
    },
    {
        "title": "Software Supply Chains (Keynote)",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "It has long been desired to build software systems predominantly through the composition of existing software components. The need for such a production model is growing given the increasing use and reliance on software for almost everything we interact with from toasters to airplanes. For some kinds of systems, we have come a long way towards meeting the production via composition through the use of libraries, frameworks and plugin architectures. But, for other systems that require tight integrations of components produced by different suppliers, we are not yet able to reliably engineer a software supply chain. In this talk, I will outline some achievements in software supply chains and describe some of the challenges that need to be met to productively provide the systems of the future.",
        "keywords": [],
        "authors": [
            "Gail C. Murphy"
        ],
        "file_path": "data/models/models15/Software supply chains -keynote-.pdf"
    },
    {
        "title": "Infrastructure as Runtime Models: Towards Model-Driven Resource Management",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The importance of continuous delivery and the emergence of tools allowing to treat infrastructure configurations programmatically have revolutionized the way computing resources and software systems are managed. However, these tools keep lacking an explicit model representation of underlying resources making it difficult to introspect, verify or reconfigure the system in response to external events.\n\nIn this paper, we outline a novel approach that treats system infrastructure as explicit runtime models. A key benefit of using such models@run.time representation is that it provides a uniform semantic foundation for resources monitoring and reconfiguration. Adopting models at runtime allows one to integrate different aspects of system management, such as resource monitoring and subsequent verification into an unified view which would otherwise have to be done manually and require to use different tools. It also simplifies the development of various self-adaptation strategies without requiring the engineers and researchers to cope with low-level system complexities.",
        "keywords": [],
        "authors": [
            "Filip Kˇrikava",
            "Romain Rouvoy",
            "Lionel Seinturier"
        ],
        "file_path": "data/models/models15/Infrastructure as runtime models Towards Model-Driven resource management.pdf"
    },
    {
        "title": "Facilitating Modeling and Simulation of Complex Systems Through Interoperable Software",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Incorporating models into the design, test, and monitoring processes of complex systems can reduce development costs and improve performance and reliability. However, developing appropriate models and calibrating these models with measurement data is often time consuming. The problems are magnified in cases of distributed systems and cyber-physical system. Additionally, real-time test and hardware-in-the-loop applications may require the partitioning of model components on heterogeneous targets with a combination of multi-core processors and field programmable gate arrays. There are various commercial and open-source software options available for developing models but multiple modeling tools may be required for a single application. \nThis talk will review current efforts to overcome these challenges in modeling and simulation. Several successful applications will be discussed as well. Research in this area is continuing and collaboration is a must.",
        "keywords": [],
        "authors": [
            "Dr. Jeannie Falcon"
        ],
        "file_path": "data/models/models17/Keynotes.pdf"
    },
    {
        "title": "A Systematic Mapping Study on Modeling for Industry 4.0",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Industry 4.0 is a vision of manufacturing in which smart, interconnected production systems optimize the complete value-added chain to reduce cost and time-to-market. At the core of Industry 4.0 is the smart factory of the future, whose successful deployment requires solving challenges from many domains. Model-based systems engineering (MBSE) is a key enabler for such complex systems of systems as can be seen by the increased number of related publications in key conferences and journals. This paper aims to characterize the state of the art of MBSE for the smart factory through a systematic mapping study on this topic. Adopting a detailed search strategy, 1466 papers were initially identiﬁed. Of these, 222 papers were selected and categorized using a particular classiﬁcation scheme. Hence, we present the concerns addressed by the modeling community for Industry 4.0, how these are investigated, where these are published, and by whom. The resulting research landscape can help to understand, guide, and compare research in this ﬁeld. In particular, this paper identiﬁes the Industry 4.0 challenges addressed by the modeling community, but also the challenges that seem to be less investigated.",
        "keywords": [],
        "authors": [
            "Andreas Wortmann",
            "Benoit Combemale",
            "Olivier Barais"
        ],
        "file_path": "data/models/models17/A Systematic Mapping Study on Modeling for Industry 4.0.pdf"
    },
    {
        "title": "Bidirectional Transformations in the Large",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "The model-driven development of systems involves multiple models, metamodels and transformations, and relation-ships between them. A bidirectional transformation (bx) is usually deﬁned as a means of maintaining consistency between “two (or more)” models. This includes cases where one model may be generated from one or more others, as well as more complex (“symmetric”) cases where models record partially overlapping information. In recent years binary bx, those relating two models, have been extensively studied. Multiary1 bx, those relating more than two models, have received less attention. In this paper we consider how a multiary consistency relation may be deﬁned in terms of binary consistency relations, and how consistency restoration may be carried out on a network of models and relationships between them. We relate this to megamodelling and discuss further research that is needed.",
        "keywords": [],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/models/models17/Bidirectional Transformations in the Large.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Copyright notice.pdf"
    },
    {
        "title": "A Model-Driven Approach to Trace Checking of Pattern-based Temporal Properties",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Trace checking is a procedure for evaluating requirements over a log of events produced by a system. This paper deals with the problem of performing trace checking of temporal properties expressed in TemPsy, a pattern-based speciﬁcation language. The goal of the paper is to present a scalable and practical solution for trace checking, which can be used in contexts where relying on model-driven engineering standards and tools for property checking is a fundamental prerequisite. The main contributions of the paper are: a model-driven trace checking procedure, which relies on the efﬁcient mapping of temporal requirements written in TemPsy into OCL constraints on a meta-model of execution traces; the implementation of this trace checking procedure in the TEMPSY-CHECK tool; the evaluation of the scalability of TEMPSY-CHECK, applied to the veriﬁcation of real properties derived from a case study of our industrial partner, including a comparison with a state-of-the-art alternative technology based on temporal logic. The results of the evaluation show the feasibility of applying our model-driven approach for trace checking in realistic settings: TEMPSY-CHECK scales linearly with respect to the length of the input trace and can analyze traces with one million events in about two seconds.",
        "keywords": [],
        "authors": [
            "Wei Dou",
            "Domenico Bianculli",
            "Lionel Briand"
        ],
        "file_path": "data/models/models17/A Model-Driven Approach to Trace Checking of Pattern-Based Temporal Properties.pdf"
    },
    {
        "title": "2017 ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Heuristic-Based Recommendation for Metamodel - OCL Coevolution.pdf"
    },
    {
        "title": "Component and Connector Views in Practice: An Experience Report",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Component and Connector (C&C) view speciﬁcations, with corresponding veriﬁcation and synthesis techniques, have been recently suggested as a means for formal yet intuitive structural speciﬁcation of C&C models. In this paper we report on our recent experience in applying C&C views in industrial practice, where we aimed to answer questions such as: could C&C views be practically used in industry, what are challenges of systems engineers that the use of C&C views could address, and what are some of the technical obstacles in bringing C&C views to the hands of systems engineers. We describe our experience in detail and discuss a list of lessons we have learned, including, e.g., a missing abstraction concept in C&C models and C&C views that we have identiﬁed and added to the views language and tool, that engineers can create graphical C&C views quite easily, and how veriﬁcation algorithms scale on real-size industry models. Furthermore, we report on the non-negligible technical effort needed to translate Simulink block diagrams to C&C models. We make all materials mentioned and used in our experience electronically available for inspection and further research.",
        "keywords": [
            "component and connector models",
            "Simulink",
            "architecture",
            "industrial case study"
        ],
        "authors": [
            "Vincent Bertram",
            "Shahar Maoz",
            "Jan Oliver Ringert",
            "Bernhard Rumpe",
            "Michael von Wenckstern"
        ],
        "file_path": "data/models/models17/Component and Connector Views in Practice An Experience Report.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Publisher-s information.pdf"
    },
    {
        "title": "Reusable Speciﬁcation Templates for Deﬁning Dynamic Semantics of DSLs",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Domain-Speciﬁc Languages (DSLs) are a central concept of Model Driven Engineering (MDE). They are considered to be very effective in software development and are being widely adopted by industry nowadays. A DSL is a programming language specialized to a speciﬁc application domain. This paper proposes a new method for deﬁning the dynamic semantics of DSLs using reusable speciﬁcation templates, bridging the gap between DSL concepts and execution platforms.",
        "keywords": [],
        "authors": [
            "Ulyana Tikhonova"
        ],
        "file_path": "data/models/models17/Reusable Specification Templates for Defining Dynamic Semantics of DSLs.pdf"
    },
    {
        "title": "Tool Support for Live Formal Veriﬁcation",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Despite an increasing interest from industry (e.g.,\nDO333 standard [1]), formal veriﬁcation is still not widely used\nin production for safety critical systems. This has been recognized\nfor a while and various causes have been identiﬁed, one of them\nbeing the lack for scalable and cost effective tools. Many such\ntools exist for formal veriﬁcation, but few of them are user-\nfriendly: using formal veriﬁcation generally still requires such\nan effort that the time spent on the tool prevents the integration\nof the method in an industrial setting. This paper presents a\ntool prototype aiming at supporting non-experts in using formal\nveriﬁcation. The tooling approach is meant to be cost effective\nand change-supportive: user-friendliness is designed not only for\nthe non-expert, but also to require minimum effort so that formal\nveriﬁcation is triggered even for the non-enthusiast who is not\nwilling to push a button. To do so, we trigger, in a background\ntask, pre-deﬁned formal veriﬁcation checks at (almost) every\nchange of the model. We only display error messages in case\nof problem: the user is not disturbed if no problem is detected.\nTo prevent checks to be triggered all the time, we decide to\nconsider only local analyses (i.e., only checks which do not\nrequire knowledge of elements in a remote position in the model).\nThis restricts the sort of formal veriﬁcation that we support,\nbut this is a conscious choice: our motto is ”Let us ﬁrst make\nbasic techniques very user-friendly; more powerful ones will be\nconsidered only when at least the basic techniques have proven\nto be accepted.”",
        "keywords": [
            "tool; cost-effective; formal veriﬁcation; user friendliness; scalable"
        ],
        "authors": [
            "Vincent Aravantinos",
            "Sudeep Kanav"
        ],
        "file_path": "data/models/models17/Tool Support for Live Formal Verification.pdf"
    },
    {
        "title": "Partial Evaluation of OCL Expressions",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "In the academic literature, many uses of the Object Constraint Language (OCL) have been proposed. By contrast, the utilization of OCL in contemporary modelling tools lags behind, suggesting that leverage of OCL remains limited in practice. We consider this undeserved, and present a scheme for partially evaluating OCL expressions that allows one to capitalize on given OCL speciﬁcations for a wide array of purposes using a single implementation: a partial evaluator of OCL.",
        "keywords": [],
        "authors": [
            "Bastian Ulke",
            "Friedrich Steimann",
            "Ralf L¨ammel"
        ],
        "file_path": "data/models/models17/Partial Evaluation of OCL Expressions.pdf"
    },
    {
        "title": "SQL-PL4OCL : An automatic code generator from OCL to SQL Procedural Language",
        "submission-date": "2017/05",
        "publication-date": "2017/05",
        "abstract": "Design models are widely spread as core artifacts in software engineering. Yet, a key problem is how to fulﬁll correctly these blueprint speciﬁcations when code components are developed. The best possible scenario occurs when a source modeling language can be perfectly linked to a target language of election. Namely, a well deﬁned mapping bridges the gap between the source and the target language. Otherwise, manual encoding of the system design is cumbersome and error prone. In this setting, we introduce a SQL-PL1 code generator for OCL expressions that, in contrast to other proposals, is able to map OCL iterate and iterator expressions thanks to our use of stored procedures. More in detail, our source language is the Object Constraint Language (OCL), which nowadays is an ISO standard used to express constraints and queries in a textual notation on UML models. Our target language is the procedural language (PL) extension to the Structured Query Language (SQL). SQL is a special-purpose programming language designed for managing data in relational database management systems (RDBMS). The purpose of PL for SQL is to combine database language and procedural programming language. Although SQL is also an ISO standard, different RDBMS implement certain syntactic variations to the standard SQL notation. Thus, we had to adapt the implementation of our mapping to each of them. As implementation targets we selected MariaDB, PostgreSQL, and MS SQL Server. MariaDB and PostgreSQL were selected because they are open source and widely used by developers. MS SQL server was selected to be able to compare evaluation time from open source to commercial RDBMS. A variety of applications arises for a mapping from OCL to SQL expressions. Among others, there are three prominent types. These are i) evaluation of OCL expressions (analysis queries and metrics) on large model’s instances, ii) identiﬁ-cation of constraints during data modeling that have to be checked as integrity constraints on actual data; iii) automatic code generation from models. Indeed, our implementation was used as a key component of a toolkit that automatically generated ready-to-deploy web applications for secure data management from design models. Our component mapped and evaluated OCL constraints speciﬁed within authorization policies. Our code generator is deﬁned recursively over the structure of OCL expressions and it is implemented in the SQL-PL4OCL tool that is publicly available at [1]. The seminal work of the mapping presented here can be found in [2], [3]. The key idea that enables the mapping from OCL iterator expressions to iterative stored procedures remains the same, but the work detailed in [4] introduces a novel mapping from OCL expressions to SQL-PL stored procedures. In the novel mapping we have taken design decisions which have facilitated the recursive deﬁnition of the code generator and simpliﬁed its deﬁnition. These decisions have also helped to signiﬁcantly decrease the time required for the evaluation of the code generated. Regarding semantics, the new mapping is able to deal properly with the three-valued evaluation semantics of OCL. In addition, our original work and implementation was intended only for the procedural extension of MySQL, while our new deﬁnition eased the implementation of the mapping into other relational database management systems. In turn, we can now evaluate the resulting code using different RDBMS, which permits us to widen our discussion regarding efﬁciency in terms of evaluation-time of the code produced by SQL-PL4OCL tool.",
        "keywords": [],
        "authors": [
            "Marina Egea",
            "Carolina Dania"
        ],
        "file_path": "data/models/models17/SQL-PL4OCL An Automatic Code Generator from OCL to SQL Procedural Language.pdf"
    },
    {
        "title": "How is ATL Really Used? Language Feature Use in the ATL Zoo",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Studies of code repositories have long been used to understand the use of programming languages and to provide insight into how they should evolve. Such studies can highlight features that are rarely used and can safely be removed to simplify the language. Conversely, combinations of features that are frequently used together can be identified and possibly replaced with new features to improve the user experience. Unfortunately, this kind of research has not been as popular in Model Driven Development (MDD). More specifically, using repositories of model transformations (in any language) to understand how the features of these languages are used has not been investigated much, despite its potential benefits. In this paper, we study the use of the ATL model transformation language in an ATL transformation repository. We identify three research questions aimed at providing insight into how ATL’s features are actually used. Using the TXL source transformation language, we implement a parser-based analyzer to extract information from the ATL Zoo. We use this information to answer these research questions and provide additional observations based on manual inspection of ATL artifacts.",
        "keywords": [
            "Model transformations",
            "MDD",
            "ATL",
            "TXL"
        ],
        "authors": [
            "Gehan M. K. Selim",
            "James R. Cordy",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models17/How is ATL Really Used- Language Feature Use in the ATL Zoo.pdf"
    },
    {
        "title": "Co-evolution of Meta-Modeling Syntax and Informal Semantics in Domain-Specific Modeling Environments - A Case Study of AUTOSAR",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "One domain-specific modeling environment is centered around a domain-specific meta-model which defines syntax (modeling elements, e.g., classes) for the domain models. However, in order for the system designers to be able to construct meaningful models, semantics of the domain-specific meta-model needs to be described as well. This semantics is often provided in a form of informal natural language specifications that contain a set of design requirements, each describing the intended use of one or more modeling elements. Intuitively, introduction of new concepts into the modeling environment is expected to require changes in both meta-modeling syntax and informal semantics in such a way that their co-evolution is highly correlated. In order to test this hypothesis, we analyzed the relation between added classes, attributes, and connectors, as meta-modeling syntax, and modified/added design requirements, as meta-modeling semantics, in a case study of the AUTOSAR meta-modeling environment. We found that new AUTOSAR concepts usually require both new modeling elements and new design requirements, but surprisingly adding more elements is not always followed by more requirements. This finding is also validated by the moderately strong correlation between the evolution of these two AUTOSAR meta-modeling artifacts (Spearman’s ρ 0,63 and Kendall’s τ 0,49). For system designers, this means that both meta-modeling syntax and informal semantics is important to be considered in the analysis of domain-specific meta-model evolution, but it may not be enough for understanding the use of all modeling elements. For designers responsible for the maintenance of domain-specific meta-models, this means that more effort shall be put into describing the semantics of all introduced modeling elements.",
        "keywords": [],
        "authors": [
            "Darko Durisic",
            "Corrado Motta",
            "Miroslaw Staron",
            "Matthias Tichy"
        ],
        "file_path": "data/models/models17/Co-Evolution of Meta-Modeling Syntax and Informal Semantics in Domain-Specific Modeling Environments - A Case Study of AUTOSAR.pdf"
    },
    {
        "title": "Revisiting Visitors for Modular Extension of Executable DSMLs",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Executable Domain-Speciﬁc Modeling Languages (xDSMLs) are typically deﬁned by metamodels that specify their abstract syntax, and model interpreters or compilers that deﬁne their execution semantics. To face the proliferation of xDSMLs in many domains, it is important to provide language engineering facilities for opportunistic reuse, extension, and customization of existing xDSMLs to ease the deﬁnition of new ones. Current approaches to language reuse either require to anticipate reuse, make use of advanced features that are not widely available in programming languages, or are not directly applicable to metamodel-based xDSMLs. In this paper, we propose a new language implementation pattern, named REVISITOR, that enables independent extensibility of the syntax and semantics of metamodel-based xDSMLs with incremental compilation and without anticipation. We seamlessly implement our approach alongside the compilation chain of the Eclipse Modeling Framework, thereby demonstrating that it is directly and broadly applicable in various modeling environments. We show how it can be employed to incrementally extend both the syntax and semantics of the fUML language without requiring anticipation or re-compilation of existing code, and with acceptable performance penalty compared to classical handmade visitors.",
        "keywords": [],
        "authors": [
            "Manuel Leduc",
            "Thomas Degueule",
            "Benoit Combemale",
            "Tijs van der Storm",
            "Olivier Barais"
        ],
        "file_path": "data/models/models17/Revisiting Visitors for Modular Extension of Executable DSMLs.pdf"
    },
    {
        "title": "Language Design with Intent",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Software languages have always been an essential component of model-driven engineering. Their importance and popularity has been on the rise thanks to language workbenches, language-oriented development and other methodologies that enable us to quickly and easily create new languages speciﬁc for each domain. Unfortunately, language design is largely a form of art and has resisted most attempts to turn it into a form of science or engineering. In this paper we borrow concepts, techniques and principles from the domain of persuasive technology, or wider yet, design with intent — which was developed as a way to inﬂuence users behaviour for social and environmental beneﬁt. Similarly, we claim, software language designers can make conscious choices in order to inﬂuence the behaviour of language users. The paper describes a process of extracting design components from 24 books of eight categories (dragon books, parsing techniques, compiler construction, compiler design, language implementa-tion, language documentation, programming languages, software languages), as well as from the original set of Design with Intent cards and papers on DSL design. The resulting language design card toolkit can be used by DSL designers to cover important design decisions and make them with more conﬁdence.",
        "keywords": [],
        "authors": [
            "Vadim Zaytsev"
        ],
        "file_path": "data/models/models17/Language Design with Intent.pdf"
    },
    {
        "title": "2017 ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems MODELS 2017",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Table of contents.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Abrahão, Silvia",
            "Faugère, Madeleine",
            "Agner, Luciane T. W.",
            "Fohler, Gerhard",
            "Al-Refai, Mohammed",
            "Fouquet, Francois",
            "Antkiewicz, Michal",
            "Ghosh, Sudipto",
            "Aravantinos, Vincent",
            "Giorgini, Paolo",
            "Barais, Olivier",
            "Greenyer, Joel",
            "Barner, Simon",
            "Guerra, Esther",
            "Batot, Edouard",
            "Gutjahr, Timo",
            "Beckmann, Martin",
            "Hartmann, Thomas",
            "Benelallam, Amine",
            "Hidaka, Sochiro",
            "Bergmann, Gábor",
            "Hutchesson, Stuart",
            "Bertram, Vincent",
            "Ipatiov, Alexandru",
            "Bianculli, Domenico",
            "Izquierdo, Javier Luis Cánovas",
            "Bourcier, Johann",
            "Jouault, Frédéric",
            "Bourdeleau, Francis",
            "Jürjens, Jan",
            "Briand, Lionel",
            "Kanav, Sudeep",
            "Burger, Erik",
            "Kessentini, Wael",
            "Cabot, Jordi",
            "Khalil, Maged",
            "Cazzola, Walter",
            "Klare, Heiko",
            "Chechik, Marsha",
            "Kokaly, Sahar",
            "Cheng, Betty",
            "Kolb, Bernd",
            "Clarisó, Robert",
            "Kolovos, Dimitrios S.",
            "Combemale, Benoit",
            "Kramer, Max",
            "Cordy, James R.",
            "Lämmel, Ralf",
            "Cosentino, Valerio",
            "Langhammer, Michael",
            "Cuadrado, Jesús Sánchez",
            "Le Traon, Yves",
            "Czarnecki, Krzysztof",
            "Leduc, Manuel",
            "Dania, Carolina",
            "Lethbridge, Timothy C.",
            "Dániel, Varró",
            "Liang, Jia Hui",
            "de Lara, Juan",
            "Maoz, Shahar",
            "Debreceni, Csaba",
            "Michalke, Vanessa N.",
            "Degueule, Thomas",
            "Migge, Jörn",
            "Denney, Ewen",
            "Moawad, Assaad",
            "Deursen, Arie van",
            "Motta, Corrado",
            "Diewald, Alexander",
            "Mouline, Ludovic",
            "Dingel, Juergen",
            "Murashkin, Alexandr",
            "Dou, Wei",
            "Pai, Ganesh",
            "Dummann, Kolja",
            "Paige, Richard",
            "Durisic, Darko",
            "Paige, Richard F.",
            "Eder, Johannes",
            "Palomares, Javier",
            "Egea, Marina",
            "Pech, Vaclav",
            "Ernadote, Dominique",
            "Pérez, Daniel Gracia",
            "Famelis, Michalis",
            "Pomerantz, Nitzan",
            "Fohler, Gerhard"
        ],
        "file_path": "data/models/models17/Author index.pdf"
    },
    {
        "title": "The Next Evolution of MDE: A Seamless Integration of Machine Learning into Domain Modeling",
        "submission-date": "2017/03",
        "publication-date": "2017/03",
        "abstract": "Advances in software and sensors have led to a new generation of systems which can help to minimize human intervention in critical infrastructures, like the power grid. However, they have mainly been designed to face predictable situations, in order to react, for example, to a critical over-load. This is called known domain knowledge. However, such systems have also to face events that are unpredictable at design time. For instance, the electric consumption of a house depends on the number of persons living there, their activities, weather conditions, used devices, and so forth. Despite such behaviour is unpredictable at design time, it is identiﬁable and a hypothesis about it can be already formulated and solved later by observing past situations, once data becomes available. Sutcliffe et al., [1] suggest to call this known unknown. Machine learning algorithms are designed to resolve these unknowns, using ﬁne- or coarse-grained learning. Coarse-grained learning means extracting the average behaviour of a large dataset. Conversely, ﬁne-grained learning means specializing learning algorithms only on speciﬁc elements. In cases where datasets are composed of independent and het-erogenous entities, which behave very differently, finding one coarse-grained common behaviour can be difﬁcult or even inappropriate. For example, considering smart grids, the daily consumption of a factory follows a very different pattern than the consumption of an apartment. Thus, coarse-grained learning alone, which is based on the “law of large numbers ”, can be inaccurate for such systems. Additionally, any data changes requires the whole learning process to be recomputed. Instead, following a divide and conquer strategy, learning on ﬁner granularities can be considerably more efﬁcient [2], [3]. In accordance to the pedagogical concept [4], we refer to small ﬁne-grained learning units as “micro learning”. However, applying micro learning on systems, such as the electric grid, can potentially lead to many ﬁne-grained learning units, that need to be combined and synchronised with domain data. Learning frameworks like TensorFlow fo-cus solely on the learning ﬂow without any relation to the domain model. Consequently, domain data and its structure is expressed in different models than learning tasks, using different languages and tools. This leads to a separation of domain data, knowledge, known unknowns, and associated learning methods. Therefore, an appropriate structure to model learning units and their relationships to domain knowledge is required. To tame such complexity, we propose to weave micro machine learning seamlessly into data modeling. Specifically, our approach aims at: (1) Structuring complex learning tasks with reusable, chainable, and independently computable micro learning units. (2) Seamlessly integrating behavioural models which are known at design time, behavioural models that need to be learned at runtime, and domain models using common modeling concepts. (3) Automating the mapping between the mathematical representation expected by a speciﬁc machine learning algorithm and the domain representation [5] and independently updating micro learning units to be fast enough for online learning. As a natural extension of model-driven engineering ap-proaches, we take advantage of relationships between domain data and behavioural elements (learned or known at design time) to implicitly deﬁne a ﬁne-grained mapping of learning units and domain data. We implemented and integrated our approach into the open-source modeling framework GreyCat, which is specifically designed for the requirements of CPSs and IoT. We evaluate our approach on a concrete smart grid case study and show that: (1) Micro machine learning for such scenarios can be more accurate than coarse-grained learning (2) Performance is fast enough to be used for real-time analytics. The full paper has been published in [6].",
        "keywords": [
            "Domain modeling",
            "Live learning",
            "Model-driven engineering",
            "Meta modeling",
            "Cyber-physical systems",
            "Smart grids"
        ],
        "authors": [
            "Thomas Hartmann",
            "Assaad Moawad",
            "Francois Fouquet",
            "and Yves Le Traon"
        ],
        "file_path": "data/models/models17/The Next Evolution of MDE A Seamless Integration of Machine Learning into Domain Modeling.pdf"
    },
    {
        "title": "Translating target to source constraints in model-to-model transformations",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Model transformations are used to automate model manipulation in Model-Driven Engineering (MDE). In particular, model-to-model transformations produce target models (confor-mant to a target meta-model) from source ones (conformant to a source meta-model). While transformation correctness is crucial in MDE, developing transformations is error-prone due to the difficulty in testing them. This problem is further aggravated if the source and target meta-models contain OCL integrity constraints, as every transformed source model should satisfy the target integrity constraints.\n\nIn order to attack this problem, we present a novel method that translates target OCL constraints to the source meta-model using the transformation deﬁnition. This way, if a source model satisﬁes the advanced constraint, the transformed model will satisfy the target constraint. The method has been implemented for the ATL transformation language and integrated with the anATLyzer tool. We show its beneﬁts in combination with model ﬁnders, and the promising results of its validation using mutation techniques and transformations developed by third parties.",
        "keywords": [
            "Model-driven engineering; model transformations; integrity constraints; OCL; quality"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Esther Guerra",
            "Juan de Lara",
            "Robert Clarisó",
            "Jordi Cabot"
        ],
        "file_path": "data/models/models17/Translating Target to Source Constraints in Model-to-Model Transformations.pdf"
    },
    {
        "title": "Removal of Redundant Elements within UML Activity Diagrams",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "As the complexity of systems continues to rise, the use of model-driven development approaches becomes more widely applied. Still, many created models are mainly used for documentation. As such, they are not designed to be used in following stages of development, but merely as a means of improved overview and communication. In an effort to use existing UML2 activity diagrams of an industry partner (Daimler AG) as a source for automatic generation of software artifacts, we discovered, that the diagrams often contain multiple instances of the same element. These redundant instances might improve the readability of a diagram. However, they complicate further approaches such as automated model analysis or traceability to other artifacts because mostly redundant instances must be handled as one distinctive element. In this paper, we present an approach to automatically remove redundant ExecutableNodes within activity diagrams as they are used by our industry partner. The removal is implemented by merging the redundant instances to a single element and adding additional elements to maintain the original behavior of the activity. We use reachability graphs to argue that our approach preserves the behavior of the activity. Additionally, we applied the approach to a real system described by 36 activity diagrams. As a result 25 redundant instances were removed from 15 affected diagrams.",
        "keywords": [],
        "authors": [
            "Martin Beckmann",
            "Vanessa N. Michalke",
            "Andreas Vogelsang",
            "Aaron Schlutter"
        ],
        "file_path": "data/models/models17/Removal of Redundant Elements within UML Activity Diagrams.pdf"
    },
    {
        "title": "An Empirical Study on the Maturity of the Eclipse Modeling Ecosystem",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Since the early days of Model-driven Engineering (MDE), our community has been discussing the reasons why MDE had not quickly became mainstream. It is now clear the answer is a mix of technical and social factors, but among the former, the lack of maturity of MDE tools is often mentioned. The goal of this paper is to explore the question of whether this lack of maturity is actually true. We do so by comparing the maturity of over a hundred modeling and non-modeling projects living together in the Eclipse ecosystem. In both cases, we use the word project to refer to a variety of tools, libraries and other artefacts to build and manipulate software components, either at the model or code level. Our maturity model is based on code-centric and community metrics that we evaluate on the repository data for both kinds of projects. Their incubation status is also considered in the assessment. Results show that there are indeed diﬀerences between modeling and non-modeling projects, though less than we expected when setting up the study. Moreover, while the incu-bation status clearly separates non-modeling projects, the same is not true for modeling projects which seem to remain much more stable across their lifespan. We believe our results help to have a better perspective on maturity of modeling support nowadays and provide ideas for further analysis towards their improvement.",
        "keywords": [],
        "authors": [
            "Javier Luis Cánovas Izquierdo",
            "Valerio Cosentino",
            "Jordi Cabot"
        ],
        "file_path": "data/models/models17/An Empirical Study on the Maturity of the Eclipse Modeling Ecosystem.pdf"
    },
    {
        "title": "DREAMS Toolchain: Model-Driven Engineering of Mixed-Criticality Systems",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Mixed-criticality systems (MCS) aim at boosting the integration density in safety-critical systems, resulting into efficient systems, while simultaneously providing increased performance. The DREAMS project provides a cross-domain architectural style for MCS based on networked, virtualized multi-cores controlled by hierarchical resource managers. However, the availability of a platform is only one side of the coin: deploying mixed-critical applications to shared resources typically requires design-time configurations (e.g., to ensure real-time constraints or separation constraints mandated by safety regulations). These configurations are the outcome of complex optimization problems which are intractable in a manual process that also hardly can guarantee the consistency of all deployable artefacts nor their traceability to the requirements. However, existing toolchains lack support for MCS integration, and particularly DREAMS’ advanced platform capabilities.\nWe present an integrated model-driven toolchain and the underlying metamodels covering all relevant aspects of MCS including applications, timing, platforms, deployments, configurations and annotations for extra-functional properties such as safety. The approach focuses on the left branch of the V-cycle, and ranges from product-line and design space exploration to resource allocation and configuration generation. We report on the integration of exploration tools and a reconfiguration graph synthesizer, and evaluate the resulting toolchains in two use cases consisting of a product-line of wind power control applications and an avionic subsystem respectively.",
        "keywords": [
            "Model-Driven Engineering",
            "Mixed-Criticalitity Systems",
            "Safety",
            "Resource Management",
            "Product-Lines",
            "Design Space Exploration"
        ],
        "authors": [
            "Simon Barner",
            "Alexander Diewald",
            "Jörn Migge",
            "Ali Syed",
            "Gerhard Fohler",
            "Madeleine Faugère",
            "Daniel Gracia Pérez"
        ],
        "file_path": "data/models/models17/DREAMS Toolchain Model-Driven Engineering of Mixed-Criticality Systems.pdf"
    },
    {
        "title": "From Secure Business Process Modeling to Design-Level Security Veriﬁcation",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Tracing and integrating security requirements throughout the development process is a key challenge in security engineering. In socio-technical systems, security requirements for the organizational and technical aspects of a system are currently dealt with separately, giving rise to substantial misconceptions and errors. In this paper, we present a model-based security engineering framework for supporting the system design on the organizational and technical level. The key idea is to allow the involved experts to specify security requirements in the languages they are familiar with: business analysts use BPMN for procedural system descriptions; system developers use UML to design and implement the system architecture. Security requirements are captured via the language extensions SecBPMN2 and UMLsec. We provide a model transformation to bridge the conceptual gap between SecBPMN2 and UMLsec. Using UMLsec policies, various security properties of the resulting architecture can be veriﬁed. In a case study featuring an air trafﬁc management system, we show how our framework can be practically applied.",
        "keywords": [],
        "authors": [
            "Qusai Ramadan",
            "Mattia Salnitri",
            "Daniel Strüber",
            "Jan Jürjens",
            "Paolo Giorgini"
        ],
        "file_path": "data/models/models17/From Secure Business Process Modeling to Design-Level Security Verification.pdf"
    },
    {
        "title": "Model-driven Development of Safety Architectures",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "We describe the use of model-driven development for safety assurance of a pioneering NASA ﬂight operation involving a ﬂeet of small unmanned aircraft systems (sUAS) ﬂying beyond visual line of sight. The central idea is to develop a safety architecture that provides the basis for risk assessment and visualization within a safety case, the formal justiﬁcation of acceptable safety required by the aviation regulatory authority. A safety architecture is composed from a collection of bow tie diagrams (BTDs), a practical approach to manage safety risk by linking the identiﬁed hazards to the appropriate mitigation measures. The safety justiﬁcation for a given unmanned aircraft system (UAS) operation can have many related BTDs. In practice, however, each BTD is independently developed, which poses challenges with respect to incremental development, maintaining consistency across different safety artifacts when changes occur, and in extracting and presenting stakeholder speciﬁc information relevant for decision making. We show how a safety architecture reconciles the various BTDs of a system, and, collectively, provide an overarching picture of system safety, by considering them as views of a uniﬁed model. We also show how it enables model-driven development of BTDs, replete with validations, transformations, and a range of views. Our approach, which we have implemented in our toolset, AdvoCATE, is illustrated with a running example drawn from a real UAS safety case. The models and some of the innovations described here were instrumental in successfully obtaining regulatory ﬂight approval.",
        "keywords": [
            "Bow tie diagram",
            "Model-driven development",
            "Safety architecture",
            "Safety case",
            "Transformation",
            "Unmanned aircraft systems",
            "Views"
        ],
        "authors": [
            "Ewen Denney",
            "Ganesh Pai",
            "and Iain Whiteside"
        ],
        "file_path": "data/models/models17/Model-Driven Development of Safety Architectures.pdf"
    },
    {
        "title": "Raising Time Awareness in Model-Driven Engineering",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "The conviction that big data analytics is a key for the success of modern businesses is growing deeper, and the mobilisation of companies into adopting it becomes increasingly important. Big data integration projects enable companies to capture their relevant data, to efficiently store it, turn it into domain knowledge, and finally monetize it. In this context, historical data, also called temporal data, is becoming increasingly available and delivers means to analyse the history of applications, discover temporal patterns, and predict future trends. Despite the fact that most data that today’s applications are dealing with is inherently temporal current approaches, methodologies, and environments for developing these applications don’t provide sufficient support for handling time. We envision that Model-Driven Engineering (MDE) would be an appropriate ecosystem for a seamless and orthogonal integration of time into domain modelling and processing. In this paper, we investigate the state-of-the-art in MDE techniques and tools in order to identify the missing bricks for raising time-awareness in MDE and outline research directions in this emerging domain.",
        "keywords": [
            "Model-Driven Engineering",
            "Analytics",
            "Big Data",
            "Temporal Data",
            "Internet of Things"
        ],
        "authors": [
            "Amine Benelallam",
            "Thomas Hartmann",
            "Ludovic Mouline",
            "Francois Fouquet",
            "Johann Bourcier",
            "Olivier Barais",
            "and Yves Le Traon"
        ],
        "file_path": "data/models/models17/Raising Time Awareness in Model-Driven Engineering Vision Paper.pdf"
    },
    {
        "title": "On Additivity in Transformation Languages",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Some areas in computer science are characterized\nby a shared base structure for data artifacts (e.g., list, table,\ntree, graph, model), and dedicated languages for transforming\nthis structure. We observe that in several of these languages\nit is possible to identify a clear correspondence between some\nelements in the transformation code and the output they generate.\nConversely given an element in an output artifact it is often\npossible to immediately trace the transformation parts that are\nresponsible for its creation.\nIn this paper we formalize this intuitive concept by deﬁning\na property that characterizes several transformation languages\nin different domains. We name this property additivity: for a\ngiven ﬁxed input, the addition or removal of program elements\nresults in a corresponding addition or removal of parts of\nthe output. We provide a formal deﬁnition for additivity and\nargue that additivity enhances modularity and incrementality\nof transformation engineering activities, by enumerating a set\nof tasks that this property enables or facilitates. Then we\ndescribe how it is instantiated in some well-known transformation\nlanguages. We expect that the development of new formal results\non languages with additivity will beneﬁt from our deﬁnitions.",
        "keywords": [],
        "authors": [
            "Sochiro Hidaka",
            "Fr´ed´eric Jouault",
            "Massimo Tisi"
        ],
        "file_path": "data/models/models17/On Additivity in Transformation Languages.pdf"
    },
    {
        "title": "Property-based Locking in Collaborative Modeling",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Large-scale model-driven engineering projects are carried out collaboratively. Enabling a high degree of concurrency is required to make the traditionally rigid development processes more agile. The increasing number of collaborators increases the probability of introducing conﬂicts which need to be resolved manually by the collaborators. In case of highly interdependent models, avoiding conﬂicts by the use of locks can save valuable time. However, traditional locking techniques such as fragment-based and object-based strategies may impose unnecessary restrictions on editing, which can decrease the efﬁciency of collaboration.\n\nIn this paper, we propose a property-based locking approach that generalizes traditional locking techniques, and further allows more ﬁne-grained locks in order to restrict modiﬁcations only when necessary. A lock is considered to be violated if a match appears or disappears for its associated graph pattern (formula), which captures the property of the model that the upcoming edit transaction can be freely executed. An initial evaluation has been carried out using a case study of the MONDO EU project.",
        "keywords": [],
        "authors": [
            "Csaba Debreceni",
            "G´abor Bergmann",
            "Istv´an R´ath",
            "D´aniel Varr´o"
        ],
        "file_path": "data/models/models17/Property-Based Locking in Collaborative Modeling.pdf"
    },
    {
        "title": "Symbolic Execution for Realizability-Checking of Scenario-based Speciﬁcations",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Scenario-based speciﬁcation with the Scenario Modeling Language (SML) is an intuitive approach for formally specifying the behavior of reactive systems. SML is close to how humans conceive and communicate requirements, yet SML is executable and simulation and formal realizability checking can ﬁnd speciﬁcation ﬂaws early. The realizability checking complexity is, however, exponential in the number of scenarios and variables. Therefore algorithms relying on explicit-state exploration do not scale and, especially when speciﬁcations have message parameters and variables over large domains, fail to unfold their potential. In this paper, we present a technique for the symbolic execution of SML speciﬁcations that interprets integer message parameters and variables symbolically. It can be used for symbolic realizability checking and interactive symbolic simulation. We implemented the technique in SCENARIOTOOLS. Evaluation shows drastic performance improvements over the explicit-state approach for a range of examples. Moreover, sym- bolic checking produces more concise counter examples, which eases the comprehension of speciﬁcation ﬂaws.",
        "keywords": [],
        "authors": [
            "Joel Greenyer",
            "Timo Gutjahr"
        ],
        "file_path": "data/models/models17/Symbolic Execution for Realizability-Checking of Scenario-Based Specifications.pdf"
    },
    {
        "title": "ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Title page iii.pdf"
    },
    {
        "title": "Ecoreiﬁcation: Making Arbitrary Java Code Accessible to Metamodel-Based Tools",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Models are used in software engineering to describe parts of a system that are relevant for the computation of speciﬁc analyses, or the provision of speciﬁc functionality. Metamodeling languages such as Ecore make it possible to realize analyses and functionality with model-driven technology, such as transformation engines. If models conform to a metamodel that was expressed using Ecore, numerous Eclipse-based tools can be reused to directly analyze, display, or transform models. In many software projects, models are, however, realized with objects of plain-old Java classes rather than an explicit metamodel, so these popular tools cannot be used.\n\nIn this new ideas paper, we present an Ecoreiﬁcation approach, which can be used to automatically extract Ecore-conforming metamodels from Java code, and a code generator that combines the beneﬁts of both worlds. The resulting code can be used exactly as before, but it also uses the modeling infrastructure and implements all interfaces for Ecore-based tooling. This way, arbitrary non-standard models can be displayed and modiﬁed, for example using graphical Sirius editors, or transformed with well-proven transformation languages, such as QVT-O or ATL.",
        "keywords": [],
        "authors": [
            "Heiko Klare",
            "Erik Burger",
            "Max Kramer",
            "Michael Langhammer",
            "Timur Sa˘glam",
            "Ralf Reussner"
        ],
        "file_path": "data/models/models17/Ecoreification Making Arbitrary Java Code Accessible to Metamodel-Based Tools.pdf"
    },
    {
        "title": "Experiences with Teaching MPS in Industry\nTowards Bringing Domain Speciﬁc Languages Closer to Practitioners",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Domain speciﬁc languages (DSLs) bring substantial increase in productivity and quality and thus look very appealing to software engineering practitioners. Because language workbenches can drastically reduce the cost of building and maintaining DSLs and associated tooling, they catch the attention of technical leads and project managers in the industry. Effective use of language engineering technologies for software development requires specific knowledge about building DSLs in general and about language workbenches in particular. Practicing software engineers need to enrich their skills with a new software development approach and the supporting tools. In this paper we present our experiences with training and coaching software practitioners in developing domain specific languages and the associated tooling with Jetbrains’ Meta-Programming System. We distill the experience that we have gained over the last three years while running 16 trainings organized by three different organizations. The trainings were attended by over 50 developers, who work in different business domains and posses a wide variety of technical backgrounds, previous experiences and concrete needs. We present a set of challenges faced while teaching language engineering technologies in the industry. To address these challenges we developed a curriculum containing increasingly complex topics and an approach, which combines classical trainings with continuous coaching either remotely or on site. Based on our experience we distill a set of lessons learnt about the dissemination of language engineering technologies to practitioners. We identify several concrete needs which are key to broader adoption of language engineering in practice.",
        "keywords": [],
        "authors": [
            "Daniel Ratiu",
            "Vaclav Pech",
            "Kolja Dummann"
        ],
        "file_path": "data/models/models17/Experiences with Teaching MPS in Industry Towards Bringing Domain Specific Languages Closer to Practitioners.pdf"
    },
    {
        "title": "Bridging Proprietary Modelling and Open-Source Model Management Tools: The Case of PTC Integrity Modeller and Epsilon",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "While the majority of research on Model-Based Software Engineering revolves around open-source modelling frameworks such as EMF, the use of commercial and closed-source modelling tools such as RSA, Rhapsody, MagicDraw and PTC Integrity Modeller appears to be the norm in industry at present. This technical gap can prohibit industrial users from reaping the beneﬁts of state-of-the-art research-based tools in their practice. In this paper, we discuss an attempt to bridge a proprietary UML modelling tool (PTC Integrity Modeller), which is used for model-based development of safety-critical systems at Rolls-Royce, with an open-source family of languages for automated model management (Epsilon). We present the architecture of our solution, the challenges we encountered in developing it, and a performance comparison against the tool’s built-in scripting interface.",
        "keywords": [],
        "authors": [
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Dimitrios S. Kolovos",
            "Richard F. Paige",
            "Stuart Hutchesson"
        ],
        "file_path": "data/models/models17/Bridging Proprietary Modelling and Open-Source Model Management Tools The Case of PTC Integrity Modeller and Epsilon.pdf"
    },
    {
        "title": "A Fuzzy Logic Based Approach for Model-based Regression Test Selection",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Regression testing is performed to verify that previously developed functionality of a software system is not broken when changes are made to the system. Since executing all the existing test cases can be expensive, regression test selection (RTS) approaches are used to select a subset of them, thereby improving the efficiency of regression testing. Model-based RTS approaches select test cases on the basis of changes made to the models of a software system. While these approaches are useful in projects that already use model-driven development methodologies, a key obstacle is that the models are generally created at a high level of abstraction. They lack the information needed to build traceability links between the models and the coverage-related execution traces from the code-level test cases. \nIn this paper, we propose a fuzzy logic based approach named FLiRTS, for UML model-based RTS. FLiRTS automatically refines abstract UML models to generate multiple detailed UML models that permit the identification of the traceability links. The process introduces a degree of uncertainty, which is addressed by applying fuzzy logic based on the refinements to allow the classification of the test cases as retestable according to the probabilistic correctness associated with the used refinement. The potential of using FLiRTS is demonstrated on a simple case study. The results are promising and comparable to those obtained from a model-based approach (MaRTS) that requires detailed design models, and a code-based approach (DejaVu).",
        "keywords": [
            "fuzzy logic",
            "model-based testing",
            "regression test selection",
            "UML models"
        ],
        "authors": [
            "Mohammed Al-Refai\nWalter Cazzola\nSudipto Ghosh"
        ],
        "file_path": "data/models/models17/A Fuzzy Logic Based Approach for Model-Based Regression Test Selection.pdf"
    },
    {
        "title": "User Experience for Model-Driven Engineering: Challenges and Future Directions",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Since its infancy, Model Driven Engineering (MDE) research has primarily focused on technical issues. Although it is becoming increasingly common for MDE research papers to evaluate their theoretical and practical solutions, extensive usability studies are still uncommon. We observe a scarcity of User eXperience (UX)-related research in the MDE community, and posit that many existing tools and languages have room for improvement with respect to UX [26], [44], [37], where UX is a key focus area in the software development industry. We consider this gap a fundamental problem that needs to be addressed by the community if MDE is to gain widespread use. In this vision paper, we explore how and where UX fits into MDE by considering motivating use cases that revolve around different dimensions of integration: model integration, tool integration, and integration between process and tool support. Based on the literature and our collective experience in research and industrial collaborations, we propose future directions for addressing these challenges.",
        "keywords": [],
        "authors": [
            "Silvia Abrahão",
            "Francis Bordeleau",
            "Betty Cheng",
            "Sahar Kokaly",
            "Richard F. Paige",
            "Harald Störrle",
            "Jon Whittle"
        ],
        "file_path": "data/models/models17/User Experience for Model-Driven Engineering Challenges and Future Directions.pdf"
    },
    {
        "title": "Bringing DSE to life: exploring the design space of an industrial automotive use case",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "In order to cope with the rising complexity of today’s systems, model-based development of software-intensive embedded systems has become a de-facto standard in recent years. Such a development approach enables a variety of front-loading methods. Design space exploration is one of those techniques. However, in order to properly perform a valid exploration, a system model has to have a certain quality. This requires dedicated, meaningful models as an input according to well-known design principles, which entails the structuring of models according to different viewpoints and usage of dedicated models for each of these viewpoints.\n\nIn this work, we demonstrate how, based on an industrial application model represented in SysML, design space exploration methods can be efficiently applied to enable the synthesis of deployments from a logical (platform-independent) system models to technical (platform-specific) system models. More-over, we will demonstrate the applicability of this approach by a project conducted with Continental.",
        "keywords": [
            "design space exploration",
            "mbse",
            "deployment"
        ],
        "authors": [
            "Johannes Eder\nSergey Zverlov\nSebastian Voss\nMaged Khalil\nAlexandru Ipatiov"
        ],
        "file_path": "data/models/models17/Bringing DSE to Life Exploring the Design Space of an Industrial Automotive Use Case.pdf"
    },
    {
        "title": "Transformations of Software Product Lines: A Generalizing Framework based on Category Theory",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.",
        "keywords": [],
        "authors": [
            "Gabriele Taentzer",
            "Rick Salay",
            "Daniel Strüber",
            "and Marsha Chechik"
        ],
        "file_path": "data/models/models17/Transformations of Software Product Lines A Generalizing Framework Based on Category Theory.pdf"
    },
    {
        "title": "Software Product Lines with Design Choices: Reasoning about Variability and Design Uncertainty",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "When designing changes to a software product line (SPL), developers are faced with uncertainty about deciding among multiple possible SPL designs. Since each SPL design encodes a set of related products, dealing with multiple designs means that developers must reason about sets of sets of products. The additional degree of multiplicity is not well described by existing product line abstractions. In this paper, we propose an approach for dealing with design uncertainty within SPLs using a novel composition of variability modelling with an abstraction for capturing and managing design uncertainty. This allows developers to accurately describe the decisions involved in making changes to an SPL during the design stage and provides them with a framework for SPL design space exploration by analyzing and enforcing SPL properties.",
        "keywords": [],
        "authors": [
            "Michalis Famelis",
            "Julia Rubin",
            "Krzysztof Czarnecki",
            "Rick Salay",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models17/Software Product Lines with Design Choices Reasoning about Variability and Design Uncertainty.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Organization.pdf"
    },
    {
        "title": "Why Is My Component and Connector Views Speciﬁcation Unsatisﬁable?",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Component and connector (C&C) views speciﬁcations, with corresponding veriﬁcation and synthesis techniques, have been recently suggested as a means for formal yet intuitive structural speciﬁcation of component and connector models. One challenge for effective use of C&C views synthesis relates to the case where the speciﬁcation is unsatisﬁable.\n\nIn this work we present an approach to deal with unsatisﬁable C&C views speciﬁcations. First, we deﬁne a notion of a C&C views speciﬁcation core, a locally minimal unsatisﬁable subset of the views speciﬁcation. Second, based on the core, we generate explicit, concrete, structured natural-language report, which explains the cause of unsatisﬁability. Finally, we extend our work to support speciﬁcations with architecture styles, library components, and Boolean formulas beyond simple conjunctions.\n\nOur views core computation relies on a new translation to SAT, via Alloy, which is reﬁned enough to allow the extraction of detailed explanations. We implemented our work and evaluated it using 12 synthetic and real-world C&C views speciﬁcations. The evaluation examines the cost of the core computation and its effectiveness in reducing the size of the speciﬁcation.",
        "keywords": [],
        "authors": [
            "Shahar Maoz",
            "Nitzan Pomerantz",
            "Jan Oliver Ringert",
            "RaﬁShalom"
        ],
        "file_path": "data/models/models17/Why is My Component and Connector Views Specification Unsatisfiable-.pdf"
    },
    {
        "title": "Synthesis and Exploration of Multi-Level, Multi-Perspective Architectures of Automotive Embedded Systems",
        "submission-date": "2017/04",
        "publication-date": "2017/04",
        "abstract": "In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today’s engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multilevel, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of questions, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates relevant to their task at hand. Finally, using the modeling language Clafer, which is supported by automated backend reasoners, we show that it is possible to synthesize and explore optimal candidate architectures for two highly configurable automotive subsystems.",
        "keywords": [
            "Architecture Synthesis; Multi-Level Architectures; Multi-Perspective Architectures; EE Architecture; Architecture Optimization; Candidate Architectures; Early Design"
        ],
        "authors": [
            "Jordan A. Ross",
            "Alexandr Murashkin",
            "Jia Hui Liang",
            "Micha Antkiewicz",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/models/models17/Synthesis and Exploration of Multi-level- Multi-perspective Architectures of Automotive Embedded Systems -SoSYM Abstract-.pdf"
    },
    {
        "title": "ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Title page i.pdf"
    },
    {
        "title": "A Survey of Tool Use in Modeling Education",
        "submission-date": "2016/12",
        "publication-date": "Not found",
        "abstract": "We present the results of a survey of tool use in software modeling education conducted from December 2016 to March 2017. The survey was conducted among 150 professors who taught modeling in 30 countries from all regions of the world. Professors reported using 32 modeling tools. Top motivations for choosing tools are simplicity of learning and installing, as well as the tools being free and supporting the most important notations. Top complaints about tools included not interacting with other tools, not supporting sufficient modeling aspects, and being complex to use. Seven of the tools were used by more than one professor as their main tools, and we analyzed these in more depth. Among these 7, lack of feedback about models emerged as another key weakness. The tools varied very considerably regarding which of these strengths and weaknesses they exhibited. The key lessons from the paper are a) that tool developers have many opportunities to improve their products, and b) that educators might benefit from introducing students to multiple different tools.",
        "keywords": [
            "modeling tool; survey; education"
        ],
        "authors": [
            "Luciane T. W. Agner",
            "Timothy C. Lethbridge"
        ],
        "file_path": "data/models/models17/A Survey of Tool Use in Modeling Education.pdf"
    },
    {
        "title": "ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems (MODELS 2017)",
        "submission-date": "2017/01",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Preface.pdf"
    },
    {
        "title": "Managing Design-Time Uncertainty",
        "submission-date": "2017/03",
        "publication-date": "2017/03",
        "abstract": "Any software system is the accumulated result of many design decisions taken by its developers. During the course of development, however, developers are often uncertain about how to make these decisions. This uncertainty reﬂects lack of knowledge about the design of the system, rather than about the environment in which the system is intended to operate. It is therefore called design-time uncertainty, and is different from environmental uncertainty [1]. Addressing environmental uncertainty requires using strategies such as self-adaptation [2], which result in fully functional software systems, capable of operating under uncertain conditions, i.e., uncertainty-aware software. In contrast, design-time un- certainty (henceforth, also simply “uncertainty”) cannot be “coded away”. Rather, it must be tackled as part of the process of software development, i.e., using uncertainty-aware software development methodologies. Existing methodologies, languages and tools assume that their inputs do not contain any uncertainty. Thus, uncertainty is rendered an undesirable characteristic that developers should either avoid or remove altogether before resuming their work. This results in either costly delays or potentially premature – and therefore risky – resolutions of uncertainty as developers make provisional decisions and attempt to keep track of them in case they need to be undone. We present an alternative strategy: the explicit management of design time uncertainty as part of the course of software development [3]. Specifically, we build on previously published work for encoding alternative design decisions in partial models which can subsequently be used for tasks such as reasoning, re- finement and transformation [4]. These techniques had been implemented as partial model operators in MU-MMINT, an interactive modelling tool [5]. We combine these point solu- tions into a coherent, tool-supported methodology for tackling design-time uncertainty. This combination allows deferring the resolution of uncertainty for as long as necessary while the development work can continue.",
        "keywords": [],
        "authors": [
            "Michalis Famelis",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models17/Managing Design-Time Uncertainty.pdf"
    },
    {
        "title": "Active Domain-Speciﬁc Languages: making every mobile user a modeller",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Domain-speciﬁc languages (DSLs) are small languages tailored to a certain application area, like logistics, web application testing or smart city planning. Traditionally, the use of DSLs has been limited to a static setting in desktop or web editors. However, in this paper, we claim that DSLs can be central components of mobile collaborative applications. In our vision, graphical DSLs can be extended to make use of mobility and context, and integrate heterogeneous information gathered from open APIs. We call this new generation languages “active DSLs”. We foresee a range of scenarios where active DSLs can be useful. On the one hand, they can be used more ﬂexibly in remote locations by enabling local collaboration of several mobile devices using their short-range communication capabilities. On the other hand, they can be extended with contextual features like geolocation, allowing the integration of maps and geo-services within the DSL, or the DSL rendering customization in response to contextual information. Active DSLs can also retrieve information from open APIs, in which case, models deﬁned with the DSL become aggregators of heterogeneous data. In this paper, we explain our vision for active DSLs and the ﬁrst steps towards its realization in the DSL-comet tool. The tool permits creating and using mobile graphical DSLs on iOS devices, and their seamless use in desktop environments.",
        "keywords": [
            "active DSL; graphical modelling language; ﬂexible modelling; mobile application; API integration"
        ],
        "authors": [
            "Diego Vaquero-Melchor",
            "Javier Palomares",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models17/Active Domain-Specific Languages Making Every Mobile User a Modeller.pdf"
    },
    {
        "title": "Ontology-Based Pattern for System Engineering",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "System engineering is a multi-domain process that encompasses the design, realization, delivery, and management of complex systems or system of systems. The Model-Based System Engineering (MBSE) approach is commonly accepted by the system engineers community that depends up on the creation of centralized models to produce the expected deliverables. Standard metamodels such as UML, SysML, or NMM/NAF are typically used to describe the relevant concepts for these descriptive models. However, there is a need to also use domain speciﬁc languages (aka ontologies) to ease the communication between all the system engineering stakeholders.\nThe author proposed an approach in previous works to reconcile the usage of complex but necessary predeﬁned metamodels with dedicated ontologies. This solution speeds up the creation of model-based documents. However, the imple-mentation of such approach revealed that the modeling users are expecting a solution in-between the frozen metamodel and the speciﬁc ontology approach; a set of predeﬁned modeling features addressing recurrent engineering concerns completed by project speciﬁc concerns. Among the recurrent concerns there are the requirement elicitation, the functional analysis, the system interface deﬁnitions. . . .\nThis paper shows how this balance can be addressed through ontology-based patterns developed as modular mod-eling features blocks. Since these blocks are applied in the context of model-based system engineering we also named them MBSE Enablers. The paper proposes a solution to a new issue raised by this pattern reuse expectations; a dynamic mapping is required between the building blocks and the existing models. The proposed method is based on the category theory which brings a theoretical foundation to ensure models are correctly managed. The global idea of the extended approach is to speed up again the modeling tool customizations letting the system engineers focusing as far as possible on the systems to be designed.",
        "keywords": [
            "MBSE",
            "Model-Based System Engineering",
            "System Engineering",
            "Metamodel",
            "Ontology",
            "Dynamic Ontology Mapping",
            "Ontology Pattern"
        ],
        "authors": [
            "Dr Dominique Ernadote"
        ],
        "file_path": "data/models/models17/Ontology-Based Pattern for System Engineering.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models19/Steering Committee.pdf"
    },
    {
        "title": "Modeling approach and evaluation criteria for adaptable architectural runtime model instances",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "An architectural runtime model is a causally connected abstract representation of a system that allows monitoring the system and adapting its conﬁguration. Since systems are often constructed to operate continuously, the corresponding runtime model instances need to be long-living and available without interruptions. An interruption occurs if a model needs to be re-instantiated with a new version of the modeling language implementation to support other kinds of information. Adaptable runtime models instances can render such interruptions unnecessary and enable changing information demands at runtime. They support multiple abstraction levels for different model parts and allow adjusting over time which details of the system and its environment are represented. This helps to focus the attention for effective and efﬁcient decision making. In this vision paper we present the fundamental idea of a generic modeling language for adaptable architectural runtime model instances and propose requirements and quality characteristics as criteria for its evaluation.",
        "keywords": [],
        "authors": [
            "Thomas Brand and Holger Giese"
        ],
        "file_path": "data/models/models19/Modeling Approach and Evaluation Criteria for Adaptable Architectural Runtime Model Instances.pdf"
    },
    {
        "title": "Model-Based Resource Analysis and Synthesis of Service-Oriented Automotive Software Architectures",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Abstract—Context: Automotive software architectures describe distributed functionality through an interplay of software components. One drawback of today’s architectures is their strong integration into the onboard communication network based on predefined dependencies at design-time. To foster independence, the idea of service-oriented architecture (SOA) provides a suitable prospect as network communication is established dynamically at run-time. Aim: We target to provide a model-based design methodology for analysing and synthesising hardware resources of automotive service-oriented architectures. Approach: For the approach, we apply the concepts of design space exploration and simulation to analyse and synthesise deployment configurations at an early stage of development. Result: We present an architecture candidate for an example function from the domain of automated driving. Based on corresponding simulation results, we gained insights about the feasibility to implement this candidate within our currently considered next E/E architecture generation. Conclusion: The introduction of service-oriented architectures strictly requires early run-time assessments. In order to get there, the usage of models and model transformations depict reasonable ways by additionally accounting quality and development speed.",
        "keywords": [
            "Service-oriented architecture",
            "real-time behaviour",
            "model-based design",
            "automotive architectures"
        ],
        "authors": [
            "Philipp Obergfell",
            "Stefan Kugele",
            "Eric Sax"
        ],
        "file_path": "data/models/models19/Model-Based Resource Analysis and Synthesis of Service-Oriented Automotive Software Architectures.pdf"
    },
    {
        "title": "Towards System-Level Testing with Coverage Guarantees for Autonomous Vehicles",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Since safety-critical autonomous vehicles need to interact with an immensely complex and continuously changing environment, their assurance is a major challenge. While systems engineering practice necessitates assurance on multiple levels, existing research focuses dominantly on component-level assurance while neglecting complex system-level trafﬁc scenarios. In this paper, we aim to address the system-level testing of the situation-dependent behavior of autonomous vehicles by combining various model-based techniques on different levels of abstraction. (1) Safety properties are continuously monitored in challenging test scenarios (obtained in simulators or ﬁeld tests) using graph query and complex event processing techniques. To precisely quantify the coverage of an existing test suite with respect regulations of safety standards, (2) we provide qualitative abstractions of causal, temporal, or geospatial data recorded in individual runs into situation graphs, which allows to systematically measure system-level situation coverage (on an abstract level) wrt. safety concepts captured by domain experts. Moreover, (3) we can systematically derive new challenging (abstract) situations which justiﬁably lead to runtime behavior which has not been tested so far by adapting consistent graph generation techniques, thus increasing situation coverage. Finally, (4) such abstract test cases are concretized so that they can be investigated in a real or simulated context.",
        "keywords": [
            "Model-based testing",
            "Autonomous vehicles",
            "Cyber-Physical Systems",
            "System-level testing",
            "Test coverage"
        ],
        "authors": [
            "Istv´an Majzik",
            "Oszk´ar Semer´ath",
            "Csaba Hajdu",
            "Krist´of Marussy",
            "Zolt´an Szatm´ari",
            "Zolt´an Micskei",
            "Andr´as V¨or¨os",
            "Aren A. Babikian and D´aniel Varr´o"
        ],
        "file_path": "data/models/models19/Towards System-Level Testing with Coverage Guarantees for Autonomous Vehicles.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "2019/00",
        "publication-date": "2019/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models19/Copyright notice.pdf"
    },
    {
        "title": "Automated Classiﬁcation of Metamodel Repositories: A Machine Learning Approach",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Manual classiﬁcation methods of metamodel repositories require highly trained personnel and the results are usually inﬂuenced by the subjectivity of human perception. Therefore, automated metamodel classiﬁcation is very desirable and stringent. In this work, Machine Learning techniques have been employed for metamodel automated classiﬁcation. In particular, a tool implementing a feed-forward neural network is introduced to classify metamodels. An experimental evaluation over a dataset of 555 metamodels demonstrates that the technique permits to learn from manually classiﬁed data and effectively categorize incoming unlabeled data with a considerably high prediction rate: the best performance comprehends 95.40% as success rate, 0.945 as precision, 0.938 as recall, and 0.942 as F1 score.",
        "keywords": [],
        "authors": [
            "Phuong T. Nguyen",
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Alfonso Pierantonio",
            "Ludovico Iovino"
        ],
        "file_path": "data/models/models19/Automated Classification of Metamodel Repositories A Machine Learning Approach.pdf"
    },
    {
        "title": "Guided Architecture Trade Space Exploration: Fusing Model Based Engineering & Design by Shopping",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Advances in model-based system engineering have greatly increased the predictive power of models and the analyses that can be run on them. At the same time, designs have become more modular and component-based. It can be difficult to manually explore all possible system designs due to the sheer number of possible architectures and configurations; design space exploration has arisen as a solution to this challenge.\nIn this work, we present the Guided Architecture Trade Space Explorer (GATSE), software which connects an existing model based engineering language (AADL) and tool (OSATE) to an existing design space exploration tool (ATSV). GATSE, AADL, and OSATE are all designed to be easily extended by users, which enables relatively straightforward domain-customizations. ATSV, combined with these customizations, lets system designers “shop” for candidate architectures and interactively explore the architectural trade space according to any quantifiable quality attribute or system characteristic. We evaluate GATSE according to an established framework for variable system architectures, and demonstrate its use on an avionics subsystem.",
        "keywords": [
            "Design Space Exploration",
            "Search-Based System Engineering",
            "Model-Based Engineering",
            "Guided Optimization"
        ],
        "authors": [
            "Sam Procter",
            "Lutz Wrage"
        ],
        "file_path": "data/models/models19/Guided Architecture Trade Space Exploration Fusing Model Based Engineering - Design by Shopping.pdf"
    },
    {
        "title": "RaM: Causally-connected and Requirements-aware Runtime Models using Bayesian Learning",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "A model at runtime can be defined as an abstract representation of a system, including its structure and behaviour, which exist alongside with the running system. Runtime models provide support for decision-making and reasoning based on design-time knowledge but, also based on information that may emerge at runtime and which was not foreseen before execution. A challenge that persists is the update of runtime models during the execution to support up-to-date information for reasoning and decision-making. New techniques based on machine learning (ML) and Bayesian Learning offer great potential to support the update of runtime models during execution. Runtime models can be updated using these new techniques to, therefore, offer better-informed decision-making based on evidence collected at runtime. The techniques we use in this paper are based on a novel implementation of Partially Observable Markov Decision Processes (POMDPs). In this paper, we demonstrate how given the requirements specification, a Requirements-aware runtime model based on POMDPs (RaM-POMDP) is defined. We study in detail the nature of such runtime models coupled with consideration of the Bayesian inference algorithms and tools that provide evidence of unexpected/surprising changes in the environment. We show how the RaM-POMDPs and the MAPE-K loop offer the basis of the software architecture presented and how the required casual connection of runtime models is realized. Specifically, we demonstrate how according to evidence of changes in the systems, collected by the monitoring infrastructure and using Bayesian inference, the runtime models are updated and inferred (i.e. the first aspect of the causal connection). We also demonstrate how the running system changes its runtime model, producing therefore the corresponding self-adaptations. These self-adaptations are reflected on the managed system (i.e. the second aspect of the causal connection) to better satisfy the requirements specifications and improve conformance to its service level agreements (SLAs). The experiments have been applied to a real case study for the networking application domain.",
        "keywords": [
            "Runtime models",
            "causal connection",
            "decision-making",
            "uncertainty",
            "POMDPs",
            "Bayesian inference/learning"
        ],
        "authors": [
            "Nelly Bencomo",
            "Luis H. Garcia-Paucar"
        ],
        "file_path": "data/models/models19/RaM Causally-Connected and Requirements-Aware Runtime Models using Bayesian Learning.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Alexander Pretschner",
            "Sebastian Voss",
            "Loli Burgueño"
        ],
        "file_path": "data/models/models19/Organizing Committee.pdf"
    },
    {
        "title": "An LSTM-Based Neural Network Architecture for Model Transformations",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Model transformations are a key element in any model-driven engineering approach. But writing them is a time-consuming and error-prone activity that requires specific knowledge of the transformation language semantics. We propose to take advantage of the advances in Artificial Intelligence and, in particular Long Short-Term Memory Neural Networks (LSTM), to automatically infer model transformations from sets of input-output model pairs. Once the transformation mappings have been learned, the LSTM system is able to autonomously transform new input models into their corresponding output models without the need of writing any transformation-specific code. We evaluate the correctness and performance of our approach and discuss its advantages and limitations.",
        "keywords": [
            "MDE",
            "model transformations",
            "LSTM ANN"
        ],
        "authors": [
            "Loli Burgueño",
            "Jordi Cabot",
            "Sébastien Gérard"
        ],
        "file_path": "data/models/models19/An LSTM-Based Neural Network Architecture for Model Transformations.pdf"
    },
    {
        "title": "Model-based, Platform-independent Logging for Heterogeneous Targets",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "A recurring issue in generative approaches, in particular if they generate code for multiple target languages, is logging. How to ensure that logging is performed consistently for all the supported languages? How to ensure that the specific semantics of the source language, e.g. a modeling language or a domain-specific language, is reflected in the logs? How to expose logging concepts directly in the source language, so as to let developers specify what to log? This paper reports on our experience developing a concrete logging approach for ThingML, a textual modeling language built around asynchronous components, statecharts and a first-class action language, as well as a set of “compilers” targeting C, Go, Java and JavaScript.",
        "keywords": [],
        "authors": [
            "Brice Morin and Nicolas Ferry"
        ],
        "file_path": "data/models/models19/Model-Based- Platform-Independent Logging for Heterogeneous Targets.pdf"
    },
    {
        "title": "CONDEnSe: Contract-Based Design Synthesis",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "It is difﬁcult to maintain consistency between artifacts produced during the development of mechatronic systems, and to ensure the successful integration of independently developed parts. The difﬁculty stems from the complex, multidisciplinary nature of the problem, with multiple artifacts produced by each engineering domain, throughout the design process, and across supplier chains.\n\nIn this work, we develop a methodology and a tool, CONDEnSe, that given a set of Assume/Guarantee (A/G) contracts that capture the system requirements, and a high-level decomposition of the system model, automatically generates design variants that respect the requirements and exports those variants to different engineering tools for analysis.\n\nOur methodology makes use of a contract-based design algebra to ensure that generated artifacts for all design variants are consistent by construction, even when the process is modularized and independently developed parts are only later integrated. In contrast with earlier work, our approach reduces the search space to models that comply with the captured design requirements.",
        "keywords": [
            "contracts",
            "synthesis",
            "mbse",
            "integration"
        ],
        "authors": [
            "C´esar Augusto Santos",
            "Amr Hany Saleh",
            "Tom Schrijvers",
            "Mike Nicolai"
        ],
        "file_path": "data/models/models19/CONDEnSe Contract Based Design Synthesis.pdf"
    },
    {
        "title": "Bootstrapping MDE Development from ROS\nManual Code - Part 2: Model Generation",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "In principle, Model-Driven Engineering (MDE) addresses central aspects of robotics software development. Domain experts could leverage the expressiveness of models; implementation details over different hardware could be handled by automatic code generation. In practice, most evidence points to manual code development as the norm, despite several MDE efforts in robotics. Possible reasons for this disconnect are the wide ranges of applications and target platforms making all-encompassing MDE IDEs hard to develop and maintain, with developers reverting to writing code manually. Acknowledging this, and given the opportunity to leverage a large corpus of open-source software widely adopted by the robotics community, we pursue modeling as a complement, rather than an alternative, to manually written code. Our previous work introduced metamodels to describe components, their interactions, and their resulting composition, as inspired by, but not limited to, the de-facto standard Robot Operating System (ROS). In this paper we put such metamodels into use through two contributions [1]. First, we automate the generation of models from manually written artifacts through extraction from source code and runtime system monitoring. Second, we make available an easy-to-use web infrastructure to perform the extraction, together with a growing database of models so generated. Our aim with this tooling, publicly available both as-a-service and as source code, is to lower the MDE barrier for practitioners and leverage models to 1) improve the understanding of manually written code; 2) perform correctness checks; and 3) systematize the deﬁnition and adoption of best practices through large-scale generation of models from existing code. A comprehensive example is provided as a walk-through for robotics software practitioners.",
        "keywords": [
            "ROS",
            "models",
            "development environments"
        ],
        "authors": [
            "Nadia Hammoudeh Garcia",
            "Ludovic Delval",
            "Mathias L¨udtke",
            "Andre Santos",
            "Bj¨orn Kahl and Mirko Bordignon"
        ],
        "file_path": "data/models/models19/Bootstrapping MDE Development from ROS Manual Code - Part 2 Model Generation.pdf"
    },
    {
        "title": "Domain-Level Observation and Control for Compiled Executable DSLs",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Executable Domain-Specific Languages (DSLs) are commonly defined with either operational semantics (i.e., interpretation) or translational semantics (i.e., compilation). An interpreted DSL relies on domain concepts to specify the possible execution states and steps, which enables the observation and control of executions using the very same domain concepts. In contrast, a compiled DSL relies on a transformation to an arbitrarily different target language. This creates a conceptual gap, where the execution can only be observed and controlled through target domain concepts, to the detriment of experts or tools that only understand the source domain. To address this problem, we propose a language engineering architecture for compiled DSLs that enables the observation and control of executions using source domain concepts. The architecture requires the definition of the source domain execution steps and states, along with a feedback manager that translates steps and states of the target domain back to the source domain. We evaluate the architecture with two different compiled DSLs, and show that it does enable domain-level observation and control while increasing execution time by 2× in the worst observed case.",
        "keywords": [
            "Software Language Engineering",
            "Domain-Specific Languages",
            "Executable DSL",
            "Compilation",
            "Feedback"
        ],
        "authors": [
            "Erwan Bousse",
            "Manuel Wimmer"
        ],
        "file_path": "data/models/models19/Domain-Level Observation and Control for Compiled Executable DSLs.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Alferez",
            "Mauricio; Atlee",
            "Joanne M.; Auffinger",
            "Yuri; Babikian",
            "Aren A.; Bencomo",
            "Nelly; Besnard",
            "Valentin; Bordignon",
            "Mirko; Bousse",
            "Erwan; Brand",
            "Thomas; Briand",
            "Lionel; Brun",
            "Matthias; Bucchiarone",
            "Antonio; Búr",
            "Márton; Burdusel",
            "Alexandru; Burgueño",
            "Loli; Cabot",
            "Jordi; Cheng",
            "Betty; Cicchetti",
            "Antonio; de Lara",
            "Juan; Deval",
            "Ludovic; DeVries",
            "Byron; Dhaussy",
            "Philippe; Dingel",
            "Juergen; Di Rocco",
            "Juri; Di Ruscio",
            "Davide; Elkhatib",
            "Yehia; Eric",
            "Sax; Ferry",
            "Nicolas; Fouquet",
            "Francois; García-Domínguez",
            "Antonio; Garcia Paucar",
            "Luis H.; García-Paucar",
            "Luis Hernán; Gérard",
            "Sébastien; Ghezzi",
            "Carlo; Giese",
            "Holger; Goes",
            "Peter; Guerra",
            "Esther; Hajdu",
            "Csaba; Hammoudeh Garcia",
            "Nadia; Hany Saleh",
            "Amr; Hartmann",
            "Thomas; Hassan",
            "Ahmed E.; Hoyos Rodriguez",
            "Horacio; Hu",
            "Zhenjiang; Iovino",
            "Ludovico; Iqbal",
            "Muhammad Zohaib; John",
            "Stefan; Jouault",
            "Frédéric; Jumagaliyev",
            "Assylbek; Jürjens",
            "Jan; Kahl",
            "Björn; Khan",
            "Muhammad Uzair; Kneisel",
            "Peter; Kolovos",
            "Dimitris; Kusmenko",
            "Evgeny; Le Traon",
            "Yves; Lüdtke",
            "Mathias; Majzik",
            "István; Marcony",
            "Annapaola; Marussy",
            "Kristóf; Micskei",
            "Zoltán; Moawad",
            "Assaad; Morin",
            "Brice; Nguyen",
            "Phuong T.; Nickels",
            "Sebastian; Nicolai",
            "Mike; Oliva",
            "Gustavo Ansaldi; Paige",
            "Richard; Parra-Ullauri",
            "Juan Marcelo; Pastore",
            "Fabrizio; Pavlitskaya",
            "Svetlana; Peldszus",
            "Sven; Philipp",
            "Obergfell"
        ],
        "file_path": "data/models/models19/Author Index.pdf"
    },
    {
        "title": "Pitfalls Analyzer: Quality Control for Model-Driven Data Science Pipelines",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Data science pipelines are a sequence of data processing steps that aim to derive knowledge and insights from raw data. Data science pipeline tools simplify the creation and automation of data science pipelines by providing reusable building blocks that users can drag and drop into their pipelines. Such a graphical, model-driven approach enables users with limited data science expertise to create complex pipelines. However, recent studies show that there exist several data science pitfalls that can yield spurious results and, consequently, misleading insights. Yet, none of the popular pipeline tools have built-in quality control measures to detect these pitfalls. Therefore, in this paper, we propose an approach called Pitfalls Analyzer to detect common pitfalls in data science pipelines. As a proof-of-concept, we implemented a prototype of the Pitfalls Analyzer for KNIME, which is one of the most popular data science pipeline tools. Our prototype is model-driven, since the detection of pitfalls is accomplished using pipelines that were created with KNIME building blocks. To showcase the effectiveness of our approach, we run our prototype on 11 pipelines that were created by KNIME experts for 3 Internet-of-Things (IoT) projects. The results indicate that our prototype ﬂags all and only those instances of the pitfalls that we were able to ﬂag while manually inspecting the pipelines.",
        "keywords": [
            "Data science pipelines",
            "model-driven engineering",
            "quality control",
            "data science pitfalls"
        ],
        "authors": [
            "Gopi Krishnan Rajbahadur",
            "Gustavo Ansaldi Oliva",
            "Ahmed E. Hassan",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models19/Pitfalls Analyzer Quality Control for Model-Driven Data Science Pipelines.pdf"
    },
    {
        "title": "Using Models to Enable Compliance Checking Against the GDPR: An Experience Report",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Damiano Torre",
            "Ghanem Soltana",
            "Mehrdad Sabetzadeh",
            "Lionel C. Briand",
            "Yuri Auffinger",
            "and Peter Goes"
        ],
        "file_path": "data/models/models19/Table of contents.pdf"
    },
    {
        "title": "Model-Driven Design of City Spaces via Bidirectional Transformations",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Technological advances enable new kinds of smart environments exhibiting complex behaviors; smart cities are a notable example. Smart functionalities heavily depend on space and need to be aware of entities typically found in the spatial domain, e.g. roads, intersections or buildings in a smart city. We advocate a model-based development, where the model of physical space, coming from the architecture and civil engineering disciplines, is transformed into an analyzable model upon which smart functionalities can be embedded. Such models can then be formally analyzed to assess a composite system design. We focus on how a model of physical space speciﬁed in the CityGML standard language can be transformed into a model amenable to analysis and how the two models can be automatically kept in sync after possible changes. This approach is essential to guarantee safe model-driven development of composite systems inhabiting physical spaces. We showcase transformations of real CityGML models in the context of scenarios concerning both design time and runtime analysis of space-dependent systems.",
        "keywords": [
            "Bidirectional Model Transformations",
            "Model-driven Engineering",
            "CityGML",
            "Cyber-physical spaces"
        ],
        "authors": [
            "Ennio Visconti",
            "Christos Tsigkanos",
            "Zhenjiang Hu",
            "Carlo Ghezzi"
        ],
        "file_path": "data/models/models19/Model-Driven Design of City Spaces via Bidirectional Transformations.pdf"
    },
    {
        "title": "Applying MDD in the Content Management System Domain: Scenarios and Empirical Assessment",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Content Management Systems (CMSs) such as Joomla and WordPress dominate today’s web. Enabled by standardized extensions, administrators can build powerful web applications for diverse customer demands. However, developing CMS extensions requires sophisticated technical knowledge, and the highly schematic code structure of an extension gives rise to errors during typical development and migration scenarios. Model-driven development (MDD) seems to be a promising paradigm to address these challenges, however it has not found adoption in the CMS domain yet. Systematic evidence of the benefit of applying MDD in this domain could facilitate its adoption; however, an empirical investigation of this benefit is currently lacking.\n\nIn this paper, we present a mixed-method empirical investigation of applying MDD in the CMS domain, based on an interview suite, a controlled experiment, and a field experiment. We consider three scenarios of developing new (both independent and dependent) CMS extensions and of migrating existing ones to a new major platform version. The experienced developers in our interviews acknowledge the relevance of these scenarios and report on experiences that render them suitable candidates for a successful application of MDD. We found a particularly high relevance of the migration scenario. Our experiments largely confirm the potentials and limits of MDD as identified for other domains. In particular, we found a productivity increase up to factor 17 during the development of CMS extensions. Furthermore, our observations highlight the importance of good tooling that seamlessly integrates with already used tool environments and processes.",
        "keywords": [
            "Model-Driven Development",
            "Content Management Systems",
            "Empirical Assessment"
        ],
        "authors": [
            "Dennis Priefer",
            "Peter Kneisel",
            "Wolf Rost",
            "Daniel Str¨uber",
            "Gabriele Taentzer"
        ],
        "file_path": "data/models/models19/Applying MDD in the Content Management System Domain Scenarios and Empirical Assessment.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Alfonso Pierantonio",
            "Antonio Cicchetti",
            "Birgit Demuth",
            "Daniel Amyot",
            "Davide Di Ruscio",
            "Daniel Varro",
            "Dimitris Kolovos",
            "Don Batory",
            "Esther Guerra",
            "Fiona Polack",
            "Friedrich Steimann",
            "Gregor Engels",
            "Hong Mei",
            "Houari Sahraoui",
            "Ileana Ober",
            "Iris Reinhartz-Berger",
            "Jocelyn Simmonds",
            "Jörg Kienzle",
            "Manuel Wimmer",
            "Michalis Famelis",
            "Mira Balaban",
            "Nelly Bencomo",
            "Peter Clarke",
            "Pieter van Gorp",
            "Regina Hebig",
            "Thomas Kühne",
            "Timothy Lethbridge",
            "Yu Jiang"
        ],
        "file_path": "data/models/models19/Program Committee.pdf"
    },
    {
        "title": "A Modelling Language to Support the Evolution of Multi-Tenant Cloud Data Architectures",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Multi-tenant data architectures enable efﬁcient resource utilization in cloud applications, but are currently being implemented in industry and research using manual coding techniques that tend to be time consuming and error prone. We propose a novel domain-speciﬁc modeling language, CadaML, to automatically manage the development and evolution of cloud data architectures that (a) adopt multi-tenancy and/or (b) comprise of a combination of different storage solutions such as relational and non-relational databases, and blob storage. CadaML provides concepts and notations to support abstract modelling of a multi-tenant data architecture, and also provides tools to validate the data architecture and automatically produce application code. We rigorously evaluate CadaML through a user experiment where developers of various capabilities are asked to re-architect the data layer of an industrial business process analysis application. We observe that CadaML users required 3.5x less development time than manual coders. In addition to improved productivity, CadaML users highlighted other beneﬁts gained in terms of reliability of generated code and usability.",
        "keywords": [
            "Domain-Speciﬁc modeling",
            "Model-Driven Engineering",
            "Cloud Computing",
            "Multi-tenancy",
            "Software Evolution",
            "Code Generation"
        ],
        "authors": [
            "Assylbek Jumagaliyev",
            "Yehia Elkhatib"
        ],
        "file_path": "data/models/models19/A Modelling Language to Support the Evolution of Multi-tenant Cloud Data Architectures.pdf"
    },
    {
        "title": "Automatic Generation of Atomic Consistency Preserving Search Operators for Search-Based Model Engineering",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Recently there has been increased interest in combining the fields of Model-Driven Engineering (MDE) and Search-Based Software Engineering (SBSE). Such approaches use meta-heuristic search guided by search operators (model mutators and sometimes breeders) implemented as model transformations. The design of these operators can substantially impact the effectiveness and efficiency of the meta-heuristic search. Currently, designing search operators is left to the person specifying the optimisation problem. However, developing consistent and efficient search-operator rules requires not only domain expertise but also in-depth knowledge about optimisation, which makes the use of model-based meta-heuristic search challenging and expensive. In this paper, we propose a generalised approach to automatically generate atomic consistency preserving search operators (aCPSOs) for a given optimisation problem. This reduces the effort required to specify an optimisation problem and shields optimisation users from the complexity of implementing efficient meta-heuristic search mutation operators. We evaluate our approach with a set of case studies, and show that the automatically generated rules are comparable to, and in some cases better than, manually created rules at guiding evolutionary search towards near-optimal solutions.",
        "keywords": [
            "model driven engineering",
            "search based optimisation",
            "search based software engineering"
        ],
        "authors": [
            "Alexandru Burdusel",
            "Steffen Zschaler",
            "Stefan John"
        ],
        "file_path": "data/models/models19/Automatic Generation of Atomic Consistency Preserving Search Operators for Search-Based Model Engineering.pdf"
    },
    {
        "title": "Towards effective mutation testing for ATL",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "The correctness of model transformations is crucial to obtain high-quality solutions in model-driven engineering. Testing is a common approach to detect errors in transformations, which requires having methods to assess the effectiveness of the test cases and improve their quality. Mutation testing permits assessing the quality of a test suite by injecting artiﬁcial faults in the system under test. These emulate common errors made by competent developers and are modelled using mutation operators. Some researchers have proposed sets of mutation operators for transformation languages like ATL. However, their suitability for an effective mutation testing process has not been investigated, and there is no automated mechanism to generate test models that increase the quality of the tests. In this paper, we use transformations created by third parties to evaluate the effectiveness ATL mutation operators proposed in the literature, and other operators that we have devised based on empirical evidence on real errors made by developers. Likewise, we evaluate the effectiveness of commonly used test model generation techniques. For the cases in which a test suite does not detect an injected fault, we synthesize test models able to detect it. As a technical contribution, we make available a framework that automates this process for ATL.",
        "keywords": [
            "Model transformations",
            "Mutation testing",
            "ATL"
        ],
        "authors": [
            "Esther Guerra",
            "Jesus Sanchez Cuadrado",
            "Juan de Lara"
        ],
        "file_path": "data/models/models19/Towards Effective Mutation Testing for ATL.pdf"
    },
    {
        "title": "Querying and annotating model histories with time-aware patterns",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Models are not static entities: they evolve over time due to changes. Changes may inadvertently and surprisingly violate constraints imposed. Therefore, the models need to be monitored for compliance. On the one hand, in traditional design-time applications, new and evolving requirements impose changes on a model over time. These changes may accidentally break design rules. Further, the growing complexity of the models may need to be tracked for manageability. On the other hand, newer applications use models at runtime; building runtime abstractions that are used to control a system. Adopters of these approaches will need to query the history of the system to check if the models evolved as expected, or to find out the reasons for a particular behavior. Changes over models at runtime are more frequent than changes over design models. To cover these demands, we argue that a flexible and scalable approach for querying the history of the models is needed to study the evolution and for compliance sake. This paper presents a set of extensions to a model query language inspired in the Object Constraint Language (the Epsilon Object Language) for traversing the history of a model, and for making temporal assertions that will allow the elicitation of historic information. As querying long histories may be costly, the paper presents an approach that annotates versions of interest as they are observed, in order to provide efficient recalls in possible future queries. The approach has been implemented in a model indexing tool, and is demonstrated through a case study from the autonomous and self-adaptive systems domain.",
        "keywords": [
            "Model querying",
            "model versioning",
            "temporal graph databases",
            "model indexing",
            "scalable model-driven engineering"
        ],
        "authors": [
            "Antonio García-Domínguez",
            "Nelly Bencomo",
            "Juan Marcelo Parra-Ullauri",
            "Luis Hernán García-Paucar"
        ],
        "file_path": "data/models/models19/Querying and Annotating Model Histories with Time-Aware Patterns.pdf"
    },
    {
        "title": "Goal-Based Modeling and Analysis of Non-Functional Requirements",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Non-functional goals specify a quality attribute of the functional goals for the system-to-be (e.g., cost, performance, security, and safety). However, non-functional goals are often cross-cutting and do not naturally ﬁt within the default decomposition expressed by a functional goal model. Further, any functional mitigations that ensure the satisfaction of a non-functional goal, or occur in the event a non-functional goal is violated, are conditionally applicable to the remainder of the system-to-be. Rather than modeling non-functional goals and their associated mitigations as a part of the system-to-be goal model, we introduce a method of modeling and analyzing non-functional goals and their associated mitigation as separate models. We illustrate our approach by applying our method to model non-functional goals related to an industry-based automotive braking system and analyzing for non-functional violations.",
        "keywords": [
            "non-functional",
            "requirements",
            "goal model"
        ],
        "authors": [
            "Byron DeVries",
            "Betty H.C. Cheng"
        ],
        "file_path": "data/models/models19/Goal-Based Modeling and Analysis of Non-Functional Requirements.pdf"
    },
    {
        "title": "Verifying and Monitoring UML Models with Observer Automata\nA Transformation-free Approach",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "The increasing complexity of embedded systems renders veriﬁcation of software programs more complex and may require applying monitoring and formal techniques, like model-checking. However, to use such techniques, system engineers usually need formal experts to express software requirements in a formal language. To facilitate the use of model-checking tools by system engineers, our approach consists of using a UML model interpreter with which the software requirements can directly be expressed as observer automata in UML as well. These observer automata are synchronously composed with the system, and can be used unchanged both for model veriﬁcation and runtime monitoring. Our approach has been evaluated on the user interface model of a cruise control system. The observer veriﬁcation results are in line with the veriﬁcation of equivalent LTL properties. The runtime overhead of the monitoring infrastructure is 6.5%, with only 1.2% memory overhead.",
        "keywords": [
            "Observer Automata",
            "Monitoring",
            "Model Interpretation",
            "Embedded Systems"
        ],
        "authors": [
            "Valentin Besnard\nCiprian Teodorov\nFrédéric Jouault\nMatthias Brun\nPhilippe Dhaussy"
        ],
        "file_path": "data/models/models19/Verifying and Monitoring UML Models with Observer Automata A Transformation-Free Approach.pdf"
    },
    {
        "title": "Proceedings 2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Marouane Kessentini",
            "Tao Yue",
            "Alexander Pretschner",
            "Sebastian Voss",
            "Loli Burgueño"
        ],
        "file_path": "data/models/models19/Title page iii.pdf"
    },
    {
        "title": "A Focus+Context Approach to Alleviate Cognitive Challenges of Editing and Debugging UML Models",
        "submission-date": "2019/00",
        "publication-date": "2019/00",
        "abstract": "Model-Driven Engineering has been proposed to increase the productivity of developing a software system. Despite its beneﬁts, it has not been fully adopted in the software industry. Research has shown that modelling tools are amongst the top barriers for the adoption of MDE by industry. Recently, researchers have conducted empirical studies to identify the most-severe cognitive difﬁculties of modellers when using UML model editors. Their analyses show that users’ prominent challenges are in remembering the contextual information when performing a particular modelling task; and locating, understanding, and ﬁxing errors in the models. To alleviate these difﬁculties, we propose two Focus+Context user interfaces that provide enhanced cognitive support and automation in the user’s interaction with a model editor. Moreover, we conducted two empirical studies to assess the effectiveness of our interfaces on human users. Our results reveal that our interfaces help users 1) improve their ability to successfully fulﬁl their tasks, 2) avoid unnecessary switches among diagrams, 3) produce more error-free models, 4) remember contextual information, and 5) reduce time on tasks.",
        "keywords": [
            "User-Centric Software Development",
            "Empirical Study",
            "UML",
            "Modelling Tools",
            "Modelling Challenges"
        ],
        "authors": [
            "Parsa Pourali",
            "Joanne M. Atlee"
        ],
        "file_path": "data/models/models19/A Focus-Context Approach to Alleviate Cognitive Challenges of Editing and Debugging UML Models.pdf"
    },
    {
        "title": "Exploiting Multi-Level Modelling for Designing and Deploying Gameful Systems",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Gamiﬁcation is increasingly used to build solutions for driving the behaviour of target users’ populations. Gameful systems are typically exploited to keep users’ involvement in certain activities and/or to modify an initial behaviour through game-like elements, such as awarding points, submitting challenges and/or fostering competition and cooperation with other players. Gamiﬁcation mechanisms are well-deﬁned and composed of different ingredients that have to be correctly amalgamated together; among these we ﬁnd single/multi-player challenges targeted to reach a certain goal and providing an adequate award for compensation. Since the current approaches are largely based on hand-coding/tuning, when the game grows in its complex- ity, keeping track of all the mechanisms and maintaining the implementation can become error-prone and tedious activities. In this paper, we describe a multi-level modelling approach for the deﬁnition of gamiﬁcation mechanisms, from their design to their deployment and runtime adaptation. The approach is implemented by means of JetBrains MPS, a text-based meta- modelling framework, and validated using two gameful systems in the Education and Mobility domains.",
        "keywords": [
            "Multi-Level Modelling",
            "Model-Driven Engineering",
            "MPS",
            "Gamiﬁcation Engine"
        ],
        "authors": [
            "Antonio Bucchiarone",
            "Antonio Cicchetti",
            "Annapaola Marconi"
        ],
        "file_path": "data/models/models19/Exploiting Multi-level Modelling for Designing and Deploying Gameful Systems.pdf"
    },
    {
        "title": "On-the-ﬂy Translation and Execution of OCL-like Queries on Simulink Models",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "MATLAB/Simulink is a tool for dynamic system modelling. Model management languages such as OCL, ATL and the languages of the Epsilon platform tend to focus on the Eclipse Modelling Framework (EMF), a de facto standard for domain specific modelling. As Simulink models are built on an entirely different technical stack, the current solution to manipulate them using such languages requires their transformation into an EMF-compatible representation. This approach is expensive as the cost of the transformation can be crippling for large models, it requires the synchronisation of the native Simulink model and its EMF counterpart, and the EMF-representation may be an incomplete copy of the model. In this paper we propose an alternative approach that uses the MATLAB API to bridge Simulink models with existing model management languages that relies on the “on-the-ﬂy” translation of model management language constructs into MATLAB commands. Our approach eliminates the cost of the transformation and of the co-evolution of the EMF-compatible representation while enabling full access to the Simulink model details. We evaluate the performance of both approaches using a set of model validation constraints executed on a sample of the largest Simulink models available on GitHub. Our evaluation suggests that the translation approach can reduce the model validation time up to 80%.",
        "keywords": [
            "Eclipse Modelling Framework",
            "MATLAB Simulink",
            "Model Driven Engineering",
            "Epsilon"
        ],
        "authors": [
            "Beatriz A. Sanchez",
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Dimitris S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/models/models19/On-the-Fly Translation and Execution of OCL-Like Queries on Simulink Models.pdf"
    },
    {
        "title": "A Model-based Testing Approach for Cockpit Display Systems of Avionics",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Avionics are highly critical systems that require extensive testing governed by international safety standards. Cockpit Display Systems (CDS) are an essential component of modern aircraft cockpits and display information from the user application (UA) using various widgets. A signiﬁcant step in the testing of avionics is to evaluate whether these CDS are displaying the correct information. A common industrial practice is to manually test the information on these CDS by taking the aircraft into different scenarios during the simulation. Such testing is required very frequently and at various changes in the avionics. Given the large number of scenarios to test, manual testing of such behavior is a laborious activity. In this paper, we propose a model-based strategy for automated testing of the information displayed on CDS. Our testing approach focuses on evaluating that the information from the user applications is being displayed correctly on the CDS. For this purpose, we develop a proﬁle for capturing the details of different widgets of the display screens using models. The proﬁle is based on the ARINC 661 standard for Cockpit Display Systems. The expected behavior of the CDS visible on the screens of the aircraft is captured using constraints written in Object Constraint Language. We apply our approach on an industrial case study of a Primary Flight Display (PFD) developed for an aircraft. Our results showed that the proposed approach is able to automatically identify faults in the simulation of PFD. Based on the results, it is concluded that the proposed approach is useful in ﬁnding display faults on avionics CDS.",
        "keywords": [
            "Model-based Testing; Cockpit Display Systems; Safety-critical Systems; ARINC 661; Object Constraint Language (OCL)"
        ],
        "authors": [
            "Muhammad Zohaib Iqbal",
            "Hassan Sartaj",
            "Muhammad Uzair Khan",
            "Fitash Ul Haq",
            "Ifrah Qaisar"
        ],
        "file_path": "data/models/models19/A Model-Based Testing Approach for Cockpit Display Systems of Avionics.pdf"
    },
    {
        "title": "Meta-Modelling Meta-Learning",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Although artiﬁcial intelligence and machine learning are currently extremely fashionable, applying machine learning on real-life problems remains very challenging. Data scientists need to evaluate various learning algorithms and tune their numerous parameters, based on their assumptions and experience, against concrete problems and training data sets. This is a long, tedious, and resource expensive task. Meta-learning is a recent technique to overcome, i.e. automate this problem. It aims at using machine learning itself to automatically learn the most appropriate algorithms and parameters for a machine learning problem. As it turns out, there are many parallels between meta-modelling—in the sense of model-driven engineering—and meta-learning. Both rely on abstractions, the meta data, to model a predeﬁned class of problems and to deﬁne the variabilities of the models conforming to this deﬁnition. Both are used to deﬁne the output and input relationships and then ﬁtting the right models to represent that behaviour. In this paper, we envision how a meta-model for meta-learning can look like. We discuss possible variabilities, for what types of learning it could be appropriate for, how concrete learning models can be generated from it, and how models can be ﬁnally selected. Last but not least, we discuss a possible integration into existing modelling tools.",
        "keywords": [
            "meta-learning",
            "meta-modelling",
            "AutoML",
            "modelling framework"
        ],
        "authors": [
            "Thomas Hartmann",
            "Assaad Moawad",
            "Cedric Schockaert",
            "Francois Fouquet",
            "Yves Le Traon"
        ],
        "file_path": "data/models/models19/Meta-Modelling Meta-Learning.pdf"
    },
    {
        "title": "Towards WCET Estimation of Graph Queries@Run.time",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Recent approaches in runtime monitoring and live data analytics have started to use expressive graph queries at runtime to capture and observe properties of interest at a high level of abstraction. However, in a critical context, such applications often require timeliness guarantees, which have not been investigated yet for query-based solutions due to limitations of existing static worst-case execution time (WCET) analysis techniques. One limitation is the lack of support for dynamic memory allocation, which is required by the dynamically evolving runtime models on which the queries are evaluated. Another open challenge is to compute WCET for asynchronously communicating programs such as distributed monitors. This paper introduces our vision about how to assess such timeliness properties and how to provide tight WCET estimates for query execution at runtime over a dynamic model. Furthermore, we present an initial solution that combines state-of-the-art parametric WCET estimations with model statistics and search plans of queries.",
        "keywords": [],
        "authors": [
            "Márton Búr",
            "Dániel Varró"
        ],
        "file_path": "data/models/models19/Towards WCET Estimation of Graph Queries-Run.time.pdf"
    },
    {
        "title": "Bridging the Gap between Requirements Modeling and Behavior-driven Development",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Acceptance criteria (AC) are implementation agnostic conditions that a system must meet to be consistent with its requirements and be accepted by its stakeholders. Each acceptance criterion is typically expressed as a natural-language statement with a clear pass or fail outcome. Writing AC is a tedious and error-prone activity, especially when the requirements speciﬁcations evolve and there are different analysts and testing teams involved. Analysts and testers must iterate multiple times to ensure that AC are understandable and feasible, and accurately address the most important requirements and workﬂows of the system being developed. In many cases, analysts express requirements through models, along with natural language, typically in some variant of the UML. AC must then be derived by developers and testers from such models. In this paper, we bridge the gap between requirements models and AC by providing a UML-based modeling methodology and an automated solution to generate AC. We target AC in the form of Behavioral Speciﬁcations in the context of Behavioral-Driven Development (BDD), a widely used agile practice in many application domains. More specially we target the well-known Gherkin language to express AC, which then can be used to generate executable test cases. We evaluate our modeling methodology and AC generation solution through an industrial case study in the ﬁnancial domain. Our results suggest that (1) our methodology is feasible to apply in practice, and (2) the additional modeling effort required by our methodology is outweighed by the beneﬁts the methodology brings in terms of automated and systematic AC generation and improved model precision.",
        "keywords": [
            "Software testing",
            "BDD",
            "modeling",
            "requirements engineering",
            "text generation",
            "Gherkin",
            "and FinTech."
        ],
        "authors": [
            "Mauricio Alferez",
            "Fabrizio Pastore",
            "Mehrdad Sabetzadeh",
            "Lionel C. Briand",
            "Jean-Richard Riccardi"
        ],
        "file_path": "data/models/models19/Bridging the Gap between Requirements Modeling and Behavior-Driven Development.pdf"
    },
    {
        "title": "MODELS 2019",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models19/Title page i.pdf"
    },
    {
        "title": "Preface to the 22nd International ACM/IEEE Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "This document is a preface to the 22nd International ACM/IEEE Conference on Model Driven Engineering Languages and Systems (MODELS 2019), held September 15-20, 2019 in Munich, Germany. It provides information about the conference location, program, submission and acceptance rates, and acknowledges the contributions of various individuals and organizations involved in its organization.",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models19/Preface.pdf"
    },
    {
        "title": "Modeling and Training of Neural Processing Systems",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "The ﬁeld of deep learning has become more and more pervasive in the last years as we have seen varieties of problems being solved using neural processing techniques. Image analysis and detection, control, speech recognition, translation are only a few prominent examples tackled successfully by neural networks. Thereby, the discipline imposes a completely new problem solving paradigm requiring a rethinking of classical software development methods. The high demand for deep learning technology has led to a large amount of competing frameworks mostly having a Python interface - a quasi standard in the community. Although, existing tools often provide great ﬂexibility and high performance, they still lack to deliver a completely domain oriented problem view. Furthermore, using neural networks as reusable building blocks with clear interfaces in productive systems is still a challenge. In this work we propose a domain speciﬁc modeling methodology tackling design, training, and integration of deep neural networks. Thereby, we distinguish between three main modeling concerns: architecture, training, and data. We integrate our methodology in a component-based modeling toolchain allowing one to employ and reuse neural networks in large software architectures.",
        "keywords": [
            "deep learning",
            "neural networks",
            "model-driven software engineering"
        ],
        "authors": [
            "Evgeny Kusmenko",
            "Sebastian Nickels",
            "Svetlana Pavlitskaya",
            "Bernhard Rumpe",
            "Thomas Timmermanns"
        ],
        "file_path": "data/models/models19/Modeling and Training of Neural Processing Systems.pdf"
    },
    {
        "title": "Using Models to Enable Compliance Checking against the GDPR: An Experience Report",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "The General Data Protection Regulation (GDPR) harmonizes data privacy laws and regulations across Europe. Through the GDPR, individuals are able to better control their personal data in the face of new technological developments. While the GDPR is highly advantageous to individuals, complying with it poses major challenges for organizations that control or process personal data. Since no automated solution with broad industrial applicability currently exists for GDPR compliance checking, organizations have no choice but to perform costly manual audits to ensure compliance. In this paper, we share our experience building a UML representation of the GDPR as a ﬁrst step towards the development of future automated methods for assessing compliance with the GDPR. Given that a concrete implementation of the GDPR is affected by the national laws of the EU member states, GDPR’s expanding body of case law and other contextual information, we propose a two-tiered representation of the GDPR: a generic tier and a specialized tier. The generic tier captures the concepts and principles of the GDPR that apply to all contexts, whereas the specialized tier describes a speciﬁc tailoring of the generic tier to a given context, including the contextual variations that may impact the interpretation and application of the GDPR. We further present the challenges we faced in our modeling endeavor, the lessons we learned from it, and future directions for research.",
        "keywords": [
            "General Data Protection Regulation",
            "Regulatory Compliance",
            "UML",
            "OCL"
        ],
        "authors": [
            "Damiano Torre",
            "Ghanem Soltana",
            "Mehrdad Sabetzadeh",
            "Lionel C. Briand",
            "Yuri Aufﬁnger",
            "Peter Goes"
        ],
        "file_path": "data/models/models19/Using Models to Enable Compliance Checking Against the GDPR An Experience Report.pdf"
    },
    {
        "title": "Secure Data-Flow Compliance Checks between Models and Code based on Automated Mappings",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "During the development of security-critical software, the system implementation must capture the security properties postulated by the architectural design. This paper presents an approach to support secure data-ﬂow compliance checks between design models and code. To iteratively guide the developer in discovering such compliance violations we introduce automated mappings. These mappings are created by searching for correspondences between a design-level model (Security Data Flow Diagram) and an implementation-level model (Program Model). We limit the search space by considering name similarities between model elements and code elements as well as by the use of heuristic rules for matching data-ﬂow structures. The main contributions of this paper are three-fold. First, the automated mappings support the designer in an early discovery of implementation absence, convergence, and divergence with respect to the planned software design. Second, the mappings also support the discovery of secure data-ﬂow compliance violations in terms of illegal asset ﬂows in the software implementation. Third, we present our implementation of the approach as a publicly available Eclipse plugin and its evaluation on ﬁve open source Java projects (including Eclipse secure storage).",
        "keywords": [
            "Security-by-design",
            "Security compliance",
            "Data Flow Diagram (DFD)",
            "Model-to-Model Transformation (M2M)"
        ],
        "authors": [
            "Sven Peldszus",
            "Katja Tuma",
            "Daniel Strüber",
            "Jan Jürjens",
            "Riccardo Scandariato"
        ],
        "file_path": "data/models/models19/Secure Data-Flow Compliance Checks between Models and Code Based on Automated Mappings.pdf"
    },
    {
        "title": "Leveraging Model-Driven Technologies for JSON Artefacts: The Shipyard Case Study",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "With JSON’s increasing adoption, the need for structural constraints and validation capabilities led to JSON Schema, a dedicated meta-language to specify languages which are in turn used to validate JSON documents. Currently, the standardisation process of JSON Schema and the implementation of adequate tool support (e.g., validators and editors) are work in progress. However, the periodic issuing of newer JSON Schema drafts makes tool development challenging. Nevertheless, many JSON Schemas as language deﬁnitions exist, but JSON documents are still mostly edited in basic text-based editors. To tackle this challenge, we investigate in this paper how Model-Driven Engineering (MDE) methods for language engineering can help in this area. Instead of re-inventing the wheel of building up particular technologies directly for JSON, we study how the existing MDE infrastructures may be utilized for JSON. In particular, we present a bridge between the JSONware and Modelware technical spaces to exchange languages and documents. Based on this bridge, our approach supports language engineers, domain experts, and tool providers in editing, validating, and generating tool support with enhanced capabilities for JSON schemas and their documents. We evaluate our approach with Shipyard, a JSON Schema-based language for the workﬂow speciﬁcation for Keptn, an open-source tool for DevOps automation of cloud-native applications. The results of the case study show that proper editors and language evolution support from MDE can be reused and, at the same time, the surface syntax of JSON is maintained.",
        "keywords": [
            "JSON",
            "JSON Schema",
            "MDE",
            "DevOps",
            "Tool Interoperability"
        ],
        "authors": [
            "Alessandro Colantoni",
            "Antonio Garmendia",
            "Luca Berardinelli",
            "Manuel Wimmer",
            "Johannes Br¨auer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a250/349500a250.pdf"
    },
    {
        "title": "Towards Reinforcement Learning for In-Place Model Transformations",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Model-driven optimization has gained much interest in the last years which resulted in several dedicated extensions for in-place model transformation engines. The main idea is to exploit domain-speciﬁc languages to deﬁne models which are optimized by applying a set of model transformation rules. Objectives are guiding the optimization processes which are currently mostly realized by meta-heuristic searchers such as different kinds of Genetic Algorithms. However, meta-heuristic search approaches are currently challenged by reinforcement learning approaches for solving optimization problems.\n\nIn this new ideas paper, we apply for the ﬁrst time reinforce-ment learning for in-place model transformations. In particular, we extend an existing model-driven optimization approach with reinforcement learning techniques. We experiment with value-based and policy-based techniques. We investigate several case studies for validating the feasibility of using reinforcement learning for model-driven optimization and compare the perfor-mance against existing approaches. The initial evaluation shows promising results but also helped in identifying future research lines for the whole model transformation community.",
        "keywords": [
            "Model Transformations",
            "Reinforcement-Learning",
            "Model-based Optimization"
        ],
        "authors": [
            "Martin Eisenberg",
            "Hans-Peter Pichler",
            "Antonio Garmendia",
            "Manuel Wimmer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a082/349500a082.pdf"
    },
    {
        "title": "A Concept for a Qualiﬁable (Meta)-Modeling Framework Deployable in Systems and Tools of Safety-critical and Cyber-physical Environments",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "The development of cyber-physical systems can significantly benefit from domain-specific modeling and requires adequate (meta)-modeling frameworks. If such systems are designed for the safety-critical area, the systems must undergo qualiﬁcation processes deﬁned and monitored by a certiﬁcation authority. To use the resulting artifacts of modeling tools without further qualiﬁcation activities, the modeling tool must be additionally qualiﬁed. Tool qualiﬁcation has to be conducted by the tool user and can be assisted by the tool developer by providing qualiﬁcation artifacts. However, state-of-the-art domain-specific modeling frameworks barely support the user in the qualiﬁcation process, which results in an extensive manual effort. To reduce this effort and to avoid modeling constructs that can hardly be implemented in a qualiﬁable way, we propose the development of an open source (meta)-modeling framework that inherently considers qualiﬁcation issues. Based on the functionality of existing frameworks, we have identiﬁed components that necessarily need to be rethought or changed. This leads to the consideration of the following six cornerstones for our framework: (1) an essential meta-language, (2) a minimal runtime, (3) deterministic transformations, (4) a qualiﬁcation artifact generation, (5) a sophisticated visualization, and (6) a decoupled interaction of framework components. All these cornerstones consider the aspect of a safety-critical (meta)-modeling framework in their own manner. This combination leads to a holistic framework usable in the safety-critical system development domain.",
        "keywords": [],
        "authors": [
            "Vanessa Tietz",
            "Julian Schoepf",
            "Andreas Waldvogel",
            "Bjoern Annighoefer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a163/349500a163.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Zhenjiang Hu",
            "Tomoji Kishi",
            "Naoyasu Ubayashi",
            "Daniel Varro",
            "Shiva Nejati",
            "Xiaoxing Ma",
            "Huseyin Ergin",
            "Shaukat Ali"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z012/349500z012.pdf"
    },
    {
        "title": "Execution Trace Analysis for a Precise Understanding of Latency Violations",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Despite the amount of proposed works for the veriﬁcation of diverse model properties, under-standing the root cause of latency requirements viola-tion in execution traces is still an open-issue especially for complex HW/SW system-level designs: is it due to an unfavorable real-time scheduling, to contentions on buses, to the characteristics of functional algorithms or hardware components? This identiﬁcation is particu-larly at stake when adding new features in a model, e.g., a new security countermeasure. The paper introduces PLAN, a new trace analysis technique whose objective is to classify execution transactions according to their impact on latency. To do so, we rely ﬁrst on a model transformation that builds up a dependency graph from an allocation model, thus including hardware and software aspects of a system model. Then, from this graph and an execution trace, our analysis can highlight how software or hardware elements contributed to the latency violation. The paper ﬁrst formalizes the problem before applying our approach to simulation traces of SysML models. A case study deﬁned in the AQUAS European project illustrates the interest of our approach.",
        "keywords": [
            "Embedded Systems",
            "Execution Trace Analysis",
            "Dependency Graph",
            "MBSE",
            "Timing analysis",
            "Simulation"
        ],
        "authors": [
            "Maysam Zoor",
            "Ludovic Apvrille",
            "Renaud Pacalet"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a123/349500a123.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Shaukat Ali",
            "Mira Balaban",
            "Olivier Barais",
            "Thorsten Berger",
            "Gábor Bergmann",
            "Marco Brambilla",
            "Loli Burgueño",
            "Michel Chaudron",
            "Juergen Dingel",
            "Jeff Gray",
            "Esther Guerra",
            "Xiao He",
            "Marianne Huchard",
            "Fuyuki Ishikawa",
            "Gabor Karsai",
            "Dimitris Kolovos",
            "Sébastien Mosser",
            "Ileana Ober",
            "Alfonso Pierantonio",
            "Gianna Reggio",
            "Riccardo Scandariato",
            "Andy Schürr",
            "Jocelyn Simmonds",
            "Daniel Strüber",
            "Matthias Tichy",
            "Massimo Tisi",
            "Antonio Vallecillo",
            "Andreas Wortmann",
            "Andrzej Wąsowski",
            "Tao Yue",
            "Steffen Zschaler"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z021/349500z021.pdf"
    },
    {
        "title": "Applying Declarative Analysis to Software Product Line Models: An Industrial Study",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Software Product Lines (SPLs) are families of related software products developed from a common set of artifacts. Most existing analysis tools can be applied to a single product at a time, but not to an entire SPL. Some tools have been redesigned/re-implemented to support the kind of variability exhibited in SPLs, but this usually takes a lot of effort, and is error-prone. Declarative analyses written in languages like Datalog have been collectively lifted to SPLs in prior work [1], which makes the process of applying an existing declarative analysis to a product line more straightforward.\nIn this paper, we take an existing declarative analysis (behaviour alteration) and apply it to a set of automotive software product lines from General Motors. We discuss the design of the analysis pipeline used in this process, present its scalability results, and provide a means to visualize the analysis results for a subset of products ﬁltered by feature expression. We also reﬂect on some of the lessons learned throughout this project.",
        "keywords": [],
        "authors": [
            "Ramy Shahin",
            "Robert Hackman",
            "Rafael Toledo",
            "Ramesh S.",
            "Joanne M. Atlee",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a145/349500a145.pdf"
    },
    {
        "title": "Model-Driven Simulation-Based Analysis for Multi-Robot Systems",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Multi-robot systems are increasingly deployed to provide services and accomplish missions whose complexity or cost is too high for a single robot to achieve on its own. Although multi-robot systems offer increased reliability via redundancy and enable the execution of more challenging missions, engineering these systems is very complex. This complexity affects not only the architecture modelling of the robotic team but also the modelling and analysis of the collaborative intelligence enabling the team to complete its mission. Existing approaches for the development of multi-robot applications do not provide a systematic mechanism for capturing these aspects and assessing the robustness of multi-robot systems. We address this gap by introducing ATLAS, a novel model-driven approach supporting the systematic robustness analysis of multi-robot systems in simulation. The ATLAS domain-speciﬁc language enables modelling the architecture of the robotic team and its mission, and facilitates the speciﬁcation of the team’s intelligence. We evaluate ATLAS and demonstrate its effectiveness on two oceanic exploration missions performed by a team of unmanned underwater vehicles developed using the MOOS-IvP robotic simulator.",
        "keywords": [
            "model-driven engineering",
            "robotics",
            "simulation"
        ],
        "authors": [
            "James Harbin",
            "Simos Gerasimou",
            "Nicholas Matragkas",
            "Athanasios Zolotas and Radu Calinescu"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a331/349500a331.pdf"
    },
    {
        "title": "Monte Carlo Tree Search and\nGR(1) Synthesis for Robot Tasks Planning in\nAutomotive Production Lines",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "In automotive production cells, complex processes\ninvolving multiple robots must be optimized for cycle time.\nWe investigated using symbolic GR(1) controller synthesis for\nautomating multi-robot task planning. Given a speciﬁcation of\nthe order of tasks and states to avoid, often multiple valid\nstrategies can be computed; in many states there are multiple\nchoices to satisfy the speciﬁcation, such as choosing different\nrobots to perform a certain task. To determine the best choices\nunder the consideration of movement times and probabilities\nthat robots may be interrupted for repairs or corrections, we\ncombine the execution of the synthesized controller with Monte\nCarlo Tree Search (MCTS), a heuristic AI-planning technique.\nThe result is a model-at-run-time approach that we present by\nthe example of a multi-robot spot welding cell. We report on\nexperiments showing that the approach (1) can reduce cycle\ntimes by choosing time-efﬁcient movement sequences and (2)\ncan choose executions that react efﬁciently to interruptions by\nchoosing to delay tasks that, if an interruption of one robot\nshould occur later, can be reallocated to another robot. Most\ninterestingly, we found, however, that (3) in some cases there is a\nconﬂict between time-efﬁcient movement sequences and ones that\nmay react efﬁciently to probable future interruptions—and when\ninterruption probabilities are low, increasing the time allocated\nfor MCTS, i.e., increasing the number of sample simulations\nmade by MCTS, does not improve cycle time.",
        "keywords": [
            "Robot tasks planning",
            "Reactive systems",
            "Monte\nCarlo Tree Search"
        ],
        "authors": [
            "Eric Wete",
            "Joel Greenyer",
            "Andreas Wortmann",
            "Oliver Flegel",
            "Martin Klein"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a320/349500a320.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Silvia Abrahão",
            "Richard Paige",
            "Don Batory",
            "Benoit Baudry",
            "Nelly Bencomo",
            "Ruth Breu",
            "Jordi Cabot",
            "Silvia Ceballos"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z016/349500z016.pdf"
    },
    {
        "title": "MoDALAS: Model-Driven Assurance for Learning-Enabled Autonomous Systems",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Increasingly, safety-critical systems include artificial intelligence and machine learning components (i.e., Learning-Enabled Components (LECs)). However, when behavior is learned in a training environment that fails to fully capture real-world phenomena, the response of an LEC to untrained phenomena is uncertain, and therefore cannot be assured as safe. Automated methods are needed for self-assessment and adaptation to decide when learned behavior can be trusted. This work introduces a model-driven approach to manage self-adaptation of a Learning-Enabled System (LES) to account for run-time contexts for which the learned behavior of LECs cannot be trusted. The resulting framework enables an LES to monitor and evaluate goal models at run time to determine whether or not LECs can be expected to meet functional objectives. Using this framework enables stakeholders to have more confidence that LECs are used only in contexts comparable to those validated at design time.",
        "keywords": [
            "goal-based modeling",
            "self-adaptive systems",
            "artificial intelligence",
            "machine learning",
            "models at run time",
            "cyber physical systems",
            "behavior oracles",
            "autonomous vehicles"
        ],
        "authors": [
            "Michael Austin Langford",
            "Kenneth H. Chan",
            "Jonathon Emil Fleck",
            "Philip K. McKinley",
            "Betty H.C. Cheng"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a182/349500a182.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Ali, Shaukat",
            "Amundson, Isaac",
            "Annighoefer, Bjoern",
            "Apvrille, Ludovic",
            "Aslam, Kousar",
            "Atlee, Joanne M.",
            "Babaei, Majid",
            "Babar, Junaid",
            "Belharbi, Khalid",
            "Bennett, Michael",
            "Berardinelli, Luca",
            "Bhatambrekar, Sachin",
            "Bittner, Paul Maximilian",
            "Boletsis, Costas",
            "Borum, Holger Stadel",
            "Bräeuer, Johannes",
            "Brown, Caroline",
            "Calinescu, Radu",
            "Carlson, Jan",
            "Chan, Kenneth H.",
            "Chechik, Marsha",
            "Cheng, Betty H.C.",
            "Chouki, Tibermacine",
            "Cicchetti, Antonio",
            "Ciccozzi, Federico",
            "Cofer, Darren",
            "Colantoni, Alessandro",
            "Combemale, Benoît",
            "Cooper, Justin",
            "Costa Seco, João",
            "Damasceno, Carlos Diego Nascimento",
            "David, Istvan",
            "De La Vega, Alfonso",
            "Dingel, Juergen",
            "Di Rocco, Juri",
            "Di Ruscio, Davide",
            "Di Sandro, Alessio",
            "Di Sipio, Claudio",
            "Dorofeev, Kirill",
            "Dumitrescu, Roman",
            "Ege, Florian",
            "Eisenberg, Martin",
            "Elyes, Cherfa",
            "Engels, Gregor",
            "Faridmoayer, Sogol",
            "Ferreira, Carla",
            "Fischbach, Jannik",
            "Fleck, Jonathon Emil",
            "Flegel, Oliver",
            "Galasso, Jessie",
            "Garcia-Ceja, Enrique",
            "Garmendia, Antonio",
            "Gerasimou, Simos",
            "Gorissen, Simon",
            "Greenyer, Joel",
            "Gross-Amblard, David",
            "Grunske, Lars",
            "Hackman, Robert",
            "Halvorsrud, Ragnhild",
            "Harbin, James",
            "Hardin, David",
            "Hernández López, José Antonio",
            "Hoyos Rodriguez, Horacio",
            "Jaskolka, Monika",
            "Jézéquel, Jean-Marc",
            "Jongeling, Robbert",
            "Kehrer, Timo",
            "Klein, Martin",
            "Kolovos, Dimitris",
            "Lago, Patricia",
            "Langford, Michael Austin",
            "Lawford, Mark",
            "Lofberg, Anders",
            "Lourenço, Hugo"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a343/349500a343.pdf"
    },
    {
        "title": "A GNN-based Recommender System to Assist the Speciﬁcation of Metamodels and Models",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Nowadays, while modeling environments provide users with facilities to specify different kinds of artifacts, e.g., metamodels, models, and transformations, the possibility of learning from previous modeling experiences and being assisted during modeling tasks remains largely unexplored. In this paper, we propose MORGAN, a recommender system based on a graph neural network (GNN) to assist modelers in performing the speciﬁcation of metamodels and models. The (meta)model being speciﬁed, and the training data are encoded in a graph-based format by exploiting natural language processing (NLP) techniques. Afterward, a graph kernel function uses the extracted graphs to provide modelers with relevant recommendations to complete the partially speciﬁed (meta)models. We evaluated MORGAN on real-world datasets using various quality metrics, i.e., precision, recall, and F-measure. The experimental results are encouraging and demonstrate the feasibility of our tool to support modelers while specifying metamodels and models.",
        "keywords": [],
        "authors": [
            "Juri Di Rocco",
            "Claudio Di Sipio",
            "Davide Di Ruscio",
            "Phuong T. Nguyen"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a070/349500a070.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z023/349500z023.pdf"
    },
    {
        "title": "Identifying manual changes to generated code: Experiences from the industrial automation domain",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "In this paper, we report on a case study in an industrial setting where code is generated from models, and, for various reasons, that generated code is then manually modified. To enhance the maintainability of both models and code, consistency between them is imperative. A first step towards establishing that consistency is to identify the manual changes that were made to the code after it was generated and deployed. Identifying the delta is not straightforward and requires pre-processing of the artifacts. The main mechanics driving our solution are higher-order transformations, which make the implementation scalable and robust to small changes in the modeling language. We describe the specific industrial setting of the problem, as well as the experiences and lessons learned from developing, implementing, and validating our solution together with our industrial partner.",
        "keywords": [
            "Model-based development",
            "round-trip engineering",
            "higher-order transformations",
            "domain-specific modeling languages",
            "industrial case study"
        ],
        "authors": [
            "Robbert Jongeling",
            "Sachin Bhatambrekar",
            "Anders Lofberg",
            "Antonio Cicchetti",
            "Federico Ciccozzi",
            "Jan Carlson"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a035/349500a035.pdf"
    },
    {
        "title": "Model-Based Development of Engine Control Systems: Experiences and Lessons Learnt",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Rolls-Royce Control Systems supplies engine control and monitoring systems for aviation applications, and is required to design, certify, and deliver these to the highest level of safety assurance. To allow Rolls-Royce to develop safe and robust systems, which continue to increase in complexity, model-based techniques are now a critical part of the software development process. In this paper, we discuss the experiences, challenges and lessons learnt when developing a bespoke domain-speciﬁc modelling workbench based on open-source modelling technologies including the Eclipse Modelling Framework (EMF), Xtext, Sirius and Epsilon. This modelling workbench will be used to architect and integrate the software for all future Rolls-Royce engine control and monitoring systems.",
        "keywords": [
            "Domain Speciﬁc Language",
            "Component Oriented Architecture",
            "Graphical Modelling Workbench",
            "Xtext",
            "Sirius",
            "EMF"
        ],
        "authors": [
            "Justin Cooper",
            "Alfonso de la Vega",
            "Richard Paige",
            "Dimitris Kolovos",
            "Michael Bennett",
            "Caroline Brown",
            "Beatriz Sanchez Pi˜na",
            "Horacio Hoyos Rodriguez"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a308/349500a308.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z004/349500z004.pdf"
    },
    {
        "title": "Collaborative Model-Driven Software Engineering: A Systematic Update",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Current software engineering practices rely on highly heterogeneous and distributed teams working in a collaborative setting. Between 2013–2020, the publication output in the ﬁeld of collaborative Model-Driven Software Engineering (MDSE) has signiﬁcantly increased. However, the only systematic mapping study available is limited to studies published until 2015. In this paper, we provide an update on that study for the complementing 2016–2020 period, and report the latest results, challenges, and trends. Our analysis led to selecting 29 clusters of 54 new peer-reviewed publications on collaborative MDSE. Based on the novel developments in the ﬁeld, we have extended and improved the original classiﬁcation framework, making it applicable to recent and future research contributions on collaborative MDSE. The insights in this paper relate to the changing trends in the ﬁeld and present new relevant information.",
        "keywords": [
            "Model-driven engineering",
            "collaborative modeling",
            "systematic mapping study",
            "systematic update"
        ],
        "authors": [
            "Istvan David",
            "Kousar Aslam",
            "Sogol Faridmoayer",
            "Ivano Malavolta",
            "Eugene Syriani",
            "Patricia Lago"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a273/349500a273.pdf"
    },
    {
        "title": "24th International Conference on Model-Driven Engineering Languages and Systems",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z001/349500z001.pdf"
    },
    {
        "title": "Automated Patch Generation for Fixing Semantic Errors in ATL Transformation Rules",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "With the growing popularity of the MDE paradigm, model transformations are becoming more and more complex. ATL transformations, in particular, are error-prone due to the declarative nature of the language and the dependency towards the involved metamodels. To alleviate the burden of developers, we propose, in this paper, an approach for ﬁxing semantic errors in ATL transformation rules without predeﬁned patch templates for speciﬁc error types. In a ﬁrst step, our approach determines the rules that are likely to contain errors starting from the discrepancy between the expected and produced outputs of test cases. Then, a second step allows to generate candidate patches for these errors using a multiobjective optimization algorithm, guided by the same test cases. In a preliminary evaluation, we show that our approach can ﬁx most of the errors for transformations with one or two errors. For those with multiple errors, more iterations are necessary to ﬁx some of the errors.",
        "keywords": [
            "Model transformation",
            "Program repair",
            "Multi-objective optimization"
        ],
        "authors": [
            "Zahra VaraminyBahnemiry",
            "Jessie Galasso",
            "Khalid Belharbi",
            "Houari Sahraoui"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a013/349500a013.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Silvia Abrahão",
            "Joanne M. Atlee",
            "Nelly Bencomo",
            "Jordi Cabot",
            "Betty H.C. Cheng",
            "Benoit Combemale",
            "Davide Di Ruscio",
            "Gregor Engels",
            "Martin Gogolla",
            "Jörg Kienzle",
            "Ana Moreira",
            "Richard Paige",
            "Eugene Syriani",
            "Gabriele Taentzer",
            "Manuel Wimmer",
            "Juan de Lara"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z020/349500z020.pdf"
    },
    {
        "title": "DataTime: a Framework to smoothly Integrate Past, Present and Future into Models",
        "submission-date": "2021/09",
        "publication-date": "2021/09",
        "abstract": "Models at runtime have been initially investigated for adaptive systems. Models are used as a reﬂective layer of the current state of the system to support the implementation of a feedback loop. More recently, models at runtime have also been identiﬁed as key for supporting the development of full-ﬂedged digital twins. However, this use of models at runtime raises new challenges, such as the ability to seamlessly interact with the past, present and future states of the system. In this paper, we propose a framework called DataTime to implement models at runtime which capture the state of the system according to the dimensions of both time and space, here modeled as a directed graph where both nodes and edges bear local states (ie. values of properties of interest). DataTime provides a unifying interface to query the past, present and future (predicted) states of the system. This unifying interface provides i) an optimized structure of the time series that capture the past states of the system, possibly evolving over time, ii) the ability to get the last available value provided by the system’s sensors, and iii) a continuous micro-learning over graph edges of a predictive model to make it possible to query future states, either locally or more globally, thanks to a composition law. The framework has been developed and evaluated in the context of the Intelligent Public Transportation Systems of the city of Rennes (France). This experimentation has demonstrated how DataTime can deprecate the use of heterogeneous tools for managing data from the past, the present and the future, and facilitate the development of digital twins.",
        "keywords": [],
        "authors": [
            "Gauthier LYAN",
            "Jean-Marc J´EZ´EQUEL",
            "David GROSS-AMBLARD",
            "Benoˆıt COMBEMALE"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a134/349500a134.pdf"
    },
    {
        "title": "24th International Conference on Model-Driven Engineering Languages and Systems",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Zhenjiang Hu",
            "Tomoji Kishi",
            "Naoyasu Ubayashi",
            "Daniel Varro",
            "Shiva Nejati"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z003/349500z003.pdf"
    },
    {
        "title": "Designing a Modeling Language for Customer Journeys: Lessons Learned from User Involvement",
        "submission-date": "2021/09",
        "publication-date": "2021/09",
        "abstract": "Although numerous methods have been formalized for handling the technical aspects of developing domain-speciﬁc modeling languages (DSMLs), user needs and usability aspects are often addressed in ad hoc manners and late in the development process. Working in this context, this paper presents the development of the customer journey modeling language (CJML), a DSML for modeling service processes from the end-user’s perspective. CJML targets a wide and heterogeneous group of users, making it especially challenging regarding usability. This paper describes how an industry-relevant DSML was systematically improved by using a variety of user-centered design techniques in close collaboration with the target group and how their feedback was used to reﬁne and evolve the syntax and semantics of CJML. We also suggest how a service-providing organization may beneﬁt from adopting CJML as a unifying language for documentation purposes, compliance analysis, and service innovation. Finally, we generalize the experience gained into lessons learned and methodological guidelines.",
        "keywords": [
            "DSML",
            "customer journey",
            "user involvement",
            "user-centered design"
        ],
        "authors": [
            "Ragnhild Halvorsrud",
            "Costas Boletsis",
            "Enrique Garcia-Ceja"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a239/349500a239.pdf"
    },
    {
        "title": "Collaborative Software Modeling in Virtual Reality",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Modeling is a key activity in conceptual design and system design. Through collaborative modeling, end-users, stakeholders, experts, and entrepreneurs are able to create a shared understanding of a system representation. While the Uni-fied Modeling Language (UML) is one of the major conceptual modeling languages in object-oriented software engineering, more and more concerns arise from the modeling quality of UML and its tool-support. Among them, the limitation of the two-dimensional presentation of its notations and lack of natural collaborative modeling tools are reported to be signiﬁcant. In this paper, we explore the potential of using Virtual Reality (VR) technology for collaborative UML software design by comparing it with classical collaborative software design using conventional devices (Desktop PC / Laptop). For this purpose, we have developed a VR modeling environment that offers a natural collaborative modeling experience for UML Class Diagrams. Based on a user study with 24 participants, we have compared collaborative VR modeling with conventional modeling with regard to efﬁciency, effectiveness, and user satisfaction. Results show that the use of VR has some disadvantages concerning efﬁciency and effectiveness, but the user’s fun, the feeling of being in the same room with a remote collaborator, and the naturalness of collaboration were increased.",
        "keywords": [
            "Collaborative Modeling",
            "Virtual Reality",
            "UML"
        ],
        "authors": [
            "Enes Yigitbas",
            "Simon Gorissen",
            "Nils Weidmann",
            "Gregor Engels"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a261/349500a261.pdf"
    },
    {
        "title": "Towards the Characterization of Realistic Model Generators using Graph Neural Networks",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "The automatic generation of software models is an important element in many software and systems engineering scenarios such as software tool certiﬁcation, validation of cyber-physical systems, or benchmarking graph databases. Several model generators are nowadays available, but the topic of whether they generate realistic models has been little studied. The state-of-the-art approach to check the realistic property in software models is to rely on simple comparisons using graph metrics and statistics. This generates a bottleneck due to the compression of all the information contained in the model into a small set of metrics. Furthermore, there is a lack of interpretation in these approaches since there are no hints of why the generated models are not realistic. Therefore, in this paper, we tackle the problem of assessing how realistic a generator is by mapping it to a classiﬁcation problem in which a Graph Neural Network (GNN) will be trained to distinguish between the two sets of models (real and synthetic ones). Then, to assess how realistic a generator is we perform the Classiﬁer Two-Sample Test (C2ST). Our approach allows for interpretation of the results by inspecting the attention layer of the GNN. We use our approach to assess four state-of-the-art model generators applied to three different domains. The results show that none of the generators can be considered realistic.",
        "keywords": [
            "Model generators",
            "Realistic models",
            "Graph neural networks",
            "Two-Sample Test"
        ],
        "authors": [
            "Jos´e Antonio Hern´andez L´opez",
            "Jes´us S´anchez Cuadrado"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a058/349500a058.pdf"
    },
    {
        "title": "On Designing Applied DSLs for Non-programming Experts in Evolving Domains",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Domain-speciﬁc languages (DSLs) have emerged as a plausible way for non-programming experts to efﬁciently express their domain knowledge. Recent DSL research has taken a technical perspective on how and why to create DSLs, resulting in a wealth of innovative tools, frameworks and technical approaches. Less attention has been paid to the design process. Namely, how can it ensure that the created DSL realises the expected beneﬁts? This paper seeks to answer this question when designing DSLs for highly specialised domains subject to resource constraints, an evolving application domain, and scarce user participation. We propose an iteration of alternating activities in a human-centred design method that seeks to minimise the need for expensive implementation and user involvement. The method moves from a low-validity exploration of highly diverse language designs towards a higher-validity exploration of more homogeneous designs. We give an in-depth case study of designing an actuarial DSL called MAL, or Management Action Language, which allows actuaries to model so-called future management actions in asset/liability projections in life insurance and pension companies. The proposed human-centred design method was synthesised from this case study, where we found it useful for iteratively identifying and removing usability problems during the design.",
        "keywords": [
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Human-centred design"
        ],
        "authors": [
            "Holger Stadel Borum",
            "Henning Niss",
            "Peter Sestoft"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a227/349500a227.pdf"
    },
    {
        "title": "Repository Mining for Changes in Simulink Models",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Model-Based Development (MBD) is widely used for\nembedded controls development, with MATLAB/Simulink being\none of the most used environments in the automotive industry.\nSimulink models are the primary design artifact and as with all\nsoftware, must be constantly maintained and evolved over their\nlifetime. It is necessary to develop models that support likely\nchanges in order to assist with evolution/maintenance processes.\nIn order to do so, the types of frequently performed changes\nmust be understood and appropriate language mechanisms must\nbe available to support these changes. However, Simulink model\nchanges are currently not well understood. We analyze a real\nindustrial software repository of our industrial partner and its\nversion control system to provide insights into the likely changes\nfor Simulink. The intent with this analysis includes providing\nguidance on how Simulink is used in industrial practice and\nhow particular model changes can impact system evolution.",
        "keywords": [
            "Simulink",
            "model-based development",
            "model change",
            "repository mining",
            "software evolution",
            "version control system"
        ],
        "authors": [
            "Monika Jaskolka",
            "Vera Pantelic",
            "Alan Wassyng",
            "Mark Lawford",
            "Richard Paige"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a046/349500a046.pdf"
    },
    {
        "title": "A Lean Approach to Building Valid Model-Based Safety Arguments",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "In recent decades, cyber-physical systems developed using Model-Driven Engineering (MDE) techniques have become ubiquitous in safety-critical domains. Safety assurance cases (ACs) are structured arguments designed to comprehensively show that such systems are safe; however, the reasoning steps, or strategies, used in AC arguments are often informal and difficult to rigorously evaluate. Consequently, AC arguments are prone to fallacies, and unsafe systems have been deployed as a result of fallacious ACs. To mitigate this problem, prior work [32] created a set of provably valid AC strategy templates to guide developers in building rigorous ACs. Yet instantiations of these templates remain error-prone and still need to be reviewed manually. In this paper, we report on using the interactive theorem prover Lean to bridge the gap between safety arguments and rigorous model-based reasoning. We generate formal, model-based machine-checked AC arguments, taking advantage of the traceability between model and safety artifacts, and mitigating errors that could arise from manual argument assessment. The approach is implemented in an extended version of the MMINT-A model management tool [10]. Implementation includes a conversion of informal claims into formal Lean properties, decomposition into formal sub-properties and generation of correctness proofs. We demonstrate the applicability of the approach on two safety case studies from the literature.",
        "keywords": [],
        "authors": [
            "Torin Viger",
            "Logan Murphy",
            "Alessio Di Sandro",
            "Ramy Shahin",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a194/349500a194.pdf"
    },
    {
        "title": "Restricted Natural Language and Model-based Adaptive Test Generation for Autonomous Driving",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "With the aim to reduce car accidents, autonomous driving attracted a lot of attentions these years. However, recently reported crashes indicate that this goal is far from being achieved. Hence, cost-effective testing of autonomous driving systems (ADSs) has become a prominent research topic. The classical model-based testing (MBT), i.e., generating test cases from test models followed by executing the test cases, is ineffective for testing ADSs, mainly because of the constant exposure to ever-changing operating environments, and uncertain internal behaviors due to employed AI techniques. Thus, MBT must be adaptive to guide test case generation based on test execution results in a step-wise manner. To this end, we propose a natural language and model-based approach, named LiveTCM, to automatically execute and generate test case speciﬁcations (TCSs) by interacting with an ADS under test and its environment. LiveTCM is evaluated with an open-source ADS and two test generation strategies: Deep Q-Network (DQN)-based and Random. Results show that LiveTCM with DQN can generate TCSs with 56 steps on average in 60 seconds, leading to 6.4 test oracle violations and covering 14 APIs per TCS on average.",
        "keywords": [
            "Natural Language and Model-based Testing",
            "Adaptive Test Generation",
            "Autonomous Driving"
        ],
        "authors": [
            "Yize Shi",
            "Chengjie Lu",
            "Man Zhang",
            "Huihui Zhang",
            "Tao Yue",
            "Shaukat Ali"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a101/349500a101.pdf"
    },
    {
        "title": "OSTRICH - A Type-safe Template Language for Low-code Development",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Low-code platforms aim at allowing non-experts to develop complex systems and knowledgeable developers to improve their productivity in orders of magnitude. The greater gain comes from (re)using components developed by experts capturing common patterns across all layers of the application, from the user interface to the data layer and integration with external systems. Often, cloning sample code fragments is the only alternative in such scenarios, requiring extensive adaptation to reach the intended use. Such customization activities require deep knowledge outside of the comfort zone of low-code. To effectively speed up the reuse, composition, and adaptation of pre-deﬁned components, low-code platforms need to provide safe and easy-to-use language mechanisms.\nThis paper introduces OSTRICH, a strongly-typed rich tem-plating language for a low-code platform (OutSystems) that builds on metamodel annotations and allows the correct in-stantiation of templates. We conservatively extend the existing metamodel and ensure that the resulting code is always well-formed. The results we present include a novel type safety veriﬁ-cation of template deﬁnitions, and template arguments, providing model consistency across application layers. We implemented this template language in a prototype of the OutSystems platform and ported nine of the top ten most used sample code fragments, thus improving the reuse of professionally designed components.",
        "keywords": [
            "metamodel templating",
            "typechecking templates",
            "low-code",
            "development productivity",
            "model reuse"
        ],
        "authors": [
            "Hugo Lourenço",
            "Carla Ferreira",
            "João Costa Seco"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a216/349500a216.pdf"
    },
    {
        "title": "Scalable N-Way Model Matching using Multi-dimensional Search Trees",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Alexander Schultheiß",
            "Paul Maximilian Bittner",
            "Lars Grunske",
            "Thomas Thüm",
            "and Timo Kehrer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z005/349500z005.pdf"
    },
    {
        "title": "Identifying Metamodel Inaccurate Structures During Metamodel/Constraint Co-Evolution",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Metamodels are subject to evolution over their lifetime. UML metamodel for instance evolved through different versions, ranging from 0.8 to 2.5 minors. These metamodels are sometimes accompanied with constraints deﬁned using OCL (Object Constraint Language). Many works in the literature developed methods for managing and assisting the co-evolution of metamodels and their constraints. These methods enable a developer to update, in an automated (or semi-automated) way, the constraints associated to a metamodel starting from the deltas identiﬁed between versions of this metamodel. In this work we complement this assistance by notifying the developer with potential inaccurate structures in the metamodel that may be introduced during evolution. We introduce in this paper an original evolution assistance method which focuses rather on the problem (notifying metamodel inaccurate structures) than on the solution (generating OCL constraints using patterns of them). The ultimate goal of this assistance is not only to enable the developer to complete existing/updated constraints with new ones, but also to accompany her/him to further check existing constraints and to test whether they still hold. A case study is presented to show the relevance of the method.",
        "keywords": [
            "Model-driven Engineering",
            "Metamodelling",
            "OCL",
            "Co-Evolution"
        ],
        "authors": [
            "Elyes Cherfa",
            "Soraya Mesli-Kesraoui",
            "Chouki Tibermacine",
            "Salah Sadou",
            "R´egis Fleurquin"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a024/349500a024.pdf"
    },
    {
        "title": "Quality Guidelines for Research Artifacts in Model-Driven Engineering",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Sharing research artifacts is known to help people to build upon existing knowledge, adopt novel contributions in practice, and increase the chances of papers receiving attention. In Model-Driven Engineering (MDE), openly providing research artifacts plays a key role, even more so as the community targets a broader use of AI techniques, which can only become feasible if large open datasets and conﬁdence measures for their quality are available. However, the current lack of common discipline-speciﬁc guidelines for research data sharing opens the opportunity for misunderstandings about the true potential of research artifacts and subjective expectations regarding artifact quality. To address this issue, we introduce a set of guidelines for artifact sharing specifically tailored to MDE research. To design this guidelines set, we systematically analyzed general-purpose artifact sharing practices of major computer science venues and tailored them to the MDE domain. Subsequently, we conducted an online survey with 90 researchers and practitioners with expertise in MDE. We investigated our participants’ experiences in developing and sharing artifacts in MDE research and the challenges encountered while doing so. We then asked them to prioritize each of our guidelines as essential, desirable, or unnecessary. Finally, we asked them to evaluate our guidelines with respect to clarity, completeness, and relevance. In each of these dimensions, our guidelines were assessed positively by more than 92% of the participants. To foster the reproducibility and reusability of our results, we make the full set of generated artifacts available in an open repository at https://mdeartifacts.github.io/",
        "keywords": [
            "Software artifacts",
            "Open Science",
            "Model-Driven Engineering",
            "Quality Management"
        ],
        "authors": [
            "Carlos Diego Nascimento Damasceno and Daniel Str¨uber"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a285/349500a285.pdf"
    },
    {
        "title": "Scalable N-Way Model Matching Using Multi-Dimensional Search Trees",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Model matching algorithms are used to identify common elements in input models, which is a fundamental pre-condition for many software engineering tasks, such as merging software variants or views. If there are multiple input models, an n-way matching algorithm that simultaneously processes all models typically produces better results than the sequential application of two-way matching algorithms. However, existing algorithms for n-way matching do not scale well, as the computational effort grows fast in the number of models and their size. We propose a scalable n-way model matching algorithm, which uses multi-dimensional search trees for efficiently finding suitable match candidates through range queries. We implemented our generic algorithm named RaQuN (Range Queries on N input models) in Java, and empirically evaluate the matching quality and runtime performance on several datasets of different origin and model type. Compared to the state-of-the-art, our experimental results show a performance improvement by an order of magnitude, while delivering matching results of better quality.",
        "keywords": [
            "Model-driven engineering",
            "n-way model matching",
            "clone-and-own development",
            "software product lines",
            "multi-view integration",
            "variability mining."
        ],
        "authors": [
            "Alexander Schultheiß",
            "Paul Maximilian Bittner",
            "Lars Grunske",
            "Thomas Thüm and Timo Kehrer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a001/349500a001.pdf"
    },
    {
        "title": "Towards Control Flow Analysis of Declarative Graph Transformations with Symbolic Execution",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "The declarative graph transformation language Henshin transforms instance models represented as graphs by applying a series of basic steps that match and replace structural patterns on parts of models. These simple transformation rules are then combined into control ﬂow constructs similar to those of imperative programming languages to create more complex transformations. However, defects in the structure of control ﬂow or in transformation rules might misschedule the application of operations, resulting in basic steps to be inapplicable or produce incorrect output. Understanding and ﬁxing these bugs is complicated by the fact that pattern matching in rules is non-deterministic. Moreover, some control ﬂow structures employ a nondeterministic choice of alternatives. This makes it challenging for developers to keep track of all the possible execution paths and interactions between them.\nFor conventional programming languages, techniques have been developed to execute a program symbolically. By abstracting over the concrete values of variables in any actual run, generalized knowledge is gained about the possible behavior of the program. This can be useful in understanding problems and fixing bugs. In this paper, we present an approach to symbolically execute graph transformations for a subset of Henshin, using symbolic path constraints based on the cardinalities of graph pattern occurrences in the model.",
        "keywords": [
            "model driven software engineering",
            "declarative graph transformations",
            "control ﬂow analysis",
            "symbolic execution"
        ],
        "authors": [
            "Florian Ege",
            "Matthias Tichy"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a156/349500a156.pdf"
    },
    {
        "title": "Efﬁcient Replay-based Regression Testing for Distributed Reactive Systems in the Context of Model-driven Development",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "As software evolves, regression testing techniques are typically used to ensure the new changes are not adversely affecting the existing features. Despite recent advances, regression testing for distributed systems remains challenging and extremely costly. Existing techniques often require running a failing system several time before detecting a regression. As a result, conven-tional approaches that use re-execution without considering the inherent non-determinism of distributed systems, and providing no (or low) control over execution are inadequate in many ways. In this paper, we present MRegTest, a replay-based regression testing framework in the context of model-driven development to facilitate deterministic replay of traces for detecting regressions while offering sufﬁcient control for the purpose of testing over the execution of the changed system. The experimental results show that compared to the traditional approaches that annotate traces with timestamps and variable values MRegTest detects almost all regressions while reducing the size of the trace signiﬁcantly and incurring similar runtime overhead.",
        "keywords": [
            "MDD",
            "Distributed Systems",
            "Regression Testing"
        ],
        "authors": [
            "Majid Babaei",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a089/349500a089.pdf"
    },
    {
        "title": "Integrated and Iterative Requirements Analysis and Test Speciﬁcation: A Case Study at Kostal",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Currently, practitioners follow a top-down approach in automotive development projects. However, recent studies have shown that this top-down approach is not suitable for the implementation and testing of modern automotive systems. Specifically, practitioners increasingly fail to specify requirements and tests for systems with complex component interactions (e.g., e-mobility systems). In this paper, we address this research gap and propose an integrated and iterative scenario-based technique for the speciﬁcation of requirements and test scenarios. Our idea is to combine both a top-down and a bottom-up integration strategy. For the top-down approach, we use a behavior-driven development (BDD) technique to drive the modeling of high-level system interactions from the user’s perspective. For the bottom-up approach, we discovered that natural language processing (NLP) techniques are suited to make textual speciﬁcations of existing components accessible to our technique. To integrate both directions, we support the joint execution and automated analysis of system-level interactions and component-level behavior. We demonstrate the feasibility of our approach by conducting a case study at Kostal (Tier1 supplier). The case study corroborates, among other things, that our approach supports practitioners in improving requirements and test speciﬁcations for integrated system behavior.",
        "keywords": [
            "Requirements Analysis",
            "Test Speciﬁcation",
            "Natural Language Processing",
            "Scenario-based Requirements Engineering",
            "Model-based Testing",
            "Scenario-based Testing"
        ],
        "authors": [
            "Carsten Wiecher",
            "Jannik Fischbach",
            "Joel Greenyer",
            "Andreas Vogelsang",
            "Carsten Wolff",
            "Roman Dumitrescu"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a112/349500a112.pdf"
    },
    {
        "title": "Exploring Architectural Design Decisions in Industry 4.0: A Literature Review and Taxonomy",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Architectural design decisions, such as service de-\nployment and composition, plant layout synthesis, or production\nplanning, are an indispensable and overarching part of an\nindustrial manufacturing system design. In the fourth industrial\nrevolution (Industry 4.0), frequent production changes trigger\ntheir synthesis, and preferably optimization. Yet, knowledge\non architecture synthesis and optimization has been scattered\naround other domains, such as generic software engineering.\nWe take a step towards synthesizing current knowledge on\narchitectural design decisions in Industry 4.0. We developed a\ntaxonomy describing architectural models, design decisions, and\noptimization possibilities. The developed taxonomy serves as a\nguideline for comparing different possibilities (e.g., application\nof different optimization algorithms) and selecting appropriate\nones for a given context. Furthermore, we reviewed and mapped\n30 relevant research works to the taxonomy, identifying research\ntrends and gaps. We discuss interesting, and yet uncovered topics\nthat emerged from our review.",
        "keywords": [
            "architecture synthesis",
            "optimization",
            "taxonomy",
            "design space exploration",
            "model-based development",
            "Industry 4.0"
        ],
        "authors": [
            "Tarik Terzimehi´c",
            "Kirill Dorofeev",
            "Sebastian Voss"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a170/349500a170.pdf"
    },
    {
        "title": "Synthesizing Veriﬁed Components for Cyber Assured Systems Engineering",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Cyber-physical systems, such as avionics, must be tolerant to cyber-attacks in the same way they are tolerant to random faults: they either gracefully recover or safely shut down as requirements dictate. The DARPA Cyber Assured Systems Engineering program is developing tools for design, analysis, and veriﬁcation that enable systems engineers to design-in cyber-resiliency in a Model-Based Systems Engineering environment. This paper describes automated model transformations that introduce high-assurance cyber-resiliency components into a system, in particular ﬁlters and monitors that prevent malicious input and detect supply chain attacks, respectively. A formal speciﬁcation deﬁnes each high-assurance component, and is used to verify that the component addresses system level cyber requirements. Implementations for these high-assurance components are directly synthesized from their speciﬁcations, and are automatically proven to preserve the exact meaning of the speciﬁcations all the way down to the binary code level. The model transformations are integrated into the Open Source AADL Tool Environment (OSATE). The paper further reports on a case study applying security-enhancing model transformations to a UAV system that uses the Air Force Research Laboratory’s OpenUxAS services for route planning. In the case study, the model transformations add ﬁlters to guard against malformed input, as well as monitors to guard against ground station spooﬁng and malicious ﬂight plans from OpenUxAS.",
        "keywords": [],
        "authors": [
            "Eric Mercer",
            "Konrad Slind",
            "Isaac Amundson",
            "Darren Cofer",
            "Junaid Babar",
            "David Hardin"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a205/349500a205.pdf"
    },
    {
        "title": "Assessing the Usefulness of a Visual Programming IDE for Large-Scale Automation Software",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Industrial control applications are usually designed by domain experts instead of software engineers. These experts frequently use visual programming languages based on standards such as IEC 61131-3 and IEC 61499. The standards apply model-based engineering concepts to abstract from hardware and low-level communication. Developing industrial control software is challenging due the fact that such systems are usually one-of-a-kind systems that have to be maintained for many years. These challenges, together with the growing complexity of control software, require very usable model-based development environments for visual programming languages. However, so far only little empirical research exists on the practical usefulness of such environments, i.e., their usability and utility. In this paper, we discuss common control software maintenance tasks and tool capabilities based on existing research and show the realization of these capabilities in 4diac IDE. We ﬁrst performed a walkthrough of the demonstrated capabilities using the cognitive dimensions of notations framework from the ﬁeld of human-computer interaction. We then improved the tool and conducted a user study involving ten industrial automation engineers, who used 4diac IDE in a realistic control software maintenance scenario. Our ﬁndings demonstrate how the usefulness of IDEs can be successfully investigated using a multi-phase approach that includes a walkthrough and a user study. We discuss lessons learned and derive general implications with respect to large-scale applications for developers of IDEs that we deem applicable in the context of (visual) model-based engineering tools.",
        "keywords": [
            "Usefulness study",
            "Open source software",
            "IEC 61499",
            "Modeling tools",
            "Model-driven engineering"
        ],
        "authors": [
            "Bianca Wiesmayr",
            "Alois Zoitl",
            "Rick Rabiser"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a297/349500a297.pdf"
    },
    {
        "title": "Preface to the 24th International ACM/IEEE Conference on Model Driven Engineering Languages and Systems (MoDELS)",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "This paper is a preface to the 24th International ACM/IEEE Conference on Model Driven Engineering Languages and Systems (MoDELS). It describes the conference, its history, the challenges faced due to the COVID-19 pandemic, and the organization of the conference, including the review process and acceptance rates for the Foundations and Practice & Innovation tracks.",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z010/349500z010.pdf"
    },
    {
        "title": "MoDLF – A Model-Driven Deep Learning Framework for Autonomous Vehicle Perception (AVP)",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Modern vehicles are extremely complex embedded systems that integrate software and hardware from a large set of contributors. Modeling standards like EAST-ADL have shown promising results to reduce complexity and expedite system development. However, such standards are unable to cope with the growing demands of the automotive industry. A typical example of this phenomenon is autonomous vehicle perception (AVP) where deep learning architectures (DLA) are required for computer vision (CV) tasks like real-time object recognition and detection. However, existing modeling standards in the automotive industry are unable to manage such CV tasks at a higher abstraction level. Consequently, system development is currently accomplished through modeling approaches like EAST-ADL while DLA-based CV features for AVP are implemented in isolation at a lower abstraction level. This significantly compromises productivity due to integration challenges. In this article, we introduce MoDLF - A Model-Driven Deep learning Framework to design deep convolutional neural network (DCNN) architectures for AVP tasks. Particularly, Model Driven Architecture (MDA) is leveraged to propose a metamodel along with a conformant graphical modeling workbench to model DCNNs for CV tasks in AVP at a higher abstraction level. Furthermore, Model-To-Text (M2T) transformations are provided to generate executable code for MATLAB® and Python. The framework is validated via two case studies on benchmark datasets for key AVP tasks. The results prove that MoDLF effectively enables model-driven architectural exploration of deep convnets for AVP system development while supporting integration with renowned existing standards like EAST-ADL.",
        "keywords": [
            "Model-Driven Architecture",
            "Model transformation",
            "Low code",
            "Autonomous vehicles perception",
            "Deep learning",
            "Computer vision"
        ],
        "authors": [
            "Aon Safdar",
            "Farooque Azam",
            "Muhammad Waseem Anwar",
            "Usman Akram and Yawar Rasheed"
        ],
        "file_path": "data/models/models22/main/papers/p187-safdar.pdf"
    },
    {
        "title": "Incremental Causal Connection for Self-Adaptive Systems Based on Relational Reference Attribute Grammars",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Even though model-driven engineering reduces complexity during the development of self-adaptive systems and models@run.time enables using them during runtime, connecting models to different external systems still involves manual work. Those connections are essential to the complete system, as they enable external systems to react to changes in the internal model and vice versa. In our case, the model is based on Relational Reference Attribute Grammars, an extension of Attribute Grammars to enable conceptual models at runtime while retaining their benefits of modular specification and an incremental evaluation scheme. We present an approach to enable concise specification of the causal connection and needed transformations to match required formats or semantics. To show its applicability, a case study showing the coordination of multiple industrial robot arms using models is presented. We show that using our approach, connections can be specified more concisely while maintaining the same efficiency as hand-written code. The artefact comprising all source code and an executable version of the case studies is available at https://doi.org/10.5281/zenodo.7009758.",
        "keywords": [
            "Reference Attribute Grammar",
            "Cyber-physical System",
            "Causal Connection",
            "Models@run.time",
            "Model-Driven Software Engineering"
        ],
        "authors": [
            "René Schöne",
            "Johannes Mey",
            "Sebastian Ebert",
            "Sebastian Götz",
            "Uwe Aßmann"
        ],
        "file_path": "data/models/models22/main/papers/p1-schoene.pdf"
    },
    {
        "title": "Feedback on the Formal Verification of UML Models in an Industrial Context: The Case of a Smart Device Life Cycle Management System",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "This paper presents experience feedback on how we managed to formally verify properties on semi-formal models of a Life Cycle Management System (LCMS) for smart devices. These devices are typically structured around a System on Chip (SoC), which can provide built-in hardware security. They can offer the possibility to make the deployment of Product-Service Systems (PSSs) to consumers easier, through traceability and collaborative consumption rule enforcement. A PSS is a business model in which products and services are tightly connected. One of the main advantages of such a PSS is that it optimizes product use, with a positive environmental impact. Associating the LCMS with a blockchain-based protocol makes it possible to avoid centralization. Semi-formal UML models of such a LCMS, as well as the informal properties it must comply with, were defined in order to explore its design space and evaluate the outcomes of specific design choices. However, the security of the LCMS implementation must be guaranteed, including protocols and architecture. For that purpose, these models and properties were later improved to be formally verifiable, which ensures the security of their implementation at the expense of added complexity. The verification was carried out using two available software tools: VerifPal for the protocol model, and AnimUML (developed by one of the authors) for the architecture model. This makes the procedure accessible for non-specialists in formal verification. Finally, our feedback on the whole process as well as on VerifPal is also provided.",
        "keywords": [
            "Formally verifiable models",
            "Formal verification tools",
            "UML",
            "Cryptographic protocol",
            "Life cycle management system"
        ],
        "authors": [
            "Maxime Méré",
            "Frédéric Jouault",
            "Loïc Pallardy",
            "Richard Perdriau"
        ],
        "file_path": "data/models/models22/main/papers/p121-mere.pdf"
    },
    {
        "title": "Modelling Program Verification Tools for Software Engineers",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "In software engineering, models are used for many different things. In this paper, we focus on program verification, where we use models to reason about the correctness of systems. There are many different types of program verification techniques which provide different correctness guarantees. We investigate the domain of program verification tools, and present a concise megamodel to distinguish these tools. We also present a data set of almost 400 program verification tools. This data set includes the category of verification tool according to our megamodel, practical information such as input/output format, repository links, and more. The categorisation enables software engineers to find suitable tools, investigate similar alternatives and compare them. We also identify trends for each level in our megamodel based on the categorisation. Our data set, publicly available at https://doi.org/10.4121/20347950, can be used by software engineers to enter the world of program verification and find a verification tool based on their requirements.",
        "keywords": [
            "Formal Methods; Program Verification; Megamodelling."
        ],
        "authors": [
            "Sophie Lathouwers and Vadim Zaytsev"
        ],
        "file_path": "data/models/models22/main/papers/p98-lathouwers.pdf"
    },
    {
        "title": "Quantifying the Variability Mismatch Between Problem and Solution Space",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "A software product line allows to derive individual software products based on a configuration. As the number of configurations is an indicator for the general complexity of a software product line, automatic #SAT analyses have been proposed to provide this information. However, the number of configurations does not need to match the number of derivable products. Due to this mismatch, using the number of configurations to reason about the software complexity (i.e., the number of derivable products) of a software product line can lead to wrong assumptions during implementation and testing. How to compute the actual number of derivable products, however, is unknown. In this paper, we mitigate this problem and present a concept to derive a solution-space feature model which allows to reuse existing #SAT analyses for computing the number of derivable products of a software product line. We apply our concept to a total of 119 subsystems of three industrial software product lines. The results show that the derivation scales for real world software product lines and confirm the mismatch between the number of configurations and the number of products.",
        "keywords": [
            "Product lines",
            "variability mismatch",
            "solution-space analyses"
        ],
        "authors": [
            "Marc Hentze",
            "Chico Sundermann",
            "Thomas Thüm",
            "Ina Schaefer"
        ],
        "file_path": "data/models/models22/main/papers/p322-hentze.pdf"
    },
    {
        "title": "Addressing the Uncertainty Interaction Problem in Software-intensive Systems: Challenges and Desiderata",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Software-intensive systems are increasingly used to support tasks that are typically characterized by high degrees of uncertainty. The modeling notations employed to design, verify, and operate such systems have increasingly started to capture different types of uncertainty, so that they can be explicitly considered when systems are developed and deployed. While these modeling paradigms consider different sources of uncertainty individually, these sources are rarely independent, and their interactions affect the achievement of system goals in subtle and often unpredictable ways. This vision paper describes the problem of uncertainty interaction in software-intensive systems, illustrating it on examples from relevant application domains. We then identify key open challenges and define desiderata that future modeling notations and model-driven engineering research should consider to address these challenges.",
        "keywords": [
            "Uncertainty interaction",
            "Modeling notations",
            "Patterns"
        ],
        "authors": [
            "Javier Cámara",
            "Radu Calinescu",
            "Betty H.C. Cheng",
            "David Garlan",
            "Bradley Schmerl",
            "Javier Troya",
            "Antonio Vallecillo"
        ],
        "file_path": "data/models/models22/main/papers/p24-camara.pdf"
    },
    {
        "title": "Machine Learning-based Incremental Learning in Interactive Domain Modelling",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "In domain modelling, practitioners manually transform informal requirements written in natural language (problem descriptions) to more concise and analyzable domain models expressed with class diagrams. With automated domain modelling support using existing approaches, manual modifications may still be required in extracted domain models and problem descriptions to make them more accurate and concise. For example, educators teaching software engineering courses at universities usually use an incremental approach to build modelling exercises to restrict students in using intended modelling patterns. These modifications result in the evolution of domain modelling exercises over time. To assist practitioners in this evolution, a synergy between interactive support and automated domain modelling is required. In this paper, we propose a bot-assisted approach to allow practitioners perform domain modelling quickly and interactively. Furthermore, we provide an incremental learning strategy empowered by machine learning to improve the accuracy of the bot’s suggestions and extracted domain models by analyzing practitioners’ decisions over time. We evaluate the performance of our bot using test problem descriptions which shows that practitioners can expect to get useful support from the bot when applied to exercises of similar size and complexity, with precision, recall, and F2 scores over 85%. Finally, we evaluate our incremental learning strategy where we observe a reduction in the required manual modifications by 70% and an improvement of F2 scores of extracted domain models by 4.2% when using our proposed approach and learning strategy together.",
        "keywords": [
            "Domain Models",
            "Natural Language Processing (NLP)",
            "Machine Learning (ML)",
            "Bot",
            "Evolution",
            "Decisions",
            "Incremental Learning"
        ],
        "authors": [
            "Rijul Saini",
            "Gunter Mussbacher",
            "Jin L.C. Guo",
            "Jörg Kienzle"
        ],
        "file_path": "data/models/models22/main/papers/p176-saini.pdf"
    },
    {
        "title": "Digital Twin as Risk-Free Experimentation Aid for Techno-socio-economic Systems",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Environmental uncertainties and hyperconnectivity force techno-socio-economic systems to introspect and adapt to succeed and survive. Current practices in decision-making are predominantly intuition-driven with attendant challenges for precision and rigor. We propose to use the concept of digital twins by combining results from Modelling & Simulation, Artificial Intelligence, and Control Theory to create a risk free ‘in silico’ experimentation aid to help: (i) understand why a system is the way it is, (ii) be prepared for possible outlier conditions, and (iii) identify plausible solutions for mitigating the outlier conditions in an evidence-backed manner. We use reinforcement learning to systematically explore the digital twin solution space. Our proposal is significant because it advances the effective use of digital twins to new problem domains that have new potential for impact. Our approach contributes an original meta model for simulatable digital twin of industry scale techno-socio-economic systems, agent-based implementation of the digital twin, and an architecture that serves as a risk-free experimentation aid to support simulation-based evidence-backed decision-making. We also discuss the rigor of our validation of the proposed approach and associated technology infrastructure through a representative sample of industry-scale real-world use cases.",
        "keywords": [
            "Digital Twin",
            "Decision Making",
            "Simulatable Model",
            "Agent Model"
        ],
        "authors": [
            "Souvik Barat",
            "Tony Clark",
            "Vinay Kulkarni",
            "Balbir Barn"
        ],
        "file_path": "data/models/models22/main/papers/p66-barat.pdf"
    },
    {
        "title": "Model-Checking Legal Contracts with SymboleoPC",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Legal contracts specify requirements for business transactions. As\nany other requirements specification, contracts may contain errors\nand violate properties expected by contracting parties. Symboleo\nwas recently proposed as a formal specification language for legal\ncontracts. This paper presents SymboleoPC, a tool for analyzing\nSymboleo contracts using model checking. It highlights the architec-\nture, implementation and testing of the tool, as well as a scalability\nevaluation with respect to the size of contracts and properties to\nbe checked through a series of experiments. The results suggest\nthat SymboleoPC can be usefully applied to the analysis of formal\nspecifications of contracts with real-life sizes and structures.",
        "keywords": [
            "Legal contracts",
            "smart contracts",
            "software requirements specifications",
            "formal specification languages",
            "model checking",
            "performance analysis",
            "nuXmv"
        ],
        "authors": [
            "Alireza Parvizimosaed",
            "Marco Roveri",
            "Aidin Rasti",
            "Daniel Amyot",
            "Luigi Logrippo",
            "John Mylopoulos"
        ],
        "file_path": "data/models/models22/main/papers/p278-parvizimosaed.pdf"
    },
    {
        "title": "Assisting in Requirements Goal Modeling: A Hybrid Approach based on Machine Learning and Logical Reasoning",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Goal modeling plays an imperative role in early requirements engineering, which has been investigated for decades. There have been many studies that show the usefulness of requirements goal models. However, the establishment of goal models is typically done manually, which is time-consuming and has a steep learning curve. In this paper, we propose a semi-automatic framework for constructing iStar models, which is a well-known goal modeling language. Specifically, we first investigate the practical needs of iStar modelers on the automation of iStar modeling by holding interviews, based on which we propose an interactive and iterative modeling process. Our proposal takes advantage of human decisions and artificial intelligence algorithms, respectively, aiming at achieving low modeling costs while maintaining the quality of models. We then propose a hybrid approach for automatically extracting goal model snippets from requirements text, which implements the automatic tasks of our proposed process. The proposed method combines logical reasoning with deep learning techniques so as to unleash the power of domain knowledge to assist with automation tasks. We have performed a series of experiments for evaluation. The experimental results show that our method achieves the F1-measure of 90.34% for actor entity extraction, 93.14% for intention entity extraction, and 83.18% for actor relation extraction, which can efficiently establish high-quality goal models. The artifacts are available at Zenodo1.",
        "keywords": [
            "goal modeling",
            "requirements engineering",
            "natural language processing",
            "machine learning"
        ],
        "authors": [
            "Qixiang Zhou",
            "Tong Li",
            "Yunduo Wang"
        ],
        "file_path": "data/models/models22/main/papers/p199-zhou.pdf"
    },
    {
        "title": "Automatic Test Amplification for Executable Models",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Behavioral models are important assets that must be thoroughly veri-\nfied early in the design process. This can be achieved with manually-\nwritten test cases that embed carefully hand-picked domain-specific\ninput data. However, such test cases may not always reach the de-\nsired level of quality, such as high coverage or being able to localize\nfaults efficiently. Test amplification is an interesting emergent ap-\nproach to improve a test suite by automatically generating new test\ncases out of existing manually-written ones. Yet, while ad-hoc test\namplification solutions have been proposed for a few programming\nlanguages, no solution currently exists for amplifying the test cases\nof behavioral models.\nIn this paper, we fill this gap with an automated and generic\napproach. Given an executable DSL, a conforming behavioral model,\nand an existing test suite, our approach generates new regression\ntest cases in three steps: (i) generating new test inputs by applying\na set of generic modifiers on the existing test inputs; (ii) running\nthe model under test with new inputs and generating assertions from\nthe execution traces; and (iii) selecting the new test cases that\nincrease the mutation score. We provide tool support for the approach\natop the Eclipse GEMOC Studio1 and show its applicability in an\nempirical study. In the experiment, we applied the approach to 71\ntest suites written for models conforming to two different DSLs, and\nfor 67 of the 71 cases, it successfully improved the mutation score\nbetween 3.17 % and 54.11 % depending on the initial setup.",
        "keywords": [
            "Test Amplification",
            "Regression Testing",
            "Executable Model",
            "Executable DSL"
        ],
        "authors": [
            "Faezeh Khorram",
            "Erwan Bousse",
            "Jean-Marie Mottu",
            "Gerson Sunyé",
            "Pablo Gómez-Abajo",
            "Pablo C. Cañizares",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models22/main/papers/p109-khorram.pdf"
    },
    {
        "title": "Schema Inference for Multi-Model Data",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "The knowledge of a structural schema of data is a crucial aspect of most data management tasks. Unfortunately, in many real-world scenarios, the data is not accompanied by it, and schema-inference approaches need to be utilised.\nIn this paper, we focus on a specific and complex use case of multi-model data where several often contradictory features of the combined models must be considered. Hence, single-model approaches cannot be applied straightforwardly. In addition, the data often reach the scale of Big Data, and thus a scalable solution is inevitable. In our approach, we reflect all these challenges. In addition, we can also infer local integrity constraints as well as intra- and inter-model references. Last but not least, we can cope with cross-model data redundancy. Using a set of experiments, we prove the advantages of the proposed approach and we compare it with related work.",
        "keywords": [
            "schema inference",
            "multi-model data",
            "cross-model references",
            "data redundancy"
        ],
        "authors": [
            "Pavel Koupil",
            "Sebastián Hricko",
            "Irena Holubová"
        ],
        "file_path": "data/models/models22/main/papers/p13-koupli.pdf"
    },
    {
        "title": "Finding with NEMO: A Recommender System to Forecast the Next Modeling Operations",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Nowadays, while modeling environments provide users with facilities to specify different kinds of artifacts, e.g., metamodels, models, and transformations, the possibility of learning from previous modeling experiences and being assisted during modeling tasks remains largely unexplored. In this paper, we propose NEMO, a recommender system based on an Encoder-Decoder neural network to assist modelers in performing model editing operations. NEMO learns from past modeling activities and performs predictions employing a deep learning technique. Such an algorithm has been successfully applied in machine translation to convert a text from a language to another foreign language and vice versa. An empirical evaluation on a dataset of BPMN change-based persistent model demonstrates that the technique permits learning from existing operations and effectively predicting the next editing operations with considerably high prediction accuracy. In particular, NEMO gets 0.977 as precision/recall and 0.992 as success rate score by the best performance.",
        "keywords": [],
        "authors": [
            "Juri Di Rocco",
            "Claudio Di Sipio",
            "Phuong T. Nguyen",
            "Davide Di Ruscio",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/models/models22/main/papers/p154-rocco.pdf"
    },
    {
        "title": "Survey of Established Practices in the Life Cycle of Domain-Specific Languages",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Domain-specific languages (DSLs) have demonstrated their usefulness within many domains such as finance, robotics, and telecommunication. This success has been exemplified by the publication of a wide range of articles regarding specific DSLs and their merits in terms of improved software quality, programmer efficiency, security, etc. However, there is little public information on what happens to these DSLs after they are developed and published. The lack of information makes it difficult for a DSL practitioner or tool creator to identify trends, current practices, and issues within the field. In this paper, we seek to establish the current state of a DSL’s life cycle by analysing 30 questionnaire answers from DSL authors on the design and development, launch, evolution, and end of life of their DSL. On this empirical foundation, we make six recommendations to DSL practitioners, scholars, and tool creators on the subjects of user involvement in the design process, DSL evolution, and the end of life of DSLs.",
        "keywords": [
            "Domain-specific languages",
            "Survey"
        ],
        "authors": [
            "Holger Stadel Borum",
            "Christoph Seidl"
        ],
        "file_path": "data/models/models22/main/papers/p266-borum.pdf"
    },
    {
        "title": "Nested OSTRICH: Hatching Compositions of Low-code Templates",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Low-code frameworks strive to simplify and speed-up application\ndevelopment. Native support for the reuse and composition of\nparameterised coarse-grain components (templates) is essential\nto achieve these goals. OSTRICH — a rich template language for\nthe OutSystems platform — was designed to simplify the use and\ncreation of such templates. However, without a built-in composition\nmechanism, OSTRICH templates are hard to create and maintain.\nThis paper presents a template composition mechanism and its\ntyping and instantiation algorithms for model-driven low-code de-\nvelopment environments. We evolve OSTRICH to support nested\ntemplates and allow the instantiation (hatching) of templates in\nthe definition of other templates. Thus, we observe a significant\nincrease code reuse potential, leading to a safer evolution of appli-\ncations. The present definition seamlessly extends the existing Out-\nSystems metamodel with template constructs expressed by model\nannotations that maintain backward compatibility with the existing\nlanguage toolchain. We present the metamodel, its annotations, and\nthe corresponding validation and instantiation algorithms. In par-\nticular, we introduce a type-based validation procedure that ensures\nthat using a template inside a template produces valid models.\nThe work is validated using the OSTRICH benchmark. Our proto-\ntype is an extension of the OutSystems IDE allowing the annotation\nof models and their use to produce new models. We also analyse\nwhich existing OutSystems sample screens templates can be im-\nproved by using and sharing nested templates.",
        "keywords": [],
        "authors": [
            "João Costa Seco",
            "Hugo Lourenço",
            "Joana Parreira",
            "Carla Ferreira"
        ],
        "file_path": "data/models/models22/main/papers/p210-seco.pdf"
    },
    {
        "title": "Verification of Railway Network Models with EVEREST",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Models – at different levels of abstraction and pertaining to different\nengineering views – are central in the design of railway networks, in\nparticular signalling systems. The design of such systems must fol-\nlow numerous strict rules, which may vary from project to project\nand require information from different views. This renders manual\nverification of railway networks costly and error-prone.\nThis paper presents EVEREST, a tool for automating the verifica-\ntion of railway network models that preserves the loosely coupled\nnature of the design process. To achieve this goal, EVEREST first\ncombines two different views of a railway network model – the\ntopology provided in signalling diagrams containing the functional\ninfrastructure, and the precise coordinates of the elements pro-\nvided in technical drawings (CAD) – in a unified model stored in the\nrailML standard format. This railML model is then verified against\na set of user-defined infrastructure rules, written in a custom modal\nlogic that simplifies the specification of spatial constraints in the\nnetwork. The violated rules can be visualized both in the signalling\ndiagrams and technical drawings, where the element(s) responsible\nfor the violation are highlighted.\nEVEREST is integrated in a long-term effort of EFACEC to im-\nplement industry-strong tools to automate and formally verify the\ndesign of railway solutions.",
        "keywords": [
            "railway engineering",
            "railway network model verification",
            "formal infrastructure rule specification",
            "railML"
        ],
        "authors": [
            "João Martins",
            "José M. Fonseca",
            "Rafael Costa",
            "José C. Campos",
            "Alcino Cunha",
            "Nuno Macedo",
            "José N. Oliveira"
        ],
        "file_path": "data/models/models22/main/papers/p345-martins.pdf"
    },
    {
        "title": "Validating the Correctness of Reactive Systems Specifications Through Systematic Exploration",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. While the synthesized system is guaranteed to be correct w.r.t. the specification, the specification itself may be incorrect w.r.t. the engineers’ intention or w.r.t. the requirements or the environment in which the system should execute in. It thus requires validation. Combinatorial coverage (CC) is a well-known coverage criterion. Its rationale and key for effectiveness is the empirical observation that in many cases, the presence of a defect depends on the interaction between a small number of features of the system at hand. In this work we propose a validation approach for a reactive system specification, based on a systematic combinatorial exploration of the behaviors of a controller that was synthesized from it. Specifically, we present an algorithm to generate and execute a small scenario suite that covers all tuples of given variable value combinations over the reachable states of the controller. We have implemented our work in the Spectra synthesis en-vironment. We evaluated it over benchmarks from the literature using a mutation approach, specifically tailored for evaluating scenario suites of temporal specifications for reactive synthesis. The evaluation shows that for pairwise coverage, our CC algorithms are feasible and provide a 1.7 factor of improvement in mutation score compared to random scenario generation. We further report on a user study with students who have participated in a work-shop class at our university and have used our tool to validate their specifications. The user study results demonstrate the potential effectiveness of our work in helping engineers detect real bugs in the specifications they write.",
        "keywords": [],
        "authors": [
            "Dor Ma’ayan",
            "Shahar Maoz",
            "Roey Rozi"
        ],
        "file_path": "data/models/models22/main/papers/p132-maayan.pdf"
    },
    {
        "title": "Solving the Instance Model-View Update Problem in AADL",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "The Architecture Analysis and Design Language (AADL) is a rich\nlanguage for modeling embedded systems through several con-\nstructs such as component extension and refinement to promote\nmodularity of component declarations. To ease processing AADL\nmodels, OSATE, the reference tool for AADL, defines another model\n(namely ‘instance’ model) computed from a base ‘declarative’ mod-\nels. An instance model is a simple object tree where all information\nfrom the declarative model is flattened so that tools can easily use\nthis information to analyze the system. However for modifications,\nthey have to make changes in the complex declarative model since\nthere is no automated backward transformation (deinstantiation)\nfrom instance to declarative models. Since the instance model is a\n‘view’ of the declarative model, this is a view-update problem. In\nthis paper, we propose the OSATE Declarative-Instance Mapping\nTool (OSATE-DIM1), an Eclipse plugin for deinstantiation of AADL\nmodels implementing a solution of this view-update problem. We\nevaluate OSATE-DIM with a benchmark of existing AADL model\nprocessing tools and verify the correctness of our deinstantiation\ntransformations. We also discuss how our approach could be use-\nful for decompilation of Object-Oriented languages’ intermediate\nrepresentations.",
        "keywords": [
            "Model-Driven Engineering",
            "Cyber-Physical Systems",
            "Embedded\nSystems",
            "View-Update Problem",
            "AADL"
        ],
        "authors": [
            "Rakshit Mittal",
            "Dominique Blouin",
            "Anish Bhobe",
            "Soumyadip Bandyopadhyay"
        ],
        "file_path": "data/models/models22/main/papers/p55-mittal.pdf"
    },
    {
        "title": "Editing Support for Software Languages: Implementation Practices in Language Server Protocols",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Effectively using software languages, be it programming or domain-specific languages, requires effective editing support. Modern IDEs, modeling tools, and code editors typically provide sophisticated support to create, comprehend, or modify instances—programs or models—of particular languages. Unfortunately, building such edit-ing support is challenging. While the engineering of languages is well understood and supported by modern model-driven tech-niques, there is a lack of engineering principles and best prac-tices for realizing their editing support. Especially domain-specific languages—often created by smaller organizations or individual developers, sometimes even for single projects—would benefit from better methods and tools to create proper editing support. We study practices for implementing editing support in 30 so-called language servers—implementations of the language server protocol (LSP). The latter is a recent de facto standard to realize editing support for languages, separated from the editing tools (e.g., IDEs or modeling tools), enhancing the reusability and quality of the editing support. Witnessing the LSP’s popularity—a whopping 121 language servers are in existence today—we take this opportunity to analyze the implementations of 30 language servers, some of which support multiple languages. We identify concerns that developers need to take into account when developing editing support, and we synthesize implementation practices to address them, based on a systematic analysis of the servers’ source code. We hope that our results shed light on an important technology for software language engineering, that facilitates language-oriented programming and systems development, including model-driven engineering.",
        "keywords": [
            "Language engineering",
            "code assistance",
            "source code editor",
            "implementation practices"
        ],
        "authors": [
            "Djonathan Barros",
            "Sven Peldszus",
            "Wesley K. G. Assunção",
            "Thorsten Berger"
        ],
        "file_path": "data/models/models22/main/papers/p232-barros.pdf"
    },
    {
        "title": "Accelerating Similarity-Based Model Matching Using On-The-Fly Similarity Preserving Hashing",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Similarity-based model matching is the foundation of model versioning. It pairs model elements based on a distance metric (e.g., edit distance). Because it is expensive to calculate the distance between two elements, a similarity-based matcher usually suffers from performance issues when the model size increases. This paper proposes a hash-based approach to accelerate similarity-based model matching. Firstly, we design a novel similarity-preserving hash function that maps a model element to a 64-bit hash value. If two elements are similar, their hashes are also very close. Secondly, we propose a 3-layer index structure and a query algorithm to quickly filter out impossible candidates for the element to be matched based on their hashes. For the remaining candidates, we employ the classical similarity-based matching algorithm to determine the final matches. Our approach has been realized and integrated into EMF Compare. The evaluation results show that our hash function is effective to preserve the similarity between model elements and our matching approach reduces 16%–72% of time costs while assuring the matching results consistent with EMF Compare.",
        "keywords": [
            "model matching",
            "similarity-preserving hashing",
            "distance function",
            "model merging"
        ],
        "authors": [
            "Xiao He",
            "Letian Tang",
            "Yutong Li"
        ],
        "file_path": "data/models/models22/main/papers/p244-he.pdf"
    },
    {
        "title": "A Comprehensive Framework for the Analysis of Automotive Systems",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Analysis models, technologies and tools are extensively used in the automotive domain to validate and optimize the design and implementation of SW systems. This is especially true for modern systems including advanced autonomous (and complex) features. The range of analysis methods that can be applied is extremely wide and goes from functional correctness to functional safety to timing (and schedulability), security, and possibly even more. The AUTOSAR automotive standard has been defined with the purpose of standardizing the SW architecture of automotive systems and enable the construction of systems by composing SW components that are portable and abstract with respect to the underlying HW/SW platform. However, AUTOSAR was originally developed with portability of code in mind, and even if it quickly evolved to include a system-level modeling language (with its metamodel) and later extensions to deal with the needs of analysis methods (and tools), it is hardly comprehensive and still affected by several omissions and limitations. To fix the limitations with respect to timing and schedulability analysis Bosch developed the Amalthea (later App4MC) metamodel and tools. In Huawei, a more general (and ambitious) approach was undertaken to support not only timing analysis, but also model checking (or other types of formal verification), safety analysis and even design optimization. The approach is based on the concepts of a unified (modular) metamodel and a framework based on Eclipse to integrate analysis methods and tools. In this paper we describe the framework and the results obtained with respect to the objectives of functional verification and timing analysis.",
        "keywords": [
            "AUTOSAR SW Systems",
            "Model-Based Development",
            "Timing Analysis",
            "Formal Verification"
        ],
        "authors": [
            "Alessandro Cimatti",
            "Sara Corfini",
            "Luca Cristoforetti",
            "Marco Di Natale",
            "Alberto Griggio",
            "Stefano Puri",
            "Stefano Tonetta"
        ],
        "file_path": "data/models/models22/main/papers/p379-cimatti.pdf"
    },
    {
        "title": "System Architecture Synthesis for Performability by Logic Solvers",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "In model-based systems engineering, system architectures often have to make compromises to meet hard constraints of functional and extra-functional requirements while optimizing for a target objective. Design space exploration (DSE) techniques have been developed to automatically propose candidate architectures over an extremely large design and configuration space. (1) Meta-heuristic exploration algorithms are often used to provide practical, best-effort solutions for DSE, but they lack any guarantees of completeness or optimality. (2) Logic synthesis based approaches may offer strong theoretical guarantees, but frequently face scalability issues. In the paper, we propose two logic solver-based approaches to evaluate complex design spaces by using partial models in order to find an optimal solution with respect to performability objectives. One approach uses performability analysis as a post-filtering of valid system architecture candidates, while the other approach uses performability analysis for guiding the actual search over partial models. We evaluate both approaches on an interferometry mission architecture case study using view transformations for performability analysis and compare our approach with a well-known DSE framework based on meta-heuristic search.",
        "keywords": [
            "performability",
            "design-space exploration",
            "logic solver",
            "model generator",
            "partial models"
        ],
        "authors": [
            "Máté Földiák",
            "Kristóf Marussy",
            "Dániel Varró",
            "István Majzik"
        ],
        "file_path": "data/models/models22/main/papers/p43-foldiak.pdf"
    },
    {
        "title": "Practical Multiverse Debugging through User-defined Reductions: Application to UML Models",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Multiverse debugging is an extension of classical debugging methods, particularly adapted to non-deterministic systems. Recently, a language-independent formalization was proposed. Moreover, multiverse debugging is particularly beneficial for specification and design languages, such as UML. However, this method suffers from scalability issues during breakpoint lookup. This problem arises due to the exhaustive exploration performed on the potentially infinite state-space of the system.\n\nIn this paper, we tackle this problem by introducing Reduced Multiverse Debugging, an extension proposing a way for the user to define reduction policies used during breakpoint lookup. We enrich the formalization of multiverse debugging with a modular breakpoint lookup strategy, which allows the integration of the reduction policy. We validate our approach by implementing a practical UML Statechart debugger in the AnimUML web framework. We show several ways the reduction can be applied, using methods such as predicate abstraction for breakpoint lookup on an infinite state-space, removing irrelevant variables, or creating classes of equivalent values. Moreover, we show the possibility to integrate probabilistic reduction strategies. Relying on hash collisions, these strategies can be iteratively refined to increase precision.",
        "keywords": [
            "multiverse debugging",
            "model analysis",
            "concurrency",
            "abstraction"
        ],
        "authors": [
            "Matthias Pasquier",
            "Ciprian Teodorov",
            "Frédéric Jouault",
            "Matthias Brun",
            "Luka Le Roux",
            "Loïc Lagadec"
        ],
        "file_path": "data/models/models22/main/papers/p87-pasquier.pdf"
    },
    {
        "title": "Towards Model-based Bias Mitigation in Machine Learning",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Models produced by machine learning are not guaranteed to be free from bias, particularly when trained and tested with data produced in discriminatory environments. The bias can be unethical, mainly when the data contains sensitive attributes, such as sex, race, age, etc. Some approaches have contributed to mitigating such biases by providing bias metrics and mitigation algorithms. The challenge is users have to implement their code in general/statistical programming languages, which can be demanding for users with little programming and fairness in machine learning experience. We present FairML, a model-based approach to facilitate bias measure- ment and mitigation with reduced software development effort. Our evaluation shows that FairML requires fewer lines of code to produce comparable measurement values to the ones produced by the baseline code.",
        "keywords": [
            "Model-Driven Engineering",
            "Generative Programming",
            "Bias Mitigation",
            "Bias Metrics",
            "Machine Learning"
        ],
        "authors": [
            "Alfa Yohannis and Dimitris Kolovos"
        ],
        "file_path": "data/models/models22/main/papers/p143-yohannis.pdf"
    },
    {
        "title": "Advanced Visualization and Interaction in GLSP-based Web Modeling: Realizing Semantic Zoom and Off-Screen Elements",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Conceptual modeling is widely adopted in industrial practices, e.g., process, software, and systems modeling. Providing adequate and usable modeling tools is essential for the efficient adoption of modeling. Metamodeling platforms provide a rich set of functionalities and maturely realize state-of-the-art modeling tools. However, despite their maturity and stability, most of these platforms only slowly – if at all – leverage the full extent of functionalities and the ease of exploitation and integration enabled by web technologies. With the Graphical Language Server Protocol (GLSP), it is now possible to realize much richer, advanced opportunities for visualizing and interacting with conceptual models. This paper presents a concept and a prototypical implementation of two advanced model visualization and interaction functionalities with the Eclipse GLSP platform: Semantic Zoom and Off-Screen Elements. We believe such advanced functionalities pave the way for a prosperous modeling future and spark innovation in modeling tool development.",
        "keywords": [
            "Modeling tools",
            "Web Modeling",
            "Language Server Protocol",
            "Visualization"
        ],
        "authors": [
            "Giuliano De Carlo",
            "Philip Langer",
            "Dominik Bork"
        ],
        "file_path": "data/models/models22/main/papers/p221-carlo.pdf"
    },
    {
        "title": "Symboleo2SC: From Legal Contract Specifications to Smart Contracts",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Smart contracts (SCs) are software systems that monitor and control the execution of legal contracts to ensure compliance with the contracts’ terms and conditions. They often exploit Internet-of-Things technologies to support their monitoring functions, and blockchain technology to ensure the integrity of their data. Ethereum and business blockchain platforms, such as Hyperledger Fabric, are popular choices for SC development. However, there is a gap in the knowledge of SCs between developers and legal experts. Symboleo is a formal specification language for legal contracts that was introduced to address this issue. Symboleo specifications directly encode legal concepts such as parties, obligations, and powers. In this paper, we propose a tool-supported method for translating Symboleo specifications into smart contracts. We have extended the current Symboleo IDE, implemented the ontology and semantics of Symboleo into a reusable library, and developed the Sym-boleo2SC tool to generate Hyperledger Fabric code exploiting this library. Symboleo2SC was evaluated with three sample contracts. The results shows that legal contract specifications in Symboleo can be fully converted to SCs for monitoring purposes. Moreover, Symboleo2SC helps simplify the SC development process, saves development effort, and helps reduce risks of coding errors.",
        "keywords": [
            "Smart contracts",
            "code generation",
            "blockchain",
            "domain-specific languages",
            "legal ontology"
        ],
        "authors": [
            "Aidin Rasti",
            "Daniel Amyot",
            "Alireza Parvizimosaed",
            "Marco Roveri",
            "Luigi Logrippo",
            "Amal Ahmed Anda",
            "John Mylopoulos"
        ],
        "file_path": "data/models/models22/main/papers/p300-rasti.pdf"
    },
    {
        "title": "A Domain-Specific Language for Simulation-Based Testing of IoT Edge-to-Cloud Solutions",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "The Internet of things (IoT) is increasingly prevalent in domains such as emergency response, smart cities and autonomous vehicles. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. In this paper, we propose a domain-specific language (DSL) for generating edge-to-cloud simulators. An edge-to-cloud simulator executes the functionality of a large array of edge devices that communicate with cloud applications. Our DSL, named IoTECS, is the result of a collaborative project with an IoT analytics company, Cheetah Networks. The industrial use case that motivates IoTECS is ensuring the scalability of cloud applications by putting them under extreme loads from IoT devices connected to the edge. We implement IoTECS using Xtext and empirically evaluate its usefulness. We further reflect on the lessons learned.",
        "keywords": [
            "Domain-Specific Languages",
            "IoT",
            "Simulation",
            "Stress Testing",
            "Xtext"
        ],
        "authors": [
            "Jia Li",
            "Shiva Nejati",
            "Mehrdad Sabetzadeh",
            "Michael McCallen"
        ],
        "file_path": "data/models/models22/main/papers/p367-li.pdf"
    },
    {
        "title": "Bug Localization in Game Software Engineering: Evolving Simulations to Locate Bugs in Software Models of Video Games",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Video games have characteristics that differentiate their development and maintenance from classic software development and maintenance. These differences have led to the coining of the term Game Software Engineering to name the emerging subfield that intersects Software Engineering and video games. One of these differences is that video game developers perceive more difficulties than other non-game developers when it comes to locating bugs. Our work proposes a novel way to locate bugs in video games by means of evolving simulations. As the baseline, we have chosen BLiMEA, which targets classic software engineering and uses bug reports and the defect localization principle to locate bugs. We also include Random Search as a sanity check in the evaluation. We evaluate the approaches in a commercial video game (Kromaia). The results for F-measure range from 46.80%. to 70.28% for five types of bugs. Our approach improved the results of the baseline by 20.29% in F-measure. To the best of our knowledge, this is the first approach that is designed specifically for bug localization in video games. A focus group with professional video game developers has confirmed the acceptance of our approach. Our approach opens a new research direction for bug localization for both game software engineering and possibly classic software engineering.",
        "keywords": [
            "Bug Localization",
            "Video Games",
            "Search-Based Software Engineering",
            "Model-Driven Engineering"
        ],
        "authors": [
            "Rodrigo Casamayor",
            "Lorena Arcega",
            "Francisca Pérez",
            "Carlos Cetina"
        ],
        "file_path": "data/models/models22/main/papers/p356-casamayor.pdf"
    },
    {
        "title": "A Declarative Modelling Framework for the Deployment and Management of Blockchain Applications",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "The deployment and management of Blockchain applications require non-trivial efforts given the unique characteristics of their infrastructure (i.e., immutability) and the complexity of the software systems being executed. The operation of Blockchain applications is still based on ad-hoc solutions that are error-prone, difficult to maintain and evolve, and do not manage their interactions with other infrastructures (e.g., a Cloud backend). This paper proposes KATENA, a framework for the deployment and management of Blockchain applications. In particular, it focuses on applications that are compatible with Ethereum, a popular general-purpose Blockchain technology. KATENA provides i) a metamodel for defining Blockchain applications, ii) a set of processes to automate the deployment and management of defined models, and iii) an implementation of the approach based on TOSCA, a standard language for Infrastructure-as-Code, and xOpera, a TOSCA-compatible orchestrator. To evaluate the approach, we applied KATENA to model and deploy three real-world Blockchain applications, and showed that our solution reduces the amount of code required for their operations up to 82.7%.",
        "keywords": [
            "blockchain",
            "dApp",
            "decentralized applications",
            "orchestration",
            "devops",
            "infrastructure-as-code",
            "iac",
            "smart contract",
            "ethereum",
            "TOSCA",
            "deployment"
        ],
        "authors": [
            "Luciano Baresi",
            "Giovanni Quattrocchi",
            "Damian Andrew Tamburri",
            "Luca Terracciano"
        ],
        "file_path": "data/models/models22/main/papers/p311-baresi.pdf"
    },
    {
        "title": "Reactive Links Across Multi-Domain Engineering Models",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "As the engineering world moves towards collaborative model-driven development, it is becoming increasingly difficult to keep all model artifacts synchronized and consistent across a myriad of tools and domains. The existing literature proposes a variety of solutions, from passive trace links to computing change propagation paths. However, these solutions require manual propagation and the use of a limited set of tools, while also lacking the efficiency and granularity required during the development of complex systems. To overcome these limitations, this paper proposes a solution based on reactive propagation links between property values across multi-domain models managed in different tools. As opposed to the traditional passive links, the propagation links automatically react to changes during engineering to assure the synchronization and consistency of the models. The feasibility and performance of our solution were evaluated in two practical scenarios. We identified a set of change propagation cases, all of which could be resolved using our solution, while also rendering a great improvement in terms of efficiency as compared to manual propagation. The contribution of our solution to the state of the practice is to enhance the engineering process by reducing the burden of manually keep-ing models synchronized, eliminating inconsistencies that can be originated in artifacts managed in a variety of tool from different domains.",
        "keywords": [],
        "authors": [
            "Cosmina Cristina Rat,iu",
            "Wesley K. G. Assunção",
            "Rainer Haas",
            "Alexander Egyed"
        ],
        "file_path": "data/models/models22/main/papers/p76-tatiu.pdf"
    },
    {
        "title": "The Influence of Software Design Representation on the Design Communication of Teams with Diverse Personalities",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Software is the main driver of added-value in many of the systems that surround us. While its complexity is increasing, so is the diversity of systems driven by software. To meet the challenges emerging from this combination, it is necessary to mobilize increasingly large and heterogeneous multidisciplinary teams, comprising software experts, as well as experts from various domains related to the systems driven by software. Hence, the quality of communication about software between stakeholders of different domains and with different personalities is becoming a key issue for successfully engineering software-intensive systems. The goal of this study, thus, is to investigate the effect of the representation of software design models on the communication of design decisions between stakeholders with diverse personality traits. As a result, this study finds that graphical representations of software design models are better than textual representations in enhancing the communication and increasing the productivity of stakeholders with diverse personalities.",
        "keywords": [
            "Software Engineering",
            "Software Design",
            "Human Aspects",
            "Personality Traits",
            "Communication"
        ],
        "authors": [
            "Rodi Jolak",
            "Maxime Savary-Leblanc",
            "Manuela Dalibor",
            "Juraj Vincur",
            "Regina Hebig",
            "Xavier Le Pallec",
            "Michel Chaudron",
            "Sébastien Gérard",
            "Ivan Polasek",
            "and Andreas Wortmann"
        ],
        "file_path": "data/models/models22/main/papers/p255-jolak.pdf"
    },
    {
        "title": "Predicate Abstractions for Smart Contract Validation",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Smart contracts are immutable programs deployed on the blockchain that can manage significant assets. Because of this, verification and validation of smart contracts is of vital importance. Indeed, it is industrial practice to hire independent specialized companies to audit smart contracts before deployment. Auditors typically rely on a combination of tools and experience but still fail to identify problems in smart contracts before deployment, causing significant losses. In this paper, we propose using predicate abstraction to construct models which can be used by auditors to explore and validate smart contact behaviour at the function call level by proposing predicates that expose different aspects of the contract. We propose predicates based on requires clauses and enum-type state variables as a starting point for contract validation and report on an evaluation on two different benchmarks.",
        "keywords": [],
        "authors": [
            "Javier Godoy",
            "Juan Pablo Galeotti",
            "Diego Garbervetsky",
            "Sebastian Uchitel"
        ],
        "file_path": "data/models/models22/main/papers/p289-godoy.pdf"
    },
    {
        "title": "Precomputing Reconfiguration Strategies based on Stochastic Timed Game Automata",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Many modern software systems continuously reconfigure themselves to (self-)adapt to ever-changing environmental contexts. Selecting presumably best-fitting next configurations is, however, very challenging, depending on functional and non-functional criteria like real-time constraints as well as inherently uncertain future contexts which makes greedy one-step decision heuristics ineffective. In addition, the computational overhead caused by reconfiguration planning at run-time should not outweigh its benefits. On the other hand, completely pre-planning reconfiguration decisions at design time is also infeasible due to the lack of knowledge about the context behavior. In this paper, we propose a game-theoretic setting for precomputing reconfiguration decisions under partially uncertain real-time behavior. We employ stochastic timed game automata as reconfiguration model to derive winning strategies which enable the first player (the system) to make fast look-ups for presumably best-fitting reconfiguration decisions satisfying the second player (the context). To cope with the high computational complexity of finding winning strategies, our tool implementation1 utilizes the statistical model-checker Uppaal Stratego to approximate near-optimal solutions. In our evaluation, we investigate efficiency/effectiveness trade-offs by considering a real-world example consisting of a reconfigurable robot support system for the construction of aircraft fuselages.",
        "keywords": [
            "Stochastic Timed Game Automata",
            "Proactive Self-Adaptation",
            "Strategy Synthesis",
            "Statistical Model-Checking"
        ],
        "authors": [
            "Hendrik Göttmann",
            "Birte Caesar",
            "Lasse Beers",
            "Malte Lochau",
            "Andy Schürr",
            "Alexander Fay"
        ],
        "file_path": "data/models/models22/main/papers/p31-goettmann.pdf"
    },
    {
        "title": "Machine Learning Methods for Model Classification: A Comparative Study",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "In the quest to reuse modeling artifacts, academics and industry have proposed several model repositories over the last decade. Different storage and indexing techniques have been conceived to facilitate searching capabilities to help users find reusable artifacts that might fit the situation at hand. In this respect, machine learning (ML) techniques have been proposed to categorize and group large sets of modeling artifacts automatically. This paper reports the results of a comparative study of different ML classification techniques employed to automatically label models stored in model repositories. We have built a framework to systematically compare different ML models (feed-forward neural networks, graph neural networks, 𝑘−nearest neighbors, support version machines, etc.) with varying model encodings (TF-IDF, word embeddings, graphs and paths). We apply this framework to two datasets of about 5,000 Ecore and 5,000 UML models. We show that specific ML models and encodings perform better than others depending on the characteristics of the available datasets (e.g., the presence of duplicates) and on the goals to be achieved.",
        "keywords": [
            "Model classification",
            "Model-Driven Engineering",
            "Machine learning"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Riccardo Rubei",
            "Jesús Sánchez Cuadrado",
            "Davide di Ruscio"
        ],
        "file_path": "data/models/models22/main/papers/p165-lopez.pdf"
    },
    {
        "title": "Modular Language Product Lines: A Graph Transformation Approach",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Modelling languages are intensively used in paradigms like model-driven engineering to automate all tasks of the development process. These languages may have variants, in which case the need arises to deal with language families rather than with individual languages. However, specifying the syntax and semantics of each language variant separately is costly, hinders reuse across variants, and may yield inconsistent semantics between variants.\nTo attack this problem, we propose a novel, modular way to describe product lines of modelling languages. Our approach is compositional, enabling the incremental definition of language families by means of modules comprising meta-model fragments, graph transformation rules, and rule extensions. Language variants are configured by selecting the desired modules, which entails the composition of a language meta-model and a set of rules defining its semantics. This paper describes a theory able to check consistent semantics among all languages within the family, an implementation as an Eclipse plugin, and an evaluation reporting drastic specification size reduction w.r.t. an enumerative approach.",
        "keywords": [
            "Model-driven engineering",
            "Graph transformation",
            "Product lines",
            "Software language engineering"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra",
            "Paolo Bottoni"
        ],
        "file_path": "data/models/models22/main/papers/p334-lara.pdf"
    },
    {
        "title": "EvoSL: A Large Open-Source Corpus of Changes in Simulink Models & Projects",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Having readily available corpora is crucial for performing replication, reproduction, extension, and verification studies of existing research tools and techniques. MATLAB/Simulink is a de-facto standard tool in several safety-critical industries for system modeling and analysis, compiling models to code, and deploying code to embedded hardware. There is no commonly used corpus for large-scale model change studies because there is no readily available corpus. EvoSL is the first large corpus of Simulink projects that includes model and project changes and allows redistribution. EvoSL is available under a permissive open-source license and contains its collection and analysis tools. Using a subset of EvoSL, we replicated a case study of model changes on a single closed-source industrial project.",
        "keywords": [
            "reproducibility",
            "replication",
            "Simulink",
            "open science",
            "Simulink model changes",
            "corpus",
            "evolution"
        ],
        "authors": [
            "Sohil Lal Shrestha",
            "Alexander Boll",
            "Shafiul Azam Chowdhury",
            "Timo Kehrer",
            "Christoph Csallner"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a273/248000a273.pdf"
    },
    {
        "title": "MODELS 2023",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "This document is the preface to the MODELS 2023 conference. It details the conference's history, location, submission process, acceptance rates, and program highlights.",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z010/248000z010.pdf"
    },
    {
        "title": "Experience in Specializing a Generic Realization Language for SPL Engineering at Airbus",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "In software product line (SPL) engineering, feature models are the de facto standard for modeling variability. A user can derive products out of a base model by selecting features of interest. Doing it automatically, however, requires a realization model, which is a description of how a base model should be modiﬁed when a given feature is selected/unselected. A realization model then necessarily depends on the base metamodel, asking for ad hoc solutions that have ﬂourished in recent years. In this paper, we propose Greal, a generic solution to this problem in the form of (1) a generic declarative realization language that can be automatically composed with one or more base metamodels to yield a domain-speciﬁc realization language and (2) a product derivation algorithm applying a realization model to a base model and a resolved model to yield a derived product. We describe how, on top of Greal, we specialized a realization language to support both positive and negative variability, ﬁt the syntax and semantics of the targeted language (BPMN) and take into account modeling practices at Airbus. We report on lessons learned of applying this approach on Program Development Plans based on business process models and discuss open problems.",
        "keywords": [],
        "authors": [
            "Damien Foures",
            "Mathieu Acher",
            "Olivier Barais",
            "Benoit Combemale",
            "Jean-Marc J´ez´equel",
            "J¨org Kienzle"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a319/248000a319.pdf"
    },
    {
        "title": "Automated Domain Modeling with Large Language Models: A Comparative Study",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Domain modeling is an essential part of software engineering and serves as a way to represent and understand the concepts and relationships in a problem domain. Typically, software engineers interpret the problem description written in natural language and manually translate it into a domain model. Domain modeling can be time-consuming and highly depends on the expertise of software engineers. Recently, Large Language Models (LLMs) have exhibited remarkable ability in language understanding, generation, and reasoning. In this paper, we conduct a comprehensive, comparative study of using LLMs for fully automated domain modeling. We assess two powerful LLMs, GPT3.5 and GPT4, employing various prompt engineering techniques on a data set containing ten diverse domain modeling examples with reference solutions created by modeling experts. Our findings reveal that while LLMs demonstrate impressive domain understanding capabilities, they are still impractical for full automation, with the top-performing LLM achieving F1 scores of 0.76 for class generation, 0.61 for attribute generation, and 0.34 for relationship generation. Moreover, the F1 score is characterized by higher precision and lower recall; thus, domain elements retrieved by LLMs are often reliable, but there are many missing elements. Furthermore, modeling best practices are rarely followed in auto-generated domain models. Our data set and evaluation provide a valuable baseline for future research in automated LLM-based domain modeling.",
        "keywords": [
            "domain modeling",
            "large language models",
            "few-shot learning",
            "chain-of-thought prompting",
            "prompt engineering"
        ],
        "authors": [
            "Kua Chen",
            "Yujing Yang",
            "Boqi Chen",
            "Jos´e Antonio Hern´andez L´opez",
            "Gunter Mussbacher",
            "D´aniel Varr´o"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a162/248000a162.pdf"
    },
    {
        "title": "Automated Grading of Use Cases",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Use cases (UCs) play a crucial role in software engineering courses, with students frequently using them in assignments, projects, and exams. However, as the number of students enrolling in Computer Science and Software Engineering programs continues to rise, manual grading of these models is becoming increasingly time-consuming. While automated grading tools for class diagrams exist, the automation of grading use case models has received limited attention.\nThis paper proposes an approach for automatically grading use cases. To compare a student’s solution to the teacher’s solution our approach uses structural matching, syntactic and semantic word matching, natural language processing for sentence matching, and ﬂattening of use case hierarchies. The grading algorithm is evaluated on three actual undergraduate and graduate assignments that involve modeling a Gas Station ﬁll-up scenario, a Supermarket checkout scenario, as well as the interactions involved in playing the board game Elfenroads. The results show that with some tuning our automatically determined grades lie within 7% difference of the instructor’s manual grades.",
        "keywords": [
            "use cases",
            "automated grading",
            "model comparison"
        ],
        "authors": [
            "Mohsen Hosseinibaghdadabadi",
            "Omar Alam",
            "Nicolas Almerge",
            "Jörg Kienzle"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a106/248000a106.pdf"
    },
    {
        "title": "OCL Rebuilt, From the Ground Up",
        "submission-date": "2023/09",
        "publication-date": "2023/09",
        "abstract": "The Object Constraint Language (OCL) serves the expression of complex conditions and queries over UML-based models in an object-oriented style. We note that OCL’s grounding in object-orientation leads to a number of issues, including subtle inconsistencies and unsafe navigation. To address these issues, we present OCL♯, a new formal foundation for OCL with borrowings from Alloy. We provide OCL♯’s syntax and semantics, prove type safety, and present a prototype implementation.",
        "keywords": [
            "OCL",
            "semantics",
            "relational language",
            "Alloy"
        ],
        "authors": [
            "Friedrich Steimann",
            "Robert Clarisó",
            "Martin Gogolla"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a194/248000a194.pdf"
    },
    {
        "title": "MODELS 2023",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Antonio Cicchetti",
            "Alfonso Pierantonio",
            "Federico Ciccozzi",
            "Thomas Kühne",
            "Gabriele Taentzer",
            "Davide Di Ruscio",
            "Leen Lambers",
            "Hugo Bruneliere",
            "Fiona Polack",
            "Nelly Bencomo",
            "Sebastian Götz",
            "Ivano Malavolta",
            "Judith Michael",
            "Juri Di Rocco",
            "Matthias Tichy",
            "Jan Carlson",
            "Maria Spichkova"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z012/248000z012.pdf"
    },
    {
        "title": "Integrating Testing into the Alloy Model Development Workﬂow",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Software models help improve the reliability of software systems: models can convey requirements, and can analyze design and implementation properties. A key strength of Alloy, a commonly used modeling language, is the Alloy Analyzer toolset. The Analyzer is an automated analysis engine that searches for all valid instances, which are assignments to the sets of the model such that all executed formulas hold, up to a user-provided scope. Unfortunately, despite the Analyzer, writing correct models remains a difﬁcult and error-prone task. To address this, a unit testing framework, AUnit, was created for Alloy. Since then, several traditional imperative testing practices, including mutation testing, fault localization and repair, have been established for Alloy models. Prior work has introduced the feasibility of these approaches and produced command line prototype tools. This paper highlights the effort to translate these research products into the Analyzer, the main model development tool for Alloy, to produce one consolidated integrated development environment that provides robust testing support.",
        "keywords": [
            "Alloy",
            "SAT Solver",
            "Software Testing"
        ],
        "authors": [
            "Allison Sullivan"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a117/248000a117.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Richard Paige",
            "Daniel Varró",
            "Silvia Abrahão",
            "Don Batory",
            "Nelly Bencomo",
            "Jordi Cabot",
            "Marsha Chechik",
            "Juergen Dingel",
            "Alexander Egyed",
            "Jeff Gray",
            "Øystein Haugen",
            "Zhenjiang Hu",
            "Marouane Kessentini",
            "Jörg Kienzle",
            "Thomas Kühne",
            "Vinay Kulkarni",
            "Timothy C. Lethbridge",
            "Shiva Nejati",
            "Kathy Park",
            "Alfonso Pierantonio",
            "Alexander Pretschner",
            "Houari Sahraoui",
            "Wolfram Schulte",
            "Eugene Sirjani",
            "Gabriele Taentzer",
            "Manuel Wimmer",
            "Andrzej Wąsowski",
            "Tao Yue",
            "Juan de Lara"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z014/248000z014.pdf"
    },
    {
        "title": "Word Embeddings for Model-Driven Engineering",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Model-Driven Engineering practitioners have to deal\nwith the construction of modelling evironments by devising meta-\nmodels, grammars, editors, etc. One of the goals of the application\nof Machine Learning to MDE is to use ML algorithms to assist\nthe MDE expert in these tasks. These algorithms cannot directly\nreceive raw models or meta-models as input, but they typically\nhave to be transformed into a numeric representation, i.e., a\nvector. In this context, a common approach is to use pre-trained\nWord Embeddings, which deﬁne mapping functions that associate\nwords to semantic vectors. However, current word embeddings\nare trained with general texts and lack the technical words which\ntypically arise in the modelling domain. To tackle this issue, we\nhave collected a corpus of modelling texts from well-known mod-\nelling venues, and we have trained two types of word embedding\nmodels. The resulting embeddings (named WordE4MDE) are\nspecialised to address ML tasks in the MDE domain. We have\nperformed an extensive evaluation using the Ecore models of\nthe ModelSet dataset and two state-of-the-art word embeddings\n(GloVe and Word2Vec) as baselines. We show that WordE4MDE\noutperforms these two baselines in three meta-modelling tasks,\nnamely meta-model classiﬁcation, meta-model clustering, and\nmeta-model concept recommendation. WordE4MDE embeddings\nare available at https://github.com/models-lab/worde4mde and\ncan be loaded using standard Python libraries for their use in\nML pipelines.",
        "keywords": [
            "Model-Driven Engineering",
            "Machine Learning",
            "Word Embeddings"
        ],
        "authors": [
            "Jos´e Antonio Hern´andez L´opez",
            "Carlos Dur´a",
            "Jes´us S´anchez Cuadrado"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a151/248000a151.pdf"
    },
    {
        "title": "Model-Driven Approach for Knowledge-Based Engineering of Industrial Digital Twins",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Digital twins are heralding a new paradigm in the process and manufacturing industries by providing near real-time decisions for a range of problems. Engineering digital twin solutions is a knowledge and effort intensive activity. Currently, this is not an easily reproducible process. For each type of industry and for each specific plant, the digital twin design and development process has to start all over, and the effort needs to be reinvested. This is not a scalable proposition. To address this, we need a systematic approach that captures and reuses knowledge such that each new problem is solved significantly more efficiently than the previous problems. We propose a knowledge modeling and implementation methodology to systematically model and capture knowledge pertaining to the industrial manufacturing plant domain, problem domain and solution domain, and a knowledge guided process that reasons with this knowledge to provide intelligent decision support in the design and development of digital twin-based solutions for problems in manufacturing industries.",
        "keywords": [
            "Digital Twins",
            "Model Driven Engineering",
            "Knowledge Modeling",
            "Industry 4.0"
        ],
        "authors": [
            "Sushant Vale",
            "Sreedhar Reddy",
            "Sivakumar Subramanian",
            "Subhrojyoti Roy Chowdhury",
            "Sri Harsha Nistala",
            "Anirudh Deodhar",
            "Venkatraman Runkan"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a013/248000a013.pdf"
    },
    {
        "title": "Lessons Learned Building a Tool for Workﬂow+",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "As automotive manufacturers continue to release more advanced autonomous features, the effort required to ensure safety is increasing. This is a result of the growing complexity of automotive systems, and the increased level of safety assurance required for higher levels of autonomy. The Workﬂow+ model-based framework was developed in response to these challenges, to provide a way for safety assurance to be developed rigorously and with automated tool support. In this paper we discuss our experiences and lessons learned while developing Eclipse-based tooling for Workﬂow+ during a collaborative research project with a large automotive OEM.",
        "keywords": [
            "Model-Based Safety Assurance",
            "Change Impact Analysis",
            "Tool Development"
        ],
        "authors": [
            "Nicholas Annable",
            "Thomas Chiang",
            "Mark Lawford",
            "Richard F. Paige",
            "Alan Wassyng"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a140/248000a140.pdf"
    },
    {
        "title": "An Experimental Evaluation of Conformance Testing Techniques in Active Automata Learning",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "Active automata learning is a technique for dynamically learning finite state machine models of black-box systems. Conformance testing is a well-known bottleneck during learning. While multiple conformance testing techniques (CTTs) for Finite State Machines have been proposed, there is a lack of empirical studies that assess the effects of these CTTs during learning. In this work, we compare the performance of eight different CTTs (W, Wp, HSI, H-ADS, H, SPY, SPY-H, I-ADS) while learning 46 models from different communication protocols. Moreover, we propose APFDL as a metric for characterizing the efficiency of automata learning experiments in terms of fault detection capacity. This metric allows identifying CTTs with a lower total cost regarding the number of symbols and resets and a higher rate of state discovery during learning. Our results indicate that while the total cost entailed by CTTs in learning tends to be negligible, we found a significant difference in fault detection rate in learning. Nevertheless, the differences in fault detection rates become negligible when CTTs are applied in randomized mode. These findings reveal the positive role that randomness can have in improving learning efficiency, despite compromising test completeness.",
        "keywords": [],
        "authors": [
            "Bharat Garhewal",
            "Carlos Diego N. Damasceno"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a217/248000a217.pdf"
    },
    {
        "title": "An extended model-based characterization of fine-grained access control for SQL queries",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "In the context of a project in model-driven security that focuses on the development of model-driven techniques for building secure data-centric (web) applications, we extend, in three (inter-related) dimensions, a recently proposed model-based characterization of fine-grained access control (FGAC) authorization for SQL queries. First, we extend the FGAC policies’ underlying data models by considering association-classes. Secondly, we extend the FGAC policies’ security modeling language by considering, as protected resources, the classes and the (explicit and implicit) associations introduced by the association-classes. Thirdly, we extend the clauses that define whether a user is authorized by an FGAC policy to execute a SQL query, to cover the case of queries retrieving information related to the association-classes. To illustrate our extensions and to demonstrate their applicability, we provide examples of authorization decisions for SQL queries with respect to FGAC policies. The artefact comprising the implementation of this model-based characterization and an executable version of the example is available at https://doi.org/10.5281/zenodo.8176237.",
        "keywords": [
            "Model-Driven Security",
            "Fine-Grained Access Control",
            "Secure Database Queries",
            "SecureUML"
        ],
        "authors": [
            "Hoang Nguyen",
            "Manuel Clavel"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a095/248000a095.pdf"
    },
    {
        "title": "Manual Abstraction in the Wild: A Multiple-Case Study on OSS Systems’ Class Diagrams and Implementations",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Models are a useful tool for software design, analysis, and to support the onboarding of new maintainers. However, these beneﬁts are often lost over time, as the system implementation evolves and the original models are not updated. Reverse engineering methods and tools could help to keep models and implementation code in sync; however, automatically reverse-engineered models are typically not abstract and contain extensive information that prevents understanding. Recent advances in AI-based content generation make it likely that we will soon see reverse engineering tools with support for human-grade abstraction. To inform the design and validation of such tools, we need a principled understanding of what manual abstraction is, a question that has received little attention in the literature so far. Towards this goal, in this paper, we present a multiple-case study of model-to-code differences, investigating ﬁve substantial open-source software projects retrieved via repository mining. To explore characteristics of model-to-code differences, we, all in all, manually matched 466 classes, 1352 attributes, and 2634 operations from source code to 338 model elements (classes, attributes, operations, and relationships). These mappings precisely capture the differences between a provided class diagram design and implementation codebase. Studying all differences in detail allowed us to derive a taxonomy of difference types and to provide a sorted list of cases corresponding to the identiﬁed types of differences. As we discuss, our contributions pave the way for improved reverse engineering methods and tools, new mapping rules for model-to-code consistency checks, and guidelines for avoiding over-abstraction and over-speciﬁcation during design.",
        "keywords": [
            "software design",
            "modeling"
        ],
        "authors": [
            "Wenli Zhang",
            "Weixing Zhang",
            "Daniel Str¨uber",
            "Regina Hebig"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a036/248000a036.pdf"
    },
    {
        "title": "Mutation Testing for Temporal Alloy Models",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Writing declarative models has numerous beneﬁts, ranging from automated reasoning and correction of design-level properties before systems are built, to automated testing and debugging of their implementations after they are built. Alloy is a declarative modeling language that is well-suited for verifying system designs. A key strength of Alloy is its scenario-ﬁnding toolset, the Analyzer, which allows users to explore all valid scenarios that adhere to the model’s constraints up to a user-provided scope. Despite the Analyzer, writing correct Alloy models remains a difﬁcult task, partly due to Alloy’s expressive operators, which allow for succinct formulations of complex properties but can be difﬁcult to reason over manually. To further add to the complexity, Alloy’s grammar was recently expanded to support linear temporal logic, increasing both the expressibility of Alloy as well as the burden for accurately expressing properties. To address this, this paper presents μAlloyτ, an extension to Alloy’s mutation testing framework that accounts for the newly introduced temporal logic, including updating μAlloyτ’s test generation capability to produce temporal test cases. Experimental results reveal μAlloyτ is efﬁcient at generating and checking mutations and μAlloyτ’s automatically generated tests are effective at detecting faulty temporal models.",
        "keywords": [
            "Alloy",
            "Mutation Testing",
            "Test Generation"
        ],
        "authors": [
            "Ana Jovanovic",
            "Allison Sullivan"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a228/248000a228.pdf"
    },
    {
        "title": "SkeMo: Sketch Modeling for Real-Time Model Component Generation",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Software modeling is a powerful tool in the design and implementation of high-quality software systems. Models can be used from high-level design to formal code generation, with various applications in between. Often, software models are initially created informally, by sketching on a whiteboard or paper during the early design phase of the system, and eventually converted into formal models using advanced modeling tools. The formalization of sketches into actual model components can be time-consuming, error-prone, and laborious. To address these shortcomings, we present SkeMo, an environment for real-time model component generation from sketch-based inputs. We curated a sketch dataset of 3000 images of various class diagram components and implemented a powerful Convolution Neural Network to classify input sketches as model components. We integrated our sketch classiﬁer into an existing web-based model editor and added a touch interface to support sketch-based modeling. We evaluated the SkeMo environment in two ways: through ten-fold cross-validation of the image classiﬁer and collection of metrics and feedback from a 20-participant user study. Based on our results, sketch-based modeling demonstrates signiﬁcant promise as an intuitive interface that is both easy to use and allows for faster model creation among most users.",
        "keywords": [
            "model-driven software engineering",
            "machine learning",
            "deep neural network",
            "convolution neural network",
            "image recognition",
            "sketch recognition",
            "class diagrams",
            "classiﬁers",
            "interface design",
            "touch interface",
            "collaborative modeling",
            "assistive modeling",
            "user studies"
        ],
        "authors": [
            "Alisha Sharma Chapai and Eric J. Rapos"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a173/248000a173.pdf"
    },
    {
        "title": "Digital Twins for Cyber-Biophysical Systems: Challenges and Lessons Learned",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Digital twinning is gaining popularity in domains outside of traditional engineered systems, including cyber-physical systems (CPS) with biological modalities, or cyber-biophysical systems (CBPS) in short. While digital twinning has well-established practices in CPS settings, it raises special challenges in the context of CBPS. In this paper, we identify such challenges and lessons learned through an industry case of a digital twin for CBPS in controlled environment agriculture.",
        "keywords": [
            "controlled environment agriculture",
            "industry",
            "model-driven",
            "report",
            "simulation"
        ],
        "authors": [
            "Istvan David",
            "Pascal Archambault",
            "Quentin Wolak",
            "Cong Vinh Vu",
            "Timoth´e Lalonde",
            "Kashif Riaz",
            "Eugene Syriani",
            "Houari Sahraoui"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a001/248000a001.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z004/248000z004.pdf"
    },
    {
        "title": "Rapid-Prototyping and Early Validation of Software Models through Uniform Integration of Hardware",
        "submission-date": "2023/09",
        "publication-date": "2023/09",
        "abstract": "Model-driven software engineering (MDSE) uses software models to make the complexity of cyber-physical and mechatronic systems (CPMS) manageable. For the validation of CPMS software models, closed-loop simulations are typically used. Since the system’s environment is part of the simulation, the software model is directly affected by the surroundings, which enables a more realistic evaluation. In early development phases, the expected target hardware platform for these software models is usually not considered, although such CPMS have a strong hardware dependency. This paper outlines a novel approach to couple these software models with hardware systems to improve the quality of these models and shorten the development cycle. The presented method allows the evaluation of functional and non-functional requirements. For this, a new in-the-loop concept is introduced where the hardware access is transparently performed using a remote procedure call mechanism. Moreover, the achieved modeling language and tool independence makes the novel approach suitable for various applications. The provided evaluation is based on two distinct modeling languages and tools to demonstrate the feasibility and performance of the new concept.",
        "keywords": [
            "model-driven software engineering",
            "model-in-the-loop",
            "cyber-physical systems",
            "remote procedure call"
        ],
        "authors": [
            "Michael Uelschen\nMarco Schaarschmidt\nJannis Budde"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a250/248000a250.pdf"
    },
    {
        "title": "Proceedings ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems MODELS 2023",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z003/248000z003.pdf"
    },
    {
        "title": "184\n2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS)",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a184/248000a184.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Acher",
            "Mathieu\nAlam",
            "Omar\nAlmerge",
            "Nicolas\nAnnable",
            "Nicholas\nArchambault",
            "Pascal\nAtlee",
            "Joanne M.\nBagherzadeh",
            "Mojtaba\nBarais",
            "Olivier\nBarat",
            "Souvik\nBarkowsky",
            "Matthias\nBeermann",
            "Laura\nBenzarti",
            "Imen\nBerger",
            "Bernhard J.\nBoll",
            "Alexander\nBork",
            "Dominik\nBudde",
            "Jannis\nCabot",
            "Jordi\nChen",
            "Boqi\nChen",
            "Kua\nChen",
            "Xiang\nChiang",
            "Thomas\nChowdhury",
            "Shafiul Azam\nChrszon",
            "Philipp\nClarisó",
            "Robert\nClavel",
            "Manuel\nCombemale",
            "Benoit\nCsallner",
            "Christoph\nDarif",
            "Ikram\nDavid",
            "Istvan\nde Lara",
            "Juan\nDíez",
            "Pablo\nDingel",
            "Juergen\nDurá",
            "Carlos\nDutta",
            "Jaya\nEl Boussaidi",
            "Ghizlane\nFelderer",
            "Michael\nFischer",
            "Philipp M.\nFoures",
            "Damien\nGarhewal",
            "Bharat\nGerndt",
            "Andreas\nGiese",
            "Holger\nGogolla",
            "Martin\nGuerra",
            "Esther\nHamann",
            "Arne\nHebig",
            "Regina\nHeldal",
            "Rogardt\nHendriks",
            "Dennis\nHernández López",
            "José Antonio\nHosseinibaghdadabadi",
            "Mohsen\nIovino",
            "Ludovico\nJézéquel",
            "Jean-Marc\nJongeling",
            "Robbert\nJovanovic",
            "Ana\nKahani",
            "Nafiseh\nKehrer",
            "Timo\nKienzle",
            "Jörg\nKotte",
            "Oliver\nKpodjedo",
            "Sègla\nKulkarni",
            "Vinay\nLalonde",
            "Timothé\nLawford",
            "Mark\nLima",
            "Keila\nMartínez-Lasaca",
            "Francisco\nMaurer",
            "Paulina\nMussbacher",
            "Gunter\nN. Damasceno",
            "Carlos Diego\nNguyễn",
            "Hoàng\nNistala",
            "Sri Harsha\nOortwijn",
            "Wytse"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a331/248000a331.pdf"
    },
    {
        "title": "Leveraging modeling concepts and techniques to address challenges in network management",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Managing a large enterprise network is a challenging task that involves configuring and monitoring a large number of networking devices from different vendors. To simplify network management, modeling techniques have been extensively applied to model network configurations and monitoring data. The most recent proposed solution in this context are OpenConﬁg models, which enable vendor-neutral automation. However, adopting networking models requires significant effort and cooperation from various stakeholders. The focus of this paper is to explore the challenges associated with adopting networking models, specifically OpenConﬁg models, from three primary viewpoints: network engineers, internet service/content providers, and networking software/hardware vendors. We also discuss possible solutions via application of software modeling techniques to aid in the successful adoption of networking models.",
        "keywords": [
            "OpenConﬁg",
            "YANG",
            "Model-based Network Management",
            "NETCONF",
            "gNMI"
        ],
        "authors": [
            "Naﬁseh Kahani",
            "Mojtaba Bagherzadeh",
            "Reza Ahmadi",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a055/248000a055.pdf"
    },
    {
        "title": "Digital Twins for Cyber-Biophysical Systems: Challenges and Lessons Learned",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Istvan David",
            "Pascal Archambault",
            "Quentin Wolak",
            "Cong Vinh Vu",
            "Timothé Lalonde",
            "Kashif Riaz",
            "Eugene Syriani",
            "Houari Sahraoui"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z005/248000z005.pdf"
    },
    {
        "title": "Marine Data Observability using KPIs: An MDSE Approach",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "The 2023 climate change report states that the current temperature rise has led to recurring and hazardous weather events, devastating communities and the planet. Ocean observation systems and marine data generated by them are crucial for predicting these extreme events, understanding the ecosystem states, and regulating marine industries. Many regional and global initiatives have been supporting the collection and sharing of more data, filling gaps in ocean observation. However, some challenges can impact the quality of marine data at different points of data delivery pipelines: from acquisition and transmission at the Internet-of-Underwater-Things (IoUT) level up to storage and sharing. IoUT devices can have challenges due to limited battery, rough underwater terrain, error-prone wireless underwater communication, or low communication bandwidth to the cloud. Thus, mechanisms must be put in place to allow monitoring of data quality throughout the delivery pipeline, to optimize the usage of data and improve decision-making based on the data. This study explores observation of marine data quality on a data platform using Key Performance Indicators (KPIs). We have created a model of the platform and specified KPIs. Both are fulfilled by platform-collected data quality metrics, with the purpose to infer the state of the data in the platform over different periods. Our results show that the model-based implementation is able to function as a semantic translator between a metric monitoring toolkit and the platform objectives, integrating it into an observable subsystem for the overall middleware data platform.",
        "keywords": [
            "data observability",
            "data quality",
            "smart ocean",
            "MDSE"
        ],
        "authors": [
            "Keila Lima",
            "Ludovico Iovino",
            "Maria Teresa Rossi",
            "Rogardt Heldal",
            "Tosin Daniel Oyetoyan",
            "Martina De Sanctis"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a024/248000a024.pdf"
    },
    {
        "title": "Model Sensemaking Strategies: Exploiting Meta-Model Patterns to Understand Large Models",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "The increasing popularity of model-based and low-code platforms has raised the need to understand large models – especially in industrial settings. However, current approaches mainly rely on graph-based visual metaphors, which do not scale well with large model sizes. To address this issue, we introduce model sensemaking strategies: purposeful model visualisations based on alternative visual metaphors. We define them as reusable patterns that yield tailored visualisations when applied to meta-models. This paper presents a catalogue of domain-specific and domain-agnostic sensemaking strategies, and a recommender that suggests suitable strategies for a given meta-model. To showcase the framework’s applicability, we have implemented some of these strategies in Dandelion, an industrial, low-code graphical language workbench for the cloud. Using this platform, we have evaluated the effectiveness of the strategies to visualise large industrial models by the UGROUND company.",
        "keywords": [
            "model sensemaking strategies",
            "large model visualisation",
            "model-driven engineering",
            "low-code platforms"
        ],
        "authors": [
            "Francisco Mart´ınez-Lasaca",
            "Pablo D´ıez",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a261/248000a261.pdf"
    },
    {
        "title": "Incremental Model Transformations with Triple Graph Grammars for Multi-version Models",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Like conventional software projects, projects in model-driven software engineering require adequate management of multiple versions of development artifacts, importantly allowing living with temporary inconsistencies. In previous work, we have introduced multi-version models for model-driven software engineering, which allow checking well-formedness and finding merge conflicts for multiple versions of a model at once. However, also for multi-version models, situations where different artifacts, that is, different models, are linked via automatic model transformations have to be handled.\n\nIn this paper, we propose a technique for jointly handling the transformation of multiple versions of a source model into corresponding versions of a target model, which enables the use of a more compact representation that may afford improved execution time of both the transformation and further analysis operations. Our approach is based on the well-known formalism of triple graph grammars and the aforementioned encoding of model version histories called multi-version models. In addition to batch transformation of an entire model version history, the technique also covers incremental synchronization of changes in the framework of multi-version models.\n\nWe show the correctness of our approach with respect to the standard semantics of triple graph grammars and conduct an empirical evaluation to investigate the performance of our technique regarding execution time and memory consumption. Our results indicate that the proposed technique affords lower memory consumption and may improve execution time for batch transformation of large version histories, but can also come with computational overhead in unfavorable cases.",
        "keywords": [
            "Multi-version Models",
            "Triple Graph Grammars",
            "Incremental Model Transformation"
        ],
        "authors": [
            "Matthias Barkowsky",
            "Holger Giese"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a296/248000a296.pdf"
    },
    {
        "title": "Model-Driven Prompt Engineering",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Generative artificial intelligence (AI) systems are capable of synthesizing complex content such as text, source code or images according to the instructions described in a natural language prompt. The quality of the output depends on crafting a suitable prompt. This has given rise to prompt engineering, the process of designing natural language prompts to best take advantage of the capabilities of generative AI systems.\nThrough experimentation, the creative and research communities have created guidelines and strategies for creating good prompts. However, even for the same task, these best practices vary depending on the particular system receiving the prompt. Moreover, some systems offer additional features using a custom platform-speciﬁc syntax, e.g., assigning a degree of relevance to speciﬁc concepts within the prompt.\nIn this paper, we propose applying model-driven engineering to support the prompt engineering process. Using a domain-speciﬁc language (DSL), we deﬁne platform-independent prompts that can later be adapted to provide good quality outputs in a target AI system. The DSL also facilitates managing prompts by providing mechanisms for prompt versioning and prompt chaining. Tool support is available thanks to a Langium-based Visual Studio Code plugin.",
        "keywords": [
            "prompt engineering",
            "model-driven engineering",
            "domain-speciﬁc language",
            "generative AI",
            "large language models"
        ],
        "authors": [
            "Robert Clarisó\nJordi Cabot"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a047/248000a047.pdf"
    },
    {
        "title": "Automatic Security-Flaw Detection\nReplication and Comparison",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Threat Modeling is an essential step in secure\nsoftware system development. It is a manual, attacker-centric\napproach for identifying architecture-level security flaws during\nthe planning phase of software systems. In the last years,\nacademia presented two methods to automate threat detection\nthat do not focus on a particular class of security flaws but offer\ngeneral-purpose means to describe security flaws.\nThis paper compares both approaches on an equal data\nfoundation that was published with one of the approaches. There-\nfore, we specify a model-to-model transformation for converting\nbetween the approaches to allow this conceptual replication.\nAdditionally, we provide security flaw patterns for the second\napproach that any user of the approach can use. We then\nreplicate the detection with the second security flaw detection\napproach to compare both approaches. We focus our analysis\non differences between automation-specific and approach-specific\nfinding misclassifications on identifying whether some flaws are\nharder to find with an automated approach than others.\nWe find that missed flaws usually stem from the imprecise\ndefinition of security flaws, while incorrectly identified flaws\nare approach-dependent. Despite that, both approaches perform\nsimilarly. The knowledge base, the transformation scripts and the\nevaluation script are publicly available to support the research\ncommunity.",
        "keywords": [
            "threat modeling",
            "dataflow diagrams",
            "security\nflaw detection",
            "automation",
            "interoperability",
            "comparison"
        ],
        "authors": [
            "Bernhard J. Berger\nChristina Plump"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a084/248000a084.pdf"
    },
    {
        "title": "A Model-driven and Template-based Approach for Requirements Speciﬁcation",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Requirements speciﬁcation and veriﬁcation play an important role in the certiﬁcation of safety-critical software (SCS). These activities are costly and error-prone because SCS exhibit a high number of requirements and most SCS manufacturers are still using natural language to specify these requirements. On one hand, natural language can introduce ambiguity and inconsistency. On the other hand, formal languages add an overhead to the requirements speciﬁcation because of their complexity. Controlled Natural Languages (CNLs) ﬁll these gaps by offering a middle-ground solution, although not yet well adopted by the industry. In this paper, we introduce an approach that combines CNLs and model-driven engineering (MDE) for requirements speciﬁcation. The approach was proposed to support an industrial partner in the certiﬁcation process of a SCS. Our approach uses templates and relies on two types of models: models that specify the templates, and a model of the domain of the system at hand. Using models of the templates enables to automate some requirements analysis tasks. Using a domain model allows the auto-completion and veriﬁcation of requirements speciﬁed using the templates. We implemented the approach and validated it using three case studies and more than a thousand requirements. We observed that our approach and underlying templates are applicable across domains and that the templates yield requirements with better quality in terms of necessity, ambiguity, completeness, singularity, and veriﬁability.",
        "keywords": [
            "Model-driven engineering",
            "Requirements engineering",
            "Requirements speciﬁcation",
            "Controlled natural language",
            "Requirement templates",
            "Safety critical systems",
            "Domain models"
        ],
        "authors": [
            "Ikram Darif",
            "Cristiano Politowski",
            "Ghizlane El Boussaidi",
            "Imen Benzarti and S`egla Kpodjedo"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a239/248000a239.pdf"
    },
    {
        "title": "Proceedings ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems MODELS 2023",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z001/248000z001.pdf"
    },
    {
        "title": "gLTSdiff: A Generalized Framework for Structural Comparison of Software Behavior",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "State machine models, such as labeled transition systems and (extended) finite automata, can be structurally compared, for instance to find potential behavioral regressions in new software versions, to evaluate the accuracy of different model learning algorithms, and to fingerprint software for security applications. The state-of-the-art LTSDiff structural comparison algorithm has limited assumptions, making it broadly applicable. However, representation-specific information is not taken into account, requiring adaptations to prevent sub-optimal or even invalid results. We propose gLTSdiff, which generalizes and extends LTSDiff, allowing a wide range of state machine models to be compared, by recursively comparing the structure of state and transition labels. Additional challenges that we faced while applying LTSDiff in industrial practice are also addressed by gLTSdiff, as it rewrites undesired difference patterns, supports comparison of any number of input models, and allows for an effort/quality trade-off. gLTSdiff is implemented as an extensible open source library for structural model comparison. Using multiple large-scale industrial and open source case studies, we evaluate both its practical value and its various improvements.",
        "keywords": [],
        "authors": [
            "Dennis Hendriks",
            "Wytse Oortwijn"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a285/248000a285.pdf"
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z017/248000z017.pdf"
    },
    {
        "title": "On Developing and Operating GLSP-based Web Modeling Tools: Lessons Learned from BIGUML",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "The development of web-based modeling tools still poses significant challenges for developers. The Graphical Language Server Platform (GLSP) reduced some of these challenges by providing the necessary frameworks to efficiently create web modeling tools. However, more knowledge and experience are required regarding developing GLSP-based web modeling tools. This paper discusses the challenges and lessons learned after working with GLSP and realizing several GLSP-based modeling tools. More concretely, experiences, concepts, steps to be followed to develop and operate a GLSP-based web modeling tool, and the advantages and disadvantages of working with GLSP are discussed. As a proof of concept, we will report on the realization of a GLSP-based UML editor called BIGUML. Through BIGUML, we show that our procedure and the reference architecture we developed resulted in a scalable and flexible GLSP-based web modeling tool. The lessons learned, the procedural approach, the reference architecture, and the critical reflection on the challenges and opportunities of using GLSP provide valuable insights to the community and shall ease the decision of whether or not to use GLSP for future tool development projects.",
        "keywords": [
            "Modeling tool",
            "GLSP",
            "web modeling",
            "lessons learned",
            "LSP",
            "eclipse"
        ],
        "authors": [
            "Haydar Metin",
            "Dominik Bork"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a129/248000a129.pdf"
    },
    {
        "title": "Variability-aware Neo4j for Analyzing a Graphical Model of a Software Product Line",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Comprehensive analysis of a software product line (SPL) is expensive because the number of products to be analyzed is exponential in the number of the SPL’s features. To compensate, we analyze a model of the SPL rather than the source code, thereby reducing the size of the artifact under analysis. In this paper, we facilitate SPL analysis by lifting the Neo4j query engine to apply to an SPL model, so that a Neo4j query returns variability-aware results that cover all the SPL’s products. We used the lifted Neo4j to analyze five nontrivial SPLs (with respect to dataflows, control-flows, component interactions, etc.) and found that the overhead for returning variability-aware results for the full SPL, versus the results for just one product, ranges from 1.88% to 456%. In comparison to related work V-Souffl´e (a lifted Datalog engine), lifted Neo4j is able to report complete path results whereas V-Souffl´e reports only endpoints of paths. When both analyzers report the same results (e.g., endpoints of paths), lifted Neo4j is usually more efficient.",
        "keywords": [
            "Graphical software models",
            "Software product line models",
            "Lifted analyses",
            "Neo4j"
        ],
        "authors": [
            "Xiang Chen",
            "David R. Cheriton",
            "Joanne M. Atlee"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a307/248000a307.pdf"
    },
    {
        "title": "Timing-Aware Software-in-the-Loop Simulation of Automotive Applications with FMI 3.0",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "In embedded real-time automotive systems, Software-in-the-Loop (SiL) represents the state-of-the-art for testing software code at design time. SiL environments focus on testing the functional software behaviour, typically neglecting the timing non-idealities introduced by the target embedded hardware. This separation of concerns prevents a credible virtual testing and validation of time-critical systems. In this paper, we propose an industry-viable modular co-simulation architecture based on the Functional Mock-up Interface (FMI) 3.0 standard, coupling timing simulation and functional simulations to obtain a timing-aware functional simulation of automotive applications. The proposed method enables an evaluation of the behaviour of functional software on the target hardware earlier in the development process. Also, our solution allows for the co-simulation of submodels generated with different tools, with minimal modifications required. Ultimately, this approach enables front-loading of development efforts, leading to reduced costs and time to market. A case study is presented to show a detailed examination of the proposed architecture.",
        "keywords": [
            "Software-in-the-Loop",
            "timing-aware simulation",
            "Functional Mock-up Interface",
            "Discrete-Event Co-Simulation"
        ],
        "authors": [
            "Srivathsan Ravi",
            "Laura Beermann",
            "Oliver Kotte",
            "Paolo Pazzaglia",
            "Mythreya Vinnakota",
            "Dirk Ziegenbein",
            "Arne Hamann"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a062/248000a062.pdf"
    },
    {
        "title": "Applicability of Model Checking for Verifying Spacecraft Operational Designs",
        "submission-date": "2023/03",
        "publication-date": "2023/03",
        "abstract": "Guaranteeing safety and correctness is one of the main objectives during the development of space systems. This is a challenging task, since many different engineering disciplines are involved in the development and the constituent parts of a spacecraft are highly interconnected and interdependent. Increasingly, formal methods such as model checking are applied to verify safety-critical parts of spacecraft designs and also implementation, since they may prove the absence of design errors. Generally, a major challenge for adopting model checking into the design process is its scalability. Usually, the whole state space of a system, which grows exponentially with, e.g., the number of parallel processes, must be explored.\nIn this paper, we consider operational designs of spacecraft as they may occur during early development phases and systematically evaluate the scalability of model checking for verifying such models. For this, we created an arbitrarily scalable operational design describing the mode management of a satellite. Transformations of the models into the modeling languages of different model-checking tools enables a comparative scalability study of various model-checking algorithms. The evaluation shows promising results for symbolic model-checking approaches. A comparatively low analysis time and memory usage suggest that model checking for early operational designs can be incorporated into existing design processes.",
        "keywords": [
            "Aerospace",
            "Formal Models",
            "Formal Methods",
            "Model Checking"
        ],
        "authors": [
            "Philipp Chrszon",
            "Paulina Maurer",
            "George Saleip",
            "Sascha M¨uller",
            "Philipp M. Fischer",
            "Andreas Gerndt",
            "Michael Felderer"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a206/248000a206.pdf"
    },
    {
        "title": "Uncertainty-aware consistency checking in industrial settings",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "In this work, we explore how we can assist engineers in managing, in a lightweight way, both consistency and design uncertainty during the creation and maintenance of models and other development artifacts. We propose annotating degrees of doubt to indicate design uncertainties on elements of development artifacts. To combine multiple opinions, we use the fusion operators of subjective logic. We show how these annotations can be used to identify, prioritize, and resolve uncertainty and inconsistency. To do so, we identify the types of design uncertainty and inconsistency to be addressed in two concrete industrial settings and show a prototype implementation of our approach to calculating the uncertainty and inconsistency in these cases. We show how making design uncertainty explicit could be used to tolerate inconsistencies with high uncertainty, prioritize inconsistencies with low associated uncertainty, and uncover previously hidden potential inconsistencies.",
        "keywords": [
            "Uncertainty",
            "Consistency management",
            "Model-Based Development"
        ],
        "authors": [
            "Robbert Jongeling",
            "Antonio Vallecillo"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a073/248000a073.pdf"
    },
    {
        "title": "MODELS 2023",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Mohammed Rizwan Ali",
            "Oussama Ben Sghaier",
            "Paul Bittner",
            "Beatriz Cabrero-Daniel",
            "Robert Clarisó",
            "Renzo Degiovanni",
            "Khanh-Hoang Doan",
            "Flo Drux",
            "Sebastian Ehmes",
            "Josselin Enet",
            "Lars Fritsche",
            "Sandra Greiner",
            "Hendrik Göttmann",
            "Liping Han",
            "Alexander Hellwig",
            "José Antonio Hernández López",
            "Matthieu Jimenez",
            "Aton Kamanda",
            "Hendrik Kausch",
            "Faezeh Khorram",
            "Yves Kirschner",
            "Max Kratz",
            "Tim Kräuter",
            "Lars König",
            "Louis-Edouard Lafontant",
            "Sami Lazreg",
            "Alexander Lieb",
            "Lukas Netz",
            "Bentley Oakes",
            "Mathias Pfeiffer",
            "Sreedhar Reddy",
            "Lucas Sakizloglou",
            "Alexander Schultheiss",
            "Mazyar Seraj",
            "Karsten Sohr",
            "Max Stachon"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z015/248000z015.pdf"
    },
    {
        "title": "EditQL: A Textual Query Language for Evolving Models",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Context: Technically sophisticated systems are the result of the\njoint work of several domain experts. However, the more people\ncollaborate, the more important it becomes to make the model evo-\nlution and its single edit operations accessible and comprehensible\nfor involved stakeholders. Objective: We developed the textual and\nsemantic aware query language EditQL. It enables domain experts\nto search for model versions, changes, and causing edit operations\nwithin a model’s edit history. Based on an operation-based ver-\nsioning system, the query language covers both edit operations\nand all model states. Method: We systematically elaborate the re-\nquirements of a query language for edit histories. Based on this,\nwe present a DSL integrated into an existing modeling tool. We\nconducted a mixed-methods usability study with 15 participants\nin which they had to answer various questions about a model’s\nevolution using EditQL. Results: All participants agreed on the\nusefulness of the query language, particularly the possibility of\nquerying for semantic changes in the model. The measured System\nUsability Scale (SUS) scores range from OK to good. In addition,\nwe identified a set of possible improvements. Conclusion: The study\nconfirmed that EditQL and the underlying concepts are suitable\ntools to help domain experts understand the evolution of a model.",
        "keywords": [
            "versioning",
            "operation-based",
            "query language",
            "model evolution",
            "usability study",
            "collaboration"
        ],
        "authors": [
            "Jakob Pietron",
            "Benedikt Jutz",
            "Alexander Raschke",
            "Matthias Tichy"
        ],
        "file_path": "data/models/models24/3640310.3674101.pdf"
    },
    {
        "title": "Towards Automated Test Scenario Generation for Assuring COLREGs Compliance of Autonomous Surface Vehicles",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "International maritime traffic is controlled by collision-avoidance regulations (COLREGs) with 41 standardized rules describing how a vessel should navigate in the proximity of other vessels. Since some rules can be overridden by human judgement when resolving critical encounters of vessels, justifying COLREGs compliance has become a significant challenge in the increasing presence of autonomous surface vehicles (ASVs) operated without (or with only remote) human control. This paper provides a high-level framework and long-term research agenda towards the automated synthesis of test scenarios to assure COLREGs compliance for ASVs by exploiting various model-driven engineering techniques. By adapting ideas from testing self-driving cars, we envisage a multi-layered test scenario generation approach involving functional, logical and concrete scenarios. In the current paper, we demonstrate how functional scenarios of COLREGs situations between given vessels can be precisely formalized by using metamodels, domain-specific graph models and first-order logic graph constraints. By using automated model generation techniques, we derive a complete set of functional-level test scenarios, which includes all possible COLREGs situations that may arise between given vessels. As initial result, we provide several dangerous situations involving only three vessels where a potential collision may occur even when all vessels follow the COLREGs, which showcases that some COLREGs rules need further clarification for the safe regulation of ASVs.",
        "keywords": [
            "autonomous surface vehicles",
            "COLREGs",
            "test scenario generation",
            "consistent model generation",
            "qualitative abstraction"
        ],
        "authors": [
            "Ulf Kargén",
            "Dániel Varró"
        ],
        "file_path": "data/models/models24/3640310.3674098.pdf"
    },
    {
        "title": "Modeling Languages for Automotive Digital Twins: A Survey Among the German Automotive Industry",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "The demand for digital twins and suitable modeling techniques in the automotive industry is increasing rapidly. Yet, there is no common understanding of digital twins in automotive, nor are there modeling techniques established to create automotive digital twins. Recent studies on digital twins focus on the analysis of the literature on digital twins for automotive or in general and, thus, neglect the industrial perspective of automotive practitioners. To mitigate this gap between scientific literature and the industrial perspective, we conducted a questionnaire survey among experts in the German automotive industry to identify i) the desired purposes for and capabilities of digital twins and ii) the modeling techniques related to engineering and operating digital twins across the phases of automotive development. To this end, we contacted 189 members of the Software-Defined Car research project and received 96 responses. The results show that digital twins are considered most useful in the usage and support phase of automotive development, representing vehicles as-operated. Moreover, simulation models, source code, and business process models are currently considered the most important models to be integrated into a digital twin alongside the associated, established tools.",
        "keywords": [
            "modeling languages",
            "digital twins",
            "automotive",
            "survey"
        ],
        "authors": [
            "Dominik Fuchß",
            "Thomas Kühn",
            "Dirk Neumann",
            "Christer Neimöck",
            "Jérôme Pfeiffer",
            "Robin Liebhart",
            "Christian Seiler",
            "Anne Koziolek",
            "Andreas Wortmann"
        ],
        "file_path": "data/models/models24/3640310.3674100.pdf"
    },
    {
        "title": "Meta-Modelling Kindness",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Kindness is a psycho-social phenomenon that is also recognized as\nan important pro-social behaviour. The use of digital technology\nprovides opportunities to promote kindness in various ways, such\nas in social media campaigns and online communities. In princi-\npale, software engineers are well positioned to develop automated\nsystems that can facilitate software-mediated kindness. However,\nin practice, incorporating kindness concerns explicitly in the de-\nvelopment and use of software systems is challenging: kindness is\nhighly context dependent, affected by a range of factors such as\nintentions and opportunity.\n\nIn this paper, we explore systematic ways in which kindness\nconcerns can be considered by software engineers. We propose a\nnovel meta-model that captures essential entities and relations as-\nociated with kindness. The meta-model enables the representation\nof possible instances or opportunities for performing acts of kind-\nness, by considering the actors involved (such as giver, receiver, and\nobserver), their psychological and social attributes that promote\nkindness (such as emotional states and social relatedness), the acts\nneeded to fulfil kindness opportunities (such as motivation, ability,\nand timeliness), and other contextual factors (such as location and\ntime). Our meta-model is demonstrated through two software ap-\nplication scenarios that enable charitable donations and kindness in\nbusiness. Overall, our proposal offers a first, tentative, but concrete\nstep towards enabling kind computing, and promoting kindness in\nsoftware systems.",
        "keywords": [
            "Kindness",
            "Meta-Modelling",
            "Software Engineering",
            "Kind Computing"
        ],
        "authors": [
            "Faeq Alrimawi",
            "Bashar Nuseibeh"
        ],
        "file_path": "data/models/models24/3640310.3674095.pdf"
    },
    {
        "title": "AlloyASG: Alloy Predicate Code Representation as a Compact Structurally Balanced Graph",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Writing declarative models has numerous benefits, ranging from automated reasoning and correction of design-level properties to automated testing and debugging of system implementations. Unfortunately, the model itself needs to be correct to gain these benefits. Alloy is a commonly used modeling language that has several existing efforts to repair faulty models automatically. Currently, these efforts are search-based methods that use an Abstract Syntax Tree (AST) representation of the model and do not scale, as ASTs suffer from exponential growth in their data size due to duplicate nodes. To address this issue, we introduce a novel code representation schema, Complex Structurally Balanced Abstract Semantic Graph (CSBASG), which represents code as a complex-weighted directed graph that lists a semantic element as a node in the graph and ensures its structural balance for almost finitely enumerable code segments. We evaluate the efficiency of our CSBASG representation for Alloy models in terms of it’s compactness compared to ASTs, and we explore if a CSBASG can ease the process of comparing two Alloy predicates. Lastly, we identify several future applications of CSBASG, including Alloy code generation and automated repair.",
        "keywords": [],
        "authors": [
            "Guanxuan Wu",
            "Allison Sullivan"
        ],
        "file_path": "data/models/models24/3640310.3674088.pdf"
    },
    {
        "title": "Mutation Testing of Java Bytecode: A Model-Driven Approach",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Mutation testing is an approach to checking the robustness of test suites. The program code is slightly changed by mutations to inject bugs. A test suite is robust enough if it finds such bugs. Mutation testing tools typically integrate sets of mutation operators such as, for example, swapping arithmetic operators; modern tools typically work with compiled code such as Java bytecode. The mutations must be defined in such a way that the mutated program can still be loaded and executed. The results of mutation tests depend directly on the possible mutations. More advanced mutations and even domain-specific mutations can pose another challenge to the test suite. Since the classical, non-model-based mutation testing tools do not support the specification of advanced mutation operators well, we propose a model-driven approach where mutations of Java bytecode can be flexibly defined by model transformation. Our approach also provides advanced mutation operators for modifying object-oriented structures, Java-specific properties and API method calls, making it the only mutation testing tool for Java bytecode that supports such mutations. To further improve the effectiveness of mutation testing, mutants are generated only for bytecode that is covered by tests. Our approach is implemented in the MMT tool. It has been evaluated against non-model-based mutation testing tools for its ability to generate mutants close to real bugs. The experiments make use of Defects4J, a well-established collection of real-world Java projects with reproducible bugs.",
        "keywords": [
            "Mutation testing",
            "Java bytecode",
            "Model transformation"
        ],
        "authors": [
            "Christoph Bockisch",
            "Deniz Eren",
            "Sascha Lehmann",
            "Daniel Neufeld",
            "Gabriele Taentzer"
        ],
        "file_path": "data/models/models24/3640310.3674103.pdf"
    },
    {
        "title": "EpiMDE: A Model-Driven Engineering Platform for Epidemiological Modeling",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Modeling is a critical step in studying epidemics. It allows us to better understand and predict the progression of a disease, design interventions such as vaccination, and assess their impact. Current epidemics are modeled using compartmental and mathematical models. While these are enough to achieve the primary goal of modeling, they suffer from shortcomings with respect to communicating and sharing the models, comparison and validation, and reproducibility. In this work, we propose the use of model-driven software engineering principles, to better represent disease models and facilitate the model management operations. We present an extensible metamodel for epidemics and an integrated development environment to allow epidemiologists to create and manage their models and simulations. We present the use of our platform on a COVID-19 model, where we show that the resulting model is more concise yet structurally and functionally equivalent to the original.",
        "keywords": [
            "epidemiological modeling",
            "integrated development environment",
            "reproducibility",
            "extensibility",
            "metamodeling"
        ],
        "authors": [
            "Bruno Curzi-Laliberté",
            "Marios Fokaefs",
            "Michalis Famelis",
            "Mohammad Hamdaqa"
        ],
        "file_path": "data/models/models24/3640310.3674104.pdf"
    },
    {
        "title": "Requirement-Driven Generation of Distributed Ledger Architectures",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Cross-organizational, blockchain-based distributed ledger networks in general, and those based on Hyperledger Fabric in particular, have an architecture which can be adapted to specific application requirements. However, network design can be a particularly challenging task, as the connection between architectural and deployment decisions and extra-functional properties can be subtle and the requirements may contradict each other, requiring trade-offs. In this paper, we propose a model-based distributed ledger architecture design approach which enables expert exploration of design options. We capture key requirements and define architecture fragments using partial modelling. We enumerate qualitatively different architectural candidates by graph generation. We evaluate and rank order candidates in logic solver tooling. As a result, our approach provides generative architectures for distributed ledger networks by enabling efficient exploration of design alternatives.",
        "keywords": [
            "Model Generation",
            "Partial Modelling",
            "Blockchain",
            "HyperLedger Fabric",
            "Design-space Exploration",
            "Generative Architecture"
        ],
        "authors": [
            "Noor Mohammed Sabr Al-Gburi",
            "András Földvári",
            "Kristóf Marussy",
            "Oszkár Semeráth",
            "Imre Kocsis"
        ],
        "file_path": "data/models/models24/3640310.3674097.pdf"
    },
    {
        "title": "Tree-Based versus Hybrid Graphical-Textual Model Editors: An Empirical Study of Testing Specifications",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Tree-based model editors and hybrid graphical-textual model editors have advantages and limitations when editing domain mod-els. Data is displayed hierarchically in tree-based model editors, whereas hybrid graphical-textual model editors capture high-level domain concepts graphically and low-level domain details textu-ally. We conducted an empirical user study with 22 participants to evaluate the implicit assumption of system modellers that hybrid notations are superior, and to investigate the tradeoffs between the default EMF-based tree model editor and a Sirius/Xtext-based hybrid model editor. The results of the user study indicate that users largely prefer the hybrid editor and are more confident with hybrid notations for understanding the meaning of conditions. Further-more, we found that the tree editor provided superior performance for analysing ordered lists of model elements, whereas activities requiring the comprehension or modelling of complex conditions were carried out faster through the hybrid editor.",
        "keywords": [
            "Hybrid Notations",
            "Model Editors",
            "Fuzz Testing",
            "Empirical Study"
        ],
        "authors": [
            "Ionut Predoaia",
            "James Harbin",
            "Simos Gerasimou",
            "Christina Vasiliou",
            "Dimitris Kolovos",
            "Antonio García-Domínguez"
        ],
        "file_path": "data/models/models24/3640310.3674102.pdf"
    },
    {
        "title": "Towards Runtime Monitoring for Responsible Machine Learning using Model-driven Engineering",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Machine learning (ML) components are used heavily in many current software systems, but developing them responsibly in practice remains challenging. ‘Responsible ML’ refers to developing, deploying and maintaining ML-based systems that adhere to human-centric requirements, such as fairness, privacy, transparency, safety, accessibility, and human values. Meeting these requirements is essential for maintaining public trust and ensuring the success of ML-based systems. However, as changes are likely in production environments and requirements often evolve, design-time quality assurance practices are insufficient to ensure such systems’ responsible behavior. Runtime monitoring approaches for ML-based systems can potentially offer valuable solutions to address this problem. Many currently available ML monitoring solutions overlook human-centric requirements due to a lack of awareness and tool support, the complexity of monitoring human-centric requirements, and the effort required to develop and manage monitors for changing requirements. We believe that many of these challenges can be addressed by model-driven engineering. In this new ideas paper, we present an initial meta-model, model-driven approach, and proof of concept prototype for runtime monitoring of human-centric requirements violations, thereby ensuring responsible ML behavior. We discuss our prototype, current limitations and propose some directions for future work.",
        "keywords": [
            "Runtime monitoring",
            "Responsible ML",
            "Human-centric requirements",
            "Machine learning components",
            "Model-driven engineering"
        ],
        "authors": [
            "Hira Naveed",
            "John Grundy",
            "Chetan Arora",
            "Hourieh Khalajzadeh",
            "Omar Haggag"
        ],
        "file_path": "data/models/models24/3640310.3674092.pdf"
    },
    {
        "title": "Text2VQL: Teaching a Model Query Language to Open-Source Language Models with ChatGPT",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "While large language models (LLMs) like ChatGPT has demonstrated impressive capabilities in addressing various software engineering tasks, their use in a model-driven engineering (MDE) context is still in an early stage. Since the technology is proprietary and accessible solely through an API, its use may be incompatible with the strict protection of intellectual properties in industrial models. While there are open-source LLM alternatives, they often lack the power of proprietary models and require extensive data fine-tuning to realize their full potential. Furthermore, open-source datasets tailored for MDE tasks are scarce, posing challenges for training such models effectively.\n\nIn this work, we introduce Text2VQL, a framework that generates graph queries captured in the VIATRA Query Language (VQL) from natural language specifications using open-source LLMs. Initially, we create a high-quality synthetic dataset comprising pairs of queries and their corresponding natural language descriptions using ChatGPT and VIATRA parser. Leveraging this dataset, we use parameter-efficient tuning to specialize three open-source LLMs, namely, DeepSeek Coder 1b, DeepSeek Coder 7b, and CodeLlama 7b for VQL query generation. Our experimental evaluation demonstrates that the fine-tuned models outperform the base models in query generation, highlighting the usefulness of our synthetic dataset. Moreover, one of the fine-tuned models achieves performance comparable to ChatGPT.",
        "keywords": [
            "large language model (LLM)",
            "model query language",
            "query generation",
            "VIATRA Query Language (VQL)",
            "ChatGPT"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Máté Földiák",
            "Dániel Varró"
        ],
        "file_path": "data/models/models24/3640310.3674091.pdf"
    },
    {
        "title": "A Comparative Analysis of Energy Consumption Between Visual Scripting models and C++ in Unreal Engine: Raising Awareness on the importance of Green MDD",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Video game engines are used in most modern video games because they simplify and speed up development. In addition, some of the most popular engines, such as Unreal Engine 5 (UE5), also integrate visual scripting tools. Visual scripting in UE5, through Blueprints, is a model-driven development approach that replaces text code, like C++, with a visual language of interconnected nodes representing functions and data flows, forming a flowchart-like logic diagram. This approach simplifies game development by abstracting complex code into intuitive, visual models, enabling creators to construct and iterate game components without extensive programming knowl-edge. Although Blueprint models usually decrease the complexity of implementing components, thus accelerating the development, they might lead to less energy-efficient runtime performance than C++. In this work, we evaluate the energy consumption of three relevant video game components (health system management, in-puts processing, and collections operations for an inventory), each implemented with Blueprint models and C++. The results show that the energy consumption per frame when using C++ is up to 48% lower than when using Blueprint models. The combination of artistic and technical profiles in video game developments has favoured the adoption of Blueprint models. However, there is a lack of works analyzing the energy consumption. Until this work, there was no evidence that the success of models for developing video games, like the one under study in this work, was accompanied by a cost in energy consumption for certain situations. Given the huge popularity of video games, this cost in energy might reach up to the equivalent of the energy consumption of 28 million European households.",
        "keywords": [
            "Energy consumption",
            "Video Games",
            "Green software",
            "Green Video Games",
            "Software sustainability",
            "Game Engines",
            "Unreal Engine",
            "Soft-ware Models",
            "Visual Scripting",
            "Blueprints",
            "C++",
            "Game Software Engineering"
        ],
        "authors": [
            "Javier Verón",
            "Carlos Pérez",
            "Coral Calero",
            "MªÁngeles Moraga",
            "Francisca Pérez",
            "Carlos Cetina"
        ],
        "file_path": "data/models/models24/3640310.3674099.pdf"
    },
    {
        "title": "A DSL for Testing LLMs for Fairness and Bias",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose Lang-BiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach.",
        "keywords": [
            "Model-Driven Engineering",
            "Domain-Specific Language",
            "Testing",
            "Ethics",
            "Bias",
            "Red Teaming",
            "Large Language Models"
        ],
        "authors": [
            "Sergio Morales",
            "Robert Clarisó",
            "Jordi Cabot"
        ],
        "file_path": "data/models/models24/3640310.3674093.pdf"
    },
    {
        "title": "10 years of Model Federation with Openflexo: Challenges and Lessons Learned",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "In the context of complex system development, heterogeneous\nmodeling responds to the need to integrate several domains. This\nneed requires the use of the most appropriate formalism and tooling\nfor each domain to be efficient. Model federation promotes the\nsemantic interoperability of heterogeneous models by providing the\nmeans to reify correspondences between different model elements,\nadd custom behaviors and bridge the gap between technological\nspaces. As such, it can be used as an infrastructure to address many\ndifferent system engineering problems. This is what we have been\ndoing for over a decade, as part of a close collaboration between\na small software engineering startup and academia. This paper\nreports on this experience.\nConcretely, we discuss the context, ambitions, and challenges\nthat led to the inception of our practice of model federation, and we\npresent five use cases experiences, stemming from real industrial\nand academic needs, and elaborate on lessons learned. In addition,\nwe also report on challenges and lessons learned regarding the\ndevelopment and maintenance of a model-driven model federation\ntool, the Openflexo framework. Finally, we set up a road map for\nthe future of model federation and Openflexo.",
        "keywords": [
            "Model federation",
            "Model management",
            "Experience report"
        ],
        "authors": [
            "Jean-Christophe Bach",
            "Antoine Beugnard",
            "Joël Champeau",
            "Fabien Dagnat",
            "Sylvain Guérin",
            "Salvador Martínez"
        ],
        "file_path": "data/models/models24/3640310.3674084.pdf"
    },
    {
        "title": "Partial Bidirectionalization of Model Transformation Languages",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "While most model-transformation languages in Model-Driven Engineering are unidirectional, bidirectionality is valuable when artifacts need two-way synchronization. Although several bidirectional transformation engines have been developed, their behavior is generally considered more difficult to formulate and predict compared to the unidirectional case. In the bidirectionalization approach, users write the forward direction of their transformations in the same unidirectional language they are used to, and obtain a system that (besides performing the complete forward transformation) can automatically propagate in the backward direction the target updates. When possible, full bidirectionalization is desirable, but far from trivial.\n\nIn this paper we propose a partial bidirectionalization approach, by partial compilation of a unidirectional language into a bidirectional language, and coupled execution of the two language engines. Forward transformation is still complete, whereas the target updates that can be back-propagated are deletions and modifications of a well-defined part of the target model. While the extent of the bidirectionalization depends on the two coupled systems, in this paper we provide a general combination scheme and we briefly discuss its well-behavedness. Then we use our technique to bidirectionalize the ATL model-transformation language on top of the GRoundTram bidirectional graph-transformation system.",
        "keywords": [
            "Model Transformation",
            "Bidirectional Transformation",
            "Bidirectionalization",
            "Runtime Interoperation",
            "Transformation Engines"
        ],
        "authors": [
            "Soichiro Hidaka and Massimo Tisi"
        ],
        "file_path": "data/models/models24/3640310.3674083.pdf"
    },
    {
        "title": "Automated Derivation of UML Sequence Diagrams from User Stories: Unleashing the Power of Generative AI vs. a Rule-Based Approach",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "User stories are informal, non-technical descriptions of features from a user’s perspective that guide collaboration and iterative development in Agile projects. However, ambiguities in user stories can lead to miscommunication among stakeholders. Design models, such as UML sequence diagrams, are essential for enhancing communication, clarifying system behavior, and improving the development process. This paper presents an automated approach for generating behavioral models specifically sequence diagrams from natural language requirements expressed as user stories. We also investigate the effectiveness of a Large Language Model (LLM) in using generative AI for this task. By applying our approach and ChatGPT to two benchmark datasets with the same set of user stories, we generated corresponding sequence diagrams for comparison. Expert evaluations in Software Engineering reveal that our approach effectively produces relevant, simplified diagrams for straightforward user stories, whereas the LLM tends to create more complex diagrams that sometimes go beyond the simplicity of the original user stories.",
        "keywords": [
            "User Story",
            "Sequence Diagram",
            "Generative Model",
            "Large Language Model",
            "Model Generation",
            "Natural Language Processing",
            "Rule-based approach"
        ],
        "authors": [
            "Munima Jahan",
            "Mohammad Mahdi Hassan",
            "Reza Golpayegani",
            "Golshid Ranjbaran",
            "Chanchal Roy",
            "Banani Roy",
            "Kevin Schneider"
        ],
        "file_path": "data/models/models24/3640310.3674081.pdf"
    },
    {
        "title": "AutoMW: Model-based Automated Medical Writing",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Medical Writing is an art of writing scientific documents which includes regulatory and research-related content. To obtain approval for marketing new medicines, pharmaceutical companies are obligated to provide drug authorities with a huge volume of documents related to clinical trials. Creating these clinical trial documents is a time, effort, and skill-intensive process as the required information exists in fragmented form distributed across various information sources. To overcome these challenges in medical writing, we propose Automated Medical Writing tool (AutoMW).  AutoMW enables the digitalization of information from different sources of information using a meta-model-based approach and leverages these models for the automated generation of clinical trial documents as per the regulatory authority document templates. This paper describes the approach and illustrates its utility and efficacy in real-world clinical trial application of two use cases - breast cancer, and diabetes.",
        "keywords": [
            "MDE",
            "Medical Writing",
            "Automated Content Generation",
            "NLP",
            "Clinical Trial Documentation"
        ],
        "authors": [
            "Asha Rajbhoj",
            "Ajim Pathan",
            "Tanay Sant",
            "Vinay Kulkarni",
            "Padmalata Nistala",
            "Rajesh Pandey",
            "Sabarinathan Narasimhan",
            "Geetha Thiagarajan"
        ],
        "file_path": "data/models/models24/3640310.3674096.pdf"
    },
    {
        "title": "Enhancing Automata Learning with Statistical Machine Learning: A Network Security Case Study",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Intrusion detection systems are crucial for network security. Verification of these systems is complicated by various factors, including the heterogeneity of network platforms and the continuously changing landscape of cyber threats. In this paper, we use automata learning to derive state machines from network-traffic data with the objective of supporting behavioural verification of intrusion detection systems. The most innovative aspect of our work is addressing the inability to directly apply existing automata learning techniques to network-traffic data due to the numeric nature of such data. Specifically, we use interpretable machine learning (ML) to partition numeric ranges into intervals that strongly correlate with a system’s decisions regarding intrusion detection. These intervals are subsequently used to abstract numeric ranges before automata learning. We apply our ML-enhanced automata learning approach to a commercial network intrusion detection system developed by our industry partner, RabbitRun Technologies. Our approach results in an average 67.5% reduction in the number of states and transitions of the learned state machines, while achieving an average 28% improvement in accuracy compared to using expertise-based numeric data abstraction. Furthermore, the resulting state machines help practitioners in verifying system-level security requirements and exploring previously unknown system behaviours through model checking and temporal query checking. We make our implementation and experimental data available online.",
        "keywords": [
            "State-machine learning; Intrusion detection; Decision trees; Denial of Service (DoS) attacks; Model checking; Query checking."
        ],
        "authors": [
            "Negin Ayoughi",
            "Shiva Nejati",
            "Mehrdad Sabetzadeh",
            "Patricio Saavedra"
        ],
        "file_path": "data/models/models24/3640310.3674087.pdf"
    },
    {
        "title": "Advancing Domain-Specific High-Integrity Model-Based Tools: Insights and Future Pathways",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Rolls-Royce Control Systems supplies engine control and monitoring systems for aviation applications, and is required to design, certify, and deliver these with the highest level of safety assurance. To allow Rolls-Royce to develop these systems, which continue to increase in complexity, model-based techniques are now a critical part of the software development process. At MODELS 2021 we presented early experiences with using and maintaining a bespoke domain-specific modelling workbench based on open-source modelling technologies, including the Eclipse Modelling Framework (EMF), Xtext, Sirius, and Epsilon. In this paper, we build on our previous paper with further insights, new challenges and lessons learnt as we have advanced and matured our domain-specific solution. We also discuss our experiences with moving towards web based modelling tools based on open-source technologies including Sirius Web, Eclipse GLSP and Eclipse Theia. Rolls-Royce intends to use a selection of these technologies to build a web-based modelling workbench, which will be used to architect and integrate the software for future Rolls-Royce engine control and monitoring systems in a collaborative way.",
        "keywords": [
            "Domain specific languages",
            "component oriented architecture",
            "web based modelling",
            "GLSP",
            "EMF"
        ],
        "authors": [
            "Qurat ul ain Ali",
            "Dimitris Kolovos",
            "Antonio Garcia-Dominguez",
            "Michael Bennett",
            "Joe Newton",
            "Piotr Zacharzewski"
        ],
        "file_path": "data/models/models24/3640310.3674094.pdf"
    },
    {
        "title": "Product Lines of Graphical Modelling Languages",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Modelling languages are essential in many disciplines to express\nknowledge in a precise way. Furthermore, some domains require\nfamilies of notations (rather than individual languages) that account\nfor variations of a language. Some examples of language families\ninclude those to define automata, Petri nets, process models or\nsoftware architectures. Several techniques have been proposed to\nengineer families of languages, but they often neglect the language’s\nconcrete syntax, especially if it is graphical.\nTo fill this gap, we propose a modular method to build product\nlines of graphical modelling languages. Language features are de-\nfined in modules, which comprise both the abstract and graphical\nconcrete syntax of the feature. A language variant is selected by\nchoosing a valid configuration of modules, from which the abstract\nand concrete syntax of the variant is synthesised. Our approach per-\nmits composition and overriding of graphical elements (e.g., symbol\nstyles, visualisation layers), the injection of pre-defined graphi-\ncal styles into language families (e.g., to obtain a high-intensity\ncontrast variant for accessibility), and the analysis of graphical con-\nflicts at the product line level. We report on an implementation atop\nEclipse/Sirius, and demonstrate its benefits by an evaluation which\nshows a substantial specification size reduction of our product line\nmethod with respect to a case-by-case specification approach.",
        "keywords": [
            "Software Language Engineering",
            "Model-driven Engineering",
            "Graphical Concrete Syntax",
            "Product Lines"
        ],
        "authors": [
            "Antonio Garmendia",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models24/3640310.3674082.pdf"
    },
    {
        "title": "AI-Driven Consistency of SysML Diagrams",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Graphical modeling languages, expected to simplify systems analysis and design, present a challenge in maintaining consistency across their varied views. Traditional rule-based methods for ensuring consistency in languages like UML often fall short in addressing complex semantic dimensions. Moreover, the integration of Large Language Models (LLMs) into Model Driven Engineering (MDE) introduces additional consistency challenges, as LLM’s limited output contexts requires the integration of responses. This paper presents a new framework that automates the detection and correction of inconsistencies across different views, leveraging formally defined rules and incorporating OpenAI’s GPT, as implemented in TTool. Focusing on the consistency between use case and block diagrams, the framework is evaluated through its application to three case studies, highlighting its potential to significantly enhance consistency management in graphical modeling.",
        "keywords": [],
        "authors": [
            "Bastien Sultan",
            "Ludovic Apvrille"
        ],
        "file_path": "data/models/models24/3640310.3674079.pdf"
    },
    {
        "title": "ModelMate: A recommender for textual modeling languages based on pre-trained language models",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Current DSL environments lack smart editing facilities intended\nto enhance modeler productivity and cannot keep pace of current\ndevelopments of integrated development environments based on AI.\nIn this paper, we propose an approach to address this shortcoming\nthrough a recommender system specifically tailored for textual\nDSLs based on the fine-tuning of pre-trained language models. We\nidentify three main tasks: identifier suggestion, line completion,\nand block completion, which we implement over the same fine-\ntuned model and we propose a workflow to apply these tasks to\nany textual DSL. We have evaluated our approach with different\npre-trained models for three DSLs: Emfatic, Xtext and a DSL to\nspecify domain entities, showing that the system performs well\nand provides accurate suggestions. We compare it against existing\napproaches in the feature name recommendation task showing that\nour system outperforms the alternatives. Moreover, we evaluate\nthe inference time of our approach obtaining low latencies, which\nmakes the system adequate for live assistance. Finally, we contribute\na concrete recommender, named ModelMate, which implements\nthe training, evaluation and inference steps of the workflow as well\nas providing integration into Eclipse-based textual editors.",
        "keywords": [
            "Recommendation",
            "Meta-modeling",
            "Model-Driven Engineering",
            "Machine learning"
        ],
        "authors": [
            "Carlos Durá Costa",
            "José Antonio Hernández López",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/models/models24/3640310.3674089.pdf"
    },
    {
        "title": "Extensions and Scalability Experiments of a Generic Model-Driven Architecture for Variability Model Reasoning",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Until recently, the state-of-the-art of Software Product Line (SPL) configuration and verification automation consisted of a collection of ad-hoc approaches tightly coupling a single input Variability Modeling Language (VML) with a single constraint solver. To remedy this situation, a novel generic model-driven architecture was then proposed that enables using a variety of VMLs and solvers. The key ideas of this proposal were (a) the use of a standard logical language (CLIF) as a pivot between VMLs and solvers, and (b) the use of a standard data exchange format (JSON) to explicilty and declaratively specify the abstract syntax and semantics of the VMLs to be used in an SPL engineering project and the automated reasoning task to be performed by the solvers.\nIn this article, we overcome the limitations of this initial proposal in three key ways: (1) we add the ability to reason on textual or hybrid VMLs, rather than only on diagrammatic VMLs, enhancing the versatility of the architecture on the input side; (2) we enable the use of solvers from a third paradigm, enhancing the versatility of the architecture on the output side; and, (3) we present the results of scalability performance experiments of an implementation of this architecture. These results have been achieved without signifi-cantly altering the architecture, demonstrating its agnosticism with respect to specific VMLs and solvers. It also shows that it can under-lie the implementation of practical variability reasoning tools that scale up to real sized variability model analysis and configuration needs.",
        "keywords": [
            "Software Product Lines",
            "Automated Reasoning",
            "Generic Architecture",
            "Configuration Automation"
        ],
        "authors": [
            "Camilo Correa Restrepo",
            "Jacques Robin",
            "Raul Mazo"
        ],
        "file_path": "data/models/models24/3640310.3674090.pdf"
    },
    {
        "title": "Toward Intelligent Generation of Tailored Graphical Concrete Syntax",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "In model-driven engineering, the concrete syntax of a domain-specific modeling language (DSML) is fundamental as it constitutes the primary point of interaction between the user and the DSML. Nevertheless, the conventional one-size-fits-all approach to concrete syntax often undermines the effectiveness of DSMLs, as it fails to accommodate the diverse constraints and specific requirements inherent to diverse users and usage contexts. Such shortcomings can lead to a significant decline in the performance, usability, and efficiency of DSMLs. This vision paper proposes a conceptual framework to generate concrete syntax intelligently. Our framework considers multiple concerns of users and aims to align the concrete syntax with the context of the DSML usage. Additionally, we detail a baseline process to employ our framework in practice, leveraging large language models to expedite the generation of tailored concrete syntax. We illustrate the potential of our vision with two concrete examples and discuss the shortcomings and research challenges of current intelligent generation techniques.",
        "keywords": [
            "Domain-specific Modeling Languages",
            "Concrete Syntax",
            "Artificial Intelligence",
            "Large Language Models"
        ],
        "authors": [
            "Meriem Ben Chaaben",
            "Oussama Ben Sghaier",
            "Mouna Dhaouadi",
            "Nafisa Elrasheed",
            "Ikram Darif",
            "Imen Jaoua",
            "Bentley Oakes",
            "Eugene Syriani",
            "Mohammad Hamdaqa"
        ],
        "file_path": "data/models/models24/3640310.3674085.pdf"
    },
    {
        "title": "Model Everything but with Intellectual Property Protection — The Deltachain Approach",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Many organizations are involved in the development of complex systems, e.g., cyber-physical systems. Organizations work collabo-ratively to describe these systems, using models, which are devel-oped using multiple languages and tools. The models may contain intellectual property that must be protected from other parties, including other contributors. To enable the ongoing exchange of models and to ensure intellectual property protection, our new idea is to use encrypted deltas, i.e., arbitrary changes made to a model. These encrypted deltas are stored on a chain, which we call Deltachain. Encryption enables free exchange of the Deltachain, e.g., on third-party commercial file storage servers. Collaborators involved in the development of the model can access the encrypted Deltachain, decrypt the parts to which they have access, and then work with those decrypted parts which are created by applying the deltas. Subsequently, the collaborators can encrypt their deltas to the model parts and append the encrypted deltas to the Deltachain. Our vision is the use of this Deltachain by collaborating organiza-tions as a single source of truth.",
        "keywords": [
            "Collaborative Software Engineering",
            "Model-Driven Engineering",
            "Cross-Organisational Collaboration",
            "Data Structures",
            "Applied Cryp-tography",
            "Deltachain"
        ],
        "authors": [
            "Thomas Weber",
            "Sebastian Weber"
        ],
        "file_path": "data/models/models24/3640310.3674086.pdf"
    }
]