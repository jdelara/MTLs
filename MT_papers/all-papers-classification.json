[
    {
        "title": "Developing with UML - Some Pitfalls and \nWorkarounds",
        "date": 1998,
        "abstract": "The object-oriented modeling language UML offers various notations for all phases of application development. The user is left alone, however, when applying UML in up-to-date application development involving distribution, data management, and component-oriented mechanisms. Moreover, various shortcomings have been encountered, most notably w.r.t. refinement of model elements throughout the development life cycle and employment of interaction diagrams to formalize use cases. The paper will shed some light on how these issues may be handled with UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting and Applying the UML Conceptual \nFramework",
        "date": 1998,
        "abstract": "The Unified Modelling Language (UML) ostensibly assumes a four level (meta) modelling framework, both for its definition and for the conceptual context in which its users operate. In practice, however, it is still dominated by the traditional two level (model + data) view of object modelling and neither supports nor applies the four level framework properly. This not only diminishes the clarity of the UML semantics, but also complicates the task of those users who do wish to fully embrace a multi-level approach. After outlining the characteristics of the intended conceptual framework, and the problems resulting from the UML’s current two-level bias, this paper presents three simple enhancements to the UML which provide the required expressive power for multi-level modelling. The paper then goes on to discuss issues in the application of the conceptual framework within the UML’s own definition.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling: Is It Turning Informal into Formal?",
        "date": 1998,
        "abstract": "This work studies the meaning of the qualifier « semi-formal », which is usually attributed to design diagrams. Starting with a UML diagram as an example, the paper deals with the three modes of expressing things about the outside world: symbols, indexes and icons. The idea that the informational process consists in formalizing an informal given is discussed with regard to the supposed informal nature of the users’ requirements. It is also shown that a modeling language such as UML, although formalized in its inner constructions, can not strictly formalize the connection to the outside world it intends to model. This framework, arising from C. S. Peirce’s semiotics, allows to account for the modeling process as a effective interpretation reasoning on diagrams which are themselves made of signs. Thus we go beyond the apparent contradiction between the formal and the informal, using the concept of Interpretant. We can then envisage the study of design reasoning as dialogs between a model, its interpretants and the outside world or domain.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Best of Both Worlds -  \nA Mapping from EXPRESS-G to UML",
        "date": 1998,
        "abstract": "On the one hand, in the world of Product Data Technology (PDT), the ISO standard STEP (Standard for the Exchange of Product Model Data) gains more and more importance. STEP includes the information model specification language EXPRESS and its graphical notation EXPRESS-G. On the other hand, in the Software Engineering world in general, mainly other modelling languages are in use - particularly the Unified Modeling Language (UML), recently adopted to become a standard by the Object Management Group, will probably achieve broad acceptance. Despite a strong interconnection of PDT with the Software Engineering area, there is a lack of bridging elements concerning the modelling language level. This paper introduces a mapping between EXPRESS-G and UML in order to define a linking bridge and bring the best of both worlds together. Hereby the feasibility of a mapping is shown with representative examples; several problematic cases are discussed as well as possible solutions presented.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Making UML Models Interoperable with UXF",
        "date": 1998,
        "abstract": "Uniﬁed Modeling Language (UML) has been widely accepted in the software engineering area, because it provides most of the con- cepts and notations that are essential for documenting object-oriented models. However, UML does not have an explicit format to describe and interchange its model information intentionally. This paper addresses the UML model interchange and presents our eﬀorts to make UML highly in- teroperable. We developed an interchange format called UXF (UML eX- change Format) based on XML (Extensible Markup Language). UXF is a simple and well-structured format to encode UML models. It leverages the tool interoperability, team development and reuse of design models by interchanging the model information with the the XML standard. Also, we propose an open distribution platform for UML models, which provides multiple levels of interoperability of UML models. Our work shows an important step in the evolution for the interoperable UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automation of Design Pattern : Concepts, Tools and \nPractices",
        "date": 1998,
        "abstract": "Model transformation is a technique that makes it possible to auto- mate design patterns. Applied to UML, the result is highly promising. However, model transformation rules have to be structured by a specific organization mechanism called viewpoint, and be coupled with the UML model extension features (tagged values, stereotypes, etc.). This has been done through a specific technique, called « hypergenericity », which is implemented by a case tool and used since 1994.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Informal Formality? The Object Constraint Language \nand Its Application in the UML Metamodel",
        "date": 1998,
        "abstract": "Within the field of object technology it is becoming recognised that constraints are a good way to produce more precise and formal specifications than with diagrams alone. Evidence of this is that UML incorporates a standard constraint language called OCL (Object Constraint Language). The availability of OCL will encourage UML users to add constraints to their UML models. This paper explains OCL and demonstrates its applicability. Probably the largest application of OCL to date was its use to define the metamodel of UML, and the experiences gained in this application are discussed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Using UML Class Diagrams for Object-Oriented Database Design Speciﬁcation of Integrity Constraints",
        "date": 1998,
        "abstract": "In the course of object-oriented software engineering, the UML class diagrams are used to specify the static structure of the system under study, such as classes, types and various kinds of static relation- ships among them. Objects of the persistent classes can be stored in object-oriented databases or in relational databases. In the former case, the UML class diagrams are actually used for conceptual object-oriented database designs. However, the standard UML class diagram lacks the ability to specify some inherent integrity constraints, such as keys and uniqueness, for object-oriented databases. This paper proposes an ex- tension to the UML metamodel, i.e., the introduction of two new model elements (key and IConstraint) and some new attributes to the existing metamodel, to accommodate further, additional features for constraint speciﬁcation. On the model level, a compartment CONSTRAINT of the class notation and some property strings for displaying the integrity con- straints are added. The database design is then mapped to the extended ODMG-ODL schema deﬁnition. Keywords: Conceptual Data Modeling, Integrity Constraints, Object- Oriented Database Design",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Literate Modelling — Capturing Business Knowledge\nwith the UML",
        "date": 1998,
        "abstract": "At British Airways, we have found during several large OO projects documented using the UML that non-technical end-users, managers and busi- ness domain experts ﬁnd it difﬁcult to understand UML visual models. This leads to problems in requirement capture and review. To solve this problem, we have developed the technique of Literate Modelling. Literate Models are UML dia- grams that are embedded in texts explaining the models. In that way end-users, managers and domain experts gain useful understanding of the models, whilst object-oriented analysts see exactly and precisely how the models deﬁne business requirements and imperatives. We discuss some early experiences with Literate Modelling at British Airways where it was used extensively in their Enterprise Object Modelling initiative. We explain why Literate Modelling is viewed as one of the critical success factors for this signiﬁcant project. Finally, we propose that Literate Modelling may be a valuable extension to many other object-oriented and non object-oriented visual modelling languages.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying UML to Design an Inter-Domain Service \nManagement Application",
        "date": 1998,
        "abstract": "We present a component-oriented approach to demonstrate the use of the Unified Modeling Language (UML) and Open Distributed Processing (ODP) concepts to design service components of a telecommunications management system. This paper is based on the work done in the design phase of the ACTS project “TRUMPET” (Inter-Domain Management with Integrity). This project undertook to produce a service management architecture suitable for emerging liberalised telecommunications markets. The criteria where that the system should be highly distributed both technologically and administratively -- i.e., across many kinds of organisations. TRUMPET project presents a good model environment to develop not only the service architecture itself but also the methodologies for producing such designs. In our approach, we use the conceptual framework of ODP and discuss some methodological issues related to ODP-viewpoint modelling of distributed systems using UML notations. We conclude with recognising the power of combining UML and ODP so as to manage the complexity of the problem.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Booster*Process\nA Software Development Process Model\nIntegrating Business Object Technology and\nUML",
        "date": 1998,
        "abstract": "This paper describes a UML-based process model (called Booster*Process) for system development founded on business object technology. It integrates business and software engineering aspects, de- scribes the speciﬁc modeling activities needed for business and software system modeling and connects the various UML diagrams, particularly taking into consideration the requirements of business objects and their component character. It propagates a multi-level approach, starting with use case, activity and class modeling at the organizational level, and then shifting to analysis and design of business applications.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Hierarchical Context Diagrams with UML:  \nAn Experience Report \n on Satellite Ground System Analysis",
        "date": 1998,
        "abstract": "Although the UML was mainly designed for software development, we have introduced its use in the requirements analysis phase of the ground segment of a complete satellite system. In the first part of the paper, we present the subset of UML, consisting mainly in Use Cases and Interaction diagrams, that we have used for the requirements analysis phase. We insist on the need to define precisely the scope of the problem, its environment. We mostly assert that the \"Context diagram\" from traditional structured methods is still relevant in an object-oriented approach, and furthermore that it can be adequately represented by a special usage of the UML Collaboration diagram. This Context diagram can even take into account progressively the underlying physical architecture of satellite systems. The second part of the paper is more prospective: it aims at extracting Context diagrams patterns, and proposes specific stereotypes for satellite system analysis.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extension of UML Sequence Diagrams for Real-Time \nSystems",
        "date": 1998,
        "abstract": "The behavior of real-time systems is specified by a number of interaction scenarios between tasks or active objects. Each scenario may be illustrated by a UML sequence diagram. We use the newly developed, textual language UMLscript-RT as input language for our tool AVUS, mainly a compiler, that automatically generates standard UML sequence diagrams. UMLscript-RT extends UML sequence diagrams in two aspects. Firstly, we introduce loops and suggest a graphical notation very similar to that used in Message Sequence Charts. Secondly we give a precise grammar for timing constraints which are mandatory for real-time applications. AVUS generates a directed graph whose vertices are the events and associates the constraints as weights to the arrows. Consistency of the timing constraints is then checked by examining the cycles of that graph.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML and User Interface Modeling",
        "date": 1998,
        "abstract": "UML and traditional CASE tools still focus more on application internals and less on application usability aspects. A user interface (UI) is modeled in terms of its internal structure and objects comprising it, the same as the rest of the application. The adoption of use cases and interaction scenarios acknowledges the importance of recognizing user tasks when developing an application, but it is still used mainly as a starting point for designing software implementing usage scenarios rather than focusing on modeling user tasks to improve application usability. Explicit modeling of user interface domain knowledge can bring important benefits when utilized by a CASE tool: additional design assistance with exploring UI design alternatives, support for evaluating and critiquing UI designs, as well as increased reuse and easier maintenance. UML can provide a notation framework for integrating user interface modeling with mainstream software engineering OO modeling. The built-in extensibility mechanisms (stereotypes, tagged values and constraints) allow the introduction of new modeling constructs with specialized semantics for UI modeling while staying within UML. The paper identifies modeling constructs needed for UI modeling and proposes a direction for extending UML to better address UI design.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Structuring UML Design Deliverables",
        "date": 1998,
        "abstract": "The idea of using Unified Modeling Language (UML) appeals to people, but actually using it can be challenging. Many would like to use UML for software development, but do not know how to structure design models and what the relationships between various UML diagrams are. This paper introduces a structure for design deliverables that can be used for software development with UML. The structure is based on a pattern of four deliverables describing classifier relationships, interactions, responsibilities and state machines. The pattern can be applied to different levels of abstraction and to different views on a software product. The paper also discusses practical considerations for documenting software design in the project repository as well as cases in which UML may not be the most appropriate notation to use. The conference presentation with speaker notes is available at this address: www.navision.com (click services).",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Considerations of and Suggestions for\na UML-Specific Process Model",
        "date": 1998,
        "abstract": "The developers of the Unified Modeling Language (UML) promote (but do not describe) a development process model that is use case-driven, architecture centric, and iterative and incremental. This paper analyzes these features and suggests some extra features needed in developing object- oriented client/server applications (including Internet). The paper is heavily based on practical experiences, where object-oriented client/server applica- tions have been built with the three mentioned requirements in mind. The pa- per outlines a process model that meets the stated features. In particular, it connects the roles of the development team and the tasks in the process model. KEYWORDS: Process model, modeling language, role.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The UML as a Formal Modeling Notation",
        "date": 1998,
        "abstract": "The Uniﬁed Modeling Language (UML) is rapidly emerging as a de-facto standard for modelling OO systems. Given this role, it is imperative that the UML needs a well-deﬁned, fully explored semantics. Such semantics is required in order to ensure that UML concepts are precisely stated and deﬁned. In this paper we motivate an approach to formalizing UML in which formal speciﬁcation techniques are used to gain insight into the semantics of UML notations and diagrams and describe a roadmap for this approach. The authors initiated the Precise UML (PUML) group in order to develop a precise semantic model for UML diagrams. The semantic model is to be used as the basis for a set of diagrammatical transformation rules, which enable formal deductions to be made about UML diagrams. A small example shows how these rules can be used to verify whether one class diagram is a valid deduction of another. Because these rules are presented at the diagrammatical level, it will be argued that UML can be successfully used as a formal modelling tool without the notational complexities that are commonly found in textual speciﬁcation techniques.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OML: Proposals to Enhance UML",
        "date": 1998,
        "abstract": "While the UML metamodel and notation aim to be comprehensive, there are a number of areas in which this modelling language is seen to be deficient. The proposals in OML (Firesmith et al., 1997) contain a number of advanced metamodelling and notational techniques which could also be of use in enhancing UML. In particular contributions can be made in the areas of modelling responsibilities and aggregations and in the provision of notational elements underpinned by semiotics and usability concerns. Other areas of potential contribution include a more consistent and thorough treatment of abstraction foci in terms of class versus type versus instance - applicable not only at the classifier level but also to packages, scenarios etc.; the ability to discriminate clearly between the various types of inheritance and to represent these notationally. It is critical that any standard support not only a use-case and a data-driven mindset but also that of a responsibility-driven modelling process and that the results of these modelling endeavours are communicated as effectively as possible both to other developers and to users.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Validating Distributed Software Modeled with\nthe Uniﬁed Modeling Language",
        "date": 1998,
        "abstract": "The development of correct OO distributed software is a daunting task as soon as the distributed interactions are not trivial. This is due to the inherent complexity of distributed systems (latency, error recovery, etc), leading to numerous problems such as deadlocks, race conditions, and many diﬃculties in trying to reproduce such error conditions and debug them. The OO technology is ill-equipped to deal with this dimension of the problem. On the other hand, the willingness of mastering this complexity in the context of telecommunication proto- cols gave birth to speciﬁc formal veriﬁcation and validation tools. The aim of this paper is to explore how the underlying technology of these tools could be made available to the designer of OO distributed software. We propose a framework allowing the integration of formal veriﬁcation and validation technology in a seamless OO life-cycle based on UML, the Uniﬁed Modeling Language.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting Disciplined Reuse and Evolution \nof UML Models",
        "date": 1998,
        "abstract": "UML provides very little support for modelling evolvable or reusable specifications and designs. To cope with this problem, the UML needs to be extended with support for reuse and evolution of model components. As a first step, this paper enhances the UML metamodel with the “reuse contract” formalism to deal with evolution of collaborating class interfaces. Such a formal semantics for reuse allows us to detect evolution and composition conflicts automatically.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying UML Extensions to Facilitate Software Reuse",
        "date": 1998,
        "abstract": "The benefits from reuse during the analysis and design stages of software development are well understood. This paper examines the contribution which UML can make to such reuse through its ability to document reusable structures. In particular, the application of the UML concepts of stereotypes, tagged values, class compartments and association roles in the definition of search criteria for reuse candidates are explored. An iterative development process, within which UML can be used, is presented and discussed. An initial implementation in a CBR environment, and results from this experimental prototype, are also presented.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending Architectural Representation in UML with View Integration",
        "date": 1999,
        "abstract": "UML has established itself as the leading OO analysis and design methodology. Recently, it has also been increasingly used as a foundation for representing numerous (diagrammatic) views that are outside the standardized set of UML views. An example are architecture description languages. The main advantages of representing other types of views in UML are 1) a common data model and 2) a common set of tools that can be used to manipulate that model. However, attempts at representing additional views in UML usually fall short of their full integration with existing views. Integration extends represen- tation by also describing interactions among multiple views, thus capturing the inter-view relationships. Those inter-view relationships are essential to enable automated identification of consistency and conformance mismatches. This work describes a view integration framework and demonstrates how an archi- tecture description language, which was previously only represented in UML, can now be fully integrated into UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enabling the Refinement of a Software Architecture ",
        "date": 1999,
        "abstract": "Software architecture research has thus far mainly addressed formal specification and analysis of coarse-grained software models. The formality of architectural descriptions, their lack of support for downstream development activities, and their poor integration with mainstream approaches have made them unattractive to a large segment of the development community. This paper demonstrates how a mainstream design notation, the Unified Modeling Language (UML), can help address these concerns. We describe a semi- automated approach developed to assist in refining a high-level architecture specified in an architecture description language (ADL) into a design described with UML. To this end, we have integrated DRADEL, an environment for architecture modeling and analysis, with Rational Rose®, a commercial off-the- shelf (COTS) UML modeling tool. We have defined a set of rules to transform an architectural representation into an initial UML model that can then be further refined. We believe this approach to be easily adaptable to different ADLs, to the changes in our understanding of UML, and to the changes in UML itself.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using the UML for Architectural Description",
        "date": 1999,
        "abstract": "There is much interest in using the Uniﬁed Modeling Lan- guage (UML) for architectural description – those techniques by which architects sketch, capture, model, document and analyze architectural knowledge and decisions about software-intensive systems. IEEE P1471, the Recommended Practice for Architectural Description, represents an emerging consensus for specifying the content of an architectural descrip- tion for a software-intensive system. Like the UML, IEEE P1471 does not prescribe a particular architectural method or life cycle, but may be used within a variety of such processes. In this paper, I provide an overview of IEEE P1471, describe its conceptual framework, and investigate the issues of applying the UML to meet the requirements of IEEE P1471. Keywords: IEEE P1471, architectural description, multiple views, view- points, Uniﬁed Modeling Language",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Viewing the OML as a Variant of the UML",
        "date": 1999,
        "abstract": "The OPEN Modelling Language, OML, was published dur- ing the standardization process which ﬁnally led to UML version 1.3. While being contributory to this process, there are still some features of the OML which have not been adopted in the current version of the UML. These features oﬀer capabilities which are complementary to those of the UML. This paper describes how these features of the OML can be made available to UML developers by viewing the OML as a variant of the UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Comparison of the Business Object Notation\nand the Uniﬁed Modeling Language",
        "date": 1999,
        "abstract": "Seamlessness, reversibility, and software contracting have been pro- posed as important techniques to be supported by object-oriented methods. These techniques are used to provide a framework for the comparison of two model- ing languages, the Business Object Notation (BON) and the Uniﬁed Modeling Language (UML). Elements of the UML and its constraint language that do not support these techniques are discussed. Suggestions for further improvements to both BON and UML are described.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalizing the UML Class Diagram Using Object-Z",
        "date": 1999,
        "abstract": "To produce a precise and analyzable software model, it is essential for the modeling technique to have formality in the syntax and the semantics of its notation, and to allow rigorous analysis of its models. In this sense, UML is not yet a truly precise modeling technique. This paper presents a formal basis for the syntactic structures and semantics of core UML class constructs, and also provides a basis for reasoning about UML class dia- grams. The syntactic structures of UML class constructs and the rules for de- veloping a well-formed class diagram are precisely described using the Z no- tation. Based on this formal description, UML class constructs are then translated to Object-Z constructs. Proof techniques provided for Object-Z can be used for reasoning about these class diagrams.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Formal Approach to Collaborations in the Uniﬁed Modeling Language",
        "date": 1999,
        "abstract": "In this paper we give a formal deﬁnition of the collaboration construct in the Uniﬁed Modeling Language (UML). We also state what it means that a use case is realized by a collaboration, and what the relationship is between the speciﬁcation part and the realization part of a subsystem in UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Formal Semantics for UML Interactions",
        "date": 1999,
        "abstract": "The UML abstract syntax and semantics speciﬁcation distin- guishes between the statics and the dynamics of collaborations: the rˆole context and interactions. We propose a formal semantics of interactions based on the abstract syntax and directly reﬂecting the speciﬁcation. The semantics is both parametric in the notion of context and in se- mantic details that are intentionally left open by the speciﬁcation, but resolves true inconsistencies. The formalisation uses temporal logic for- mulae in the style of Manna and Pnueli. We illustrate the ﬂexibility of our semantics by discussing instantiations for a running example; its in- tuitiveness is substantiated by proving that the temporal formulae give rise to partial orders that also directly can be inferred from interactions.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML 2.0 Architectural Crossroads: Sculpting or Mudpacking? Panel",
        "date": 1999,
        "abstract": "As the UML reaches the venerable age of four, both its proponents and critics are scanning the changes in the UML 1.3 revision and the proposed UML 2.0 roadmap. They are asking pointed questions, such as: Has the modeling language matured or bloated during the standardization process? Will UML be able to adapt to the changing requirements of software development? These questions and the strong emphasis of the UML Revision Task Force’s final report on architectural issues suggest that UML is approaching an architectural crossroads at the OMG and elsewhere. This panel explores the nature and the extent of the architectural challenges facing UML, and makes constructive recommendations to address them.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Core Meta-Modelling Semantics of UML: The pUML Approach",
        "date": 1999,
        "abstract": "The current UML semantics documentation has made a sig- niﬁcant step towards providing a precise description of the UML. How- ever, at present the semantic model it proposes only provides a descrip- tion of the language’s syntax and well-formedness rules. The meaning of the language, which is mainly described in English, is too informal and unstructured to provide a foundation for developing formal anal- ysis and development techniques. Another problem is the scope of the model, which is both complex and large. This paper describes work cur- rently being undertaken by the precise UML group (pUML), an interna- tional group of researchers and practitioners, to address these problems. A formalisation strategy is presented which concentrates on giving a pre- cise denotational semantics to core elements of UML. This is illustrated through the development of precise deﬁnitions of two important con- cepts: generalization and packages. Finally, a viewpoint architecture is proposed as a means of providing improved separation of concerns in the semantics deﬁnition.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A\nMetamo\ndel\nfor\nOCL",
        "date": 1999,
        "abstract": "The Ob ject Constrain t Language (OCL) allo ws the exten-",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tool-Supported Compression of UML Class Diagrams",
        "date": 1999,
        "abstract": "Techniques for tool-supported compression of UML class diagrams are developed. These techniques allow abstract representations of class diagrams by effacing (less essential) parts of the diagram. The hidden parts can be made again visible at selected points. The user can start examining a class diagram with only few main classes visible and refine the diagram gradually to the interesting directions, proceeding from abstract view to details. The proposed techniques help in managing large class diagrams and in extracting high-level views from object-oriented legacy systems, thus supporting the understanding of the overall architecture of the system. The construction of the compressed form of a class diagram can be either automatic or it can be controlled by a human. An algorithm is given for managing compressed class diagrams, and a prototype implementation is described.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Pragmatic Approach for Building a User-Friendly and Flexible UML Model Repository",
        "date": 1999,
        "abstract": "In France Telecom research center in Lannion (France) we have been working for three years on OO modeling as a promising technology for unifying the representation of data. This has led us to develop a Model Repository Tool, which offers, as its default configuration, a full support for the UML 1.3 meta- model. The tool enables the manipulation of models by means of a Java or Py- thon API. It provides a rich and flexible registration capability based on an ex- plicit identification, relying on a two-leveled hierarchical naming space. The paper focuses on the design aspects of the repository tool and highlights its similarities and differences with the design principles of OMG Meta Object Fa- cility specification.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Dynamic Software Components in UML",
        "date": 1999,
        "abstract": "UML provides modeling support for static software compo- nents through hierarchical packages. We describe a small extension of UML for modeling dynamic software components which can be instan- tiated at runtime, customized, made persistent, migrated and be aggre- gated to larger components. For example, this extension can be used to describe systems built with JavaBeans, ActiveX-Controls, Voyager Agents or CORBA Objects by Value. With our extension, the lifecycle of a dynamic software component can be expressed in terms of UML. We can not only describe a system at design time, but also monitor its runtime behaviour. A re-engineering tool is presented that exploits our UML extension for a high-level visualization of the interaction between dynamic components in an object-oriented system.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending UML for\nModeling Reﬂective Software Components",
        "date": 1999,
        "abstract": "This paper describes our extension of the UML metamodel for specifying reﬂective software components. Reﬂection is a design prin- ciple that allows a system to have a representation of itself in the manner that makes it easy to adapt the system to a changing environment. It has matured to the point where it is used to address real-world problems in various areas. We describe how to document reﬂective components in the framework of UML. Our work allows for recognizing and understanding reﬂective components in the upper levels of abstraction at an earlier stage of the development process. It leverages the documentation, learning, vi- sual modeling, reuse and roundtrip development of metalevel designs. We also demonstrate the seamless model exchange between diﬀerent de- velopment tools and model continuity across development phases with application-neutral interchange formats.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Classiﬁcation of Stereotypes for Object-Oriented Modeling Languages",
        "date": 1999,
        "abstract": "The Uniﬁed Modeling Language UML and the Open Modeling Lan- guage both have introduced stereotypes as a new means for user-deﬁned exten- sions of a given base language. Stereotypes are a very powerful feature. They allow modiﬁcations ranging from slight notational changes up to the redeﬁnition of the base language. However, the power of stereotypes entails risk. Badly designed stereotypes can do harm to a modeling language. In order to exploit the beneﬁts of stereotypes and to avoid their risks, a better understanding of the nature and the properties of stereotypes is necessary.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "First-Class Extensibility for UML \u0000\nPackaging of Profiles, Stereotypes, Patterns",
        "date": 1999,
        "abstract": "We discuss a first-class extensibility mechanism for the UML based on Catalysis packages and frameworks [3]. Packages define and structure meta-model extensions for different modeling language \u0000profiles\u0000. Package frameworks support lightweight extensions like stereotypes as well as heavyweight extensions. OCL can be used to define constraints and rules for profiles and frameworks. Our approach rationalizes and consolidates some core concepts within the UML standard, uses a simple general mechanism for layering facilities onto that core in a precise and well-defined way, and offers a way to simplify and re- factor the UML specification.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML-Based Fusion Analysis",
        "date": 1999,
        "abstract": "In recent times, there has been an increased requirement for soft- ware to be distributed. The well-known Fusion development method, how- ever, can only be used to develop sequential reactive systems, and certain restricted kinds of concurrent systems. In contrast, the Unified Modeling Lan- guage (UML) provides a rich set of notations that can be used to model sys- tems that are distributed. In addition, UML provides the ability to introduce rigor into diagrams through its constraint language OCL. In this paper, we present a UML-based Fusion analysis phase by way of a simple bank case study, and we discuss some enhancements that were made in addition to a mapping of notations; our proposal is the first step towards providing a Fusion-based analysis phase which supports high-level modeling of distrib- uted systems.",
        "keywords": [
            "Fusion",
            "UML",
            "Analysis",
            "Object-Oriented  Software Development."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Framework for Describing UML Compatible Development Processes",
        "date": 1999,
        "abstract": "Have you ever tried to specify an accurate development process for your organization and later faced difficulties with the complexity of the description? Instead of describing a specific process, it might help to describe a process framework and reuse it by creating specific processes for specific needs. This paper describes the object-oriented framework of a development process, which considers software development artifacts as objects and evolution as collaborations between the objects. Such an object-oriented process definition can deal with the complexity of a development process in a better way than a traditional description based on workflow. This paper discusses features of such a process framework with an eye towards approaches such as Fusion, OPEN and the Rational Unified Process.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML-RT as a Candidate for Modeling Embedded Real-Time Systems in the Telecommunication Domain",
        "date": 1999,
        "abstract": "UML-RT oﬀers a set of extensions to the UML, which are a basis for modeling real-time systems. Some investigations have shown that UML-RT provides concepts, which are close to the constructs used at Ericsson for designing mobile switching systems in the GSM (Global System for Mobile Communication) telecommunication domain. Tele- communication systems are some of the most challenging systems regard- ing size, complexity, and real-time constraints. This article describes the main constructs used at Ericsson, which display a robust framework for building real-time systems; and shows the mapping to UML-RT. An ad- dition to UML-RT is presented; it allows UML-RT to be a candidate for modeling embedded real-time systems in the telecommunication domain.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Based Performance Modeling Framework for Object-Oriented Distributed Systems1",
        "date": 1999,
        "abstract": "As object-oriented distributed systems, e.g. those based on CORBA, Java, and DCOM, are entering the mainstream of information technology, it is increasingly important to predict and understand their performance characteris- tics. To support this need, we describe a framework that allows UML diagrams to be used for building performance models for such systems. A mapping is proposed from the high-level UML notation to queuing networks with simulta- neous resource possessions, so that the models can be solved for the relevant performance metrics. The main goal of the framework is to support performance engineering and, thus, flexibility and ease of use have been emphasized.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Defining the Context of OCL Expressions",
        "date": 1999,
        "abstract": "Expressions written in Object Constraint Language (OCL) within a UML model assume a context, depending upon where they are written. Currently the exact nature of this context is not fully defined. Furthermore there is no mechanism for defining the context for OCL expressions in extensions to UML. This paper defines the context of OCL expressions, and proposes precise and flexible mechanisms for how to specify this context.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mixing Visual and Textual Constraint Languages",
        "date": 1999,
        "abstract": "The Object Constraint Language (OCL) is a precise language for notating behavioural constraints on UML models. Constraint diagrams have been proposed as a means of notating similar constraints, but in a visual form. This paper explores the utility of these two notations for depicting constraints, and shows how they can be used effectively together. The goal of this work is to provide more intuitive and expressive languages to support the construction and presentation of rich and precise models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correct Realizations of Interface Constraints with OCL⋆",
        "date": 1999,
        "abstract": "We present an OCL-like formal notation for interface con- straints, called ICL, suited to describe the required observable behavior of any correct interface implementation (provided by some class). The semantics of the ICL notation is deﬁned by a translation to the obser- vational logic institution. For specifying constraints on classes we use a subset of OCL to express invariants and pre- and post-conditions on operations. The semantics of the OCL expressions is deﬁned by a transla- tion into an algebraic speciﬁcation. Using these semantic foundations we introduce a formal correctness notion for implementation relations be- tween interfaces and classes and we show how to prove implementation correctness by using observational proof techniques.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating Tests from UML Speciﬁcations",
        "date": 1999,
        "abstract": "Although most industry testing of complex software is con- ducted at the system level, most formal research has focused on the unit level. As a result, most system level testing techniques are only de- scribed informally. This paper presents a novel technique that adapts pre-deﬁned state-based speciﬁcation test data generation criteria to gen- erate test cases from UML statecharts. UML statecharts provide a solid basis for test generation in a form that can be easily manipulated. This technique includes coverage criteria that enable highly eﬀective tests to be developed. To demonstrate this technique, a tool has been developed that uses UML statecharts produced by Rational Software Corporation’s Rational Rose tool to generate test data. Experimental results from using this tool are presented.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "F ormalisi ng UML State Mac hines for Mo del Chec king",
        "date": 1999,
        "abstract": "The pap er discusses a complete formalisation of UML state",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Behavior: Inheritance and Implementation in Current Object-Oriented Languages",
        "date": 1999,
        "abstract": "The UML dynamic model is described using notions like state, event or active object that current object-oriented languages don't support. When the implementation is not done using a state machine interpreter, these notions had to be translated into the target language. This work aims to study how to translate as automatically as possible UML state diagrams into current object- oriented languages (OOLs), distinguishing sequential and concurrent execution. This translation requires to map UML notions onto OOLs ones, to adapt the abstract state machine, and to add information to state diagrams. Behavior inheritance is a key problem, and both theoretical and practical solutions are examined to ensure behavior substitutability. Then, two main ways for state representation are compared from the inheritance point of view, and automatic code generation is discussed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Collaboration Diagrams and Their Transformation to Java",
        "date": 1999,
        "abstract": "UML provides a variety of diagram types for specifying both the structure and the behavior of a system. During the development process, models speciﬁed by use of these diagram types have to be trans- formed into corresponding code. In the past, mainly class diagrams and state diagrams have been considered for an automatic code generation. In this paper, we focus on collaboration diagrams. As an important pre- requisite for a consistent transformation into Java code, we ﬁrst provide methodical guidelines on how to deploy collaboration diagrams to model functional behavior. This understanding yields a reﬁned meta model and forms the base for the deﬁnition of a transformation algorithm. The au- tomatically generated Java code fragments build a substantial part of the functionality and prevent the loss of important information during the transition from a model to its implementation. Keywords: Collaboration diagram, methodical guidelines, code gener- ation, Java, pattern-based transformation algorithm",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Typechecking UML Static Models",
        "date": 1999,
        "abstract": "UML static models are expressed using a mixture of class diagrams and OCL expressions. In a well formed static model, the OCL expressions and class diagrams are type consistent. Checking for type consistency of static models involves both inclusion and parametric poly- morphism. This paper deﬁnes a semantics of type consistency in terms of a type theory for UML static models. The type theory is shown to be correct with respect to a value semantics for OCL. The existence of a consistency checking algorithm for UML static models is established.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Analysing\nUML\nUse\nCases\nas\nCon\ntracts",
        "date": 1999,
        "abstract": "The Uni ed Mo deling Language (UML) consists in a set of",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Closing the Gap between Object-Oriented Modeling of Structure and Behavior",
        "date": 1999,
        "abstract": "The UML as standardized language for visual object-oriented modeling allows to capture the requirements as well as the structure and behavior of complex software systems. With the increasing demands of todays systems, behavior aspects like concurrency, distribution and re- activity become more important. But the language concepts of the UML for describing behavioral aspects are weak compared to its concepts for describing structures. Besides a lack of visual expressiveness, a deeper integration with the structure speciﬁcation is missing. In order to close this gap, an expressive language for modeling object-oriented behavior is proposed with the OCoN approach. It describes contracts, object schedul- ing as well as control and data ﬂow of services in a Petri-net-like form. A seamless visual embedding of contract speciﬁcations into service and object scheduling speciﬁcations is provided by diﬀerent net types.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Black and White Diamonds",
        "date": 1999,
        "abstract": "This study of the semantics of UML’s shared aggregation and composition (black and white diamonds) is based on previous detailed analyses of the semantics of aggregation in object modelling in which primary axioms were identiﬁed. All forms of aggregation must comply with these primary axioms. We conclude that both kinds of UML Aggre- gation do not possess the full complement of primary characteristics and that their secondary characteristics, which deﬁne various “ﬂavours” of aggregation, are overlapping and incomplete. We recommend revisions to UML’s two kinds of aggregation: completion of the primary set of axiomatic characteristics and then careful selection of secondary charac- teristics for deﬁning black and white diamond aggregation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Interconnecting Objects via Contracts",
        "date": 1999,
        "abstract": "The evolution of today's markets and the high volatility of business requirements put an increasing emphasis on the flexibility of sys- tems, i.e. on the ability for systems to accommodate the changes required by new or different organisational needs with a minimum impact on the imple- mented services. In this paper, we put forward an extension of UML with a semantic primitive – contract – for representing explicitly the rules that de- termine the way object interaction needs to be coordinated to satisfy busi- ness requirements, as well as the mechanisms that make it possible to reflect changes of the business requirements without having to modify the basic objects that compose the system. Contracts are proposed as extended forms of association classes whose semantics rely on principles that have been used in Software Architectures and Distributed System Design for supporting dynamic reconfiguration.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How Can a Subsystem Be Both a Package and a Classifier?",
        "date": 1999,
        "abstract": "The UML specifies that a subsystem is both a package and a classifier. This paper explores what that could possibly mean and explains why that was the right choice. It points out a key to the use of the concept in CASE tools, mentions the historical precedent for that key, and challenges CASE tools to support the flexibility that architects and designers need. Along the way, the paper reviews a method for discovering a good partition of a system into subsystems, describes a scheme for using UML to build a model of a system, and suggests some changes to the UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using UML/OCL Constraints for Relational Database Design",
        "date": 1999,
        "abstract": "Integrating relational databases into object-oriented appli- cations is state of the art in software development practice. In database applications, it is beneﬁcial if constraints like business rules are encoded as part of the database schema and not in the application programs. The Object Constraint Language (OCL) as part of the Uniﬁed Mod- eling Language (UML) provides the posssibility to express constraints in a conceptual model unambiguously. We show how OCL, UML and SQL can be used in database constraint modeling, and discuss their ad- vantages and limitations. Furthermore, we present patterns for mapping OCL expressions to SQL code.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards a UML Extension for Hypermedia Design⋆",
        "date": 1999,
        "abstract": "The acceptance of UML as a de facto standard for the de- sign of object-oriented systems, together with the explosive growth of the World Wide Web has raised the need for UML extensions to model hy- permedia applications running on the Internet. In this paper we propose such an extension for modeling the navigation and the user interfaces of hypermedia systems. Similar to other design methods for hypermedia systems we view the design of hypermedia systems as consisting of three models: the conceptual, navigational and presentational model. The con- ceptual model consists of a class diagram identifying the objects of the problem domain and their relations. The navigational model describes the navigation structure of the hypermedia application by a class di- agram specifying which navigational nodes are deﬁned and an object diagram showing how these navigational nodes are visited. Finally, the presentational model describes the abstract user interface by composite objects and its dynamic behavior by state diagrams. Each model is built using the notations provided by the UML, applying the extension mech- anism of the UML, i.e. stereotypes and OCL constraints, when necessary. Keywords: Uniﬁed Modeling Language, Object-Oriented Design, Mul- timedia, Hypermedia, WWW.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Why Uniﬁed Is not Universal UML Shortcomings for Coping with Round-Trip Engineering",
        "date": 1999,
        "abstract": "UML is currently embraced as “the” standard in object- oriented modeling languages, the recent work of OMG on the Meta Object Facility (MOF) being the most noteworthy example. We wel- come these standardisation eﬀorts, yet warn against the tendency to use UML as the panacea for all exchange standards. In particular, we argue that UML is not suﬃcient to serve as a tool-interoperability stan- dard for integrating round-trip engineering tools, because one is forced to rely on UML’s built-in extension mechanisms to adequately model the reality in source-code. Consequently, we propose an alternative meta- model (named FAMIX), which serves as the tool interoperability stan- dard within the FAMOOS project and which includes a number of con- structive suggestions that we hope will inﬂuence future releases of the UML and MOF standards. Keywords: Meta model, uniﬁed modeling language (UML), meta-object facility (MOF), interoperability standard, famoos information exchange (FAMIX).",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Timed Sequence Diagrams and T o ol-Based Analysis \u0015 A Case Study",
        "date": 1999,
        "abstract": "W e use UML timed Sequence Diagrams to sp ecify the real-",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Timing Analysis of UML Sequence Diagrams",
        "date": 1999,
        "abstract": "For real-time systems, UML sequence diagrams describe in- teraction among objects, which show the scenarios of system behaviour. In this paper, we give the solution for timing analysis of simple UML sequence diagrams which describe exactly one scenario without any al- ternatives and loops, and develop an algorithm for checking the compo- sitions of UML sequence diagrams, which describe multiple scenarios, for timing consistency.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Normal Object Form: \nBridging the Gap from Models to Code",
        "date": 1999,
        "abstract": "The value of graphical modeling within the analysis and design activities of object-oriented development is predicated on the assumption that the resulting models can be mapped correctly, optimally and efficiently into executable (normally textual) code. In practice, however, because of the large potential mismatch in abstraction levels, the mapping of graphical models into code is often one of the weakest and most error prone links in the chain of development steps. This paper describes a practical approach for addressing this problem based upon the definition of a restricted extension of the UML known as the Normal Object Form (NOF). The basic purpose of the NOF is to provide a set of UML modeling concepts which are \"semantically close\" to those found in object-oriented programming languages. Highly abstract UML models can then be mapped into corresponding executable code by means of a series of semantically small refinement (intra-UML) and translation (extra- UML) translation steps, rather than in one large (often ad hoc) step. This not only increases the chances of a correct and optimal mapping, but also signifi- cantly improves the traceability of UML constructs to and from code con- structs, with all the associated advantages for maintenance and reuse.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Exceptional Behavior",
        "date": 1999,
        "abstract": "While exception handling mechanisms are very useful for implementing systems, they are equally useful when building business models. The standard method of modeling exceptions in UML is to in- clude them in class diagrams as a kind of signal, as stereotyped classes. However this alone is insuﬃcient, since by the very nature of exceptions, the circumstances under which particular exceptions may be raised, as well as the details of what actions the class handling the exception will perform in doing so, tend to be rather complex and in general diﬃcult to express pictorially. In this paper, we consider how this information may be speciﬁed using the Object Constraint Language (OCL). We illustrate our approach by applying it to a simple example.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the Extension of UML \nwith Use Case Maps Concepts",
        "date": 2000,
        "abstract": "Descriptions of reactive systems focus heavily on behavioral aspects, often in terms of scenarios. To cope with the increasing complexity of services provided by these systems, behavioral aspects need to be handled early in the design process with flexible and concise notations as well as expressive con- cepts. UML offers different notations and concepts that can help describe such services. However, several necessary concepts appear to be absent from UML, but present in the Use Case Map (UCM) scenario notation. In particular, Use Case Maps allow scenarios to be mapped to different architectures composed of various component types. The notation supports structured and incremental de- velopment of complex scenarios at a high level of abstraction, as well as their integration. UCMs specify variations of run-time behavior and scenario struc- tures through sub-maps \"pluggable\" into placeholders called stubs. This paper presents how UCM concepts could be used to extend the semantics and nota- tions of UML for the modeling of complex reactive systems. Adding a \"UCM view\" to the existing UML views can help bridging the gap separating require- ments and use cases from more detailed views (e.g. expressed with interaction diagrams and statechart diagrams). Examples from telecommunications systems are given and a corresponding design trajectory is also suggested.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "HyperMSCs and Sequence Diagrams for Use Case Modelling and Testing",
        "date": 2000,
        "abstract": "UML-Sequence Diagrams can be seen as an object oriented variant of the ITU-T standard language Message Sequence Chart (MSC) which is very popular mainly in the telecommunication area. Both notations would benefit from a unification together with a further elaboration. A comparison of Sequence Diagrams and MSCs demonstrates the big advantage of MSCs con- cerning composition mechanisms, particularly with respect to the branching con- struct in Sequence Diagrams. Therefore, MSC inline expressions and High Level MSCs (HMSCs) are of special interest for the inclusion into Sequence Dia- grams. High Level MSCs may be employed for formalizing and structuring the construction of scenarios for Use Cases. In order to arrive at a most intuitive rep- resentation, HMSCs are re-interpreted in a way which has an analogy in hyper- text-like specifications. Because of this analogy, the notation ‘HyperMSC’ is introduced. The scenarios derived from Use Cases in form of HyperMSCs can be employed also as a basis for the specification of test cases.",
        "keywords": [
            ". UML",
            "MSC",
            "Sequence Diagrams",
            "Use Cases",
            "OO",
            "software engi- neering",
            "testing",
            "distributed systems",
            "real time systems",
            "telecommunication"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Business-Oriented Constraint Language",
        "date": 2000,
        "abstract": "The Business-oriented Constraint Language (BCL) is proposed as a means of annotating diagrams in UML. BCL is grounded in the Object Con- straint Language (OCL) but is designed particularly to address the needs of people who are concerned with enterprise application integration (EAI), al- though it may be more widely applicable. EAI often requires a loosely coupled event-based architecture in which timing and statistical measures are important; these are described in another paper (1). BCL provides these features together with a syntax that is flexible and extensible. It is intended to be accessible to most practitioners, including those who do not have a mathematical back- ground.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Processes, Roles, and Events:\nUML Concepts for Enterprise Architecture",
        "date": 2000,
        "abstract": "This paper presents an integrated approach for modelling enterprise architectures using UML. To satisfy a need for a wide range of modelling choices, we provide a rich set of process-based and role-based modelling concepts, together with a flexible way of associating business events with business processes and roles. Our approach enriches Unified Modelling Language (UML) to support the requirements of enterprise distributed object computing (EDOC) systems and is currently being considered by the Object Management Group (OMG) for standardisation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Statistical Constraints for EAI",
        "date": 2000,
        "abstract": "Enterprise Application Integration (EAI) often requires a loosely coupled event-based architecture in which timing and statistical measures are important. Statistical constraints are proposed to address some of these requirements, although they may well have broader applicability. This idea is extended to the use of approximate terms such as “some” that can be given a statistical interpretation, as in “Some fault reports give rise to subscriptions.” These can be a convenient way of expressing constraints that are important to the application architecture but are not absolute — abstraction through approximation. They may be used to annotate diagrams in UML and are based on the existing Object Constraint Language (OCL) (1). Some of the examples use a more flexible syntax that is the subject of another paper (2).",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards a UML Profile for Interaction Design: The Wisdom Approach",
        "date": 2000,
        "abstract": "The UML is recognized to be the dominant diagrammatic modeling language in the software industry. However, it’s support for building interac- tive systems is still acknowledged to be insufficient. There is a common mis- conception that the same models developed to support the design of the appli- cation internals are also adequate to support interaction design, leveraging the usability aspects of the applications. In this paper we identify and discuss the major problems using the UML to document, specify and design interactive systems. Here we propose a UML profile for interactive systems development that leverages on human-computer interaction domain knowledge under the common notation and semantics of the UML. Our proposal integrates with ex- isting object-oriented software engineering best practice, fostering co- evolutionary development of interactive systems and enabling artifact change between software engineering and human-computer interaction.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UMLi: The Uniﬁed Modeling Language for Interactive Applications",
        "date": 2000,
        "abstract": "User interfaces (UIs) are essential components of most soft- ware systems, and signiﬁcantly aﬀect the eﬀectiveness of installed appli- cations. In addition, UIs often represent a signiﬁcant proportion of the code delivered by a development activity. However, despite this, there are no modelling languages and tools that support contract elaboration be- tween UI developers and application developers. The Uniﬁed Modeling Language (UML) has been widely accepted by application developers, but not so much by UI designers. For this reason, this paper introduces the notation of the Uniﬁed Modelling Language for Interactive Appli- cations (UMLi), that extends UML, to provide greater support for UI design. UI elements elicited in use cases and their scenarios can be used during the design of activities and UI presentations. A diagram notation for modelling user interface presentations is introduced. Activity diagram notation is extended to describe collaboration between interaction and domain objects. Further, a case study using UMLi notation and method is presented.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Diagrammatic Tool for Representing User Interaction in UML",
        "date": 2000,
        "abstract": "The UML suggests the employment of use cases for capturing the requirements and for specifying the interaction between the users and the sys- tem being modeled. Use cases are easily understood by users since they are es- sentially textual descriptions, but lack the precision and the conciseness ac- complished by the other diagrammatic tools of UML. Besides, there is no sys- tematic method that helps the designer to obtain such UML diagrams from a set of use cases. In this paper we present a diagrammatic tool to represent the us- ers/system interaction called User Interaction Diagram (UID). UIDs have proven to be a valuable tool to gather requirements since they describe the ex- change of information between the system and the user in a high level of ab- straction, without considering specific user interface aspects and design details as in other UML diagrams. We show how UIDs can be incorporated into the requirements and analysis workflows of the Unified Process for software de- velopment.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Extension for ASAM-GDI Device Capability Description",
        "date": 2000,
        "abstract": "The ASAM standard has identiﬁed subsystems within au- tomation and measuring systems as well as standard interfaces between these subsystems. One of these interfaces, called GDI (Generic Device Interface), deﬁnes the connection to measurement devices and intelligent subsystems. ASAM-GDI ensures the interoperability of real-time subsys- tems by separating the implementation code of device drivers and their interface descriptions, called DCD (Device Capability Description). DCD describes an object-like interface of procedural real-time components (de- vice drivers) using the DCD language. In this paper it is shown how the DCD language and its constructs can be mapped to UML notation using the standard UML extension mechanisms. Advantages of modeling DCD using UML are numerous: uniform and standard graphical representa- tion of device capabilities, improvement in DCD development, straight- forward extension of DCD using UML notation, standard exchange text format of DCD documents using XMI (XML Metadata Interchange), etc. The deﬁnition of UML extensions for DCD contributes to the acceptance of the ASAM-GDI standard and simpliﬁes the development of ASAM- GDI tools in the future. It is also an example how similar constructs, i.e. blocks with inputs and outputs, can be speciﬁed using UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Swinging UML How to Make Class Diagrams and State Machines Amenable to Constraint Solving and Proving",
        "date": 2000,
        "abstract": "Swinging types (STs) provide a speciﬁcation and veriﬁcation formalism for designing software in terms of many-sorted logic. Current formalisms, be they set- or order-theoretic, algebraic or coalgebraic, rule- or net-based, handle either static system components (in terms of func- tions or relations) or dynamic ones (in terms of transition systems) and either structural or behavioral aspects, while STs combine equational, Horn and modal logic for the purpose of applying computation and proof rules from all three logics. UML provides a collection of object-oriented pictorial speciﬁcation tech- niques, equipped with an informal semantics, but hardly cares about consistency, i.e. the guarantee that a speciﬁcation has models and thus can be implemented. To achieve this goal and to make veriﬁcation possi- ble a formal semantics is indispensable. Swinging types have term models that are directly derived from the speciﬁcations. The paper takes ﬁrst steps towards a translation of class diagrams, OCL constraints and state machines into STs. Partly, we proceed along examples, partly we describe generally how, e.g., classes can be turned into signatures. Swinging types are particularly suitable for interpreting UML models because they integrate static and dynamic components. UML treats them separately, STs handle them within the same formalism. Hence, one may check, for instance, whether static operations are correctly reﬁned to local message passing primitives. A crucial point of a formal semantics of UML models is a reasonable notion of state. If constraints involve static data as well as states and state transitions, the modal-logic view on states as (implicit) predicates is less adequate than the ST representation as terms denoting tuples of attribute values, “histories” of object manipulations or compositions of substates (composite states).",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Based Performance Modeling of Distributed Systems",
        "date": 2000,
        "abstract": "The development of distributed software systems satisfying performance requirements is achievable only spending careful attention to performance goals throughout the lifecycle, and especially from its very beginning. The aim of our approach is to encompass the perfor- mance validation task as an integrated activity within the development process of distributed systems. To this end we consider object oriented distributed systems based on UML, the Uniﬁed Modeling Language. We show how a system modeled by UML diagrams can be translated into a queueing network based performance model. The main contribution of this work consists of an extensive application to a case study of our methodological approach for the automatic generation of performance models. The considered case study falls in the domain of distributed software systems, where the proposed methodology suitably exploits and combines information derived from diﬀerent UML diagrams to generate a quite accurate performance model.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Radical Revision of UML’s Role Concept",
        "date": 2000,
        "abstract": "UML’s current definition of the role concept comes with many problems, not the least being that it is difficult to understand and communi- cate. This paper proposes a revised UML metamodel building on a much sim- pler role definition. Moreover, it replaces the rather unusual notions of association role and association end role as well as the rarely used association generalization with the more popular concept of overloading, thereby leading to a considerable reduction in the number of modelling concepts. Despite the rather radical nature of the proposed alterations, no changes in UML notation become necessary. However, a notable change in modelling style including in particular a clearer separation of structure and interaction diagrams are among the likely effects of the proposed revision.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ensuring Quality of Geographic Data with UML and OCL⋆",
        "date": 2000,
        "abstract": "Geographic data is the backbone of sophisticated applica- tions such as car navigation systems and Geographic Information Sys- tems (GIS). Complexity quickly arises in the production of geographic data when trying to ensure quality. We deﬁne quality as the integrity and well-formedness of the contents of the geographic data, usually enforced by external applications where constraints ensuring quality (referred to as quality constraints) are implicit, low-level and scattered throughout the application code. This has signiﬁcant consequences with respect to manageability, adaptability and reuse of these constraints. This paper explains our use of UML class diagrams as conceptual model for geographic data, and how we exploited the Object Constraint Lan- guage (OCL) for describing the quality constraints in an explicit, declar- ative and high-level way. As our use of OCL is slightly diﬀerent than it was originally intended, we present our adaptations and explain the main issues of evaluating the resulting OCL. We are conﬁdent that our speciﬁc application of OCL can be put to use in other domains where complex constraints need to be expressed in a knowledge-oriented domain.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Contextual Diagrams as Structuring Mechanisms for Designing Conﬁguration Knowledge Bases in UML",
        "date": 2000,
        "abstract": "Lower prices, shorter product cycles, and the customer indi- vidual production of highly variant products are the main reasons for the success of product conﬁguration systems in various application domains (telecommunication industry, automotive industry, computer industry). In this paper we show how to employ UML in order to design complex conﬁguration knowledge bases. We introduce the notion of contextual diagrams in order to cope with the intrinsic complexity of conﬁgura- tion knowledge. Since domain experts mostly think in terms of contexts, this approach leads to a more intuitive way of modeling conﬁguration knowledge.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The UML Family: Profiles, Prefaces and Packages",
        "date": 2000,
        "abstract": "This paper overviews the status of UML (Unified Modeling Language) considered as a family of languages, and reviews critically various approaches to defining variants of UML within this family.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Validating UML Models and OCL Constraints",
        "date": 2000,
        "abstract": "The UML has been widely accepted as a standard for mod- eling software systems and is supported by a great number of CASE tools. However, UML tools often provide only little support for vali- dating models early during the design stage. Also, there is generally no substantial support for constraints written in the Object Constraint Lan- guage (OCL). We present an approach for the validation of UML models and OCL constraints that is based on animation. The USE tool (UML- based Speciﬁcation Environment) supports developers in this process. It has an animator for simulating UML models and an OCL interpreter for constraint checking. Snapshots of a running system can be created, in- spected, and checked for conformance with the model. As a special case study, we have applied the tool to parts of the UML 1.3 metamodel and its well-formedness rules. The tool enabled a thorough and systematic check of the OCL well-formedness rules in the UML standard.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modular Architecture for a Toolset Supporting OCL",
        "date": 2000,
        "abstract": "The practical application of the Object Constraint Language, which is part of the UML speciﬁcation since version 1.1, depends cru- cially on the existence of adequate tool support. This paper discusses general design issues for OCL tools. It is argued that the nature of OCL will lead to a large variety of tools, applied in combination with a variety of diﬀerent UML tools. Therefore, a ﬂexible modular architecture for a UML/OCL toolset is proposed. The paper reports on the ﬁrst results of an ongoing project which aims at the provision of such an OCL toolset for the public domain.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistency Checking and Visualization of OCL Constraints⋆",
        "date": 2000,
        "abstract": "Part of the success of the Uniﬁed Modeling Language (UML) as a speciﬁcation language is due to its diagrammatic nature. Its mean- ing is expressed by its meta model, a combination of class diagrams and constraints written in the Object Constraint Language (OCL), a textual language of expressions. Recent eﬀorts have tried to give a formal seman- tics to OCL in a classical way. In this paper, we propose a graph-based semantics for OCL and a systematic translation of OCL constraints into expressions over graph rules. Besides providing a semantical formaliza- tion of OCL, this translation can be employed to check the consistency of UML model instances wrt. the constraints , using a general purpose graph transformation machine like AGG or PROGRES. The translation of OCL constraints into graph rules suggests a way to express the con- straints in a more intuitive visual form.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Strict Profiles: Why and How",
        "date": 2000,
        "abstract": "The definition of a clean profile mechanism will play a crucial role in the UML's future in terms of how useful it will be to modellers and how well tool vendors may implement the new facilities. Unfortunately, in an attempt to restrict profile definitions to a single meta level, predefined modeling elements are currently specified exclusively at the meta-model level, and therefore can be applied solely through the mechanism of meta-instantiation. We identify the problems associated with such a restriction and explain why model level inheri- tance also has a role to play in the definition of predefined modeling elements. We point out the fundamental differences and relationships between the two mechanisms in the context of defining UML profiles and provide guidelines as to which mechanism should be used under which circumstance. We conclude by describing the necessity for the use of both mechanisms in the definition of UML profiles within a strict metamodeling framework.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dynamic Meta Modeling: A Graphical Approach to the Operational Semantics of Behavioral Diagrams in UML",
        "date": 2000,
        "abstract": "In this paper, dynamic meta modeling is proposed as a new approach to the operational semantics of behavioral UML diagrams. The dynamic meta model extends the well-known static meta model by a speciﬁcation of the system’s dynamics by means of collaboration dia- grams. In this way, it is possible to deﬁne the behavior of UML diagrams within UML. The conceptual idea is inherited from Plotkin’s structured operational semantics (SOS) paradigm, a style of semantics speciﬁcation for concur- rent programming languages and process calculi: Collaboration diagrams are used as deduction rules to specify a goal-oriented interpreter for the language. The approach is exempliﬁed using a fragment of UML state- chart and object diagrams. Formally, collaboration diagrams are interpreted as graph transformation rules. In this way, dynamic UML semantics can be both mathematically rigorous so as to enable formal speciﬁcations and proofs and, due to the use of UML notation, understandable without prior knowledge of heavy mathematic machinery. Thus, it can be used as a reference by tool developers, teachers, and advanced users.",
        "keywords": [
            "UML meta model",
            "statechart diagrams",
            "precise behavioral semantics",
            "graph transformation"
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Interacting Subsystems in UML",
        "date": 2000,
        "abstract": "In this paper we give a description of the subsystem con- struct in the Uniﬁed Modeling Language, emphasizing its dynamic as- pects, thus giving a detailed description of the semantics of interaction with subsystems. Depending on whether the surroundings of the subsys- tem make use of public elements in the subsystem or not, the subsystem is considered to be open or closed, respectively. This leads to two diﬀer- ent ways to use the services of the subsystem: either importing it and directly accessing its public elements, or associating it and only com- municating with the subsystem itself. We also discuss some implications which closed subsystems have on collaborations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistent Behaviour Representation in Activity and Sequence Diagrams",
        "date": 2000,
        "abstract": "The paper proposes a formal approach for constructing UML activity diagrams from sequence diagrams by using graph transformations. Activity diagrams are good at describing the overall flow of control in a system, as they provide support for conditional and parallel behaviour, but do not capture well object interactions. Activity diagrams are mostly used in the preliminary stages of analysis and design. As the design progresses, more detailed descriptions of object interactions become necessary, and interaction diagrams are used for this purpose. During the transition from a high level to a detailed design, the mapping between the behavior represented in activity diagrams and that described in interaction diagrams may be lost, and the two views may become inconsistent. By reconstructing the activity diagrams from sequence diagrams, consistency is re-enforced. Every activity block is cross-referenced with the corresponding sequence diagram messages, which helps designers to correlate the two views. The transformation from sequence to activity diagrams is based on PROGRES, a known visual language and environment for programming with graph rewriting systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "PROGRES"
        }
    },
    {
        "title": "Using UML Collaboration Diagrams for Static Checking and Test Generation",
        "date": 2000,
        "abstract": "Software testing can only be formalized and quantiﬁed when a solid basis for test generation can be deﬁned. Tests are commonly gen- erated from program source code, graphical models of software (such as control ﬂow graphs), and speciﬁcations/requirements. UML collabo- ration diagrams represent a signiﬁcant opportunity for testing because they precisely describe how the functions the software provides are con- nected in a form that can be easily manipulated by automated means. This paper presents novel test criteria that are based on UML collab- oration diagrams. The most novel aspect of this is that tests can be generated automatically from the software design, rather than the code or the speciﬁcations. Criteria are deﬁned for both static and dynamic testing of speciﬁcation-level and instance-level collaboration diagrams. These criteria allow a formal integration tests to be based on high level design notations, which can help lead to software that is signiﬁcantly more reliable.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting Several Levels of Restriction in the UML",
        "date": 2000,
        "abstract": "The emergence of the Unified Modeling Language (UML) has provided software developers with an effective and efficient shared language. However, UML is often too restrictive in initial, informal, and creative modelling, and it is in some cases not restrictive enough, e.g., for code generation. Based on user studies, we propose that tool and meta-level support for several levels of restriction in diagrams and models is needed. We furthermore present a tool, Knight, which supports several levels of restriction as well as ways of transferring models from one level of restriction to another. This approach potentially increases the usability of the UML, and thus ultimately leads to greater quality and adoption of UML models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML-Based Methodology for Hypermedia Design",
        "date": 2000,
        "abstract": "We propose a methodology for hypermedia design which is based on a UML profile for the hypermedia domain. Starting with a use case analysis and a conceptual model of the application we first provide guidelines for modeling the navigation space. From the navigation space model we can derive, in a next step, a navigational structure model which shows how to navigate through the navigation space using access elements like indexes, guided tours, queries and menus. Finally, a presentation model is constructed that can be directly implemented by HTML frames. The different models of the design process are represented by using a hypermedia extension of UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Object Oriented Methodology Based on UML for Urban Traffic System Modeling",
        "date": 2000,
        "abstract": "This paper proposes an object-oriented modeling methodology, which is based on global and structured UTS modeling approach, using UML We show how UML diagrams are used in our methodology and why. The first part of this paper is dedicated to the domain analysis (generic) and shows which diagram to use. In the second part, we deal with system dedicated analysis. We then present a real case study we have dealt with.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending OCL to Include Actions",
        "date": 2000,
        "abstract": "The UML’s Object Constraint Language provides the modeller of object-oriented systems with ways to express the semantics of a model in a precise and declarative manner. The constraints which can be expressed in this language, all state requirements on the static aspects of the system. The Object Constraint Language currently lacks a way to express that events have happened or will happen, that signals are or will be send, or that operations are or will be called.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Structured Approach to Develop Concurrent Programs in UML ⋆",
        "date": 2000,
        "abstract": "This paper presents a methodology to develop synchroniza- tion code based on the global invariant (GI) approach in the context of the Uniﬁed Process in UML. This approach has the following advantages: (1) it is a formal approach that enables formal veriﬁcation of programs being developed, (2) the most important activity in the programming process lies at a high level; namely, speciﬁcation of GIs, (3) GIs are plat- form independent, and (4) existing GIs may be composed to produce GIs for more complex synchronization. We provide a set of useful GIs which work as basic patterns. Programmers can compose these GIs to produce appropriate GIs for speciﬁc applications.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Describing AI Analysis Patterns with UML",
        "date": 2000,
        "abstract": "We discuss the use of the UML to describe “Analysis Patterns” in AI, an area where OAD techniques are not widely used, in spite of the fact that some of the inspiration for the object approach can be traced to developments in this area. We study the relation between the notion of analysis pattern in the context of OO software development methods, and that of Generic Task in AI software development methods such as CommonKADS. Our interest is motivated by the belief that in the analysis and design of certain AI applications, particularly in Distributed AI, OO style patterns may be more appropriate than Generic Tasks. To illustrate the relation between these concepts, we provide a UML description of the heuristic multiattribute decision pattern, a corresponding Generic Task having already been proposed in the literature. We illustrate the wide applicability of this pattern by specialising it to obtain a therapy decision pattern. We discuss the suitability of the UML, together with OCL, for describing this and other analysis patterns arising in AI.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Precise Modeling of Design Patterns",
        "date": 2000,
        "abstract": "Design Patterns are now widely accepted as a useful con- cept for guiding and documenting the design of object-oriented software systems. Still the UML is ill-equipped for precisely representing design patterns. It is true that some graphical annotations related to parame- terized collaborations can be drawn on a UML model, but even the most classical GoF patterns, such as Observer, Composite or Visitor cannot be modeled precisely this way. We thus propose a minimal set of modiﬁ- cations to the UML 1.3 meta-model to make it possible to model design patterns and represent their occurrences in UML, opening the way for some automatic processing of pattern applications within CASE tools. We illustrate our proposal by showing how the Visitor and Observer patterns can be precisely modeled and combined together using our UM- LAUT tool. We conclude on the generality of our approach, as well as its perspectives in the context of the deﬁnition of UML 2.0.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Architectural Patterns for Metamodeling:  \nThe Hitchhiker’s Guide to the UML Metaverse",
        "date": 2000,
        "abstract": "Metamodels are playing an increasingly important role in the specification of distributed object and component architectures, such as CORBA, CORBA Component Model, Enterprise JavaBeans and DCOM/COM+. By recursively abstracting the details associated with implementation, metamodels improve rigor and facilitate system integration and interoperability. The uses of metamodels range from specifying modeling languages and metadata repositories to defining data interchange formats and software processes (methods).",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reconciling the Needs of Architectural Description with Object-Modeling Notations",
        "date": 2000,
        "abstract": "Complex software systems require expressive notations for representing their software architectures. Two competing paths have emerged. One is to use a specialized notation for architecture – or architecture description language (ADL). The other is to adapt a general-purpose modeling notation, such as UML. The latter has a number of benefits, including familiarity to developers, close mapping to im- plementations, and commercial tool support. However, it remains an open question as to how best to use object-oriented notations for architectural description, and, indeed, whether they are sufficiently expressive, as currently defined. In this paper we take a systematic look at these questions, examining the space of possible mappings from ADLs into object notations. Specifically, we describe (a) the principle strategies for representing architectural structure in UML; (b) the benefits and limitations of each strategy; and (c) aspects of architectural description that are intrinsically difficult to model in UML using the strategies.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards a UML Profile for Software Architecture Descriptions",
        "date": 2000,
        "abstract": "To formally describe architectures of software systems, specific lan- guages called Architecture Description Languages (ADLs) have been developed by academic institutions and research labs. However, more and more research and industrial projects are using the standard Unified Modeling Language (UML) for representing software architectures of systems. In this paper, we fo- cus on how to extend the UML by incorporating some key abstractions found in current ADLs, such as connectors, components and configurations, and how the UML can be used for modeling architectural viewpoints. Our approach is dem- onstrated by the software architecture of a video surveillance system. It is there- fore the purpose of the paper to show that a UML profile for software architec- ture abstractions is needed. Keywords: Software architecture abstractions, software architecture descrip- tion, architectural modeling, architectural viewpoint, architectural view, ADL, UML, connector, component, configuration.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Rewrite Rules and Operational Semantics for Model Checking UML Statecharts",
        "date": 2000,
        "abstract": "Model checking of UML statecharts is the main concern of this pa- per. To model check it, however, its description has to be translated into the in- put language of the model checker SMV. For the purpose of translating UML statecharts as closely as possible into SMV, we use rewrite rules and its opera- tional semantics.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Part-Whole Statecharts for the Explicit\nRepresentation of Compound Behaviours",
        "date": 2000,
        "abstract": "Although very eﬀective, the adoption of Statecharts in object- oriented software development methods poses many problems, since their way to compose behavioral abstractions can be framed in the general con- text of implicit composition. In particular, the need to embed references from one behavioral description to other ones has mayor drawbacks since the description of a single entity behaviour is not self-contained, and the global behaviour results implicitly deﬁned by following references from one entity to the other. In other words, both single and global behav- iors are diﬃcult to understand, modify and reuse. The paper proposes to overcome most of such problems by adopting Part-Whole Statecharts, whose primary policy for controlling complexity strictly enforces distinct layers for wholes and their parts. Since wholes may become parts of other aggregations, a recursive syntax and semantics can be given straightfor- wardly.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling of Architectures with UML Panel",
        "date": 2000,
        "abstract": "A critical level of abstraction in the modeling of a large, complex system is its architecture. At an architectural level one models the principal system elements and their interaction. Architectural models are typically used to provide an intellectually tractable, birds-eye view of a system and to permit design-time reasoning about system-level concerns such as performance, reliability, portability, and conformance to external standards and architectural styles. In practice most architectural descriptions are informal documents. They are usually centered on box-and-line diagrams, with explanatory prose. Visual conventions are idiosyncratic, and usually project specific. As a result, architectural descriptions are only vaguely understood by developers, they cannot be analyzed for consistency or completeness, they are only hypothetically related to implementations, their properties cannot be enforced as a system evolves, and they cannot be supported by tools to help software architects with their tasks. There exist several architecture description languages, but we are interested in the use of UML. We aim to identify requirements on architectural modeling and how different modeling concepts of UML meet these requirements. This paper is not intended as a critique of the UML but as a discussion of approaches to modeling architectures that have been tried, more or less successfully.*",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Preacher at Arrakeen",
        "date": 2001,
        "abstract": "In the Dune novels, Paul Atreides ﬁghts a battle for sur- vival against nefarious forces and succeeds in uniting the Universe under his control. Eventually, however, a bureaucratic and militaristic religion grows up around his legend. Disillusioned by the atrocities committed in his name, Paul abandons his throne and returns in disguise as the mys- terious Preacher at Arrakeen to denounce the bureaucracy, fanaticism, and tyranny of his out-of-control followers. Sometimes that’s how I feel about UML. This talk (sermon?) will denounce the excesses of the UML cult and see if it can be saved from its friends.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Action Semantics for MML",
        "date": 2001,
        "abstract": "This paper describes an action semantics for UML based on the Meta-Modelling Language (MML) - a precise meta-modelling lan- guage designed for developing families of UML languages. Actions are deﬁned as computational procedures with side-eﬀects. The action seman- tics are described in the MML style, with model, instance and semantic packages. Diﬀerent actions are described as specializations of the basic action in their own package. The aim is to show that by using a Cataly- sis like package extension mechanism, with precise mappings to a simple semantic domain, a well-structured and extensible model for an action language can be obtained.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Essence of Multilevel Metamodeling",
        "date": 2001,
        "abstract": "As the UML attempts to make the transition from a sin- gle, albeit extensible, language to a framework for a family of languages, the nature and form of the underlying meta-modeling architecture will assume growing importance. It is generally recognized that without a simple, clean and intuitive theory of how metamodel levels are created and related to one another, the UML 2.0 vision of a coherent family of languages with a common core set of concepts will remain elusive. However, no entirely satisfactory metamodeling approach has yet been found. Current (meta-)modeling theories used or proposed for the UML all have at least one fundamental problem that makes them unsuitable in their present form. In this paper we bring these problems into focus, and present some fundamental principles for overcoming them. We be- lieve that these principles need to be embodied within the metamodeling framework ultimately adopted for the UML 2.0 standard.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mapping between Levels in the Metamodel\nArchitecture",
        "date": 2001,
        "abstract": "The Meta-Modeling Language is a static object-oriented modeling language whose focus is the declarative definition of languages. It aims to enable the UML metamodel to be precisely defined, and to enable UML to evolve into a family of languages. This paper argues that although MML takes a metamodeling approach to language definition, it cannot be described as strict metamodeling. This has significant implications for the nature of the metamodel architecture it supports, yet without contravening the OMG’s requirements for the UML 2.0 infrastructure. In particular it supports a rich generic nested architecture as opposed to the linear architecture that strict metamodeling imposes. In this nested architecture, the transformation of any model between its representations at two adjacent metalevels can be described by an information preserving one-to-one mapping. This mapping, which can itself be defined in UML, provides the basis for a powerful area of functionality that any potential metamodeling tool should seek to exploit.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Execution Algorithm for UML Activity Graphs",
        "date": 2001,
        "abstract": "We present a real-time execution semantics for UML activity graphs that is intended for workﬂow modelling. The semantics is deﬁned in terms of execution algorithms that deﬁne how components of a work- ﬂow system execute an activity graph. The semantics stays close to the semantics of UML state machines, but diﬀers from it in some minor points. Our semantics deals with real time. The semantics provides a basis for veriﬁcation of UML activity graphs, for example using model checking, and also for executing UML activity graphs using simulation tools. We illustrate an execution by means of a small example.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Timing Analysis of UML Activity Diagrams⋆",
        "date": 2001,
        "abstract": "UML activity diagrams can be used for modeling the dy- namic aspects of systems and for constructing executable systems through forward and reverse engineering. They are very suitable for de- scribing the model of program behaviour. In this paper, we extend UML activity diagrams by introducing timing constraints so that they can be used to model real-time software systems, and give the solution for tim- ing analysis of UML activity diagrams. We give the solution for timing analysis of simple UML activity diagrams (containing no loop) by linear programming, and present an algorithm for checking UML activity dia- grams using integer time veriﬁcation techniques. This work forms a base for veriﬁcation of real-time software systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Activity Diagrams as a Workﬂow Speciﬁcation Language",
        "date": 2001,
        "abstract": "If UML activity diagrams are to succeed as a standard in the area of organisational process modeling, they need to compare well to alternative languages such as those provided by commercial Workﬂow Management Systems. This paper examines the expressiveness and the adequacy of activity diagrams for workﬂow speciﬁcation, by systemati- cally evaluating their ability to capture a collection of workﬂow patterns. This analysis provides insights into the relative strengths and weaknesses of activity diagrams. In particular, it is shown that, given an appropriate clariﬁcation of their semantics, activity diagrams are able to capture situ- ations arising in practice, which cannot be captured by most commercial Workﬂow Management Systems. On the other hand, the study shows that activity diagrams fail to capture some useful situations, thereby suggesting directions for improvement.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Querying UML Data Models with OCL",
        "date": 2001,
        "abstract": "UML is the de-facto standard language for Object-Oriented analysis and design of information systems. Persistent storage and extraction of data in such systems is supported by databases and query languages. UML sustains many aspects of software engineering; however, it does not provide explicit facility for writing queries. It is crucial for any such query language to have, at least, the expressive power of Relational Algebra, which serves as a benchmark for evaluating its expressiveness. The combination of UML and OCL can form queries with the required expressive power. However, certain extensions to OCL are essential if it is to be used effectively as a Query Language. The adoption of the ideas presented in this paper will enable query expressions to be written using OCL, that are elegant and ideally suited for use in conjunction with UML data models. This technique is illustrated by expressing the UML equivalent of an example Relational data model and associated query expressions.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OCL as a Speciﬁcation Language for Business Rules in Database Applications",
        "date": 2001,
        "abstract": "Business rules are often speciﬁed only implicitly by appli- cations to express user-deﬁned constraints. OCL provides the chance to explicitly and automatically deal with business rules when building object-oriented applications. We investigate how OCL constraints can be handled in database applications as one of the most important kind of business applications. Based on our OCL toolset prototype and ear- lier research work we particularly experiment with various strategies for the evaluation of OCL constraints in object-oriented applications which use relational databases. For this work, a ﬂexible SQL code generator is needed which can be used and adapted for diﬀerent relational database systems and diﬀerent object-to-table mappings. We implement such a database tool as an additional module for our OCL toolset using XML techniques.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Formal Semantics for OCL 1.4",
        "date": 2001,
        "abstract": "The OCL 1.4 speciﬁcation introduces let-declarations for adding auxiliary class features in static structures of the UML. We pro- vide a type inference system and a big-step operational semantics for the OCL 1.4 that treat UML static structures and UML object models abstractly and accommodate for additional declarations; the operational semantics satisﬁes a subject reduction property with respect to the type inference system. We also discuss an alternative, non-operational inter- pretation of let-declarations as constraints.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Refactoring UML Models",
        "date": 2001,
        "abstract": "Software developers spend most of their time modifying and maintaining existing products. This is because systems, and consequently their design, are in perpetual evolution before they die. Nevertheless, dealing with this evolution is a complex task. Before evolving a sys- tem, structural modiﬁcations are often required. The goal of this kind of modiﬁcation is to make certain elements more extensible, permitting the addition of new features. However, designers are seldom able to evaluate the impact, on the whole model, of a single modiﬁcation. That is, they cannot precisely verify if a change modiﬁes the behavior of the modeled system. A possible solution for this problem is to provide designers with a set of basic transformations, which can ensure behavior preservation. These transformations, also known as refactorings, can then be used, step by step, to improve the design of the system. In this paper we present a set of refactorings and explain how they can be designed so as to preserve the behavior of a UML model. Some of these refactorings are illustrated with examples.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Support for Designing Software Systems as a\nComposition of Design Patterns",
        "date": 2001,
        "abstract": "Much of the research work on design patterns has primarily focused on discovering and documenting patterns. Design patterns promise early reuse benefits at the design stage. To reap the benefits of deploying these proven de- sign solutions, we need to develop techniques to construct applications using patterns. These techniques should define a composition mechanism by which patterns can be integrated and deployed in the design of software applications. Versatile design models should be used to model the patterns themselves as well as their composition. In this paper, we describe an approach called Pattern- Oriented Analysis and Design (POAD) that utilizes UML modeling capabilities to compose design patterns at various levels of abstractions. In POAD, the in- ternal details of the pattern structure are hidden at high design levels (pattern views) and are revealed at lower design levels (class views). We define three hierarchical traceable logical views based on UML models for developing pat- tern-oriented designs; namely the Pattern-Level view, the Pattern Interfaces view, and the Detailed Pattern-Level view. The discussion is illustrated by a case study of building a framework for feedback control systems.",
        "keywords": [
            "Pattern-Oriented Design",
            "Design Patterns",
            "and Pattern Composition."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating the ConcernBASE Approach with SADL",
        "date": 2001,
        "abstract": "We describe ConcernBASE, a UML-based approach that is an instanti- ation of the IEEE’s Conceptual Framework (Std 1471) for describing software architectures. We show how the approach supports advanced separation of con- cerns in software architecture by allowing one to identify and define multiple viewpoints, concern spaces and views of an architecture. Our work focuses on integrating the ConcernBASE approach with the Structural Architecture Descrip- tion Language (SADL) in order to make the verification capabilities of SADL available to those who develop in UML. The result is a UML profile for structural description of software architecture. The paper also presents a prototype tool that supports this UML profile.",
        "keywords": [
            "Software Architecture",
            "Unified Modeling Language",
            "UML",
            "Structural Architecture Description",
            "SADL",
            "Advanced Separation of Concerns."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Message Paradigm in Object-Oriented Analysis",
        "date": 2001,
        "abstract": "The message paradigm is one of the most specific concepts of object orientation. This paradigm works well as long as one object is involved. When more than one object is involved, a choice has to be made with which type the message will be associated. In our opinion, this choice has to be postponed during object-oriented analysis. We propose to extend the concept of the message paradigm to messages with more than one implicit argument. Postponing the choice results in one model for one reality. Another problem rises when no object is involved. In our opinion this issue can best be tackled by introducing a domain layer and a functionality layer.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML-Based Approach to System Testing",
        "date": 2001,
        "abstract": "System testing is concerned with testing an entire system based on its specifications. In the context of object-oriented, UML development, this means that system test requirements are derived from UML analysis artifacts such as use cases, their corresponding sequence and collaboration diagrams, class diagrams, and possibly the use of the Object Constraint Language across all these artifacts. Our goal is to support the derivation of test requirements, which will be transformed into test cases, test oracles, and test drivers once we have detailed design information. Another important issue we address is the one of testability. Testability requirements (or rules) need to be imposed on UML artifacts so as to be able to support system testing efficiently. Those testability requirements result from a trade-off between analysis and design overhead and improved testability. The potential for automation is also an overriding concern all across our work as the ultimate goal is to fully support testing activities with high-capability tools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Modelling and Performance Analysis\nof Mobile Software Architectures",
        "date": 2001,
        "abstract": "Modern distributed software applications generally operate in complex and heterogeneous computing environments (like the World Wide Web). Different paradigms (client-server, mobility based, etc.) have been suggested and adopted to cope with the complexity of designing the software architecture of distributed applications for such environments, and deciding the \"best\" paradigm is a typical choice to be made in the very early software design phases. Several factors should drive this choice, one of them being the impact of the adopted paradigm on the application performance. Within this framework, the contribute of this paper is twofold: we suggest an extension of UML to best modeling the possible adoption of mobility-based paradigms in the software architecture of an application; we introduce a complete methodology that, starting from a software architecture described using this extended notation, generates a performance model (namely a Markov Reward or Decision Process) that allows the designer to evaluate the convenience of introducing logical mobility into a software application.",
        "keywords": [
            ".  Distributed  systems",
            "Architecture  modelling",
            "Extensions",
            "Performance analysis",
            "Mobile components"
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending UML for Object-Relational Database Design",
        "date": 2001,
        "abstract": "The most common way of designing databases is using de E/R model without taking into account other views of the system. However, new object-oriented design languages, such as UML (Unified Modelling Language), permit modelling the full system, including the database schema, in a uniform way. Besides, as UML is an extensible language, it allows introducing new stereotypes for specific applications if it is needed. There are some proposals to extend UML with stereotypes for database design but, unfortunately, they are focused on relational databases. However, new applications require representing complex objects related with complex relationships and object-relational databases are more appropriated to support the new application requirements. The framework of this paper is an Object- Relational Database Design Methodology. The methodology defines new UML stereotypes for Object-Relational Database Design and proposes some guidelines to translate an UML schema into an object-relational one. The guidelines are based on the SQL:1999 object-relational model and on Oracle8i as an example of product. In this paper we focus on the UML extensions required for object-relational database design.",
        "keywords": [
            "UML extensions",
            "Stereotypes",
            "Database Design",
            "Object- Relational Databases",
            "Design Methodology",
            "UML",
            "SQL1999",
            "Oracle8i"
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Understanding UML – Pains and Rewards",
        "date": 2001,
        "abstract": "UML is there – it’s accepted, it’s booming, and even more: it’s a standard. From telecom to train systems to avionics: using UML to capture the system is “in”. But do we really understand what we model? This talk takes for granted, that models are alive, are executable, are used to explore the design space, are used to communicate design decisions, and ultimately are evolving to target code. And it asks plenty of nasty questions about the meaning of all these diagrams, which are so intuitive, but which require clariﬁcation if viewed from the most rigorous possible perspective – that of a formal semantics. Formal Semantics are to modeling languages what X rays are to the human body: they bring to the surface problem spots not typically seen – and this process is painful. It shows, that what we see, is possibly far from what we expect: it highlights design decisions in giving a rigorous semantics to UML, which could have signiﬁcant impact on e.g. meeting timeliness requirements. But it also shows the rewards derivable from this painful exercise: giv- ing a rigorous semantics oﬀers the ﬂoor for powerful analysis techniques allowing to boost the quality of models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Formal Semantics of UML State Machines\nBased on Structured Graph Transformation⋆",
        "date": 2001,
        "abstract": "UML state machines are quite popular and useful to specify dynamic components of software systems. They have a formal static se- mantics but their execution semantics is described only informally. Graph transformation, on the other hand, constitutes a well-studied area with many theoretical results and practical application domains. In this paper, an operational semantics for a subset of UML state machines is proposed which is based on graph transformation. In more detail, a UML state ma- chine is described as a structured graph transformation system in such a way that the wellformedness rules of UML state machines are satisﬁed and the ﬁring of a (maximum) set of enabled non-conﬂicting transitions corresponds to the application of a graph transformation rule. The pre- sented approach uses the concept of transformation units, a recently developed modularization concept for graph transformation systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Visualization of OCL Using Collaborations⋆",
        "date": 2001,
        "abstract": "We propose a visualization of OCL within the context of the UML meta model, so that OCL expressions are represented by extending collaboration diagrams. We exploit the OCL meta model introduced in [9] and further elaborated on in [1] and base the description of properties of objects on collaborations, while classiﬁer and association roles are used to describe navigation paths. Operations computing properties are described by interactions consisting of messages between classiﬁer roles. The introduction of new graphical core elements is kept to a minimum. New notation mainly concerns the predeﬁned operations in OCL and provides more convenient visual forms for the notation by interactions here. The proposed visualization is described in detail and is illustrated with examples taken from an industrial project under development.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Rule-Based Speciﬁcation of Behavioral\nConsistency Based on the UML Meta-model",
        "date": 2001,
        "abstract": "Object-oriented modeling favors the modeling of object behavior from diﬀerent viewpoints and at diﬀerent levels of abstrac- tion. This gives rise to consistency problems between overlapping or semantically related submodels. The absence of a formal semantics for the UML and the numerous ways of employing the language within the development process lead to a number of diﬀerent consistency notions. Therefore, general meta-level techniques are required for specifying, analyzing, and communicating consistency constraints. In this paper, we discuss the issue of consistency of behavioral models in the UML and present techniques for specifying and analyzing consistency. Using meta-model rules we transform elements of UML models into a semantic domain. Then, consistency constraints can by speciﬁed and validated using the language and the tools of the semantic domain. This general methodology is exempliﬁed by the problem of protocol statechart inheritance.",
        "keywords": [
            "meta modeling",
            "model veriﬁcation",
            "behavioral consistency"
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A New UML Profile for Real-Time System Formal\nDesign and Validation",
        "date": 2001,
        "abstract": "UML solutions in competition on the real-time system market share three common drawbacks: an incomplete formal semantics, temporal operators with limited expression and analysis power, and implementation-oriented tools with limited verification capabilities. To overcome these limitations, the paper proposes a UML profile designed with real-time system validation in mind. Extended class diagrams with associations attributed by composition operators give an explicit semantics to associations between classes. Enhanced activity diagrams with a deterministic delay, a non deterministic delay and a time- limited offering make it possible to work with temporal intervals in lieu of timers with fixed duration. The UML profile is given a precise semantics via its translation into the Formal Description Technique RT-LOTOS. A RT-LOTOS validation tool generates simulation chronograms and reachability graphs for RT-LOTOS specifications derived from UML class and activity diagrams. A coffee machine serves as example. The proposed profile is under evaluation on a satellite-based software reconfiguration system.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Representing Embedded System Sequence Diagrams as a\nFormal Language",
        "date": 2001,
        "abstract": "Sequence Diagrams (SDs) have proven useful for describing transaction-oriented systems, and can form a basis for creating statecharts. However, distributed embedded systems require special support for branching, state information, and composing SDs. Actors must traverse many SDs when using a complex embedded system. Current techniques are insufficiently rich to represent the behavior of real systems, such as elevators, without augmentation, and cannot identify the correct SD to execute next from any given state of the system. We propose the application of formal language theory to ensure that SDs (which can be thought of as specifying a grammar) have sufficient information to create statecharts (which implement the automata that recognize that grammar). A promising approach for SD to statechart synthesis then involves ‘compiling‘ SDs represented in an LL(1) grammar into statecharts, and permits us to bring the wealth of formal language and compiler theory to bear on this problem area.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scenario-Based Monitoring and\nTesting of Real-Time UML Models",
        "date": 2001,
        "abstract": "In this paper it is shown how Sequence Diagrams can be used both for monitoring and testing functional and real-time requirements of an executable UML design. We show how this testing approach can be integrated in an UML-based development process. In addition, we will present how a prototype which implements the described monitoring and testing methods is integrated in a well known UML design tool.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Semantics of the Minimum Multiplicity in Ternary\nAssociations in UML",
        "date": 2001,
        "abstract": "The concept of multiplicity in UML derives from that of cardinality in entity-relationship modeling techniques. The UML documentation defines this concept but at the same time acknowledges some lack of obviousness in the specification of multiplicities for n-ary associations. This paper shows an ambiguity in the definition given by UML documentation and proposes a clarification to this definition, as well as a simple extension to the current notation to represent other multiplicity constraints, such as participation constraints, that are equally valuable in understanding n-ary associations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending UML to Support Ontology Engineering for the Semantic Web",
        "date": 2001,
        "abstract": "There is rapidly growing momentum for web enabled agents that reason about and dynamically integrate the appropriate knowledge and services at run-time. The World Wide Web Consortium and the DARPA Agent Markup Language (DAML) program have been actively involved in furthering this trend. The dynamic integration of knowl- edge and services depends on the existence of explicit declarative seman- tic models (ontologies). DAML is an emerging language for specifying machine-readable ontologies on the web. DAML was designed to sup- port tractable reasoning.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Associations in the Uniﬁed Modelling Language",
        "date": 2001,
        "abstract": "Associations between classiﬁers are among the most funda- mental of UML concepts. However, there is considerable room for dis- agreement concerning what an association is, semantically. These have implications for the modeller because they can result in serious misunder- standings of static structure diagrams; similarly, they have implications for tool developers. In this paper we describe and classify the variants which have implicitly or explicitly been described. We discuss the scope for, and diﬃculties in, understanding these as specialisations of a more general notion and we address the implications for future versions of UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "iState: A Statechart Translator",
        "date": 2001,
        "abstract": "We describe formal steps in the design of iState, a tool for translating statecharts into programming languages. Currently iState generates code in either Pascal, Java, or the Abstract Machine Notation of the B method. The translation proceeds in several phases. The focus of this paper is the formal description of the intermediate representations, for which we use class diagrams together with their textual counterparts. We describe how the class diagrams are further reﬁned. The notions of representable, normalized, and legal statecharts are introduced, where normalized statecharts appear as an intermediate representation and code is generated only for legal statecharts.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Specifying Concurrent System Behavior and Timing \nConstraints Using OCL and UML",
        "date": 2001,
        "abstract": "Despite advances in implementation technologies for distributed sys- tems during the last few years, little attention has been given to distributed sys- tems within software development methodologies. The contribution of this paper is a UML-based approach for specifying concurrent behavior and timing con- straints—often inherent characteristics of distributed systems. We propose a novel approach for specifying concurrent behavior of reactive systems in OCL and several constructs for precisely describing timing constraints on UML state- machines. More precisely, we show how we enriched operation schemas—pre- and post- condition assertions of system operations written in OCL—by extending the cur- rent calculus with constructs for asserting synchronization on shared resources. Also, we describe how we use new and existing constructs for UML statema- chines to specify timing constraints on the system interface protocol (SIP)—a restricted form of UML protocol statemachine. Finally, we discuss how both the extended system operation and SIP models are complementary. Keywords: Unified Modeling Language (UML), Object Constraint Language (OCL), Pre- and Postcondition, Software System Specification, Concurrent Pro- gramming, Timing Constraints.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalization of UML-Statecharts",
        "date": 2001,
        "abstract": "The Uniﬁed Modeling Language (UML) has gained wide acceptance in very short time because of its variety of well-known and intuitive graphical notations. However, this comes at the prize of an unprecise and incomplete se- mantics deﬁnition. This insufﬁciency concerns single UML diagram notations on their own as well as their integration. In this paper, we focus on the notation of UML-Statecharts. Starting with a precise textual syntax deﬁnition, we develop quite a concise structured operational semantics (SOS) for UML-Statecharts based on labeled transition systems. Besides the support of interlevel transitions and in contrast to related work, our semantics deﬁnition supports characteristic UML- Statechart features like the history mechanism as well as entry and exit actions.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML for Agent-Oriented Software Development:\nThe Tropos Proposal*",
        "date": 2001,
        "abstract": "We describe a software development methodology called Tropos for agent-oriented software systems. The methodology adopts the i* modeling framework [29], which offers the notions of actor, goal and (actor) dependency, and uses these as a foundation to model early and late requirements, architectural and detailed design. The paper outlines the methodology, and shows how the concepts of Tropos can be accommodated within UML. In addition, we also adopt recent proposals for extensions of UML to support design specifications for agent software. Finally the paper compares Tropos to other research on agent-oriented software development.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML Meta-model for Contract Aware Components⋆",
        "date": 2001,
        "abstract": "We present an extension to the UML meta-model which al- lows modelling of contract aware components. Contracts are a novel way of describing the functional and non-functional behaviour of components. The usage of contracts in component diagrams allows tools to check whether all requirements for a successful assembly and deployment of the components are fulﬁlled. Furthermore, we investigate how compo- nents can be used in the diﬀerent development phases and how design phase transitions can be managed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Speciﬁcation Model for Interface Suites⋆",
        "date": 2001,
        "abstract": "The paper describes a model and tool support for a UML- based speciﬁcation approach, extending UML with templates for struc- tured speciﬁcations deriving from the ISpec approach. The approach is component-oriented where the unit of description is an interface suite: a coherent collection of interfaces deﬁning interactions that transcend component boundaries. To handle complexity, descriptions from various points of view are necessary, expressed by UML diagrams, templates, etc. The issue is to ensure that the views are consistent. For this, we pro- vide a model to integrate the views. The model is sequence-based; the elements of the sequences are carefully designed tuples that reﬂect the interface suite approach. Abstractions from the model reﬂect the views. The model provides the underlying structure for tooling. We developed extensions to Rational Rose by customizing speciﬁcations, automating diagram generation and enabling some consistency checks.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Against Use Case Interleaving",
        "date": 2001,
        "abstract": "Use cases are a powerful and widely recognised tool for functional requirements elicitation and specification of prospective software applications. However, there still are major problems and misunderstandings about the use case approach. One of these is the troublesome notion of use case interleaving which is discussed in this work. Interleaving is still present in the current UML specification. A. Simons correctly realised that interleaving compares with goto/comefrom semantics that were already judged harmful by Dijkstra at the emergence of the Structured Programming era. Simons, thus, has requested the explicit dropping of interleaving semantics. The authors give further support for Simons´ request by showing that interleaving causes severe inconsistencies within UML and contradicts other proven and practically relevant use case concepts such as Goal-Based Use Cases of A. Cockburn, and contractual specifications of use cases expressed by pre- and postcondition approaches. Significant fixes to UML are proposed, in addition to those suggested by Simons. These will dramatically clarify prevailing problems and confusion with use cases and use case relationships among both practitioners and researchers.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Estimating Software Development Effort Based on Use\nCases – Experiences from Industry",
        "date": 2001,
        "abstract": "Use case models are used in object-oriented analysis for capturing and describing the functional requirements of a system. Several methods for estimating software development effort are based on attributes of a use case model. This paper reports the results of three industrial case studies on the application of a method for effort estimation based on use case points. The aim of this paper is to provide guidance for other organizations that want to improve their estimation process applying use cases. Our results support existing claims that use cases can be used successfully in estimating software development effort. The results indicate that the guidance provided by the use case points method can support expert knowledge in the estimation process. Our experience is also that the design of the use case models has a strong impact on the estimates.",
        "keywords": [
            "Use cases",
            "estimation",
            "industrial experience"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Workshops and Tutorials at the UML 2001 Conference",
        "date": 2001,
        "abstract": "As part of the UML 2001 conference, nine tutorials and ﬁve workshops were held. In the following a brief summary of these events is given, including references for further information.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Descriptions in Software Development",
        "date": 2002,
        "abstract": "The central activity in software development is the creation and use of descriptions. We make descriptions to capture our understand- ing of requirements, to describe the properties of the problem domain, to design the behaviour and structure of the software we are building, and for many other purposes too. To work eﬀectively we must make our descriptions as exact as possible, and this is one goal of the designers of formal notations. But we must also be clear-headed about the purpose and subject matter of each descrip- tion. As John von Neumann observed: “There is no point in using exact methods where there is no clarity in the concepts and issues to which they are to be applied”. Without a clear understanding of the purpose and subject matter of each description we can easily lose much of the beneﬁt of eﬀective notations. This talk presents a view of software development based on the notion of problem frames. Each problem frame is associated with a class of simple problems, and with a set of concerns that arise in the solution of problems of the class. Making and using descriptions are seen as activities aimed at addressing those concerns.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Metamodel for the Unified Modeling Language",
        "date": 2002,
        "abstract": "\u0006 \u00141\u0010\u0018\u0010,\u000e\u0006\u000f\u0014\u0018 \u000e \u0006%\u0010.5 %\u0006.5\u0010\u0011\u00060\u0014\u0018 \u0006* 0\u0014\u000f \u0006.5 \u0006\u0015 ,\u0006\u0010%.&/\u00100.\u000e \u0014/\u0006\u000e\u0014/.1\u0010% \u0006\u0018 9 \u0014\"\u000f \u0011.\u0003\u0006\u0013\u0014\u0011\u000e \b \u0011. , \u0006.5&\u000e\u0006%\u0010&\u000e \u000e\u0006.5 \u0006 9 \u0006\u0014/\u0006% \b &% \u0004 \u000f \u0011.\u000e\u0006/\u0014%\u0006\u000f\u0014\u0018 &\u0011'\u0006 \u0010\u0011' \u0010' \u000e\u0006\u0014\u0011\u000615&05\u0006\u000f\u0014\u0018 &\u0011'\u0006\"%\u00100.&.&\u0014\u0011 %\u000e\u0006\u000e5\u0014 \u0018 % ,\u0006&\u0011\u0006.5 &%\u00061\u0014%\u0015\u0003\u0006+\u0006\u000f&\u0011\u0014%\u0006&\u00110\u0014\u0011\u000e&\u000e. \u00110,\u0006\u0014/\u0006\u0010\u0006\u000f\u0014\u0018 &\u0011'\u0006 \u0010\u0011' \u0010' \u0006\u000f .\u0010\u0004 \u000f\u0014\u0018 \u0006 \u000f\u0010,\u0006 0\u0010 \u000e \u0006 \u000f\u0010:\u0014%\u0006 \"%\u0014* \u000f\u000e\u0006 &\u0011\u0006 .5 \u0006 \u0010\u0011' \u0010' \u0006 \u0010\"\" &0\u0010.&\u0014\u0011\u000e;\u0006 .5 \u000e 1&.5\u0006.5 \u0006\u000f\u0014\u0018 \u0006\u0018%&9 \u0011\u0006\u000e,\u000e. \u000f\u000e\u0006\u0018 9 \u0014\"\u000f \u0011.\u0006.5 \u0006\u000e\u0014 &\u0018\u0011 \u000e\u000e\u0006\u0014/\u0006\u000f\u0014\u0018 &\u0011' \u0010\u0011' \u0010' \u000e\u0006 \u000f .\u0010\u000f\u0014\u0018 \u000e\u0006 * 0\u0014\u000f \u000e\u0006 \"\u0010%.&0 \u0010% ,\u0006 &\u000f\"\u0014%.\u0010\u0011.\u0003\u0006 3\u0011\u0006 &.\u000e\u0006 0 %% \u0011. \u000e.\u0010. \u0006 .5 \u0006 \u001b\u0005 \u0006 \u000f .\u0010\u000f\u0014\u0018 \u0006 \u00109 \u000e\u0006 \u0010\u0006 \u000e&'\u0011&/&0\u0010\u0011.\u0006 \u0010% \u0010\u0006 /\u0014%\u0006 &\u000f\"%\u00149 \u000f \u0011.\u0003 - \u0006\"% \u000e \u0011.\u0006\u0010\u0011\u0006\u0010 . %\u0011\u0010.&9 \u0006\u000f .\u0010\u000f\u0014\u0018 \u0006.5\u0010.\u00061\u0010\u000e\u0006&\u0011\u000e\"&% \u0018\u0006*,\u0006.5 \u0006<\u0005\u0004=>7 \u000e.\u0010\u0011\u0018\u0010%\u0018\u0006 \u0010\u0011\u0018\u0006 .5\u0010.\u0006 \u000e\u0014 9 \u000e\u0006 .5 \u0006 \"%\u0014* \u000f\u000e\u0006 \u0014/\u0006 \u001b\u0005 \u0003\u0006 <\u0005\u0004=>7\u0006 1\u0010\u000e\u0006 \u000f \u0011\u0004 .&\u0014\u0011 \u0018\u0006 &\u0011\u0006 \u001b\u0005 \u0006 \u000e\" 0&/&0\u0010.&\u0014\u0011\u000e\u0006 \u0010\u000e\u0006 \u0010\u0006 /%\u0010\u000f 1\u0014%\u0015\u0006 .5\u0010.\u0006 5\u0010\u000e\u0006 \u0010 % \u0010\u0018,\u0006 &\u0011/ \u0004 \u00110 \u0018\u0006\u001b\u0005 \u0003\u0006= %\u0006\u000f .\u0010\u000f\u0014\u0018 \u00061\u0010\u000e\u0006/\u0014%\u000f\u0010 &\u0007 \u0018 \u0006.5 \u000e\u0006&.\u000e\u0006% \u000e .&\u0011'\u0006\u000f\u0014\u0018 \u000e 0\u0010\u0011\u0006* \u0006\u000e&\u000f \u0010. \u0018\u0006\u0010\u0011\u0018\u000605 0\u0015 \u0018\u0006/\u0014%\u00060\u0014\u0011\u000e&\u000e. \u00110,\u0003\u0006\u0012\u0014 \u0006\u0014 %\u0006\"%\u0014\"\u0014\u000e \u0018\u0006\u000e\u0014 \u0004 .&\u0014\u0011\u0006 1&.5\u0006 0\u0014\u0011\u000e.% 0.&9 \u0006 \"\u0014. \u0011.&\u0010 \u0006 .\u00141\u0010%\u0018\u000e\u0006 &\u000f\"%\u00149 \u000f \u0011.\u0006 \u0014/\u0006 .5 \u0006 \u001b\u0005 \u000f .\u0010\u000f\u0014\u0018 \u0006\u000f\u0010,\u00065\u00109 \u0006\u0010\u0006\u000e&'\u0011&/&0\u0010\u0011.\u0006\"%\u00100.&0\u0010 \u0006&\u000f\"\u00100.\u0006\u0014\u0011\u0006.5 \u0006\u001b\u0005 \u0006\u000e\" 0&\u0004 /&0\u0010.&\u0014\u0011\u000e\u0003",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Metamodeling Mathematics:\nA Precise and Visual Framework\nfor Describing Semantics Domains\nof UML Models⋆",
        "date": 2002,
        "abstract": "As UML 2.0 is evolving into a family of languages with in- dividually speciﬁed semantics, there is an increasing need for automated and provenly correct model transformations that (i) assure the integra- tion of local views (diﬀerent diagrams) of the system into a consistent global view, and, (ii) provide a well–founded mapping from UML mod- els to diﬀerent semantic domains (Petri nets, Kripke automaton, pro- cess algebras, etc.) for formal analysis purposes as foreseen, for instance, in submissions for the OMG RFP for Schedulability, Performance and Time. However, such transformations into diﬀerent semantic domains typically require the deep understanding of the underlying mathemat- ics, which hinders the use of formal speciﬁcation techniques in industrial applications. In the paper, we propose a UML-based metamodeling tech- nique with precise static and dynamic semantics (based on a reﬁnement calculus and graph transformation) where the structure and operational semantics of mathematical models can be deﬁned in a UML notation without cumbersome mathematical formulae. Keywords: metamodeling, formal semantics, reﬁnement, model trans- formation, graph transformation",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Radical Reduction of UML’s Core Semantics",
        "date": 2002,
        "abstract": "UML’s current core semantics suffers both from excessive com- plexity and from being overly general. Resultant is a language definition that is difficult to master and to repair. This is the more disturbing as the current core and its extensions do very little to integrate statics and dynamics, even though the inseparability of these is a property of software from which many of the modelling difficulties arise. To better this unsatisfactory situation, we suggest a simple modelling core with few concepts that are easy to understand, yet cover most static and dynamic modelling aspects. We present our work, which is founded in elementary set theory, in natural language making it equally accessible for both practitioners and formalists.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Conﬁguration Knowledge Representation\nUsing UML/OCL",
        "date": 2002,
        "abstract": "Today’s economy is exhibiting a growing trend towards high- ly specialized solution providers cooperatively oﬀering conﬁgurable prod- ucts and services to their customers. In this context, knowledge based conﬁgurators which support the conﬁguration of complex products and services, must be enhanced with capabilities of knowledge sharing and distributed conﬁguration problem solving. In this paper we demonstrate how UML/OCL can be used as knowledge representation language sup- porting standardized knowledge interchange thus enabling cooperative problem solving by diﬀerent conﬁguration environments. We show the representation of conﬁguration domain speciﬁc types of constraints in OCL and present an OCL based knowledge acquisition workbench which enables conﬁguration knowledge base development, maintenance and in- terchange.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using UML for Information Modeling in Industrial\nSystems with Multiple Hierarchies",
        "date": 2002,
        "abstract": "Traditionally, information models for industrial plants have been for- mulated based on domain-specific languages and tools satisfying the require- ments of given standards like IEC 750, IEC 61346 or the Power Station Designation System (KKS). There is however a trend in the automation industry to use common IT standards like XML and UML. The current paper shows our experiences in applying UML for information mod- eling of industrial plant applications, which typically consist of multiple struc- tural hierarchies. We introduce a meta-model, which describes, how the information models can be expressed in UML. Further, we discuss a simple case study, which applies this model in the context of ABB`s Industrial IT platform. Finally, we describe our experiences with UML-based modeling in this domain and discuss the differences between a UML-based representation and the con- cepts of IEC 61346.",
        "keywords": [
            ". Industrial experience",
            "Metamodelling",
            "Business modelling."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Adapting the UML\nto Business Modelling’s Needs - Experiences\nin Situational Method Engineering",
        "date": 2002,
        "abstract": "In 1999 the Swiss Mobiliar Insurance Company (Mobiliar) started a program to transform to a process oriented organization. Based on reengineered business processes, the refocusing of the informa- tion system infrastructure was strived. To ensure a uniﬁed methodical procedure during this transformation a competence center was founded. It supports project speciﬁc needs by situational method engineering. At inception PROMET BPR and the Uniﬁed Modeling Language (UML) were chosen. During the ﬁrst projects, the requirements for method sup- port changed from being purely process engineered, to a focus of sup- porting process improvement. This paper characterizes the phases and the speciﬁc needs they raise. It shows the evolution and integration of the method fragments. Thereby, the main focus is to describe how the concepts of UML would be linked in particular to ratios of process man- agement in the meta-model of this reengineered method.",
        "keywords": [
            "linking business and technical requirements",
            "application and extensions of the UML",
            "situational method engineering"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Analysis of UML Stereotypes\nwithin the UML Metamodel",
        "date": 2002,
        "abstract": "Stereotypes are a powerful and potentially expressive exten- sion mechanism in the Uniﬁed Modeling Language (UML). However, it seems that stereotypes are diﬃcult to handle because using stereotypes needs an understanding of the UML metamodel and, in particular, an un- derstanding of OCL constraints. Stereotypes are often applied in a wrong or at least sloppy way without proper declaration. There are also diﬀer- ences between the various versions of UML with respect to subtle details in the stereotype part. A graphical syntax for stereotypes including ex- amples has been introduced only late in UML 1.4. Other diﬃculties are that constraints are used in the stereotype context in two completely diﬀerent ways and that no full support of stereotypes is yet oﬀered by tools. The paper points out these diﬃculties in detail, analyses the UML metamodel part dealing with stereotypes, and makes various suggestions for improving the deﬁnition and use of stereotypes.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Stereotypical Encounters of the Third Kind",
        "date": 2002,
        "abstract": "As one of the UML’s main extension mechanisms, stereoty- pes play a crucial role in the UML’s ability to serve a wide and growing base of users. However, the precise meaning of stereotypes and their in- tended mode of use has never been entirely clear and has even generated much debate among experts. Two basic ways of using UML stereotypes have been observed in practice: one to support the classiﬁcation of classes as a means of emulating metamodel extensions, the other to support the classiﬁcation of objects as a means of assigning them certain properties. In this paper we analyze these two recognized stereotype usage scenarios and explain the rationale for explicitly identifying a third form of usage scenario. We propose some notational concepts which could be used to explicitly distinguish the three usage scenarios and provide heuristics as to when each should be used. Finally, we conclude by proposing enhan- cements to the UML which could support all three forms cleanly and concisely.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Digging into Use Case Relationships",
        "date": 2002,
        "abstract": "\u0006\u001b\u000e \u00060\u0010\u000e \u0006\u0018&\u0010'%\u0010\u000f\u000e\u0006\u0010% \u0006\u0014\u0011 \u0006\u0014/\u0006.5 \u0006\u0015 ,\u00060\u0014\u00110 \".\u000e\u0006&\u0011\u0006.5 \u0006\u001b\u0011&/& \u0018 \u0005\u0014\u0018 &\u0011'\u0006 \u0010\u0011' \u0010' \u0006* .\u0006.5 &%\u0006\u000e \u000f\u0010\u0011.&0\u000e\u0006\u0010\u0011\u0018\u0006\u0011\u0014.\u0010.&\u0014\u0011\u00065\u00109 \u0006\u000e\u0014\u000f \u0006'\u0010\"\u000e .5\u0010.\u0006 \u0010\u0018\u0006.\u0014\u0006/% \b \u0011.\u0006\u000f&\u000e \u0011\u0018 %\u000e.\u0010\u0011\u0018&\u0011'\u000e\u0006\u0010\u000f\u0014\u0011'\u0006\"%\u00100.&.&\u0014\u0011 %\u000e \u0006 9 \u0011\u0006\u0010*\u0014 . 9 %,\u0006*\u0010\u000e&0\u0006\b \u000e.&\u0014\u0011\u000e\u0003\u00063\u0011\u0006.5&\u000e\u0006\"\u0010\" %\u00061 \u0006\u0010\u0018\u0018% \u000e\u000e\u0006\u000e\u0014\u000f \u0006&\u000e\u000e \u000e\u0006% '\u0010%\u0018&\u0011'\u0006.5 % \u0010.&\u0014\u0011\u000e5&\"\u000e\u0006&\u0011\u000615&05\u0006 \u000e \u00060\u0010\u000e \u000e\u0006\u000f\u0010,\u0006.\u0010\u0015 \u0006\"\u0010%.\u0003\u000645 \u00063\u00110 \u0018 \u0006\u0010\u0011\u0018\u0006\u0017?. \u0011\u0018 % \u0010.&\u0014\u0011\u000e5&\"\u000e\u0006 * .1 \u0011\u0006 .1\u0014\u0006 \u000e \u0006 0\u0010\u000e \u000e\u0006 5\u00109 \u0006 \"% \u000e \u0011. ,\u0006 \u0010\u0011\u0006 &\u00110\u0014\u0011\u000e&\u000e. \u0011. \u0018 /&\u0011&.&\u0014\u0011 \u0006\u000e&\u00110 \u0006.5 ,\u0006\u0010% \u0006% \"% \u000e \u0011. \u0018\u0006\u0010\u000e\u0006\u000e. % \u0014.,\" \u0018\u0006\u0018 \" \u0011\u0018 \u00110& \u000e \u0006* . .5 ,\u0006\u0010% \u0006\u0011\u0014.\u0006.% \u0006\u0018 \" \u0011\u0018 \u00110& \u000e\u0006&\u0011\u0006.5 \u0006\u000f .\u0010\u000f\u0014\u0018 \u0003\u0006) \u000e&\u0018 \u000e \u0006.5 \u0006\u0018&% 0.&\u0014\u0011 \u0014/\u0006.5 \u0006\u0018 \" \u0011\u0018 \u00110,\u0006\u0010%%\u00141\u0006&\u0011\u0006.5 \u0006\u0017?. \u0011\u0018\u0006% \u0010.&\u0014\u0011\u000e5&\"\u00060\u0010\u0011\u0006* \u0006\u000f&\u000e \u0010\u0018&\u0011' \u0011\u0011\u0010. %\u0010 \u0006 \u0010\u0011\u0018\u0006 \u0018&//&0 .\u0006 .\u0014\u0006 \u0011\u0018 %\u000e.\u0010\u0011\u0018\u0006 /\u0014%\u0006 .5 \u0006 0\u0014\u000f\u000f\u0014\u0011\u0006 \"%\u00100.&.&\u0014\u0011 %\u0003 2&\u0011\u0010 , \u00061 \u0006\u000e5\u00141\u0006\u0010 \u000e\u0014\u0006\u000e\u0014\u000f \u00060\u0014\u00110 \". \u0010 \u0006\"%\u0014* \u000f\u000e\u0006% '\u0010%\u0018&\u0011'\u0006.5 \u0006&\u00110 \u0018 \u0018 \u0014%\u0006 ?. \u0011\u0018&\u0011'\u0006 \u000e \u00060\u0010\u000e \u000e \u000615&05\u0006&\u0011\u0006\u0014 %\u0006\u0014\"&\u0011&\u0014\u0011\u0006\u0010% \u0006\u0011\u0014.\u0006.% \u0006 \u000e \u00060\u0010\u000e \u000e\u0003",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Practical Experiences in the Application of MDA",
        "date": 2002,
        "abstract": "\u0006 > %&\u0011'\u0006 .5 \u0006 \u0010\u000e.\u0006 . \u0011\u0006 , \u0010%\u000e\u0006 \u0010\u0006 \u0014.\u0006 \u0014/\u0006 \u0006 0\u0014\u00110 \".\u000e\u0006 5\u00109 \u0006 \u000f %' \u0018 /%\u0014\u000f\u0006\u000e\u0014/.1\u0010% \u0006 \u0011'&\u0011 %&\u0011'\u0003\u0006\u0005\u0014\u000e.\u0006\u0014/\u0006.5 \u000f\u0006.%,\u0006.\u0014\u0006/\u0014%\u000f\u0010 &\u0007 \u0006\u0010\u0011\u0018\u0006\u0014%'\u0010\u0011&\u0007 \u000e\u0014/.1\u0010% \u0006 \u0011'&\u0011 %&\u0011'\u0006\u0015\u0011\u00141 \u0018' \u0006\u0010.\u0006\u0010\u00065&'5 %\u0006 9 \u0006\u0014/\u0006\u0010*\u000e.%\u00100.&\u0014\u0011 \u0006'&9&\u0011' 0\u0014\u0018 \u0006\u0010\u0006\u000e 0\u0014\u0011\u0018\u0010%,\u0006%\u0014 \u0003\u0006\u0005\u0014\u0018 \u0006>%&9 \u0011\u0006+%05&. 0. % \u0006\u0016\u0005>+\u0019 \u0006\" ..&\u0011'\u0006.5 0\u0014\u00110 \".\u0006\u0014/\u0006.5 \u0006\u000f\u0014\u0018 \u0006\u0014\u0011\u0006.5 \u00060%&.&0\u0010 \u0006\"\u0010.5\u0006\u0014/\u0006\u000e\u0014/.1\u0010% \u0006\u0018 9 \u0014\"\u000f \u0011. \u0006&\u000e \u0010*\u0014 .\u0006.\u0014\u000605\u0010\u0011' \u0006.5&\u000e\u0006\u000e&. \u0010.&\u0014\u0011 \u0006. %\u0011&\u0011'\u0006.5 \u0006%\u0014 \u0006\u0014/\u0006\u000f\u0014\u0018 \u000e\u0006/%\u0014\u000f\u00060\u0014\u0011. \u000f\u0004 \" \u0010.&9 \u0006.\u0014\u0006\"%\u0014\u0018 0.&9 \u0003\u000645&\u000e\u0006\"\u0010\" %\u0006&\u0011.%\u0014\u0018 0 \u000e\u0006\u000e\u0014\u000f \u0006\"%\u0014* \u000f\u000e\u0006\u0018 . 0. \u0018\u0006&\u0011 .5 \u0006\"%\u00140 \u000e\u000e\u0006\u0014/\u0006\u0010\u0018\u0014\".&\u0014\u0011\u0006\u0014/\u0006\u0005>+\u0006\u000f .5\u0014\u0018\u000e\u0003\u000645 \u0006\u0010\"\" &0\u0010.&\u0014\u0011\u0006\u0014/\u0006\u0005>+\u0006/\u0014% \u0010\u0006\u000e\" 0&/&0\u0006\u0018\u0014\u000f\u0010&\u0011 \u0006. 05\u0011&\b \u0006\u0014%\u0006. 05\u0011\u0014 \u0014',\u0006% \b &% \u000e\u0006.5 \u0006Description of Specialized Modeling Language\u0003\u000641\u0014\u0006\u000f\u0010&\u0011\u0006. 05\u0011&\b \u000e\u0006\"%\u00149&\u0018 \u0006\u000e \"\"\u0014%. /\u0014%\u0006 .5 \u0006 \u0018 \u000e0%&\".&\u0014\u0011\u0006 \u0014/\u0006 \u001b\u0005 \u0006 ?. \u0011\u000e&\u0014\u0011\u000e \u0006 MOF meta-models\u0006 \u0010\u0011\u0018\u0006 UML profiles\u0003\u000645 \u0006Process of Mapping Description\u0006% \b &% \u000e\u0006'\u0014\u0014\u0018\u0006\u000e \"\"\u0014%.\u0006.\u0014 &\u0018 \u0011.&/,\u0006.5 \u0006 \u000f \u0011.\u000e\u0006\u0014/\u0006.5 \u0006\u000e\u0014 %0 \u0006\u000f\u0014\u0018 &\u0011'\u0006 \u0010\u0011' \u0010' \u0006.5\u0010.\u0006\u0010% \u0006\u000f\u0010\"\" \u0018 \u0010\u0011\u0018\u0006.5 \u0006\u000f\u0014\u0018 \u0006 \u000f \u0011.\u000e\u0006\u0014/\u0006.5 \u0006\u0018 \u000e.&\u0011\u0010.&\u0014\u0011\u0006\u000f\u0014\u0018 &\u0011'\u0006 \u0010\u0011' \u0010' \u0006.5\u0010.\u00060\u0014%% \u0004 \u000e\"\u0014\u0011\u0018\u0006.\u0014\u0006.5 \u0006\u000e\u0014 %0 \u0006 \u000f \u0011.\u000e\u0003\u000645&\u000e\u0006\"%\u00140 \u000e\u000e\u0006% \b &% \u0006\u000e\" 0&/&0\u0006\u000e\u0014 .&\u0014\u0011\u000e\u0003",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Executable Design Models\nfor a Pervasive Healthcare Middleware System",
        "date": 2002,
        "abstract": "UML is applied in the design of a pervasive healthcare mid- dleware system for the hospitals in Aarhus County, Denmark. It works well for the modelling of static aspects of the system, but with respect to describing the behaviour, UML is not suﬃcient. This paper explains why and, as a remedy, suggests to supplement the UML models with behaviour descriptions in the modelling language Coloured Petri Nets, CPN. CPN models are executable and ﬁne-grained, and a combined use of UML and CPN thus supports design-time investigation of the detailed behaviour of system components. In this way, the behavioural conse- quences of alternative design proposals may be evaluated and compared, based on models and prior to implementation.",
        "keywords": [
            "Executable models",
            "detailed behaviour",
            "Petri nets",
            "CPN",
            "system design",
            "middleware",
            "pervasive and mobile computing",
            "supple- menting UML."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating Code from UML\nwith Velocity Templates",
        "date": 2002,
        "abstract": "The value of automated code generation is increasingly rec- ognized, and the application model becomes the central artefact in the software development process. Model-driven development requires a rapid and ﬂexible code generation mechanism. This paper discusses code generation based on templates that actively access UML model in- formation to ﬁll an implementation skeleton. Diﬀerent templates result in diﬀerent generated code, providing a highly ﬂexible generation mecha- nism. Along with a discussion on the potential of such a code generation, an existing framework for code generation with templates is presented.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Does Your Software Creak as It Runs?",
        "date": 2002,
        "abstract": "We are often so overwhelmed with the diﬃculty of writing logically correct software that we tend to underplay or even ignore the inﬂuence of the underlying computing platform. In some cases, this negli- gence has been raised to the level of a design principle, based on a danger- ously naive interpretation of the idea of “platform independence”. After all, it is the platform that gives life to our logic and, as we demonstrate, its eﬀect on software can be profound. We argue that software is not as far removed from physics as many imagine (or hope), that quantity can aﬀect quality, and that, paradoxically, true platform independence cannot be achieved unless the platform is properly factored into design. We then outline a general approach that addresses this issue and show how it can be realized with UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating the Synchronous Paradigm into\nUML: Application to Control-Dominated\nSystems",
        "date": 2002,
        "abstract": "The Synchronous Paradigm proposes an abstract model in- tegrating concurrency and communication, deterministic thus simple, semantically well-founded thus suitable to formal analysis, producing safe and eﬃcient code. However combining this model with the object- oriented approach is still challenging. This paper explores how an UML- based methodology can be set up, making it possible to use the Syn- chronous Paradigm in combination with other (more classical) techniques to develop control-dominated systems. It addresses the issue of represent- ing behavior in a semantically sound way using the synchronous mod- els, of relating behavior and structure, and of mixing synchronous and asynchronous behavior though an extended notion of (ROOM-like) “cap- sules”, the synchronous islets. We also brieﬂy mention the extensions and modiﬁcations in the UML meta-model necessary to support this method- ology.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML Proﬁle for Real-Time Constraints\nwith the OCL",
        "date": 2002,
        "abstract": "This article presents a UML proﬁle for an OCL extension that enables modelers to specify behavioral, state-oriented real-time con- straints in OCL. In order to perform a seamless integration into the upcoming UML2.0 standard, we take the latest OCL2.0 metamodel pro- posal by Warmer et al. [22] as a basis. A formal semantics of our temporal OCL extension is given by a mapping to time-annotated temporal logics formulae. To give an example of the applicability of our extension, we consider a modeling approach for manufacturing systems called MFERT. We present a corresponding UML proﬁle for that approach and combine both proﬁles for formal veriﬁcation by real-time model checking.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "HOL-OCL: Experiences, Consequences\nand Design Choices",
        "date": 2002,
        "abstract": "Based on experiences gained from an embedding of the Ob- ject Constraint Language (OCL) in higher-order logic [3], we explore several key issues of the design of a formal semantics of the OCL. These issues comprise the question of the interpretation of invariants, pre- and postconditions, an executable sub-language and the possibilities of reﬁne- ment notions. A particular emphasize is put on the issue of mechanized deduction in UML/OCL speciﬁcation. Keywords: OCL, formal semantics, constraint languages, reﬁnement",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistency-Preserving Model Evolution\nthrough Transformations",
        "date": 2002,
        "abstract": "With model-based development being on the verge of becom- ing an industrial standard, the topic of research of statically checking the consistency of a model made up of several submodels has already received increasing attention. The evolution of models within software engineer- ing requires support for incremental consistency analysis techniques of a new version of the model after evolution, thereby avoiding a complete reiteration of all consistency tests. In this paper, we discuss the problem of preserving consistency within model-based evolution focusing on UML-RT models. We introduce the concept of a model transformation rule that captures an evolution step. Composition of several evolution steps leads to a complex evolution of a model. For each evolution step, we study the eﬀects on the consistency of the overall model and provide localized consistency checks for those parts of the model that have changed. For a complex evolution of a model, consistency can then be established by incrementally performing those localized consistency checks associated to the transformation rules applied within the evolution.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Transformations\nand Software Modeling Languages:\nAutomating Transformations in UML",
        "date": 2002,
        "abstract": "This paper investigates the role of transformations in the Uniﬁed Modeling Language, speciﬁcally UML class diagrams with OCL constraints. To date, the use of transformations in software modeling and design has not been fully explored. A framework for expressing trans- formations is presented along with concrete examples that, for exam- ple, infer new inheritance links, or transform constraints. In particular, a technique for checking that two UML class diagrams are refactorings of each other is described.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Relational Approach to Defining Transformations\nin a Metamodel",
        "date": 2002,
        "abstract": "\u0006 \u0005 .\u0010\u000f\u0014\u0018 &\u0011'\u0006 &\u000e\u0006 * 0\u0014\u000f&\u0011'\u0006 \u0010\u0006 \u000e.\u0010\u0011\u0018\u0010%\u0018\u0006 1\u0010,\u0006 \u0014/\u0006 \u0018 /&\u0011&\u0011' \u0010\u0011' \u0010' \u000e\u0006 \u000e 05\u0006 \u0010\u000e\u0006 .5 \u0006 \u001b\u0005 \u0003\u0006 +\u0006 \u0010\u0011' \u0010' \u0006 \u0018 /&\u0011&.&\u0014\u0011\u0006 \u0018&\u000e.&\u0011' &\u000e5 \u000e * .1 \u0011\u0006 0\u0014\u00110% . \u0006 \u000e,\u0011.\u0010? \u0006 \u0010*\u000e.%\u00100.\u0006 \u000e,\u0011.\u0010?\u0006 \u0010\u0011\u0018\u0006 \u000e \u000f\u0010\u0011.&0\u000e\u0006 \u0018\u0014\u000f\u0010&\u0011\u0003\u0006 3.\u0006 &\u000e \"\u0014\u000e\u000e&* \u0006.\u0014\u0006\u0018 /&\u0011 \u0006\u0010 \u0006.5% \u0006 \u000e&\u0011'\u0006\u0010\u0006 \u000f .\u0010\u000f\u0014\u0018 &\u0011'\u0006\u0010\"\"%\u0014\u001005 \u0006* .\u0006&.\u0006 &\u000e \u000e\u000e\u00060 \u0010%\u00065\u00141\u0006.\u0014\u0006\u0018 /&\u0011 \u0006.5 \u0006.%\u0010\u0011\u000e/\u0014%\u000f\u0010.&\u0014\u0011\u000e\u0006* .1 \u0011\u0006.5 \u000f\u0003\u000645&\u000e\u0006\"\u0010\" % \"%\u0014\"\u0014\u000e \u000e\u0006\u0010\u0011\u0006\u0010\"\"%\u0014\u001005\u000615&05\u0006 \u000e \u000e\u0006\u000f .\u0010\u000f\u0014\u0018 &\u0011'\u0006\"\u0010.. %\u0011\u000e\u0006.5\u0010.\u00060\u0010\". % .5 \u0006 \u000e\u000e \u00110 \u0006\u0014/\u0006\u000f\u0010.5 \u000f\u0010.&0\u0010 \u0006% \u0010.&\u0014\u0011\u000e\u0003\u00063.\u0006\u000e5\u00141\u000e\u00065\u00141\u0006.5 \u000e \u0006\"\u0010.. %\u0011\u000e\u00060\u0010\u0011 * \u0006 \u000e \u0018\u0006 .\u0014\u0006 \u0018 /&\u0011 \u0006 *\u0014.5\u0006 .5 \u0006 % \u0010.&\u0014\u0011\u000e5&\"\u0006 * .1 \u0011\u0006 0\u0014\u00110% . \u0006 \u000e,\u0011.\u0010?\u0006 \u0010\u0011\u0018 \u0010*\u000e.%\u00100.\u0006\u000e,\u0011.\u0010? \u0006\u0010\u0011\u0018\u0006* .1 \u0011\u0006\u0010*\u000e.%\u00100.\u0006\u000e,\u0011.\u0010?\u0006\u0010\u0011\u0018\u0006\u000e \u000f\u0010\u0011.&0\u000e\u0006\u0018\u0014\u000f\u0010&\u0011 \u0006/\u0014% \u0010\u0006/%\u0010'\u000f \u0011.\u0006\u0014/\u0006\u001b\u0005 \u0003\u0006+\u0006'\u0014\u0010 \u0006\u0014/\u0006.5 \u0006\u0010\"\"%\u0014\u001005\u0006&\u000e\u0006.\u0014\u0006\"%\u00149&\u0018 \u0006\u0010\u00060\u0014\u000f\" . \u000e\" 0&/&0\u0010.&\u0014\u0011\u0006 \u0014/\u0006 \u0010\u0006 \u0010\u0011' \u0010' \u0006 /%\u0014\u000f\u0006 15&05\u0006 &\u0011. &' \u0011.\u0006 .\u0014\u0014 \u000e\u0006 0\u0010\u0011\u0006 * ' \u0011 %\u0010. \u0018\u0003\u0006 45 \u0006 ?. \u0011.\u0006 .\u0014\u0006 15&05\u0006 .5 \u0006 \u0010\"\"%\u0014\u001005\u0006 \u000f .\u000e\u0006 .5&\u000e\u0006 '\u0014\u0010 \u0006 &\u000e \u0018&\u000e0 \u000e\u000e \u0018\u0006&\u0011\u0006.5 \u0006\"\u0010\" %\u0003",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "On Customizing the UML for Modeling\nPerformance-Oriented Applications⋆",
        "date": 2002,
        "abstract": "Modeling of parallel and distributed applications was a pre- occupation of numerous research groups in the past. The increasing im- portance of applications that mix shared memory parallelism with mes- sage passing has complicated the modeling eﬀort. Despite the fact that UML represents the de-facto standard modeling language, little work has been done to investigate whether UML can be employed to model performance-oriented parallel and distributed applications. This paper provides a critical look at the utility of UML to model shared mem- ory and message passing applications by employing the UML extension mechanisms. The basic idea is to develop UML building blocks for the most important sequential, shared memory, and message passing con- structs. These building blocks can be enriched with additional infor- mation, for instance, performance and control ﬂow data. Subsequently, building blocks are combined to represent basically arbitrary complex applications. We will further describe how to model the mapping of ap- plications onto process topologies.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending the UML for Multidimensional\nModeling⋆",
        "date": 2002,
        "abstract": "Multidimensional (MD) modeling is the foundation of data warehouses, MD databases, and On-Line Analytical Processing (OLAP) applications. In the past few years, there have been some proposals for representing the main MD properties at the conceptual level providing their own notations. In this paper, we present an extension of the Uni- ed Modeling Language (UML), by means of stereotypes, to elegantly represent main structural and dynamic MD properties at the conceptual level. We make use of the Object Constraint Language (OCL) to specify the constraints attached to the de ned stereotypes, thereby avoiding an arbitrary use of these stereotypes. The main advantage of our proposal is that it is based on a well-known standard modeling language, thereby designers can avoid learning a new speci c notation or language for MD systems. Finally, we show how to use these stereotypes in Rational Rose 2000 for MD modeling.",
        "keywords": [
            "UML",
            "UML extensions",
            "multidimensional modeling",
            "OCL",
            "Ra- tional Rose"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Metamodel for Package Extension with Renaming",
        "date": 2002,
        "abstract": "7\u00100\u0015\u0010' \u0006 ?. \u0011\u000e&\u0014\u0011\u0006\u0010\u0011\u0018\u0006. \u000f\" \u0010. \u0006\u000f 05\u0010\u0011&\u000e\u000f\u000e\u00061 % \u0006\u0014%&'&\u0011\u0010 , \"%\u0014\"\u0014\u000e \u0018\u0006\u0010\u000e\u0006\"\u0010%.\u0006\u0014/\u0006.5 \u0006\u0013\u0010.\u0010 ,\u000e&\u000e\u0006\u000f .5\u0014\u0018\u0003\u00063.\u00065\u0010\u000e\u0006\u000e&\u00110 \u0006* \u0011\u0006\u000e '' \u000e. \u0018 .5\u0010.\u0006 .5 ,\u0006 0\u0010\u0011\u0006 * \u0006 \u000e \u0018\u0006 .\u0014\u0006 0\u0010\". % \u0006 \"\u0010.. %\u0011\u000e\u0006 \u0010\u0011\u0018\u0006 /\u0014%\u0006 \u0010\u000e\" 0.\u0004\u0014%& \u0011. \u0018 \u000f\u0014\u0018 &\u0011'\u0006 \u0010\u0011\u0018\u0006 \u000f .\u0010\u000f\u0014\u0018 &\u0011'\u0003\u0006 45&\u000e\u0006 \"\u0010\" %\u0006 \"%\u00149&\u0018 \u000e\u0006 \u0010\u0006 %&'\u0014%\u0014 \u000e \u000f .\u0010\u000f\u0014\u0018 \u0006\u0018 /&\u0011&.&\u0014\u0011\u0006\u0014/\u0006.5 \u0006\"\u00100\u0015\u0010' \u0006 ?. \u0011\u000e&\u0014\u0011\u0006\u000f 05\u0010\u0011&\u000e\u000f\u0003\u00063.\u0006. %\u0011\u000e\u0006\u0014 . .5\u0010.\u0006.5 \u0006\u0018 /&\u0011&.&\u0014\u0011\u0006&\u000e\u0006\u000f\u0014% \u0006\u000e *. \u0006.5\u0010\u0011\u0006\u0014\u0011 \u0006\u000f&'5.\u0006\u0010.\u0006/&%\u000e.\u0006.5&\u0011\u0015 \u0006\u0010\u0011\u0018\u0006\u000e\u0014\u000f \u0014/\u0006.5 \u0006\u000e *. .& \u000e\u0006\u0010% \u0006 ?\"\u0014\u000e \u0018\u0006&\u0011\u0006.5 \u0006\"\u0010\" %\u0003\u000645 \u0006\"\u0010\" %\u00060\u0014\u00110 \u0018 \u000e\u00061&.5\u0006\u0010\u0011 \u00149 %9& 1\u0006 \u0014/\u0006 5\u00141\u0006 .5 \u0006 0\u0014% \u0006 \u0018 /&\u0011&.&\u0014\u0011\u0006 \u000f\u0010,\u0006 * \u0006 ?\"\u0010\u0011\u0018 \u0018\u0006 .\u0014\u0006 &\u00110 \u0018 . \u000f\" \u0010. \u000e\u0006\u0010\u0011\u0018\u0006.\u0014\u0006\u0018 \u0010 \u00061&.5\u0006\u0010\u0006%&05 %\u0006*\u0010\u000e \u0006 \u0010\u0011' \u0010' \u0003\u00064\u0014\u0014 \u0006&\u000f\" \u000f \u0011.\u0010.&\u0014\u0011 &\u000e\u0006\u0010 \u000e\u0014\u0006\u0018&\u000e0 \u000e\u000e \u0018\u0003",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Applying MDA Concepts to Develop\na Domain CORBA Facility for E-learning",
        "date": 2002,
        "abstract": "A well-conceived service or facility is always based on an underlying semantic model that is independent of the target platform. However, the model may not be distilled explicitly, and this is the case with OMG’s vertical domain speciﬁcations because the model is not ex- pressed separately from its IDL interfaces. Therefore, these services and facilities have not received the recognition and use that they deserve out- side of the CORBA environment. This paper reports on the application of MDA concepts on the development of a draft proposal for a Domain CORBA Facility for e-learning. In order to maximize the utility and im- pact of the domain facility in the MDA, it was modelled in the form of a normative Platform Independent Model (PIM) expressed using UML, augmented by a normative Platform Speciﬁc Model (PSM) expressed using the UML proﬁle for CORBA and IDL interface deﬁnitions.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Rapid Development of Modular Dynamic Web Sites Using UML Tim Schattkowsky1, Marc Lohmann2",
        "date": 2002,
        "abstract": "Development of dynamic Web sites is often performed by teams consisting of graphic designers and software developers. Communication between these different team members has to be supported with a simple modeling approach that considers their different academical backgrounds. Dynamic Web sites can contain multiple modules that may reappear on different pages. Reuse of both business logic and visual design modules would be desirable. Furthermore, a considerable amount of time is usually consumed by the implementation of data flows that are already defined in the model. Rapid development is enabled by providing roundtrip engineering capabilities with support for automatic code generation. We propose a simple subset of the UML adapted to the problem domain by means of stereotypes as well as a strategy for generating code templates from such models. These templates are tailored to the tasks of each team member. This enables parallel work and automated reintegration of results.",
        "keywords": [
            "Unified Modeling Language",
            "Hypermedia",
            "WWW",
            "Object-Oriented  Design",
            "Code Generation"
        ],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Software, Heal Thyself!",
        "date": 2002,
        "abstract": "Traditional software engineering is predicated on the dis- tinction between development time (when a software system is designed, built, and tested) and run time (when a software system is deployed and executed). However, increasingly software systems are expected to run continuously, adapting as they execute to changes in environment and even user requirements. For such systems the traditional distinctions break down. In this talk I explore the role of development-time models in making it possible for systems to adapt at run time. In particular, I will argue that architectural models of software systems have a pivotal part to play as an enabler for self-adaptive and self-healing systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Speciﬁcation of UML Collaborations as\nInteraction Components",
        "date": 2002,
        "abstract": "One of the touchstones of Object-Oriented Design is that the management of complexity is seldom located within any single object. It should instead be an emerging property of the collaborations within a society of objects, each one of these being as simple as possible. These collaborations can easily be speciﬁed using UML collaboration diagrams. We propose to reify UML collaborations as interaction components. This allows the easy handling and reusing of interaction abstractions among components at both speciﬁcation and implementation levels. This paper focuses on the speciﬁcation of these components. We propose criteria to deﬁne the type and the “frontier” of an interaction abstrac- tion. We present a UML collaboration speciﬁcation methodology that deals with the constraints of component speciﬁcation.",
        "keywords": [
            "UML collaborations",
            "speciﬁcation methodology",
            "interaction abstractions",
            "interaction components"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Measuring OO Design Metrics from UML",
        "date": 2002,
        "abstract": "\u00060 \u000e&'\u0011\u0006\u000f .%&/\u000e\u0006\u0010% \u0006 \u000e 5 \u0006\u000f \u0010\u0011\u000e\u00065\u0014%\u0006&\u000f\"%\u00146&\u0011'\u0006.- \u0006\b \u0010 &.3\u0006\u00145 \u000e\u00145.,\u0010% \u0003\u00062\u0006\u0011 \u000f* %\u0006\u00145\u0006\u0014*7 /.\u0004\u0014%& \u0011. \u0018\u0006\u000f .%&/\u000e\u0006-\u00106 \u0006* \u0011\u0006\u000e '' \u000e. \u0018\u0006\u0010\u000e * &\u0011'\u0006 - \"5 \u0006 5\u0014%\u0006 % \u000e\u0014 %/ \u0006 \u0010 \u0014/\u0010.&\u0014\u0011\u0006 &\u0011\u0006 \u000e\u00145.,\u0010% \u0006 \u0018 6 \u0014\"\u000f \u0011.\u0003\u0006 +- \u000e \u000f .%&/\u000e\u0006\u0010% \u0006\"\u0010%.&/ \u0010% 3\u0006 \u000e 5 \u00065\u0014%\u0006&\u0018 \u0011.&53&\u0011'\u00065\u0010 .\u0004\"%\u0014\u0011 \u0006/ \u0010\u000e\u000e \u000e\u0006\u0010\u0011\u0018\u00065\u0014% \"% \u0018&/.&\u0011'\u0006 % \b &% \u0018\u0006 \u000f\u0010&\u0011. \u0011\u0010\u0011/ \u0006 55\u0014%.\u000e \u0006 \"%\u0014\u0018 /.&6&.3 \u0006 \u0010\u0011\u0018\u0006 % ,\u0014%\u0015\u0006 5\u0004 5\u0014%.\u000e\u0003\u0006+\u0014\u0006\u0014*.\u0010&\u0011\u0006.- \u0006\u0018 \u000e&'\u0011\u0006\u000f .%&/\u000e\u0006\u00145\u0006.- \u0006\u000e\u00145.,\u0010% \u0006 \u0011\u0018 %\u0006\u0018 6 \u0014\"\u000f \u0011. \u000f\u0014\u000e.\u0006 8&\u000e.&\u0011'\u0006 \u0010\"\"%\u0014\u0010/- \u000e\u0006 \u000f \u0010\u000e % \u0006 .- \u0006 \u000f .%&/\u000e\u0006 *3\u0006 \"\u0010%\u000e&\u0011'\u0006 .- \u0006 \u000e\u0014 %/ /\u0014\u0018 \u0006\u00145\u0006.- \u0006\u000e\u00145.,\u0010% \u0003\u0006\u0012 /-\u0006\u0010\"\"%\u0014\u0010/- \u000e\u0006/\u0010\u0011\u0006\u0014\u0011 3\u0006* \u0006\" %5\u0014%\u000f \u0018\u0006&\u0011\u0006\u0010\u0006 \u0010. \"-\u0010\u000e \u0006\u00145\u0006\u000e\u00145.,\u0010% \u0006\u0018 6 \u0014\"\u000f \u0011. \u0006.- \u000e\u0006 &\u000f&.&\u0011'\u0006.- \u0006 \u000e 5 \u0011 \u000e\u000e\u0006\u00145\u0006.- \u0006\u0018 \u0004 \u000e&'\u0011\u0006\u000f .%&/\u000e\u0006&\u0011\u0006% \u000e\u0014 %/ \u0006\u0010 \u0014/\u0010.&\u0014\u0011\u0003\u00069\u0011\u0006.-&\u000e\u0006\"\u0010\" % \u0006, \u0006\"% \u000e \u0011.\u0006\u0010\u0006\u000f .-\u0014\u0018\u0004 \u0014 \u0014'3\u0006 .-\u0010.\u0006 /\u0014\u000f\"& \u000e\u0006 \u001b\u0005 \u0006 \u000e\" /&5&/\u0010.&\u0014\u0011\u000e\u0006 .\u0014\u0006 \u0014*.\u0010&\u0011\u0006 \u0018 \u000e&'\u0011\u0006 &\u00115\u0014%\u000f\u0010.&\u0014\u0011 \u0010\u0011\u0018\u0006.\u0014\u0006/\u0014\u000f\" . \u0006.- \u0006\u0018 \u000e&'\u0011\u0006\u000f .%&/\u000e\u0006\u0010.\u0006\u0010\u0011\u0006 \u0010% 3\u0006\u000e.\u0010' \u0006\u00145\u0006\u000e\u00145.,\u0010% \u0006\u0018 6 \u0004 \u0014\"\u000f \u0011.\u0003\u0006+- \u0006/ %% \u0011.\u00066 %\u000e&\u0014\u0011\u0006\u00145\u0006\u0014 %\u0006.\u0014\u0014 \u0006 \u000e \u000e\u0006\u0018&\u0010'%\u0010\u000f\u000e\u0006\"%\u0014\u0018 / \u0018\u0006*3\u0006.- :\u0010.&\u0014\u0011\u0010 \u0006:\u0014\u000e \u0006.\u0014\u0014 \u0006\u0010\u0011\u0018\u0006/\u0014\u000f\" . \u000e\u0006;;\u0006\u000f .%&/\u000e\u0006.-\u0010.\u0006-\u00106 \u0006* \u0011\u0006\u000e '' \u000e. \u0018 \u0010\u000e\u0006 * &\u0011'\u0006 '\u0014\u0014\u0018\u0006 &\u0011\u0018&/\u0010.\u0014%\u000e\u0006 5\u0014%\u0006 &\u0018 \u0011.&53&\u0011'\u0006 5\u0010 .\u000e\u0006 % \u0010. \u0018\u0006 .\u0014\u0006 ;*7 /.\u0004 ;%& \u0011. \u0018\u0006 5 \u0010. % \u000e\u0003\u0006 ; %\u0006 . /-\u0011&\b \u0006 \u0010\u00186\u0010\u0011/ \u000e\u0006 .- \u0006 \u000e.\u0010. \u0006 \u00145\u0006 .- \u0006 \u000f .%&/\u000e \u000f \u0010\u000e %&\u0011'\u0006\"%\u0014/ \u000e\u000e<\u0006.- \u000e\u0006&.\u0006&\u000e\u0006 8\" /. \u0018\u0006.\u0014\u0006\u000e.%\u0014\u0011' 3\u0006\"%\u0014\u000f\u0014. \u0006.- \u0006 \u000e \u0006\u00145\u0006\u0018 \u0004 \u000e&'\u0011\u0006\u000f .%&/\u000e\u0006\u0010\u0011\u0018\u0006\u000e&'\u0011&5&/\u0010\u0011. 3\u0006&\u0011/% \u0010\u000e \u0006.- &%\u0006&\u000f\"\u0010/.\u0006\u0014\u0011\u0006&\u000f\"%\u00146&\u0011'\u0006\u000e\u00145.\u0004 ,\u0010% \u0006\b \u0010 &.3\u0003",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Cow_Suite Approach to Planning\nand Deriving Test Suites in UML Projects",
        "date": 2002,
        "abstract": "\u0006\u0013\u0014,L\u0012 &. \u0006\"%\u00146&\u0018 \u000e\u0006\u0010\u0011\u0006&\u0011. '%\u0010. \u0018\u0006\u0010\u0011\u0018\u0006\"%\u0010/.&/\u0010 \u0006\u0010\"\"%\u0014\u0010/-\u0006.\u0014 .- \u0006\u000e.%\u0010. '&/\u0006' \u0011 %\u0010.&\u0014\u0011\u0006\u0010\u0011\u0018\u0006\" \u0010\u0011\u0011&\u0011'\u0006\u00145\u0006\u001b\u0005 \u0004*\u0010\u000e \u0018\u0006. \u000e.\u0006\u000e &. \u000e \u0006\u000e&\u0011/ .- \u0006 \u0010% 3\u0006 \u000e.\u0010' \u000e\u0006 \u00145\u0006 \u000e3\u000e. \u000f\u0006 \u0010\u0011\u0010 3\u000e&\u000e\u0006 \u0010\u0011\u0018\u0006 \u000f\u0014\u0018 &\u0011'\u0003\u0006 9.\u0006 /\u0014\u0011\u000e&\u000e.\u000e\u0006 \u00145\u0006 .,\u0014 \u0014%&'&\u0011\u0010 \u0006 /\u0014\u000f\"\u0014\u0011 \u0011.\u000e\u0006 ,\u0014%\u0015&\u0011'\u0006 &\u0011\u0006 /\u0014\u000f*&\u0011\u0010.&\u0014\u0011\u001a\u0006 .- \u0006 \u0013\u0014,. \u000e.\u0006 \u000e.%\u0010. '3 ,-&/-\u0006 \u0014%'\u0010\u0011&\u0007 \u000e\u0006 .- \u0006 . \u000e.&\u0011'\u0006 \"%\u0014/ \u000e\u000e\u0006 \u0010\u0011\u0018\u0006 - \"\u000e\u0006 .- \u0006 \u000f\u0010\u0011\u0010' %\u0006 .\u0014\u0006 \u000e /. \u0010\u000f\u0014\u0011'\u0006 .- \u0006 \u000f\u0010\u00113\u0006 \"\u0014. \u0011.&\u0010 \u0006 . \u000e.\u0006 /\u0010\u000e \u000e \u0006 \u0010\u0011\u0018\u0006 .- \u0006 \u001b9+\u0006 \u000f .-\u0014\u0018 \u0006 ,-&/- \" %5\u0014%\u000f\u000e\u0006 .- \u0006 \u0010 .\u0014\u000f\u0010. \u0018\u0006 ' \u0011 %\u0010.&\u0014\u0011\u0006 \u00145\u0006 . \u000e.\u0006 /\u0010\u000e \u000e\u0006 5%\u0014\u000f\u0006 .- \u0006 \u001b\u0005 \u0018&\u0010'%\u0010\u000f\u000e\u0003\u0006+- \u0006\u0010\"\"%\u0014\u0010/-\u0006/\u0010\u0011\u0006* \u0006 \u000e \u0018\u0006&\u0011\u0006&\u0011/% \u000f \u0011.\u0010 \u0006,\u00103 \u0006\u000e.\u0010%.&\u0011'\u00065%\u0014\u000f .- \u0006 \"% &\u000f&\u0011\u0010%3\u0006 \u0016 6 \u0011\u0006 &\u0011/\u0014\u000f\" . \u0019\u0006 \u001b\u0005 \u0006 \u0018&\u0010'%\u0010\u000f\u000e \u0006 \u0010\u0011\u0018\u0006 &\u000e\u0006 \u0010\"\" & \u0018\u0006 .\u0014 &\u0011. '%\u0010.&\u0014\u0011\u0006 \u000e *\u000e3\u000e. \u000f\u000e \u0006 \u0010\u000e\u0006 &\u0011. %\u0010/.&6 3\u0006 \u000e /. \u0018\u0006 *3\u0006 .- \u0006 . \u000e. %\u0003\u0006 +- \u000f\"-\u0010\u000e&\u000e\u0006&\u000e\u0006\u0014\u0011\u0006 \u000e\u0010*& &.3 \u0006&\u0011\u0006.-\u0010.\u0006, \u0006 \u000e \u0006 8\u0010/. 3\u0006.- \u0006\u000e\u0010\u000f \u0006\u001b\u0005 \u0006\u0018&\u0010'%\u0010\u000f\u000e \u0018 6 \u0014\" \u0018\u0006 5\u0014%\u0006 \u0010\u0011\u0010 3\u000e&\u000e\u0006 \u0010\u0011\u0018\u0006 \u0018 \u000e&'\u0011 \u0006 ,&.-\u0014 .\u0006 % \b &%&\u0011'\u0006 \u0010\u00113\u0006 \u0010\u0018\u0018&.&\u0014\u0011\u0010 5\u0014%\u000f\u0010 &\u000e\u000f\u0006\u0014%\u0006\u0010\u0018\u0004-\u0014/\u0006 55\u0014%.\u0006\u000e\" /&5&/\u0010 3\u00065\u0014%\u0006. \u000e.&\u0011'\u0006\" %\"\u0014\u000e \u000e\u0003\u0006\u0013\u0014,L\u0012 &. -\u0010\u000e\u0006 * \u0011\u0006 &\u000f\" \u000f \u0011. \u0018\u0006 &\u0011\u0006 \u0010\u0006 \"%\u0014.\u0014.3\" \u0006 .\u0014\u0014 \u0006 \u0010\u0011\u0018\u0006 &\u000e\u0006 / %% \u0011. 3\u0006 * &\u0011' 6\u0010 &\u0018\u0010. \u0018\u0006&\u0011\u0006\u0010\u0011\u0006&\u0011\u0018 \u000e.%&\u0010 \u0006\u0018 6 \u0014\"\u000f \u0011.\u0006 \u00116&%\u0014\u0011\u000f \u0011.\u0003",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Diagram Interchange for UML",
        "date": 2002,
        "abstract": "XMI is a standardized mechanism for exchanging UML mod- els. However, this mechanism does not suﬃciently fulﬁll the goal of a model interchange: it does not include the exchange of diagram in- formation. XMI as deﬁned for UML 1.x is only capable of transporting information on the elements in an UML model but not information as to how these elements are represented and laid out in diagrams. This paper proposes an extension to the UML metamodel to represent di- agram information in a graph-oriented manner. The approach presented is able to ﬁx the deﬁciency for UML 1.x and solve the problem for UML 2.0. The approach was handed in for standardization to the OMG in response to the Diagram Interchange RFP.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UMLsec: Extending UML\nfor Secure Systems Development⋆",
        "date": 2002,
        "abstract": "Developing secure-critical systems is diﬃcult and there are many well-known examples of security weaknesses exploited in practice. Thus a sound methodology supporting secure systems development is urgently needed. Our aim is to aid the diﬃcult task of developing security-critical systems in an approach based on the notation of the Uniﬁed Modeling Language. We present the extension UMLsec of UML that allows to express security- relevant information within the diagrams in a system speciﬁcation. UMLsec is deﬁned in form of a UML proﬁle using the standard UML extension mechanisms. In particular, the associated constraints give cri- teria to evaluate the security aspects of a system design, by referring to a formal semantics of a simpliﬁed fragment of UML. We demonstrate the concepts with examples.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SecureUML: A UML-Based Modeling Language\nfor Model-Driven Security⋆",
        "date": 2002,
        "abstract": "We present a modeling language for the model-driven de- velopment of secure, distributed systems based on the Uniﬁed Model- ing Language (UML). Our approach is based on role-based access con- trol with additional support for specifying authorization constraints. We show how UML can be used to specify information related to access control in the overall design of an application and how this information can be used to automatically generate complete access control infras- tructures. Our approach can be used to improve productivity during the development of secure distributed systems and the quality of the result- ing systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Workshops and Tutorials\nat the UML 2002 Conference",
        "date": 2002,
        "abstract": "As part of the UML 2002 conference, six tutorials and ﬁve workshops were held. In the following, a brief summary of these events is given, including references for further information.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Agile Processes: Developing Your Own “Secret\nRecipes”",
        "date": 2003,
        "abstract": "Every enterprise competes with its software-development processes, its ”recipes” for building better software. In this session, you’ll gain understanding regarding the recipes and mini-recipes advocated within Uniﬁed Process (UP), Feature-Driven Development (FDD), and Extreme Programming (XP). You’ll discover recipes and mini-recipes that could be more eﬀective and appealing and eﬀective for your team, your company, your risks, and your challenges–and gain insights into how to incrementally advance your own ”secret recipes” for competitive advantage.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Diﬀerence and Union of Models",
        "date": 2003,
        "abstract": "This paper discusses the diﬀerence and union of models in the context of a version control system. We show three metamodel- independent algorithms that calculate the diﬀerence between two models, merge a model with the diﬀerence of two models and calculate the union of two models. We show how to detect union conﬂicts and how they can be resolved either automatically or manually. We present an application of these algorithms in a version control system for MOF-based models.",
        "keywords": [
            "metamodelling",
            "delta calculation",
            "revision control",
            "UML."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "GREAT: UML Transformation Tool for Porting\nMiddleware Applications",
        "date": 2003,
        "abstract": "Design and maintenance of enterprise applications is compli- cated due to dependencies on technical requirements of the middleware framework. Especially, porting enterprise applications to another middle- ware layer or even new versions thereof requires a lot of handiwork and coding, since abstraction-, transformation-, and reﬁnement steps have to be performed. Transformations should be assisted by a tool set which facilitates the migration process from one to another middleware plat- form. This paper presents GREAT, a rule-based transformation frame- work which facilitates transformations among models on the same or diﬀerent abstraction levels. The feasibility of GREAT is shown by the transformation of a real world application conforming to EJB standard 1.1 into a version which complies to EJB standard 2.0.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Centric Engineering with the Evolution\nand Validation Environment",
        "date": 2003,
        "abstract": "Reuse is an important aspect of software engineering that promises advantages like faster time-to-market, cost reduction, better maintainability etc. The software industry focuses on components as commercials oﬀ-the-shelf in order to gain reusable assets. However, reuse on the design level usually is not addressed. If we come to perceive mod- els as assets of the software process, then the design moves from the periphery of software engineering to the center. This implies several ad- vantages, like an improved system’s overview and insight, because of greater abstraction and easier comprehension of the design concepts. Current modeling tools primarily provide a conﬁned work-place for the deﬁnition of models including proprietary services like code generation. This paper proposes separation of services and modeling tools to enable independent reuse of services. Thereby the eﬃciency of the modeling pro- cess increases as services become globally shared assets. Also the models, which are expressed in an open available format - independent of a par- ticular modeling tool - facilitate exchange and reuse. As a result, a user community grows and the quality of model artifacts improves because of frequent use, correction and peer review. This paper describes how these ideas are reﬂected in the design and func- tionality of the Evolution and Validation Environment EVE. EVE pro- vides an interoperability platform for model exchange. It consists solely of components which adhere to open speciﬁcations, such as XMI, UML, and OCL. EVE is designed as a loosely coupled system, which allows users to execute and combine services locally or over the Internet in arbitrary ways.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Representing Temporal Information in UML",
        "date": 2003,
        "abstract": "The UML is a non-temporal conceptual modeling language. Con- ceptual schemas in the UML assume that the information base contains the cur- rent instances of entity and relationship types. For many information systems, the above assumption is acceptable. However, there are some information sys- tems for which that assumption is a severe limitation. This happens when the functions of the information system require the knowledge of past states of the information base. In this paper we extend the UML to define a set of temporal features of entity and relationship types, and to provide notational devices to refer to any past state of the information base. Using this extension, a designer may use the UML/OCL as if it were a temporal conceptual modeling language. We also present a method for the transformation of a conceptual schema in this ex- tended language into a conventional one. The method can be automated, and we describe an implementation. The result of our transformation method is a conceptual schema that can be processed by ordinary CASE tools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Formal Semantics of UML with Real-Time\nConstructs⋆",
        "date": 2003,
        "abstract": "This paper describes a formal framework for expressing the semantics of UML augmented with real-time constraints. Supported parts of UML include concurrent interacting statecharts and sequence di- agrams, both with real-time constraints. The approach is compositional in the sense that semantics of real-time behavior is captured indepen- dently from traditional statechart semantics, thus allowing for simple adaptation to other semantic variations. This compositionality is sup- ported through the use of a two-dimensional temporal logic to indepen- dently capture ﬂow of control as well as ﬂow of time. The paper deﬁnes this logic, and shows how the UML diagrams can be translated into this formalism. The goal is to provide a simple and intuitive compositional semantics that can be validated and used for further formal analysis.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A QoS-Oriented Extension of UML Statecharts⋆",
        "date": 2003,
        "abstract": "Performance, dependability and quality of service (QoS) are prime aspects of the UML modeling domain. To capture these aspects eﬀectively in a modeling language requires easy-to-use support for the speciﬁcation and analysis of randomly varying behaviors. This paper introduces an extension of UML statecharts with randomly varying du- rations, by enriching a speciﬁc syntactic construct: The “after” operator is equipped with (discrete or continuous) probability distributions, deter- mining the duration of the delay caused by this operator. The semantics of this extension is given in terms of a variant of stochastic automata. It is shown how existing model-checking tools can be used to calculate model- inherent QoS characteristics automatically. We study a UML model of an automatic teller machine scenario using this approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CheckVML: A Tool for Model Checking Visual\nModeling Languages",
        "date": 2003,
        "abstract": "In the paper, we present a tool for model checking dynamic consistency properties in arbitrary well-formed instance models of any modeling language deﬁned visually by metamodeling and graph transformation techniques. Our tool ﬁrst translates such high-level speciﬁcations into a tool independent abstract representation of transition systems deﬁned by a corresponding metamodel. From this intermediate representation the input language of the back-end model checker tool (i.e., SPIN in our case) is generated automatically.",
        "keywords": [
            "visual modeling languages",
            "metamodeling",
            "graph transforma- tion",
            "model checking",
            "formal veriﬁcation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ProGUM-Web: Tool Support for Model-Based\nDevelopment of Web Applications",
        "date": 2003,
        "abstract": "ProGUM-Web is a tool that supports model-based development of Web applications using an extension of UML. It accounts for the characteristics of Web applications and their specific development. Code templates are gener- ated from the model for both graphic designers and software developers. These code templates can iteratively and independently be advanced and are re- integrated within ProGUM-Web. Prototypes of Web applications can automati- cally be generated throughout the development.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "106",
        "date": 2003,
        "abstract": "The success of object-oriented software modelling depends to a large extent on the ability to create adequate abstractions. While abstraction itself must remain an intellectual process, a modelling language can support or hinder this process by offering different kinds or dimensions of abstraction. For in- stance, adhering to the object-oriented paradigm UML incorporates classifica- tion and generalization as its key abstraction mechanisms. When it comes to taking the complexity out of real systems, however, we argue that classification and generalization alone are ill-suited to produce abstractions that are both manageable and meaningful. As a remedy, we propose to regard composition as an alternative form of abstraction, and find that it naturally comes with proper- ties that are practically needed. We contrast our view of composition with that of it being a special kind of association, with the composition of deployable elements, and with UML’s model management constructs such as packaging.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Compositional and Relational Reasoning during Class\nAbstraction",
        "date": 2003,
        "abstract": "Class diagrams are among the most widely used object-oriented de- sign techniques. They are effective in modeling the structure of software sys- tems at any stages of the software life cycle. Still, class diagrams can become as complex and overwhelming as the software systems they describe. This paper describes a technique for abstracting lower-level class structures into higher- level ones by ‘collapsing’ lower-level class patterns into single, higher-level classes and relationships. This paper is an extension to an existing technique that re-interprets the transitive meaning of lower-level classes into higher-level relationships (relational reasoning). The existing technique is briefly summa- rized. The extensions proposed in this paper are two-fold: This paper augments the set of abstraction rules to also collapse class patterns into higher-level classes (compositional reasoning). While this augmentation is simple and in sync with traditional views of refinement and abstraction, it has drawbacks in defining class features like methods and attributes. This paper thus also demon- strates how to filter low-level class features during abstraction. Our approach requires some human guidance in deciding when to use compositional or rela- tional reasoning but is otherwise fully automated. Our approach is conservative in its results guaranteeing completeness but at the expense of some false posi- tives (i.e., the filter errs in favor of not eliminating in case of doubt). The pro- posed technique is applicable to model understanding, inconsistency detection, and reverse engineering.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML/MDA Reality Check: Heterogenous\nArchitecture Style",
        "date": 2003,
        "abstract": "This talk looks at the OMG’s Model-Driven Architecture initiative and at UML from the perspective of an innovative approach to engineering software for space missions at NASA’s Jet Propulsion Lab- oratory (JPL). The Mission Data System (MDS) project at JPL breaks the conventional mold of space m¨oission software blending two heteroge- nous software architecture styles: state analysis (the idea that ”state”, operations involving state and expressions of intent about state are at the core of all space-based systems and therefore must form the central ba- sis for engineering such systems) and explicit software architecture (the idea that component architecture establishes the elements of software design and of their coherent integration.) From a modeling perspective, state analysis is a domain-speciﬁc software architecture style just like component architecture is an engineering-speciﬁc software architecture style. The heterogenous composition of multiple styles in MDS is an ex- ample of extreme architecture engineering. This talk will describe how this approach stretches in many ways the current state of practice and tool support in UML and MDA into the realm of bleeding edge software engineering research.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Automating Source-Consistent UML\nRefactorings",
        "date": 2003,
        "abstract": "With the increased interest in refactoring, UML tool vendors seek ways to support software developers in applying a (sequence of) refactoring(s). The problem with such tools is that the UML metamodel – on which their repository is based – is inadequate to maintain the consistency between the model and the code while one of them gets refactored. Therefore, we propose a set of minimal extensions to the UML metamodel, which is sufﬁcient to reason about refactoring for all common OO languages. For instance, by specifying pre- and postconditions in OCL, we are able to compose primitive refactorings, verify preservation of program behavior, and trigger refactorings based on code smells. This way, we provide future MDA tools with the ability to improve existing UML designs, yet keeping them in synch with the underlying code base.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Refactorings as Rule-Based Update\nTransformations",
        "date": 2003,
        "abstract": "A model refactoring is a model transformation that improves the design described in the model. A refactoring should only aﬀect a pre- viously chosen subset of the original model. In this paper, we discuss how to deﬁne and execute model refactorings as rule-based transformations. We also present an experimental tool to execute these transformations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "None"
        }
    },
    {
        "title": "Reﬂective Model Driven Engineering",
        "date": 2003,
        "abstract": "In many large organizations, the model transformations allowing the engineers to more or less automatically go from platform- independent models (PIM) to platform-speciﬁc models (PSM) are increasingly seen as vital assets. As tools evolve, it is critical that these transformations are not prisoners of a given CASE tool. Considering in this paper that a CASE tool can be seen as a platform for processing a model transformation, we propose to reﬂectively apply the MDA to itself. We propose to describe models of transformations that are CASE tool independent (platform-independent transformations or PIT) and from them to derive platform-speciﬁc transformations (PST). We show how this approach might help in reaching a consensus in the RFP on MOF QVT, including a solution for the declarative/imperative dilemma. We ﬁnally explore the consequences of this approach on the development life-cycle.",
        "keywords": [
            "model-driven engineering",
            "model transformation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "MOF QVT"
        }
    },
    {
        "title": "A Model-Driven Runtime Environment for Web\nApplications",
        "date": 2003,
        "abstract": "A large part of software development these days deals with building so-called Web applications. Many of these applications are data- base-powered and exhibit a page layout and navigational structure that is close to the class structure of the entities being managed by the sys- tem. Also, there is often only limited application-speciﬁc business logic. This makes the usual three-tier architectural approach unappealing, be- cause it results in a lot of unnecessary overhead. One possible solution to this problem is the use of model-driven architecture (MDA). A simple platform-independent domain model describing only the entity structure of interest could be transformed into a platform-speciﬁc model that in- corporates a persistence mechanism and a user interface. Yet, this raises a number of additional problems caused by the one-way, multi-transform- ational nature of the MDA process. To cope with these problems, the authors propose the notion of a model-driven runtime (MDR) environ- ment that is able to execute a platform-independent model for a speciﬁc purpose instead of transforming it. The paper explains the concepts of an MDR that interprets OCL-annotated class diagrams and state machines to realize Web applications. It shows the authors’ implementation of the approach, the Infolayer system, which is already used by a number of applications. Experiences from these applications are described, and the approach is compared to others.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Using UML and XMI for Generating Adaptive\nNavigation Sequences in Web-Based Systems",
        "date": 2003,
        "abstract": "In this paper we discuss a method for modelling and generating adap- tive navigation sequences from the UML state diagrams. The method is discussed on a case of adaptive e-course. Latest advances in UML model representation by means of XML based metadata interchange format can be successfully utilized for adaptive generation of the adaptive navigation sequences and can speed up a prototyping of navigation support in adaptive web-based systems. Adaptive generation means that generator can be parametrized. The generator can generate modiﬁed navigation support and appearance of information based on the observed user features according to the parameters . The widely accepted standard based means and tools for XML technology are used for implementing a method for transforming UML state diagrams into web site graph and visualization of that graph.",
        "keywords": [
            "adaptive web-based systems",
            "generator",
            "the UML state diagrams",
            "XMI."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Platform Independent Web Application Modeling",
        "date": 2003,
        "abstract": "This paper discusses platform independent web application modeling in the context of model-driven engineering. A specific metamodel (and associ- ated notation), companion of the UML metamodel, is introduced and motivated for the modeling of dynamic web specific concerns. Web applications are rep- resented in three independent aspects (business, hypertext and presentation). A kind of action language (based on OCL and Java) is used throughout these as- pects to write methods and actions, specify constraints and express conditions. The concepts described in the paper have been implemented in a tool and op- erational model-driven web information systems have been successfully de- ployed.",
        "keywords": [
            "model-driven engineering",
            "MDA",
            "web",
            "metamodel",
            "PIM."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Rigorous Testing by Merging Structural and\nBehavioral UML Representations",
        "date": 2003,
        "abstract": "Error detection and correction in the design phase can re- duce total costs and time to market. Yet, testing of design models usually consists of walk-throughs and inspections both of which lack the rigor of systematic testing. Test adequacy criteria for UML models help de- ﬁne necessary objectives during the process of test creation. These test criteria require coverage of various parts of UML models, such as struc- tural (Class Diagram) and behavioral (Sequence Diagram) views. Test criteria are speciﬁc to a particular UML view. Test cases on the other hand should cover parts of multiple views. To understand testing needs better, it is useful to be able to observe the eﬀect of tests on both Class Diagrams and Sequence Diagrams. We propose a new graph that en- capsulates the many paths that exist between objects via their method calls as a directed acyclic graph (OMDAG). We also introduce the object method execution table (OMET) that captures both execution sequence and associated attribute values by merging the UML views. The merging process is deﬁned in an algorithm that generates and executes tests.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Automated Support for Deriving Test Data\nfrom UML Statecharts",
        "date": 2003,
        "abstract": "Many statechart-based testing strategies result in specifying a set of paths to be executed through a (flattened) statechart. These techniques can usually be easily automated so that the tester does not have to go through the tedious procedure of deriving paths manually to comply with a coverage criterion. The next step is then to take each test path individually and derive test data, i.e., fully specified test cases. This requires that we determine the system state required for each event/transition that is part of the path to be tested and the input parameter values for all events and actions on the transitions. We propose here a methodology to automate this procedure, which is based on a careful normalization and analysis of event/action contracts and transition guards written with the Object Constraint Language (OCL). It is illustrated by a case study that exemplifies the steps and provides an initial validation. Though many of the steps are automated, some inputs are still required from the modeler and tester and further work is necessary to push the automation even further and solve a number of remaining issues. This paper is a first attempt at clarifying the issues and identifying the analysis steps required in such an endeavor.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Validation of UML and OCL Models by\nAutomatic Snapshot Generation",
        "date": 2003,
        "abstract": "We study the testing and certiﬁcation of UML and OCL models as supported by the validation tool USE. We extend the available USE features by introducing a language for deﬁning properties of desired snapshots and by showing how such snapshots are generated. Within the approach, it is possible to treat test cases and validation cases. Test cases show that snapshots having desired properties can be constructed. Vali- dation cases show that given properties are consequences of the original UML and OCL model.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Critique of UML’s Deﬁnition of the Use-Case\nClass",
        "date": 2003,
        "abstract": "UML is revealed to contain three diﬀerent defects concern- ing the use-case class that were buried in OOSE and handed over to it. These defects are: 1) the use-case class and its instance are unusu- ally deﬁned, 2) a conjecture that is against the deﬁnition of the class is introduced without any reasons, and 3) the execution procedure of a use-case instance does not actually work because of some ﬂaws concern- ing the execution control. These defects have been causing unnecessary confusion in UML’s speciﬁcation of the use-case class. An object-oriented real-world model is built that represents a typical situation of using a use case in the analysis and design stages, and another deﬁnition of the use-case class is constructed that successfully solves the problems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling Database Views with Derived Classes in the\nUML/OCL-Framework",
        "date": 2003,
        "abstract": "One of the central notions in database modelling is the notion of a database view. A database view closely corresponds to the notion of derived class in UML. This paper will show how the notion of a relational database view can be correctly expressed as a derived class in UML/OCL (version 2.0). A central part of our investigation concerns the generality of our manner of rep- resenting relational views in OCL. Since, in general terms, a database view closely corresponds to the notion of a named query, an important problem that we address in our paper is the expressiveness of OCL as a query language. In particular, we will discuss the relational completeness of OCL (w.r.t the rela- tional algebra). We will show that OCL (version 2.0) is relationally complete in a minimal sense, but not in a (desired) maximal sense. As a consequence, we will argue for certain language extensions in OCL in order to achieve that OCL is maximally relationally complete.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An OCL Extension for Low-Coupling Preserving\nContracts",
        "date": 2003,
        "abstract": "Design by contract, as introduced by B.Meyer, is of increasing im- portance to the OO community in the specification, reuse, and monitoring of classes. We strongly feel that class libraries of all programming languages should be equipped with contracts, insofar as these constitute a powerful and simple interface definition. Very powerful and expressive contracts can be written using the OCL language, although for operations with many effects on the system state, these contracts can become unmanageable and incomprehen- sible. In order to maintain contracts at a manageable level of complexity, we claim that the OCL powerful mechanism of navigation through associations should be used moderately when building contracts, and that the effects of non- query operations should be allowed to be referred to within pre- and post- conditions. To achieve that purpose, we propose an extension to OCL and pres- ent a formal semantics for it.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "325",
        "date": 2003,
        "abstract": "One of the challenges of the UML is that it means diﬀerent things to diﬀerent people, and the priorities of those who develop the UML are not the same as all the UML’s users. In this talk I’ll talk about these purposes of the UML and what I think these mean for the UML’s future.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Using Description Logic to Maintain\nConsistency between UML Models",
        "date": 2003,
        "abstract": "A software design is often modelled as a collection of UML diagrams. There is an inherent need to preserve consistency between these diagrams. Moreover, through evolution those diagrams get mod- iﬁed leading to possible inconsistency between diﬀerent versions of the diagrams. State-of-the-art UML CASE tools provide poor support for consistency maintenance. To solve this problem, an extension of the UML metamodel enabling support for consistency maintenance and a classi- ﬁcation of inconsistency problems is proposed. To achieve the detection and resolution of consistency conﬂicts, the use of description logic (DL) is presented. DL has the important property of being a decidable frag- ment of ﬁrst-order predicate logic. By means of a number of concrete experiments in Loom, we show the feasibility of using this formalism for the purpose of maintaining consistency between (evolving) UML models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling and Testing Legacy Data Consistency\nRequirements",
        "date": 2003,
        "abstract": "An increasing number of data sources are available on the Internet, many of which oﬀer semantically overlapping data, but based on diﬀerent schemas, or models. While it is often of interest to integrate such data sources, the lack of consistency among them makes this inte- gration diﬃcult. This paper addresses the need for new techniques that enable the modeling and consistency checking for legacy data sources. Speciﬁcally, the paper contributes to the development of a framework that enables consistency testing of data coming from diﬀerent types of data sources. The vehicle is UML and its accompanying XMI. The paper presents techniques for modeling consistency requirements using OCL and other UML modeling elements: it studies how models that describe the required consistencies among instances of legacy models can be de- signed in standard UML tools that support XMI. The paper also con- siders the automatic checking of consistency in the context of one of the modeling techniques. The legacy model instances that are inputs to the consistency check must be represented in XMI.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Consistency Workbench: A Tool for\nConsistency Management in UML-Based\nDevelopment⋆",
        "date": 2003,
        "abstract": "With the Uniﬁed Modeling Language becoming applied in diverse contexts, the ability of deﬁning and checking customized con- sistency conditions is gaining increasing importance. In this paper, we introduce the Consistency Workbench for deﬁning and establishing con- sistency in a UML-based development process.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Developing Safety-Critical Systems with UML",
        "date": 2003,
        "abstract": "Safety-critical systems have to be developed carefully to pre- vent loss of life and resources due to system failures. Some of their mech- anisms (for example, providing fault-tolerance) can be complicated to design and use correctly in the system context and are thus error-prone. We show how one can use UML for model-based development of safety- critical systems with the aim to increase the quality of the developed systems without an unacceptable increase in cost and time-to-market. Speciﬁcally, we describe how to use the UML extension mechanisms to include safety-requirements in a UML model which is then analyzed for satisfaction of the requirements. The approach can thus be used to en- capsulate safety engineering knowledge. It is supported by a prototypical XMI-based tool performing the analysis.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistent and Complete Access Control Policies in Use\nCases",
        "date": 2003,
        "abstract": "Security requirements of a software product need to receive attention throughout its development life cycle. This paper proposes several design arti- facts that specify the details of access control policies formally and precisely in the requirement and analysis phases. The work is based on extending the use cases in Unified Modeling Language, with access control schemas and tables. In addition, we propose a methodology to resolve several issues such as con- sistency and completeness of access control specifications that are not totally resolved before.",
        "keywords": [
            "access control policies",
            "security engineering",
            "use cases",
            "and formal methods."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "STAIRS – Steps to Analyze Interactions with Refinement\nSemantics",
        "date": 2003,
        "abstract": "The paper presents STAIRS, an approach to the compositional de- velopment of UML interactions supporting the specification of mandatory as well as potential behavior. STAIRS has been designed to facilitate the use of interactions for requirement capture as well as test specification. STAIRS as- signs a precise interpretation to the various steps in incremental system devel- opment based on an approach to refinement known from the field of formal methods, and provides thereby a foundation for compositional analysis. An in- teraction may characterize three main kinds of traces. A trace may be (1) posi- tive in the sense that it is valid, legal or desirable, (2) negative meaning that it is invalid, illegal or undesirable, or (3) considered irrelevant for the interaction in question. This categorization corresponds well with that of testing where the verdict of a test execution is either pass, fail or inconclusive. The basic incre- ments in system development are structured into three kinds referred to as sup- plementing, narrowing and detailing. Supplementing categorizes inconclusive traces as either positive or negative. Narrowing reduces the set of positive traces to capture new design decisions or to match the problem more adequately. De- tailing involves introducing a more detailed description without significantly altering the externally observable behavior.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Workshops at the UML 2003 Conference",
        "date": 2003,
        "abstract": "This year the UML conference hosts nine one-day work- shops.These selected workshops cover a wide range of topics related to the Uniﬁed Modeling Language. The high number of workshop propos- als received is a reﬂection of the attention that the conference is getting from both the academia and the industry.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tutorials at the UML 2003 Conference",
        "date": 2003,
        "abstract": "The UML 2003 conference provides ﬁve half-day tutorials on advanced topics related to UML, presented by recognized worldwide experts. A short summary of each tutorial and a list of its respective presenters are given, as follows.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Empirically Driven Use Case Metamodel Evolution⋆",
        "date": 2004,
        "abstract": "Metamodel evolution is rarely driven by empirical evidences of meta- model drawbacks. In this paper, the evolution of the use case metamodel used by the publicly available requirements management tool REM is presented. This evolution has been driven by the analysis of empirical data obtained during the assessment of several metrics–based veriﬁcation heuristics for use cases devel- oped by some of the authors and previously presented in other international fora. The empirical analysis has made evident that some common defects found in use cases developed by software engineering students were caused not only by their lack of experience but also by the expressive limitations imposed by the un- derlying use case metamodel used in REM. Once these limitations were clearly identiﬁed, a number of evolutionary changes were proposed to the REM use case metamodel in order to increase use case quality, i.e. to avoid those situations in which the metamodel were the cause of defects in use case speciﬁcations.",
        "keywords": [
            "metamodel evolution",
            "use cases",
            "empirical software engineering"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying OO Metrics to Assess UML Meta-models",
        "date": 2004,
        "abstract": "UML has been coming of age through more than seven years devel- opment, in which there are not only minor revision like from UML 1.1 to UML 1.2, but also significant improvement like final adoption of UML 2.0 submis- sions. However there is so far lack of an objective assessment to UML meta- models, which can be used to control and predict the evolution of the UML. In this paper we regard UML meta-models as the equivalent of Object-Oriented (OO) design models. Therefore, we can adapt OO design metrics and criteria as a method to assess UML meta-models. Our method conducts the assessment of stability and design quality to UML meta-models in versions of 1.1, 1.3, 1.4 (with Action Semantics), 1.5, and 2.0. Based on the results we analyze the evo- lution of the UML versions and provide the applicability suggestions to the method.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An OCL Formulation of UML2 Template\nBinding",
        "date": 2004,
        "abstract": "After being considered only as documentation for a long time, models are gaining more and more importance in the software development lifecycle, as full software artefacts. The UML standard con- tributes a lot to this mutation, with the identiﬁcation and the structura- tion of models space dimensions and constructs. Models can nowadays be explicitly manipulated through metamodeling techniques, dedicated tools or processes such as model transformation chains. This is ”Model Driven Engineering”. Once it is clear that models are full software ingre- dients, we are faced with new problems (needs!) such as the possibility of their reusability and composability. As a consequence, speciﬁc constructs are introduced in order to facilitate this, such as the template notion ini- tiated by UML1.3. Applications of this notion are growing more and more so that it was deeply revisited and strengthened in UML2. Though, its speciﬁcation still lacks precision, particularly concerning the ”binding” mechanism that allows to obtain models from templates. We propose a set of OCL constraints which strengthens the deﬁnition and helps in verifying the correctness of resulting models. These constraints apply to the UML2 metamodel and were implemented in an OCL veriﬁer that we integrated in the Eclipse environment.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Metamodel for Generating Performance Models from \nUML Designs",
        "date": 2004,
        "abstract": "Several different kinds of performance models can be generated from sets of scenarios that describe typical responses of a system, and their use of re- sources. The Core Scenario Model described here integrates the scenario and resource elements defined in a UML model with performance annotations, pre- paratory to generating performance models. It is based on, and aligned with the UML Profile for Schedulability, Performance and Time, and supports the gen- eration of predictive performance models using queueing networks, layered queueing, or timed Petri nets. It is proposed to develop it as an intermediate language for all performance formalisms.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the Classification of UML’s Meta Model Extension \nMechanism",
        "date": 2004,
        "abstract": "Although the UML meta model extension mechanism has been used in many modeling fields in which extension of UML is needed, UML specification has little necessary classification and application guidance on the meta model extension mechanism. This paper defines four levels of UML’s meta model extension mechanism, and discusses the readability, expression capability, use scope and tool support on the basis of precise definitions of each level. The work on the paper reinforces the maneuverability of the UML meta model extension mechanism, and provides a reliable theoretical base for the development of modeling tools that support meta model extension.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Business Processes in Web Applications\nwith ArgoUWE⋆",
        "date": 2004,
        "abstract": "The CASE tool ArgoUWE supports the systematic design of Web ap- plications using the UML-based Web Engineering (UWE) approach. The design methodology of UWE is based on a metamodel which is deﬁned as a lightweight extension of the UML metamodel in the form of a proﬁle and comprises the sep- arate modeling of the different aspects of a Web application: content, structure, layout, and business logic. ArgoUWE is implemented as a plugin into the open- source tool ArgoUML. In this paper, we focus on the latest improvements of the ArgoUWE tool: On the one hand, ArgoUWE supports the design of workﬂow- driven Web applications where business logic can be captured by process struc- ture and process ﬂow models. On the other hand, ArgoUML’s design critic mech- anism has been extended to indicate deﬁciencies and inconsistencies in UWE models based on the UWE metamodel and its OCL well-formedness rules.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Composition Directives",
        "date": 2004,
        "abstract": "An aspect-oriented design model consists of a set of aspect models and a primary model. Each of these models consists of a number of different kinds of UML diagrams. The models must be composed to identify conflicts and analyze the system as a whole. We have developed a systematic approach for composing class diagrams in which a default composition procedure based on name matching can be customized by user-defined composition directives. This paper describes a set of composition directives that constrain how class diagrams are composed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Query Models",
        "date": 2004,
        "abstract": "The need for querying software artifacts is a new emerging design issue in modern software development. Novel techniques such as Model-Driven Architecture or Aspect-Oriented Software Development heavily depend on powerful designation means to allocate elements in software artifacts, which are then either modified by transformation or enhanced by weaving processes. In this paper we present a new modeling notation for representing queries using the UML. We introduce special symbols for common selection purposes and specify their OCL selection semantics, which may be executed on existing UML models in order to allocate all selected model elements therein. By doing so, we aim to give forth the advantages of modeling to query design: Our query models facilitate the specification of queries independent from particular programming languages, ease their comprehension, and support their validation in a modeling context.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Specifying Cross-Cutting Requirement Concerns",
        "date": 2004,
        "abstract": "Addressing non-orthogonal software concerns that arise from requirements can significantly contribute to the complexity of developing large systems. Difficulties arise from the need to: locate related requirements, reason about the software concerns they represent, and analyze the impact of changing requirements. We address these issues through the use of requirements aspects. We present a method to identify requirements aspects from viewpoints, to associate requirements aspects with generic design solutions based on domain experience, and to specify the generic solutions using the UML. We demonstrate these techniques using a smart home controller application.",
        "keywords": [
            "cross-cutting concerns",
            "requirements aspects",
            "specification",
            "UML"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML Proﬁle to Model Mobile Systems",
        "date": 2004,
        "abstract": "The introduction of adaptation features in the design of ap- plications that operate in a mobile computing environment has been suggested as a viable solution to cope with the high heterogeneity and variability of this environment. Mobile code paradigms can be used to this purpose, since they allow to dynamically modify the load of the host- ing nodes and the internode traﬃc, to adapt to the resources available in the nodes and to the condition of the (often wireless) network link. In this paper we propose a UML proﬁle to deal with all the relevant issues of a mobile system, concerning the mobility of both physical (e.g. com- puting nodes) and logical (e.g. software components) entities. The proﬁle is deﬁned as a lightweight customization of the UML 2.0 metamodel, so remaining fully compliant with it. In the deﬁnition of this proﬁle, the underlying idea has been to model mobility (in both physical and logical sense) as a feature that can be “plugged” into a pre-existing architecture, to ease the modelling of both diﬀerent physical mobility scenarios, and of diﬀerent adaptation strategies based on code mobility. Besides deﬁning the proﬁle, we give some examples of use of its features.",
        "keywords": [
            "mobile computing",
            "code mobility",
            "UML proﬁle."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Experimental Evaluation of the UML Proﬁle for\nSchedulability, Performance, and Time",
        "date": 2004,
        "abstract": "We present a performance engineering methodology based upon the construction and solution of performance models generated me- chanically from UML sequence diagrams, annotated using the UML Pro- ﬁle for Schedulability, Performance and Time (SPT). The target platform for the performance analysis is the Labelled Transition System Analyser (LTSA) tool which supports model solution via discrete-event simula- tion. Simultaneously, LTSA allows functional properties of a system to be explored formally, and we show how this can be used to detect func- tional anomalies, such as unnecessary sequentialisation and deadlock, prior to analysing the performance aspects of a system. The approach is evaluated with reference to a case study – a simple robot-based man- ufacturing system. The main objective is to explore the ways in which UML, the SPT proﬁle and the LTSA tool can be used to design systems that satisfy speciﬁed behavioural and performance properties, through successive reﬁnement.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML Profile for Executable and Incremental \nSpecification-Level Modeling",
        "date": 2004,
        "abstract": "Model executability is widely considered an important enabling fac- tor for model driven development. However, executability of Unified Modeling Language (UML) models tends to imply quite a low level of abstraction, which causes executable models to resemble diagrammatically structured program code. In this article, a UML profile that enables executable specification-level modeling using an incremental approach is proposed. The profile employs the Object Constraint Language (OCL) with multi-object joint actions to declara- tively specify behavior on a higher level of abstraction than sequences of mes- sages between objects. A nondeterministic mode of execution removes the need for explicit control flow, greatly simplifying the models. A variant of superpo- sition is used to construct specification models incrementally, utilizing aspect- oriented layers, and preserving safety properties. The proposed mechanism also aims at bridging the gap between use cases and design level specifications. As the profile is based on ideas taken from the DisCo modeling language, origi- nally designed for formal specification of reactive systems, there is a straight- forward mapping that enables use of existing DisCo tools for animation, verifi- cation and synthesis. A running example is presented to illustrate the use of the proposed approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying Refactoring Techniques to UML/OCL Models",
        "date": 2004,
        "abstract": "The Object Constraint Language (OCL) plays an important role in the elaboration of precise UML models. Although OCL was designed to be both formal and simple, UML/OCL models may be difficult to understand and evolve, particularly when constraints containing complex or duplicate expressions are present. Moreover, the evaluation of how changes in the definition of the underlying classes impact the OCL part of a model may be a difficult and time-consuming task. In this paper, we discuss how refactoring techniques can be applied in order to improve the understandability of a UML/OCL model and how to support its evolution. In particular, we present a collection of refactorings and discuss how they can be specified and automated. We also show how the model animation features can be used to increase our confidence that the semantics of a model is preserved when a refactoring is manually performed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Detecting OCL Traps\nin the UML 2.0 Superstructure:\nAn Experience Report",
        "date": 2004,
        "abstract": "Currently, the OMG is developing a new version of the Uni- ﬁed Modeling Language (UML), UML 2.0, which involves major innova- tions in its metamodel. As for previous versions of the UML, the Object Constraint Language (OCL) is employed to give restrictions on the use of UML and for the formulation of additional operations. It seems that the OCL expressions in the current version of the UML 2.0 Superstructure have not been checked with a tool. In this paper we report on an experi- ment in checking and validating the well-formedness rules and operation deﬁnitions of the UML 2.0 Superstructure w.r.t. syntax and type check- ing by using our tool USE (UML-based Speciﬁcation Environment). For this purpose we classify the errors detected by USE in appropriate error categories. We develop statistical information on error frequencies w.r.t. package location and error category. All errors detected by USE and their detailed description are made available in a separate EXCEL ﬁle.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From Informal to Formal Speciﬁcations in UML",
        "date": 2004,
        "abstract": "In this paper, we consider a way of bridging informal and for- mal speciﬁcation. Most projects have a need for an informal description of the requirements of the system which all people involved can understand. At the same time, there is a need to make some of the requirements more formal. We present a way to relate informal requirements, in form of use cases, to more formal speciﬁcations, written in the Object Constraint Language (OCL). Our approach gives the customers of software systems a way of guiding the development of formal speciﬁcations. Conversely, the formal speciﬁcation can improve the informal understanding of the system by exposing gaps and ambiguities in the informal speciﬁcation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Building Precise UML Constructs to Model\nConcurrency Using OCL",
        "date": 2004,
        "abstract": "The UML has established itself as the main tool for build- ing software designs. However, one area that hasn’t been completely ex- plored is the semantically precise speciﬁcation of behavior for concurrent programs. We have studied the feasibility of creating precise, unambigu- ous UML concurrency speciﬁcations using the Object Constraint Lan- guage (OCL) as a cornerstone, particularly focusing on constructs for concurrent access to shared variables. In this paper, we show that such speciﬁcations are possible, and that we can create basic concurrency ab- stractions that are precise, speciﬁcally semaphores and monitors. These constructs can be successfully applied to model solutions to classic con- current problems, as we show in a monitor-based solution to the Sleeping Barber problem.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An ASM Deﬁnition of the Dynamic OCL 2.0 Semantics",
        "date": 2004,
        "abstract": "The recently adopted OCL 2.0 speciﬁcation comes with a formal se- mantics that is based on set theory with a notion of an object model and system states. System states keep the runtime information relevant for the evaluation of OCL expressions. However, not all new language concepts of OCL 2.0 are al- ready addressed in that formal semantics. We show how to overcome this by introducing new components to the object model and system states deﬁning a dynamic semantics of OCL. In order to give precise rules that determine when the current system state has to be updated according to a change in the referred UML model, we make use of adequate mathematical means, namely Abstract State Machines (ASMs). Though our ASM speciﬁcation also gives a clear def- inition for the evaluation of OCL constraints, it leaves sufﬁcient ﬂexibility for application speciﬁc implementations that have to determine when constraints are to be checked.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards a Framework for Mapping Between \nUML/OCL and XML/XQuery",
        "date": 2004,
        "abstract": "The Unified Modeling Language is the standard language for modeling systems. UML has been extended to model web applications. At the same time, Web technology has become largely relying on XML documents. The structure of XML documents, namely the XML schema or DTD for these documents can be modeled using UML data structures. UML tools are usually concerned with the generation of the structure and behavior of the system that is captured by models in their equivalents in the selected platform. In this paper we introduce a novel approach for the integration between UML and XML families of technologies. We model the structure of XML using UML class diagrams and based on this, we study how queries on XML documents, namely XQuery expressions can be described using UML techniques. Here we show that modeling of XML documents and its queries represented by XQuery expressions is possible using the querying capabilities of UML Class diagram and the Object Constraint Language (OCL). As a result, we see how these two technologies compare, what the advantages of both technologies are and how they can be combined.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Architecture for Automatic-Control:      \nAn Experience Report",
        "date": 2004,
        "abstract": "In the context of teaching distributed and embedded process-control it is difficult to bring students to perform efficient practical work, because the low-level underlying software technologies require too much time investment. Object-oriented modeling together with model-driven architecture provides a good solution to simplify such practical work, by automating the generation of the low-level code directly from the specifications of the process-control application. To support this approach, we have developed a model-driven application framework of about 130 distributed and embedded real-time components for RISC microcontrollers. This way, students can focus on the process-control side of their work, without having to dive (and often get lost) in the platform complexity.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Development for Non-functional\nProperties: Reﬁnement Through Model Transformation",
        "date": 2004,
        "abstract": "Model driven architecture (MDA) views application development as a continuous transformation of models of the target system. We propose a method- ology which extends this view to non-functional properties. Our basic idea is the separation of two different roles in the development process: the role of the mea- surement designer and the role of the application designer. The former provides a library of measurement deﬁnitions which is later used by the latter to anno- tate functional application models with non-functional property speciﬁcations. In this paper we deﬁne the notion of context models to allow the measurement designer to provide measurement deﬁnitions at different levels of abstraction in- dependently of concrete applications. Requiring the measurement designer to deﬁne transformations between context models and applying them to measurement deﬁnitions, enables us to provide tool support for reﬁnement of non-functional constraints to the application designer. The concepts presented in this paper form the basis of a tool which we are cur- rently developing.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generic and Meta-transformations for\nModel Transformation Engineering⋆",
        "date": 2004,
        "abstract": "The Model Driven Architecture necessitates not only the application of software engineering disciplines to the speciﬁcation of modeling languages (language-ware) but also to design inter and intra- language model transformations (transformation-ware). Although many model transformation approaches exist, their focus is almost exclusively put on functional correctness and intuitive description language while the importance of engineering issues such as reusability, maintainabil- ity, performance or compactness are neglected. To tackle these prob- lems following the MDA philosophy, we argue in the paper that model transformations should also be regarded as models (i.e., as data). More speciﬁcally, we demonstrate (i) how generic transformations can provide a very compact description of certain transformation problems and (ii) how meta-transformations can be designed that yield eﬃcient transfor- mations as their output model. Keywords: model transformation, metamodeling, meta-transformation, generic transformation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting Model Refactorings Through\nBehaviour Inheritance Consistencies",
        "date": 2004,
        "abstract": "This paper addresses the problem of consistency preserva- tion in model-driven software development. Software models typically embody many diﬀerent views that need to be kept consistent. In the context of consistency within a model, behaviour inheritance consisten- cies restrict the way the behaviour of a subclass can specialize the be- haviour of a superclass. In the context of model evolution, model refac- torings restructure a model while preserving its behavioural properties. It is still an open research question how to deﬁne behaviour preservation properties for model refactorings. We claim that behaviour inheritance consistencies correspond, in an evolution context, to the preservation of behavioural properties between model versions. To illustrate this claim, we implemented consistency rules and preservation behaviour rules in Racer, a reasoning engine for description logics. We show how the same logic rules can be used to detect behaviour inheritance inconsistencies in a model and to detect the preservation of call behaviour properties during model refactoring.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Determining the Structural Events That May Violate an \nIntegrity Constraint",
        "date": 2004,
        "abstract": "Any implementation of an information system must ensure that an operation is only applied if its execution does not lead to a violation of any of the integrity constraints defined in its conceptual schema. In this paper we propose a method to automatically determine the operations that may potentially violate an OCL integrity constraint in conceptual schemas defined in the UML. This is done by determining the structural events that may violate the constraint and checking whether those events appear in the operation specification. In this way, our method helps to improve efficiency of integrity checking since its results can be used to discard many irrelevant tests.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deductive Veriﬁcation of UML Models in\nTLPVS⋆",
        "date": 2004,
        "abstract": "In recent years, UML has been applied to the development of reactive safety-critical systems, in which the quality of the developed software is a key factor. In this paper we present an approach for the deductive veriﬁcation of such systems using the PVS interactive theorem prover. Using a PVS speciﬁcation of a UML kernel language semantics, we generate a formal representation of the UML model. This represen- tation is then veriﬁed using tlpvs, our PVS-based implementation of linear temporal logic and some of its proof rules. We apply our method by verifying two examples, demonstrating the feasibility of our approach on models with unbounded event queues, object creation, and variables of unbounded domain. We deﬁne a notion of fairness for UML systems, allowing us to verify both safety and liveness properties.",
        "keywords": [
            "Formal Veriﬁcation",
            "Deductive Veriﬁcation",
            "PVS",
            "UML",
            "State Machines",
            "Semantics",
            "Temporal Logic"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating a Security Requirement Language\nwith UML",
        "date": 2004,
        "abstract": "We present an approach that integrates a language for pre- cise and high-level speciﬁcation of application security requirements, the Security Requirement Language (SRL), with an existing modeling tech- nique, namely, the Uniﬁed Modeling Language (UML). SRL is based on ﬁrst-order logic extended with a small set of modal operators and a syntactic abstraction mechanism. It oﬀers extensibility in that new application/domain-speciﬁc requirements can be deﬁned and reused. The focus of SRL is the security of communication in distributed systems. The integrated framework enables developers to add to system models secu- rity requirements, such as conﬁdentiality, non-repudiation, and authenti- cation, at an early stage of development, making security an integral part of the system development process. We illustrate the practical usability of our approach by presenting an example, and discuss the experiences that the users of our approach, i.e., system developers, have reported.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Veriﬁcation of UMLsec Models\nfor Security Requirements",
        "date": 2004,
        "abstract": "For model-based development to be a success in practice, it needs to have a convincing added-value associated with its use. Our goal is to provide such added-value by developing tool-support for the anal- ysis of UML models against diﬃcult system requirements. Towards this goal, we describe a UML veriﬁcation framework supporting the construc- tion of automated requirements analysis tools for UML diagrams. The framework is connected to industrial CASE tools using XMI and allows convenient access to this data and to the human user. As a particular example for usage of this framework, we present veri- ﬁcation routines for verifying models of the security extension UMLsec of UML. These plug-ins should not only contribute towards usage of UMLsec in practice by oﬀering automated analysis routines connected to popular CASE tools. The veriﬁcation framework should also allow advanced users of the UMLsec approach to themselves implement veri- ﬁcation routines for the constraints of self-deﬁned stereotypes, in a way that allows them to concentrate on the veriﬁcation logic. In particular, we focus on an analysis plug-in that utilises the model-checker Spin to verify security properties of UMLsec models which make use of cryptography (such as cryptographic protocols).",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending OCL for Secure Database Development",
        "date": 2004,
        "abstract": "The Model Driven Architecture (MDA) is becoming an important aspect of software development, since it considers languages and models that can represent an information system at different abstraction levels, and makes it possible a coherent transformation of the system from the domain context into the machine context. In this paper, we present the Object Security Constraint Language V.2. (OSCL2), which is based on the well-known Object Constraint Language V.2. (OCL) of the Unified Modeling Language (UML), and which needs an extension of the UML1 metamodel. This language is defined to be used in secure database development process, incorporating security informa- tion and constraints in a Platform Independent Model (UML class model). This security information and constraints are then translated into a Platform Specific Model (multilevel relational model). Finally, they are implemented in a particu- lar Database Management System (DBMS), such as Oracle9i Label Security. These transformations can be done automatically or semi-automatically using OSCL2 compilers. Keywords: OCL, security constraints, multilevel databases, UML, confidentiality.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "OSCL2"
        }
    },
    {
        "title": "Test Driven Development of UML Models\nwith SMART Modeling System",
        "date": 2004,
        "abstract": "We are developing a methodology for Test-Driven Develop- ment of Models (TDDM) based on an experimental UML 2.0 modeling tool SMART. Our experience shows that TDDM is quite useful for agile model developments. SMART provides guidance on how to build models based on compiler errors of testcases, something similar to what Quick Fix of Eclipse does. It also provides such guidance from failures of test- cases, which seems diﬃcult in the case of TDD of programs.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Behavioral Domain Analysis – The Application-Based \nDomain Modeling Approach",
        "date": 2004,
        "abstract": "Being part of domain engineering, domain analysis enables identifying domains and capturing their ontologies in order to assist and guide system developers to design domain-specific applications. Domain analysis should consider commonalities and differences of systems in a domain, organize an understanding of the relationships between the various elements in that domain, and represent this understanding in a formal, yet easy to use, way. Several studies suggest using metamodeling techniques for modeling domains and their constraints. These metamodels are basically structural and present static constraints only. We propose an Application-based DOmain Modeling (ADOM) approach for domain analysis. This approach treats a domain as a regular application that needs to be modeled before systems of that domain are specified and designed. This way, the domain structure and behavior are modeled, enforcing static and dynamic constraints on the relevant application models. The ADOM approach consists of three-layers: the language layer handles modeling language ontologies and their constraints, the domain layer holds the building elements of domains and the relations among them, and the application layer consists of domain-specific systems. Furthermore, the ADOM approach defines dependency and enforcement relations between these layers. In this paper we focus on applying the ADOM approach to UML and especially to its class and sequence diagrams.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using UML-based Feature Models and UML\nCollaboration Diagrams to Information Modelling for\nWeb-Based Applications",
        "date": 2004,
        "abstract": "Web oriented software technology has provided access to informa- tion serving environments for a broad audience. This situation requires web- based software applications which satisfy increasing variety of requirements of the broad audience. Such variability can be found in requirements for informa- tion but also for environment which is serving the information. In this paper, we discuss a method which utilizes the UML-based feature modelling to support the need to model the variability. The information and environment conﬁgurations are modelled as common and variable features of application domain and en- vironment concepts. Separation of feature models into application domain and environment allows us to select several conﬁgurations of environments to deliver particular information. The UML collaboration diagrams model collaborations between the application domain and environment concept and feature instances as an abstraction for presented information fragments in a web application.",
        "keywords": [
            "Feature modelling",
            "information modelling",
            "web-based application",
            "UML collaboration diagrams"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Workshops at the UML 2004 Conference",
        "date": 2004,
        "abstract": "UML 2004 conference hosts twelve workshops. These selected events cover a wide range of hot topics related to the Uniﬁed Modeling Language. In the following, a brief summary of each workshop, along with the list of organizers and references for further information are given.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Tutorials at the UML 2004 Conference",
        "date": 2004,
        "abstract": "The UML 2004 conference provides six half-day tutorials on advanced topics related to UML, presented by recognized worldwide ex- perts. A short summary of each tutorial and a list of its respective pre- senters are given in section 2.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Activity Diagram Patterns for Modeling Quality \nConstraints in Business Processes",
        "date": 2005,
        "abstract": "Quality management is an important aspect of business processes. Organizations must implement quality requirements, e.g., according to standards like ISO 9001. Existing approaches on business process modeling provide no explicit means to enforce such requirements. UML Activity Diagrams are a well recognized way of representing those business processes. In this paper, we present an approach for enforcing quality requirements in such business processes through the application of process quality patterns to Activity Diagrams. These patterns are defined using a pattern description language, being a light-weight extension of UML Activity Diagrams. Accordingly, such patterns can be used in forward-engineering of business processes that incorporate quality constraints right from the beginning.",
        "keywords": [
            "UML Activity Diagrams",
            "Business Process",
            "Process Quality",
            "ISO 9001"
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML4SPM: A UML2.0-Based Metamodel for Software \nProcess Modelling1",
        "date": 2005,
        "abstract": "In the context of Model Driven Development, models play a central role. Since models can nowadays be executed, they are used not only for description but also for production [32][30][24]. In the field of software process modelling, the current version of the OMG SPEM standard (ver1.1) has not yet reached the level required for the specification of executable models. The purpose of SPEM1.1 was limited at providing process descriptions to be read by humans and to be supported by tools, but not to be executed. Therefore, the OMG issued a new RFP in order to improve SPEM1.1 [35]. Since we intend to participate in the next major revision of SPEM, namely SPEM2.0, in this work, we: 1) compare SPEM1.1 both with primary process model elements (i.e. Activity, Product, Role,…) and with basic requirements that any Process Modelling Language should support (i.e. expressiveness, understandability, executability,…); 2) identify its major limitations and advantages and 3) propose a new UML2.0-based metamodel for software process modelling named: UML4SPM. It extends a subset of UML2.0 concepts - with no impact on the standard - in order to fit software process modelling.",
        "keywords": [
            "MDD",
            "Software Process Modelling",
            "Process Modelling  Languages",
            "SP Metamodel."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Realizing Model Driven Security for  \nInter-organizational Workflows with WS-CDL and \n UML 2.0 \nBringing Web Services, Security and UML Together",
        "date": 2005,
        "abstract": "The growing popularity of standards related to Web services, Web services security and workflows boosted the implementation of powerful infrastructures supporting interoperability for inter-organizational workflows. Nevertheless, the realization of such workflows is a very complex task, in many aspects still bound to low-level technical knowledge and error-prone. We provide a framework for the realization and the management of security-critical workflows based on the paradigm of Model Driven Security. The framework complies with a hierarchical stack of Web services specifications and related technologies. In this paper, we introduce a UML based approach for the modeling of security-critical inter-organizational workflows and map it to the Web Services Choreography Description Language. Our approach is based on a set of security patterns, which are integrated into UML class and activity diagrams. A tool translates the models into executable artifacts configuring a reference architecture based on Web services.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Code Generation from UML Models with Semantic Variation Points⋆",
        "date": 2005,
        "abstract": "UML semantic variation points provide intentional degrees of freedom for the interpretation of the metamodel semantics. The inter- est of semantic variation points is that UML now becomes a family of languages sharing lot of commonalities and some variabilities that one can customize for a given application domain. In this paper, we propose to reify the various semantic variation points of UML 2.0 statecharts into models of their own to avoid hardcoding the semantic choices in the tools. We do the same for various implementation choices. Then, along the line of the OMG’s Model Driven Architecture, these semantic and implementation models are processed along with a source UML model (that can be seen as a PIM) to provide a target UML model (a PSM) where all semantic and implementation choice are made explicit. This target model can in turn serve as a basis for a consistent use of code generation, simulation, model-checking or test generation tools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Composing Domain-Specific Languages for                  \nWide-Scope Software Engineering Applications",
        "date": 2005,
        "abstract": "Domain-Specific Languages (DSL) offer many advantages over gen- eral languages, but their narrow scope makes them really effective only in very focused domains, for example Product Lines. The recent Model Driven Engi- neering (MDE) approach seeks to provide a technology to compose and com- bine models coming from different metamodels. Adapted to DSL, it means that it should be possible to compose “programs” written in different DSLs, which will enable the use of the DSL approach to build applications spanning different domains. The paper presents the Mélusine environment, where such a composi- tion technology has been developed and experimented.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Typing for Improving Reuse in\nModel-Driven Engineering",
        "date": 2005,
        "abstract": "Where object-oriented languages deal with objects as de- scribed by classes, model-driven development uses models, as graphs of interconnected objects, described by metamodels. A number of new lan- guages have been and continue to be developed for this model-based paradigm, both for model transformation and for general programming using models. Many of these use single-object approaches to typing, de- rived from solutions found in object-oriented systems, while others use metamodels as model types, but without a clear notion of polymorphism. Both of these approaches lead to brittle and overly restrictive reuse char- acteristics. In this paper we propose a simple extension to object-oriented typing to better cater for a model-oriented context, including a simple strategy for typing models as a collection of interconnected objects. Us- ing a simple example we show how this extended approach permits more ﬂexible reuse, while preserving type safety.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML Vs. Classical Vs. Rhapsody Statecharts:\nNot All Models Are Created Equal",
        "date": 2005,
        "abstract": "State machines, represented by statecharts or statechart dia- grams, are an important formalism for behavioural modelling. According to the research literature, the most popular statechart formalisms ap- pear to be Classical, UML, and that implemented by Rhapsody. These three formalisms seem to be very similar; however, there are several key syntactic and semantic diﬀerences. These diﬀerences are enough that a model written in one formalism could be ill-formed in another formalism. Worse, a model from one formalism might actually be well-formed in an- other, but be interpreted diﬀerently due to the semantic diﬀerences. This paper summarizes the results of a comparative study of these three for- malisms with the help of several illustrative examples. Then, we present a classiﬁcation of the diﬀerences together with a comprehensive overview.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating the Effect of Composite States on the \nUnderstandability of UML Statechart Diagrams",
        "date": 2005,
        "abstract": "UML statechart diagrams have become an important technique for describing the dynamic behavior of a software system. They are also a signifi- cant element of OO design, especially in code generation frameworks such as Model Driven Architecture (MDA). In previous works we have defined a set of metrics for evaluating structural properties of UML statechart diagrams and have validated them as early understandability indicators, through a family of controlled experiments. Those experiments have also revealed that the number of composite states had, apparently, no influence on the understandability of the diagrams. This fact seemed a bit suspicious to us and we decided to go a step further. So in this work we present a controlled experiment and a replication, focusing on the effect of composite states on the understandability of UML statechart diagrams. The results of the experiment confirm, to some extent, our intuition that the use of composite states improves the understandability of the diagrams, so long as the subjects of the experiment have had some previous ex- perience in using them. There are educational implications here, as our results justify giving extra emphasis to the use of composite states in UML statechart diagrams in Software Engineering courses.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Computing Refactorings of Behavior Models",
        "date": 2005,
        "abstract": "For given behavior models expressed in statechart-like for- malisms, we show how to compute semantically equivalent but struc- turally diﬀerent models. These refactorings are deﬁned by user-provided logical predicates that partition the system’s state space and that char- acterize coherent parts—modes or control states—of the behavior.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dynamic Secure Aspect Modeling with UML: From Models to Code",
        "date": 2005,
        "abstract": "Security engineering deals with modeling, analysis, and im- plementation of complex security mechanisms. The dynamic nature of such mechanisms makes it diﬃcult to anticipate undesirable emergent behavior. In this work, we propose an approach to develop and analyze security-critical speciﬁcations and implementations using aspect-oriented modeling. Since we focus on the dynamic views of a system, our work is complementary to existing approaches to security aspects mostly con- cerned with static views. Our approach includes a link to implementa- tions in so far as the code which is constructed from the models can be analyzed automatically for satisfaction of the security requirements stated in the UML diagrams. We present tool support for our approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Performance Analysis of UML Models Using                \nAspect-Oriented Modeling Techniques",
        "date": 2005,
        "abstract": "Aspect-Oriented Modeling (AOM) techniques allow software de- signers to isolate and address separately solutions for crosscutting concerns (such as security, reliability, new functional features, etc.) This paper proposes an approach for analyzing the performance effects of a given aspect on the overall system performance, after the composition of the aspect model with the primary model of a system. Performance analysis of UML models is enabled by the \"UML Performance Profile for Schedulability, Performance and Time\" (SPT) standardized by OMG, which defines a set of quantitative performance annotations to be added to a UML model. The first step of the proposed ap- proach is to add performance annotations to both the primary model and to the aspect model(s). An aspect model is generic at first, and therefore its perform- ance annotations must be parameterized. A generic model will be converted into a context-specific aspect model with concrete values assigned to its per- formance annotations. The latter is composed with the primary model, generat- ing a complete annotated UML model. By using existing techniques, the com- plete model is transformed automatically into a Layered Queueing Network (LQN) performance model, which can be analyzed with existing solvers. The proposed approach is illustrated with a case study system, whose primary model is enhanced with some security features by using AOM. The LQN model of the primary system was validated against measurements in previous work. The per- formance effects of the security aspect under consideration are analyzed in two design alternatives by using the LQN model of the composed system.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain Models Are Aspect Free",
        "date": 2005,
        "abstract": "Proponents of aspect orientation have successfully seeded the im- pression that aspects—like objects—are so fundamental a notion that they should pervade all phases and artefacts of the software development process. Aspect orientation has therefore proliferated from programming to design to analysis to requirements, sparing neither software processes nor their favourite languages. Since modelling plays an important role in software engineering, much effort is currently being invested in making modelling languages aspect ready. However, based on an observed lack of examples for domain level (or functional) aspects this paper argues the case against the omnipresence of as- pects, particularly the existence of aspects in domain models, and offers some informal arguments as well as a semiformal proof in favour of the claims made.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Representing and Applying Design Patterns:              \nWhat Is the Problem?",
        "date": 2005,
        "abstract": "Design patterns embody proven solutions to recurring design problems. Ever since the gang of four popularized the concept, researchers have been trying to develop methods for representing design patterns, and applying them to modeling problems. To the best of our knowledge, none of the approaches proposed so far represents the design problem that the pattern is meant to solve, explicitly. An explicit representation of the problem has several advantages, including 1) a better characterization of the problem space addressed by the pattern—better than the textual description embodied in pattern documentation templates, 2) a more natural representation of the transformations embodied in the application of the pattern, and 3) a better handle on the automatic detection and application of patterns. In this paper, we describe the principles underlying our approach, and the current implementation in the Eclipse Modeling Framework™.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Properties of Stereotypes from the Perspective of Their \nRole in Designs",
        "date": 2005,
        "abstract": "Stereotypes in object-oriented software development can be perceived in various ways and they can be used for various purposes. As a consequence of these variations, assessing quality of stereotypes needs to be purpose-specific. In this paper we identify eight types of stereotypes and provide a set of criteria for assessing quality of stereotypes. The criteria for each type are formed by a set of properties that characterizes its stereotypes. The identified types are based on the purpose of each stereotype (its role in designs) and its expressiveness. We identified the types of stereotypes and their properties in an empirical way by investigating stereotypes from UML profiles used in industrial software development. The properties are intended to be used in our further research for developing guidelines for creating and using stereotypes in a more efficient way.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A Modelling and Simulation Based Approach to\nDependable System Design",
        "date": 2005,
        "abstract": "Complex real-time system design needs to address dependability re- quirements, such as safety, reliability, and security. We introduce a modelling and simulation based approach which allows for the analysis and prediction of dependability constraints. Dependability can be improved by making use of fault tolerance techniques. The de-facto example in the real-time system literature of a pump control system in a mining environment is used to demonstrate our model- based approach. In particular, the system is modelled using the Discrete EVent system Speciﬁcation (DEVS) formalism, and then extended to incorporate fault tolerance mechanisms. The modularity of the DEVS formalism facilitates this extension. The simulation demonstrates that the employed fault tolerance tech- niques are effective. That is, the system performs satisfactorily despite the pres- ence of faults. This approach also makes it possible to make an informed choice between different fault tolerance techniques. Performance metrics are used to measure the reliability and safety of the system, and to evaluate the dependabil- ity achieved by the design. In our model-based development process, modelling, simulation and eventual deployment of the system are seamlessly integrated.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending Profiles with  \nStereotypes for Composite Concepts1",
        "date": 2005,
        "abstract": "This paper proposes an extension of the UML 2.0 profiling mecha- nism. This extension facilitates a language designer to introduce composite concepts as separate conceptual and notational elements in a modelling lan- guage. Composite concepts are compositions of existing concepts. To facilitate the introduction of composite concepts, the notion of stereotype is extended. This extension defines how a composite concept can be specified and added to a language’s metamodel, without modifying the existing metamodel. From the definition of the stereotype, rules can be derived for transforming a language element that represents a composite concept into a composition of language elements that represent the concepts that constitute the composite. Such a trans- formation facilitates tool developers to introduce tool support for composite concepts, e.g., by re-using existing tools that support the constituent concepts. To illustrate our ideas, example definitions of stereotypes and transformations for composite concepts are presented.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Transformation from CIM to PIM: A Feature-Oriented \nComponent-Based Approach",
        "date": 2005,
        "abstract": "Model Transformation is a crucial part of Model-Driven Architecture (MDA). However, most of the current researches only focus on the transforma- tion from PIM to PSM, and pay little attention to the CIM-to-PIM transforma- tion. One of the results is that converting CIM to PIM will depend much on de- signers’ personal experience or creativity, and thus the quality of PIM can not be well controlled. This paper presents a feature-oriented component-based ap- proach to the CIM-to-PIM transformation. In this approach, features and com- ponents are adopted as the key elements of CIM and PIM, respectively. One important characteristic of this approach is that it provides a method to decom- pose the n-to-n relations between features and components into two groups of 1-to-n relations. The other important characteristic is that this approach pro- poses a way to create components by clustering responsibilities which are op- erationalized from features. These two characteristics partially resolve two ba- sic problems related to the CIM-to-PIM transformation: one is the traceability problem between CIM and PIM, the other is the problem of CIM-based PIM construction.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Weaving Executability into                                       \nObject-Oriented Meta-languages",
        "date": 2005,
        "abstract": "Nowadays, object-oriented meta-languages such as MOF (Meta- Object Facility) are increasingly used to specify domain-specific languages in the model-driven engineering community. However, these meta-languages focus on structural specifications and have no built-in support for specifications of operational semantics. In this paper we explore the idea of using aspect- oriented modeling to add precise action specifications with static type checking and genericity at the meta level, and examine related issues and possible solutions. We believe that such a combination would bring significant benefits to the community, such as the specification, simulation and testing of operational semantics of metamodels. We present requirements for such statically-typed meta-languages and rationales for the aforementioned benefits.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Refactoring OCL Annotated UML Class Diagrams⋆",
        "date": 2005,
        "abstract": "Refactoring of UML class diagrams is an emerging research topic and heavily inspired by refactoring of program code written in object-oriented implementation languages. Current class diagram refac- toring techniques concentrate on the diagrammatic part but neglect OCL constraints that might become syntactically incorrect by changing the underlying class diagram. This paper formalizes the most important refactoring rules for class diagrams and classiﬁes them with respect to their impact on annotated OCL constraints. For refactoring rules, whose application on class diagrams could make attached OCL constraints in- correct, we formally describe how the OCL constraints have to be refac- tored to preserve their syntactical correctness. Our refactoring rules are deﬁned in the graph-grammar based formalism proposed by the QVT Merge Group for the speciﬁcation of model transformations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Replicators: Transformations to Address Model \nScalability",
        "date": 2005,
        "abstract": "In Model Integrated Computing, it is desirable to evaluate different design alternatives as they relate to issues of scalability. A typical approach to address scalability is to create a base model that captures the key interactions of various components (i.e., the essential properties and connections among modeling entities). A collection of base models can be adorned with necessary information to characterize their replication. In current practice, replication is accomplished by scaling the base model manually. This is a time-consuming process that represents a source of error, especially when there are deep interactions between model components. As an alternative to the manual process, this paper presents the idea of a replicator, which is a model transformation that expands the number of elements from the base model and makes the correct connections among the generated modeling elements. The paper motivates the need for replicators through case studies taken from models supporting different domains.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Simplifying Transformations of OCL Constraints",
        "date": 2005,
        "abstract": "With the advent of Model Driven Architecture, OCL con- straints are no longer necessarily written by humans. They can be part of models that emerge from a chain of transformations. They might be the result of instantiating templates, of combining prefabricated parts, or of more general computation. Such generated speciﬁcations will of- ten contain redundancies that reduce their readability. In this paper, we explore the possibilities of transforming OCL formulae to a simpler form through the repeated application of simple rules. We discuss the diﬀerent kinds of rules that are needed, and we describe a prototypical implementation of the approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lessons Learned from Automated Analysis of Industrial UML Class Models (An Experience Report) ⋆⋆⋆",
        "date": 2005,
        "abstract": "Automated analysis of object-oriented design models can provide insight into the quality of a given software design. Data obtained from automated analysis, however, is often too complex to be easily un- derstood by a designer. This paper examines the use of an automated analysis tool on industrial software UML class models, where one set of models was created as part of the design process and the other was obtained from reverse engineering code. The analysis was performed by DesignAdvisor, a tool developed by Siemens Corporate Research, that supports metrics-based analysis and detection of design guideline viola- tions. The paper describes the lessons learned from using the automated analysis techniques to assess the quality of these models. We also assess the impact of design pattern use in the overall quality of the models. Based on our lessons learned, identify design guidelines that would min- imize the occurrence of these errors.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reliability Prediction in Model-Driven Development",
        "date": 2005,
        "abstract": "Evaluating the implications of an architecture design early in the soft- ware development lifecycle is important in order to reduce costs of development. Reliability is an important concern with regard to the correct delivery of software system service. Recently, the UML Proﬁle for Modeling Quality of Service has deﬁned a set of UML extensions to represent dependability concerns (including reliability) and other non-functional requirements in early stages of the software development lifecycle. Our research has shown that these extensions are not com- prehensive enough to support reliability analysis for model-driven software engi- neering, because the description of reliability characteristics in this proﬁle lacks support for certain dynamic aspects that are essential in modeling reliability. In this work, we deﬁne a proﬁle for reliability analysis by extending the UML 2.0 speciﬁcation to support reliability prediction based on scenario speciﬁcations. A UML model speciﬁed using the proﬁle is translated to a labelled transition system (LTS), which is used for automated reliability prediction and identiﬁcation of im- plied scenarios; the results of this analysis are then fed back to the UML model. The result is a comprehensive framework for addressing software reliability mod- eling, including analysis and evolution of reliability predictions. We exemplify our approach using the Boiler System used in previous work and demonstrate how reliability analysis results can be integrated into UML models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Based Scalability Estimation in Inception-Phase \nSoftware Architecture",
        "date": 2005,
        "abstract": "Scalability is one of the crucial nonfunctional requirements that must be evaluated in the Inception Phase of the Rational Unified Process [9]. This is the phase in which the least information is generally available to form a principled evaluation. We demonstrate how an estimate of user scalability can be formed using sequence diagrams of the common user scenarios, together with experimentation (ranging from simple timing measurements to more complex architectural prototypes), published study data, and performance data from baseline systems. Despite being quite inexpensive, the techniques used by our team enabled us to identify and guide corrective actions for major bottlenecks before they became serious design flaws in the Elaboration and Construction phases of the Unified Process. The same techniques also allowed us to quickly evaluate the effects of high-level architecture and technology alternatives on user scalability and response time.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Explicit Platform Models for MDA",
        "date": 2005,
        "abstract": "The main drive for Model-Driven Architecture is that many software applications have to be deployed on a variety of platforms. The way MDA achieves this is by transforming a platform-independent model of the software to a platform-speciﬁc model, given a platform model. In current MDA approaches, the model transformations implicitly represent this platform model. Therefore, the number of diﬀerent target platforms is limited to the number of supported model transformations. We pro- pose a separate platform model, based on description logics, that can can be used to automatically select and conﬁgure a number of reusable model transformations for a concrete platform. This platform model can be extended to describe the relevant platform information, including con- crete platform instances as well as platform constraints for each model transformation. This separates the model transformation concern from the platform concern and, since the model transformations are no longer limited to targeting one platform, more platforms can be supported with the same set of transformations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrated Model-Based Software Development,\nData Access, and Data Migration",
        "date": 2005,
        "abstract": "In this paper we describe a framework for robust system maintenance that addresses speciﬁc challenges of data-centric applica- tions. We show that for data-centric applications, classical simultaneous roundtrip engineering approaches are not suﬃcient. Instead we propose an architecture that is an integrated model-based approach for software development, database access and data migration. We explain the canon- ical development process to exploit its features. We explain how the approach ﬁts into the model-driven architecture vision. We report on ex- periences with the approach in the IMIS environmental mass database project.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Concepts for Comparing Modeling Tool Architectures",
        "date": 2005,
        "abstract": "As model-driven development techniques grow in importance so do the capabilities and features of the tools that support them, especially tools that allow users to customize their modeling language. Superficially, many model- ing tools seem to offer similar functionality, but under the surface there are important differences that can have an impact on tool builders and users depen- ding on the tool architecture chosen. At present, however, there is no estab- lished conceptual framework for characterizing and comparing different tool architectures. In this paper we address this problem by first introducing a con- ceptual framework for capturing tool architectures, and then—using this framework—discuss the choices available to designers of tools. We then com- pare and contrast the main canonical architectures in use today.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scenario Construction Tool Based on Extended UML Metamodel",
        "date": 2005,
        "abstract": "Scenario based notations are becoming more and more popu- lar as means for user requirements elicitation. They can be used in more formal speciﬁcations as part of detailed use case templates or in agile processes to capture informal user stories. Despite their signiﬁcance in software engineering, scenarios seem not to be properly supported by appropriate tools. This paper describes a scenario construction tool that oﬀers clear separation of the actual story from notions used therein. The tool is constructed as an extension to visual notation of UML’s use cases. It is based on an extended UML metamodel in the area of activi- ties and classiﬁers. This formal basis makes the tool capable of supplying the existing UML tools with an additional layer of requirements models based on scenarios and notions. This layer makes it possible to trans- form requirements directly into design-level models. The tool oﬀers such transformation capabilities based on a simple model mapping. This trans- formation supports human eﬀorts to keep the system’s design consistent with the user’s needs expressed through scenarios.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Impact of UML 2.0 on Existing UML 1.4 Models",
        "date": 2005,
        "abstract": "The Unified Modeling Language (UML) is the accepted standard for object-oriented modeling across the software design industry. Version 2.0 of the UML represents a major new revision to this standard and includes many changes to the current industry state of the practice (UML 1.4). These revisions include the removal or renaming of some existing features as well as the addition of several new capabilities. As tool vendors and software engineers begin to adopt UML 2.0, there is a potential to greatly impact legacy systems and practitioners employing UML 1.4. This report aims at providing an understanding of the changes made in UML 2.0 and their potential impacts, both positive and negative, to the UML 1.4 modeling community.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards UML 2 Extensions for Compact\nModeling of Regular Complex Topologies",
        "date": 2005,
        "abstract": "The MARTE RFP (Modeling and Analysis of Real-Time and Embedded systems) was issued by the OMG in February 2005. This request for proposals solicits submissions for a UML proﬁle that adds ca- pabilities for modeling Real Time and Embedded Systems (RTES), and for analyzing schedulability and performance properties of UML speciﬁ- cations. One of the particular request of this RFP concerns the deﬁnition of common high-level modeling constructs for factorizing repetitive struc- tures, for software, hardware and allocation modeling of RTES. We pro- pose an answer to this particular requirement, based on the introduction of multi-dimensional multiplicities and mechanisms for the description of regular connection patterns between model elements. This proposition is domain independent. We illustrate the use of these mechanisms in an in- tensive computation embedded system co-design methodology. We focus on what these factorization mechanisms can bring for each of the aspects of the co-design: application, hardware architecture, and allocation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using UML 2.0 Collaborations for Compositional\nService Speciﬁcation",
        "date": 2005,
        "abstract": "Collaborations and collaboration uses are features new to UML 2.0. They possess many properties that support rapid and compositional service en- gineering. The notion of collaboration corresponds well with the notion of a ser- vice, and it seems promising to use them for service speciﬁcation. We present an approach where collaborations are used to specify services, and show how col- laborations enable high level feature composition by means of collaboration uses. We also show how service goals can be combined with behavior descriptions of collaborations to form what we call semantic interfaces. Semantic interfaces can be used to ensure compatibility when binding roles to classes and when compos- ing systems from components. Various ways to compose collaboration behaviors are outlined and illustrated with telephony services.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Engineering in a Large Industrial\nContext — Motorola Case Study",
        "date": 2005,
        "abstract": "In an ongoing eﬀort to reduce development costs in spite of increasing system complexity, Motorola has been a long-time adopter of Model-Driven Engineering (MDE) practices. The foundation of this approach is the creation of rigorous models throughout the development process, thereby enabling the introduction of automation. In this paper we present our experiences within Motorola in deploying a top-down approach to MDE for more than 15 years. We describe some of the key competencies that have been developed and the impact of MDE within the organization. Next we present some of the main issues encountered during MDE deployment, together with some possible resolutions.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using a Domain-Speciﬁc Language and Custom Tools to Model a Multi-tier Service-Oriented Application — Experiences and Challenges",
        "date": 2005,
        "abstract": "A commercial Customer Relationship Management applica- tion of approx. 1.5 MLOC of C++ code is being reimplemented, in stages, as a service-oriented, multi-tier application in C# on Microsoft .NET. We have chosen to use a domain-speciﬁc language both to model the external service-oriented interfaces, and to manage the transition to the internal, object-oriented implementation. Generic UML constructs such as class diagrams do not capture enough semantics to model these con- cepts. By deﬁning a UML Proﬁle that incorporates the concepts we wish to model, we have in eﬀect created a Domain-Speciﬁc Language for our application. The models are edited using Rational XDE, but we have sub- stituted our own code generator. This generator is a relatively generic text-substitution engine, which takes a template text and performs sub- stitutions based on the model. The generator uses reﬂection to convert the UML and Proﬁle concepts into substitution tags, which are in turn used in the template text. In this way, we can translate the semantics of the model into executable code, WSDL or other formats in a ﬂexible way. We have successfully used this approach on a prototype scale, and are now transitioning to full-scale development.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Uniform Support for Modeling Crosscutting Structure",
        "date": 2005,
        "abstract": "We propose bottom-up support for modeling crosscutting structure in UML by adding a simple join point model to the meta-model. This supports built-in crosscutting modeling constructs such as sequence diagrams. It also facilitates adding new kinds of crosscutting modeling constructs such as role bindings, inter-type declarations, and advice. A simple weaver produces a uniform representation of the crosscutting structure, which can then be displayed or analyzed in a variety of ways.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Crosscutting Services with UML Sequence \nDiagrams",
        "date": 2005,
        "abstract": "Current software systems increasingly consist of distributed interact- ing components. The use of web services and similar middleware technologies strongly fosters such architectures. The complexity resulting from a high degree of interaction between distributed components – that we face with web service orchestration for example – poses severe problems. A promising approach to handle this intricacy is service-oriented development; in particular with a do- main-unspecific service notion based on interaction patterns. Here, a service is defined by the interplay of distributed system entities, which can be modeled using UML Sequence Diagrams. However, we often face functionality that af- fects or is spanned across the behavior of other services; a similar concept to aspects in Aspect-Oriented Programming. In the service-oriented world, such aspects form crosscutting services. In this paper we show how to model those; we introduce aspect-oriented modeling techniques for UML Sequence Dia- grams and show their usefulness by means of a running example.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Formal Enforcement Framework for\nRole-Based Access Control Using\nAspect-Oriented Programming",
        "date": 2005,
        "abstract": "Many of today’s software applications require a high-level of security, deﬁned by a detailed policy and attained via mechanisms such as role-based access control (RBAC), mandatory access control, digital signatures, etc. The integration of the design/implementation processes of access-control policies with runtime enforcement mechanisms is crucial to achieve an acceptable level of security for a software application. Our prior research focused on formalizing the concept of a role slice, which is a uniﬁed modeling language (UML) artifact that captures RBAC security requirements by deﬁning permissions in the form of allowable or prohib- ited methods, and by specifying roles as specialized class diagrams that contain those methods. This paper augments this eﬀort by introducing a formal framework for the security of software applications that supports the automatic translation of a role-slice access-control policy (RBAC re- quirements) into aspect-oriented programming (AOP) enforcement code that is seamlessly integrated with the application. The formal framework provides the necessary underpinnings to automate the integration of se- curity policies into software. A prototyping eﬀort based on Borland’s UML tool Together Control Center for deﬁning role-slice diagrams and the associated AOP code generator is under development.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Domain Model for Dynamic System Reconfiguration",
        "date": 2005,
        "abstract": "In this paper, a domain model of dynamic system reconfiguration is presented. The intent of this model is to provide a comprehensive conceptual framework within which to address problems and solutions related to dynamically reconfigurable systems in a systematic and consistent manner. The model identifies and categorizes the various types of change that may be required, the relationship between those types, and the key factors that need to be considered and actions to be performed when such changes take place. A rigorous formal methodology, based on the Alloy language and tools, is employed to specify precisely and formally the detailed relationships between various parts of the model.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Exceptional Use Cases",
        "date": 2005,
        "abstract": "Many exceptional situations arise during the execution of an application. When developing dependable software, the ﬁrst step is to foresee these exceptional situations and document how the system should deal with them. This paper outlines an approach that extends use case based requirements elicitation with ideas from the exception handling world. After deﬁning the actors and the goals they pursue when inter- acting with the system, our approach leads a developer to systematically investigate all possible exceptional situations that the system may be ex- posed to: exceptional situations arising in the environment that change user goals and system-related exceptional situations that threaten to fail user goals. Means are deﬁned for detecting the occurrence of all ex- ceptional situations, and the exceptional interaction between the actors and the system necessary to recover from such situations is described in handler use cases. To conclude the requirements phase, an extended UML use case diagram summarizes the standard use cases, exceptions, handlers and their relationships.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Turnpike Frontend System: A Model-Driven \nDevelopment Framework Leveraging UML \nMetamodeling and Attribute-Oriented Programming*",
        "date": 2005,
        "abstract": "This paper describes and empirically evaluates a new model-driven development framework, called Modeling Turnpike (or mTurnpike). It allows developers to model and program domain-specific concepts (ideas and mechanisms specific to a particular business or technology domain) and to transform them to the final (compilable) source code. By leveraging UML metamodeling and attribute-oriented programming, mTurnpike provides an abstraction to represent domain-specific concepts at the modeling and programming layers simultaneously. The mTurnpike frontend system transforms domain-specific concepts from the modeling layer to programming layer, and vise versa, in a seamless manner. Its backend system combines domain-specific models and programs, and transforms them to the final (compilable) source code. This paper focuses on the frontend system of mTurnpike, and describes its design, implementation and performance implications. In order to demonstrate how to exploit mTurnpike in application development, this paper also shows a development process using an example DSL (domain specific language) to specify service-oriented distributed systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Simplifying Autonomic Enterprise Java Bean \nApplications Via Model-Driven Development:  \nA Case Study",
        "date": 2005,
        "abstract": "Autonomic computer systems aim to reduce the configuration, op- erational, and maintenance costs of distributed applications by enabling them to self-manage, self-heal, self-optimize, self-configure, and self-protect. This pa- per provides two contributions to the model-driven development (MDD) of autonomic computing systems using Enterprise Java Beans (EJBs). First, we describe the structure and functionality of an MDD tool that formally captures the design of EJB applications, their quality of service (QoS) requirements, and the autonomic properties applied to the EJBs to support the rapid development of autonomic EJB applications via code generation, automatic checking of model correctness, and visualization of complex QoS and autonomic properties. Second, the paper describes how MDD tools can generate code to plug EJBs into a Java component framework that provides an autonomic structure to monitor, configure, and execute EJBs and their adaptation strategies at run- time. We present a case study that evaluates how these tools and frameworks work to reduce the complexity of developing autonomic applications.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Invariant Maintenance Via OCL Compilation",
        "date": 2005,
        "abstract": "UML design models, specifically their declarative OCL invariants, must be refined into delivered code. A key problem is the need to integrate this logic with programmer-written code in a non-intrusive way. We recently developed an approach, called mode components, for compiling OCL con- straints into modules that implement logic for transparently maintaining these constraints at run time. Specifically, mode components are implemented as nested C++ class template instantiations. The approach makes use of a key device—status variables. The attributes of a component to which other components are sensitive are called its status. A status variable is a lightweight wrapper on a status attribute that detects changes to its value and transparently invokes a method to handle announcements to dependent components. A mode component is a wrapped code unit containing one or more status variables. The contribution of this paper is a technique for achieving this integration using metaprogramming techniques.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SelfSync: A Dynamic Round-Trip Engineering\nEnvironment",
        "date": 2005,
        "abstract": "Model-Driven Engineering (MDE) advocates the generation of software applications from models, which are views on certain aspects of the software. In this paper, we focus on a particular setup which con- sists of a graphical data modeling view and a view on an object-oriented implementation, which can be either textual or graphical. A challenge that arizes in the context of MDE is the notion of Round-Trip Engineer- ing (RTE), where elements from both views can be manipulated and thus need to be synchronized. We systematically identify four funda- mental RTE scenarios. In this paper, we employ the framework of these scenarios for explaining SelfSync, our approach and tool for providing dynamic support for RTE. In SelfSync, the entities of the data model- ing view and the corresponding implementation objects are one and the same. Additionally, we present a comparison with related work accom- panied by an extensive discussion.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML for Document Modeling: \nDesigning Document Structures for Massive and \nSystematic Production of XML-based Web Contents",
        "date": 2005,
        "abstract": "This paper discusses the applicability of modeling methods originally meant for business applications, on the design of the complex markup vocabularies used for XML Web-content production.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Metamodel Reuse with MOF",
        "date": 2005,
        "abstract": "As model-driven development promotes metamodels as key assets it raises the issue of their reuse throughout a model-driven product line life cycle. One recurrent reuse need occurs when metamodeling integrated multi-language platforms: one construct from one language is integrated to constructs from other languages by generalizing it, making it more expressive. None of the metamodel assembly facilities provided by MOF and UML (import, merge and combine) or others proposed in previous work adequately addresses this need. We thus propose a new reuse and generalize facility for such purpose.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling the User Interface of\nMultimedia Applications",
        "date": 2005,
        "abstract": "Multimedia applications are a branch of software develop- ment with growing importance. Typical application areas are training applications and simulations, infotainment systems - e.g. in cars - or computer games. However, there is still a lack of tailored concepts for a structured development of this kind of application. The current pa- per proposes a modeling approach for the user interface of multimedia applications with the goal of a model-driven development. We identify the special properties of multimedia application development and the resulting aspects to be covered by the user interface model. Existing conventional user interface modeling approaches are not suﬃcient, as they do not cover the media-speciﬁc aspects of the application. However, a multimedia application usually includes conventional user interface el- ements as well. Thus, we ﬁrst propose a solution for the media-speciﬁc part. Second, we elaborate an integration of our approach with existing conventional approaches. Finally, we discuss the overall model-driven de- velopment approach and outline its beneﬁts.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Ontology-Based Approach for Evaluating the Domain \nAppropriateness and Comprehensibility Appropriateness of \nModeling Languages",
        "date": 2005,
        "abstract": "In this paper we present a framework for the evaluation and (re)design of modeling languages. We focus here on the evaluation of the suit- ability of a language to model a set of real-world phenomena in a given domain. In our approach, this property can be systematically evaluated by comparing the level of homomorphism between a concrete representation of the worldview underlying the language (captured in a metamodel of the language), with an ex- plicit and formal representation of a conceptualization of that domain (a refer- ence ontology). The framework proposed comprises a number of properties that must be reinforced for an isomorphism to take place between these two entities. In order to illustrate the approach proposed, we evaluate and extend a fragment of the UML static metamodel for the purpose of conceptual modeling, by com- paring it with an excerpt of a philosophically and cognitive well-founded refer- ence ontology.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tutorials at the MODELS 2005 Conference",
        "date": 2005,
        "abstract": "The MoDELS 2005 conference provides six half-day tutorials on advanced topics related to model-driven engineering, presented by recognized worldwide experts. Here, there is a short summary of each tutorial and the list of presenters.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "9th International Workshop on  \nAspect-Oriented Modeling",
        "date": 2006,
        "abstract": "This report summarizes the outcomes of the 9th Workshop on Aspect-Oriented Modeling (AOM) held in conjunction with the 9th International Conference on Model Driven Engineering Languages and Systems – MoDELS 2006 – in Genoa, Italy, on the 1st of October 2006. The workshop brought together approximately 25 researchers and practitioners from two communities: aspect-oriented software development and software model engineering. It provided a forum for discussing the state of the art in modeling crosscutting concerns at different stages of the software development process: requirements elicitation and analysis, software architecture, detailed design, and mapping to aspect-oriented programming constructs. This paper gives an overview of the accepted submissions and summarizes the results of the different discussion groups. Papers and presentation slides of the workshop are available at http://www.aspect-modeling.org/.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Features in Aspect-Based Product \nLines with Use Case Slices: \nAn Exploratory Case Study",
        "date": 2006,
        "abstract": "A significant number of techniques that exploit aspects in software design have been proposed in recent years. One technique is use case slices by Jacobson and Ng, that builds upon the success of use cases as a common modeling practice. A use case slice modularizes the implementation of a use case and typically consists of a set of aspects, classes, and interfaces. Work on Feature Oriented Programming (FOP) has shown how features, increments in program functionality, can be modularized and algebraically modeled for the synthesis of product lines. When AspectJ is used in FOP, the structure of feature modules resembles that of use case slices. In this paper, we explore the relations between use case slices modeling and FOP program synthesis and describe their potential synergy for modeling and synthesizing aspect-based product lines.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Join Point Patterns:\nA High-Level Join Point Selection Mechanism",
        "date": 2006,
        "abstract": "Aspect-Oriented Programming is a powerful technique to better mod- ularize object-oriented programs by introducing crosscutting concerns in a safe and noninvasive way. Unfortunately, most of the current join point models are too coupled with the application code. This fact hinders the concerns separability and reusability since each aspect is strictly tailored on the base application. This work proposes a possible solution to this problem based on modeling the join points selection mechanism at a higher level of abstraction. In our view, the aspect designer does not need to know the inner details of the application such as a speciﬁc implementation or the used name conventions rather he exclusively needs to know the application behavior to apply his/her aspects. In the paper, we present a novel join point model with a join point selection mechanism based on a high-level program representation. This high-level view of the application decouples the aspects deﬁnition from the base program structure and syntax. The separation between aspects and base program will render the aspects more reusable and independent of the manipulated application.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Critical Systems Development Using Modeling \nLanguages – CSDUML 2006 Workshop Report",
        "date": 2006,
        "abstract": "The CSDUML 2006 workshop is a continuation of the series regarding development of critical systems using modeling languages. The report summarizes papers presented and discussion at the workshop.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling an Electronic Throttle Controller Using the\nTimed Abstract State Machine Language and Toolset",
        "date": 2006,
        "abstract": "In this paper, we present an integrated toolset that implements the features of the Timed Abstract State Machine (TASM) language, a novel speciﬁ- cation language for embedded real-time systems. The toolset enables the creation of executable speciﬁcations with well-deﬁned execution semantics, abstraction mechanisms, and composition semantics. The features of the toolset are demon- strated using an Electronic Throttle Controller (ETC) from a major automotive vendor. The TASM toolset is used to analyze the resource consumption resulting from the mode switching logic of the ETC, and to verify the completeness and consistency of the speciﬁcation.",
        "keywords": [
            "Formal Speciﬁcation",
            "Modeling",
            "Simulation",
            "Real-Time Systems",
            "Embedded Systems."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Checking of UML 2.0 Interactions",
        "date": 2006,
        "abstract": "The UML 2.0 integrates a dialect of High-Level Message Sequence Charts (HMSCs) for interaction modelling. We describe a translation of UML 2.0 interactions into automata for model checking whether an interaction can be sat- isﬁed by a given set of message exchanging UML state machines. The translation supports basic interactions, state invariants, strict and weak sequencing, alterna- tives, ignores, and loops as well as forbidden interaction fragments. The transla- tion is integrated into the UML model checking tool HUGO/RT.",
        "keywords": [
            "Scenarios",
            "UML 2.0 interactions",
            "model checking."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Unified Ontology-Based Process Model for Software \nMaintenance and Comprehension",
        "date": 2006,
        "abstract": "In this paper, we present a formal process model to support the com- prehension and maintenance of software systems. The model provides a formal ontological representation that supports the use of reasoning services across dif- ferent knowledge resources. In the presented approach, we employ our Descrip- tion Logic knowledge base to support the maintenance process management, as well as detailed analyses among resources, e.g., the traceability between various software artifacts. The resulting unified process model provides users with ac- tive guidance in selecting and utilizing these resources that are context-sensitive to a particular comprehension task. We illustrate both, the technical foundation based on our existing SOUND environment, as well as the general objectives and goals of our process model.",
        "keywords": [
            "Software maintenance",
            "process modeling",
            "ontological reasoning",
            "software comprehension",
            "traceability",
            "text mining."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalizing the Well-Formedness Rules of EJB3QL in UML + OCL",
        "date": 2006,
        "abstract": "This paper reports the application of language metamodel- ing techniques to EJB3QL, the object-oriented query language for Java Persistence recently standardized in JSR-220. Five years from now, to- day’s EJB3 applications will be legacy. We see our metamodel as an enabler for increasing the eﬃciency of reverse engineering activities. It has already proven useful in uncovering spots where the EJB3QL spec is vague. The case study reported in this paper involved (a) expressing the abstract syntax and well-formedness rules of EJB3QL in UML and OCL respectively; (b) deriving from that metamodel software artifacts required for several language-processing tasks, targeting two modeling platforms (Eclipse EMF and Octopus); and (c) comparing the gener- ated artifacts with their counterparts in the reference implementation of EJB3 (which was not developed following a language-metamodeling approach). The metamodel of EJB3QL constitutes the basis for apply- ing model-checkers to aid in assuring conformance of tools claiming to support the speciﬁcation.",
        "keywords": [
            "Metamodel",
            "OCL",
            "Static semantics",
            "EJB3QL."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistency of Business Process Models and Object Life Cycles",
        "date": 2006,
        "abstract": "Business process models and object life cycles can provide two diﬀerent views on behavior of the same system, requiring that these models are consistent with each other. However, it is diﬃcult to reason about consistency of these two types of models since their relation is not well-understood. We clarify this relation and propose an approach to establishing the required consistency. Object state changes are ﬁrst made explicit in a business process model and then the process model is used to generate life cycles for each object type used in the process. We deﬁne two consistency notions for a process model and an object life cycle and express these in terms of conditions that must hold between a given life cycle and a life cycle generated from the process model.",
        "keywords": [
            "consistency",
            "business process model",
            "object life cycle",
            "activ- ity diagram",
            "state machine",
            "UML."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Qualitative Investigation of UML Modeling Conventions",
        "date": 2006,
        "abstract": "Analogue to the more familiar notion of coding conventions, modeling conventions attempt to ensure uniformity and prevent common modeling defects. While it has been shown that modeling conventions can decrease defect density, it is currently unclear whether this decreased de- fect density results in higher model quality, i.e., whether models created with modeling conventions exhibit higher ﬁtness for purpose.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Driven Development of Advanced User Interfaces (MDDAUI) – MDDAUI’06 Workshop Report",
        "date": 2006,
        "abstract": "This paper reports on the 2nd Workshop on Model Driven Development of Advanced User Interfaces (MDDAUI’06) held on Octo- ber 2nd, 2006 at the MoDELS’06 conference in Genova, Italy. It brieﬂy describes the workshop topic and provides a short overview on the work- shop structure. In the main part it introduces the four topics discussed in the workshop’s afternoon sessions and summarizes the discussion results.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Driven Approach to the Engineering of Multiple User Interfaces",
        "date": 2006,
        "abstract": "In this paper, we describe MANTRA1, a model-driven ap- proach to the development of multiple consistent user interfaces for one application. The common essence of these user interfaces is captured in an abstract UI model (AUI) which is annotated with constraints to the dialogue ﬂow. We consider in particular how the user interface can be adapted on the AUI level by deriving and tailoring dialogue struc- tures which take into account constraints imposed by front-end platforms or inexperienced users. With this input we use model transformations described in ATL (Atlas Transformation Language) to derive concrete, platform-speciﬁc UI models (CUI). These can be used to generate im- plementation code for several UI platforms including GUI applications, dynamic web sites and mobile applications. The generated user interfaces are integrated with a multi tier application by referencing WSDL-based interface descriptions and communicating with the application core over web service protocols.",
        "keywords": [
            "Model-driven",
            "multiple user interfaces",
            "multiple front-ends",
            "user interface engineering",
            "user interface modelling",
            "model transforma- tion",
            "ATL",
            "Atlas Transformation Language."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL, Atlas Transformation Language"
        }
    },
    {
        "title": "Model-Driven Dynamic Generation of\nContext-Adaptive Web User Interfaces",
        "date": 2006,
        "abstract": "The systematic development of user interfaces that enhance interaction quality by adapting to the context of use is a desirable, but also highly challenging task. This paper examines to which extent contex- tual knowledge can be systematically incorporated in the model-driven dynamic generation of Web user interfaces that provide interaction for operational features. Three parts of the generation process are distin- guished: selection, parameterization, and presentation. A semantically enriched service-oriented approach is presented that is based on the Cat- walk framework for model interpretation and generation of adaptive, context-aware Web applications. Automation possibilities are addressed and an exemplary case study is presented.",
        "keywords": [
            "Context-aware Web User Interfaces",
            "Web Service Integration",
            "Ontology-based Modeling",
            "Model Interpretation",
            "Model-Driven User In- terface Generation",
            "Parameterization",
            "Semantically Enriched SOA."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling and Analysis of Real Time and Embedded \nSystems – Using UML",
        "date": 2006,
        "abstract": "This paper presents an overview on the outcomes of the workshop MARTES on Modelling and Analysis of Real Time and Embedded Systems that has taken place for the second time in association with the MoDELS/UML 2006 conference. Important themes discussed at this workshop concerned (1) tools for analysis and model transformation and (2) concepts for modelling quantitative aspects with the perspective of analysis.",
        "keywords": [
            "Modelling",
            "Analysis",
            "Real Time",
            "Embedded Systems."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Time Exceptions in Sequence Diagrams",
        "date": 2006,
        "abstract": "UML sequence diagrams partially describe a system. We show how the description may be augmented with exceptions triggered by the violation of timing constraints and compare our approach to those of the UML 2.1 simple time model, the UML Testing Proﬁle and the UML proﬁle for Schedulability, Performance and Time. We give a for- mal deﬁnition of time exceptions in sequence diagrams and show that the concepts are compositional. An ATM example is used to explain and motivate the concepts.",
        "keywords": [
            "speciﬁcation",
            "time constraints",
            "exception handling",
            "formal semantics",
            "reﬁnement."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying Model Intelligence Frameworks for Deployment Problem in Real-Time and Embedded Systems",
        "date": 2006,
        "abstract": "There are many application domains, such as distributed real-time and embedded (DRE) systems, where the domain constraints are so restrictive and the solution spaces so large that it is infeasible for modelers to produce correct solution manually using a conventional graphical model-based approach. In DRE systems the available resources, such as memory, CPU, and bandwidth, must be managed carefully to ensure a certain level of quality of service. This paper provides three contributions to simplify modeling of complex application domains: (1) we present our approach of combining model intelligence and domain- speciﬁc solvers with model-driven engineering (MDE) environments, (2) we show techniques for automatically guiding modelers to correct solu- tions and how to support the speciﬁcation of large and complex systems using intelligent mechanisms to complete partially speciﬁed models, and (3) we present the results of applying an MDE tool that maps software components to Electronic Control Units (ECUs) using the typical auto- motive modeling and middleware infrastructure.",
        "keywords": [
            "modeling",
            "Prolog",
            "constraint solver",
            "model completion",
            "model checking",
            "automotive."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OCL for (Meta-)Models in Multiple Application Domains",
        "date": 2006,
        "abstract": "The workshop OCLApps 2006 was organized as a part of MoDELS/UML Conference in Genova, Italy. It continues the series of ﬁve OCL (Object Constraint Language) workshops held at previous UML/MoDELS conferences between 2000 - 2005. Similar to its predeces- sors, the workshop addressed both people from academia and industry. The advent of the MDA (Model Driven Architecture) vision and the rapid acceptance of MDE (Model Driven Engineering) approaches em- phasize new application domains (like Semantic Web or Domain Speciﬁc Languages) and call for new OCL functionalities. In this context, the OCLApps 2006 Workshop, was conceived as a forum enabling researchers and industry experts to present and debate how the OCL could support these new requirements.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OCL-Based Validation of a Railway Domain Proﬁle",
        "date": 2006,
        "abstract": "Domain-speciﬁc languages become more and more important these days as they facilitate the close collaboration of domain experts and software developers. One eﬀect of this general tendency is the increasing number of UML proﬁles. UML itself as a popular modeling language is capable of modeling all kinds of systems but it is often ineﬃcient due to its wide-spectrum approach. Proﬁles tailor the UML to a speciﬁc domain and can hence be seen as domain-speciﬁc dialects of UML. At the mo- ment, they mainly introduce new terminology, often in combination with OCL constraints which describe the new constructs more precisely. As most tools do not support validation of OCL expressions let alone supple- menting proﬁles with OCL constraints, it is diﬃcult to check if models based on a proﬁle comply to this proﬁle. A related problem is check- ing whether constraints in the proﬁle contradict constraints in the UML speciﬁcation. In this paper, it is shown how to complete these tasks with the tool USE. As an example, a proﬁle from the railway control systems domain is taken which describes the use of its modeling elements strictly my means of OCL. Models based on this proﬁle serve as a foundation for automated code generation and require unambiguous meaning.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OCL Support in an Industrial Environment",
        "date": 2006,
        "abstract": "In this paper, we report on our experiences integrating OCL evaluation support in an industrial-strength (meta-)modeling infrastruc- ture. We focus on the approach taken to improve eﬃciency through what we call impact analysis of model changes to decrease the number of nec- essary (re-)evaluations. We show how requirements derived from appli- cation scenarios have led to design decisions that depart from or resp. extend solutions found in (academic) literature.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Report on the 3rd MoDeVa Workshop – Model Design and Validation",
        "date": 2006,
        "abstract": "Software systems are becoming increasingly large and com- plex, and run the risk of serious failures from unforeseen behaviour. Model driven development (MDD) is emerging as a solution with strong potential for dealing with these diﬃculties using models and model trans- formations. However, eﬀective validiation and veriﬁcation techniques are required to take full advantage of the expected beneﬁts of MDD.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Model-Driven Unit Testing",
        "date": 2006,
        "abstract": "The Model-Driven Architecture (MDA) approach for constructing software systems advocates a stepwise reﬁnement and transformation process starting from high-level models to concrete program code. In contrast to numer- ous research efforts that try to generate executable function code from models, we propose a novel approach termed model-driven monitoring. On the model level the behavior of an operation is speciﬁed with a pair of UML composite struc- ture diagrams (visual contract), a visual notation for pre- and post-conditions. The speciﬁed behavior is implemented by a programmer manually. An automatic translation from our visual contracts to JML assertions allows for monitoring the hand-coded programs during their execution.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Validation of Model Transformations – First Experiences Using a White Box Approach",
        "date": 2006,
        "abstract": "Validation of model transformations is important for ensur- ing their quality. Successful validation must take into account the char- acteristics of model transformations and develop a suitable fault model on which test case generation can be based. In this paper, we report our experiences in validating a number of model transformations and pro- pose three techniques that can be used for constructing test cases.",
        "keywords": [
            "Model transformations",
            "Testing."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Summary of the 2006 Model Size Metrics Workshop",
        "date": 2006,
        "abstract": "A standardized and consistent means of determining the size of an artifact is fundamental to the ability to collect metrics such as de- fect density and productivity about the artifact. For example, source lines of code is often used as the size metric for C code. However, the concept of lines of code does not readily apply to modeling languages such as UML and SDL. This report summarizes the presentations and discussions on this topic from the 2006 Model Size Metrics workshop.",
        "keywords": [
            "Model Size Metrics",
            "Model-Driven Engineering",
            "UML",
            "SDL."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Size Matters",
        "date": 2006,
        "abstract": "Size is an important attribute of software artefacts; for most artefact types exists a body of measurement knowledge. As software en- gineering is becoming more and more model-centric, it is surprising that there exists only little work on model size metrics (MoSMe). In this po- sition paper we identify the goals justifying the need for MoSMe, such as prediction, description and progress measurement. Additionally, we iden- tify challenges that make it diﬃcult to measure the size of UML models and that MoSMe have to deal with. Finally, we propose a classiﬁcation of MoSMe and concrete examples of metrics for the size of UML models.",
        "keywords": [
            "Models",
            "UML",
            "Size",
            "Metrics",
            "Measurement",
            "Prediction",
            "GQM."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the Application of Software Metrics to UML Models",
        "date": 2006,
        "abstract": "In this position paper we discuss a number of issues relating to model metrics, with particular emphasis on metrics for UML models. Our discussion is presented as a series of nine observations where we examine some of the existing work on applying metrics to UML models, present some of our own work in this area, and specify some topics for future research that we regard as important. Furthermore, we identify three categories of challeges for model metrics and describe how our nine observations can be partitioned into these categories.",
        "keywords": [
            "software metrics",
            "object-oriented systems",
            "UML",
            "metamodels."
        ],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Summary of the Workshop Models@run.time at \nMoDELS 2006",
        "date": 2006,
        "abstract": "The first edition of the workshop Models@run.time was co-located with the ACM/IEEE 9th International Conference on Model Driven Engineering Languages and Systems (formerly the UML series of conferences). The workshop took place in the antique city of Genoa, Italy, on the 1st of October, 2006. The workshop was organised by Gordon Blair, Robert France, and Nelly Bencomo. This summary gives an overview an account of the presentations and lively discussions that took place during the workshop.",
        "keywords": [
            "model-driven engineering",
            "reflection",
            "run-time systems."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using Runtime Models to Unify and Structure the Handling of Meta-information in Reﬂective Middleware⋆",
        "date": 2006,
        "abstract": "Reﬂection plays an important role in the ﬂexibilisation of middleware platforms. Through dynamic inspection, middleware inter- faces can be discovered and invoked at runtime, and through adaptation the structure and behaviour of the platform can be modiﬁed on-the- ﬂy to meet new user or environment demands. Metamodeling, on the other hand, has shown its value for the static conﬁguration of middle- ware and other types of system as well. Both techniques have in common the pervasive use of meta-information as the means to provide the sys- tem’s self-representation. However similar they are, these two techniques usually fall on diﬀerent sides of a gap, namely development time and runtime, with little interplay between them. In this paper, we review our approach for the combination of reﬂection and metamodeling, presenting some concrete applications of the concept in the context of distributed systems middleware, as well as propossing further potential applications.",
        "keywords": [
            "Runtime metamodels",
            "Structural reﬂection",
            "Reﬂective middleware."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying OMG D&C Speciﬁcation and ECA Rules for Autonomous Distributed Component-Based Systems",
        "date": 2006,
        "abstract": "Manual administration of complex distributed applications is almost impossible to achieve. On the one side, work in autonomic computing focuses on systems that maintain themselves, driven by high- level policies. Such a self-administration relies on the concept of a control loop. The autonomic computing control loop involves an abstract repre- sentation of the system to analyze the situation and to adapt it properly. On the other side, models are currently used to ease design of complex distributed systems. Nevertheless, at runtime, models remain useless, be- cause they are decoupled from the running system, which dynamically evolves. Our proposal, named Dacar, introduces models in the control loop. Using adequate models, it is possible to design and execute both the distributed systems and their autonomic policies. The metamodel suggested in this paper mixes both OMG Deployment and Conﬁgura- tion (OMG D&C) speciﬁcation and the Event-Condition-Action (ECA) metamodels. This paper addresses the diﬀerent concerns involved in the control loop and focuses on the metamodel concepts that are required to express entities of the control loop. This paper also gives an overview of our Dacar prototype and illustrates it on a ubiquitous application case study.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Summary of the Workshop on Multi-Paradigm Modeling: Concepts and Tools",
        "date": 2006,
        "abstract": "This paper reports on the ﬁndings of the ﬁrst Workshop on Multi-Paradigm Modeling: Concepts and Tools. It contains an overview of the presented papers and of the results of three working groups which addressed multiple views, abstraction, and evolution. Besides this, a def- inition of the problem space, the main concepts, and an appropriate ter- minology for multi-paradigm modeling as presented and discussed during the workshop are provided.",
        "keywords": [
            "Modeling",
            "Meta-modeling",
            "Multi-Paradgim Modeling",
            "Multi-Formalism."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Think Global, Act Local:\nImplementing Model Management with\nDomain-Speciﬁc Integration Languages⋆",
        "date": 2006,
        "abstract": "In recent years a number of model transformation languages have emerged that deal with ﬁne-grained, local transformation speciﬁca- tions, commonly known as programming in the small [13]. To be able to develop complex transformation systems in a scalable way, mechanisms to work directly on the global model level are desirable, referred to as programming in the large [26]. In this paper we show how domain speciﬁc model integration languages can be deﬁned, and how they can be com- posed in order to achieve complex model management tasks. Thereby, we base our approach on the deﬁnition of declarative model integration languages, of which implementing transformations are derived. We give a categorization of these transformations and rely on an object-oriented mechanism to realize complex model management tasks.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Model Driven Security Engineering for the Realization of Dynamic Security Requirements in Collaborative Systems",
        "date": 2006,
        "abstract": "Service Oriented Architectures with underlying technologies like web services and web services orchestration have opened the door to a wide range of novel application scenarios, especially in the context of inter-organizational cooperation. One of the remaining obstacles for a wide-spread use of these techniques is security. Companies and organiza- tions open their systems and core business processes to partners only if a high level of trust can be guaranteed. The emergence of web services secu- rity standards provides a valuable and eﬀective paradigm for addressing the security issues arising in the context of inter-organizational cooper- ation. The low level of abstraction of these standards is, however, still an unresolved issue which makes them inaccessible to the domain expert and remains a major obstacle when aligning security objectives with the customer needs. Their complexity makes implementation easily prone of error. This paper provides a bird eye view of a doctoral work, where an eﬀort is made to develop a conceptual framework – called SECTET in order to apply model driven security engineering techniques for the realization of high-level security requirements.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "If You’re Not Modeling, You’re Just Programming: \nModeling Throughout an Undergraduate Software \nEngineering Program",
        "date": 2006,
        "abstract": "Modeling is a hallmark of the practice of engineering. Through centuries, engineers have used models ranging from informal “back of the envelope” scribbles to formal, verifiable mathematical models. Whether circuit models in electrical engineering, heat-transfer models in mechanical engineering, or queuing theory models in industrial engineering, modeling makes it possible to perform rigorous analysis that is the cornerstone of modern engineering. By considering software development as fundamentally an engineering endeavor, RIT’s software engineering program strives to instill a culture of engineering practice by exposing our students to both formal and informal modeling of software systems throughout the entire curriculum. This paper describes how we have placed modeling in most aspects of our curriculum. The paper also details the specific pedagogy that we use in several courses to teach our students how to create, analyze and implement models of software systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Teaching Software Modeling in a Simulated Project Environment",
        "date": 2006,
        "abstract": "Teaching software engineering in the academia always faces the problem of inability to show problems of real life development projects. The courses seem to be unable to properly show the need of us- ing software modeling as important means of coping with complexity and handling communication within the project. The paper presents format of a course that tries to overcome this. It focuses on application of mod- eling tools in a realistic software engineering environment. The objective is to teach best practices of software design and implementation with the use of UML. The students can practice design and communication tech- niques based around CASE tools in teams of 12 to 14 people. The paper summarizes 5 years of experience in teaching modeling with CASE tools. Authors present a concept of how to simulate the roles of architects, de- signers and programmers as close to reality as possible. The paper also discusses the problems of organizing laboratory work for a large group of students. Authors present the tasks and their arrangement during the course.",
        "keywords": [
            "software modeling",
            "education",
            "CASE tools",
            "project commu- nication",
            "UML."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Repository for Model Driven Development (ReMoDD)",
        "date": 2006,
        "abstract": "The Repository for MDD (ReMoDD) project is concerned with devel- oping a repository that will contain artifacts that support research and education in model-driven development (MDD). The ReMoDD platform will also provide interfaces and interchange mechanisms that will enable a variety of tools to re- trieve artifacts from the repository and submit candidate artifacts to the reposi- tory. ReMoDD artifacts will include documented MDD case studies, examples of models reﬂecting good and bad modeling practices, reference models (including metamodels) that can be used as the basis for comparing and evaluating MDD techniques, generic models and transformation reﬂecting reusable modeling ex- perience, descriptions of modeling techniques, practices and experiences, and modeling exercises and problems that can be used to develop classroom assign- ments and projects. In this paper we outline plans for developing ReMoDD.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "2nd UML 2 Semantics Symposium: Formal Semantics for UML",
        "date": 2006,
        "abstract": "The purpose of this symposium, held in conjunction with MoDELS 2006, was to present the current state of research of the UML 2 Semantics Project. Equally important to receiving feedback from an au- dience of experts was the opportunity to invite researchers in the ﬁeld to discuss their own work related to a formal semantics for the Uniﬁed Modeling Language. This symposium is a follow-on to our ﬁrst workshop, held in conjunction with ECMDA 2005.",
        "keywords": [
            "UML",
            "Formal Semantics."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Analysis of UML Activities with\nDynamic Meta Modeling Techniques",
        "date": 2006,
        "abstract": "Based on a semantics of UML Activities speciﬁed with the Dynamic Meta Modeling approach, we analyze the dynamic semantics of Activities at modeling time.",
        "keywords": [
            "UML",
            "semantics",
            "behavior",
            "veriﬁcation",
            "DMM."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bidirectional Model Transformations in QVT: Semantic Issues and Open Questions",
        "date": 2007,
        "abstract": "We consider the OMG’s Queries, Views and Transforma- tions (QVT) standard as applied to the speciﬁcation of bidirectional transformations between models. We discuss what is meant by bidirec- tional transformations, and the model-driven development scenarios in which they are needed. We analyse the fundamental requirements on tools which support such transformations, and discuss some semantic issues which arise. We argue that a considerable amount of basic re- search is needed before suitable tools will be fully realisable, and suggest directions for this future research.",
        "keywords": [
            "bidirectional model transformation",
            "QVT."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Reconciling TGGs with QVT",
        "date": 2007,
        "abstract": "The Model Driven Architecture (MDA) is an approach to develop software based on different models. There are separate models for the business logic and for platform specific details. Moreover, code can be generated automatically from these models. This makes transformations a core technology for MDA. QVT (Query/View/Transformation) is the transformation technology recently proposed for this purpose by the OMG.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "UniTI: A Uniﬁed Transformation Infrastructure⋆",
        "date": 2007,
        "abstract": "A model transformation can be decomposed into a sequence of sub- transformations, i.e. a transformation chain, each addressing a limited set of con- cerns. However, with current transformation technologies it is hard to (re)use and compose subtransformations without being very familiar with their implemen- tation details. Furthermore, the difﬁculty of combining different transformation technologies often thwarts choosing the most appropriate technology for each subtransformation. In this paper we propose a model-based approach to reuse and compose subtransformations in a technology-independent fashion. This is accom- plished by developing a uniﬁed representation of transformations and facilitating detailed transformation speciﬁcations. We have implemented our approach in a tool called UniTI, which also provides a transformation chain editor. We have evaluated our approach by comparing it to alternative approaches.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guided Development with Multiple Domain-Speciﬁc Languages",
        "date": 2007,
        "abstract": "We study the Apache Open for Business (OFBiz), an industrial-strength platform for enterprise applications. OFBiz is an example of a substantial project using model-driven development with multiple domain-speciﬁc languages (DSLs). We identify consistency management as one of its key challenges. To address this challenge, we present SmartEMF, which is an extension of the Eclipse Modeling Frame- work that provides support for representing, checking, and maintaining constraints in the context of multiple loosely-coupled DSLs. SmartEMF provides a simple form of user guidance by computing the valid set of editing operations that are available in a given context. We evaluate the prototype by applying it to the OFBiz project.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven, Network-Context Sensitive Intrusion \nDetection",
        "date": 2007,
        "abstract": "Intrusion Detection Systems (IDSs) have the reputation of generating many false positives. Recent approaches, known as stateful IDSs, take the state of communication sessions into account to address this issue. A substantial reduction of false positives, however, requires some correlation between the state of the session, known vulnerabilities, and the gathering of more network context information by the IDS than what is currently done (e.g., configuration of a node, its operating system, running applications). In this paper we present an IDS approach that attempts to decrease the number of false positives by collecting more network context and combining this information with known vulnerabilities. The approach is model-driven as it relies on the modeling of packet and network information as UML class diagrams, and the definition of intrusion detection rules as OCL expressions constraining these diagrams. The approach is evaluated using real attacks on real systems, and appears to be promising.",
        "keywords": [
            "Intrusion Detection",
            "UML modeling",
            "OCL constraints."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Empirical Study of the Impact of OCL Smells and \nRefactorings on the Understandability of OCL \nSpecifications",
        "date": 2007,
        "abstract": "The Object Constraint Language (OCL) is a OMG standard that plays an important role in the elaboration of precise models. However, it is not hard to find models and metamodels containing overly complex OCL expressions. Refactoring is a technique that can be used in this context since its goal is to reduce complexity by incrementally improving the internal software quality. Indeed several refactorings have already been proposed to improve the quality of OCL expressions. This paper presents the results of an empirical study that investigates the impact of poor OCL constructs, also known as OCL Smells, and OCL refactorings on the understandability of OCL expressions. Current results show that most refactorings significantly improve the understandability of OCL specifications.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Metamodeling in Megamodels",
        "date": 2007,
        "abstract": "Model-Driven Engineering (MDE) introduced the notion of metamodeling as the main means for defining modeling languages. As a well organized engineering discipline, MDE should also have its theory clearly defined in terms of the relationships between key MDE concepts. Following the spirit of MDE, where models are first class citizens, even the MDE theory can be defined by models, or so called megamodels. In this paper, we use Favre’s megamodel that was already used for defining linguistic metamodeling. Starting from the premise that this megamodel can also be used for defining other MDE concepts, we use it to specify the notion of ontological metamodeling. Here, we show that in order for this megamodel to be able to fully capture all the concepts of ontological metamodeling, some refinements should be applied to its definition. We also show how these new changes are in the same direction with the work of Kühne in defining linguistic and ontological metamodels.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Magritte – A Meta-driven Approach to Empower Developers and End Users",
        "date": 2007,
        "abstract": "Model-driven engineering is a powerful approach to build large-scale applications. However, an application’s metamodel often re- mains static after the initial development phase and cannot be changed unless a new development eﬀort occurs. Yet, end users often need to rapidly adapt their applications to new needs. In many cases, end users would know how to make the required adaptations, if only the application would let them do so. In this paper we present how we built a runtime- dynamic meta-environment into Smalltalk’s reﬂective language model. Our solution oﬀers the best of both worlds: developers can develop their applications using the same tools they are used to and gain the power of meta-programming. We show in particular that our approach is suitable to support end user customization without writing new code: the adap- tive model of Magritte not only describes existing classes, but also lets end users build their own metamodels on the ﬂy.",
        "keywords": [
            "Meta-Modeling",
            "Meta-Data",
            "Adaptive Object Model",
            "Busi- ness Application Development",
            "Smalltalk."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Matching Model-Snippets",
        "date": 2007,
        "abstract": "An important demand in Model-Driven Development is the simple and eﬃcient expression of model patterns. Current approaches tend to distinguish the language they use to express patterns from the one for modelling. Consequently, productivity is reduced by dealing with a distinct new language, and new intermediate steps are introduced in order to support pattern-matching. In this paper we propose a frame- work for expressing patterns as model-snippets. We present how model- snippets are speciﬁed upon concepts in a given domain (meta-model), and how we perform pattern-matching with model-snippets, whatever the meta-model. We also provide an implementation which is well inte- grated with existing technologies, such as Eclipse Modelling Framework.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving Inconsistency Resolution with Side-Eﬀect Evaluation and Costs",
        "date": 2007,
        "abstract": "Consistency management is a major requirement in software engineering. Although this problem has attracted signiﬁcant attention in the literature, support for inconsistency resolution is still not standard for modeling tools. In this paper, we introduce explicit side-eﬀect expres- sions for each inconsistency resolution and costs for each inconsistency type. This allows a ﬁne-grained evaluation of each possible inconsistency resolution for a particular inconsistent model. We further show how an inconsistency resolution module for a modeling tool can be designed and implemented based on our approach. We demonstrate the applicability of our approach for resolution of inconsistencies between object life cycles and process models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Composition in Product Lines and Feature \nInteraction Detection Using Critical Pair Analysis",
        "date": 2007,
        "abstract": "Software product lines (SPL) are an established technology for developing families of systems. In particular, they focus on modeling commonality and variability, that is, they are based on identifying features common to all members of the family and variable features that appear only in some members. Model-based development methods for product lines advocate the construction of SPL requirements, analysis and design models for features. This paper describes an approach for maintaining feature separation during modeling using a UML composition language based on graph transformations. This allows models of features to be reused more easily. The language can be used to compose the SPL models for a given set of features. Furthermore, critical pair analysis is used to detect dependencies and conflicts between features during analysis and design modeling. The approach is supported by a tool that allows automated composition of UML models of features and detection of some kinds of feature interactions.",
        "keywords": [
            "Software product lines",
            "model transformation",
            "feature interaction."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "UML composition language"
        }
    },
    {
        "title": "Automated Semantic Analysis of Design Models",
        "date": 2007,
        "abstract": "Based on several years of experience in generating code from large SDL and UML models in the telecommunications domain, it has become apparent that model analysis must be used to augment more traditional validation and testing techniques. While model correctness is extremely important, the diﬃculty of use and non-scalability of most formal veriﬁcation techniques when applied to large-scale design models renders them insuﬃcient for most applications. We have also repeat- edly seen that even the most complete test coverage fails to ﬁnd many problems. In contrast, sophisticated model analysis techniques can be applied without human interaction to large-scale models. A discussion of the model analysis techniques and the model defects that they can detect is provided, along with some real-world examples of defects that have been caught.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Piecewise Modelling with State Subtypes",
        "date": 2007,
        "abstract": "Models addressing both structure and behaviour of a system are usu- ally quite complex. Much of the complexity is caused by the necessity to distin- guish between different cases, such as legal vs. illegal constellations of objects, typical vs. rare scenarios, and normal vs. exceptional flows of control. The re- sult is an explosion of cases causing large and deeply nested case analyses. While those based on the kinds of objects involved can be tackled with standard dynamic dispatch, possibilities for differentiations based on the state of objects have not yet been considered for modelling. We show how the handling of class and state-induced distinctions can be unified under a common subtyping scheme, and how this scheme allows the simplification of models by splitting them into piecewise definitions. Using a running example, we demonstrate the potential of our approach and explain how it serves the consistent integration of static and dynamic specifications.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deriving Operation Contracts from UML Class Diagrams",
        "date": 2007,
        "abstract": "Class diagrams must be complemented with a set of system operations that describes how users can modify and evolve the system state. To be useful, such a set must be complete (i.e. through these operations, users should be able to modify the population of all elements in the class diagram) and executable (i.e. for each operation, there must exist a system state over which the operation can be successfully applied). Manual specification of these operations is an error-prone and time-consuming activity. Therefore, the goal of this paper is to automatically provide a basic set of system operations that verify these two properties. Operations are drawn from the elements (classes, attributes, etc) of the class diagram and take into account the possible dependencies between the different change events (i.e. inserts/updates/deletes) that may be applied to them. Afterwards, the designer could reuse our proposal to build up more complex operations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Finding the Pattern You Need: The Design Pattern Intent Ontology",
        "date": 2007,
        "abstract": "Since the seminal book by the Gang of Four, design pat- terns have proven an important tool in software development. Over time, more and more patterns have been discovered and developed. The sheer amount of patterns available makes it hard to ﬁnd patterns useful for solving a speciﬁc design problem. Hence, tools supporting searching and ﬁnding design patterns appropriate to a certain problem are required. To develop such tooling, design patterns must be described formally such that they can be queryed by the problem to be solved. Current ap- proaches to formalising design patterns focus on the solution structure of the pattern rather than on the problems solved. In this paper, we present a formalisation of the intent of the 23 patterns from the Gang-of-Four book. Based on this formalisation we have developed a Design Pattern Wizard that proposes applicable design patterns based on a description of a design problem.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Approach for Managing \nHuman Interface Design Life Cycle",
        "date": 2007,
        "abstract": "Designing a large application user interface is an iterative process. Commonly used tools lack models to support this iterative process. Research on model-driven UI design has over the years focused on modeling UI at a higher level of abstraction but lacked support during in the iteration process. This paper briefly presents the context of our research – transforming a business model into a base UI model for further customization. Specifically, we present a feature that helps reflect changes from the business model in the user interface design tool. We designed it so that the human designers can choose to react to these changes as they see appropriate. The technique is one of our attempts to apply the model-drive approach to better support design iteration through requirement changes.",
        "keywords": [
            "User Interface Model",
            "Human Computer Interaction Model",
            "User- Centered Design",
            "User Interface Modeling Tools",
            "Model Transformations",
            "Model Engineering Methodologies."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating Heterogeneous Tools into Model-Centric Development of Interactive Applications",
        "date": 2007,
        "abstract": "The development of successful interactive applications often requires high eﬀorts in creative design tasks to build high quality user interfaces. Such creative development tasks – such as user interface design or design of speciﬁc features like 3D objects – are usually performed using diﬀerent tools optimized for the respective task. For example, in early development stages, tools like Photoshop or Flash are established for creating user interface prototypes. 3D graphics is usually developed using 3D authoring tools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A Business-Process-Driven Approach for Generating  \nE-Commerce User Interfaces",
        "date": 2007,
        "abstract": "A business process contains a set of interdependent activities that describe operations provided by an organization. E-commerce applications are designed to automate business processes. A business process specification (i.e., a workflow) is defined by a business analyst from the viewpoint of the end- users. The process encapsulates the knowledge related to the natural work rhythms that a business user would follow when using an e-commerce application. In this paper, we analyze the information embedded in business process specifications, and infer the functional and usability requirements. We use the inferred information in a model-driven approach to automatically generate user interfaces (UIs) from a business process specification through a set of transformations. To improve the usability of UIs for the e-commerce applications, each transformation is guided by usability principles.",
        "keywords": [
            "Business process",
            "User interface generation",
            "Usability",
            "Model  driven engineering",
            "Task model",
            "Dialog model",
            "Presentation model."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing UML Extensions with Operational Semantics Behaviored Proﬁles with Templates",
        "date": 2007,
        "abstract": "The objective of the ongoing OMG standard about a foun- dational UML subset semantics (fUML) is twofold: providing operational semantics for a UML subset, and ease unambiguous and automatic model exploitations. Its impact could however be limited if usual UML proﬁl- ing practices do not evolve. Proﬁles are the traditional way to specialize UML semantics and handle semantic variation points. However, they are usually deﬁned in a way that only informally addresses the semantic issue, potentially limiting the beneﬁts that fUML could bring in UML based methodologies. UML proﬁling practices must evolve: we propose to explicitly encapsulate operational semantics into stereotype opera- tions, and provide a way to intuitively handle semantic variation points through template parameters. We illustrate the usage of these mecha- nisms and demonstrate their potential beneﬁts. We also show that no UML metamodel modiﬁcations are required to support them, so that their implementation in L3-compliant UML tools is straightforward1.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrated Deﬁnition of Abstract and Concrete Syntax for Textual Languages",
        "date": 2007,
        "abstract": "An understandable concrete syntax and a comprehensible abstract syntax are two central aspects of deﬁning a modeling language. Both representations of a language signiﬁcantly overlap in their structure and also information, but may also diﬀer in parts of the information. To avoid discrepancies and problems while handling the language, concrete and abstract syntax need to be consistently deﬁned. This will become an even bigger problem, when domain speciﬁc languages will become used to a larger extent. In this paper we present an extended grammar for- mat that avoids redundancy between concrete and abstract syntax by allowing an integrated deﬁnition of both for textual modeling languages. For an amendment of the usability of the abstract syntax it furthermore integrates meta-modeling concepts like associations and inheritance into a well-understood grammar-based approach. This forms a sound foun- dation for an extensible grammar and therefore language deﬁnition.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Architectural Aspects in UML",
        "date": 2007,
        "abstract": "Architecture descriptions are important for reasoning about system properties in order to make the right architectural decisions for building systems with adequate quality. Modularising concerns at the architecture description level may ease system conﬁgurability and cater for variations in architectural require- ments. We devise a technique for modularising and composing complex architec- tural connectors described in UML using structured classes. We deﬁne a binding language with lexical and graphical syntax to support the composition. Finally, we discuss the relationship with standard UML constructs.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain Speciﬁc Modeling Methodology for Reconﬁgurable Networked Systems",
        "date": 2007,
        "abstract": "Our empirical study shows that reconﬁgurable networked systems ex- ecuting software components deployed on interconnected heterogeneous hard- ware nodes highly beneﬁt from an effective framework and a normative design methodology relying on domain speciﬁc models supporting both application and platform domains. The approach is based on building metamodels for various expert domains to enable precise knowledge codiﬁcation in the form of inter- pretable and analyzable information frameworks. The core platform architecture is characterized by interlinked on-the-ﬂy reconﬁgurable communicating compo- nents whose behavior is speciﬁed by ﬁnite state machine model of computation. The proposed methodology covers the whole development and operation life cycle.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Modelling Method for Rigorous and Automated Design of Large-Scale Industrial Systems",
        "date": 2007,
        "abstract": "Compositional architecture-driven and model-based system design holds huge potential to increase design eﬃciency and improve de- sign quality for large-scale industrial systems. Transition to such design paradigm is hampered by the lack of domain-speciﬁc methods and tools that give adequate support for both behavioral and structural modeling and development automation. This paper introduces an enhancement to Lyra, a rigorous service-oriented modeling method for the design of communicating distributed systems that brings process algebraic think- ing into industrial system speciﬁcation with particular focus on behav- ior. This enhancement oﬀers a sound basis for implementing the ideas of MDA in automation of system design, functional veriﬁcation and confor- mance testing. The Lyra method and its enhancement are exempliﬁed using UML2 to model a critical and complex part of the mobile WiMAX wireless system.",
        "keywords": [
            "model-based system design",
            "MDA",
            "UML2",
            "design automa- tion",
            "formal methods."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Relating Navigation and Request Routing Models in Web \nApplications",
        "date": 2007,
        "abstract": "A navigation model describes the possible sequences of web pages a user can visit, and a request routing model describes how server side compo- nents handle each request. Earlier we developed formal models and analysis operations for such models. While each is useful independently, their utility is greatly improved by relating the models, which is the contribution described in this paper. We describe mappings between the models, and show that the map- pings preserve navigation behavior and are bijective, thus supporting traceabil- ity and allowing the models to be used in round-trip engineering. With these mappings built into our Model Helper tool, it is now possible to automatically determine whether a Request Routing model conforms to the navigation design, and to automatically generate a Request Routing model from a navigation model. Finally, we describe one of a number of case studies where we used Model Helper in a round-trip engineering scenario.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML2 Profile for Service Modeling",
        "date": 2007,
        "abstract": "In this article we provide an embedding of an interaction-based service notion into UML2. Such an embedding is needed, because to this date, UML2 has only limited support for services – they are certainly not first-class modeling elements of the notation. This is despite the ever increasing importance of services as an integration paradigm for ultra large scale systems. The embedding we provide rests on two observations: (i) services are fundamentally defined by component collaborations; (ii) to support a seamless development process, the service notion must span both logical and deployment architecture. To satisfy (i) and (ii) we introduce modifications to the UML that focus on interaction modeling, and the mapping from logical to deployment service architectures. The result is a novel and comprehensive UML2 profile for service-oriented systems.",
        "keywords": [
            "Rich Services",
            "Service-oriented Architectures",
            "Web Services",
            "Model Driven Architectures."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic Generation of  \nWorkflow-Extended Domain Models",
        "date": 2007,
        "abstract": "The specification of business processes is becoming a more and more critical aspect for organizations. Such processes are specified as workflow models expressing the logical precedence among the different business activi- ties (i.e. the units of work). Up to now, workflow models have been commonly managed through specific subsystems, called workflow management systems. In this paper we advocate for the integration of the workflow specification in the system domain model. This workflow-extended domain model is automati- cally derived from the initial workflow specification. Then, model-driven development methods may depart from the extended domain model to auto- matically generate an implementation of the system enforcing the business processes in any final technology platform, thus avoiding the need of basing the implementation on a dedicated workflow engine.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Practical Perspective on the Design and \nImplementation of Service-Oriented Solutions",
        "date": 2007,
        "abstract": "Business-driven development is an approach that focuses on automating the path from business understanding to IT solution. IBM’s experiences with customers taking a business-driven approach to develop services-oriented solutions are highlighting a number of best practices that are important to share and discuss. This paper focuses on how companies adopting a service-oriented approach are assembling the appropriate environment to be successful. The paper identifies three design techniques for SOA and describes when each of them can be used in practice, depending on the business and IT drivers and the organization’s maturity. We then highlight how to use structured enterprise models together with the tools and methods to automate the design of service-oriented solutions. These scenarios and examples are playing an important role in the development of future method content and tooling requirements for IBM Rational tools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Constructive Techniques for Meta- and Model-Level Reasoning",
        "date": 2007,
        "abstract": "The structural semantics of UML-based metamodeling were recently explored[1], providing a characterization of the models adher- ing to a metamodel. In particular, metamodels can be converted to a set of constraints expressed in a decidable subset of ﬁrst-order logic, an extended Horn logic. We augment the constructive techniques found in logic programming, which are also based on an extended Horn logic, to produce constructive techniques for reasoning about models and meta- models. These methods have a number of practical applications: At the meta-level, it can be decided if a (composite) metamodel characterizes a non-empty set of models, and a member can be automatically con- structed. At the model-level, it can be decided if a submodel has an embedding in a well-formed model, and the larger model can be con- structed. This amounts to automatic model construction from an in- complete model. We describe the concrete algorithms for constructively solving these problems, and provide concrete examples.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Metamodel-Based Approach for Analyzing Security-Design Models",
        "date": 2007,
        "abstract": "We have previously proposed an expressive UML-based lan- guage for constructing and transforming security-design models, which are models that combine design speciﬁcations for distributed systems with speciﬁcations of their security policies. Here we show how the same framework can be used to analyze these models: queries about proper- ties of the security policy modeled are expressed as formulas in UML’s Object Constraint Language and evaluated over the metamodel of the security-design language. We show how this can be done in a semanti- cally precise and meaningful way and demonstrate, through examples, that this approach can be used to formalize and check non-trivial se- curity properties of security-design models. The approach and examples presented have been implemented and checked in the SecureMOVA tool.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML2Alloy: A Challenging Model Transformation",
        "date": 2007,
        "abstract": "Alloy is a formal language, which has been applied to mod- elling of systems in a wide range of application domains. It is supported by Alloy Analyzer, a tool, which allows fully automated analysis. As a result, creating Alloy code from a UML model provides the opportunity to exploit analysis capabilities of the Alloy Analyzer to discover possible design ﬂaws at early stages of the software development. Our research makes use of model based techniques for the automated transformation of UML class diagrams with OCL constraints to Alloy code. The paper demonstrates challenging aspects of the model transformation, which originate in fundamental diﬀerences between UML and Alloy. We shall discuss some of the diﬀerences and illustrate their implications on the model transformation process. The presented approach is explained via an example of a secure e-business system.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "i2MAP: An Incremental and Iterative Modeling and Analysis Process⋆",
        "date": 2007,
        "abstract": "Detecting errors early within the development process for an embedded system assists a developer in avoiding excessive error correc- tion costs and minimizing catastrophic losses resulting from failures in deployed systems. Towards that end, this paper presents i2MAP, an iter- ative and incremental goal-driven process for constructing an analysis- level UML model of an embedded system. The UML model is formally analyzed for adherence to the behavioral properties captured in a com- panion goal model. The process uses goal modeling to capture the requirements of the system, and uses UML to capture analysis-level structural and behavioral information. Both types of i2MAP models can be used to drive a rigorous approach to model-driven development of embedded systems. In this paper, we illustrate the i2MAP process and the accompanying tool suite in the development of an embedded system model for an adaptive light control system.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Driven Measurement Procedure for Sizing Web \nApplications: Design, Automation and Validation*",
        "date": 2007,
        "abstract": "This paper introduces the Object-Oriented Hypermedia Function Points (OO-HFP), which is a functional size measurement procedure for Web projects developed using the Object-Oriented Hypermedia (OO-H) method. This method provides model-driven and transformation-based support for the development of Web applications. Using OO-HFP, a size measure is obtained once a Web application’s conceptual model is completed. We follow the steps of a process model for software measurement in order to detail the design and automation of OO-HFP. Finally, we present the validation of OO-HFP for Web effort estimation by comparing the prediction accuracy that it provides to the accuracy provided by another set of validated size measures (the Tukutuku measures) that was found to be a good effort predictor. The results of a study using industrial data show that the effort estimates obtained for projects that are sized using OO-HFP were similar to those using the Tukutuku measures, thus suggesting that the OO-HFP is a suitable effort predictor.",
        "keywords": [
            "Model-driven development",
            "Web Engineering",
            "Functional Size  Measurement",
            "Web Effort Estimation",
            "OO-H."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Engineering for Software\nMigration in a Large Industrial Context",
        "date": 2007,
        "abstract": "As development techniques, paradigms and platforms evolve far more quickly than domain applications, software modernization and migration, is a constant challenge to software engineers. For more than ten years now, the Sodifrance company has been intensively using Model- Driven Engineering (MDE) for both development and migration projects. In this paper we report on the use of MDE as an eﬃcient, ﬂexible and reliable approach for a migration process (reverse-engineering, transfor- mation and code generation). Moreover, we discuss how MDE is eco- nomically proﬁtable and is cost-eﬀective over the migration through out-sourced manual re-development. The paper is illustrated with the migration of a large-scale banking system from Mainframe to J2EE.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Introducing Variability into Aspect-Oriented Modeling Approaches",
        "date": 2007,
        "abstract": "Aspect-Oriented Modeling (AOM) approaches propose to model reusable aspects, or cross-cutting concerns, that can be composed in diﬀerent systems at a model or code level. Building complex systems with reusable aspects helps managing software complexity. But in gen- eral, reusability of an aspect is limited to a particular context. On the one hand, if the target model does not match the template point-to-point, the aspect cannot be applied. On the other hand, even when it is actually applied, it is woven into the target model always in the same way. In this paper1, we point out the needs of variability in the AOM approaches and introduce seamless variability mechanisms in an existing AOM approach to improve reusability. Our aspects can ﬁt various contexts and can be composed into the base model in diﬀerent ways. Introducing variability into AOM approaches will turn standard aspects into highly reusable aspects.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Expressive Aspect Composition Language for UML \nState Diagrams",
        "date": 2007,
        "abstract": "The goal of aspect-oriented software development is to maintain a clear separation of concerns throughout the software lifecycle. Concerns that are separated, however, must be composed at some point. The hypothesis in this paper is that existing aspect-oriented modeling composition methods are not expressive enough for composing state-dependent behavioral models. The paper presents a new aspect composition language, SDMATA, for UML state diagrams. SDMATA supports a richer form of model composition than previous approaches to aspect-oriented modeling. Firstly, pointcuts are given as patterns which allows for sequence pointcuts, loop pointcuts, etc. Secondly, SDMATA supports rich forms of composition including parallel composition and alternative composition. The language is applied to the use case slice technique of Jacobson and Ng. The findings are that it is possible to maintain the separation of state-dependent models during software design and that expressive model composition methods are necessary to do this in practice.",
        "keywords": [
            "aspect-oriented development",
            "state machines",
            "use cases."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "SDMATA"
        }
    },
    {
        "title": "Enhancing UML State Machines with Aspects",
        "date": 2007,
        "abstract": "Separation of Concerns (SoC) is an important issue to reduce the com- plexity of software. Recent advances in programming language research show that Aspect-Oriented Programming (AOP) may be helpful for enhancing the SoC in software systems: AOP provides a means for describing concerns which are normally spread throughout the whole program at one location. The arguments for introducing aspects into programming languages also hold for modeling lan- guages. In particular, modeling state-crosscutting behavior is insufﬁciently sup- ported by UML state machines. This often leads to model elements addressing the same concern scattered all over the state machine. We present an approach to aspect-oriented state machines, which show considerably better modularity in modeling state-crosscutting behavior than standard UML state machines.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Complementary Use Case Scenario\nRepresentations Based on Domain Vocabularies",
        "date": 2007,
        "abstract": "Use cases are commonly used as notation for capturing func- tional requirements through scenarios. The problem is that there is no universal notation for use case contents which is capable of accommodat- ing all the needs of software project participants. Business analysts and stakeholders need understandability and informality, while for architects and designers, precision and unambiguity are the most crucial features. In this paper we propose a metamodel and concrete syntax for three com- plementary representations of use case scenarios. These representations present the same information, but put emphasis on diﬀerent aspects of it thus accommodating for diﬀerent readers. This metamodel utilises the idea of separation of requirements as such from their representations as well as the idea of clear distinction between description of the system’s behaviour and of the problem domain.",
        "keywords": [
            "use cases",
            "requirements",
            "scenarios",
            "activity diagrams",
            "inter- action diagrams."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Time(s)",
        "date": 2007,
        "abstract": "Time and timing features are an important aspect of mod- ern electronic systems, often of embedded nature. We argue here that in early design phases, time is often of logical (rather than physical) nature, even possibly multiform. The compilation/synthesis of heterogeneous ap- plications onto architecture platforms then largely amounts to adjusting the former logical time(s) demands onto the latter physical time abili- ties. Many distributed scheduling techniques pertain to this approach of “time reﬁnement”.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML Profile for Developing Airworthiness-Compliant \n(RTCA DO-178B), Safety-Critical Software",
        "date": 2007,
        "abstract": "Many safety-related, certification standards exist for developing safety-critical systems. System safety assessments are common practice and system certification according to a standard requires submitting relevant software safety information to appropriate authorities. The airworthiness standard, RTCA DO-178B, is the de-facto standard for certifying aerospace systems containing software. This research introduces an approach to improve communication and collaboration among safety engineers and software engineers by proposing a Unified Modeling Language (UML) profile that allows software engineers to model safety-related concepts and properties in UML, the de-facto software modeling language. Key safety-related concepts are extracted from RTCA DO-178B, and then a UML profile is defined to enable their precise modeling. We show that the profile improves the line of communication between safety engineers and software engineers, for instance by allowing the automated generation of certification-related information from UML models. This is illustrated through a case study on developing an aircraft’s navigation controller subsystem.",
        "keywords": [
            "UML",
            "UML Profile",
            "Airworthiness",
            "RTCA DO-178B",
            "Safety",
            "Safety-Critical",
            "Safety Assessment",
            "Certification",
            "Certification Authority."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Forensic Debugging of Model Transformations",
        "date": 2007,
        "abstract": "Software bugs occur in model-driven development, just as they do with traditional development techniques. We explore the types of bugs that occur in model transformations and identify debugging ap- proaches that can be applied or adapted to a model-driven context. In- vestigation shows that the detailed source-to-target traceability avail- able with model transformations enables eﬀective post-hoc, or forensic, debugging. Forensic debugging techniques are introduced for automated bug localisation in model transformations. The methods discussed are grounded with examples using the Eclipse Modeling Framework (EMF) and Tefkat, a declarative model transformation engine.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Tefkat"
        }
    },
    {
        "title": "Runtime Debugging Using Reverse-Engineered UML",
        "date": 2007,
        "abstract": "Finding runtime faults in object-oriented code can be very difﬁcult even with the aid of modern runtime debuggers. Failures may manifest them- selves due to decisions in the code that were executed much earlier in the pro- gram. Tracing execution paths and values backward from a failure to the faulty code can be a daunting task. We propose a fault ﬁnding approach that uses unit tests to exercise source code in order to trace object-method execution paths. This is similar to reverse-engineering techniques used to create Sequence Dia- grams from code. It is often too complex to debug a program using a large set of reverse-engineered Sequence Diagrams each obtained from an individual ex- ecution. Therefore, our approach partitions and aggregates individual execution paths into into fault and non-fault revealing categories. By examining the differ- ences between fault and non-fault paths, we are left with a simpliﬁed graph. The graph can then be transformed into a useful Sequence Diagram that may reveal the location of the faulty code.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formally Deﬁning a Graphical Language for\nMonitoring and Checking Object Interactions",
        "date": 2007,
        "abstract": "Monitoring and checking object interactions is an impor- tant activity for testing/debugging scenario implementation in an object- oriented system. In our previous work, we proposed behavior view diagrams (BVD) as a graphical language for writing programs that auto- mate such monitoring and checking process. In this paper, we illustrate the formal deﬁnition of the syntax and the semantics of an extended ver- sion of BVD that can also be used to describe multi-threaded scenarios. This formal deﬁnition provides a critical foundation both for understand- ing the language and for building its tool support.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Statechart Development Beyond WYSIWYG",
        "date": 2007,
        "abstract": "Modeling systems based on semi-formal graphical formalisms, such as Statecharts, have become standard practice in the design of reactive embedded devices. Statecharts are often more intuitively under- standable than equivalent textual descriptions, and their animated simu- lation can help to visualize complex behaviors. However, in terms of editing speed, project management, and meta-modeling, textual descrip- tions have advantages.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Based Design of Computer-Controlled Game Character Behavior",
        "date": 2007,
        "abstract": "Recently, the complexity of modern, real-time computer games has increased drastically. The need for sophisticated game AI, in particular for Non-Player Characters, grows with the demand for re- alistic games. Writing consistent, re-useable and eﬃcient AI code has become hard. We demonstrate how modeling game AI at an appropri- ate abstraction level using an appropriate modeling language has many advantages. A variant of Rhapsody Statecharts is proposed as an ap- propriate formalism. The Tank Wars game by Electronic Arts (EA) is used to demonstrate our concrete approach. We show how the use of the Statecharts formalism leads quite naturally to layered modeling of game AI and allows modelers to abstract away from choices between, for example, time-slicing and discrete-event time management. Finally, our custom tools are used to synthesize eﬃcient C++ code to insert into the Tank Wars main game loop.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Construction of Certiﬁed Binaries",
        "date": 2007,
        "abstract": "Proof-Carrying Code (PCC) and Certifying Model Checking (CMC) are established paradigms for certifying the run-time behavior of programs. While PCC allows us to certify low-level binary code against relatively simple (e.g., memory-safety) policies, CMC enables the certiﬁ- cation of a richer class of temporal logic policies, but is typically restricted to high-level (e.g., source) descriptions. In this paper, we present an auto- mated approach to generate certiﬁed software component binaries from UML Statechart speciﬁcations. The proof certiﬁcates are constructed us- ing information that is generated via CMC at the speciﬁcation level and transformed, along with the component, to the binary level. Our tech- nique combines the strengths of PCC and CMC, and demonstrates that formal certiﬁcation technology is compatible with, and can indeed ex- ploit, model-driven approaches to software development. We describe an implementation of our approach that targets the Pin component tech- nology, and present experimental results on a collection of benchmarks.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tutorials at MODELS 2007",
        "date": 2007,
        "abstract": "The MODELS 2007 conference offered four high-quality tutorials from leading experts in the area of model-driven engineering. Each tutorial was presented as a half-day event that was organized during the first two days of the conference. This short overview provides an introduction to the tutorials program and a summary of each tutorial as submitted by the presenters.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Objects and Arrows of Computational Design",
        "date": 2008,
        "abstract": "Computational Design (CD) is a paradigm where both program de- sign and program synthesis are computations. CD merges Model Driven Engi- neering (MDE) which synthesizes programs by transforming models, with Software Product Lines (SPL) where programs are synthesized by composing transformations called features. In this paper, basic relationships between MDE and SPL are explored using the language of modern mathematics.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Algebraic Models for Bidirectional Model Synchronization⋆",
        "date": 2008,
        "abstract": "The paper presents several algebraic models for semantics of bidirectional model synchronization and transformation. Diﬀerent pat- terns of model synchronization are analyzed (including view updates and incremental synchronization), and this analysis motivates the formal de- ﬁnitions. Relationships between the formal models are precisely speciﬁed and discussed. A new formal model of updates is proposed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Invariant-Based Method for the Analysis of Declarative Model-to-Model Transformations",
        "date": 2008,
        "abstract": "In this paper we propose a method to derive OCL invari- ants from declarative speciﬁcations of model-to-model transformations. In particular we consider two of the most prominent approaches for speci- fying such transformations: Triple Graph Grammars and QVT. Once the speciﬁcation is expressed in the form of invariants, the transformation developer can use such description to verify properties of the original transformation (e.g. whether it deﬁnes a total, surjective or injective function), and to validate the transformation by the automatic genera- tion of valid pairs of source and target models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Precise Semantics of EMF Model Transformations by Graph Transformation",
        "date": 2008,
        "abstract": "Model transformation is one of the key activities in model-driven soft- ware development. An increasingly popular technology to deﬁne modeling lan- guages is provided by the Eclipse Modeling Framework (EMF). Several EMF model transformation approaches have been developed, focusing on different transformation aspects. To validate model transformations wrt. functional behav- ior and correctness, a formal foundation is needed. In this paper, we deﬁne EMF model transformations as a special kind of typed graph transformations using node type inheritance. Containment constraints of EMF model transformations are translated to a special kind of EMF model transformation rules such that their application leads to consistent transformation results only. Thus, we identify a kind of EMF model transformations which behave like algebraic graph trans- formations. As a consequence, the rich theory of algebraic graph transformation can be applied to these EMF model transformations to show functional behavior and correctness. We illustrate our approach by selected refactorings of simpliﬁed statechart models.",
        "keywords": [
            "Model-driven software development",
            "Eclipse Modeling Framework",
            "model transformation",
            "graph transformation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Formal Metamodel for Problem Frames",
        "date": 2008,
        "abstract": "Problem frames are patterns for analyzing, structuring, and character- izing software development problems. This paper presents a formal metamodel for problem frames expressed in UML class diagrams and using the formal spec- iﬁcation notation OCL. That metamodel clariﬁes the nature of the different syn- tactical elements of problem frames, as well as the relations between them. It provides a framework for syntactical analysis and semantic validation of newly deﬁned problem frames, and it prepares the ground for tool support for the prob- lem frame approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Visualization of Use Cases through Automatically Generated Activity Diagrams",
        "date": 2008,
        "abstract": "Functional requirements are often written using use cases formatted by textual templates. This textual approach has the advantage to be easy to adopt, but the requirements can then hardly be processed for further purposes like test generation. In this paper, we propose to generate automatically through a model transformation an activity diagram modeling the use case scenario. Such an activity diagram allows us to guess in a glimpse the global behavior of a use case, and can easily be processed. The transformation is defined using the QVT-Relational language, and is illustrated on a case study using a supporting tool.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-Relational"
        }
    },
    {
        "title": "Requirements Modeling and Validation Using Bi-layer Use Case Descriptions",
        "date": 2008,
        "abstract": "Extension of the modeling notations and formal languages for use case description are the commonly suggested solutions for adding precision to use case models. Practitioners have often argued against adoption of such techniques citing reasons like the steep learning curve for formal languages; and the quickness in using imprecise use case de- scriptions for communicating to diﬀerent stake-holders of the system. In this paper we introduce the Archetest modeling environment, which through a unique bi-layer approach accepts use case descriptions in their imprecise form and then assists in adding precision through a wizard driven process. Thereby, it lends itself to both quick and precise mod- eling. Also the two forms of the use case models are self contained and cross-linked. This allows diﬀerent modelers, the precise and the impre- cise, to collaborate and also supports stake-holder speciﬁc feedbacks of the automated analysis. We describe the structure of Archetest’s use case models, and show how these models are amenable to automated process- ing. We present a case study which reports on typical modeling times using Archetest and demonstrates its scalability.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "WebWorkFlow: An Object-Oriented Workﬂow Modeling Language for Web Applications",
        "date": 2008,
        "abstract": "Workﬂow languages are designed for the high-level descrip- tion of processes and are typically not suitable for the generation of complete applications. In this paper, we present WebWorkFlow, an object-oriented workﬂow modeling language for the high-level descrip- tion of workﬂows in web applications. Workﬂow descriptions deﬁne pro- cedures operating on domain objects. Procedures are composed using sequential and concurrent process combinators. WebWorkFlow is an em- bedded language, extending WebDSL, a domain-speciﬁc language for web application development, with workﬂow abstractions. The extension is implemented by means of model-to-model transformations. Rather than providing an exclusive workﬂow language, WebWorkFlow supports inter- action with the underlying WebDSL language. WebWorkFlow supports most of the basic workﬂow control patterns.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Future of Train Signaling",
        "date": 2008,
        "abstract": "Producing the source code for a railway interlocking system based on the description of a station has traditionally been a multistage manual process. We show how this process can be automated and made less error-prone by in- troducing model-driven development (MDD). This paper addresses the exp erience of developing a Domain Specific Language (DSL) to describe railway stations, Train Control Language (TCL), and tools to support this language. In the railroad domain where there are extreme safety requirements, it is essential to show that consistency and completeness can be assured. We address how the model is used to generate several different representations for different pur- poses. We look at advantages and challenges with our approach, and we discuss improvements to existing technologies to support our case better.",
        "keywords": [
            "Train",
            "signaling",
            "interlocking",
            "DSL",
            "model-driven development",
            "MoSiS."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "NAOMI – An Experimental Platform for Multi–modeling",
        "date": 2008,
        "abstract": "Domain-speciﬁc modeling languages (DSMLs) are designed to pro- vide precise abstractions of domain-speciﬁc constructs. However, models for complex systems typically do not ﬁt neatly within a single domain and capturing all important aspects of such a system requires developing multiple models using different DSMLs. Combining these models into multi-models presents difﬁcult challenges, most importantly those of integrating the various models and keeping both the models and their associated data synchronized. To this end, we present NAOMI, an experimental platform for enabling multiple models, developed in different DSMLs, to work together. NAOMI analyzes model dependencies to de- termine the impact of changes to one model on other dependent models and co- ordinates the propagation of necessary model changes. NAOMI also serves as a useful testbed for exploring how diverse modeling paradigms can be combined.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Abstraction and Modelling — A Complementary Partnership",
        "date": 2008,
        "abstract": "Why is it that some software engineers are able to produce clear, elegant designs and programs, while others cannot? Is it purely a matter of intelligence? One hypothesis is that the answer lies in ab- straction: the ability to exhibit abstraction skills and perform abstract thinking and reasoning. Abstraction is a cognitive means by which en- gineers, mathematicians and others deal with complexity. It covers both aspects of removing detail as well as the identiﬁcation of generalisations or common features, and has been identiﬁed as a crucial skill for software engineering professionals. Is it possible to improve the skills and abilities of those less able through further education and training? Are there any means by which we can measure the abstraction skills of an individual? In this talk, we explore these questions, and argue that abstraction and modelling are complementary partners: that abstraction is the key skill for modelling and that modelling provides a sound means for practising and improving abstraction skills.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Transformation as an Optimization Problem",
        "date": 2008,
        "abstract": "Most of the available work on model transformation is based on the hypothesis that transformation rules exist and that the important issue is how to express them. But in real life, the rules may be diﬃcult to deﬁne; this is often the case when the source and/or target formalisms are not widely used or proprietary. In this paper, we consider the trans- formation mechanism as a combinatorial optimization problem where the goal is to ﬁnd a good transformation starting from a small set of available examples. Our approach, named model transformation as optimization by examples (MOTOE), combines transformation blocks extracted from examples to generate a target model. To that end, we use an adapted version of particle swarm optimization (PSO) where transformation so- lutions are modeled as particles that exchange transformation blocks to converge towards an optimal transformation solution. MOTOE has two main advantages: It proposes a transformation without the need to de- rive transformation rules ﬁrst, and it can operate independently from the source and target metamodels.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Example-Based Program Transformation",
        "date": 2008,
        "abstract": "Software changes. During their life cycle, software systems experi- ence a wide spectrum of changes, from minor modiﬁcations to major architectural shifts. Small-scale changes are usually performed with text editing and refactor- ings, while large-scale transformations require dedicated program transformation languages. For medium-scale transformations, both approaches have disadvan- tages. Manual modiﬁcations may require a myriad of similar yet not identical edits, leading to errors and omissions, while program transformation languages have a steep learning curve, and thus only pay off for large-scale transformations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "None"
        }
    },
    {
        "title": "Detecting Patterns of Poor Design Solutions Using Constraint Propagation",
        "date": 2008,
        "abstract": "We are proposing an approach for applying design patterns that con- sists of recognizing occurrences of the modeling problem solved by the design pattern (problem pattern) in input models, which are then transformed accord- ing to the solution proposed by the design pattern (solution pattern). In this pa- per, we look at the issue of identifying instances of problem patterns in input models, and marking the appropriate entities so that the appropriate transforma- tions can be applied. Model marking within the context of MDA is a notori- ously difficult problem, in part because of the structural complexity of the patterns that we look for, and in part because of the required design knowledge- - and expertise. Our representation of design problem patterns makes it rela- tively easy to express the pattern matching problem as a constraint satisfaction problem. In this paper, we present our representation of design problem pat- terns, show how matching such patterns can be expressed as a constraint satis- faction problem, and present an implementation using ILOG JSolver, a commercial CSP solver.",
        "keywords": [
            "Marking models",
            "constraint satisfaction problems",
            "transformations",
            "design patterns."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A General Approach for Scenario Integration⋆",
        "date": 2008,
        "abstract": "An approach to integrating UML Sequence Diagrams is pre- sented. It rests on a well-established theory, is generalizable to a large class of requirements engineering models, and supports many diﬀerent kinds of scenario integration operations. An implementation of the ap- proach as an Eclipse extension is described. Lessons learned from the implementation and during ﬁrst, preliminary experiments to study the practical aspects of the approach, are discussed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Behavioral Modelling and Composition of Object Slices Using Event Observation",
        "date": 2008,
        "abstract": "Some analysis and design methods for complex software sys- tems lead to the speciﬁcation of components (classes) by slices. It is the case of the use-case slicing technique proposed by Jacobson and Ng, and of view-based modelling proposed by Nassar et al. The composition of class slices is known from the literature to be closer to aspect composi- tion than to traditional interface-based composition, but remains largely an open problem.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scenario-Based Static Analysis of UML Class Models",
        "date": 2008,
        "abstract": "Static analysis tools, such as OCLE and USE, can be used to analyze structural properties of class models. The USE tool also provides support for analyzing specified operations through interactive simulations in which users provide operation parameters, and manually assign values to state elements to reflect the effect of an operation. In this paper we describe an approach to stati- cally analyzing behavior that does not require a user to manually simulate be- havior. The approach involves transforming a class model into a static model of behavior, called a Snapshot Model. A Snapshot Model characterizes se- quences of snapshots, where a snapshot describes an application state. A sce- nario describing a sequence of operation invocations can be verified against a Snapshot Model using tools such as USE and OCLE. We illustrate our ap- proach by verifying a scenario against a Snapshot Model that describes the be- havior of some operations in a role-based access control (RBAC) application.",
        "keywords": [
            "UML",
            "Model Analysis",
            "Behavioral Properties",
            "Snapshot."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Constructing Models with the Human-Usable Textual Notation",
        "date": 2008,
        "abstract": "We present an implementation of the OMG’s Human-Usable Textual Notation (HUTN) [6] that provides a generic concrete syntax for MOF-based metamodels. The notation is summarised. Ways in which HUTN can be applied in order to improve the productivity of Model- Driven Engineering are identiﬁed. The use of HUTN to improve the quality of test suites for verifying model management operations (such as model-to-model transformation) is described. We also present a com- parison of generic and domain-speciﬁc concrete syntax with HUTN.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "X3D-UML: 3D UML State Machine Diagrams",
        "date": 2008,
        "abstract": "X3D-UML utilises X3D (eXtensible 3D) to enable standards-based advanced 3D UML visualisations. Using X3D-UML, 3D UML State Machine Diagrams have been evaluated against actual user tasks and data, using the Se- quential Evaluation methodology. The results of User Task Analysis, Heuristic Evaluation and Formative Evaluation phases provide clear evidence that the use of UML extended with 3D is a practical solution for visualising complex sys- tem behaviour. RoseRT model metrics show between 56%-90% of state ma- chine diagram work would benefit from such 3D UML extensions; hence the 3D improvement can deliver considerable benefit to organisations.",
        "keywords": [
            "X3D-UML",
            "3D UML",
            "X3D",
            "3D Software Visualization",
            "VRML."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assessing the Influence of Stereotypes on the  Comprehension of UML Sequence Diagrams: A Controlled Experiment",
        "date": 2008,
        "abstract": "The main goal of this paper is to provide empirical evidence, through a controlled experiment, of the influence of stereotypes when modelers, devel- opers, and maintainers have to comprehend UML sequence diagrams. The comprehension of UML sequence diagrams with and without stereotypes was analyzed from three different perspectives: semantic comprehension, retention and transfer. The experiment was carried out with 77 fourth year undergraduate students of Computer Science from the University of Bari in Italy. The results obtained show a slight tendency in favor of the use of stereotypes in facilitating the comprehension of UML sequence diagrams, although it is not statistically significant. Further replications are needed to obtain more conclusive results.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "3D Parametric Models for Aeroplanes — From Idea to Design",
        "date": 2008,
        "abstract": "The early design phase of an aircraft is characterized by a large variation of studies in a short period of time. In order to support the variation of aircraft shapes, an approach is taken in which the diﬀer- ent aircraft component models are deﬁned by a limited set of parameters. The challenge lies in fulﬁlling numerous and partially conﬂicting engi- neering requirements. Models should be as ﬂexible as possible so that virtually “any” aircraft shape can be represented, in parallel the model should address the needs of diﬀerent engineering disciplines located on diﬀerent sites and in diﬀerent countries. On the other hand each engineer- ing discipline re-quests simple models with the smallest set of parameters possible to address their speciﬁc need. Finally, aircraft design is not only geometric and interfaces with the numerical world need to be established. During this talk we will be exploring the challenges and identiﬁed solu- tions for abstracting the aircraft geometry in a set of parametric models that can be shared and commonly used; we will see how dedicated CAD tools support the engineer and how the geometric models can be linked with the numerical (e.g. systems) world.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MOOGLE: A Model Search Engine",
        "date": 2008,
        "abstract": "Models are becoming increasingly important in the software process. As a consequence, the number of models being used is increasing, and so is the need for eﬃcient mechanisms to search them. Various exist- ing search engines could be used for this purpose, but they lack features to properly search models, mainly because they are strongly focused on text-based search. This paper presents Moogle, a model search engine that uses metamodeling information to create richer search indexes and to allow more complex queries to be performed. The paper also presents the results of an evaluation of Moogle, which showed that the metamodel information improves the accuracy of the search.",
        "keywords": [
            "Model Search",
            "Software Reuse",
            "Model-Driven Development."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Managing Model Conﬂicts in Distributed Development",
        "date": 2008,
        "abstract": "The growing complexity of current software systems naturally con- veyed their development toward incremental and distributed approaches to speed up the process. Several developers update the same artefact operating concurrent manipulations which need to be coherently combined. The interaction among those changes inevitably involves conﬂicts which must be detected and recon- ciled.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Metamodel Matching for Automatic Model Transformation Generation⋆",
        "date": 2008,
        "abstract": "Applying Model-Driven Engineering (MDE) leads to the cre- ation of a large number of metamodels, since MDE recommends an in- tensive use of models deﬁned by metamodels. Metamodels with similar objectives are then inescapably created. A recurrent issue is thus to turn compatible models conforming to similar metamodels, for example to use them in the same tool. The issue is classically solved developing ad hoc model transformations. In this paper, we propose an approach that au- tomatically detects mappings between two metamodels and uses them to generate an alignment between those metamodels. This alignment needs to be manually checked and can then be used to generate a model trans- formation. Our approach is built on the Similarity Flooding algorithm used in the ﬁelds of schema matching and ontology alignment. Experi- mental results comparing the eﬀectiveness of the application of various implementations of this approach on real-world metamodels are given.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Sufﬁcient Criteria for Consistent Behavior Modeling with Reﬁned Activity Diagrams",
        "date": 2008,
        "abstract": "In use case-driven approaches to requirements modeling, UML ac- tivity diagrams are a wide-spread means for reﬁning the functional view of use cases. Early consistency validation of activity diagrams is therefore desirable but difﬁcult due to the semi-formal nature of activity diagrams. In this paper, we specify well-structured activity diagrams and deﬁne activities more precisely by pre- and post- conditions. They can be modeled by interrelated pairs of object di- agrams based on a domain class diagram. This activity reﬁnement is based on the theory of graph transformation and paves the ground for a consistency analysis of the required system behavior. A formal semantics for activity diagrams reﬁned by pre- and post-conditions allows us to establish sufﬁcient criteria for consistency. The semi-automatic checking of these criteria is supported by a tool for graph transformation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Implementation of the Conformance Relation for\nIncremental Development of Behavioural Models",
        "date": 2008,
        "abstract": "In this paper, we show how to implement the conformance relation on transition systems. The computability of this relation relies on the composition of two operators: the reduction relation whose computability has been proven in our previous work, and the merge function of acceptance graphs associated with transition systems under comparison. It is formally demonstrated, and illus- trated through a case study whose analysis is performed by a JAVA prototype we have developed. This research work is developed in order to be applied in a larger context: our goal is to support modelers to develop UML state machines through an incremental modelling method which is able to guarantee that model upgrad- ing does not introduce inconsistencies. Hence, these works lead to a semantics for the specialisation relation between UML State Machines.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Based Framework for Statically and\nDynamically Checking Component Interactions⋆",
        "date": 2008,
        "abstract": "Building applications by assembling software components requires analyses of Architecture Description (AD) models for checking that component interactions respect the application and runtime context requirements. Most ex- isting interaction model analyses are static: they do not take into account runtime information, e.g., parameter values.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal Deﬁnition of MOF 2.0 Metamodel Components and Composition",
        "date": 2008,
        "abstract": "The Meta Object Facility (MOF) is one of the most fre- quently used languages for the deﬁnition of a DSL’s abstract syntax. However, its lack of sophisticated modularization concepts in compari- son to GPLs such as Ada or component-oriented ADLs makes it hard to maintain a large number of complex metamodels. MOF 2.0 packages can be used to a certain extent to deﬁne, reﬁne, and compose language descriptions, but do not oﬀer appropriate support for information hiding as well as for the speciﬁcation of parametrizable metamodeling compo- nents. Motivated by a running example we, therefore, extend MOF 2.0 with concepts for the speciﬁcation of proper metamodel components with provided export and required import interfaces. Furthermore, we present a formalization of a metamodel component composition operator based on graph morphisms. The resulting component-oriented version of MOF allows language developers to describe reoccurring, parametrizable sub- languages once and instantiate them diﬀerently in several metamodels.",
        "keywords": [
            "Metamodeling",
            "MOF 2.0",
            "software components",
            "reusability."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Interfaces and Metainterfaces for Models and Metamodels",
        "date": 2008,
        "abstract": "Evolution and customization of component-based systems require an explicit understanding of component inter-dependencies. Im- plicit assumptions, poor documentation and hidden dependencies turn even simple changes into challenges. The problem is exacerbated in XML- intensive projects due to the use of soft references and the lack of infor- mation hiding. We address this with dependency tracking interface types for models and metamodels. We provide automatic compatibility checks and a heuristic inference procedure for our interfaces, which allows easy and incremental adoption of our technique even in mature projects. We have implemented a prototype and applied it to two large cases: an en- terprise resource planning system and a healthcare information system.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model&Metamodel, Metadata and Document\nRepository for Software and Data Integration",
        "date": 2008,
        "abstract": "Model-based software engineering (MBSE) projects require and generate numerous artifacts. While MBSE methodology and design tools have reached certain maturity level, the issue of artifact persistence and management has been somewhat left in the background. We present design and implementation of the repository that supports storing and managing of artifacts such as metamodels, models, constraints, meta- data, speciﬁcations, transformation rules, code, templates, conﬁguration or documentation, and their metadata.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Construction with External Constraints:\nAn Interactive Journey from Semantics to Syntax",
        "date": 2008,
        "abstract": "Mainstream development environments have recently assimilated guidance technologies based on constraint satisfaction. We investigate one class of such technologies, namely, interactive guided derivation of models, where the editing system assists a designer by providing hints about valid editing operations that maintain global cor- rectness. We provide a semantics-based classiﬁcation of such guidance systems and investigate concrete guidance algorithms for two kinds of modeling languages: a simple subset of class-diagram-like language and for feature models. Both algorithms are eﬃcient and provide exhaustive guidance.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Benchmark for OCL Engine\nAccuracy, Determinateness, and Eﬃciency",
        "date": 2008,
        "abstract": "The Object Constraint Language (OCL) is a central element in modeling and transformation languages like UML, MOF, and QVT. Consequently approaches for MDE (Model-Driven Engineering) depend on OCL. However, OCL is present not only in these areas inﬂuenced by the OMG but also in the Eclipse Modeling Framework (EMF). Thus the quality of OCL and its realization in tools seems to be crucial for the suc- cess of model-driven development. Surprisingly, up to now a benchmark for OCL to measure quality properties has not been proposed. This pa- per puts forward in the ﬁrst part the concepts of a comprehensive OCL benchmark. Our benchmark covers (A) OCL engine accuracy (e.g., for the undeﬁned value and the use of variables), (B) OCL engine determi- nateness properties (e.g., for the collection operations any and ﬂatten), and (C) OCL engine eﬃciency (for data type and user-deﬁned opera- tions). In the second part, this paper empirically evaluates the proposed benchmark concepts by examining a number of OCL tools. The paper discusses several diﬀerences in handling particular OCL language fea- tures and underspeciﬁcations in the OCL standard.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Contrary-to-Duties Constraints: From UML to Relational Model",
        "date": 2008,
        "abstract": "Sometimes, because of an atypical situation, an important mandatory association between classes in a UML Class Diagram must be replaced by an optional one. That semantic and functional impoverishment happens because the mandatory constraint must have a boolean value. In this paper we analyze the use of soft constraints in the UML Class Diagram, and their automatic re- percussion in the corresponding Relational Model. The soft (deontic) con- straints allow the formal representation of requirements, which ideally should always be fulfilled, but can be violated in atypical situations. In this paper we enrich a previous deontic approach, by introducing the ability to explicitly rep- resent the so called Contrary-To-Duties requirements, i.e., domain integrity requirements that emerge as a consequence of an unfulfilled mandatory con- straint. We support our approach with the UML/OCL language.",
        "keywords": [
            "UML",
            "Contrary-To-Duties",
            "Relational Model",
            "Deontic Constraints."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML/SPT Model Analysis Methodology for \nConcurrent Systems Based on Genetic Algorithms",
        "date": 2008,
        "abstract": "Concurrency problems, such as deadlocks, should be identified early in the design process. This is made increasingly difficult as larger and more complex concurrent systems are being developed. We propose here an ap- proach, based on the analysis of specific models expressed in the Unified Mod- eling Language (UML) that uses a specifically designed genetic algorithm to detect deadlocks. Our main motivations are (1) to devise practical solutions that are applicable in the context of UML design without requiring additional modeling and (2) to achieve scalable automation. All relevant concurrency in- formation is extracted from systems’ UML models that comply with the UML Schedulability, Performance and Time profile, a standardized specialization of UML for real-time, concurrent systems. Our genetic algorithm is then used to search for execution sequences exhibiting deadlocks. Results on three case stud- ies show that our approach can achieve efficient results.",
        "keywords": [
            "MDD",
            "deadlocks",
            "model analysis",
            "concurrent systems",
            "UML",
            "SPT",
            "genetic algorithms."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating Performance Analysis in the Model Driven Development of Software Product Lines",
        "date": 2008,
        "abstract": "The paper proposes to integrate performance analysis in the early phases of the model-driven development process for Software Product Lines (SPL). We start by adding generic performance annotations to the UML model representing the set of core reusable SPL assets. The annotations are generic and use the MARTE Profile recently adopted by OMG. A first model transfor- mation realized in the Atlas Transformation Language (ATL), which is the fo- cus of this paper, derives the UML model of a specific product with concrete MARTE performance annotations from the SPL model. A second transforma- tion generates a Layered Queueing Network performance model for the given product by applying an existing transformation approach named PUMA, developed in previous work. The proposed technique is illustrated with an e- commerce case study that models the commonality and variability in both struc- tural and behavioural SPL views. A product is derived and the performance of two design alternatives is compared.",
        "keywords": [
            "Software Product Line",
            "Performance Analysis",
            "Model to model  Transformation",
            "UML",
            "MARTE",
            "ATL."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "A Model-Driven Measurement Approach",
        "date": 2008,
        "abstract": "Companies using domain speciﬁc languages in a model-driven development process need to measure their models. However, developing and maintaining a measurement software for each domain speciﬁc model- ing language is costly. Our contribution is a model-driven measurement approach. This measurement approach is model-driven from two view- points: 1) it measures models of a model-driven development process; 2) it uses models as unique and consistent metric speciﬁcations, w.r.t a metric speciﬁcation metamodel. This declarative speciﬁcation of metrics is then used to generate a fully ﬂedged implementation. The beneﬁt from apply- ing the approach is evaluated by two applications. They indicate that this approach reduces the domain-speciﬁc measurement software development cost.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Specifying Service Composition Using UML 2.x and Composition Policies",
        "date": 2008,
        "abstract": "In the current and future service environment, service parts are being developed separately while being dynamically combined at run- time. In this paper we address the problem of deﬁning a model-driven process for enabling dynamic composition of services. Composition poli- cies are used to deﬁne choices in behaviour under which service roles involved in a composite service can be dynamically combined at run- time. We model policy-ruled choreography of collaboration components using a policy enforcement state machine (PESM). We also deﬁne trans- formation rules for translating a global PESM diagram into a set of local PESM diagrams, one for each role. As an example, we consider the case of dynamically composing an existing service with a set of authentication and authorization collaborations. The approach is supported by a formal syntax and semantics.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Based Framework for Security Policy Specification, Deployment and Testing",
        "date": 2008,
        "abstract": "In this paper, we propose a model-driven approach for specifying, deploying and testing security policies in Java applications. First, a security policy is specified independently of the underlying access control language (OrBAC, RBAC). It is based on a generic security meta-model which can be used for early consistency checks in the security policy. This model is then automatically transformed into security policy for the XACML platform and in- tegrated in the application using aspect-oriented programming. To qualify test cases that validate the security policy in the application, we inject faults into the policy. The fault model and the fault injection process are defined at the meta- model level, making the qualification process language-independent. Empirical results on 3 case studies explore both the feasibility of the approach and the ef- ficiency of a full design & test MDE process.",
        "keywords": [
            "Metamodeling",
            "Model-driven engineering methodology",
            "Security."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Pattern Language Veriﬁer\nfor Web-Based Enterprise Applications",
        "date": 2008,
        "abstract": "The Pattern Language Veriﬁer (PLV) is a process for verify- ing the application of a pattern language in a design. The PLV process focuses on a pattern language for the design of web-based enterprise applications. We show how PLV exploits the ideas of programming lan- guage compilers to detect the structural, syntactic, and semantic errors in a design model and then guides the designer in ﬁxing the problems. To provide tool support, we integrate PLV into the ArgoUML modeling tool. We use the tool to design a simple student registration system as a case study, and show how the process ﬁnds the mistakes in the model and helps the designer in repairing the detected problems.",
        "keywords": [
            "Pattern Language",
            "Model Driven Engineering",
            "Proﬁle."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatically Generating Behavioral Models of Adaptive Systems to Address Uncertainty⋆",
        "date": 2008,
        "abstract": "Increasingly, high-assurance applications rely on dynami- cally adaptive systems (DASs) to respond to environmental changes, while satisfying functional requirements and non-functional preferences. Examples include critical infrastructure protection and transportation systems. A DAS comprises a collection of (non-adaptive) target systems (represented as UML models) and a set of adaptations that realize tran- sitions among target systems. Two sources of uncertainty inherent to DASs are: (1) predicting the future execution environment, and (2) us- ing functional and non-functional trade-oﬀs to respond to the changing environment. To address this uncertainty, we are inspired by living or- ganisms that are astonishingly adept at adapting to changing environ- mental conditions using evolution. In this paper, we describe a digital evolution-based approach to generating models that represent possible target systems suitable for diﬀerent environmental conditions, enabling the developer to identify the functional and non-functional trade-oﬀs be- tween the models, and then assisting the developer in selecting target systems for the DAS.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Autonomic Management Policy Speciﬁcation: From UML to DSML⋆",
        "date": 2008,
        "abstract": "Autonomic computing is recognized as one of the most promizing solutions to address the increasingly complex task of distributed environments’ administration. In this context, many projects relied on software components and architectures to provide autonomic management frameworks. We designed such a component-based autonomic management framework, but observed that the in- terfaces of a component model are too low-level and difﬁcult to use. Therefore, we introduced UML diagrams for the modeling of deployment and management policies. However, we had to adapt/twist the UML semantics in order to meet our requirements, which led us to deﬁne DSMLs. In this paper, we present our experience in designing the Tune system and its support for management policy speciﬁcation, relying on UML diagrams and on DSMLs. We analyse these two approaches, pinpointing the beneﬁts of DSMLs over UML.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Empirical Analysis of the Relation between Level of Detail in UML Models and Defect Density",
        "date": 2008,
        "abstract": "This paper investigates the relation between the level of detail (LoD) in UML models and defect density of the associated im- plementation. We propose LoD measures that are applicable to both class- and sequence diagrams. Based on empirical data from an indus- trial software project we have found that classes with higher LoD, calcu- lated using sequence diagram LoD metrics, correlates with lower defect density. Overall, this paper discusses a novel and practical approach to measure LoD in UML models and describes its application to a signiﬁ- cant industrial case study.",
        "keywords": [
            "Uniﬁed Modeling Language",
            "Design Metrics",
            "Quality Mea- sure",
            "Correlation Analyses."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Empirical Investigation on Dynamic Modeling in Requirements Engineering",
        "date": 2008,
        "abstract": "Modeling is a fundamental activity within the requirements engineering process concerning the construction of abstract descriptions of system requirements that are amenable to interpretation and vali- dation. In this paper we report on a controlled experiment aimed at assessing whether dynamic modeling of system requirements provides an accurate account of stakeholders’ requirements. The context is con- stituted of second year Bachelor students in Computer Science at the University of Basilicata. The data analysis reveals that there is not sig- niﬁcant diﬀerence in the comprehension of system requirements achieved by using or not dynamic modeling.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Heterogeneous Coupled Evolution of Software Languages",
        "date": 2008,
        "abstract": "As most software artifacts, meta-models can evolve. Their evolution requires conforming models to co-evolve along with them. Coupled evolution supports this. Its applicability is not limited to the modeling domain. Other do- mains are for example evolving grammars or database schemas. Existing ap- proaches to coupled evolution focus on a single, homogeneous domain. They solve the co-evolution problems locally and repeatedly. In this paper we present a systematic, heterogeneous approach to coupled evolution. It provides an auto- matically derived domain speciﬁc transformation language; a means of executing transformations at the top level; a derivation of the coupled bottom level trans- formation; and it allows for generic abstractions from elementary transforma- tions. The feasibility of the architecture is evaluated by applying it to data model evolution.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Automatability of Coupled Evolution of Metamodels and Models in Practice",
        "date": 2008,
        "abstract": "Model-based software development promises to increase pro- ductivity by oﬀering modeling languages tailored to a problem domain. Such modeling languages are often deﬁned by a metamodel. In conse- quence of changing requirements and technological progress, these modeling languages and thus their metamodels are subject to change. Manually migrating models to a new version of their metamodel is te- dious, error-prone and heavily hampers cost-eﬃcient model-based devel- opment practice. Automating model migration in response to metamodel adaptation promises to substantially reduce eﬀort. Unfortunately, little is known about the types of changes occurring during metamodel adap- tation in practice and, consequently, to which degree reconciling model migration can be automated. We analyzed the changes that occurred during the evolution history of two industrial metamodels and classi- ﬁed them according to their level of potential automation. Based on the results, we present a list of requirements for eﬀective tool support for coupled evolution of metamodels and models in practice.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enriching Reverse Engineering with Annotations",
        "date": 2008,
        "abstract": "Much of the knowledge about software systems is implicit, and therefore diﬃcult to recover by purely automated techniques. Archi- tectural layers and the externally visible features of software systems are two examples of information that can be diﬃcult to detect from source code alone, and that would beneﬁt from additional human knowledge. Typical approaches to reasoning about data involve encoding an explicit meta-model and expressing analyses at that level. Due to its informal na- ture, however, human knowledge can be diﬃcult to characterize up-front and integrate into such a meta-model. We propose a generic, annotation- based approach to capture such knowledge during the reverse engineering process. Annotation types can be iteratively deﬁned, reﬁned and trans- formed, without requiring a ﬁxed meta-model to be deﬁned in advance. We show how our approach supports reverse engineering by implement- ing it in a tool called Metanool and by applying it to (i) analyzing archi- tectural layering, (ii) tracking reengineering tasks, (iii) detecting design ﬂaws, and (iv) analyzing features.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards a Formal Account of a Foundational Subset for Executable UML Models",
        "date": 2008,
        "abstract": "A current Request for Proposal [1] from the OMG describes the requirements for an “Executable UML Foundation”. This subset of UML 2 would serve as a shared foundation for higher-level modeling concepts, such as activities, state machines, and interactions. In a sense, this subset would deﬁne a basic virtual machine for UML, allowing the execution and analysis of runtime behavior of models. Regardless of the executable subset chosen, a precise deﬁnition of execution semantics of UML actions is required. To the best of our knowledge, no formal se- mantics of such a subset yet exists. We present our work on clarifying the semantics and pragmatics of UML actions. In particular, we sketch a formalization of a subset of UML actions and discuss common usage scenarios for the most complex actions, identifying usage assumptions that are not explicit in the UML 2 speciﬁcation.",
        "keywords": [
            "Executable UML",
            "actions",
            "activities",
            "formal mapping",
            "se- mantics",
            "Model-driven Engineering."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Lightweight Approach for Deﬁning the Formal Semantics of a Modeling Language",
        "date": 2008,
        "abstract": "To deﬁne the formal semantics of a modeling language, one normally starts from the abstract syntax and then deﬁnes the static se- mantics and dynamic semantics. Having a formal semantics is important for reasoning about the language but also for building tools for the lan- guage. In this paper we propose a novel approach for this task based on the Alloy language. With the help of a concrete example language, we contrast this approach with traditional methods based on formal lan- guages, type checking, meta-modeling and operational semantics. Al- though both Alloy and traditional techniques yield a formal semantics of the language, the Alloy-based approach has two key advantages: a uni- form notation, and immediate automatic analyzability using the Alloy analyzer. Together with the simplicity of Alloy, our approach oﬀers the prospect of making formal deﬁnitions easier, hopefully paving the way for a wider adoption of formal techniques in the deﬁnition of modeling languages.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Semantically Conﬁgurable Code Generation",
        "date": 2008,
        "abstract": "In model-driven engineering (MDE), software development is centred around a formal description (model) of the proposed software system, and other software artifacts are derived directly from the model. We are investigating semantically conﬁgurable MDE, in which speciﬁers are able to conﬁgure the semantics of their models. The goal of this work is to provide a modelling environment that oﬀers ﬂexible, conﬁgurable modelling notations, so that speciﬁers are better able to represent their ideas, and yet still provides the types of analysis tools and code genera- tors normally associated with model-driven engineering. In this paper, we present a semantically conﬁgurable code-generator generator, which cre- ates a Java-code generator for a modelling notation given the notation’s semantics expressed as a set of parameter values. We are able to simu- late multiple diﬀerent model-based code generators, though at present the performance of our generated code is about an order of magnitude slower than that produced by commercial-grade generators.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Safety Hazard Identification by Misuse Cases: \nExperimental Comparison of Text and Diagrams",
        "date": 2008,
        "abstract": "In general, diagrams and text are both considered to have their ad- vantages and disadvantages for the representation of use case models, but this is rarely investigated experimentally. This paper describes a controlled experiment where we compare safety hazard identification by means of misuse cases based on use case diagrams and textual use cases. The experiment participants found use case diagrams and textual use cases equally easy to use. In most cases those who used textual use cases were able to identify more failure modes or threats. The main reason for this seems to be that use cases encourage analysts to spe- cifically focus on threats related to the functions mentioned in the use case, and textual use cases include more functional details than diagrams. The focus is decided by information in each use case which will thus decide the number of threats identified.",
        "keywords": [
            "Use cases",
            "misuse cases",
            "safety hazards",
            "experiment."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Adding Dependability Analysis Capabilities to the MARTE Proﬁle⋆",
        "date": 2008,
        "abstract": "Dependability is a non-functional property that should be assessed early in the software lifecycle. Although several UML proﬁles exist for quantitative annotations of non-functional properties, none of them provides concrete capabilities for dependability analysis of UML system models. In this paper, we propose a dependability analysis and modeling proﬁle. The objective is twofold: to reuse proposals from the literature on deriving dependability models from UML annotated speci- ﬁcations and to be compliant with the recently adopted MARTE proﬁle, which provides a framework for general quantitative analysis concepts that can be specialized to a particular analysis domain. The proﬁle def- inition process was done in several steps. Firstly, an in depth analysis of the literature has been carried out to collect the information requirements for the proﬁle. Secondly, a domain model for dependability analysis was deﬁned independently of UML. Thirdly, the domain model was mapped to UML extensions by specializing MARTE.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Visual ScatterUnit: A Visual Model-Driven Testing \nFramework of Wireless Sensor Networks Applications",
        "date": 2008,
        "abstract": "We present a model-driven test environment called Visual Scatter- Unit, which optimizes the application testing process of wireless sensor net- works. Instead of having to implement the test case completely manually, the model-driven test environment allows the abstract modeling of the test process. At the same time the test case’s technical implementation requirements are kept hidden from the user.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Aspect-Oriented Model Weaving  \nBeyond Model Composition and Model Transformation*",
        "date": 2008,
        "abstract": "Research in Aspect-Oriented Software Development (AOSD) has brought up powerful abstractions in order to specify under which conditions an aspect affects the base software. So far, Model-Driven Development (MDD) approaches to AOSD have mostly concentrated on the weaving process and, as a result, they have come up with manifold ways to compose aspect models and base models. All too often, however, the approaches disregard the benefits that the aspect-oriented abstractions can bring to software development, though. This paper discusses the implications that such negligence has on the specifica- tion of aspect-oriented models in MDD. Furthermore, it presents a weaver that is able to cope with sophisticated join point selection abstractions, as they are known from many aspect-oriented programming languages, and which go far beyond the selection capabilities provided by current model weavers. By means of this weaver, models can realize both a higher separation of concerns as well as a higher level of abstraction.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Aspect-Oriented and Model-Driven Approach for Managing Dynamic Variability⋆",
        "date": 2008,
        "abstract": "Constructing and executing distributed systems that can adapt to their operating context in order to sustain provided services and the service qualities are complex tasks. Managing adaptation of multiple, interacting services is par- ticularly difﬁcult since these services tend to be distributed across the system, interdependent and sometimes tangled with other services. Furthermore, the ex- ponential growth of the number of potential system conﬁgurations derived from the variabilities of each service need to be handled. Current practices of writing low-level reconﬁguration scripts as part of the system code to handle run time adaptation are both error prone and time consuming and make adaptive systems difﬁcult to validate and evolve. In this paper, we propose to combine model driven and aspect oriented techniques to better cope with the complexities of adaptive systems construction and execution, and to handle the problem of exponential growth of the number of possible conﬁgurations. Combining these techniques allows us to use high level domain abstractions, simplify the representation of variants and limit the problem pertaining to the combinatorial explosion of pos- sible conﬁgurations. In our approach we also use models at runtime to generate the adaptation logic by comparing the current conﬁguration of the system to a composed model representing the conﬁguration we want to reach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Managing Variability Complexity in Aspect-Oriented Modeling⋆",
        "date": 2008,
        "abstract": "Aspect-Oriented Modeling (AOM) approaches propose to model reusable aspects that can be applied to diﬀerent systems at the model level. To improve reusability, several contributions have pointed out the needs of variability in the AOM approaches. Nevertheless, the support of variability makes the aspect design more complex and the introduction of several dimensions of variability (advice, pointcut and weaving) creates a combinatorial explosion of variants and a risk of in- consistency in the aspect model. As the integration of an aspect model may be a complex task, the AOM framework has to be a support for the designer to ensure the consistency of the resulting model. This pa- per presents an approach describing how to ensure that an aspect model with variability can be safely integrated into an existing model. Veriﬁ- cation includes static checking of aspect model consistency and dynamic checking through testing with a focus on the parts of the model that are impacted by the aspect.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mapping the UML2 Semantics of Associations to a Java Code Generation Model",
        "date": 2008,
        "abstract": "It is state of the art to provide UML modeling by means of class diagrams and code generation from there. But whereas drawing di- agrams is most often well supported, code generation is limited in scope. Association classes, multiplicities, aggregation and composition are not correctly or not at all processed by most code generators. One reason may be that the UML semantics is not formally deﬁned in the UML speciﬁcation. As a result of that, associations are usually transformed into code by using properties of the same type as the associated classes or corresponding typed sets. This approach must fail although the UML2 Superstructure Speciﬁcation considers association ends owned by a class to be equal to a property of the owning class. In this paper, we describe why associations should be implemented as classes when generating code from class diagrams.",
        "keywords": [
            "UML",
            "Associations",
            "Code Generation",
            "Java."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Meaningful Composite Structures On the Semantics of Ports in UML2",
        "date": 2008,
        "abstract": "UML2 composite structures are a natural solution for the basic modeling issues associated with component-oriented approaches. They provide mechanisms for deﬁning reusable \"pieces\" of design, which are well-encapsulated through explicit interaction ports. While intuitive in principle, the semantics of request propagation across ports may cause semantic ambiguities if the composition mechanisms are not used con- sistently, thus leading to meaningless composite structures (that cannot be safely reused within the context of a particular environment). To en- sure consistent usage, this article proposes an empirical study that pro- vides an intuitive description of composite structure semantics focusing on request propagations across ports. It supplements this description by highlighting cases conducive to semantic ambiguities and oﬀers practical solutions and a rationale for building composite structures that avoid them. Among possible solutions, the opportuneness of encapsulating ex- plicit behaviors in ports is discussed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Systematic Approach to Connectors in a Multi-level Modeling Environment",
        "date": 2008,
        "abstract": "The advantage of supporting a uniform modeling approach across multiple, logical (or ontological) instantiation levels has been well documented in the literature. However, the published approaches for achieving this have fo- cused on making it possible for classes and objects to be treated uniformly across multiple instantiation levels, but have neglected the problems involved in doing the same thing for “connectors” (i.e. concepts rendered as edges in graph based depiction of models rather than nodes). On closer examination, this turns out to be a significant problem, because without an effective strategy for model- ing connectors in a uniform way, multi-level modeling as a whole is not possi- ble. In this paper we describe the problems arising from the way in which connectors (e.g. associations, links, generalizations etc.) are currently supported in mainstream modeling languages such as the UML and why they are incom- patible with multi-level modeling. We then define three fundamental connector rendering and representation principles that rectify the identified problems.",
        "keywords": [
            "Metamodeling",
            "Multi-Level Modeling",
            "Connector",
            "Association."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Based Quality Assurance of Automotive Software⋆",
        "date": 2008,
        "abstract": "Software in embedded (e.g. automotive) systems requires a high level of reliability. Model-based development techniques are increasingly used to reach this goal, but so far there is relatively little published knowledge on the compar- ative beneﬁts in using different assurance techniques. We investigate different and potentially complementary model-based software quality assurance methods (namely simulation and white-box testing vs. model-checking) at the hand of an application to the software component of a door control unit. We draw conclu- sions with regards to suitable application use cases.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ontology Guided Evolution of Complex Embedded Systems Projects in the Direction of  MDA",
        "date": 2008,
        "abstract": "Implementation of MDA in large, product developing organizations involves changing processes, practices, tools, and communication infrastruc- tures. The paper presents a case study, in which modeling related needs of a unit within Ericsson were compared to features of current and envisioned MDA tools, using qualitative methods. The paper’s main contribution is an ontology defining areas and sub-areas of improvement associated with the introduction of MDA in complex embedded systems projects. The ontology is grounded in in- terviews with senior modellers at Ericsson and in survey publications from within the field of MDA. It identifies 26 improvement areas concerned with model content, modeling activities, and the management of modeling projects. The ontology has been presented to stakeholders within the unit studied, with positive feedback: appreciated were its groundedness, traceability, holistic scope, and potential as platform and checklist for several recurrent analysis and communication tasks related to software process improvement within Ericsson.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "General Mode Controller for Software on Artificial Satellite with Model-Based Validation Tool",
        "date": 2008,
        "abstract": "High quality and flexibility are strongly required for embedded soft- ware on artificial satellites. High quality is very important because any single halt is not allowed over 15 years operation period without any significant main- tenance. Flexibility is also important to adapt customer requirements which may vary after operation experiences. A quick and reliable way to modify the soft- ware functions is required. The model-based approach will contribute to solve this problem. Mitsubishi Electric Corporation, one of the representing artificial satellite suppliers in Japan, is now implementing a General Mode Controller on on-board software of satellites. We noticed functions related to modes and se- quences are often required to modify from previous projects. The General Mode Controller can modify them in a reliable way by changing control parameters without modifying source code. The control parameters are verified by using a model-based tool, Matlab Simulink and Stateflow, to enable quick and low risk modifications.",
        "keywords": [
            "Mode controller",
            "embedded system",
            "high quality",
            "flexibility",
            "Simu- link",
            "Stateflow."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Educators Symposium at MODELS 2008",
        "date": 2008,
        "abstract": "Model-driven engineering is becoming increasingly popular in software development projects as it raises level of abstraction, thus improving our ability to handle complex systems. In many academic and industrial centers, software modeling has already been introduced into their curricula. Despite this, it seems that education does not yet support the modeling paradigm well enough thus limiting its acceptance as a mature method of developing software systems. The goal of this symposium was to ﬁnd ways to change this situation. Speciﬁcally, the symposium sought ways of showing beneﬁts of modeling in a way that is pedagogically eﬀective and attractive to the students. It also tried to make recommendations for placing the modeling courses in the overall software development educational path, which should include not only UML fundamentals but also a demonstration of the importance and place of modeling in the overall path from business (environment) to software products.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models. Models. Models. So What?",
        "date": 2009,
        "abstract": "In 1985, in an interview for some then-popular magazine, I was asked when models and model-driven development would become commonplace. ”In three years time,” I replied conﬁdently. In 1987, I was asked the same question, and my answer remained the same. And 1989. And ’91. Were you to ask me the same question today, I would answer it in the same way. Perhaps I should have gone surﬁng instead.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Modeling",
        "date": 2009,
        "abstract": "Model-driven engineering and model-based approaches have perme- ated all branches of software engineering; to the point that it seems that we are using models, as Molière’s Monsieur Jourdain was using prose, without know- ing it. At the heart of modeling, there is a relation that we establish to represent something by something else. In this paper we review various definitions of models and relations between them. Then, we define a canonical set of relations that can be used to express various kinds of representation relations and we propose a graphical concrete syntax to represent these relations. Hence, this pa- per is a contribution towards a theory of modeling.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Representation and Traversal of Large Clabject Models",
        "date": 2009,
        "abstract": "Multi-level modeling using so-called clabjects has been proposed as an alternative to UML for modeling domains that feature more than one classi- fication level. In real-world applications, however, this modeling formalism has not yet become popular, because it is a challenge to efficiently represent large models, and providing fast access to all information spread across the meta- levels at the same time. In this paper we present the model representation con- cept that relies on a permanent condensed view of the model, the corresponding traversal algorithms, and their implementations that proved adequate for model- driven engineering of industrial automation systems consisting of hundreds of thousands of model elements.",
        "keywords": [
            "Clabject",
            "Multi-Level Modeling",
            "Efficient Representation."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Meta-model Pruning⋆",
        "date": 2009,
        "abstract": "Large and complex meta-models such as those of Uml and its proﬁles are growing due to modelling and inter-operability needs of numerous stakeholders. The complexity of such meta-models has led to coining of the term meta-muddle. Individual users often exercise only a small view of a meta-muddle for tasks ranging from model creation to construction of model transformations. What is the eﬀective meta-model that represents this view? We present a ﬂexible meta-model pruning al- gorithm and tool to extract eﬀective meta-models from a meta-muddle. We use the notion of model typing for meta-models to verify that the al- gorithm generates a super-type of the large meta-model representing the meta-muddle. This implies that all programs written using the eﬀective meta-model will work for the meta-muddle hence preserving backward compatibility. All instances of the eﬀective meta-model are also instances of the meta-muddle. We illustrate how pruning the original Uml meta- model produces diﬀerent eﬀective meta-models.",
        "keywords": [
            "Meta-model pruning",
            "GPML",
            "DSML",
            "UML",
            "Kermeta",
            "eﬀec- tive modelling domain",
            "test input domain."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML/MARTE Model Analysis Method for Detection of Data Races in Concurrent Systems",
        "date": 2009,
        "abstract": "The earlier concurrency problems are identified, the less costly they are to fix. As larger, more complex concurrent systems are developed, early detection of problems is made increasingly difficult. We have developed a gen- eral approach meant to be used in the context of Model Driven Development. Our approach is based on the analysis of design models expressed in the Uni- fied Modeling Language (UML) and uses specifically designed genetic algorithms to detect concurrency problems. Our main motivation is to devise practical solutions that are applicable in the context of UML design of concur- rent systems without requiring additional modeling. All relevant concurrency information is extracted from UML models that comply with the UML Model- ing and Analysis of Real-Time and Embedded Systems (MARTE) profile. Our approach was shown to work for both deadlocks and starvation. The current pa- per addresses data race detection, further illustrating how our approach can be tailored to other concurrency issues. Results on a case study inspired from the Therac-25 radiation machine show that our approach is effective in the detec- tion of data races.",
        "keywords": [
            "MDD",
            "data races",
            "model analysis",
            "concurrent systems",
            "UML",
            "MARTE",
            "genetic algorithms."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Driven Performance Measurement and Assessment with MoDePeMART⋆",
        "date": 2009,
        "abstract": "Software performance is one of important software Quality of Service attributes. For this reason, several approaches integrate perfor- mance prediction in Model Driven Engineering(MDE). However, MDE still lacks a systematic approach for performance measurement and met- rics assessment. This paper presents MoDePeMART, an approach for Model Driven Performance Measurement and Assessment with Rela- tional Traces. The approach suggests declarative speciﬁcation of per- formance metrics in a domain speciﬁc language and usage of relational databases for storage and metric computation. The approach is eval- uated with the implementation of a UML Proﬁle for UML Class and State diagrams and transformations from proﬁle to a commercial rela- tional database management system.",
        "keywords": [
            "Software Performance Measurement and Assessment",
            "Model Driven Engineering",
            "Transformational and Reactive Systems."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Security Analysis of a Biometric Authentication System Using UMLsec and JML*",
        "date": 2009,
        "abstract": "Quality assurance for security-critical systems is particularly chal- lenging: many systems are developed, deployed, and used that do not satisfy their security requirements. A number of software engineering approaches have been developed over the last few years to address this challenge, both in the context of model-level and code-level security assurance. However, there is lit- tle experience so far in using these approaches in an industrial context, the chal- lenges and benefits involved and the relative advantages and disadvantages of different approaches. This paper reports on experiences from a practical appli- cation of two of these security assurance approaches. As a representative of model-based security analysis, we considered the UMLsec approach and we in- vestigated the JML annotation language as a representative of a code-level as- surance approach. We applied both approaches to the development and security analysis of a biometric authentication system and performed a comparative evaluation based on our experiences.",
        "keywords": [
            "Security analysis",
            "JML",
            "UMLsec",
            "biometric authentication."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatically Discovering Hidden\nTransformation Chaining Constraints",
        "date": 2009,
        "abstract": "Model transformations operate on models conforming to pre- cisely deﬁned metamodels. Consequently, it often seems relatively easy to chain them: the output of a transformation may be given as input to a second one if metamodels match. However, this simple rule has some obvious limitations. For instance, a transformation may only use a sub- set of a metamodel. Therefore, chaining transformations appropriately requires more information.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CSP(M): Constraint Satisfaction Problem over Models⋆",
        "date": 2009,
        "abstract": "Constraint satisfaction programming (CSP)has been successfully used in model-driven development (MDD) for solving a wide range of (combinatorial) problems. In CSP, declarative constraints capture restrictions over variables with ﬁnite domains where both the number of variables and their domains are required to be a priori ﬁnite. However, the existing formulation of constraint satisfaction problems can be too restrictive to support dynamically evolving domains and con- straints necessitated in many MDD applications as the graph nature of the under- lying models needs to be encoded with variables of ﬁnite domain. In the paper, we reformulate the constraint satisfaction problem directly on the model-level by using graph patterns as constraints and graph transformation rules as labeling op- erations. This allows expressing problems composed of dynamic model manipu- lation and complex graph structural constraints in an intuitive way. Furthermore, we present a prototype constraint solver for the domain of graph models built upon the VIATRA2 model transformation framework, and provide an initial evaluation of its performance.",
        "keywords": [
            "Constraint satisfaction programming",
            "graph transformation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Parsing SBVR-Based Controlled Languages⋆,⋆⋆",
        "date": 2009,
        "abstract": "Conceptual schemas (CS) are core elements of information systems knowledge. A challenging issue in the management processes is to allow decision makers, such as business people, to directly deﬁne and reﬁne their schemas using a pseudo-natural language. The recently pub- lished Semantics for Business Vocabulary and Rules (SBVR) is a good candidate for an intermediate layer: it oﬀers an abstract syntax able to express a CS, as well as a concrete syntax based on structured English. In this article, we propose an original method for extracting a SBVR ter- minal model out of a controlled English text and then transform it into a UML class diagram. We describe a model-driven engineering approach in which constraint-programming based search is combined with model transformation. The use of an advanced resolution technique (conﬁgura- tion) as an operation on models allows for non-deterministic parsing and language ﬂexibility. In addition to the theoretical results, preliminary experiments on a running example are provided.",
        "keywords": [
            "Controlled languages",
            "parsing",
            "SBVR",
            "model-driven engi- neering",
            "constraints",
            "conﬁguration",
            "model search."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SLIM—A Lightweight Environment for Synchronous Collaborative Modeling",
        "date": 2009,
        "abstract": "UML diagrams have become the de-facto standard for the vi- sual modeling of software systems. The creation and discussion of these diagrams is a critical factor impacting the quality of the artifacts un- der development. Traditionally, facilitating the collaboration of globally distributed team members with heterogeneous system environments has been a costly and time-consuming endeavor. This paper aims to advance the state-of-the-art of model-based development by providing a collabo- ration environment, which supports the synchronous distributed creation and manipulation of UML diagrams and also lowers the technical entry barriers for participating in the modeling process. We present a prototyp- ical implementation of a collaborative editor for synchronous lightweight modeling (SLIM). Applying innovative techniques, which only rely on functionality natively supported by modern web browsers, technical is- sues impeding clients to be integrated into the collaborative environment are avoided and ad hoc collaboration is facilitated.",
        "keywords": [
            "Collaborative Modeling",
            "Web 2.0",
            "Real-Time Editor."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Language-Independent Change Management of Process Models",
        "date": 2009,
        "abstract": "In model-driven development approaches, process models are used at different levels of abstraction and are described by different languages. Similar to other software artifacts, process models are developed in team environments and underlie constant change. This requires reusable techniques for the detection of changes between different process models and the computation of dependen- cies and conﬂicts between changes. In this paper, we propose a framework for the construction of process model change management solutions that provides generic techniques for the detection of differences and the computation of de- pendencies and conﬂicts between changes. The framework contains an abstract representation for process models that serves as a common denominator for dif- ferent process models. In addition, we show how the framework is instantiated exemplarily for BPMN.",
        "keywords": [
            "Process model change management",
            "process model differences."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Requirements for Practical Model Merge – An Industrial Perspective*",
        "date": 2009,
        "abstract": "All the support tools that developers are used to must be in place, if the use of model-centric development in companies has to take off. Industry deals with big models and many people working on the same model. Collabora- tion in a team inevitably leads to parallel work creating different versions that eventually will have to be merged together. However, our experience is that at present the support for model merge is far from optimal. In this paper, we put forward a number of requirements for practical merge tools, based on our analysis of literature, merge tool evaluations, interviews with developers, and a number of use cases for concurrent development of models. We found future work to do for both tool vendors and academic research. Fortunately we also uncovered a few tips and tricks that companies using model-centric develop- ment can implement on the short term while waiting for better times.",
        "keywords": [
            "Model merge",
            "diff",
            "version control",
            "parallel work",
            "team co- ordination",
            "industrial experience."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating the Impact of UML Modeling on\nSoftware Quality: An Industrial Case Study⋆",
        "date": 2009,
        "abstract": "The contribution of formal modeling approaches in software development has always been a subject of debates. The proponents of model-driven development argue that big upfront designs although re- quire substantial investment will payoﬀlater in the implementation phase in terms of increased productivity and quality. On the other hand, soft- ware engineers who are not very keen on modeling perceive the activity as simply a waste of time and money without any real contribution to the ﬁnal software product. Considering present advancement of model- based software development in software industry, we are challenged to investigate the real contribution of modeling in software development. Therefore, in this paper we report on an empirical investigation on the impact of UML modeling on the quality of software system. In particu- lar, we focus on defect density as a measure of software quality. Based on a signiﬁcant industrial case study, we have found that the use of UML modeling potentially reduces defect density in software system.",
        "keywords": [
            "UML",
            "Complexity",
            "Coupling",
            "Defect Density",
            "Case Study."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Concern Visibility in Base Station Development – An Empirical Investigation*",
        "date": 2009,
        "abstract": "Contemporary model driven development tools only partially support the abstractions occurring in complex embedded systems development. The pa- per presents an interpretive case study in which the concerns held by 7 engi- neers in a large product developing organization were compared to the concerns supported by the modeling tool in use. The paper’s main finding is an empiri- cally grounded catalogue of concerns, categorized with respect to visibility in models and other artefacts in use. In the studied case, 26% of the concerns were visible in the models, whereas 38% were visible elsewhere and 36% not visible at all. The catalogue has been presented to several stakeholders in the unit stud- ied, with positive feedback: particularly appreciated were the notion of concern visibility as indicator of degree of implementation of model driven develop- ment, and that concerns have traceable connections to experiences of the unit’s engineers.",
        "keywords": [
            "Model driven development (MDD)",
            "aspect oriented modeling  (AOM)",
            "software architecture",
            "viewpoints",
            "concerns",
            "base stations",
            "telecommu- nication systems",
            "embedded systems."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Inﬂuencing Factors in Model-Based Testing with UML State Machines: Report on an Industrial Cooperation⋆",
        "date": 2009,
        "abstract": "Automatic model-based test generation is inﬂuenced by many factors such as the test generation algorithm, the structure of the used test model, and the applied coverage criteria. In this paper, we report on an industrial cooperation for model-based testing: We used a UML state machine to generate test suites, the original system under test was not pro- vided, and we conducted mutation analysis on artiﬁcial implementations. The focus of this report is on tuning the inﬂuencing factors of the test gen- eration and showing their impact on the generated test suites. This report raises further questions, e.g. about the role of test model transformations for coverage criteria satisfaction.",
        "keywords": [
            "Model-Based Testing",
            "State Machines",
            "Coverage Criteria",
            "Mutation Analysis",
            "Industrial Cooperation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Composite Model Transformations Using Distributed Graph Transformation Concepts",
        "date": 2009,
        "abstract": "Model-based development of highly complex software systems leads to large models. Storing them in repositories offers the possibility to work with these models in a distributed environment. However, they are not modularized and thus, do not especially support distributed development. An alternative is to con- sider composite models such that several teams can work largely independently. In this paper, we consider a general approach to composite models and their trans- formation based on graph transformation concepts. To illustrate this approach, we present a concrete setting for composite models based on the Eclipse Modeling Framework (EMF). EMF models can be distributed over several sites. While re- mote references can express import relations, export and import interfaces are not explicitly deﬁned. In our approach, we sketch composite models with explicit and implicit interfaces using concepts of distributed graph transformation and outline different kinds of composite model transformations.",
        "keywords": [
            "Distributed modeling",
            "graph transformation",
            "Eclipse."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On-the-Fly Construction, Correctness and Completeness of Model Transformations Based on Triple Graph Grammars",
        "date": 2009,
        "abstract": "Triple graph grammars (TGGs) are a formal and intuitive concept for the speciﬁcation of model transformations. Their main ad- vantage is an automatic derivation of operational rules for bidirectional model transformations, which simpliﬁes speciﬁcation and enhances us- ability as well as consistency.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal Support for QVT-Relations with Coloured Petri Nets",
        "date": 2009,
        "abstract": "QVT is the OMG standard language for specifying model- to-model transformations in MDA. Even though it plays a crucial role in model driven development, there are scarce tools supporting the ex- ecution of its sublanguage QVT-Relations, and none for its analysis or veriﬁcation. In order to alleviate this situation, this paper provides a for- mal semantics for QVT-Relations through its compilation into Coloured Petri nets, enabling the execution and validation of QVT speciﬁcations. The theory of Petri nets provides useful techniques to analyse trans- formations (e.g. reachability, model-checking, boundedness and invari- ants) and to determine their conﬂuence and termination given a starting model. We also report on using CPNTools for the execution, debugging, and analysis of transformations, and on a tool chain to transform QVT- Relations speciﬁcations into the input format of CPNTools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-Relations"
        }
    },
    {
        "title": "An Example Is Worth a Thousand Words:\nComposite Operation Modeling By-Example⋆",
        "date": 2009,
        "abstract": "Predeﬁned composite operations are handy for eﬃcient mod- eling, e.g., for the automatic execution of refactorings, and for the in- troduction of patterns in existing models. Some modeling environments provide an initial set of basic refactoring operations, but hardly oﬀer any extension points for the user. Even if extension points exist, the intro- duction of new composite operations requires programming skills and deep knowledge of the respective metamodel.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Refactoring-Safe Modeling of Aspect-Oriented Scenarios",
        "date": 2009,
        "abstract": "Aspects use pointcut expressions to specify patterns that are matched against a base model, hence defining the base locations to which aspects are ap- plied. The fragile pointcut problem is well-known in aspect-oriented modeling, as small changes in the base may lead to non-matching patterns. Consequently, aspects are not applied as desired. This is especially problematic for refactoring. Even though the meaning of the model has not changed, pointcut expressions may no longer match. We present an aspect-oriented modeling technique for scenarios that is refactoring-safe. The scenarios are modeled with Aspect- oriented Use Case Maps (AoUCM), an extension of the recent ITU standard User Requirements Notation. AoUCM takes the semantics of the modeling no- tation into account, thus ensuring pointcut expressions still match even after, for example, refactoring a single use case map into several hierarchical maps. Fur- thermore, AoUCM allows the composed model to be viewed without having to resolve complex layout issues. The general principles of our approach are also applicable to other aspect-oriented modeling notations.",
        "keywords": [
            "Aspects-oriented Modeling",
            "User Requirements Notation",
            "Aspect- oriented Use Case Maps."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Based Testing Using LSCs and S2A⋆,⋆⋆",
        "date": 2009,
        "abstract": "We report on our preliminary experience in using high-level visual scenario-based models for tests speciﬁcation, test generation, and aspect-based test execution, in the context of an industrial application. To specify scenario- based tests, we used a UML2-compliant variant of live sequence charts (LSC). To automatically generate testing code from the models, we used a modiﬁed ver- sion of the S2A Compiler, outputting AspectC++ code. Finally, to examine the results of the tests, we used the Tracer, a prototype tool for model-based trace visualization and exploration. Our experience reveals the advantages of integrat- ing models into industrial settings, speciﬁcally for model-based test speciﬁcation and aspect-based execution: generating aspect code from visual models enables exploiting the expressive power of aspects for testing without manual coding and without knowledge of their rather complex syntax and semantics. We further dis- cuss technological and other barriers for the future successful integration of our initial work in industrial context.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Driven Development of Graphical User  \nInterfaces for Enterprise Business Applications – \nExperience, Lessons Learnt and a Way Forward*",
        "date": 2009,
        "abstract": "We discuss our experience in applying model-driven techniques to build Graphical User Interfaces (GUI) of large enterprise business applications. Our approach involves capturing various user interface patterns in the form of platform independent parameterized templates and instantiating them with rele- vant application data, serving as the template arguments. Models thus instantiated are translated to platform specific GUI implementation artifacts by a set of tem- plate-specific code generators. We describe this approach in detail and share our experiences and the lessons learnt from using the approach in developing large da- tabase-centric business applications for the past fourteen years. Our ongoing work to address some of the limitations of this approach, especially on variability man- agement of GUI in software product lines, is also presented in brief.",
        "keywords": [
            "Modeling",
            "Graphical User Interfaces",
            "Meta Modeling",
            "Code   Generation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Business Process Models as a Showcase for\nSyntax-Based Assistance in Diagram Editors",
        "date": 2009,
        "abstract": "Recently, a generic approach for syntax-based user assistance in diagram editors has been proposed that requires the syntax of the vi- sual language to be deﬁned by a graph grammar. The present paper describes how this approach can be applied to the language of business process models (BPMs), which is widely used nowadays. The resulting BPM editor provides the following assistance features: combination or completion of BPM fragments, generation of BPM examples, an exten- sive set of correctness-preserving editing operations for BPMs, and auto- link, i.e., the automatic connection of activities by sequence ﬂow.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Rule-Enhanced Business Process Modeling Language for Service Choreographies",
        "date": 2009,
        "abstract": "To address problem of modeling service choreographies, the paper tackles the following challenges of the state of the art in choreography model- ing: i) choreography models are not well-connected with the underlying busi- ness vocabulary models. ii) there is limited support for decoupling parts of business logic from complete choreography models. This reduces dynamic changes of choreographies; iii) choreography models contain redundant ele- ments of shared business logic, which might lead to an inconsistent implemen- tation and incompatible behavior. Our proposal – rBPMN – is an extension of a business process modeling language with rule and choreography modeling sup- port. rBPMN is defined by weaving the metamodels of the Business Process Modeling Notation (BPMN) and REWERSE Rule Markup Language (R2ML).",
        "keywords": [
            "BPMN",
            "R2ML",
            "rules",
            "processes",
            "metamodels",
            "MDE."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "R2ML"
        }
    },
    {
        "title": "Change-Driven Model Transformations⋆ Derivation and Processing of Change Histories",
        "date": 2009,
        "abstract": "Nowadays, evolving models are prime artefacts of model-driven soft- ware engineering. In tool integration scenarios, a multitude of tools and modeling languages are used where complex model transformations need to incrementally synchronize various models residing within different external tools. In the pa- per, we investigate a novel class of transformations, that are directly triggered by model changes. First, model changes in the source model are recorded incremen- tally by a change history model. Then a model-to-model transformation is carried out to generate a change model for the target language. Finally, the target change history model is processed (at any time) to incrementally update the target model itself. Moreover, our technique also allows incremental updates in an external model where only the model manipulation interface is under our control (but not the model itself). Our approach is implemented within the VIATRA2 framework, and it builds on live transformations and incremental pattern matching.",
        "keywords": [
            "Incremental model transformation",
            "change models",
            "change-driven transformations."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Incremental Algorithm for\nHigh-Performance Runtime Model Consistency⋆",
        "date": 2009,
        "abstract": "We present a novel technique for applying two-level runtime models to distributed systems. Our approach uses graph rewriting rules to transform a high-level source model into one of many possible target models. When either model is changed at runtime, the transformation is incrementally updated. We describe the theory underlying our approach, and show restrictions suﬃcient for a simple and eﬃcient implementation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Traceability-Based Change Awareness",
        "date": 2009,
        "abstract": "Many tools in software engineering projects support the visualization and collaborative modification of custom sets of artifacts. This includes tools for requirements engineering, UML tools for design, project management tools, developer tools and many more. A key factor for success in software engineer- ing projects is the collective understanding of changes applied to these artifacts. To support this, there are several strategies to automatically notify project par- ticipants about relevant changes. Known strategies are limited to a fixed set of artifacts and/or make no use of traceability information to supply change notifi- cations. This paper proposes a change notification approach based on traceabil- ity in a unified model and building upon operation-based change tracking. The unified model explicitly combines system specification models and project management models into one fully traceable model. To show the benefit of our approach we compare it to related approaches in a case study.",
        "keywords": [
            "Change awareness",
            "traceability",
            "unified model",
            "operation-based",
            "notification."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Interaction Design and Model-Driven Development",
        "date": 2009,
        "abstract": "Throughout the evolution of software development and soft- ware engineering methods, human interaction and the interfaces that support it have been too often ignored or treated as secondary con- cerns. Most modern modeling languages and methods - UML and the uniﬁed process most deﬁnitely among them - have been devised with a highly focused concern for representing procedures, information, and software structures. The needs of interaction design and designers have been addressed, if at all, in afterthought. Instead of well-conceived no- tations and techniques, interaction designers have been given awkward adaptations of models conceived for completely diﬀerent and largely in- compatible purposes. Instead of placing users and use at the center of developmental and methodological focus, the dominant modeling lan- guages and methods have relegated them to the periphery. Despite noble calls for rapprochement and valiant attempts to build bridges, the gap between software engineering on the one side and human-computer in- teraction on the other remains stubbornly deep and wide, ﬁlled with misunderstanding and devoid of meaningful integration.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Test-Driven Semantics Speciﬁcation",
        "date": 2009,
        "abstract": "Behavioral models are getting more and more important within the software development cycle. To get the most use out of them, their behavior should be deﬁned formally. As a result, many approaches exist which aim at specifying formal semantics for behavioral languages (e.g., Dynamic Meta Modeling (DMM), Semantic Anchoring). Most of these approaches give rise to a formal semantics which can e.g. be used to check the quality of a particular language instance, for instance using model checking techniques.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scalable Semantic Annotation Using Lattice-Based Ontologies⋆",
        "date": 2009,
        "abstract": "Including semantic information in models helps to expose modeling errors early in the design process, engage a designer in a deeper understanding of the model, and standardize concepts and terminology across a development team. It is impractical, however, for model builders to manually annotate every modeling element with semantic properties. This paper demonstrates a correct, scalable and automated method to infer semantic properties using lattice-based ontologies, given relatively few manual annotations. Semantic concepts and their relationships are formalized as a lattice, and relationships within and between components are expressed as a set of constraints and acceptance criteria relative to the lattice. Our inference engine automatically infers properties wher- ever they are not explicitly speciﬁed. Our implementation leverages the infrastructure in the Ptolemy II type system to get eﬃcient and scalable inference and consistency checking. We demonstrate the approach on a non-trivial Ptolemy II model of an adaptive cruise control system.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OntoDSL: An Ontology-Based Framework for Domain-Speciﬁc Languages",
        "date": 2009,
        "abstract": "Domain-speciﬁc languages (DSLs) are high-level and should provide abstractions and notations for better understanding and easier modeling of applications of a special domain. Current shortcomings of DSLs include learning curve and formal semantics. This paper reports on a novel approach that allows the use of ontologies to describe DSLs. The formal semantics of OWL together with reasoning services allow for addressing constraint deﬁnition, progressive evaluation, suggestions, and debugging. The approach integrates existing metamodels, concrete syntaxes and a query language. A scenario in which domain models for network devices are created illustrates the development environment.",
        "keywords": [
            "Domain-Speciﬁc Languages",
            "Technical Space",
            "Ontologies",
            "Reasoning Services."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain-Speciﬁc Languages in Practice: A User Study on the Success Factors⋆",
        "date": 2009,
        "abstract": "In this paper we present an empirical study on the use of a domain- speciﬁc language(DSL) in industry. This DSL encapsulates the details of services that communicate using Windows Communication Foundation (WCF). From def- initions of the data contracts between clients and servers, WCF/C# code for ser- vice plumbing is generated. We conducted a survey amongst developers that use this DSL while developing applications for customers. The DSL has been used in about 30 projects all around the world.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating Context Descriptions and Property  \nDefinition Patterns for Software Formal Validation*",
        "date": 2009,
        "abstract": "A well known challenge in the formal methods domain is to improve their integration with practical engineering methods. In the context of embed- ded systems, model checking requires first to model the system to be validated, then to formalize the properties to be satisfied, and finally to describe the be- havior of the environment. This last point which we name as the proof context is often neglected. It could, however, be of great importance in order to reduce the complexity of the proof. The question is then how to formalize such a proof context. We experiment a language, named CDL (Context Description Lan- guage), for describing a system environment using actors and sequence dia- grams, together with the properties to be checked. The properties are specified with textual patterns and attached to specific regions in the context. Our contri- bution is a report on several industrial embedded system applications.",
        "keywords": [
            "Formal methods",
            "context description",
            "property patterns",
            "observers",
            "timed automata",
            "model checking."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Anatomy of a Visual Domain-Specific Language Project in an Industrial Context*",
        "date": 2009,
        "abstract": "Domain-specific languages (DSL) are specialized modeling lan- guages targeting a narrow domain. In this paper, we present the results of a re- search project on visual DSLs set in an industrial context, using the domain of elevator controllers. After domain analysis and inception of new, abstract mod- eling concepts a language prototype was developed, considering aspects such as usability, combination of visual and textual DSLs, and performance of gener- ated code. We describe the challenges encountered during the project, such as defining a user-friendly concrete syntax or tool limitations, and analyze them in retrospective. The paper concludes with several metrics to support the findings.",
        "keywords": [
            "Domain-specific language",
            "visual",
            "textual",
            "code generation."
        ],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A Goal-Based Modeling Approach to Develop Requirements of an Adaptive System with Environmental Uncertainty⋆",
        "date": 2009,
        "abstract": "Dynamically adaptive systems (DASs) are intended to mon- itor the execution environment and then dynamically adapt their behav- ior in response to changing environmental conditions. The uncertainty of the execution environment is a major motivation for dynamic adap- tation; it is impossible to know at development time all of the possible combinations of environmental conditions that will be encountered. To date, the work performed in requirements engineering for a DAS includes requirements monitoring and reasoning about the correctness of adap- tations, where the DAS requirements are assumed to exist. This paper introduces a goal-based modeling approach to develop the requirements for a DAS, while explicitly factoring uncertainty into the process and resulting requirements. We introduce a variation of threat modeling to identify sources of uncertainty and demonstrate how the RELAX speciﬁ- cation language can be used to specify more ﬂexible requirements within a goal model to handle the uncertainty.",
        "keywords": [
            "Requirements engineering",
            "goal models",
            "uncertainty",
            "dynam- ically adaptive systems."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Use Case Modeling Approach to Facilitate the Transition towards Analysis Models: Concepts and Empirical Evaluation",
        "date": 2009,
        "abstract": "Use case modeling (UCM) is commonly applied to document re- quirements. Use case specifications (UCSs) are usually structured, unrestricted textual documents complying with a certain template. However, because they remain essentially textual, ambiguities are inevitable. In this paper, we propose a new UCM approach, which is composed of a set of well-defined restriction rules and a new template. The goal is to reduce ambiguity and facilitate auto- mated analysis, though the later point is not addressed in this paper. We also re- port on a controlled experiment which evaluates our approach in terms of its ease of application and the quality of the analysis models derived by trained in- dividuals. Results show that the restriction rules are overall easy to apply and that our approach results in significant improvements over UCM using a stan- dard template and no restrictions in UCSs, in terms of the correctness of derived class diagrams and the understandability of UCSs.",
        "keywords": [
            "Use Case",
            "Use Case Modeling",
            "Use Case Template",
            "Restriction  Rules",
            "Analysis Model",
            "Controlled Experiment."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Polymorphic Scenario-Based Speciﬁcation Models: Semantics and Applications⋆",
        "date": 2009,
        "abstract": "We present polymorphic scenarios, a generalization of a UML2-compliant variant of Damm and Harel’s live sequence charts (LSC) in the context of object-orientation. Polymorphic scenarios are visualized using (modal) sequence diagrams where lifelines may represent classes and interfaces rather than concrete objects. Their semantics takes ad- vantage of inheritance and interface realization to allow the speciﬁcation of most expressive, succinct, and reusable universal and existential inter- object scenarios for object-oriented system models. We motivate the use of polymorphic scenarios, formally deﬁne their trace-based semantics, and present their application for scenario-based testing and execution, as implemented in the S2A compiler developed in our group.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Aspect Model Unweaving⋆",
        "date": 2009,
        "abstract": "Since software systems need to be continuously available, their ability to evolve at runtime is a key issue. The emergence of mod- els@runtime, combined with Aspect-Oriented Modeling techniques, is a promising approach to tame the complexity of adaptive systems. How- ever, with no support for aspect unweaving, these approaches are not ag- ile enough in an adaptive system context. In case of small modiﬁcations, the adapted model has to be generated by again weaving all the aspects, even those unchanged. This paper shows how aspects can be unwoven, based on a precise traceability metamodel dedicated to aspect model weaving. We analyze traceability models, which describe how aspects were woven into a base, to determine the extent to which an aspect has aﬀected the woven model in order to determine how it can be unwoven. Aspect unweaving is ﬁnally performed by applying inverse operations of a sub-sequence of the weaving operations in opposite order.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Composition Contracts",
        "date": 2009,
        "abstract": "The state-of-the-art in aspect-oriented programming and modeling provides ﬂexible querying and composition mechanisms that allow virtually un- restricted modiﬁcations to base code or models using static or dynamic weaving. There is, however, a lack of support for specifying and controlling the permit- ted effects of compositions with respect to the base models involved. We present model composition contracts, which govern access to the base models via aspects; in essence, the contracts control how aspect compositions may or may not access and change the models, or the underlying code reﬂected by models. The compo- sition contracts deﬁne constraints in terms of pre- and post-conditions restricting the eligibility for composition. We argue that composition contracts improve re- liability of model composition in software engineering, and evaluate their effects on model designs and implementations using a case study. We support the app- roach with a prototype tool for specifying and checking contracts.",
        "keywords": [
            "Model composition",
            "designbycontract",
            "aspect-oriented development."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Abstracting Complex Languages through Transformation and Composition⋆",
        "date": 2009,
        "abstract": "Domain-speciﬁc languages (DSLs) can simplify the develop- ment of complex software systems by providing domain-speciﬁc abstrac- tions. However, the complexity of some domains has led to a number of DSLs that are themselves complex, limiting the original beneﬁts of using DSLs. We show how to develop DSLs as abstractions of other DSLs by transfering translational approaches for textual DSLs into the domain of modelling languages. We argue that existing model transformation languages are at too low a level of abstraction for succinctly expressing transformations between abstract and concrete DSLs. Patterns identi- ﬁed in such model transformations can be used to raise the level of ab- straction. We show how we can allow part of the transformation to be expressed using the concrete syntax of the concrete DSL.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "None"
        }
    },
    {
        "title": "An Approach for Evolving Transformation Chains⋆",
        "date": 2009,
        "abstract": "A transformation chain (TC) generates applications from high-level models that are deﬁned in terms of problem domain concepts. The result is a low-level model that is rooted in the solution domain. The evolution of a TC is a complex and expensive endeavor since there are intricate dependencies between all its constituent parts. More speciﬁc, an evolution problem arises when we need to add an unanticipated con- cern (e.g., security) that does not ﬁt the expressiveness of the high-level metamodel, because such an addition forces us to adapt existing assets (i.e., metamodels, models, and transformations). We present a solution that adds a new concern model to the TC, in an independent way.",
        "keywords": [
            "Model Driven Engineering",
            "Model transformation",
            "Model composition."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deterministic UML Models for Interconnected Activities and State Machines",
        "date": 2009,
        "abstract": "The interconnection between UML activities and state ma- chines enables the comprehensible modeling of systems based on data ﬂows and events. In this paper, we propose a novel approach to guarantee a deterministic behavior for models in which activity and state diagrams work together. At ﬁrst, deterministic models are ensured independently within both diagrams by using our UML proﬁle for Deterministic Mod- els for signal processing embedded systems (DMOSES). The relationship between executions of the model elements is analyzed according to in- terconnections of the activity and state diagrams described in the UML standard. To avoid nondeterministic models, we deﬁne the execution be- havior of cooperating activities and state machines. The interconnection of both diagrams and their corresponding behavior are illustrated in an embedded system example that uses parallel processing for data as well as for events. Our approach simpliﬁes the development of deterministic embedded systems by code generation from UML models.",
        "keywords": [
            "Activity",
            "state machine",
            "deterministic behavior."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Encapsulation of UML Activities for Incremental Development and Veriﬁcation",
        "date": 2009,
        "abstract": "With their revision in the UML 2.x standard, activities have been extended with streaming parameters. This facilitates a reuse-orien- ted speciﬁcation style, in which dedicated functions can be contributed by self-contained activities as building blocks: Using streaming param- eters, activities can be composed together in a quite powerful manner, since streaming parameters may also pass information while activities are executing. However, to compose them correctly, we must know in which sequence an activity may emit or accept these streaming parame- ters. Therefore, we propose special UML state machines that specify the externally visible behavior of activities. Further, we develop an algorithm to construct these state machines automatically for an activity based on model checking. Using these behavioral contracts, activities can then be composed without looking at their internal details. Moreover, the con- tracts can be used during system veriﬁcation to reduce the complexity of the analysis.",
        "keywords": [
            "System Composition",
            "UML Activities",
            "UML State Machines",
            "UML Streaming Parameters",
            "Model Reuse",
            "Veriﬁcation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using UML Statecharts with Knowledge Logic Guards⋆",
        "date": 2009,
        "abstract": "This paper describes an extension of UML statecharts, called K-statechart, suitable for the formal speciﬁcation, modeling, and runtime veriﬁcation of system behavior that depends on knowledge and belief in distributed multi-agent systems. With K-statecharts, statechart transi- tion guards allow the use of knowledge-logic formulae, a form of modal logic used for reasoning about multi-agent systems. We demonstrate the proposed formalism using an example of a multi-agent system that con- sists of three traﬃc-light controllers. We also describe a newly developed K-statechart code generator that is part of the StateRover Eclipse-IDE plug-in for statechart-based modeling and formal speciﬁcation.",
        "keywords": [
            "K-statechart",
            "knowledge-logic",
            "adaptive behavior",
            "formal speciﬁcation",
            "runtime veriﬁcation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Modeling Language for Activity-Oriented Composition of Service-Oriented Software Systems",
        "date": 2009,
        "abstract": "The proliferation of smart spaces and emergence of new standards, such as Web Services, have paved the way for a new breed of software systems. Often the complete functional and QoS requirements of such software systems are not known a priori at design-time, and even if they are, they may change at run-time. Unfortunately, the majority of existing software engineering tech- niques rely heavily on human reasoning and manual intervention, making them inapplicable for automatic composition of such software systems at run-time. Moreover, these approaches are primarily intended to be used by technically knowledgeable software engineers, as opposed to domain users. In this paper, we present Service Activity Schemas (SAS), an activity-oriented language for modeling software system’s functional and QoS requirements. SAS targets ser- vice-oriented software systems, and relies on an ontology to provide domain experts with modeling constructs that are intuitively understood. SAS forms the centerpiece of a framework intended for user-driven composition and adapta- tion of service-oriented software systems in a pervasive setting. We provide a detailed description of SAS in the context of a case study and formally specify its structural and dynamic properties.",
        "keywords": [
            "Requirements Modeling",
            "Domain Specific Modeling Languages",
            "Model Driven Development",
            "Autonomic Computing",
            "Pervasive Systems."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Domain Specific Modeling Language Supporting Specification, Simulation and Execution of Dynamic Adaptive Systems*",
        "date": 2009,
        "abstract": "Constructing and executing distributed systems that can automati- cally adapt to the dynamic changes of the environment are highly complex tasks. Non-trivial challenges include provisioning of efficient design time and run time representations, system validation to ensure safe adaptation of interde- pendent components, and scalable solutions to cope with the possible combina- torial explosions of adaptive system artifacts such as configurations, variant dependencies and adaptation rules. These are all challenges where current ap- proaches offer only partial solutions. Furthermore, in current approaches the adaptation logic is typically specified at the code level, tightly coupled with the main system functionality, making it hard to control and maintain. This paper presents a domain specific modeling language (DSML) allowing specification of the adaptation logic at the model level, and separation of the adaptation logic from the main system functionality. It supports model-checking and design- time simulation for early validation of adaptation policies. The model level specifications are used to generate the adaptation logic. The DSML also pro- vides indirection mechanisms to cope with combinatorial explosions of adaptive system artifacts. The proposed approach has been implemented and validated through case studies.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Executable Domain Speciﬁc Language for Message-Based System Integration",
        "date": 2009,
        "abstract": "Heterogeneous IT-systems rarely rely on a common data for- mat and structure, so in order to integrate them, the corresponding data/message transformations must be developed. Transformations may also be required by the business logic. We present a platform-independent approach for message transformation speciﬁcation, in form of a system integration DSL, and discuss approaches for making it executable.",
        "keywords": [
            "System integration",
            "domain speciﬁc language",
            "model execu- tion."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Architectural Mining: The Other Side of the MDD",
        "date": 2009,
        "abstract": "A back-of-the-envelope calculation suggests that - very, very conservatively - the world produces well over 33 billion lines of new or modiﬁed code every year. Curiously, the moment that code springs into being and is made manifest in a running system, it become legacy. The re- lentless accretion of code over months, years, even decades quickly turns every successful new project into a brownﬁeld one. Although software has no mass, it does have weight, weight that can ossify any system by creating intertia to change and deadly creeping complexity. It requires energy to make such a system simple, and to intentionally apply that en- ergy requires that one be able to reason about, understand, and visualize the system as built.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Generic Model Refactorings⋆",
        "date": 2009,
        "abstract": "Many modeling languages share some common concepts and principles. For example, Java, MOF, and UML share some aspects of the concepts of classes, methods, attributes, and inheritance. However, model transformations such as refactorings speciﬁed for a given language cannot be readily reused for another language because their related meta- models may be structurally diﬀerent. Our aim is to enable a ﬂexible reuse of model transformations across various metamodels. Thus, in this pa- per, we present an approach allowing the speciﬁcation of generic model transformations, in particular refactorings, so that they can be applied to diﬀerent metamodels. Our approach relies on two mechanisms: (1) an adaptation based mainly on the weaving of aspects; (2) the notion of model typing, an extension of object typing in the model-oriented con- text. We validated our approach by performing some experiments that consisted of specifying three well known refactorings (Encapsulate Field, Move Method, and Pull Up Method) and applying each of them onto three diﬀerent metamodels (Java, MOF, and UML).",
        "keywords": [
            "Adaptation",
            "Aspect Weaving",
            "Genericity",
            "Model Typing",
            "Refactoring."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Constraining Type Parameters of UML 2 Templates with Substitutable Classiﬁers",
        "date": 2009,
        "abstract": "Generic programming is a ﬁeld of computer science which consists in deﬁning abstract and reusable representations of eﬃcient data structures and algorithms. In popular imperative languages, it is usually supported by a template-like notation, where generic elements are represented by templates exposing formal parameters. Deﬁning such generic artifacts may require deﬁning constraints on the actual types that can be provided in a particular substitution. UML 2 templates support two mechanisms for expressing such constraints. Unfortunately, the UML speciﬁcation provides very few details on their usage. The purpose of our article is to provide such details with regard to one of these constraining mechanisms (namely, \"substitutable constraining classiﬁers\") as well as modeling patterns inspired by practices from generic programming.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating Assertion Code from OCL:\nA Transformational Approach Based on\nSimilarities of Implementation Languages",
        "date": 2009,
        "abstract": "The Object Constraint Language (OCL) carries a platform independent characteristic allowing it to be decoupled from implementa- tion details, and therefore it is widely applied in model transformations used by model-driven development techniques. However, OCL can be found tremendously useful in the implementation phase aiding assertion code generation and allowing system veriﬁcation. Yet, taking full advan- tage of OCL without destroying its platform independence is a diﬃcult task. This paper proposes an approach for generating assertion code from OCL constraints by using a model transformation technique to abstract language speciﬁc details away from OCL high-level concepts, showing wide applicability of model transformation techniques. We take advan- tage of structural similarities of implementation languages to describe a rewriting framework, which is used to easily and ﬂexibly reformulate OCL constraints into any target language, making them executable on any platform. A tool is implemented to demonstrate the eﬀectiveness of this approach.",
        "keywords": [
            "OCL",
            "constraints",
            "assertion code",
            "programming languages."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OCLLib, OCLUnit, OCLDoc: Pragmatic\nExtensions for the Object Constraint Language⋆",
        "date": 2009,
        "abstract": "The usage of the Uniﬁed Modeling Language in the indus- trial context becomes increasingly popular. There is an agreement in academia that the Object Constraint Language (OCL) is suitable for deﬁning model constraints and queries. However, it has not yet been broadly adopted by practitioners because they ﬁnd it diﬃcult to deﬁne OCL expressions. Thus, simpliﬁcation is desirable to increase the use of OCL in practice. We propose OCL libraries (OCLLib), which simplify the development of OCL expressions and enable a high reuse factor, are conﬁgurable, testable (OCLUnit) and documented (OCLDoc). In this paper we present the underlying concepts related to OCL library devel- opment we used in UML speciﬁc and domain speciﬁc projects conducted in academic and industrial contexts, respectively.",
        "keywords": [
            "Systematic development of OCL",
            "OCL libraries",
            "OCL test- ing",
            "OCL documentation."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Variability within\nModeling Language Deﬁnitions",
        "date": 2009,
        "abstract": "We present a taxonomy of the variability mechanisms of- fered by modeling languages. The deﬁnition of a formal language en- compasses a syntax and a semantic domain as well as the mapping that relates them, thus language variabilities are classiﬁed according to which of those three pillars they address. This work furthermore proposes a framework to explicitly document and manage the variation points and their corresponding variants of a variable modeling language. The frame- work enables the systematic study of various kinds of variabilities and their interdependencies. Moreover, it allows a methodical customization of a language, for example, to a given application domain. The taxon- omy of variability is explicitly of interest for the UML to provide a more precise understanding of its variation points.",
        "keywords": [
            "Modeling languages",
            "variability",
            "formal semantics",
            "UML."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Variability Modelling throughout the Product Line Lifecycle*",
        "date": 2009,
        "abstract": "This paper summarizes our experience with introducing feature mod- elling into several product lines within Siemens. Feature models are used for solving various tasks in the product line lifecycle, starting with scoping the re- usable asset base up to support for actual product configuration. Using feature models as primary artefacts for managing variability early in the lifecycle, we could improve the efficiency and transparency of scoping activities considera- bly and made the development efforts way easier to schedule. On the other end of the lifecycle, feature models lowered the engineering efforts in solution busi- ness in supporting product configuration and instantiation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Weaving Variability into Domain Metamodels⋆",
        "date": 2009,
        "abstract": "Domain-Speciﬁc Modeling Languages (DSMLs) describe the concepts of a particular domain and their relationships, in a metamodel. From a given DSML, it is possible to describe a wide range of diﬀer- ent models. These models often share a common base and vary on some parts. Current approaches tend to distinguish the variability language from the DSMLs themselves, implying greater learning curve for DSMLs stakeholders and a signiﬁcant overhead in product line engineering of DSMLs. We propose to consider variability as an independent aspect to be woven into the DSML to introduce variability capabilities. In partic- ular we detail how variability is woven and how to perform product line derivation. We validate our approach through the weaving of variabil- ity into two very diﬀerent metamodels: Ecore and SmartAdapter, our Aspect-Oriented modeling weaver, thus adding ﬂexibility in the weaving process itself. These results emphasize how new abilities of the language can be provided by this means.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic Domain Model Migration to Manage Metamodel Evolution",
        "date": 2009,
        "abstract": "Metamodel evolution is a signiﬁcant problem in domain spe- ciﬁc software development for several reasons. Domain-speciﬁc modeling languages (DSMLs) are likely to evolve much more frequently than pro- gramming languages and commonly used software formalisms, often re- sulting in a large number of valuable instance models that are no longer compliant with the metamodel. In this paper, we present the Model Change Language (MCL), aimed at satisfying these requirements.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Model Change Language (MCL)"
        }
    },
    {
        "title": "Model Transformation by Demonstration",
        "date": 2009,
        "abstract": "Model transformations provide a powerful capability to automate model refinements. However, the use of model transformation languages may present challenges to those who are unfamiliar with a specific transformation language. This paper presents an approach called model transformation by demonstration (MTBD), which allows an end-user to demonstrate the exact transformation desired by actually editing a source model and demonstrating the changes that evolve to a target model. An inference engine built into the un- derlying modeling tool records all editing operations and infers a transformation pattern, which can be reused in other models. The paper motivates the need for the approach and discusses the technical contributions of MTBD. A case study with several sample inferred transformations serves as a concrete example of the benefits of MTBD.",
        "keywords": [
            "Model transformation",
            "Program inference",
            "Refactoring."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reviving QVT Relations: Model-Based Debugging Using Colored Petri Nets⋆",
        "date": 2009,
        "abstract": "The standardized QVT Relations language, one cornerstone of Model-Driven Architecture (MDA), has not yet gained widespread use in practice, not least due to missing tool support in general and inadequate debugging support in particular. Transformation engines in- terpreting QVT Relations operate on a low level of abstraction, hide the operational semantics of a transformation and scatter metamodels, models, QVT code, and traces across diﬀerent artifacts. We propose a model-based debugger representing QVT Relations on bases of TROPIC, a model transformation framework which utilizes a variant of Colored Petri Nets (CPNs) providing an explicit runtime model and a homoge- nous view on all artifacts of a transformation.",
        "keywords": [
            "QVT Relations",
            "Debugging",
            "Model Transformations",
            "CPN."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT Relations"
        }
    },
    {
        "title": "Incremental Development of Model Transformation Chains Using Automated Testing⋆",
        "date": 2009,
        "abstract": "Model transformations are a key technique in model-driven engi- neering. If several transformations are composed into a model transformation chain, an approach is needed that allows software engineers to incrementally improve the quality of the model transformation chain. In this paper, we propose incremental development of model transformation chains based on automated testing. We present four test design techniques and a test framework architecture for testing transformation chains and report on the validation of our approach when developing a transformation chain for model version management.",
        "keywords": [
            "Model transformation",
            "automated testing."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Test-Driven Development of Model Transformations⋆",
        "date": 2009,
        "abstract": "Model transformations enable the automated development paradigm proposed by Model Driven Engineering. However, since the requirements for building a model transformation are usually expressed informally, requirements descriptions are diﬃcult to keep updated and synchronized with their corresponding implementations. Therefore, hu- man eﬀort is usually required for validating model transformations. The present work deﬁnes a test-driven method for the development process of model-to-model transformations. This method is focused on the capture of requirements for transformations in such a way that guides the devel- opment and the documentation of model transformations. Requirements are expressed by means of test cases that can be automatically validated. The proposal has been applied to the MOSKitt open source CASE tool in an industrial scenario.",
        "keywords": [
            "Model-to-model transformations",
            "test-driven development."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Uniﬁed Approach to Modeling and Programming",
        "date": 2010,
        "abstract": "SIMULA was a language for modeling and programming and provided a uniﬁed approach to modeling and programming in contrast to methodologies based on structured analysis and design. The current development seems to be going in the direction of separation of modeling and programming. The goal of this paper is to go back to the future and get inspiration from SIMULA and propose a uniﬁed approach. In addi- tion to reintroducing the contributions of SIMULA and the Scandina- vian approach to object-oriented programming, we do this by discussing a number of issues in modeling and programming and argue1 why we consider a uniﬁed approach to be an advantage.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generic Meta-modelling with Concepts,\nTemplates and Mixin Layers",
        "date": 2010,
        "abstract": "Meta-modelling is a key technique in Model Driven Engi- neering, where it is used for language engineering and domain modelling. However, mainstream approaches like the OMG’s Meta-Object Facility provide little support for abstraction, modularity, reusability and ex- tendibility of (meta-)models, behaviours and transformations. In order to alleviate this weakness, we bring three elements of generic programming into meta-modelling: concepts, templates and mixin layers. Concepts permit an additional typing for models, enabling the deﬁni- tion of behaviours and transformations independently of meta-models, making speciﬁcations reusable. Templates use concepts to express re- quirements on their generic parameters, and are applicable to models and meta-models. Finally, we deﬁne functional layers by means of meta- model mixins which can extend other meta-models. As a proof of concept we also report on MetaDepth, a multi-level meta-modelling framework that implements these ideas.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Observer-Based Notion of Model Inheritance",
        "date": 2010,
        "abstract": "A model-based engineering discipline presupposes that models are or- ganised by creating relationships between them. While there has been consider- able work on understanding what it means to instantiate one model from another, little is known about when a model should be considered to be a specialisation of another one. This paper motivates and discusses ways of deﬁning specialisa- tion relationships between models, languages, and transformations respectively. Several alternatives of deﬁning a specialisation relationship are considered and discussed. The paper’s main contribution is the introduction of the notions of an observer and a context in order to deﬁne and validate specialisation relationships. The ideas and discussions presented in this paper are meant to provide a stepping stone towards a systematic basis for organising models.",
        "keywords": [
            "model inheritance",
            "model compatibility",
            "language engineering",
            "model evolution."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MDE-Based Approach for Generalizing Design Space Exploration",
        "date": 2010,
        "abstract": "Design Space Exploration (DSE) is the exploration of de- sign alternatives before the implementation. Existing DSE frameworks are domain-speciﬁc where the representation, evaluation method as well as exploration algorithm are tightly coupled with domain-dependent as- sumptions. Although the tasks involved in DSE are similar, the inﬂex- ibility of the existing frameworks restricts their reuse for solving DSE problems from other domains.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Comparison of Model Migration Tools",
        "date": 2010,
        "abstract": "Modelling languages and thus their metamodels are sub- ject to change. When a metamodel evolves, existing models may no longer conform to the evolved metamodel. To avoid rebuilding them from scratch, existing models must be migrated to conform to the evolved metamodel. Manually migrating existing models is tedious and error- prone. To alleviate this, several tools have been proposed to build a mi- gration strategy that automates the migration of existing models. Little is known about the advantages and disadvantages of the tools in diﬀer- ent situations. In this paper, we thus compare a representative sample of migration tools – AML, COPE, Ecore2Ecore and Epsilon Flock – using common migration examples. The criteria used in the comparison aim to support users in selecting the most appropriate tool for their situation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incremental Evaluation of Model Queries over EMF Models⋆",
        "date": 2010,
        "abstract": "Model-driven development tools built on industry standard platforms, such as the Eclipse Modeling Framework (EMF), heavily utilize model queries in model transformation, well-formedness constraint validation and domain-speciﬁc model execution. As these queries are executed rather frequently in interactive modeling applications, they have a signiﬁcant impact on runtime performance and end user experience. However, due to their complexity, these queries can be time consuming to implement and optimize on a case-by-case basis. Conse- quently, there is a need for a model query framework that combines an easy-to-use and concise declarative query formalism with high runtime performance.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Active Operations on Collections",
        "date": 2010,
        "abstract": "Collections are omnipresent within models: collections of ref- erences can represent relations between objects, and collections of values can represent object attributes. Consequently, manipulating models often consists of performing operations on collections. For example, transfor- mations create target collections from given source collections. Similarly, constraint evaluations perform computation on collections. Recent re- search works focus on making such transformations or constraint evalu- ations active (i.e. incremental, or live). However, they propose their own solutions to the issue by the introduction of speciﬁc languages and/or systems. This paper proposes a mathematical formalism, centered on collections and independent of languages and systems, that describes how the implementation of standard operations on collections can be made active. The formalism also introduces a reversed active assignment dedicated to bidirectional operations. A case study illustrates how to use the formalism and its Active Kermeta implementation for creating an active transformation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Active Kermeta"
        }
    },
    {
        "title": "transML: A Family of Languages to Model\nModel Transformations",
        "date": 2010,
        "abstract": "Model transformation is one of the pillars of Model-Driven Engineering (MDE). The increasing complexity of systems and modelling languages has dramatically raised the complexity and size of model trans- formations. Even though many transformation languages and tools have been proposed in the last few years, most of them are directed to the implementation phase of transformation development. However, there is a lack of cohesive support for the other phases of the transformation development, like requirements, analysis, design and testing. In this paper, we propose a uniﬁed family of languages to cover the life-cycle of transformation development. Moreover, following an MDE approach, we provide tools to partially automate the progressive reﬁne- ment of models between the diﬀerent phases and the generation of code for speciﬁc transformation implementation languages.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "transML"
        }
    },
    {
        "title": "Henshin: Advanced Concepts and Tools for In-Place EMF Model Transformations",
        "date": 2010,
        "abstract": "The Eclipse Modeling Framework (EMF) provides model- ing and code generation facilities for Java applications based on struc- tured data models. Henshin is a new language and associated tool set for in-place transformations of EMF models. The Henshin transforma- tion language uses pattern-based rules on the lowest level, which can be structured into nested transformation units with well-deﬁned operational semantics. So-called amalgamation units are a special type of transfor- mation units that provide a forall-operator for pattern replacement. For all of these concepts, Henshin oﬀers a visual syntax, sophisticated editing functionalities, execution and analysis tools. The Henshin transformation language has its roots in attributed graph transformations, which oﬀer a formal foundation for validation of EMF model transformations. The transformation concepts are demonstrated using two case studies: EMF model refactoring and meta-model evolution.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Henshin"
        }
    },
    {
        "title": "A Technique for Automatic Validation of Model Transformations",
        "date": 2010,
        "abstract": "We present in this paper a technique for proving properties about model transformations. The properties we are concerned about relate the structure of an input model with the structure of the trans- formed model. The main highlight of our approach is that we are able to prove the properties for all models, i.e. the transformation designer may be certain about the structural soundness of the results of his/her transformations. In order to achieve this we have designed and experi- mented with a transformation model checker, which builds what we call a state space for a transformation. That state space is then used as in classical model checking to prove the property or, in case the property does not hold to produce a counterexample. If the property holds this information can be used as a certiﬁcation for the transformation, other- wise the counterexample can be used as debug information during the transformation design process.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Static- and Dynamic Consistency Analysis of\nUML State Chart Models",
        "date": 2010,
        "abstract": "UML state chart models describing the behavior of a system can be used as a formal speciﬁcation thereof. The existence of advanced modeling tools allows for model simulation and enables the execution of manually created tests on the models. In this work the usage of static and dynamic model analysis techniques is proposed to reveal errors in these models. The static analysis focuses on the syntax, communication structure and non-determinism. The dynamic analysis is based on a ran- dom test approach and can reveal bugs like deadlocks and inter-model loops. Further the data generated during the dynamic analysis allows for additional correctness checks such as e.g. the number or lengths of paths. The presented approach is implemented in a prototype and revealed sev- eral bugs in an industrial case study not found during simulation and manual model testing.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Verifying Semantic Conformance of State Machine-to-Java Code Generators",
        "date": 2010,
        "abstract": "When applying model-driven engineering to safety-critical systems, the correctness of model transformations is crucial. In this pa- per, we investigate a novel approach to verifying the conformance to source language semantics of model-to-code transformations that uses annotations in the generated code. These annotations are inserted by the transformation and are used to guide a model checker to verify that the generated code satisﬁes the semantics of the source language – UML state machines in this paper. Verifying the generated output in this way is more eﬃcient than formally verifying the transformation’s deﬁnition. The veriﬁcation is performed using Java Pathﬁnder (JPF) [1], a model checker for Java source code. The approach has been applied to verify three UML state machine to Java code generators: one developed by us and two commercial generators (Rhapsody and Visual Paradigm). We were able to detect non-conformance in both commercial tools, which failed some semantic properties extracted from the UML speciﬁcation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Dynamic-Priority Based Approach to Fixing Inconsistent Feature Models",
        "date": 2010,
        "abstract": "In feature models’ construction, one basic task is to ensure the consistency of feature models, which often involves detecting and ﬁxing of inconsistencies in feature models. Several approaches have been proposed to detect inconsistencies, but few focus on the problem of ﬁxing inconsistent feature models. In this paper, we propose a dynamic-priority based approach to ﬁxing inconsistent feature models, with the purpose of helping domain analysts ﬁnd solutions to inconsistencies eﬃciently. The basic idea of our approach is to ﬁrst recommend a solution auto- matically, then gradually reach the desirable solution by dynamically adjusting priorities of constraints. To this end, we adopt the constraint hierarchy theory to express the degree of domain analysts’ conﬁdence on constraints (i.e. the priorities of constraints) and resolve inconsistencies among constraints. Two case studies have been conducted to demon- strate the usability and scalability of our approach.",
        "keywords": [
            "Feature Model",
            "Priority",
            "Inconsistency Fixing."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Taming Graphical Modeling",
        "date": 2010,
        "abstract": "Visual models help to understand complex systems. How- ever, with the user interaction paradigms established today, activities such as creating, maintaining or browsing visual models can be very te- dious. Valuable engineering time is wasted with archaic activities such as manual placement and routing of nodes and edges. This paper presents an approach to enhance productivity by focusing on the pragmatics of model-based design. Our contribution is twofold: First, the concept of meta layout enables the synthesis of diﬀerent diagrammatic views on graphical models. This modularly employs sophisticated layout algorithms, closing the gap be- tween MDE and graph drawing theory. Second, a view management logic harnesses this auto-layout to present customized views on models. These concepts have been implemented in the open source Kiel In- tegrated Environment for Layout Eclipse Rich Client (KIELER). Two applications—editing and simulation—illustrate how view management helps to increase developer productivity and tame model complexity.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Taming EMF and GMF\nUsing Model Transformation",
        "date": 2010,
        "abstract": "EMF and GMF are powerful frameworks for implementing tool support for modelling languages in Eclipse. However, with power comes complexity; implementing a graphical editor for a modelling lan- guage using EMF and GMF requires developers to hand craft and main- tain several low-level interconnected models through a loosely-guided, labour-intensive and error-prone process. In this paper we demonstrate how the application of model transformation techniques can help with taming the complexity of GMF and EMF and deliver signiﬁcant produc- tivity, quality, and maintainability beneﬁts. We also present EuGENia, an open-source tool that implements the proposed approach, illustrate its functionality through an example, and report on the community’s response to the tool.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Visual Traceability Modeling Language",
        "date": 2010,
        "abstract": "Software traceability is eﬀort intensive and must be applied strategically in order to maximize its beneﬁts and justify its costs. Unfor- tunately, development tools provide only limited support for traceability, and as a result users often construct trace queries using generic query languages which require intensive knowledge of the data-structures in which artifacts are stored. In this paper, we propose a usage-centered traceability process that utilizes UML class diagrams to deﬁne trace- ability strategies for a project and then visually represents trace queries as constraints upon subsets of the model. The Visual Trace Modeling Language (VTML) allows users to model queries while hiding the un- derlying technical details and data structures. The approach has been demonstrated through a prototype system and and evaluated through a preliminary experiment to evaluate the expressiveness and readability of VTML in comparison to generic SQL queries.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Application Logic Patterns – Reusable Elements\nof User-System Interaction",
        "date": 2010,
        "abstract": "Patterns of various kind are commonly used to reduce costs and improve quality in software development. This paper introduces the concept of patterns at the level of detailed descriptions of the user-system dialogue. Application Logic Patterns deﬁne generalised sequences of in- teractions performed by the system and its users in the context of an ab- stract problem domain. The patterns are organised into a library. They are precisely described by a language which is deﬁned through a strict meta-model. It extends the notation and semantics of the UML activ- ities and use cases. Each of the patterns describing the visible system dynamics is linked to an abstract domain model central to all the pat- terns. The patterns can be easily instantiated by substituting abstract domain notions with the notions speciﬁc to a given domain. This ease of use and reduction in eﬀort is validated in a controlled experiment using an open-source tool.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Metamodel-Based Approach for Automatic User \nInterface Generation",
        "date": 2010,
        "abstract": "One of the advantages of following a MDA-based approach in the development of interactive applications is the possibility of generating multiple platform-specific user interfaces (UI) from the same platform independent UI model. However, the effort required to create the UI model may be significant. In the case of data-intensive applications, a large part of the UI structure and functionality is closely related with the structure and functionality of the do- main entities described in the domain model, and the access rules specified in the use case model. This paper presents an approach to reduce the effort re- quired to create platform independent UI models for data intensive applications, by automatically generating an initial UI model from domain and use case models. For that purpose, UML-aligned metamodels for domain and use case models are defined, together with a MOF-based metamodel for user interface models. The transformation rules that drive the UI model generation are intro- duced. It is also proposed a MDA-based process for the development of data in- tensive interactive applications based on the proposed model architecture and transformations.",
        "keywords": [
            "MDD",
            "MDA",
            "Metamodel",
            "User Interface Automatic Generation",
            "Model Transformation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Rapid UI Development for Enterprise\nApplications: Combining Manual and\nModel-Driven Techniques",
        "date": 2010,
        "abstract": "UI development for enterprise applications is a time-consum- ing and error-prone task. In fact, approximately 50% of development resources are devoted to UI implementation tasks [1]. Model-driven UI development aims to reduce this eﬀort. However, the quality of the ﬁnal layout is a problem of this approach, especially when dealing with large and complex domain models. We share our experience in successfully using model-driven UI development in a large-scale enterprise project. Our approach mitigates the problems of model-driven UI development by combining manual layout with automatic inference of UI elements from a given domain model. Furthermore, we provide means to inﬂuence the UI generation at design time and to customize the UI at runtime. Thus, our approach signiﬁcantly reduces the UI implementation eﬀort while retaining control of the resulting UI.",
        "keywords": [
            "Model-Driven UI Development",
            "UI Generation",
            "UI Cus- tomisation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Environment Modeling with UML/MARTE to Support \nBlack-Box System Testing for Real-Time Embedded \nSystems: Methodology and Industrial Case Studies",
        "date": 2010,
        "abstract": "The behavior of real-time embedded systems (RTES) is driven by their environment. Independent system test teams normally focus on black-box testing as they have typically no easy access to precise design information. Black-box testing in this context is mostly about selecting test scenarios that are more likely to lead to unsafe situations in the environment. Our Model-Based Testing (MBT) methodology explicitly models key properties of the environ- ment, its interactions with the RTES, and potentially unsafe situations triggered by failures of the RTES under test. Though environment modeling is not new, we propose a precise methodology fitting our specific purpose, based on a lan- guage that is familiar to software testers, that is the UML and its extensions, as opposed to technologies geared towards simulating natural phenomena. Fur- thermore, in our context, simulation should only be concerned with what is visible to the RTES under test. Our methodology, focused on black-box MBT, was assessed on two industrial case studies. We show how the models are used to fully automate black-box testing using search-based test case generation techniques and the generation of code simulating the environment.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving Test Models for Large Scale Industrial \nSystems: An Inquisitive Study",
        "date": 2010,
        "abstract": "Although documentation of software tests is becoming increasingly important, there is little knowledge on whether modeling languages and tools are effective in industrial projects. Recent reports have pointed out that test modeling techniques might be barely used by software developers due to their inability to cover test concepts relevant in real-life large applications. This pa- per reports an inquisitive multi-phase study aimed at revealing test-relevant concepts not supported by modeling languages. The study encompassed several questionnaire responses and interviews with developers, and observational analyses run over two years in large-scale software projects. Various test con- cepts were brought forth and they fall in three categories: (i) test cases and software evolution, (ii) interdependencies between test cases, and (iii) categori- zation and grouping of test cases. Finally, the relevance of the identified test concepts is discussed in terms of an industrial system for inventory and supply control of petroleum products.",
        "keywords": [
            "Modeling",
            "Software Testing",
            "Industrial Applications."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatically Discovering Properties That\nSpecify the Latent Behavior of UML Models⋆,⋆⋆",
        "date": 2010,
        "abstract": "Formal analysis can be used to verify that a model of the system adheres to its requirements. As such, traditional formal analysis focuses on whether known (desired) system properties are satisﬁed. In contrast, this paper proposes an automated approach to generating tem- poral logic properties that specify the latent behavior of existing UML models; these are unknown properties exhibited by the system that may or may not be desirable. A key component of our approach is Marple, a evolutionary-computation tool that leverages natural selection to dis- cover a set of properties that cover diﬀerent regions of the model state space. The Marple-discovered properties can be used to reﬁne the mod- els to either remove unwanted behavior or to explicitly document a desir- able property as required system behavior. We use Marple to discover unwanted latent behavior in two applications: an autonomous robot nav- igation system and an automotive door locking control system obtained from one of our industrial collaborators.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards a Semantics of Activity Diagrams with\nSemantic Variation Points",
        "date": 2010,
        "abstract": "UML activity diagrams have become an established nota- tion to model control and data ﬂow on various levels of abstraction, ranging from ﬁne-grained descriptions of algorithms to high-level work- ﬂow models in business applications. A formal semantics has to capture the ﬂexibility of the interpretation of activity diagrams in real systems, which makes it inappropriate to deﬁne a ﬁxed formal semantics. In this paper, we deﬁne a semantics with semantic variation points that allow for a customizable, application-speciﬁc interpretation of activity diagrams. We examine concrete variants of the activity diagram semantics which may also entail variants of the syntax reﬂecting the intended use at hand.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An AADL-Based Approach to Variability Modeling of\nAutomotive Control Systems",
        "date": 2010,
        "abstract": "While the complexity of automotive systems is increasing, nowadays, most of the newly developed functionalities are implemented by software. This implies that software plays an important role in the development of automotive systems. However, several ineﬃciency problems related to software remain un- resolved. One problem is to ﬁnd an eﬀective way to handle a large-scale varia- tion of automotive systems. Hence, this paper presents an AADL (Architecture Analysis & Design Language)-based approach to the variation-related problem. The proposed approach captures the variation of automotive systems and yields their variability models. The obtained models promote an eﬃcient development that exploits system variation. In this paper, we explain the detailed procedure of AADL-based development with the help of an example of development of cruise control systems.",
        "keywords": [
            "ADL",
            "architecture description language",
            "AADL",
            "architecture analy- sis & design language",
            "variability modeling",
            "automotive electronics systems."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending Variability for OCL Interpretation",
        "date": 2010,
        "abstract": "In recent years, OCL advanced from a language used to con- strain UML models to a constraint language that is applied to various modelling languages. This includes Domain Speciﬁc Languages (DSLs) and meta-modelling languages like MOF or Ecore. Consequently, it is rather common to provide variability for OCL parsers to work with dif- ferent modelling languages. A second variability dimension relates to the technical space that models are realised in. Current OCL interpreters do not support such variability as their implementation is typically bound to a speciﬁc technical space like Java, Ecore, or a speciﬁc model reposi- tory. In this paper we propose a generic adaptation architecture for OCL that hides models and model instances behind well-deﬁned interfaces. We present how the implementation of such an architecture for DresdenOCL enables reuse of the same OCL interpreter for various technical spaces and evaluate our approach in three case studies.",
        "keywords": [
            "OCL",
            "OCL Infrastructure",
            "OCL Tool",
            "MDSD",
            "Modelling",
            "Constraint Interpretation",
            "Technological Spaces",
            "Variability",
            "Adaptation."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Inter-modelling: From Theory to Practice",
        "date": 2010,
        "abstract": "We deﬁne inter-modelling as the activity of building models that describe how modelling languages should be related. This includes many common activities in Model Driven Engineering, like the speciﬁca- tion of model-to-model transformations, the deﬁnition of model match- ing and model traceability constraints, the development of inter-model consistency maintainers and exogenous model management operators. Recently, we proposed a formal approach to specify the allowed and forbidden relations between two modelling languages by means of bidi- rectional declarative patterns. Such speciﬁcations were used to generate graph rewriting rules able to enforce the relations in (forward and back- ward) model-to-model transformation scenarios. In this paper we extend the usage of patterns for two further inter-modelling scenarios – model matching and model traceability – and report on an EMF-based tool im- plementing them. The tool allows a high-level analysis of speciﬁcations based on the theory developed so far, as well as manipulation of traces by compilation of patterns into the Epsilon Object Language.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon Object Language"
        }
    },
    {
        "title": "Consistent Modeling Using Multiple UML Profiles",
        "date": 2010,
        "abstract": "The design of complex technical system invariably involves multiple domain-specific languages to cover the many different facets of such systems. However, unless the languages are designed to be used in combination, this typically leads to conflicting specifications that are difficult to reconcile due to the ontological and other differences between the languages used. In this paper, we describe a pragmatic but systematic approach to resolving this problem for the special but common case in which the domain-specific languages are all defined as UML profiles.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Systematic Review on the Deﬁnition of UML Proﬁles⋆",
        "date": 2010,
        "abstract": "This article reports a systematic review on the deﬁnition of UML proﬁles in the research literature. Several exploratory statis- tical analyses have been performed in order to characterise both the idiosyncrasy of UML proﬁles and how they are reported in the litera- ture. This study uncovers the diﬀerences between presentation styles for behavioural and structural domains, and shows how UML proﬁles based on Class, Association, and Property structural metaclasses clearly out- number any other kind. Also, this review reveals how half of the examined UML proﬁles merely extend the abstract syntax, without adding neither icons nor constraints. The main contribution of this study is therefore a clear picture of the state-of-the-art in UML proﬁling, together with a set of open questions regarding its future.",
        "keywords": [
            "UML",
            "modelling",
            "proﬁles",
            "review."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Value in Muddling Around Modelling",
        "date": 2011,
        "abstract": "Software is a designed artifact. In other design disciplines, such as building architecture, there is a well-established tradition of design studies which inform not only the discipline itself but also tool design, processes, and collaborative work. This talk considers software from such a 'design studies' perspective. The talk will present a series of observations from empirical studies of expert software designers, and will draw on examples from actual professional practice. It will consider what experts’ mental imagery, software visualisations, and sketches suggest about software design thinking. It will discuss which representations designers use when allowed to choose freely, how designers’ informal representations relate to the formal representations from their discipline, how the character of their informal representations facilitates design discussions, and why many of the functions afforded by their sketching are not well supported by existing CAD systems. It will consider what the observations and sketches reveal about requirements for an idea-capture tool that supports collaborative design. The talk will also discuss some of the deliberate practices experts use to promote innovation. Finally, it will open discussion on the tensions between observed software design practices and received methodology in software engineering.",
        "keywords": [
            "empirical studies",
            "expert design",
            "software design",
            "flexible modeling",
            "software engineering practice."
        ],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Towards Quality Driven Exploration of Model Transformation Spaces⋆",
        "date": 2011,
        "abstract": "Verifying that a software system has certain non-functional properties is a primary concern in many engineering ﬁelds. Although several model-driven approaches exist to predict quality attributes from system models, they still lack the proper level of automation envisioned by Model Driven Software Development. When a potential issue con- cerning non-functional properties is discovered, the identiﬁcation of a solution is still entirely up to the engineer and to his/her experience. This paper presents QVT-Rational, our multi-modeling solution to auto- mate the detection-solution loop. We leverage and extend existing model transformation techniques with constructs to elicit the space of the al- ternative solutions and to bind quality properties to them. Our frame- work is highly customizable, it supports the deﬁnition of non-functional requirements and provides an engine to automatically explore the solu- tion space. We evaluate our approach by applying it to two well-known software engineering problems — Object-Relational Mapping and com- ponents allocation — and by showing how several solutions that satisfy given performance requirements can be automatically identiﬁed.",
        "keywords": [
            "Feedback Provisioning",
            "Model Transformations."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-Rational"
        }
    },
    {
        "title": "Automated Model-to-Metamodel Transformations Based on the Concepts of Deep Instantiation",
        "date": 2011,
        "abstract": "Numerous systems, especially component-based systems, are based on a multi-phase development process where an ontological hierar- chy is established. Solutions based on modeling / metamodeling can be used for such systems, but all of them are aﬄicted with diﬀerent draw- backs. The main problem is that elements representing both CLAsses and oBJECTs (clabjects), which are needed to specify an ontological hi- erarchy, are not supported by standard metamodeling frameworks. This paper presents the combination of two approaches, namely deep instanti- ation and model-to-metamodel transformations. The resulting approach combines the clean and compact speciﬁcation of deep instantiation with the easy applicability of model-to-metamodel transformations in an au- tomated way. Along with this a set of generic operators to specify these transformations is identiﬁed.",
        "keywords": [
            "Model-to-Metamodel (M2MM)",
            "Model-to-Model (M2M)",
            "Model Transformation",
            "Deep Instantiation",
            "Transformation Operator",
            "Clabject",
            "Model-Driven Software Development (MDSD)."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lazy Execution of Model-to-Model Transformations",
        "date": 2011,
        "abstract": "The increasing adoption of Model-Driven Engineering in in- dustrial contexts highlights scalability as a critical limitation of several MDE tools. Most of the current model-to-model transformation engines have been designed for one-shot translation of input models to output models, and present eﬃciency issues when applied to very large models. In this paper, we study the application of a lazy-evaluation approach to model transformations. We present a lazy execution algorithm for ATL, and we empirically evaluate a prototype implementation. With it, the elements of the target model are generated only when (and if) they are accessed, enabling also transformations that generate inﬁnite target models. We achieve our goal on a signiﬁcant subset of ATL by extending the ATL compiler.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Measuring UML Models Using Metrics\nDeﬁned in OCL within the SQUAM Framework",
        "date": 2011,
        "abstract": "In software engineering practice, measurements may reduce development costs by improving processes and products at early stages. In model driven approaches, measurements can be conducted right from the start of a project. For UML models, a collection of metrics has been empirically validated, however, these need to be precisely deﬁned in or- der to be useful. Deﬁnition of UML metrics in OCL oﬀers a high degree of precision and portability, but due to shortcomings of this language this approach is not widespread. We propose the SQUAM framework, a tool– supported methodology to develop OCL speciﬁcations, which incorpo- rates best practices in software development, such as libraries, testing and documentation. As a proof of concept we have developed 26 metrics for UML class diagrams in the academic context. This demonstrated the high eﬀectiveness of our approach: quick learning, high satisfaction of develop- ers, low imposed complexity and potential time reduction through reuse.",
        "keywords": [
            "model analysis",
            "UML metrics",
            "OCL speciﬁcation",
            "OCL pragmatic extensions",
            "OCL development process."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Model Slicers⋆",
        "date": 2011,
        "abstract": "Among model comprehension tools, model slicers are tools that ex- tract a subset from a model, for a speciﬁc purpose. Model slicers are tools that let modelers rapidly gather relevant knowledge from large models. However, exist- ing slicers are dedicated to one modeling language. This is an issue when we ob- serve that new domain speciﬁc modeling languages (DSMLs), for which we want slicing abilities, are created almost on a daily basis. This paper proposes the Kom- pren language to model and generate model slicers for any DSL (e.g. software de- velopment and building architecture) and for different purposes (e.g. monitoring and model comprehension). Kompren’s abilities for model slicers construction is based on case studies from various domains.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Kompren"
        }
    },
    {
        "title": "Morsa: A Scalable Approach for Persisting and Accessing Large Models⋆",
        "date": 2011,
        "abstract": "Applying Model-Driven Engineering (MDE) in industrial- scale systems requires managing complex models which may be very large. These models must be persisted in a scalable way that allows their manipulation by client applications without fully loading them.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Expressing Aspectual Interactions in Design: Experiences in the Slot Machine Domain",
        "date": 2011,
        "abstract": "In the context of an industrial project we are implementing the software of a casino slot machine. This software has a signiﬁcant amount of cross-cutting concerns that depend on, and interact with each other, as well as with the modular concerns. We therefore wish to express our design using an appropriate Aspect-Oriented Modeling methodol- ogy and notation. We evaluated two of the most mature methodologies: Theme/UML and WEAVR, to establish their suitability. Remarkably, neither of these allow us to express any of the dependencies and inter- actions to our satisfaction. In both cases, half of the interaction types cannot be expressed at all while the other half need to be expressed using a workaround that hides the intention of the design. As a result, we consider both methodologies and notations unsuitable for expressing the dependencies and interactions present in the slot machine domain. In this paper we describe our evaluation experience.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Industrial Application of Robustness Testing Using \nAspect-Oriented Modeling, UML/MARTE, and Search \nAlgorithms",
        "date": 2011,
        "abstract": "Systematic and rigorous robustness testing is very critical for embedded systems, as for example communication and control systems. Robustness testing aims at testing the behavior of a system in the presence of faulty situations in its operating environment (e.g., sensors and actuators). In such situations, the system should gracefully degrade its performance instead of abruptly stopping execution. To systematically perform robustness testing, one option is to resort to model-based robustness testing (MBRT), based for example on UML/MARTE models. However, to successfully apply MBRT in industrial contexts, new technology needs to be developed to scale to the complexity of real industrial systems. In this paper, we report on our experience of performing MBRT on video conferencing systems developed by Cisco Systems, Norway. We discuss how we developed and integrated various techniques and tools to achieve a fully automated MBRT that is able to detect previously uncaught software faults in those systems. We provide an overview of how we achieved scalable modeling of robustness behavior using aspect-oriented modeling, test case generation using search algorithms, and environment emulation for test case execution. Our experience and lessons learned identify challenges and open research questions for the industrial application of MBRT.",
        "keywords": [
            "Model-based testing",
            "aspect-oriented modeling",
            "search algorithms",
            "MARTE",
            "UML",
            "robustness."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Aspect-Oriented Modelling\nfor Distributed Systems",
        "date": 2011,
        "abstract": "Aspect-Oriented Modelling techniques allow a modeller to describe within a single aspect model all model elements that deﬁne the structural and/or behavioural properties of a concern. When applied to a base model, the model weaver ensures that the entire aspect is reﬂected in the woven model. While this is essential for centralized systems, it is not the case when model elements of a concern are scattered over nodes in a distributed system. We propose an extension to our Reusable Aspect Models that allows the modeller to augment an aspect model of a concern that can crosscut the nodes of a distributed system with distribution role deﬁnitions. A distributed system conﬁguration ﬁle speciﬁes the diﬀerent node types of the distributed system, and which roles of a distributed as- pect are assigned to which nodes. The weaver makes sure that every role of a distributed aspect is assigned to at least one node in the system to ensure consistent aspect use. The weaver then generates for each node a ﬁnal application model that only contains the model elements pertaining to the distribution roles the node plays.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Precise Style for Business Process Modelling:\nResults from Two Controlled Experiments",
        "date": 2011,
        "abstract": "We present a precise style for the modelling of business pro- cesses based on the UML activity diagrams and two controlled exper- iments to compare this style with a lighter variant. The comparison has been performed with respect to the comprehensibility of business processes and the eﬀort to comprehend them. The ﬁrst experiment has been conducted at the Free University of Bolzano-Bozen, while the sec- ond experiment (i.e., a diﬀerentiated replication) at the University of Genova. The participants to the ﬁrst experiment were Master students and so more experienced than the participants to the replication, who were Bachelor students. The results indicate that: (a) all the participants achieved a signiﬁcantly better comprehension level with the precise style; (b) the used style did not have any signiﬁcant impact on the eﬀort; and (c) more experienced participants beneﬁted more from the precise style.",
        "keywords": [
            "Business Process Modelling",
            "UML activity diagrams",
            "Con- trolled experiment",
            "Precise and Ultra-light styles."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Semantically Conﬁgurable Consistency Analysis for Class and Object Diagrams",
        "date": 2011,
        "abstract": "Checking consistency between an object diagram (OD) and a class diagram (CD) is an important analysis problem. However, several variations in the semantics of CDs and ODs, as used in diﬀerent contexts and for diﬀerent purposes, create a challenge for analysis tools. To ad- dress this challenge in this paper we investigate semantically conﬁgurable model analysis. We formalize the variability in the languages semantics using a feature model: each conﬁguration that the model permits induces a diﬀerent semantics. Moreover, we develop a parametrized analysis that can be instantiated to comply with every legal conﬁguration of the fea- ture model. Thus, the analysis is semantically conﬁgured and its results change according to the semantics induced by the selected feature conﬁg- uration. The ideas are implemented using a parametrized transformation to Alloy. The work can be viewed as a case study example for a formal and automated approach to handling semantic variability in modeling languages.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Identifying the Weaknesses of UML Class\nDiagrams during Data Model Comprehension",
        "date": 2011,
        "abstract": "In this paper we present an experiment and two replications aimed at comparing the support provided by ER and UML class dia- grams during comprehension activities by focusing on the single build- ing blocks of the two notations. This kind of analysis can be used to identify weakness in a notation and/or justify the need of preferring ER or UML for data modeling. The results reveal that UML class diagrams are generally more comprehensible than ER diagrams, even if the former has some weaknesses related to three building blocks, i.e., multi-value attribute, composite attribute, and weak entity. These ﬁndings suggest that a UML class diagram extension should be considered to overcome these weaknesses and improve the comprehensibility of the notation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Engineering Android Applications\nBased on UML Activities",
        "date": 2011,
        "abstract": "With the evolving capabilities of devices, mobile applications are emerging towards complex reactive systems. To handle this complex- ity and shorten development time by increased reuse, we propose an en- gineering approach based on UML activities, which are used like building blocks to construct applications. Libraries of such building blocks make Android-speciﬁc features available. Tool support provides automatic for- mal analysis for soundness and automatic implementation. Furthermore, the approach is easily extensible, since new features can be provided by new building blocks, without changing the tools or notation. We demon- strate the method by a voice messaging application.",
        "keywords": [
            "Mobile Applications",
            "Android",
            "UML Activities",
            "Model-Driven Engineering."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain-Speciﬁc Model Transformation in\nBuilding Quantity Take-Oﬀ",
        "date": 2011,
        "abstract": "The two core concepts of model-driven engineering are mod- els and model transformations. Domain-Speciﬁc Modelling has become accepted as a powerful means of providing domain experts and end users with the ability to create and manipulate models within the systems that they use. In this paper we argue that there are domains for which it is appropriate to also provide domain experts with the ability to modify and develop model transformations. One such domain is that of quantity surveying, and speciﬁcally the taking-oﬀof quantities from a building design. We describe a language for expressing transformations between building models and bills of quantities, and its implementation within an automated quantity take-oﬀtool, reﬂecting on the commonalities and diﬀerences between this language and a general-purpose model transfor- mation language/tool.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Improving Scalability and Maintenance of\nSoftware for High-Performance Scientiﬁc\nComputing by Combining MDE and Frameworks",
        "date": 2011,
        "abstract": "In recent years, numerical simulation has attracted increas- ing interest within industry and among academics. Paradoxically, the development and maintenance of high performance scientiﬁc computing software has become more complex due to the diversiﬁcation of hardware architectures and their related programming languages and libraries. In this paper, we share our experience in using model-driven develop- ment for numerical simulation software. Our approach called MDE4HPC proposes to tackle development complexity by using a domain speciﬁc modeling language to describe abstract views of the software. We present and analyse the results obtained with its implementation when deriving this abstract model to target Arcane, a development framework for 2D and 3D numerical simulation software.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Critical Review of Applied MDA for\nEmbedded Devices: Identiﬁcation of Problem\nClasses and Discussing Porting Eﬀorts in\nPractice",
        "date": 2011,
        "abstract": "Model-driven development (MDD) has seen wide application in research, but still has limitations in real world industrial projects. One project which applies such MDD principles is about developing the software of a feature phone. While advantages seem to outweigh any dis- advantages in theory, several problems arise when applying the model- driven methodology in practice. Problems when adopting this approach are shown as well as a practical solution to utilize one of the main ad- vantages of MDD—portability. Issues that originate from using a tool which supports a model-driven approach are presented. A conclusion sums up the personal experiences made when applying MDD in a real world project.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Designing Heterogeneous Component Based\nSystems: Evaluation of MARTE Standard and\nEnhancement Proposal",
        "date": 2011,
        "abstract": "Building complex real-time embedded systems requires as- sembly of heterogeneous components, possibly using various computation and communication models. A great challenge is to be able to design such systems using models where these heterogeneity characteristics are de- scribed precisely to assist the next step of the development including im- plementation or analysis. Although the new MARTE standard provides the core concepts to model real-time components using various commu- nication paradigms, we state in this paper that MARTE extensions have still to be made and we propose to extract common features from several component based approaches in order to support ﬁner compositions of heterogeneous sub-systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Semantic Clone Detection for Model-Based\nDevelopment of Embedded Systems",
        "date": 2011,
        "abstract": "With model-based development becoming an increasingly common development methodology in embedded systems engineering, models have become an important asset of the the software development process. Therefore, techniques for the automatic detection of clones in those models have been developed to improve their maintainability. As these approaches currently only consider syntactic clones, the detection of clones is limited to syntactically equivalent copies. Using the concept of normal forms, these approaches can be extended to also cover seman- tic clones with identical behavior but diﬀerent structure. The submission presents a generalized concept of clones for Simulink models, describes a pattern-based normal-form approach, and discusses results of the appli- cation of an implementation of this approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Instant and Incremental QVT Transformation for Runtime Models",
        "date": 2011,
        "abstract": "As a dynamic representation of the running system, a run- time model provides a model-based interface to monitor and control the system. A key issue for runtime models is to maintain their causal connec- tions with the running system. That means when the systems change, the models should change accordingly, and vice versa. However, for the ab- stract runtime models that are heterogeneous to their target systems, it is challenging to maintain such causal connections. This paper presents a model-transformation-based approach to maintaining causal connections for abstract runtime models. We deﬁne a new instant and incremental transformation semantics for the QVT-Relational language, according to the requirements of runtime models, and develop the transformation algorithm following this semantics. We implement this approach on the mediniQVT transformation engine, and apply it to provide the runtime model for an intelligent oﬃce system named SmartLab.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-Relational"
        }
    },
    {
        "title": "Service–Oriented Architecture Modeling:\nBridging the Gap between Structure and Behavior",
        "date": 2011,
        "abstract": "Model–driven development of large-scale software systems is highly likely to produce models that describe the systems from many diverse perspec- tives using a variety of modeling languages. Checking and maintaining consis- tency of information captured in such multi-modeling environments is known to be challenging. In this paper we describe an approach to systematically synchro- nize multi–models. The approach speciﬁcally addresses the problem of synchronizing business processes and domain models in a Service-oriented Ar- chitecture development environment. In the approach, the human effort required to synchronize independently developed models is supplemented with signiﬁcant automated support. This process is used to identify concept divergences, that is, a concept in one model which cannot be matched with concepts in the other model. We automate the propagation of divergence resolution decisions across the conﬂicting models. We illustrate the approach using models developed for a Car Crash Crisis Management System (CCCMS), a case study problem used to assess Aspect–oriented Modeling approaches.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From State- to Delta-Based Bidirectional Model\nTransformations: The Symmetric Case",
        "date": 2011,
        "abstract": "A bidirectional transformation (BX) keeps a pair of interre- lated models synchronized. Symmetric BXs are those for which neither model in the pair fully determines the other. We build two algebraic frameworks for symmetric BXs, with one correctly implementing the other, and both being delta-based generalizations of known state-based frameworks. We identify two new algebraic laws—weak undoability and weak invertibility, which capture important semantics of BX and are use- ful for both state- and delta-based settings. Our approach also provides a ﬂexible tool architecture adaptable to diﬀerent user’s needs.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enforcing S&D Pattern Design in RCES with Modeling\nand Formal Approaches",
        "date": 2011,
        "abstract": "The requirement for higher security and dependability of systems is continuously increasing even in domains not traditionally deeply involved in such issues. Yet, evolution of embedded systems towards devices connected via In- ternet, wireless communication or other interfaces requires a reconsideration of secure and trusted embedded systems engineering processes. In this paper, we propose an approach that associates model driven engineering (MDE) and formal validation to build security and dependability (S&D) patterns for trusted RCES applications. The contribution of this work is twofold. On the one hand, we use model-based techniques to capture a set of artifacts to encode S&D patterns. On the other hand, we introduce a set of artifacts for the formal validation of these patterns in order to guarantee their correctness. The formal validation in turn fol- lows the the MDE process and thus links concrete validation results to the S&D requirements identiﬁed at higher levels of abstraction.",
        "keywords": [
            "Resource Constrained Embedded Systems",
            "Trust",
            "Security",
            "Depend- ability",
            "Pattern",
            "Meta-model",
            "Model Driven Engineering",
            "Formal Modeling."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Based and Automated Approach to Size \nEstimation of Embedded Software Components",
        "date": 2011,
        "abstract": "Accurate estimation of Software Code Size is important for developing cost-efficient embedded systems. The Code Size affects the amount of system resources needed, like ROM and RAM memory, and processing capacity. In our previous work, we have estimated the Code Size based on CFP (COSMIC Function Points) within 15% accuracy, with the purpose of deciding how much ROM memory to fit into products with high cost pressure. Our manual CFP measurement process would require 2,5 man years to estimate the ROM size required in a typical car. In this paper, we want to investigate how the manual effort involved in estimation of Code Size can be minimized. We define a UML Profile capturing all information needed for estimation of Code Size, and develop a tool for automated estimation of Code Size based on CFP. A case study will show how UML models save manual effort in a realistic case.",
        "keywords": [
            "UML Profile",
            "UML components",
            "software components",
            "functional  size measurement",
            "code size estimation."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MDE to Manage Communications with and\nbetween Resource-Constrained Systems",
        "date": 2011,
        "abstract": "With the emergence of Internet of Things (IoT), many things which typically used to be isolated or operated in small local networks, will be interconnected through the Internet. One main challenge to tackle in IoT is eﬃcient management of communication between things, since things can be very diﬀerent in terms of available resources, size and communication protocols. Current Internet-enabled devices are typically powerful enough to rely on common operating systems, standard net- work protocols and middlewares. In IoT many devices will be too con- strained to rely on such resource-consuming infrastructures; they run ad-hoc proprietary protocols. The contribution of this paper is a model- based approach for the eﬃcient provisioning and management of the communication between heterogeneous resource-constrained devices. It includes a DSML which compiles to a set of interoperable communication libraries providing an abstract communication layer that can integrate both powerful and resource-constrained devices. The approach is imple- mented in an IDE for the development resource-constrained Things.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Diagram Definition: A Case Study with the UML Class \nDiagram",
        "date": 2011,
        "abstract": "The abstract syntax of a graphical modeling language is typically defined with a metamodel while its concrete syntax (diagram) is informally defined with text and figures. Recently, the Object Management Group (OMG) released a beta specification, called Diagram Definition (DD), to formally define both the interchange syntax and the graphical syntax of diagrams. In this paper, we validate DD by using it to define a subset of the UML class diagram. Specifically, we define the interchange syntax with a MOF-based metamodel and the graphical syntax with a QVT mapping to a graphics metamodel. We then run an experiment where we interchange and render an example diagram. We highlight various design decisions and discuss challenges of using DD in practice. Finally, we conclude that DD is a sound approach for formally defining diagrams that is expected to facilitate the interchange and the consistent rendering of diagrams between tools.",
        "keywords": [
            "Diagram",
            "Definition",
            "Model",
            "MOF",
            "UML",
            "QVT",
            "DD",
            "SVG."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Reducing Multiplicities in Class Diagrams",
        "date": 2011,
        "abstract": "In class diagrams, so-called multiplicities are integer ranges attached to association ends. They constrain the number of instances of the associated class that an instance may be linked to, or in an alterna- tive reading, the number of links to instances of the associated class. In complex diagrams with several chains of associations between two classes (arising e.g. in conﬁguration management) it may happen that the lower or upper bound of a range can never be attained because of restrictions imposed by a parallel chain. In this paper we investigate how multiplicities behave when chain- ing associations together, and we characterise situations where intervals can be tightened due to information from other chains. Detecting and eliminating such redundancies provides valuable feedback to the user, as redundancies may hint at some underlying misconception.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Creating Models for Simulating the Face",
        "date": 2011,
        "abstract": "Creating animated computer generated faces which can withstand scrutiny on the large screen is a daunting task. How does the face move? How does it reflect light? What information is relevant? How can it be captured and then transformed to convincingly breathe life into a digital human or fantastic creature? The talk will give examples of new technologies and methodologies developed to achieve this in blockbuster films including “Avatar” and will point the way to the next generation of computer generated characters by showing the increasing importance of computational simulation and discovering and modeling what is really going on underneath the skin.",
        "keywords": [
            "computer animation",
            "computer generated characters",
            "computational  simulation."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EUnit: A Unit Testing Framework for Model\nManagement Tasks",
        "date": 2011,
        "abstract": "Validating and transforming models are essential steps in model-driven engineering. These tasks are often implemented as opera- tions in general purpose programming languages or task-speciﬁc model management languages. Just like other software artefacts, these tasks must be tested to reduce the risk of defects. Testing model management tasks requires testers to select and manage the relevant combinations of input models, tasks and expected outputs. This is complicated by the fact that many technologies may be used in the same system, each with their own integration challenges. In addition, advanced test oracles are required: tests may need to compare entire models or directory trees. To tackle these issues, we propose creating an integrated unit test- ing framework for model management operations. We have developed the EUnit unit testing framework to validate our approach. EUnit tests specify how models and tasks are to be combined, while staying decou- pled from the speciﬁc technologies used.",
        "keywords": [
            "Software testing",
            "unit testing",
            "model management",
            "test frameworks",
            "model validation",
            "model transformation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Verifying UML-RT Protocol Conformance Using Model Checking⋆",
        "date": 2011,
        "abstract": "In UML-RT, capsules communicate via protocols which con- nect capsule ports. Protocol State Machines (PSMs) allow the description of the legal message sequences of a port and are potentially very useful for the modular development and veriﬁcation of systems. However, it is unclear how exactly conformance of a capsule to its PSMs should be deﬁned and how this can be checked automatically. In this paper, we pro- vide a deﬁnition of protocol conformance and show how software model checking can be used to check protocol conformance automatically. We describe the design and implementation of a tool that checks the confor- mance of a capsule with Java action code with respect to the PSMs of all its ports. The results of the validation of the tool on three case studies are summarized.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Based Coverage-Driven Test Suite\nGeneration for Software Product Lines",
        "date": 2011,
        "abstract": "Software Product Line (SPL) engineering is a popular ap- proach for the systematic reuse of software artifacts across a large num- ber of similar products. Unfortunately, testing each product of an SPL separately is often unfeasible. Consequently, SPL engineering is in con- ﬂict with standards like ISO 26262, which require each installed software conﬁguration of safety-critical SPLs to be tested using a model-based approach with well-deﬁned coverage criteria. In this paper we address this dilemma and present a new SPL test suite generation algorithm that uses model-based testing techniques to derive a small test suite from one variable 150% test model of the SPL such that a given coverage criterion is satisﬁed for the test model of every product. Furthermore, our algorithm simpliﬁes the subsequent selection of a small, representative set of products (w.r.t. the given coverage criterion) on which the generated test suite can be executed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Constraint-Based Model Refactoring",
        "date": 2011,
        "abstract": "The UML standard specifies well-formedness rules as constraints on UML models. To be correct, refactoring of a model must take these constraints into account and check that they are still satisfied after a refactoring has been performed — if not, the refactoring must be refused. With constraint-based re- factoring, constraint checking is replaced by constraint solving, lifting the role of constraints from permitting or denying a tentative refactoring to computing additional model changes required for the refactoring to be executable. Thus, to the degree that the semantics of a modelling language is specified using con- straints, refactorings based on these constraints are guaranteed to be meaning preserving. To enable the reuse of pre-existing constraints for refactoring, we present a mapping from well-formedness rules as provided by the UML stan- dard to constraint rules as required by constraint-based refactoring. Using these mappings, models can be refactored at no extra cost; if refactorings fail, the lack of meaning preservation points us to how the constraint-based semantic specifi- cations of the modelling language can be improved.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting Design Model Refactoring\nfor Improving Class Responsibility Assignment",
        "date": 2011,
        "abstract": "Although a responsibility driven approach in object oriented analysis and design methodologies is promising, the assignment of the identiﬁed respon- sibilities to classes (simply, class responsibility assignment: CRA) is a crucial issue to achieve design of higher quality. The GRASP by Larman is a guideline for CRA and is being put into practice. However, since it is described in an infor- mal way using a natural language, its successful usage greatly relies on designers’ skills. This paper proposes a technique to represent GRASP formally and to au- tomate appropriate CRA based on them. Our computerized tool automatically detects inappropriate CRA and suggests alternatives of appropriate CRAs to de- signers so that they can improve a CRA based on the suggested alternatives. We made preliminary experiments to show the usefulness of our tool.",
        "keywords": [
            "object-oriented design",
            "class responsibility assignment",
            "GRASP."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Vision Paper: The Essence of Structural Models",
        "date": 2011,
        "abstract": "Models should represent the essential aspects of a system and leave out the inessential details. In this paper we propose an automatic approach to determine whether a model indeed focuses on the essential aspects. We deﬁne a new metric, structural essence, that quantiﬁes the fraction of essential elements in a model. Our approach targets structural models, such as the prevalent UML class diagrams. It is inspired by the idea of algorithmic essence – the amount of repetitive constructs in a program – and the duality between behavior and structure. We present a framework for computing the essence of a structural model based on a transformation of that model into a “distilled model” and on an existing graph algorithm operating on that distilled model. We discuss the meaning of our concept of structural essence based on a set of example models. We hope that our notion of structural essence will spark discussions on the purpose and the essence of models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Vision Paper:\nTowards Model-Based Energy Testing",
        "date": 2011,
        "abstract": "Today, energy consumption is one of the major challenges for optimisation of future software applications and ICT infrastructures. To develop software w.r.t. its energy consumption, testing is an essen- tial activity, since testing allows quality assurance and thus, energy con- sumption reduction during the software’s development. Although ﬁrst approaches measuring and predicting software’s energy consumption for its execution on a speciﬁc hardware platform exist, no model-based test- ing approach has been developed, yet. In this paper we present our vi- sion of a model-based energy testing approach that uses a combination of abstract interpretation and run-time proﬁling to predict the energy consumption of software applications and to derive energy consumption test cases.",
        "keywords": [
            "Energy consumption testing",
            "abstract interpretation",
            "pro- ﬁling",
            "unit testing",
            "model-based testing."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Vision Paper: Make a Difference! (Semantically)⋆",
        "date": 2011,
        "abstract": "Syntactic difference between models is a wide research area with applications in tools for model evolution, model synchronization and version con- trol. On the other hand, semantic difference between models is rarely discussed. We point out to main use cases of semantic difference between models, and then propose a framework for deﬁning well-formed difference operators on model semantics as adjoints of model combinators such as conjunction, disjunction and structural composition. The framework is deﬁned by properties other then con- structively. We instantiate the framework for two rather different modeling lan- guages: feature models and automata speciﬁcations. We believe that the algebraic theory of semantic difference will allow to deﬁne practical model differencing tools in the future.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic Derivation of Utility Functions for Monitoring Software Requirements⋆",
        "date": 2011,
        "abstract": "Utility functions can be used to monitor requirements of a dynamically adaptive system (DAS). More speciﬁcally, a utility function maps monitoring information to a scalar value proportional to how well a requirement is satisﬁed. Utility functions may be manually elicited by requirements engineers, or indirectly inferred through statistical regres- sion techniques. This paper presents a goal-based requirements model- driven approach for automatically deriving state-, metric-, and fuzzy logic-based utility functions for RELAXed goal models. State- and fuzzy logic-based utility functions are responsible for detecting requirements vi- olations, and metric-based utility functions are used to detect conditions conducive to a requirements violation. We demonstrate the proposed ap- proach by applying it to the goal model of an intelligent vehicle system (IVS) and use the derived utility functions to monitor the IVS under diﬀerent environmental conditions at run time.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Logic-Based Model-Level Software Development with F-OML",
        "date": 2011,
        "abstract": "Models are at the heart of the emerging Model-driven En- gineering (MDE) approach in which software is developed by repeated transformations of models. Intensive eﬀorts in the modeling community in the past two decades have produced an impressive variety of tool sup- port for models. Nonetheless, models are still not widely used throughout the software evolution life cycle and, in many cases, they are neglected in later stages of software development. To make models more useful, one needs a powerful model-level IDE that supports a wide range of object modeling tasks. Such IDEs must have a consistent formal foundation.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal Veriﬁcation of QVT Transformations for\nCode Generation",
        "date": 2011,
        "abstract": "We present a formal calculus for operational QVT. The cal- culus is implemented in the interactive theorem prover KIV and allows to prove properties of QVT transformations for arbitrary meta models. Additionally we present a framework for provably correct Java code generation. The framework uses a meta model for a Java abstract syntax tree as the target of QVT transformations. This meta model is mapped to a formal Java semantics in KIV. This makes it possible to formally prove with the QVT calculus that a transformation always generates a Java model (i.e. a program) that is type correct and has certain semantical properties. The Java model can be used to generate source code by a model-to-text transformation or byte code directly.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Model-Based (Mechanical) Product Design",
        "date": 2011,
        "abstract": "Mechanical product engineering is a research and industrial activity which studies the design of complex mechanical systems. The process, which involves the collaboration of various experts using domain- speciﬁc software, raises syntactic and semantic interoperability issues which are not addressed by existing software solutions or their underly- ing concepts. This article proposes a ﬂexible model-based software archi- tecture that allows for a federation of experts to deﬁne and collaborate in innovative design processes. The presented generic approach is backed and validated by its implementation on an academic usecase.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying a Model-Based Approach to IT Systems \nDevelopment Using SysML Extension",
        "date": 2011,
        "abstract": "Model-based system engineering (MBSE) is regarded as an effective way of developing systems. We are now applying the model-based approach to IT system development/integration (SI) because we urgently need to reduce the cost of SI. However, there are various challenges imposed when applying MBSE to SI. One of these is that reducing the cost to update models is more significant than that in other MBSE domains such as embedded systems. We adopted SysML to handle these issues and extended it to modeling IT systems. We present the details on this SysML extension and how it overcame these issues. We are developing an in-house SI-support tool called \"CASSI\", which evaluates the non-functional requirements; performance and availability of the IT system's models written in that extended manner and helps these models to be reused. This paper also includes industrial case studies of CASSI, and its effectiveness is discussed.",
        "keywords": [
            "Model-based",
            "IT systems development",
            "system modeling."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Early Experience with Agile Methodology in a  \nModel-Driven Approach",
        "date": 2011,
        "abstract": "We are in the business of delivering software intensive business systems using model-driven techniques. Developing suitable code generators is an important step in model-based development of purpose-specific business applications. Hence, it becomes critical to ensure that code generator development doesn’t become a bottleneck for the project delivery. After establishing a sophisticated technology infrastructure to facilitate quick and easy adaptation of model-based code generators, we experimented with agile methodology. In this paper, we discuss why pure agile methodology does not work for model-driven software development. We propose a modification to the agile methodology in the form of meta-sprints as a golden mean between agile method and traditional plan-driven method. Early experience with the proposed development method is shared along with the lessons learnt.",
        "keywords": [
            "model-driven development",
            "agile method",
            "software intensive  business systems."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CD2Alloy: Class Diagrams Analysis Using Alloy Revisited",
        "date": 2011,
        "abstract": "We present CD2Alloy, a novel, powerful translation of UML class diagrams (CDs) to Alloy. Unlike existing translations, which are based on a shallow embedding strategy, and are thus limited to check- ing consistency and generating conforming object models of a single CD, and support a limited set of CD language features, CD2Alloy uses a deeper embedding strategy. Rather than mapping each CD construct to a semantically equivalent Alloy construct, CD2Alloy deﬁnes (some) CD constructs as new concepts within Alloy. This enables solving sev- eral analysis problems that involve more than one CD and could not be solved by earlier works, and supporting an extended list of CD language features. The ideas are implemented in a prototype Eclipse plug-in. The work advances the state-of-the-art in CD analysis, and can also be viewed as an interesting case study for the diﬀerent possible translations of one modeling language to another, their strengths and weaknesses.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Engineering and Optimizing Compilers:\nA Bridge Too Far?",
        "date": 2011,
        "abstract": "A primary goal of Model Driven Engineering (MDE) is to reduce the cost and effort of developing complex software systems using techniques for transforming abstract views of software to concrete implementations. The rich set of tools that have been developed, especially the growing maturity of model transformation technologies, opens the possibility of applying MDE technologies to transformation-based problems in other domains. In this paper, we present our experience with using MDE technologies to build and evolve compiler infrastructures in the optimizing compiler domain. We illus- trate, through our two ongoing research compiler projects for C and a functional language, the challenging aspects of optimizing compiler research and show how mature MDE technologies can be used to address them. We also identify some of the pitfalls that arise from unrealistic expectations of what can be accomplished using MDE and discuss how they can lead to unsuccessful and frustrating appli- cation of MDE technologies.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Towards a General Composition Semantics for Rule-Based Model Transformation",
        "date": 2011,
        "abstract": "As model transformations have become an integral part of the automated software engineering lifecycle, reuse, modularisation, and composition of model transformations becomes important. One way to compose model transformations is to compose modules of transformation rules, and execute the composition as one transformation (internal com- position). This kind of composition can provide ﬁne-grained semantics, as it is part of the transformation language. This paper aims to gener- alise two internal composition mechanisms for rule-based transformation languages, module import and rule inheritance, by providing executable semantics for the composition mechanisms within a virtual machine. The generality of the virtual machine is demonstrated for diﬀerent rule-based transformation languages by compiling those languages to, and execut- ing them on this virtual machine. We will discuss how ATL and graph transformations can be mapped to modules and rules inside the virtual machine.",
        "keywords": [
            "Model transformation",
            "Model transformation composition",
            "ATL",
            "Graph transformation."
        ],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "ATL, Graph transformation"
        }
    },
    {
        "title": "Properties of Realistic Feature Models Make\nCombinatorial Testing of Product Lines Feasible",
        "date": 2011,
        "abstract": "Feature models and associated feature diagrams allow mod- eling and visualizing the constraints leading to the valid products of a product line. In terms of their expressiveness, feature diagrams are equiv- alent to propositional formulas which makes them theoretically expensive to process and analyze. For example, satisfying propositional formulas, which translates into ﬁnding a valid product for a given feature model, is an NP-hard problem, which has no fast, optimal solution. This theo- retical complexity could prevent the use of powerful analysis techniques to assist in the development and testing of product lines. However, we have found that satisfying realistic feature models is quick. Thus, we show that combinatorial interaction testing of product lines is feasible in practice. Based on this, we investigate covering array generation time and results for realistic feature models and ﬁnd where the algorithms can be improved.",
        "keywords": [
            "Software Product Lines",
            "Testing",
            "Feature Models",
            "Practical",
            "Realistic",
            "Combinatorial Interaction Testing."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reasoning about Metamodeling with Formal\nSpeciﬁcations and Automatic Proofs",
        "date": 2011,
        "abstract": "Metamodeling is foundational to many modeling frameworks, and so it is important to formalize and reason about it. Ideally, correct- ness proofs and test-case generation on the metamodeling framework should be automatic. However, it has yet to be shown that extensive au- tomated reasoning on metamodeling frameworks can be achieved. In this paper we present one approach to this problem: Metamodeling frame- works are speciﬁed modularly using algebraic data types and constraint logic programming (CLP). Proofs and test-case generation are encoded as CLP satisﬁability problems and automatically solved.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correctness of Model Synchronization Based on Triple Graph Grammars",
        "date": 2011,
        "abstract": "Triple graph grammars (TGGs) have been used successfully to ana- lyze correctness and completeness of bidirectional model transformations, but a corresponding formal approach to model synchronization has been missing. This paper closes this gap by providing a formal synchronization framework with bidi- rectional update propagation operations. They are generated from a TGG, which speciﬁes the language of all consistently integrated source and target models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Toolchain for the Detection of Structural and Behavioral Latent System Properties⋆",
        "date": 2011,
        "abstract": "The cost to repair a requirements-based defect in software- based systems increases substantially with each successive phase of the software lifecycle in which the error is allowed to propagate. While tools exist to facilitate early detection of design ﬂaws, such tools do not detect ﬂaws in system requirements, thus allowing such ﬂaws to propagate into system design and implementation. This paper describes an experience report using a toolchain that supports a novel combination of structural and behavioral analysis of UML state diagrams that is not currently available in commercial UML modeling tools. With the toolchain, mod- els can be incrementally and systematically improved through syntax- based analysis, type checking, and detection of latent behavioral system properties, including feature interactions. This paper demonstrates use of the toolchain on an industry-provided model of onboard electronics for an automotive application.",
        "keywords": [
            "requirements engineering",
            "UML",
            "latent properties",
            "model checking."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deﬁning MARTE’s VSL as an Extension of Alf",
        "date": 2011,
        "abstract": "VSL and Alf are two OMG standards providing a textual notation for complex mathematical expressions and detailed activities respectively. Since these two notations have been designed by separate communities (real-time embedded for VSL and software engineering for Alf), they diﬀer in syntax and semantics. Nevertheless, they clearly exhibit intersections in their form and use cases. The purpose of this article is to demonstrate that an alignment eﬀort between the two lan- guages would be beneﬁcial for both users and tool providers. We show that most of the syntactic constructs introduced in VSL are related to general-purpose concerns (i.e., they are not speciﬁc to the real-time do- main), most of them being covered by Alf. In this paper, we ﬁrst identify the subset of VSL which is valuable for the real-time domain, and then propose a way of extending Alf with this subset1.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using Delta Model for Collaborative Work of Industrial \nLarge-Scaled E/E Architecture Models",
        "date": 2011,
        "abstract": "Development of model-based Electric/Electronic (E/E) architecture in the automotive industry poses a high demand on the data management of models. The collaborative modeling work involves stakeholders dispersed across various locations and departments, while the models themselves are of- ten extremely large-scaled. In this paper, we present our approach addressing the model data management issue for both asynchronous and synchronous modeling. Compared to asynchronous modeling, which is based on the lock/commit mechanism for cross-department collaboration, synchronous mod- eling is targeted to assist quick and efficient interaction among small groups of members. We use the delta model for versioning in the database as well as for the synchronous modeling functionality. Furthermore, other versatile uses of the delta model such as the cumulative delta model and the reverse delta model are also introduced.",
        "keywords": [
            "Delta Model",
            "Collaborative Modeling",
            "Real-time Collaboration",
            "Versioning",
            "Groupware",
            "Computer Supported Cooperative Work (CSCW)."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bottom-Up Meta-Modelling:\nAn Interactive Approach",
        "date": 2012,
        "abstract": "The intensive use of models in Model-Driven Engineering (MDE) raises the need to develop meta-models with diﬀerent aims, like the construction of textual and visual modelling languages and the spec- iﬁcation of source and target ends of model-to-model transformations. While domain experts have the knowledge about the concepts of the do- main, they usually lack the skills to build meta-models. These should be tailored according to their future usage and speciﬁc implementation plat- form, which demands knowledge available only to engineers with great expertise in MDE platforms. These issues hinder a wider adoption of MDE both by domain experts and software engineers. In order to alleviate this situation we propose an interactive, iterative approach to meta-model construction enabling the speciﬁcation of model fragments by domain experts, with the possibility of using informal draw- ing tools like Dia. These fragments can be annotated with hints about the intention or needs for certain elements. A meta-model is automati- cally induced, which can be refactored in an interactive way, and then compiled into an implementation meta-model using proﬁles and patterns for diﬀerent platforms and purposes.",
        "keywords": [
            "Meta-Modelling",
            "Domain-Speciﬁc Modelling Languages",
            "Interactive Meta-Modelling",
            "Meta-Model Design Exploration."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "FacadeMetamodel: Masking UML",
        "date": 2012,
        "abstract": "UML profiling is pragmatic choice that lets language designers define a Domain-Specific Modeling Language (DSML) by tuning UML to meet specific domain. An alternative approach is to define a pure-DSML. Each approach has its own benefits and drawbacks. We propose an approach and a tool that helps get the best from both approaches; maximizing reuse while retaining a focused and adapted DSML. We guide the language designer in the definition of a metamodel based on one or more UML profiles. Language designers then recast UML so that only what they need will appear in this metamodel. From that, the tool automatically generates the pure-DSML and the transformations between it and UML. However, the new pure-DSML is only a facade; models can be manipulated using the pure-DSML abstract syntax but they are actually stored in fully-compliant UML abstract syntax and therefore remain compatible with UML tools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "T□: A Domain Speciﬁc Language for Rapid\nWorkﬂow Development",
        "date": 2012,
        "abstract": "In MDE, software systems are always synchronized with their models since changes are made ﬁrst to the model whenever there are changes in the requirement speciﬁcations. While MDE has a lot of po- tential, it requires maturity and tool support. In this research we present a framework for a workﬂow management system based on the MDE approach. We propose a domain speciﬁc language, T□(T-Square) for rapidly specifying details of (workﬂow) tasks and their associated user interfaces which may be used with the NOVA Workﬂow, an executable workﬂow management system. T□includes syntax for writing procedu- ral statements, for querying an ontology, for declaring user interfaces, for applying access control policy, and for scheduling tasks, using Xtext to write the grammar. We apply transformation methods, based on Xtend, to generate executable software from the abstract task speciﬁcations. A running example from health services delivery illustrates the usefulness of this approach.",
        "keywords": [
            "Workﬂow Management System",
            "Model Driven Engineering",
            "Ontology",
            "Domain Speciﬁc Language."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Xtend"
        }
    },
    {
        "title": "Relaxing Claims: Coping with Uncertainty\nWhile Evaluating Assumptions at Run Time",
        "date": 2012,
        "abstract": "Self-adaptation enables software systems to respond to chang- ing environmental contexts that may not be fully understood at design time. Designing a dynamically adaptive system (DAS) to cope with this uncertainty is challenging, as it is impractical during requirements anal- ysis and design time to anticipate every environmental condition that the DAS may encounter. Previously, the RELAX language was proposed to make requirements more tolerant to environmental uncertainty, and Claims were applied as markers of uncertainty that document how design assumptions aﬀect goals. This paper integrates these two techniques in order to assess the validity of Claims at run time while tolerating mi- nor and unanticipated environmental conditions that can trigger adap- tations. We apply the proposed approach to the dynamic reconﬁguration of a remote data mirroring network that must diﬀuse data while mini- mizing costs and exposure to data loss. Results show RELAXing Claims enables a DAS to reduce adaptation costs.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dynamic Evolution of Context-Aware Systems with Models at Runtime⋆",
        "date": 2012,
        "abstract": "Model-driven techniques have proven to yield signiﬁcant beneﬁts for context-aware systems. Speciﬁcally, semantically-rich models are used at runtime to monitor the system context and guide necessary changes. Under the closed- world assumption, adaptations are fully known at design time. Nevertheless, it is difﬁcult to foresee all the possible situations that may arise in uncertain and complex contexts. In this paper, we present a model-based framework to support the dynamic evolution of context-aware systems to deal with unexpected context events in the open world. If model adaptations are not enough to solve uncertainty, our model-based evolution planner guides the evolution of the supporting models to preserve high-level requirements. A case study about a context-aware Web service composition, which is executed in a distributed computing infrastructure, illustrates the applicability of our framework. A realization methodology and a prototype system support our approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Eclipse Modelling Framework Alternative\nto Meet the Models@Runtime Requirements",
        "date": 2012,
        "abstract": "Models@Runtime aims at taming the complexity of software dynamic adaptation by pushing further the idea of reﬂection and con- sidering the reﬂection layer as a ﬁrst-class modeling space. A natural approach to Models@Runtime is to use MDE techniques, in particular those based on the Eclipse Modeling Framework. EMF provides facilities for building DSLs and tools based on a structured data model, with tight integration with the Eclipse IDE. EMF has rapidly become the defacto standard in the MDE community and has also been adopted for building Models@Runtime platforms. For example, Frascati (implementing the Service Component Architecture standard) uses EMF for the design and runtime tooling of its architecture description language. However, EMF has primarily been thought to support design-time activities. This paper highlights speciﬁc Models@Runtime requirements, discusses the bene- ﬁts and limitations of EMF in this context, and presents an alternative implementation to meet these requirements.",
        "keywords": [
            "Model@Runtime",
            "EMF",
            "adaptation."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated and Transparent Model\nFragmentation for Persisting Large Models",
        "date": 2012,
        "abstract": "Existing model persistence frameworks either store models as a whole or object by object. Since most modeling tasks work with larger aggregates of a model, existing persistence frameworks either load too many objects or access many objects individually. We propose to persist a model broken into larger fragments. First, we assess the size of large models and describe typical usage patterns to show that most applications work with aggregates of model objects. Secondly, we provide an analytical framework to assess execution time gains for partially loading models fragmented with diﬀerent gran- ularity. Thirdly, we propose meta-model-based fragmentation that we implemented in an EMF based framework. Fourthly, we analyze our ap- proach in comparison to other persistence frameworks based on four com- mon modeling tasks: create/modify, traverse, query, and partial loads. We show that there is no generally optimal fragmentation, that frag- mentation can be achieved automatically and transparently, and that fragmentation provides considerable performance gains.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formally Deﬁning and Iterating Inﬁnite Models ⋆",
        "date": 2012,
        "abstract": "The wide adoption of MDE raises new situations where we need to manipulate very large models or even inﬁnite model streams gathered at runtime (e.g., monitoring). These new uses cases for MDE raise challenges that had been unforeseen by the time standard modeling framework were designed. This paper proposes a formal deﬁnition of an inﬁnite model, as well as a formal framework to reason on queries over inﬁnite models. This formal query deﬁnition aims at supporting the design and veriﬁcation of operations that manipulate inﬁnite mod- els. First, we precisely identify the MOF parts which must be reﬁned to support inﬁnite structure. Then, we provide a formal coinductive deﬁnition dealing with unbounded and potentially inﬁnite graph-based structure.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Query-Driven Soft Interconnection of EMF Models⋆",
        "date": 2012,
        "abstract": "Model repositories based on the Eclipse Modeling Framework (EMF) play a central role in the model-driven development of complex software-intensive systems by oﬀering means to persist and manipulate models obtained from heterogeneous languages and tools. Complex EMF models can be assembled by interconnecting model fragments by hard links, i.e. regular references, where the target end points to external re- sources using storage-speciﬁc URIs. This approach, in certain application scenarios, may prove to be a too rigid and error prone way of interlinking models. As a ﬂexible alternative, we propose to combine derived features of EMF models with advanced incremental model queries as means for soft interlinking of model elements residing in diﬀerent model resources. These soft links can be calculated on-demand with graceful handling for temporarily unresolved references. In the background, the interlinks are maintained eﬃciently and ﬂexibly by using incremental model queries as provided by the EMF-IncQuery framework.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling the Linguistic Architecture\nof Software Products",
        "date": 2012,
        "abstract": "Understanding modern software products is challenging along several dimensions. In the past, much attention has been focused on the logical and physical architecture of the products in terms of the rele- vant components, features, ﬁles, and tools. In contrast, in this paper, we focus on the linguistic architecture of software products in terms of the involved software languages and related technologies, and technolog- ical spaces with linguistic relationships such as membership, subset, or conformance. We develop a designated form of megamodeling with cor- responding language and tool support. An important capability of the megamodeling approach is that entities and relationships of the meg- amodel are linked to illustrative software artifacts. This is particularly important during the understanding process for validation purposes. We demonstrate such megamodeling for a technology for Object/XML map- ping. This work contributes to the 101companies community project.",
        "keywords": [
            "Megamodel",
            "Linguistic architecture",
            "Software language",
            "Soft- ware technology",
            "Technological space",
            "Object/XML mapping",
            "MegaL."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Cross-Language Support Mechanisms\nSigniﬁcantly Aid Software Development",
        "date": 2012,
        "abstract": "Contemporary software systems combine many artifacts speciﬁed in various modeling and programming languages, domain- speciﬁc and general purpose as well. Since multi-language systems are so widespread, working on them calls for tools with cross-language support mechanisms such as (1) visualization, (2) static checking, (3) navigation, and (4) refactoring of cross-language relations. We investigate whether these four mechanisms indeed improve eﬃciency and quality of devel- opment of multi-language systems. We run a controlled experiment in which 22 participants perform typical software evolution tasks on the JTrac web application using a prototype tool implementing these mecha- nisms. The results speak clearly for integration of cross-language support mechanisms into software development tools, and justify research on au- tomatic inference, manipulation and handling of cross-language relations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Do Professional Developers Beneﬁt from Design\nPattern Documentation? A Replication in the\nContext of Source Code Comprehension",
        "date": 2012,
        "abstract": "We present the results of a diﬀerentiated replication conducted with professional developers to assess whether the presence and the kind of documentation for the solutions or instances of design patterns aﬀect source code comprehension. The participants were di- vided into three groups and asked to comprehend a chunk of the JHot- Draw source code. Depending on the group, each participant was or not provided with the graphical and textual representations of the design pattern instances implemented within that source code. In the case of graphically documented instances, we used UML class diagrams, while textually documented instances are reported as comment in the source code. The results revealed that participants provided with the docu- mentation of the instances achieved a signiﬁcantly better comprehension than the participants with source code alone. The eﬀect of the kind of documentation is not statistically signiﬁcant.",
        "keywords": [
            "Design Patterns",
            "Controlled Experiment",
            "Maintenance",
            "Replications",
            "Software Models",
            "Source Code Comprehension."
        ],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Incremental Consistency Checking for Complex Design\nRules and Larger Model Changes",
        "date": 2012,
        "abstract": "Advances in consistency checking in model-based software develop- ment made it possible to detect errors in real-time. However, existing approaches assume that changes come in small quantities and design rules are generally small in scope. Yet activities such as model transformation, re-factoring, model merg- ing, or repairs may cause larger model changes and hence cause performance problems during consistency checking. The goal of this work is to increase the performance of re-validating design rules. This work proposes an automated and tool supported approach that re-validates the affected parts of a design rule only. It was empirical evaluated on 19 design rules and 30 small to large design models and the evaluation shows that the approach improves the computational cost of consistency checking with the gains increasing with the size and complexity of design rules.",
        "keywords": [
            "consistency checking",
            "performance",
            "incremental checking."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating the Impact of Aspects on Inconsistency \nDetection Effort: A Controlled Experiment",
        "date": 2012,
        "abstract": "Design models represent modular realizations of stakeholders’ con- cerns and communicate the design decisions to be implemented by developers. Unfortunately, they often suffer from inconsistency problems. Aspect-oriented modeling (AOM) aims at promoting better modularity. However, there is no empirical knowledge about its impact on the inconsistency detection effort. To address this gap, this work investigates the effects of AOM on: (1) the develop- ers’ effort to detect inconsistencies; (2) the inconsistency detection rate; and (3) the interpretation of design models in the presence of inconsistencies. A con- trolled experiment was conducted with 26 subjects and involved the analysis of 520 models. The results, supported by statistical tests, show that the effort of detecting inconsistencies is 20 percent lower in AO models than in their OO counterparts. On the other hand, the inconsistency detection rate and the num- ber of misinterpretations are 43 and 37 percent higher in AO models than in OO models, respectively.",
        "keywords": [
            "Aspect-Oriented Modeling",
            "Model Composition",
            "Inconsistency",
            "Developer Effort",
            "Empirical Studies."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Integrating\nStructure and Behavior Modeling with OCL",
        "date": 2012,
        "abstract": "Precise modeling with UML and OCL traditionally focuses on structural model features like class invariants. OCL also allows the developer to handle behavioral aspects in form of operation pre- and postconditions. However, behavioral UML models like statecharts have rarely been integrated into UML and OCL modeling tools. This pa- per discusses an approach that combines precise structure and behav- ior modeling: Class diagrams together with class invariants restrict the model structure and protocol state machines constrain the model behav- ior. Protocol state machines can take advantage of OCL in form of OCL state invariants and OCL guards and postconditions for state transitions. Protocol state machines can cover complete object lifecycles in contrast to operation pre- and postconditions which only aﬀect single operation calls. The paper reports on the chosen UML language features and their implementation in a UML and OCL validation and veriﬁcation tool.",
        "keywords": [
            "Structure modeling",
            "Behavior modeling",
            "UML",
            "OCL",
            "Pro- tocol state machine",
            "State invariant",
            "Guard",
            "Transition postcondition."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multi-perspectives on Feature Models",
        "date": 2012,
        "abstract": "Domain feature models concisely express commonality and variability among variants of a software product line. For supporting separation of concerns, e.g., due to legal restrictions, technical consid- erations and business requirements, multi-view approaches restrict the conﬁguration choices on feature models for diﬀerent stakeholders. How- ever, recent approaches lack a formalization for precise, yet ﬂexible spec- iﬁcations of views that ensure every derivable conﬁguration perspective to obey feature model semantics. Here, we introduce a novel approach for preconﬁguring feature models to create multi-perspectives. Such cus- tomized perspectives result from composition of various concern-relevant views. A structured view model is used to organize features in view groups, wherein a feature may be contained in multiple views. We pro- vide formalizations for view composition and guaranteed consistency of perspectives w.r.t. feature model semantics. Thereupon, an eﬃcient algo- rithm to verify consistency for entire multi-perspectives is provided. We present an implementation and evaluate our concepts by means of various experiments.",
        "keywords": [
            "Software Product Lines",
            "Feature Models",
            "Preconﬁguration",
            "Customization",
            "Automated View Composition."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating Better Partial Covering Arrays\nby Modeling Weights on Sub-product Lines",
        "date": 2012,
        "abstract": "Combinatorial interaction testing is an approach for testing product lines. A set of products to test can be set up from the cover- ing array generated from a feature model. The products occurring in a partial covering array, however, may not focus on the important feature interactions nor resemble any actual product in the market. Knowledge about which interactions are prevalent in the market can be modeled by assigning weights to sub-product lines. Such models enable a covering array generator to select important interactions to cover ﬁrst for a par- tial covering array, enable it to construct products resembling those in the market and enable it to suggest simple changes to an existing set of products to test for incremental adaption to market changes. We report experiences from the application of weighted combinatorial interaction testing for test product selection on an industrial product line, TOMRA’s Reverse Vending Machines.",
        "keywords": [
            "Product Lines",
            "Software",
            "Hardware",
            "Testing",
            "Combinatorial Interaction Testing",
            "Evolution."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Business Application Product Lines",
        "date": 2012,
        "abstract": "With continued increase in business dynamics, it is becoming increa- singly harder to deliver purpose-specific business systems in the ever-shrinking window of opportunity. Code-centric software product line engineering (SPLE) techniques show unacceptable responsiveness as business applications are sub- jected to changes along multiple dimensions that continue to evolve simulta- neously. Through clear separation of functional concerns from technology, model-driven approaches enable easy delivery of the same functionality into multiple technology platforms. However, business systems for same functional intent tend to have similar but non-identical functionality. This makes a strong case for bringing in SPLE ideas i.e., what can change where and when, to mod- els. We propose an abstraction that aims to address composition, variability and resolution in a unified manner; describe its model-based realization; and outline the key enablers necessary for raising business application product lines. Early experience of our approach and issues that remain to be addressed for industry acceptance are highlighted.",
        "keywords": [
            "software product lines",
            "model driven engineering."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Inter-association Constraints in UML2: Comparative Analysis, Usage Recommendations, and Modeling Guidelines",
        "date": 2012,
        "abstract": "UML speciﬁcation is verbal and imprecise, the exact mean- ing of many class diagram constructs and their interaction is still obscure. There are major problems with the inter-association constraints subsets, union, redeﬁnition, association specialization, association-class special- ization. Although their standard semantics is ambiguous and their inter- action unclear, the UML meta-model intensively uses these constraints.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Coroutine Model of Computation",
        "date": 2012,
        "abstract": "This paper presents a general denotational formalism called the Coroutine Model of Computation for control-oriented computational models. This formalism characterizes atomic elements with control be- havior as Continuation Actors, giving them a static semantics with a functional interface. Coroutine Models are then deﬁned as networks of Continuation Actors, representing a set of control locations between which control traverses during execution. This paper gives both a strict and non-strict denotational semantics for Coroutine Models in terms of compositions of Continuation Actors and their interfaces. In the strict form, the traversal of control locations forms a control path producing output values, whereas in the non-strict form, execution traverses a tree of potential control locations producing partial information about out- put values. Furthermore, the given non-strict form of these semantics is claimed to have useful monotonicity properties.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assume-Guarantee Scenarios: Semantics and Synthesis⋆",
        "date": 2012,
        "abstract": "The behavior of open reactive systems is best described in an assume-guarantee style speciﬁcation: a system guarantees certain prescribed behavior provided that its environment follows certain given assumptions. Scenario-based modeling languages, such as variants of message sequence charts, have been used to specify reactive systems behavior in a visual, modular, intuitive way. However, none have yet provided full support for assume-guarantee style speciﬁcations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Exploratory Study of Forces and Frictions\nAﬀecting Large-Scale Model-Driven\nDevelopment",
        "date": 2012,
        "abstract": "In this paper, we investigate model-driven engineering, re- porting on an exploratory case-study conducted at a large automotive company. The study consisted of interviews with 20 engineers and man- agers working in diﬀerent roles. We found that, in the context of a large organization, contextual forces dominate the cognitive issues of using model-driven technology. The four forces we identiﬁed that are likely in- dependent of the particular abstractions chosen as the basis of software development are the need for diﬃng in software product lines, the needs for problem-speciﬁc languages and types, the need for live modeling in ex- ploratory activities, and the need for point-to-point traceability between artifacts. We also identiﬁed triggers of accidental complexity, which we refer to as points of friction introduced by languages and tools. Examples of the friction points identiﬁed are insuﬃcient support for model diﬃng, point-to-point traceability, and model changes at runtime.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Driven Approach to Support \nEngineering Changes in Industrial Robotics Software",
        "date": 2012,
        "abstract": "Software development has improved greatly over the past decades with the introduction of new programming languages and tools. However, software development in the context of industrial robotics is dominated by practices that require attention to low-level accidental complexities related to the solution space of a particular domain. Most vendor-specific robotics platforms force the developer to be concerned with many low-level implementation details, which presents a maintenance challenge in the context of making engineering changes to the robotics solution. Additionally, satisfying the timing requirements across the platforms of multiple robot vendors represents an additional challenge. We introduce our work using Domain- Specific Modeling to support the control of industrial robots using models that are at a higher level of abstraction than traditional robot programming languages. Our modeling approach assists robotics developers to plan the schedule, validate timing requirements, optimize robot control, handle engineering changes, and support multiple platforms.",
        "keywords": [
            "Domain-Specific Modeling",
            "Robotics",
            "Software Maintenance",
            "Digital Factory",
            "Digital Master."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Managing Related Models in Vehicle Control\nSoftware Development",
        "date": 2012,
        "abstract": "Model management is critical for large software-intensive system development as it ensures the consistency and correctness of the models that are separately developed but interrelated. It is especially crucial when the models are acquired from diﬀerent sources and evolve frequently. Traditional approaches to model management in vehicle con- trol software development rely on information examination guarded by a rigorous development process, which requires a high-level of knowledge and may be less eﬀective than is desirable. To address this issue, we in- vestigate the applicability of the macromodel concept – a formal method for the speciﬁcation of model relationships – to model management of vehicle control system development. Through studying some represen- tative relationships, we build a macromodel based management method and demonstrate its eﬀectiveness using the ﬂow diagrams in a functional architecture model from industry.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Detecting Speciﬁcation Errors in Declarative\nLanguages with Constraints",
        "date": 2012,
        "abstract": "Declarative speciﬁcation languages with constraints are used in model-driven engineering to specify formal semantics, deﬁne model transformations, and describe domain constraints. While these languages support concise speciﬁcations, they are nevertheless prone to diﬃcult se- mantic errors. In this paper we present a type-theoretic approach to the static detection of speciﬁcation errors. Our approach infers approx- imations of satisfying assignments and represents them via a canonical regular type system. Type inference is experimentally eﬃcient and type judgments are comprehensible by the user.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From UML and OCL\nto Relational Logic and Back",
        "date": 2012,
        "abstract": "Languages like UML and OCL are used to precisely model systems. Complex UML and OCL models therefore represent a crucial part of model-driven development, as they formally specify the main sys- tem properties. Consequently, creating complete and correct models is a critical concern. For this purpose, we provide a lightweight model valida- tion method based on eﬃcient SAT solving techniques. In this paper, we present a transformation from UML class diagram and OCL concepts into relational logic. Relational logic in turn represents the source for advanced SAT-based model instance ﬁnders like Kodkod. This paper fo- cuses on a natural transformation approach which aims to exploit the features of relational logic as directly as possible through straitening the handling of main UML and OCL features. This approach allows us to explicitly beneﬁt from the eﬃcient handling of relational logic in Kodkod and to interpret found results backwards in terms of UML and OCL.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Verifying ATL Transformations\nUsing ‘off-the-shelf’ SMT Solvers",
        "date": 2012,
        "abstract": "MDE is a software development process where models constitute pivotal elements of the software to be built. If models are well-speciﬁed, trans- formations can be employed for various purposes, e.g., to produce ﬁnal code. However, transformations are only meaningful when they are ‘correct’: they must produce valid models from valid input models. A valid model has conformance to its meta-model and fulﬁls its constraints, usually written in OCL. In this paper, we propose a novel methodology to perform automatic, unbounded veriﬁcation of ATL transformations. Its main component is a novel ﬁrst-order semantics for ATL transformations, based on the interpretation of the corresponding rules and their execution semantics as ﬁrst-order predicates. Although, our semantics is not complete, it does cover a signiﬁcant subset of the ATL language. Using this se- mantics, transformation correctness can be automatically veriﬁed with respect to non-trivial OCL pre- and postconditions by using SMT solvers, e.g. Z3 and Yices.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "ATLTest: A White-Box Test Generation\nApproach for ATL Transformations",
        "date": 2012,
        "abstract": "MDE is being applied to the development of increasingly complex systems that require larger model transformations. Given that the speciﬁcation of such transformations is an error-prone task, techniques to guarantee their quality must be provided. Testing is a well-known technique for ﬁnding errors in programs. In this sense, adop- tion of testing techniques in the model transformation domain would be helpful to improve their quality. So far, testing of model transforma- tions has focused on black-box testing techniques. Instead, in this paper we provide a white-box test model generation approach for ATL model transformations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "ATL"
        }
    },
    {
        "title": "Empirical Evaluation on FBD Model-Based Test\nCoverage Criteria Using Mutation Analysis",
        "date": 2012,
        "abstract": "Function Block Diagram (FBD), one of the PLC program- ming languages, is a graphical modeling language which has been increas- ingly used to implement safety-critical software such as nuclear reactor protection software. With increased importance of structural testing for FBD models, FBD model-based test coverage criteria have been intro- duced. In this paper, we empirically evaluate the fault detection eﬀective- ness of the FBD coverage criteria using mutation analysis. We produce 1800 test suites satisfying the FBD criteria and generate more than 600 mutants automatically for the target industrial FBD models. Then we evaluate mutant detection of the test suites to assess the fault detection eﬀectiveness of the coverage criteria. Based on the experimental results, we analyze strengths and weaknesses of the FBD coverage criteria, and suggest possible improvements for the test coverage criteria.",
        "keywords": [
            "Function block diagram",
            "mutation analysis",
            "test coverage criteria."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Seeing Errors:\nModel Driven Simulation Trace Visualization",
        "date": 2012,
        "abstract": "Powerful theoretical frameworks exist for model validation and veriﬁcation, yet their use in concrete projects is limited. This is partially due to the fact that the results of model veriﬁcation and sim- ulation are diﬃcult to exploit. This paper reports on a model driven approach that supports the user during the error diagnosis phases, by allowing customizable simulation trace visualization. Our thesis is that we can use models to signiﬁcantly improve the information visualiza- tion during the diagnosis phase. This thesis is supported by Metaviz - a model-driven framework for simulation trace visualization. Metaviz uses the IFx-OMEGA model validation platform and a state-of-the-art information visualization reference model together with a well-deﬁned development process guiding the user into building custom visualiza- tions,essentially by deﬁning model transformations. This approach has the potential to improve the practical usage of modeling techniques and to increase the usability and attractiveness of model validation tools.",
        "keywords": [
            "Software visualization",
            "trace exploration",
            "embedded sys- tems",
            "model based validation",
            "model dynamic analysis."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Modeling Approach to Support\nthe Similarity-Based Reuse of Conﬁguration Data",
        "date": 2012,
        "abstract": "Product conﬁguration in families of Integrated Control Sys- tems (ICSs) involves resolving thousands of conﬁgurable parameters and is, therefore, time-consuming and error-prone. Typically, these systems consist of highly similar components that need to be conﬁgured similarly. For large-scale systems, a considerable portion of the conﬁguration data can be reused, based on such similarities, during the conﬁguration of each individual product. In this paper, we propose a model-based approach to automate the reuse of conﬁguration data based on the similarities within an ICS product. Our approach enables conﬁguration engineers to manip- ulate the reuse of conﬁguration data, and ensures the consistency of the reused data. Evaluation of the approach, using a number of conﬁgured products from an industry partner, shows that more than 60% of con- ﬁguration data can be automatically reused using our similarity-based approach, thereby reducing conﬁguration eﬀort.",
        "keywords": [
            "Product conﬁguration",
            "Internal similarities",
            "Model-based software engineering",
            "UML/OCL",
            "Feature Modeling."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Driven Configuration of Fault Tolerance Solutions \nfor Component-Based Software System",
        "date": 2012,
        "abstract": "Fault tolerance is very important for complex component-based software systems, but its configuration is complicated and challenging. In this paper, we propose a model driven approach to semi-automatic configuration of fault tolerance solutions. At design time, a set of reusable fault tolerance solu- tions are modeled as architecture styles, with the key properties verified by model checking. At runtime, the runtime software architecture of the target sys- tem is automatically constructed by the code generated from the given architec- tural meta-model. Then, the impact of each component on the system reliability is automatically analyzed to recommend which components should be consi- dered in the fault tolerance configuration. Finally, after which components are guaranteed by what fault tolerance solution is decided by the system administra- tion, the architecture model is automatically changed by merging with the selected fault tolerance styles and finally, these changes are automatically propagated to the target system. This approach is evaluated on Java enterprise systems.",
        "keywords": [
            "fault tolerance",
            "component-based system",
            "dynamic configuration",
            "mode driven approach",
            "software architecture."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying a Consistency Checking Framework\nfor Heterogeneous Models and Artifacts\nin Industrial Product Lines",
        "date": 2012,
        "abstract": "Product line engineering relies on heterogeneous models and artifacts to deﬁne and implement the product line’s reusable assets. The complexity and heterogeneity of product line artifacts as well as their interdependencies make it hard to maintain consistency during develop- ment and evolution, regardless of the modeling approaches used. Engi- neers thus need support for detecting and resolving inconsistencies within and between the various artifacts. In this paper we present a framework for checking and maintaining consistency of arbitrary product line arti- facts. Our approach is ﬂexible and extensible regarding the supported artifact types and the deﬁnition of constraints. We discuss tool support developed for the DOPLER product line tool suite. We report the re- sults of applying the approach to sales support applications of industrial product lines.",
        "keywords": [
            "Model-based product lines",
            "consistency checking",
            "sales support."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generation of Operational Transformation Rules\nfrom Examples of Model Transformations",
        "date": 2012,
        "abstract": "Model transformation by example (MTBE) aims at deﬁning a model transformation according to a set of examples of this transforma- tion. Examples are given in the form of pairs, each having an input model and its corresponding output transformed model, with the transforma- tion traces. The transformation rules are then automatically extracted from the examples. In this paper, we propose a two-step approach to gen- erate the transformation rules. In a ﬁrst step, transformation patterns are learned from the examples through a classiﬁcation of the model el- ements of the examples, and a classiﬁcation of the transformation links using Formal Concept Analysis. In a second step, those transformation patterns are analyzed in order to select the more pertinent ones and to transform them into operational transformation rules written for the Jess rule engine. The generated rules are then executed on examples to evaluate their relevance through classical precision/recall measures.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Jess"
        }
    },
    {
        "title": "Using Feature Model to Build Model\nTransformation Chains",
        "date": 2012,
        "abstract": "Model transformations are intrinsically related to model- driven engineering. According to the increasing size of standardised meta- model, large transformations need to be developed to cover them. Several approaches promote separation of concerns in this context, that is, the deﬁnition of small transformations in order to master the overall com- plexity. Unfortunately, the decomposition of transformations into smaller ones raises new issues: organising the increasing number of transforma- tions and ensuring their composition (i.e. the chaining). In this paper, we propose to use feature models to classify model transformations ded- icated to a given business domain. Based on this feature models, au- tomated techniques are used to support the designer, according to two axis: (i) the deﬁnition of a valid set of model transformations and (ii) the generation of an executable chain of model transformation that ac- curately implement designer’s intention. This approach is validated on Gaspard2, a tool dedicated to the design of embedded system.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Generic Approach Simplifying\nModel-to-Model Transformation Chains",
        "date": 2012,
        "abstract": "The model-driven architecture proposes stepwise model re- ﬁnement. The resulting model-to-model (M2M) transformation chains can consist of many steps. For realizing the transformations two ap- proaches exist: Exogenous transformations, where input and output use diﬀerent metamodels, and endogenous transformations, that use the same metamodel for input and output. Due to the particularities of embedded systems, using only endogenous transformations is not ap- propriate. For exogenous transformations, problems arise with respect to creation and maintenance of the subsequent metamodels. Another problem of these M2M transformation chains is that for one transforma- tion step typically large parts of the model data remain unchanged. The resulting M2M transformation does often include many copy operations that distract the developers from the “real” transformations and increase implementation overhead. This paper introduces a generic approach that solves these issues by a (semi-) automatic metamodel construction and copy operation of unchanged model data between subsequent steps.",
        "keywords": [
            "Transformation Chain",
            "Model-to-Model Transformation",
            "Metamodel-to-Metamodel Transformation",
            "Model-driven Software Development",
            "Model-driven Architecture."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Approach for Synchronizing UML Models\nand Narrative Text in Literate Modeling",
        "date": 2012,
        "abstract": "A major challenge in adopting UML in industrial environ- ments is the lack of accessibility and comprehensibility of some diagram types by non-technical stakeholders. Literate Modeling improves compre- hension of these diagrams by adding narrative text, but lacks good tool support for synchronizing model and text. This paper presents an ap- proach for keeping model and text synchronized by eﬀectively combining state-of-the-art natural language processing technology with OCL model querying. Thereby, consistency of element names in the UML model with their counterparts in the text is achieved by using text annotations to provide the semantic link. At a structural level, we propose an algorithm that checks element relationships in the UML model using a set of vali- dation constraints when particular sentence characteristics are detected. An analysis of the runtime complexity shows the feasibility of including the proposed solution in one of today’s CASE tools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Model Matching for Trace Link Generation in Model-Driven Software Development",
        "date": 2012,
        "abstract": "With the advent of Model-driven Software Engineering, the advantage of generating trace links between source and target model elements automatically, eases the problem of creating and maintaining traceability data. Yet, an existing transformation engine as in the above case is not always given in model-based development, (i.e. when transfor- mations are implemented manually) and can not be leveraged for the sake of trace link generation through the transformation mapping. We tackle this problem by using model matching techniques to generate trace links for arbitrary source and target models. Thereby, our approach is based on a novel, language-agnostic concept deﬁning three similarity measures for matching. To achieve this, we exploit metamodel matching techniques for graph-based model matching. Furthermore, we evaluate our approach according to large-scale SAP business transformations and the ATL Zoo.",
        "keywords": [
            "Traceability",
            "Model Matching",
            "Software Quality."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Matching Business Process Workﬂows\nacross Abstraction Levels",
        "date": 2012,
        "abstract": "In Business Process Modeling, several models are deﬁned for the same system, supporting the transition from business require- ments to IT implementations. Each of these models targets a diﬀerent abstraction level and stakeholder perspective. In order to maintain con- sistency among these models, which has become a major challenge not only in this ﬁeld, the correspondence between them has to be identiﬁed. A correspondence between process models establishes which activities in one model correspond to which activities in another model. This pa- per presents an algorithm for determining such correspondences. The algorithm is based on an empirical study of process models at a large company in the banking sector, which revealed frequent correspondence patterns between models spanning multiple abstraction levels. The algo- rithm has two phases, ﬁrst establishing correspondences based on similar- ity of model element attributes such as types and names and then reﬁning the result based on the structure of the models. Compared to previous work, our algorithm can recover complex correspondences relating whole process fragments rather than just individual activities. We evaluate the algorithm on 26 pairs of business-technical and technical-IT level mod- els from four real-world projects, achieving overall precision of 93% and recall of 70%. Given the substantial recall and the high precision, the al- gorithm helps automating signiﬁcant part of the correspondence recovery for such models.",
        "keywords": [
            "BPMN Matching",
            "Consistency Management",
            "Change Extraction."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Experiences of Applying UML/MARTE  \non Three Industrial Projects",
        "date": 2012,
        "abstract": "MARTE (Modeling and Analysis of Real-Time and Embedded Sys- tems) is a UML profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In previous years, we have applied UML/MARTE to three distinct industrial problems in various industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication- intensive systems, and model-based environment simulator generation of large- scale RTES for testing. In this paper, we report on our experiences of solving these problems by applying UML/MARTE on four industrial case studies. Based on our common experiences, we derive a framework to help practitioners for fu- ture applications of UML/MARTE. The framework provides a set of detailed guidelines on how to apply MARTE in industrial contexts and will help reduce the gap between the modeling standards and industrial needs.",
        "keywords": [
            "UML",
            "MARTE",
            "Real-time Embedded Systems",
            "Architecture  Modeling",
            "Model-based Testing."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Cost Estimation for Model-Driven Engineering",
        "date": 2012,
        "abstract": "Cost estimation studies in model-driven engineering (MDE) are scarce; ﬁrst, due to diﬃculty in quantifying qualitative characteris- tics of MDE that supposedly inﬂuence software development eﬀort and second, due to the complexity of measuring varied artifacts that are gen- erated and used in an end-to-end MDE toolset. A cost estimation ap- proach is therefore needed that can incorporate characteristics of MDE that aﬀect economies of scale and eﬀort in application development with the size computation of various artifacts in MDE. We plan to use the con- structive cost model (COCOMO) II to obtain baseline cost estimation of MDE applications. Our main contributions are a method to capture the qualitative characteristics of MDE in terms of cost drivers in COCOMO II and a method for computation of various artifacts generated by an MDE toolset. Our initial exploration of these ideas suggests that it is possible to automate cost estimation for MDE.",
        "keywords": [
            "Model-driven Engineering",
            "Cost Estimation",
            "COCOMO II."
        ],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Evaluating the Effort of Composing Design Models:  \nA Controlled Experiment",
        "date": 2012,
        "abstract": "The lack of empirical knowledge about the effects of model composi- tion techniques on developers’ effort is the key impairment for their widespread adoption in practice. This problem applies to both existing categories of model composition techniques, i.e. specification-based (e.g. Epsilon) and heuristic- based (e.g. IBM RSA) techniques. This paper reports on a controlled experiment that investigates the effort to: (1) apply both categories of model composition techniques, and (2) detect and resolve inconsistencies in the output composed models. The techniques are investigated in 144 evolution scenarios, where 2304 compositions of elements of class diagrams were produced. The results suggest that: (1) the employed heuristic-based techniques require less effort to produce the intended model than the chosen specification-based technique, (2) the cor- rectness of the output composed models generated by the techniques is not sig- nificantly different, and (3) the use of manual heuristics for model composition outperforms their automated counterparts.",
        "keywords": [
            "Model composition effort",
            "empirical studies",
            "effort measurement."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Transition to Model-Driven Engineering What Is Revolutionary, What Remains the Same?",
        "date": 2012,
        "abstract": "A considerable amount of research has been dedicated to bring the vision of model-driven engineering (MDE) to fruition. How- ever, the practical experiences of organizations that transition to MDE are underreported. This paper presents a case study of the organizational consequences experienced by one large organization after transitioning to MDE. We present four ﬁndings from our case study. First, MDE brings development closer to the domain experts, but software engineers are still necessary for many tasks. Second, though MDE presents an opportunity to achieve incremental improvements in productivity, the organizational challenges of software development remain unchanged. Third, switch- ing to MDE may disrupt the balance of the organizational structure, creating morale and power problems. Fourth, the cultural and institu- tional infrastructure of MDE is underdeveloped, and until MDE becomes better established, transitioning organizations need to exert additional adoption eﬀorts. We oﬀer several observations of relevance to researchers and practitioners based on these ﬁndings.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards an Automatic Service Discovery\nfor UML-Based Rich Service Descriptions",
        "date": 2012,
        "abstract": "Service-oriented computing (SOC) promises to solve many issues in the area of distributed software development, e.g. the realization of the loose coupling pattern in practice through service discovery and in- vocation. For this purpose, service descriptions must comprise structural as well as behavioral information of the services otherwise an accurate service discovery is not possible. We addressed this issue in our previ- ous paper and proposed a UML-based rich service description language (RSDL) providing comprehensive notations to specify service requests and oﬀers.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Product Line Modeling and Configuration \nMethodology to Support Model-Based Testing:  \nAn Industrial Case Study",
        "date": 2012,
        "abstract": "Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to sup- port systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Con- ferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behav- ioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.",
        "keywords": [
            "Aspect-Oriented Modeling",
            "Product Line Engineering",
            "Behavioral  Variability",
            "Model-based Testing",
            "UML State Machine."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Sensitivity Analysis in Model-Driven\nEngineering",
        "date": 2012,
        "abstract": "Sensitivity analysis has been used in scientiﬁc research to ex- plore the validity of models. Software engineering is inherently uncertain; we propose that sensitivity analysis can be used to analyse and quantify the eﬀects of uncertainty when model management operations are applied to models. In this paper, we consider forms and measures of uncertainty in software engineering models. Focusing on data uncertainty, we present a framework for sensitivity analysis, and create an instantiation of the framework for the CATMOS decision-support tool. We show how this can be used to qualify the output of the entailed model management operations and thus improve both the conﬁdence and understanding of models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling and Analysis of CPU Usage in Safety-Critical\nEmbedded Systems to Support Stress Testing",
        "date": 2012,
        "abstract": "Software safety certiﬁcation needs to address non-functional constraints with safety implications, e.g., deadlines, throughput, and CPU and memory usage. In this paper, we focus on CPU usage constraints and provide a framework to support the derivation of test cases that maximize the chances of vi- olating CPU usage requirements. We develop a conceptual model specifying the generic abstractions required for analyzing CPU usage and provide a mapping between these abstractions and UML/MARTE. Using this model, we formulate CPU usage analysis as a constraint optimization problem and provide an imple- mentation of our approach in a state-of-the-art optimization tool. We report an application of our approach to a case study from the maritime and energy do- main. Through this case study, we argue that our approach (1) can be applied with a practically reasonable overhead in an industrial setting, and (2) is effective for identifying test cases that maximize CPU usage.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Weaving-Based Conﬁguration and Modular Transformation of Multi-layer Systems⋆",
        "date": 2012,
        "abstract": "In model-driven development of multi-layer systems (e.g. application, platform and infrastructure), each layer is usually described by separate models. When generating analysis models or code, these separate models ﬁrst of all need to be linked. Hence, existing model transformations for single layers cannot be simply re-used.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Research-Based Innovation: A Tale of Three\nProjects in Model-Driven Engineering",
        "date": 2012,
        "abstract": "In recent years, we have been exploring ways to foster a closer collaboration between software engineering research and industry both to align our research with practical needs, and to increase awareness about the importance of research for innovation. This paper outlines our expe- rience with three research projects conducted in collaboration with the industry. We examine the way we collaborated with our industry part- ners and describe the decisions that contributed to the eﬀectiveness of the collaborations. We report on the lessons learned from our experience and illustrate the lessons using examples from the three projects. The lessons focus on the applications of Model-Driven Engineering (MDE), as all the three projects we draw on here were MDE projects. Our goal from structuring and sharing our experience is to contribute to a better understanding of how researchers and practitioners can collaborate more eﬀectively and to gain more value from their collaborations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Industrial System Engineering Process\nIntegrating Model Driven Architecture and\nModel Based Design",
        "date": 2012,
        "abstract": "We present an industrial model-driven engineering process for the design and development of complex distributed embedded systems. We outline the main steps in the process and the evaluation of its use in the context of a radar application. We show the meth- ods and tools that have been developed to allow interoperability among requirements management, SysML modeling and MBD simulation and code generation.",
        "keywords": [
            "System Engineering",
            "Model-Driven Architecture",
            "Model- Based Design",
            "Platform-Based Design."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Magic of Software",
        "date": 2013,
        "abstract": "Software allows for many models of computation. We cre- ate models to understand and reason about these computations (e.g., did the aircraft change its course because there was a hill in front of it or because a model indicated the presence of a hill?). As computers and software become more and more ubiquitous, the tangible world and computer models of the world are merging. We are re-designing our basic systems from networks, cars and aircrafts, to ﬁnancial and health sys- tems to reduce their costs and increase their eﬀectiveness using software that, by necessity, must incorporate a model of the environment and its characteristics. Models can also take us outside of this reality and let us explore alternative timelines — what we call simulation. Today, pro- gramming languages are the primary way to communicate our intentions of these systems in software. Notation, syntax and semantics make the mental programming language models concrete for us as humans. But the computer does not really need the notation, syntax and semantics models of the software in the same way as we humans do. In this talk, we will trace the magic of software that enabled this progression from Moore’s law, through computer languages, to the Digital Artifacts of today. We will investigate it carefully and come to some surprising con- clusions that question the mainstream thinking around software models. What if we let go of some of our learned beliefs about software models and think diﬀerently about models of instructing computers?",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Based Development of Software:\nA Panacea or Academic Poppycock",
        "date": 2013,
        "abstract": "In recent years, the use of models in developing complex soft- ware systems has been steadily increasing. Advocates of model-based de- velopment argue that models can help reduce the time, cost, and eﬀort needed to build software systems which satisfy their requirements and that model-based approaches are eﬀective not only in system develop- ment but throughout a system’s life-time. Thus the problem addressed by researchers in software and system modeling encompasses not only the original construction of a complex system but its complete life-cycle. This talk will address signiﬁcant issues in model-based system and software development, including: What is the current and future role of models in software system development? What beneﬁts can we obtain from the use of models not only in development but throughout the system life-cycle? What are the barriers to using models in software system development and evolution? What are the major challenges for system and software modeling researchers during the next decade?",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Creativity vs Rigor:\nInformal Modeling is OK",
        "date": 2013,
        "abstract": "Single large project courses with clients from industry have been established as capstone courses in many software engineering cur- ricula. They are considered a good way of teaching industry relevant software engineering practices, in particular model-based software devel- opment. One particular challenge is how to balance between modeling and timely delivery. If we focus too much on modeling, the students do not have enough time to deliver the system (“analysis paralysis”). If we focus too much on the delivery of the system, the quality of the models usually goes down the drain. Another challenge is the balance between informal models intended for human communication and speciﬁcation models in- tended for CASE tools. I argue that teachers often put too much weight on the rigor of the models, and less on the creative and iterative aspects of modeling. Modeling should be allowed to be informal, incomplete and inconsistent, especially during the early phases of software development. I have been teaching capstone courses for almost 25 years, initially at the senior and junior level. During this time excellent automatic build and release management tools have been developed. They reduce the need for heroic delivery eﬀorts at the end of a course, especially if they are coupled with agile methods, allowing the teacher to spend more time on the creative aspects of modeling. I will use several examples from my courses to demonstrate how it is possible to include informal mod- eling techniques in project courses with real customers involving a large number of students at the sophomore and even freshmen level without compromising the ideas of model-driven software development.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Industrial Adoption of Model-Driven\nEngineering: Are the Tools Really the Problem?",
        "date": 2013,
        "abstract": "An oft-cited reason for lack of adoption of model-driven en- gineering (MDE) is poor tool support. However, studies have shown that adoption problems are as much to do with social and organizational fac- tors as with tooling issues. This paper discusses the impact of tools on MDE adoption and places tooling within a broader organizational con- text. The paper revisits previous data on MDE adoption (19 in-depth interviews with MDE practitioners) and re-analyzes the data through the speciﬁc lens of MDE tools. In addition, the paper presents new data (20 new interviews in two speciﬁc companies) and analyzes it through the same lens. The key contribution of the paper is a taxonomy of tool-related considerations, based on industry data, which can be used to reﬂect on the tooling landscape as well as inform future research on MDE tools.",
        "keywords": [
            "model-driven engineering",
            "modeling tools",
            "organizational change."
        ],
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Generic Model Assist",
        "date": 2013,
        "abstract": "Model assist is a feature of modelling environments aiding their users with entering well-formed models into an editor. Current implementations of model assist are mostly hard-coded in the editor and duplicate the logic captured in the environment’s validation methods used for post hoc checking of models for well-formedness. We propose a fully declarative approach which computes legal model assists from a modelling language’s well-formedness rules via constraint solving, covering a large array of assistance scenarios with only minor differences in the assistance specifications. We describe an implementation of our approach and evaluate it on 299 small to medium size open source models. Although more research will be needed to explore the boundaries of our approach, first results presented here suggest that it is feasible.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Adding Spreadsheets to the MDE Toolkit",
        "date": 2013,
        "abstract": "Spreadsheets are widely used to support software develop- ment activities. They have been used to collect requirements and soft- ware defects, to capture traceability information between requirements and test cases, and in general, to ﬁll in gaps that are not covered satis- factorily by more specialised tools. Despite their widespread use, spread- sheets have received little attention from researchers in the ﬁeld of Model Driven Engineering. In this paper, we argue for the usefulness of model management support for querying and modifying spreadsheets, we iden- tify the conceptual gap between contemporary model management lan- guages and spreadsheets, and we propose an approach for bridging it. We present a prototype that builds atop the Epsilon and Google Drive plat- forms and we evaluate the proposed approach through a case study that involves validating and transforming software requirements captured us- ing spreadsheets.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Extraction\nand Analysis of Network Security Policies",
        "date": 2013,
        "abstract": "Firewalls are a key element in network security. They are in charge of ﬁltering the traﬃc of the network in compliance with a number of access-control rules that enforce a given security policy. In an always-evolving context, where security policies must often be up- dated to respond to new security requirements, knowing with precision the policy being enforced by a network system is a critical information. Otherwise, we risk to hamper the proper evolution of the system and compromise its security. Unfortunately, discovering such enforced policy is an error-prone and time consuming task that requires low-level and, often, vendor-speciﬁc expertise since ﬁrewalls may be conﬁgured using diﬀerent languages and conform to a complex network topology. To tackle this problem, we propose a model-driven reverse engineering approach able to extract the security policy implemented by a set of ﬁrewalls in a working network, easing the understanding, analysis and evolution of network security policies.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SafetyMet: A Metamodel for Safety Standards",
        "date": 2013,
        "abstract": "In domains such as automotive, avionics, and railway, critical systems must comply with safety standards to allow their operation in a given context. Safety compliance can be an extremely demanding activity as practitioners have to show fulfilment of the safety criteria specified in the standards and thus that a system can be deemed safe. This is usually both costly and time consuming, and becomes even more challenging when, for instance, a system changes or aims to be reused in another project or domain. This paper presents SafetyMet, a metamodel for safety standards targeted at facilitating safety compliance. The metamodel consists of entities and relationships that abstract concepts common to different safety standards from different domains. Its use can help practitioners to show how they have followed the recommendations of a standard, and particularly in evolutionary or cross- domain scenarios. We discuss the benefits of the use of the metamodel, its limitations, and open issues in order to clearly present the aspects of safety compliance that are facilitated and those that are not addressed.",
        "keywords": [
            "safety standard",
            "metamodel",
            "safety compliance",
            "safety assurance",
            "safety certification",
            "SafetyMet",
            "OPENCOSS."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Generic Fault Model for Quality Assurance",
        "date": 2013,
        "abstract": "Because they are comparatively easy to implement, structural coverage criteria are commonly used for test derivation in model- and code- based testing. However, there is a lack of compelling evidence that they are useful for ﬁnding faults, speciﬁcally so when compared to random testing. This paper challenges the idea of using coverage criteria for test selection and instead proposes an approach based on fault models. We deﬁne a gen- eral fault model as a transformation from correct to incorrect programs and/or a partition of the input data space. Thereby, we leverage the idea of fault injection for test assessment to test derivation. We instantiate the developed general fault model to describe existing fault models. We also show by example how to derive test cases.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards an Operationalization of the “Physics of Notations” for the Analysis of Visual Languages",
        "date": 2013,
        "abstract": "We attempt to validate the conceptual framework “Physics of Notation” (PoN) as a means for analysing visual languages by ap- plying it to UML Use Case Diagrams. We discover that the PoN, in its current form, is neither precise nor comprehensive enough to be applied in an objective way to analyse practical visual software engineering no- tations. We propose an operationalization of a part of the PoN, highlight conceptual shortcomings of the PoN, and explore ways to address them.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Teaching Model Driven Engineering\nfrom a Relational Database Perspective",
        "date": 2013,
        "abstract": "We reinterpret MDE from the viewpoint of relational databases to provide an alternative way to teach, understand, and demonstrate MDE using concepts and technologies that should be familiar to undergraduates. We use (1) relational databases to express models and metamodels, (2) Prolog to express con- straints and M2M transformations, (3) Java tools to implement M2T and T2M transformations, and (4) OO shell-scripting languages to compose MDE transfor- mations. Case studies demonstrate the viability of our approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "Prolog"
        }
    },
    {
        "title": "Big Metamodels Are Evil Package Unmerge –– A Technique for Downsizing Metamodels",
        "date": 2013,
        "abstract": "While reuse is typically considered a good practice, it may also lead to keeping irrelevant concerns in derived elements. For instance, new metamodels are usually built upon existing metamodels using additive techniques such as profiling and package merge. With such additive techniques, new metamodels tend to become bigger and bigger, which leads to harmful overheads of com- plexity for both tool builders and users. In this paper, we introduce «package unmerge» - a proposal for a subtractive relation between packages - which complements existing metamodel-extension techniques.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating Modeling Tools  \nin the Development Lifecycle with OSLC: A Case Study",
        "date": 2013,
        "abstract": "Models play a central role in a model driven development process. They realize requirements, specify system design, abstract source code, drive test cases, etc. However, for a modeling tool to be most effective, it needs to integrate its data and workflows with other tools in the development lifecycle. This is often problematic as these tools are usually disparate. OSLC is an emerging specification for integrating lifecycle tools using the principles of linked data. In this paper, we describe how OSLC can be used to integrate MOF-based modeling tools with other lifecycle tools. We demonstrate this in a case study involving an EMF-based modeling tool. We show how we made the tool conform to the OSLC specification and discuss how this enabled it to integrate seamlessly with other lifecycle tools to support some key end-to-end development lifecycle workflows.",
        "keywords": [
            "Model",
            "Lifecycle",
            "OSLC",
            "Semantic Web",
            "OWL",
            "RDF",
            "UML",
            "MOF."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Recommending Auto-completions\nfor Software Modeling Activities",
        "date": 2013,
        "abstract": "Auto-completion of textual inputs beneﬁts software develop- ers using IDEs. However, graphical modeling tools used to design software do not provide this functionality. The challenges of recommending auto- completions for graphical modeling activities are largely unexplored. Rec- ommending such auto-completions requires detecting meaningful partly completed activities, tolerating variance in user actions, and determining most relevant activities that a user wants to perform. This paper proposes an approach that works in the background while a developer is creating or evolving models and handles all these challenges. Editing operations are analyzed and matched to a predeﬁned but extensible catalog of common modeling activities for structural UML models. In this paper we solely focus on determining recommendations rather than automatically com- pleting activities. We demonstrated the quality of recommendations generated by our approach in a controlled experiment with 16 students evolving models. We recommended 88% of a user’s activities within a short list of ten recommendations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatically Searching for Metamodel Well-Formedness Rules in Examples and Counter-Examples",
        "date": 2013,
        "abstract": "Current metamodeling formalisms support the deﬁnition of a metamodel with two views: classes and relations, that form the core of the metamodel, and well-formedness rules, that constraints the set of valid models. While a safe application of automatic operations on mod- els requires a precise deﬁnition of the domain using the two views, most metamodels currently present in repositories have only the ﬁrst one part. In this paper, we propose to start from valid and invalid model examples in order to automatically retrieve well-formedness rules in OCL using Ge- netic Programming. The approach is evaluated on metamodels for state machines and features diagrams. The experiments aim at demonstrating the feasibility of the approach and at illustrating some important design decisions that must be considered when using this technique.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Testing M2T/T2M Transformations",
        "date": 2013,
        "abstract": "Testing model-to-model (M2M) transformations is becoming a promi- nent topic in the current Model-driven Engineering landscape. Current approaches for transformation testing, however, assume having explicit model representa- tions for the input domain and for the output domain of the transformation. This excludes other important transformation kinds, such as model-to-text (M2T) and text-to-model (T2M) transformations, from being properly tested since adequate model representations are missing either for the input domain or for the output domain. The contribution of this paper to overcome this gap is extending Tracts, a M2M transformation testing approach, for M2T/T2M transformation testing. The main mechanism we employ for reusing Tracts is to represent text within a generic metamodel. By this, we transform the M2T/T2M transformation spec- iﬁcation problems into equivalent M2M transformation speciﬁcation problems. We demonstrate the applicability of the approach by two examples and present how the approach is implemented for the Eclipse Modeling Framework (EMF). Finally, we apply the approach to evaluate code generation capabilities of several existing UML tools.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Approach to Testing Java Implementation  \nagainst Its UML Class Model",
        "date": 2013,
        "abstract": "Model Driven Engineering (MDE) aims to expedite the software de- velopment process by providing support for transforming models to running systems. Many modeling tools provide forward engineering features that auto- matically translate a model into a skeletal program that developers must com- plete. Inconsistencies between a design model and its implementation can result as a consequence of manually-added code. Manually checking that an imple- mentation conforms to the model is a daunting task. Thus, there is a need for MDE tools that developers can use to check whether an implementation con- forms to a model, especially when generated code is manually modified. This paper presents an approach for testing that an implementation satisfies the con- straints specified in its design model. We also describe a prototypical tool that supports the approach, and we describe how its application to two Eclipse UML2 projects uncovered errors.",
        "keywords": [
            "UML",
            "Class diagram",
            "Java",
            "Model checking."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Test Case Selection Using Feature Model:  \nAn Industrial Case Study",
        "date": 2013,
        "abstract": "Automated test case selection for a new product in a product line is challenging due to several reasons. First, the variability within the product line needs to be captured in a systematic way; second, the reusable test cases from the repository are required to be identified for testing a new product. The objec- tive of such automated process is to reduce the overall effort for selection (e.g., selection time), while achieving an acceptable level of the coverage of testing functionalities. In this paper, we propose a systematic and automated methodol- ogy using a Feature Model for Testing (FM_T) to capture commonalities and variabilities of a product line and a Component Family Model for Testing (CFM_T) to capture the overall structure of test cases in the repository. With our methodology, a test engineer does not need to manually go through the re- pository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM_T at a higher level of abstraction for a product and a set of relevant test cases will be selected automatically. We applied our methodology to a product line of video confe- rencing systems called Saturn developed by Cisco and the results show that our methodology can reduce the selection effort significantly. Moreover, we con- ducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM_T and CFM_T. The results show that test en- gineers are positive about adapting our methodology and models (FM_T and CFM_T) in their current practice.",
        "keywords": [
            "Test Case Selection",
            "Product Line",
            "Feature Model",
            "Component  Family Model."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Customizable Model Migration Schemes for Meta-model Evolutions with Multiplicity Changes⋆",
        "date": 2013,
        "abstract": "Modeling languages tailored to speciﬁc application domains promise to increase the productivity and quality of model-driven soft- ware development. Nevertheless due to, for example, evolving require- ments, modeling languages, and their meta-models evolve which means that existing models have to be migrated accordingly. In our approach, such co-evolutions are speciﬁed as related graph transformations ensur- ing well-typed model migration results. Model migrations are speciﬁed by transformation rules that can be automatically deduced from given meta-model evolution rules and further customized to special needs. Up to now, meta-model constraints have not been taken into account. In this paper, we extend our approach to handle multiplicity constraints and illustrate this extension using several examples.",
        "keywords": [
            "meta-model evolution",
            "model migration",
            "graph transforma- tion."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Fine-Grained Software Evolution\nUsing UML Activity and Class Models",
        "date": 2013,
        "abstract": "Modern software systems that play critical roles in society’s infrastructures are often required to change at runtime so that they can continuously provide essential services in the dynamic environments they operate in. Updating open, distributed software systems at runtime is very challenging. Using runtime models as an interface for updating soft- ware at runtime can help developers manage the complexity of updating software while it is executing. In this work we describe an approach to updating Java software at runtime through the use of runtime models consisting of UML class and activity diagrams. Changes to models are turned into changes on Java source code, which is then propagated to the runtime system using the JavAdaptor technology. In particular, the pre- sented approach permits in-the-small software changes, i.e., changes at the code statement level, as opposed to in-the-large changes, i.e., changes at the component level. We present a case study that demonstrates the major aspects of the approach and its use.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting the Co-evolution of Metamodels\nand Constraints through Incremental Constraint\nManagement",
        "date": 2013,
        "abstract": "Design models must abide by constraints that can come from diverse sources, like metamodels, requirements, or the problem domain. Modelers intent to live by these constraints and thus desire automated mechanism that provide instant feedback on constraint violations. How- ever, typical approaches assume that constraints do not evolve over time, which, unfortunately, is becoming increasingly unrealistic. For example, the co-evolution of metamodels and models requires corresponding con- straints to be co-evolved continuously. This demands eﬃcient constraint adaptation mechanisms to ensure that validated constraints are up-to- date. This paper presents an approach based on constraint templates that tackles this evolution scenario by automatically updating constraints. We developed the Cross-Layer Modeler (XLM) approach which relies on in- cremental consistency-checking. As a case study, we performed evolutions of the UML-metamodel and 21 design models. Our approach is sound and the empirical evaluation shows that it is near instant and scales with increasing model sizes.",
        "keywords": [
            "Co-evolution",
            "metamodeling",
            "consistency-checking."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Checking of UML-RT Models Using Lazy Composition",
        "date": 2013,
        "abstract": "Formal analysis of models is an important aspect of the Model Driven Development (MDD) paradigm. In this paper we intro- duce a technique to analyze models with hierarchically organized and asynchronously communicating components as found in, e.g., UML-RT. Typically, the more components are composed during analysis, the less scalable it becomes. In our technique we reduce composition by lever- aging the communication topology and the property to be checked. To this end we introduce an extension of Computation Tree Logic (CTL) to express properties of models and we show an algorithm to check such properties. In the algorithm, components are represented by their sym- bolic execution trees and their composition is lazy, i.e., only performed when necessary. To demonstrate some of the beneﬁts of the technique, its implementation for UML-RT models and case studies are discussed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Behavioural Veriﬁcation in Embedded Software, from Model to Source Code",
        "date": 2013,
        "abstract": "To reduce the veriﬁcation costs and to be more conﬁdent on software, static program analysis oﬀers ways to prove properties on source code. Unfortunately, these techniques are diﬃcult to apprehend and to use for non-specialists. Modelling allows users to specify some aspects of software in an easy way. More precisely, in embedded soft- ware, state machine models are frequently used for behavioural design. The aim of this paper is to bridge the gap between model and code by oﬀering automatic generation of annotations from model to source code. These annotations are then veriﬁed by static analysis in order to ensure that the code behaviour conforms to the model-based design. The mod- els we consider are UML state machines with a formal non-ambiguous semantics, the annotation generation and veriﬁcation is implemented in a tool and applied to a case study.",
        "keywords": [
            "Veriﬁcation",
            "UML",
            "Formal Methods",
            "Model Driven Engi- neering."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal Veriﬁcation Integration Approach for DSML⋆",
        "date": 2013,
        "abstract": "The application of formal methods (especially, model check- ing and static analysis techniques) for the veriﬁcation of safety critical embedded systems has produced very good results and raised the inter- est of system designers up to the application of these technologies in real size projects. However, these methods usually rely on speciﬁc veriﬁca- tion oriented formal languages that most designers do not master. It is thus mandatory to embed the associated tools in automated veriﬁcation toolchains that allow designers to rely on their usual domain-speciﬁc modeling languages (DSMLs) while enjoying the beneﬁts of these power- ful methods. More precisely, we propose a language to formally express system requirements and interpret veriﬁcation results so that system designers (DSML end-users) avoid the burden of learning some formal veriﬁcation technologies. Formal veriﬁcation is achieved through trans- lational semantics. This work is based on a metamodeling pattern for executable DSML that favors the deﬁnition of generative tools and thus eases the integration of tools for new DSMLs.",
        "keywords": [
            "Domain speciﬁc modeling language",
            "Formal veriﬁcation",
            "Model checking",
            "Translational semantics",
            "Traceability",
            "Veriﬁcation feedback."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Composing Your Compositions\nof Variability Models",
        "date": 2013,
        "abstract": "Modeling and managing variability is a key activity in a growing number of software engineering contexts. Support for composing variability models is arising in many engineering scenarios, for instance, when several subsystems or modeling artifacts, each coming with their own variability and possibly developed by diﬀerent stakeholders, should be combined together. In this paper, we consider the problem of com- posing feature models (FMs), a widely used formalism for representing and reasoning about a set of variability choices. We show that several composition operators can actually be deﬁned, depending on both match- ing/merging strategies and semantic properties expected in the composed FM. We present four alternative forms and their implementations. We discuss their relative trade-oﬀs w.r.t. reasoning, customizability, trace- ability, composability and quality of the resulting feature diagram. We summarize these ﬁndings in a reading grid which is validated by revisiting some relevant existing works. Our contribution should assist developers in choosing and implementing the right composition operators.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Constraints: The Core of Supporting Automated Product \nConfiguration of Cyber-Physical Systems*",
        "date": 2013,
        "abstract": "In the context of product line engineering of cyber-physical systems, there exists a large number of constraints to support, for example, consistency checking of design decisions made in hardware and software components during configuration. Manual configuration is not feasible in this context considering that managing and manipulating all these constraints in a real industrial context is very complicated and thus warrants an automated solution. Typical automation activities in this context include automated configuration value inference, optimizing configuration steps and consistency checking. However, to this end, relevant constraints have to be well-specified and characterized in the way such that automated configuration can be enabled. In this paper, we classify and characterize constraints that are required to be specified to support most of the key functionalities of any automated product configuration solution, based on our experience of studying three industrial product lines.",
        "keywords": [
            "Product  Line  Engineering",
            "Configuration",
            "Constraints",
            "Classification",
            "Industrial Case Studies",
            "Cyber-Physical Systems."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Defining and Validating a Multimodel Approach  \nfor Product Architecture Derivation and Improvement",
        "date": 2013,
        "abstract": "Software architectures are the key to achieving the non-functional requirements (NFRs) in any software project. In software product line (SPL) development, it is crucial to identify whether the NFRs for a specific product can be attained with the built-in architectural variation mechanisms of the product line architecture, or whether additional architectural transformations are required. This paper presents a multimodel approach for quality-driven product architecture derivation and improvement (QuaDAI). A controlled experiment is also presented with the objective of comparing the effectiveness, efficiency, perceived ease of use, intention to use and perceived usefulness with regard to participants using QuaDAI as opposed to the Architecture Tradeoff Analysis Method (ATAM). The results show that QuaDAI is more efficient and perceived as easier to use than ATAM, from the perspective of novice software architecture evaluators. However, the other variables were not found to be statistically significant. Further replications are needed to obtain more conclusive results.",
        "keywords": [
            "Software Product Lines",
            "Architectural Patterns",
            "Quality Attributes",
            "Model Transformations",
            "Controlled Experiment."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evolution of the UML Interactions Metamodel",
        "date": 2013,
        "abstract": "UML Interactions represent one of the three different behavior kinds of the UML. In general, they specify the exchange of messages among parts of a system. Although UML Interactions can reside on different level of abstractions, they seem to be sufficiently elaborated for a higher-level of abstraction where they are used for sketching the communication among parts. Its metamodel reveals some fuzziness and imprecision where definitions should be accurate and concise, though. In this paper, we propose improvements to the UML Interactions’ metamodel for Message arguments and Loop CombinedFragments that make them more versatile. We will justify the needs for the improvements by precisely showing the shortcomings of the related parts of the metamodel. We demonstrate the expressiveness of the improvements by applying them to examples that current Interactions definition handles awkwardly.",
        "keywords": [
            "UML",
            "Interactions",
            "Sequence  Diagram",
            "Messages",
            "CombinedFragments."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Graph-Pattern Based Approach for Meta-Model \nSpecific Conflict Detection in a General-Purpose Model \nVersioning System",
        "date": 2013,
        "abstract": "Model driven engineering is the key paradigm in many large system development efforts today. A good versioning system for models is essential for change management and coordinated development of these systems. Support for conflict detection and reconciliation is one of the key functionalities of a versioning system. A large system uses a large number of different kinds of models, each specifying a different aspect of the system. The notion of conflict is relative to the semantics of a meta-model. Hence conflicts should be detected and reported in a meta-model specific way. In this paper we discuss a general purpose model versioning system that can work with models of any meta-model, and a graph-pattern based approach for specifying conflicts in a meta-model specific way. We also present an efficient algorithm that uses these graph-patterns to detect conflicts at the right level of abstraction.",
        "keywords": [
            "Model driven engineering",
            "Model versioning",
            "Meta-model."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the Complex Nature of MDE Evolution",
        "date": 2013,
        "abstract": "In Model-Driven Engineering (MDE) the employed setting of languages as well as automated and manual activities has major impact on productivity. Furthermore, such settings for MDE evolve over time. However, currently only the evolution of (modeling) languages, tools, and transformations is studied in research. It is not clear whether these are the only relevant changes that characterize MDE evolution in practice. In this paper we address this lack of knowledge. We ﬁrst discuss possible changes and then report on a ﬁrst study that demonstrates that these forms of evolution can be commonly observed in practice. To investigate the complex nature of MDE evolution in more depth, we captured the evolution of three MDE settings from practice and derive eight observa- tions concerning reasons for MDE evolution. Based on the observations we then identify open research challenge concerning MDE evolution.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Simpliﬁcation and Correctness of UML Class Diagrams – Focusing on Multiplicity and Aggregation/Composition Constraints",
        "date": 2013,
        "abstract": "Model-driven Engineering requires eﬃcient powerful methods for verifying model correctness and quality. Class Diagram is the central language within UML. Its main problems involve correctness problems, which include the consistency and the ﬁnite satisﬁability problems, and quality problems, which include the redundancy and incomplete design problems. Two central constraints in class diagrams are the multiplicity and the aggregation/composition constraints. They are essential in mod- eling conﬁguration management, features, biology, computer-aided design and database systems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Speciﬁcation of Cyber-Physical Components\nwith Formal Semantics –\nIntegration and Composition",
        "date": 2013,
        "abstract": "Model-Based Engineering of Cyber-Physical Systems (CPS) needs correct-by-construction design methodologies, hence CPS model- ing languages require mathematically rigorous, unambiguous, and sound speciﬁcations of their semantics. The main challenge is the formaliza- tion of the heterogeneous composition and interactions of CPS systems. Creating modeling languages that support both the acausal and causal modeling approaches, and which has well-deﬁned and sound behavior across the heterogeneous time domains is a challenging task. In this pa- per, we discuss the diﬃculties and as an example develop the formal semantics of a CPS-speciﬁc modeling language called CyPhyML. We formalize the structural semantics of CyPhyML by means of constraint rules and its behavioral semantics by deﬁning a semantic mapping to a language for diﬀerential algebraic equations. The speciﬁcation language is based on an executable subset of ﬁrst-order logic, which facilitates model conformance checking, model checking and model synthesis.",
        "keywords": [
            "Cyber-Physical Systems",
            "formalization",
            "formal speciﬁcation",
            "Model-Based Engineering",
            "heterogeneous composition."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Endogenous Metamodeling Semantics\nfor Structural UML 2 Concepts",
        "date": 2013,
        "abstract": "A lot of work has been done in order to put the Uniﬁed Mod- eling Language (UML) on a formal basis by translating concepts into var- ious formal languages, e.g., set theory or graph transformation. While the abstract UML syntax is deﬁned by using an endogenous approach, i. e., UML describes its abstract syntax using UML, this approach is rarely used for its semantics. This paper shows how to apply an endogenous approach called metamodeling semantics for central parts of the UML standard. To this end, we enrich existing UML language elements with constraints speciﬁed in the Object Constraint Language (OCL) in order to describe a semantic domain model. The UML speciﬁcation explicitly states that complete runtime semantics is not included in the standard because it would be a major amount of work. However, we believe that certain central concepts, like the ones used in the UML standard and in particular property features as subsets, union and derived, need to be explicitly modeled to enforce a common understanding. Using such an endogenous approach enables the validation and veriﬁcation of the UML standard by using oﬀ-the-shelf UML and OCL tools.",
        "keywords": [
            "Metamodeling",
            "Semantics",
            "Validation",
            "UML",
            "OCL."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Computer Assisted Integration of Domain-Specific \nModeling Languages Using Text Analysis Techniques",
        "date": 2013,
        "abstract": "Following the principle of separation of concerns, the Model-driven Engineering field has developed Domain-Specific Modeling Languages (DSML) to address the increasing complexity of the systems design. In this context of heterogeneous modeling languages, engineers and language design- ers are facing the critical problem of language integration. To address this problem, instead of doing a syntactic analysis based on the domain models or metamodels as it is common practice today, we propose to adopt natural lan- guage processing techniques to do a semantic analysis of the language specifi- cations. We evaluate empirically our approach on seven real test cases and compare our results with five state of the art tools. Results show that the seman- tic analysis of textual descriptions that accompany DSMLs can efficiently assist engineers to make well-informed integration choices.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards the Notation-Driven\nDevelopment of DSMLs",
        "date": 2013,
        "abstract": "Domain-Speciﬁc Modeling Languages (DSML) enable domain experts to leverage Model-Driven Engineering methods and tools through concepts and notations from their own domain. The notation of a DSML is critical because it is the sole interface domain experts will have with their tool. Unfortunately, the current process for the development of DSMLs strongly emphasizes the abstract syntaxes and often treats the notations (concrete syntaxes) as byproducts. Focusing on the case of visual DSMLs, this paper proposes to automatically generate a DSML’s abstract syntax from the speciﬁcation of its concrete syntax. This shift towards the notation-driven development of DSMLs is expected to en- able the production of DSMLs closer to domain experts’ expectations. This approach is validated by its implementation in a prototype, its ap- plication on an industrial case and the results of an empirical study.",
        "keywords": [
            "Domain-Speciﬁc Modeling",
            "Visual Languages."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Validation of Derived Features and Well-Formedness Constraints in DSLs⋆ By Mapping Graph Queries to an SMT-Solver",
        "date": 2013,
        "abstract": "Despite the wide range of existing generative tool support, constructing a design environment for a complex domain-speciﬁc lan- guage (DSL) is still a tedious task as the large number of derived features and well-formedness constraints complementing the domain metamodel necessitate special handling. Incremental model queries as provided by the EMF-IncQuery framework can (i) uniformly specify derived features and well-formedness constraints and (ii) automatically refresh their re- sult set upon model changes. However, for complex domains, derived features and constraints can be formalized incorrectly resulting in in- complete, ambiguous or inconsistent DSL speciﬁcations. To detect such issues, we propose an automated mapping of EMF metamodels enriched with derived features and well-formedness constraints captured as graph queries in EMF-IncQuery into an eﬀectively propositional fragment of ﬁrst-order logic which can be eﬃciently analyzed by the Z3 SMT-solver. Moreover, overapproximations are proposed for complex query features (like transitive closure and recursive calls). Our approach will be illus- trated on analyzing a DSL being developed for the avionics domain.",
        "keywords": [
            "model validation",
            "model queries",
            "SMT-solvers."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Self-adaptation with End-User Preferences:\nUsing Run-Time Models and Constraint Solving⋆",
        "date": 2013,
        "abstract": "This paper presents an approach to developing self-adaptive systems that takes the end users’ preferences into account for adaptation planning, while tolerating incomplete and conﬂicting adaptation goals. The approach transforms adaptation goals, together with the run-time model that describes current system contexts and conﬁgurations, into a constraint satisfaction problem. From that, it diagnoses the conﬂicting adaptation goals to ignore, and determines the required re-conﬁguration that satisﬁes all remaining goals. If users do not agree with the solution, they can revise some conﬁguration values. The approach records their preferences embedded in the revisions by tuning the weights of existing goals, so that subsequent adaptation results will be closer to the users’ preferences. The experiments on a medium-sized simulated smart home system show that the approach is eﬀective and scalable.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Runtime Model Based Management  \nof Diverse Cloud Resources",
        "date": 2013,
        "abstract": "Due to the diversity of resources and different management require- ments, Cloud management is faced with great challenges in complexity and dif- ficulty. For constructing a management system to satisfy a specific management requirement, a redevelopment solution based on existing system is usually more practicable than developing the system from scratch. However, the difficulty and workload of redevelopment are very high. In this paper, we present a run- time model based approach to managing diverse Cloud resources. First, we con- struct the runtime model of each kind of Cloud resources. Second, we construct the composite runtime model of all managed resources through model merge. Third, we make Cloud management meet personalized requirements through model transformation from the composite model to the customized models. Fi- nally, all the management tasks can be carried out through executing operating programs on the customized model. The feasibility and efficiency of the ap- proach are validated through a real case study.",
        "keywords": [
            "runtime model",
            "Cloud management",
            "diverse Cloud resources."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Semantic Web as a Software Modeling Tool:\nAn Application to Citizen Relationship\nManagement",
        "date": 2013,
        "abstract": "The choice of a modeling language in software engineering is traditionally restricted to the tools and meta-models invented specif- ically for that purpose. On the other hand, semantic web standards are intended mainly for modeling data, to be consumed or produced by soft- ware. However, both spaces share enough commonality to warrant an attempt at a uniﬁed solution. In this paper, we describe our experience using Web Ontology Language (OWL) as the language for Model-Driven Development (MDD). We argue that there are beneﬁts of using OWL to formally describe both data and software within an integrated modeling approach by showcasing an e-Government platform that we have built for citizen relationship management. We describe the platform architec- ture, development process and model enactment. In addition, we explain some of the limitations of OWL as an MDD formalism as well as the shortcomings of current tools and suggest practical ways to overcome them.",
        "keywords": [
            "semantic web",
            "owl",
            "model-driven development",
            "e-government",
            "live system",
            "executable models."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Concern-Oriented Software Design",
        "date": 2013,
        "abstract": "There exist many solutions to solve a given design problem, and it is diﬃcult to capture the essence of a solution and make it reusable for future designs. Furthermore, many variations of a given solution may exist, and choosing the best alternative depends on application-speciﬁc high-level goals and non-functional requirements. This paper proposes Concern-Oriented Software Design, a modelling technique that focuses on concerns as units of reuse. A concern groups related models serving the same purpose, and provides three interfaces to facilitate reuse. The variation interface presents the design alternatives and their impact on non-functional requirements. The customization interface of the selected alternative details how to adapt the generic solution to a speciﬁc con- text. Finally, the usage interface speciﬁes the provided behaviour. We illustrate our approach by presenting the concern models of variations of the Observer design pattern, which internally depends on the Association concern to link observers and subjects.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Analyzing Enterprise Models\nUsing Enterprise Architecture-Based Ontology",
        "date": 2013,
        "abstract": "Development and maintenance of enterprise systems is becoming more diﬃcult due to change drivers along multiple interconnected dimensions. It is advisable to model the enterprise ﬁrst and analyze it for potential concerns. For modeling enterprises, ontologies have been considered apt and have been used in the past for the same, but application of ontologies for EA analysis based on concepts of enterprise and relations between them have been scarce. We present our ongoing work on analyzing enterprise models using EA-based ontological representation of enterprise. Our contributions are twofold: ﬁrst, we show how an existing EA modeling language can be leveraged to create EA ontology and sec- ond, we show how two known EA analyses can be realized using this ontology. Initial results suggest that ontology representation facilitates basic EA analysis prototyping due to right mix of representation and inference functionalities and is extensible for more involved EA analyses.",
        "keywords": [
            "Ontology",
            "Enterprise Models",
            "Analysis",
            "Enterprise Architecture."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Analyzing the Effort of Composing Design Models  \nof Large-Scale Software in Industrial Case Studies",
        "date": 2013,
        "abstract": "The importance of model composition in model-centric software de- velopment is well recognized by researchers and practitioners. However, little is known about the critical factors influencing the effort that developers invest to combine design models, detect and resolve inconsistencies in practice. This pa- per, therefore, reports on five industrial case studies where the model composi- tion was used to evolve and reconcile large-scale design models. These studies aim at: (1) gathering empirical evidence about the extent of composition effort when realizing different categories of changes, and (2) identifying and analyz- ing their influential factors. A series of 297 evolution scenarios was performed on the target systems, leading to more than 2 million compositions of model elements. Our findings suggest that: the inconsistency resolution effort is much higher than the upfront effort to apply the composition technique and detect in- consistencies; the developer’s reputation significantly influences the resolution of conflicting changes; and the evolutions dominated by additions required less effort.",
        "keywords": [
            "Model composition effort",
            "empirical studies",
            "effort measurement."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Parallel Execution of ATL Transformation Rules",
        "date": 2013,
        "abstract": "Industrial environments that make use of Model-Driven En- gineering (MDE) are starting to see the appearance of very large models, made by millions of elements. Such models are produced automatically (e.g., by reverse engineering complex systems) or manually by a large number of users (e.g., from social networks). The success of MDE in these application scenarios strongly depends on the scalability of model manipulation tools. While parallelization is one of the traditional ways of making computation systems scalable, developing parallel model trans- formations in a general-purpose language is a complex and error-prone task. In this paper we show that rule-based languages like ATL have strong parallelization properties. Transformations can be developed with- out taking into account concurrency concerns, and a transformation engine can automatically parallelize execution. We describe the imple- mentation of a parallel transformation engine for the current version of the ATL language and experimentally evaluate the consequent gain in scalability.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "ATL"
        }
    },
    {
        "title": "Transformation of Models Containing\nUncertainty",
        "date": 2013,
        "abstract": "Model transformation techniques typically operate under the assumption that models do not contain uncertainty. In the presence of uncertainty, this forces modelers to either postpone working or to arti- ﬁcially remove it, with negative impacts on software cost and quality. Instead, we propose a technique to adapt existing model transforma- tions so that they can be applied to models even if they contain un- certainty, thus enabling the use of transformations earlier. Building on earlier work, we show how to adapt graph rewrite-based model transfor- mations to correctly operate on May uncertainty, a technique that allows explicit uncertainty to be expressed in any modeling language. We eval- uate our approach on the classic Object-Relational Mapping use case, experimenting with models of varying levels of uncertainty.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Veriﬁcation of Model\nTransformations in the Automotive Industry⋆",
        "date": 2013,
        "abstract": "Many companies have adopted MDD for developing their software systems. Several studies have reported on such industrial expe- riences by discussing the eﬀects of MDD and the issues that still need to be addressed. However, only a few studies have discussed using au- tomated veriﬁcation of industrial model transformations. We previously demonstrated how transformations can be used to migrate GM legacy models to AUTOSAR models. In this study, we investigate using au- tomated veriﬁcation for such industrial transformations. We report on applying an automated veriﬁcation approach to the GM-to-AUTOSAR transformation that is based on checking the satisﬁability of a relational transformation representation, or a transformation model, with respect to well-formedness OCL constraints. An implementation of this approach is available as a prototype for the ATL language. We present the veriﬁ- cation results of this transformation and discuss the practicality of using such tools on industrial size problems.",
        "keywords": [
            "Model Transformation",
            "Automated Veriﬁcation",
            "Automo- tive Industry."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Data-Flow Based Model Analysis\nand Its Applications",
        "date": 2013,
        "abstract": "In this paper we present a data-ﬂow based approach to static model analysis to address the problem of current methods being either limited in their expressiveness or employing formalisms which complicate seamless integration with standards and tools in the modeling domain. By applying data-ﬂow analysis - a technique widely used for static pro- gram analysis - to models, we realize what can be considered a generic “programming language” for context-sensitive model analysis through declarative speciﬁcations. This is achieved by enriching meta models with data-ﬂow attributes which are afterward instantiated for models. The resulting equation system is subjected to a ﬁxed-point computation that yields a static approximation of the model’s dynamic behavior as speciﬁed by the analysis. The applicability of the approach is evaluated in the context of a running example, the examination of viable application domains and a statistical review of the algorithm’s performance.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Contract-Aware Slicing of UML Class Models",
        "date": 2013,
        "abstract": "Slicing is a reduction technique that has been applied to class models to support model comprehension, analysis, and other modeling activities. In particular, slicing techniques can be used to produce class model fragments that include only those elements needed to analyze se- mantic properties of interest. In this paper we describe a class model slicing approach that takes into consideration invariants and operation contracts expressed in the Object Constraint Language (OCL). The ap- proach is used to produce model fragments, each of which consists of only the model elements needed to analyze speciﬁed properties. We use the slicing approach to support a technique for analyzing sequences of opera- tion invocations to uncover invariant violations. The slicing technique is used to produce model fragments that can be analyzed separately. The preliminary evaluation we performed provides evidence that the pro- posed slicing technique can signiﬁcantly reduce the time to perform the analysis.",
        "keywords": [
            "Class model slicing",
            "UML/OCL",
            "Contract."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Usability Inspection in Model-Driven Web Development: \nEmpirical Validation in WebML",
        "date": 2013,
        "abstract": "There is a lack of empirically validated usability evaluation methods that can be applied to models in model-driven Web development. Evaluation of these models allows an early detection of usability problems perceived by the end-user. This motivated us to propose WUEP, a usability inspection method which can be integrated into different model-driven Web development processes. We previously demonstrated how WUEP can effectively be used when following the Object-Oriented Hypermedia method. In order to provide evidences about WUEP’s generalizability, this paper presents the operationalization and empirical validation of WUEP into another well-known method: WebML. The effectiveness, efficiency, perceived ease of use, and satisfaction of WUEP were evaluated in comparison to Heuristic Evaluation (HE) from the viewpoint of novice inspectors. The results show that WUEP was more effective and efficient than HE when detecting usability problems on models. Also, inspectors were satisfied when applying WUEP, and found it easier to use than HE.",
        "keywords": [
            "Model-driven Web development",
            "Usability inspections",
            "Measure  operationalization",
            "Empirical validation",
            "WebML."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Approach  \nfor Supporting the Mapping of Parallel Algorithms  \nto Parallel Computing Platforms",
        "date": 2013,
        "abstract": "The trend from single processor to parallel computer architectures has increased the importance of parallel computing. To support parallel compu- ting it is important to map parallel algorithms to a computing platform that consists of multiple parallel processing nodes. In general different alternative mappings can be defined that perform differently with respect to the quality re- quirements for power consumption, efficiency and memory usage. The mapping process can be carried out manually for platforms with a limited number of pro- cessing nodes. However, for exascale computing in which hundreds of thou- sands of processing nodes are applied, the mapping process soon becomes in- tractable. To assist the parallel computing engineer we provide a model-driven approach to analyze, model, and select feasible mappings. We describe the de- veloped toolset that implements the corresponding approach together with the required metamodels and model transformations. We illustrate our approach for the well-known complete exchange algorithm in parallel computing.",
        "keywords": [
            "Model Driven Software Development",
            "Parallel Computing",
            "High  Performance Computing",
            "Domain Specific Language",
            "Tool Support."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Compositional Synthesis of Controllers\nfrom Scenario-Based Assume-Guarantee\nSpeciﬁcations",
        "date": 2013,
        "abstract": "Modern software-intensive systems often consist of multiple components that interact to fulﬁll complex functions in sometimes safety- critical situations. During the design, it is crucial to specify the system’s requirements formally and to detect inconsistencies as early as possi- ble in order to avoid ﬂaws in the product or costly iterations during its development. We propose to use Modal Sequence Diagrams (MSDs), a formal, yet intuitive formalism for specifying the interaction of a system with its environment, and developed a formal synthesis approach that allows us to detect inconsistencies and even to automatically synthesize controllers from MSD speciﬁcations. The technique is suited for speciﬁ- cations of technical systems with real-time constraints and environment assumptions. However, synthesis is computationally expensive. In order to employ synthesis also for larger speciﬁcations, we present, in this pa- per, a novel assume-guarantee-style compositional synthesis technique for MSD speciﬁcations. We provide evaluation results underlining the beneﬁt of our approach and formally justify its correctness.",
        "keywords": [
            "Scenario-Based Speciﬁcation",
            "Compositional Controller Synthesis",
            "Consistency Checking",
            "Assume-Guarantee."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Why Formal Modeling Language\nSemantics Matters",
        "date": 2014,
        "abstract": "The point of modeling languages is not just modeling, but modeling as a powerful means of making software development much more reliable, reusable, automated, and cost eﬀective. For all these pur- poses, model transformations, as a disciplined technique to systematically relate models within a modeling language and across languages, play a crucial role. In particular, automatic code generation from models is one of its great advantages. As in the case of programming languages and compilers for such lan- guages — which can be seen as a speciﬁc, special case of modeling lan- guages and model transformations — there are two ways of going about all this: (i) the usual, engineering way of building and using practical tools, like parsers, compilers, and debuggers and, likewise, modeling tools and model transformations, where the semantics is implicit in the tools themselves and informal; and (ii) a formal semantics based approach, where the diﬀerent languages involved are given a formal semantics and correctness issues, such as the correctness of programs and models, and of compilers and model transformers, can be addressed head-on with powerful methods. It seems fair to say that, both for programming and for modeling languages, the usual engineering approach is at present the prevailing one. But this should not blind us to the existence of intrinsi- cally superior technological possibilities for the future. Furthermore, the reasons for taking formal semantics seriously are even more compelling for modeling languages than for programming languages. Speciﬁcally, the following crucial advantages can be gained: 1. Formal Analysis of Model-Based Designs to uncover costly design errors much earlier in the development cycle. 2. Correct-by-Construction Model Transformations based on formal pat- terns, that can be amortized across many instances. 3. Modeling-Language-Generic Formal Analysis tools that are seman- tics-based and can likewise be amortized across many languages. 4. Correct-by-Construction Code Generators, a burning issue for cyber- physical systems, and a must for high-quality, highly reliable imple- mentations. Although the full potential for enjoying all these advantages has yet to be exploited and much work remains ahead, none of this is some pie-in-the-sky day dreaming. There is already a substantial body of re- search, tools, and case studies demonstrating that a formal semantics based approach to modeling languages is a real possibility. For example, formal approaches to modeling language semantics based on: (i) type theory, (ii) graph transformations, and (iii) rewriting logic, all converge",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling: A Practical Perspective",
        "date": 2014,
        "abstract": "My talk will explore some of the basic ideas of the modeling approach as they apply to software engineering. I will use the domain of model-based testing, and discuss its foundations and adoption successes and pitfalls. A major focus will be on the bells and whistles which may help with getting modeling into mainstream. I will also discuss opportu- nities and challenges for model-based software development which arise from the cloud computing environment found in most of today’s industry.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Data-Driven Models\nof Human Behavior",
        "date": 2014,
        "abstract": "We live in a world of data, of big data, a big part of which has been generated by humans through their interactions with both the physical and digital world. A key element in the exponential growth of human behavioral data is the mobile phone. There are almost as many mobile phones in the world as humans. The mobile phone is the piece of technology with the highest levels of adoption in human history. We carry them with us all through the day (and night, in many cases), leaving digital traces of our physical interactions. Mobile phones have become sensors of human activity in the large scale and also the most personal devices. In my talk, I will present some of the work that we are doing at Tele- fonica Research in the area of modeling humans from large-scale human behavioral data, such as inferring personality, socio-economic status, at- tentiveness to messages or taste. I will highlight opportunities and chal- lenges associated with building data-driven models of human behavior.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Development of Mobile\nApplications Allowing Role-Driven Variants⋆",
        "date": 2014,
        "abstract": "Rapidly increasing numbers of applications and users make the development of mobile applications to one of the most promising ﬁelds in software engineering. Due to short time-to-market, diﬀering plat- forms and fast emerging technologies, mobile application development faces typical challenges where model-driven development can help. We present a modeling language and an infrastructure for the model-driven development (MDD) of Android apps supporting the speciﬁcation of dif- ferent app variants according to user roles. For example, providing users may continuously conﬁgure and modify custom content with one app variant whereas end users are supposed to use provided content in their variant. Our approach allows a ﬂexible app development on diﬀerent ab- straction levels: compact modeling of standard app elements, detailed modeling of individual elements, and separate provider models for spe- ciﬁc custom needs. We demonstrate our MDD-approach at two apps: a phone book manager and a conference guide being conﬁgured by confer- ence organizers for participants.",
        "keywords": [
            "model-driven development",
            "mobile application",
            "Android."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Based System to Automate Cloud Resource \nAllocation and Optimization",
        "date": 2014,
        "abstract": "Cloud computing offers a flexible approach to elastically allocate computing resources for web applications without significant upfront hardware acquisition costs. Although a diverse collection of cloud resources is available, choosing the most optimized and cost-effective set of cloud resources to meet the QoS requirements is not a straightforward task. Manual load testing, monitoring of resource utilization, followed by bottleneck analysis is time consuming and complex due to limitations of the abstractions of load testing tools, challenges characterizing resource utilization, significant manual test orchestration effort, and complexity of selecting resource configurations to test. This paper introduces a model-based approach to simplify, optimize, and automate cloud resource allocation decisions to meet QoS goals for web applications. Given a high-level application description and QoS requirements, the model-based approach automatically tests the application under a variety of load and resources to derive the most cost-effective resource configuration to meet the QoS goals.",
        "keywords": [
            "Cloud Computing",
            "Resource Allocation",
            "Resource Optimization",
            "Model-Based System",
            "Domain-Specific Language."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Evaluation of the Effectiveness\nof the Atomic Section Model",
        "date": 2014,
        "abstract": "Society increasingly depends on web applications for business and pleasure. As the use of web applications continues to increase, the number of fail- ures, some minor and some major, continues to grow. A signiﬁcant problem is that we still have relatively weak abilities to test web applications. Traditional testing techniques do not adequately model or test these novel technologies. The atomic section model (ASM), models web applications to support design, analysis, and testing. This paper presents an empirical study to evaluate the effectiveness of the ASM. The model was implemented into a tool, WASP, which extracts the ASM from the implementation and supports various test criteria. We studied ten web applications, totaling 156 components and 11,829 lines of code. Using WASP, we generated 207 tests, which revealed 31 faults. Seventeen of those faults exposed internal information about the application and server.",
        "keywords": [
            "Web applications",
            "Test criteria",
            "Model based testing",
            "Atomic section modeling."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Parsing in a Broad Sense",
        "date": 2014,
        "abstract": "Having multiple representations of the same instance is com- mon in software language engineering: models can be visualised as graphs, edited as text, serialised as XML. When mappings between such represen- tations are considered, terms “parsing” and “unparsing” are often used with incompatible meanings and varying sets of underlying assumptions. We investigate 12 classes of artefacts found in software language processing, present a case study demonstrating their implementations and state-of- the-art mappings among them, and systematically explore the technical research space of bidirectional mappings to build on top of the existing body of work and discover as of yet unused relationships.",
        "keywords": [
            "Parsing",
            "unparsing",
            "pretty-printing",
            "model synchronisation",
            "technical space bridging",
            "bidirectional model transformation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Streaming Model Transformations By Complex Event Processing⋆",
        "date": 2014,
        "abstract": "Streaming model transformations represent a novel class of transformations dealing with models whose elements are continuously produced or modiﬁed by a background process [1]. Executing streaming transformations requires eﬃcient techniques to recognize the activated transformation rules on a potentially inﬁnite input stream. Detecting a series of events triggered by compound structural changes is especially challenging for a high volume of rapid modiﬁcations, a characteristic of an emerging class of applications built on runtime models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the Use of Signatures for Source Incremental\nModel-to-text Transformation",
        "date": 2014,
        "abstract": "Model-to-text (M2T) transformation is an important model management operation, used to implement code and documentation gen- eration, model serialisation (enabling model interchange), and model vi- sualisation and exploration. Despite the importance of M2T transforma- tion, contemporary M2T transformation languages cannot be used to eas- ily produce transformations that scale well as the size of the input model increases, which limits their applicability in practice. In this paper, we propose an extension to template-based M2T languages that adds sup- port for signatures, lightweight and concise proxies for templates, which are used to reduce the time taken to re-execute a M2T transformation in response to changes to the input model. We report our initial results in applying signatures to two existing M2T transformations, which indicate a reduction of 33-47% in transformation execution time.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Modeling Systemic Behavior by State-Based\nHolonic Modular Units",
        "date": 2014,
        "abstract": "The paper explores a vision in modeling the behavior of com- plex systems by modular units hosting state machines arranged in part- whole hierarchies and communicating through event ﬂows. Each modular unit plays at the same time the double role of part and whole, i.e. it is inspired by the philosophical idea of holon, providing both an interface and an implementation by which other component state machines may be controlled in order to achieve a global behavior. It is moreover observed that it is possible to assign a formal characterization to such state mod- ules, due to their part-whole arrangement, since higher-level behaviors can derive formally their meaning from lower-level component behaviors. Such a way of arranging behavioral modules allows to establish directly correct-by-construction safety and liveness properties of state-based sys- tems thus challenging the current approach by which state machines interact at the same level and have to be model-checked for ensuring correctness.",
        "keywords": [
            "state-based modeling",
            "holons",
            "component-based modeling",
            "model checking",
            "correctness by construction."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Semantic Model Differencing Utilizing Behavioral\nSemantics Speciﬁcations",
        "date": 2014,
        "abstract": "Identifying differences among models is a crucial prerequisite for sev- eral development and change management tasks in model-driven engineering. The majority of existing model differencing approaches focus on revealing syn- tactic differences which can only approximate semantic differences among mod- els. Signiﬁcant advances in semantic model differencing have been recently made by Maoz et al. [16] who propose semantic diff operators for UML class and activ- ity diagrams. In this paper, we present a generic semantic differencing approach which can be instantiated to realize semantic diff operators for speciﬁc model- ing languages. Our approach utilizes the behavioral semantics speciﬁcation of the considered modeling language, which enables to execute models and capture execution traces representing the models’ semantic interpretation. Based on this semantic interpretation, semantic differences can be revealed.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalizing Execution Semantics of UML\nProﬁles with fUML Models",
        "date": 2014,
        "abstract": "UML Proﬁles are not only sets of annotations. They have se- mantics. Executing a model on which a proﬁle is applied requires seman- tics of this latter to be considered. The issue is that in practice semantics of proﬁles are mainly speciﬁed in prose. In this form it cannot be pro- cessed by tools enabling model execution. Although latest developments advocate for a standard way to formalize semantics of proﬁles, no such approach could be found in the literature. This paper addresses this issue with a systematic approach based on fUML to formalize the execution semantics of UML proﬁles. This approach is validated by formalizing the execution semantics of a subset of the MARTE proﬁle. The proposal is compatible with any tool implementing UML and clearly identiﬁes the mapping between stereotypes and semantic deﬁnitions.",
        "keywords": [
            "fUML",
            "Alf",
            "Proﬁle",
            "Semantics",
            "Execution",
            "MARTE."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Who Knows/Uses What of the UML: A Personal Opinion Survey",
        "date": 2014,
        "abstract": "UML is a comprehensive notation, offering a very large set of dia- grams and constructs covering any possible modelling need. As consequence, on one hand, it is difﬁcult and time consuming to master it, and on the other hand, people tend, naturally, to consider only a part of it. In practice, many UML di- agrams/constructs seem scarcely used or even their existence is not known. By means of a study covering any possible source of information (e.g. UML books and tools), we started to assess which part of the UML is considered and used in practice. Here, we present some results about knowledge and usage of the UML diagrams by means of a personal opinion survey with 275 participants from both industry and academy, analysing also the inﬂuence of different factors: work- ing environment (academia vs. industry), working role, seniority, education, and gender.",
        "keywords": [
            "UML Usage and Knowledge",
            "Personal Opinion Survey",
            "Empirical Study."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assessing the State-of-Practice of Model-Based\nEngineering in the Embedded Systems Domain",
        "date": 2014,
        "abstract": "Model-Based Engineering (MBE) aims at increasing the ef- fectiveness of engineering by using models as key artifacts in the devel- opment process. While empirical studies on the use and the eﬀects of MBE in industry exist, there is only little work targeting the embedded systems domain. We contribute to the body of knowledge with a study on the use and the assessment of MBE in that particular domain. We col- lected quantitative data from 112 subjects, mostly professionals working with MBE, with the goal to assess the current State of Practice and the challenges the embedded systems domain is facing. Our main ﬁndings are that MBE is used by a majority of all participants in the embedded systems domain, mainly for simulation, code generation, and documenta- tion. Reported positive eﬀects of MBE are higher quality and improved reusability. Main shortcomings are interoperability diﬃculties between MBE tools, high training eﬀort for developers and usability issues.",
        "keywords": [
            "Model-Based Engineering",
            "Model-Driven Engineering",
            "Em- bedded Systems",
            "Industry",
            "Modeling",
            "Empirical Study",
            "State-of-Practice."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Relevance of Model-Driven Engineering  \nThirty Years from Now",
        "date": 2014,
        "abstract": "Although model-driven engineering (MDE) is now an established approach for developing complex software systems, it has not been universally adopted by the software industry. In order to better understand the reasons for this, as well as to identify future opportunities for MDE, we carried out a week-long design thinking experiment with 15 MDE experts. Participants were facilitated to identify the biggest problems with current MDE technologies, to identify grand challenges for society in the near future, and to identify ways that MDE could help to address these challenges. The outcome is a reflection of the current strengths of MDE, an outlook of the most pressing challenges for socie- ty at large over the next three decades, and an analysis of key future MDE re- search opportunities.",
        "keywords": [
            "Model-driven engineering",
            "challenges",
            "research opportunities."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Verifying Compilation\nof Synchronous Distributed Applications⋆",
        "date": 2014,
        "abstract": "We present an approach, based on model-driven verifying compilation, to construct distributed applications that satisfy user- speciﬁed safety speciﬁcations, assuming a ”synchronous network” model of computation. Given a distributed application Pd and a safety speciﬁ- cation ϕ in a domain speciﬁc language dasl (that we have developed), we ﬁrst use a combination of sequentialization and software model checking to verify that Pd satisﬁes ϕ. If veriﬁcation succeeds, we generate an im- plementation of Pd that uses a novel barrier-based synchronizer protocol (that we have also developed) to implement the synchronous network se- mantics. We present the syntax and semantics of dasl. We also present, and prove correctness of, two sequentialization algorithms, and the syn- chronizer protocol. Finally, we evaluate the two sequentializations on a collection of distributed applications with safety-critical requirements.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Environment-Centric Contracts\nfor Design of Cyber-Physical Systems",
        "date": 2014,
        "abstract": "A contract splits the responsibilities between a component and its environment into a guarantee that expresses an intended property under the responsibility of the component, given that the environment fulﬁlls the assumptions. Although current contract theories are limited to express contracts over interfaces of components, speciﬁcations that are not limited to interfaces are used in practice and are needed in or- der to properly express safety requirements. A framework is therefore presented, generalizing current contract theory to environment-centric contracts - contracts that are not limited to the interface of components. The framework includes revised deﬁnitions of properties of contracts, as well as theorems that specify exact conditions for when the properties hold. Furthermore, constraints are introduced, limiting the ports over which an environment-centric contract is expressed where the constraints constitute necessary conditions for the guarantee of the contract to hold in an architecture.",
        "keywords": [
            "Environment-Centric",
            "Contracts",
            "Architecture."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Removing Redundancies and Deducing Equivalences in UML Class Diagrams",
        "date": 2014,
        "abstract": "The emerging Model-driven Engineering approach puts mod- els at the heart of the software development process. The Class Diagram language is central within the UML. Automated support for class dia- grams involves identiﬁcation and repair of correctness and quality prob- lems.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Native Versioning Concept to Support\nHistorized Models at Runtime",
        "date": 2014,
        "abstract": "Models@run.time provides semantically rich reﬂection lay- ers enabling intelligent systems to reason about themselves and their surrounding context. Most reasoning processes require not only to ex- plore the current state, but also the past history to take sustainable decisions e.g. to avoid oscillating between states. Models@run.time and model-driven engineering in general lack native mechanisms to eﬃciently support the notion of history, and current approaches usually generate redundant data when versioning models, which reasoners need to nav- igate. Because of this limitation, models fail in providing suitable and sustainable abstractions to deal with domains relying on history-aware reasoning. This paper tackles this issue by considering history as a na- tive concept for modeling foundations. Integrated, in conjunction with lazy load/storage techniques, into the Kevoree Modeling Framework, we demonstrate onto a smart grid case study, that this mechanisms enable a sustainable reasoning about massive historized models.",
        "keywords": [
            "Models@run.time",
            "Model-driven engineering",
            "Modelversion- ing",
            "Historized models."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling Adaptation Policies\nas Domain-Speciﬁc Constraints⋆",
        "date": 2014,
        "abstract": "In order to develop appropriate adaptation policies for self- adaptive systems, developers usually have to accomplish two main tasks: (i) identify the application-level constraints that regulate the desired sys- tem states for the various contexts, and (ii) ﬁgure out how to transform the system to satisfy these constraints. The second task is challenging because typically there is complex interaction among constraints, and a signiﬁcant gap between application domain expertice and state transi- tion expertice. In this paper, we present a model-driven approach that relieves developers from this second task, allowing them to directly write domain-speciﬁc constraints as adaptation policies. We provide a language to model both the domain concepts and the application-level constraints. Our runtime engine transforms the model into a Satisﬁability Modulo Theory problem, optimises it by pre-processing on the current system state at runtime, and computes required modiﬁcations according to the speciﬁed constraints using constraints solving. We evaluate the approach addressing a virtual machine placement problem in cloud computing.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scalable Armies of Model Clones\nthrough Data Sharing",
        "date": 2014,
        "abstract": "Cloning a model is usually done by duplicating all its runtime objects into a new model. This approach leads to memory consumption problems for operations that create and manipulate large quantities of clones (e.g., design space exploration). We propose an original approach that exploits the fact that operations rarely modify a whole model. Given a set of immutable properties, our cloning approach determines the ob- jects and ﬁelds that can be shared between the runtime representations of a model and its clones. Our generic cloning algorithm is parameter- ized with three strategies that establish a trade-oﬀbetween memory savings and the ease of clone manipulation. We implemented the strate- gies within the Eclipse Modeling Framework (EMF) and evaluated mem- ory footprints and computation overheads with 100 randomly generated metamodels and models. Results show a positive correlation between the proportion of shareable properties and memory savings, while the worst median overhead is 9,5% when manipulating the clones.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Three Cases of Feature-Based\nVariability Modeling in Industry⋆",
        "date": 2014,
        "abstract": "Large software product lines need to manage complex vari- ability. A common approach is variability modeling—creating and main- taining models that abstract over the variabilities inherent in such systems. While many variability modeling techniques and notations have been proposed, little is known about industrial practices and how indus- try values or criticizes this class of modeling. We attempt to address this gap with an exploratory case study of three companies that apply vari- ability modeling. Among others, our study shows that variability mod- els are valued for their capability to organize knowledge and to achieve an overview understanding of codebases. We observe centralized model governance, pragmatic versioning, and surprisingly little constraint mod- eling, indicating that the eﬀort of declaring and maintaining constraints does not always pay oﬀ.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting Multiplicity and Hierarchy\nin Model-Based Conﬁguration:\nExperiences and Lessons Learned",
        "date": 2014,
        "abstract": "When developing large-scale industrial software systems en- gineers need to instantiate, conﬁgure, and deploy diverse reusable compo- nents. The number of component instances required depends on customer requirements only known during conﬁguration and is typically unknown when modeling the systems’ variability. Also, the hierarchy of dynam- ically created component instances leads to complex dependencies be- tween conﬁguration decisions. Dealing with component multiplicity and hierarchy thus requires an approach capable of expressing the depen- dencies among dynamically instantiated components and conﬁguration decisions. Furthermore, users need tool support for navigating the com- plex decision space during conﬁguration. In this experience paper we report on applying a decision-oriented modeling approach for deﬁning component variability, multiplicity, and hierarchy. We further present a conﬁguration tool that guides end users through the complex decision space. We report applications of the approach to industrial software sys- tems and describe patterns and lessons learned.",
        "keywords": [
            "Variability models",
            "multiplicity",
            "hierarchy",
            "conﬁguration tool."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Propagating Decisions to Detect and Explain\nConﬂicts in a Multi-step Conﬁguration Process",
        "date": 2014,
        "abstract": "In conﬁguration processes with multiple stakeholders, con- ﬂicts are very likely because each decision maker has a diﬀerent concerns and expectations about the product. They may not be aware of features selected by others or the restrictions that these selections impose. To help solve the conﬂicts, this paper introduces a new approach to provide explanations about their causes. Our approach is based on representing features from diﬀerent concerns using diﬀerent Feature Models (FMs), and relating them through Feature-Solution Graphs. An FSG contains dependency relationships between two FMs: one feature from the left side forces or prohibits the selection of features in the right side feature model. The strategy to detect and explain conﬂicts is based on propaga- tion of constraints over the FSGs. We claim that our approach is more expressive and eﬃcient than when using a single FM that contains all concerns and SAT solvers to detect conﬂicts.",
        "keywords": [
            "Multi-level conﬁguration processes",
            "Feature Models",
            "Feature-Solution Graphs",
            "Conﬂict explanation."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An MDA Approach for the Generation\nof Communication Adapters Integrating SW\nand FW Components from Simulink",
        "date": 2014,
        "abstract": "We present the tools, metamodels and code generation tech- niques in use at Elettronica SpA for the development of communication adapters for software and ﬁrmware systems from heterogeneous models. The process start from a SysML system model, developed according to the platform-based design (PBD) paradigm, in which a functional model of the system is paired to a model of the execution platform. Subsystems are reﬁned as Simulink models or hand coded in C++. In turn, Simulink models are implemented as software code or ﬁrmware on FPGA, and an automatic generation of the implementation is obtained. Based on the SysML system architecture speciﬁcation, our framework drives the gener- ation of Simulink models with consistent interfaces, allows the automatic generation of the communication code among all subsystems (including the HW-FW interface code).",
        "keywords": [
            "System Engineering",
            "Model-Driven Architecture",
            "Model- Based Design",
            "Platform-Based Design",
            "Automatic Code Generation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML Model-Driven Approach to Eﬃciently\nAllocate Complex Communication Schemes",
        "date": 2014,
        "abstract": "To increase the performance of embedded devices, the cur- rent trend is to shift from serial to parallel and distributed computing with simultaneous instructions execution. The performance increase of parallel computing wouldn’t be possible without eﬃcient transfers of data and control information via complex communication architectures. In UML/SysML/MARTE, diﬀerent solutions exist to describe and map computations onto parallel and distributed systems. However, these lan- guages lack expressiveness to clearly separate computation models from communication ones, thus strongly impacting models’ portability, espe- cially when performing Design Space Exploration. As a solution to this issue, we present Communication Patterns, a novel UML modeling arti- fact and model-driven approach to assist system engineers in eﬃciently modeling and mapping communications for parallel and distributed sys- tem architectures. We illustrate the eﬀectiveness of our approach with the design of a parallel signal processing algorithm mapped to a multi- processor platform with a hierarchical bus-based interconnect.",
        "keywords": [
            "Model Driven Engineering",
            "Hardware-Software Co-Design",
            "Design Space Exploration",
            "Parallel Computing",
            "Embedded Systems."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Integrating Software Components",
        "date": 2014,
        "abstract": "In order to handle complexity of software systems, component- based as well as model-driven approaches have become popular in the past. In a model-driven development process the problem arises that over time model and code may be not aligned. Thus, in order to avoid this steadily increasing distance between models and code, we propose the integration of (executable) models and code at the component level. Redundancy – the source of inconsistencies – is reduced by interpreting models directly. Moreover,variability and adaptivity can beachieved by queryingand trans- forming the embedded models. As the basis for such Model-Integrating Components (MoCos), we introduce a component realization concept that iscompatiblewith existingcomponenttechnologies.Weprovidea reference implementation using Java, OSGi and TGraphs and apply it successfully in a feasibility study on AndroidTM.",
        "keywords": [
            "Model-integrating component",
            "model execution",
            "ﬂexibility."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Experiences in Applying Model Driven Engineering  \nto the Telescope and Instrument Control System Domain",
        "date": 2014,
        "abstract": "The development of control systems for large telescopes is frequent- ly challenged by the combination of research and industrial development processes, the bridging of astronomical and engineering domains, the long de- velopment and maintenance time-line, and the need to support multiple hard- ware and software platforms. This paper illustrates the application of a model driven engineering approach to mitigate some of these recurring issues. It de- scribes the lessons learned from introducing a modeling language and creating model transformations for analysis, documentation, simulation, validation, and code generation.",
        "keywords": [
            "model driven engineering",
            "telescope control systems",
            "model trans- formation",
            "model validation",
            "code generation."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Driven Grant Proposal Engineering",
        "date": 2014,
        "abstract": "We demonstrate the application of Model Driven Engineer- ing techniques to support the development of research grant proposals. In particular, we report on using model-to-text transformation and model validation to enhance productivity and consistency in research proposal writing, and present unanticipated opportunities that were revealed after establishing an MDE infrastructure. We discuss the types of models and the technologies used, reﬂect on our experiences, and assess the produc- tivity beneﬁts of our MDE solution through automated analysis of data extracted from the version control repository of a successful grant pro- posal; our evaluation indicates that the use of MDE techniques improved productivity by at least 58%.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Agile Model-Driven Engineering in Mechatronic\nSystems - An Industrial Case Study",
        "date": 2014,
        "abstract": "Model-driven engineering focuses on structuring systems as well as permitting domain experts to be directly involved in the soft- ware development. Agile methods aim for fast feedback and providing crucial knowledge early in the project. In our study, we have seen a successful combination of MDE and agile methods to support the devel- opment of complex, software-driven mechatronic systems. We have inves- tigated how combining MDE and agile methods can reduce the number of issues caused by erroneous assumptions in the software of these mecha- tronic systems. Our results show that plant models to simulate mechan- ical systems are needed to enable agile MDE during the mechatronic development. They enable developers to run, verify, and validate mod- els before the mechanical systems are delivered from suppliers. While two case studies conducted at Volvo Car Group conﬁrm that combining MDE and agile works, there are still challenges e.g. how to optimize the development of plant models.",
        "keywords": [
            "Model Driven Engineering",
            "Agile",
            "Mechatronic Software De- velopment",
            "Virtual Testing",
            "Assumptions",
            "Plant Models."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using UML for Modeling Procedural Legal Rules: Approach and a Study of Luxembourg’s Tax Law",
        "date": 2014,
        "abstract": "Many laws, e.g., those concerning taxes and social beneﬁts, need to be operationalized and implemented into public administration procedures and eGovernment applications. Where such operationaliza- tion is warranted, the legal frameworks that interpret the underlying laws are typically prescriptive, providing procedural rules for ensuring legal compliance. We propose a UML-based approach for modeling pro- cedural legal rules. With help from legal experts, we investigate actual legal texts, identifying both the information needs and sources of com- plexity in the formalization of procedural legal rules. Building on this study, we develop a UML proﬁle that enables more precise modeling of such legal rules. To be able to use logic-based tools for compliance analy- sis, we automatically transform models of procedural legal rules into the Object Constraint Language (OCL). We report on an application of our approach to Luxembourg’s Income Tax Law providing initial evidence for the feasibility and usefulness of our approach.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Resolution of Interfering Product Fragments\nin Software Product Line Engineering",
        "date": 2014,
        "abstract": "The Common Variability Language (CVL) allows deriving new products in a software product line by substituting fragments (place- ment) in the base model. Relations between elements of diﬀerent place- ment fragments are an issue. Substitutions involving interfering place- ments may give unexpected and unintended results. However, there is a pragmatic need to deﬁne and execute fragments with interference. The need emerges when several diagrams are views of a single model, such as a placement in one diagram and a placement in another diagram reference the same model elements. We handle the issue by 1) classifying inter- fering fragments, 2) ﬁnding criteria to detect them, and 3) suggesting solutions via transformations. We implement our ﬁndings in the tooling available for downloading.",
        "keywords": [
            "Graph transformations",
            "software product lines",
            "fragment substitutions",
            "adjacent",
            "interference",
            "cvl",
            "conﬂict resolution."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "CVL"
        }
    },
    {
        "title": "Ontology-Based Modeling\nof Context-Aware Systems",
        "date": 2014,
        "abstract": "Context-aware systems aim to improve the interaction be- tween a computer and a human being by using contextual information about the system itself, the user, and their environment. The number of relevant contextual information is expected to grow rapidly within the next years which tends to result in a complex, error-prone and hence, expensive task of programming context-aware systems. Model-based de- velopment can overcome these issues. Current approaches do not allow to model calculation of reliabilities and do not oﬀer options to handle multiple sources of contextual information. In this paper, we present an approach of modeling contextual in- formation of a context-aware system using the example of a context- aware in-car infotainment system. In particular, we show how develop- ers of context-aware in-car infotainment systems can model reliability calculations of contextual information and handling of multiple sources of contextual information by using a hybrid, ontology-based modeling technique.",
        "keywords": [
            "Context-aware",
            "ontology",
            "infotainment",
            "modeling."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Comprehending Feature Models Expressed in CVL",
        "date": 2014,
        "abstract": "Feature modeling is a common way to present and manage variability of software and systems. As a prerequisite for effective variability management is comprehensible representation, the main aim of this paper is to investigate difficulties in understanding feature models. In particular, we focus on the com- prehensibility of feature models as expressed in Common Variability Language (CVL), which was recommended for adoption as a standard by the Architectur- al Board of the Object Management Group. Using an experimental approach with participants familiar and unfamiliar with feature modeling, we analyzed comprehensibility in terms of comprehension score, time spent to complete tasks, and perceived difficulty of different feature modeling constructs. The re- sults showed that familiarity with feature modeling did not influence the com- prehension of mandatory, optional, and alternative features, although unfamiliar modelers perceived these elements more difficult than familiar modelers. OR relations were perceived as difficult regardless of the familiarity level, while constraints were significantly better understood by familiar modelers. The time spent to complete tasks was higher for familiar modelers.",
        "keywords": [
            "Variability analysis",
            "Software Product Line Engineering",
            "Model  Comprehension."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the Impact of Layout Quality\nto Understanding UML Diagrams:\nSize Matters",
        "date": 2014,
        "abstract": "Practical experience suggests that usage and understanding of UML diagrams is greatly aﬀected by the quality of their layout. While existing research failed to provide conclusive evidence in support of this hypothesis, our own previous work provided substantial evidence to this eﬀect. When studying diﬀerent factors like diagram type and expertise level, it became apparent that diagram size plays an important role, too. Since we lack an adequate understanding of this notion, in this paper, we deﬁne diagram size metrics and study their impact to modeler performance. We ﬁnd that there is a strong negative correlation between diagram size and modeler performance. Our results are highly signiﬁcant. We utilize these results to derive a recommendation on diagram sizes that are optimal for model understanding.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enabling the Development of Cognitive Eﬀective\nVisual DSLs",
        "date": 2014,
        "abstract": "The development of graphical editors for visual DSLs is far from being a trivial task. There are consequently several tools that pro- vide technical support for this task. However, this paper shows that the analysis of the main characteristics of such tools leaves some space for improvement as regard the cognitive eﬀectiveness of the visual nota- tions produced with them. To deal with this issue, this work introduces CEViNEdit, a GMF-based framework for the development of visual DSLs which takes into account Moody’s principles for the development and evaluation of graphical notations. To that end, CEViNEdit eases the selection of values for the visual variables of which the notation is com- posed, computes a set of metrics to assess the appropriateness of these values and then automates the generation of the graphical editor.",
        "keywords": [
            "Model Driven Engineering (MDE)",
            "Domain Speciﬁc Lan- guage (DSL)",
            "Visual Notation",
            "Cognitive Eﬀectiveness."
        ],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "JUMP—From Java Annotations to UML Proﬁles⋆",
        "date": 2014,
        "abstract": "The capability of UML proﬁles to serve as annotation mechanism has been recognized in both industry and research. Today’s modeling tools offer pro- ﬁles speciﬁc to platforms, such as Java, as they facilitate model-based engineer- ing approaches. However, the set of available proﬁles is considerably smaller compared to the number of existing Java libraries using annotations. This is be- cause an effective mapping between Java and UML to generate proﬁles from annotation-based libraries is missing. In this paper, we present JUMP to over- come this limitation, thereby continuing existing mapping efforts by emphasiz- ing on annotations and proﬁles. We demonstrate the practical value of JUMP by contributing proﬁles that facilitate reverse-engineering and forward-engineering scenarios for the Java platform. The evaluation of JUMP shows that proﬁles can be automatically generated from Java libraries exhibiting equal or even improved quality compared to proﬁles currently used in practice.",
        "keywords": [
            "Java Annotations",
            "UML Proﬁles",
            "Model-Based Engineering",
            "Forward Engineering",
            "Reverse Engineering."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SIGMA: Scala Internal Domain-Speciﬁc\nLanguages for Model Manipulations",
        "date": 2014,
        "abstract": "Model manipulation environments automate model opera- tions such as model consistency checking and model transformation. A number of external model manipulation Domain-Speciﬁc Languages (DSL) have been proposed, in particular for the Eclipse Modeling Frame- work (EMF). While their higher levels of abstraction result in gains in expressiveness over general-purpose languages, their limitations in ver- satility, performance, and tool support together with the need to learn new languages may signiﬁcantly contribute to accidental complexities. In this paper, we present Sigma, a family of internal DSLs embed- ded in Scala for EMF model consistency checking, model-to-model and model-to-text transformations. It combines the beneﬁts of external model manipulation DSLs with general-purpose programming taking full ad- vantage of Scala versatility, performance and tool support. The DSLs are compared to the state-of-the-art Epsilon languages in non-trivial model manipulation tasks that resulted in 20% to 70% reduction in code size and signiﬁcantly better performance.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "SIGMA"
        }
    },
    {
        "title": "A Framework to Benchmark NoSQL Data Stores\nfor Large-Scale Model Persistence",
        "date": 2014,
        "abstract": "We present a framework and methodology to benchmark NoSQL stores for large scale model persistence. NoSQL technologies potentially improve performance of some applications and provide schema- less data-structures, so are particularly suited to persisting large and het- erogeneous models. Recent studies consider only a narrow set of NoSQL stores for large scale modelling. Benchmarking many technologies re- quires substantial eﬀort due to the disparate interface each store pro- vides. Our experiments compare a broad range of NoSQL stores in terms of processor time and disc space used. The framework and methodology is evaluated through a case study that involves persisting large reverse- engineered models of open source projects. The results give tool engineers and practitioners a basis for selecting a store to persist large models.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Chaining of Model Transformations\nwith Incompatible Metamodels",
        "date": 2014,
        "abstract": "In Model-Driven Engineering (MDE) models are ﬁrst-class entities that are manipulated by means of model transformations. The development of complex and large transformations can beneﬁt from the reuse of smaller ones that can be composed according to user requirements. Composing transforma- tions is a complex problem: typically smaller transformations are discovered and selected by developers from different and heterogeneous sources. Then the iden- tiﬁed transformations are chained by means of manual and error-prone composi- tion processes. In this paper we propose an approach to automatically discover and compose transformations: developers provide the system with the source models and spec- ify the target metamodel. By relying on a repository of model transformations, all the possible transformation chains are calculated. Importantly, in case of in- compatible intermediate target and source metamodels, proper adapters are auto- matically generated in order to chain also transformations that otherwise would be discarded by limiting the reuse possibilities of available transformations.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Classiﬁcation of Model Transformation Tools:\nPattern Matching Techniques",
        "date": 2014,
        "abstract": "While comparing diﬀerent model transformation languages (MTLs), it is common to refer to their syntactic and semantic features and overlook their supporting tools’ performance. Performance is one of the aspects that can hamper the application of MDD to industrial scenar- ios. An highly declarative MTL might simply not scale well when using large models due to its supporting implementation. In this paper, we fo- cus on the several pattern matching techniques (including optimization techniques) employed in the most popular transformation tools, and dis- cuss their eﬀectiveness w.r.t. the expressive power of the languages used. Because pattern matching is the most costly operation in a transforma- tion execution, we present a classiﬁcation of the existing model transfor- mation tools according to the pattern matching optimization techniques they implement. Our classiﬁcation complements existing ones that are more focused at syntactic and semantic features of the languages sup- ported by those tools.",
        "keywords": [
            "Model Transformations",
            "Languages Design",
            "Pattern Match- ing Techniques."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Learning Implicit and Explicit Control in Model\nTransformations by Example",
        "date": 2014,
        "abstract": "We propose an evolutionary approach that, in addition to learn model transformation rules from examples, allows to capture im- plicit and explicit control over the transformation rules. The derivation of both transformation and control knowledge is performed through a heuristic search, i.e., a genetic programming algorithm, guided by the conformance with examples of past transformations supplied as pairs of source and target models. Our approach is evaluated on four model transformation problems that require non-trivial control. The obtained results are convincing for three of the four studied problems.",
        "keywords": [
            "Model transformation by example",
            "transformation control",
            "genetic programming."
        ],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "IncQuery-D: A Distributed Incremental Model Query Framework in the Cloud⋆",
        "date": 2014,
        "abstract": "Queries are the foundations of data intensive applications. In model-driven software engineering (MDE), model queries are core tech- nologies of tools and transformations. As software models are rapidly increasing in size and complexity, traditional tools exhibit scalability issues that decrease productivity and increase costs [17]. While scala- bility is a hot topic in the database community and recent NoSQL ef- forts have partially addressed many shortcomings, this happened at the cost of sacriﬁcing the ad-hoc query capabilities of SQL. Unfortunately, this is a critical problem for MDE applications due to their inherent workload complexity. In this paper, we aim to address both the scal- ability and ad-hoc querying challenges by adapting incremental graph search techniques – known from the EMF-IncQuery framework – to a distributed cloud infrastructure. We propose a novel architecture for distributed and incremental queries, and conduct experiments to demon- strate that IncQuery-D, our prototype system, can scale up from a single workstation to a cluster that can handle very large models and complex incremental queries eﬃciently.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Translating OCL to Graph Patterns⋆",
        "date": 2014,
        "abstract": "Model-driven tools use model queries for many purposes, including validation of well-formedness rules and speciﬁcation of de- rived features. The majority of declarative model query corpus avail- able in industry appears to use the OCL language. Graph pattern based queries, however, would have a number of advantages due to their more abstract speciﬁcation, such as performance improvements through ad- vanced query evaluation techniques. As query performance can be a key issue with large models, evaluating graph patterns instead of OCL queries could be useful in practice.",
        "keywords": [],
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Fully Verifying Transformation Contracts for Declarative ATL",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The Atlas Transformation Language (ATL) is today a de-facto standard in model-driven development. It is understood by the community that methods for exhaustively verifying such transformations provide an important pillar for achieving a stronger adoption of model-driven development in industry. In this paper we propose a method for verifying ATL model transformations by translating them into DSLTrans, a transformation language with limited expressiveness. Pre-/post-condition contracts are then veriﬁed on the resulting DSLTrans speciﬁcation using a symbolic-execution property prover. The technique we present in this paper is exhaustive for the declarative ATL subset, meaning that if a contract holds, it will hold when any input model is passed to the ATL transformation being checked. We explore the scalability of our technique using a set of examples, including a model transformation developed in collaboration with our industrial partner.",
        "keywords": [
            "Model transformation",
            "Formal veriﬁcation",
            "ATL",
            "Contracts",
            "Symbolic execution",
            "Pre-/Post-conditions"
        ],
        "authors": [
            "Bentley James Oakes",
            "Javier Troya",
            "Levi L´ucio",
            "and Manuel Wimmer"
        ],
        "file_path": "data/models/models15/Fully verifying transformation contracts for declarative ATL.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "ATL, DSLTrans"
        }
    },
    {
        "title": "Modeling User Intentions for In-Car Infotainment Systems using Bayesian Networks",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "To support users in operating a computer system with a varying set of functions, it is fundamental to understand their intentions, e.g., within an in-car infotainment system. Although the development of current in-car infotainment systems is already model-based, explicitly gathering and modeling user intentions is currently not supported. However, manually creating software that predicts user intentions is complex, error-prone and expensive. Model-based development can help in overcoming these issues. In this paper, we present an approach for modeling a user’s intention based on Bayesian networks. We support developers of in-car infotainment systems by providing means to model possible intentions of users according to the current situation. We further allow modeling of user preferences and show how the modeled intentions may change during run-time as a result of the user’s behavior. We demonstrate feasibility of our approach using an industrial example of an intention-aware in-car infotainment system.",
        "keywords": [],
        "authors": [
            "Daniel L¨uddecke",
            "Christoph Seidl",
            "Jens Schneider",
            "Ina Schaefer"
        ],
        "file_path": "data/models/models15/Modeling user intentions for in-car infotainment systems using Bayesian networks.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSPa: A System of Security Design Patterns for Systematically Engineering Secure Systems",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model-Driven Security (MDS) for secure systems development still has limitations to be more applicable in practice. A recent systematic review of MDS shows that current MDS approaches have not dealt with multiple security concerns systematically. Besides, catalogs of security patterns which can address multiple security concerns have not been applied eﬃciently. This paper presents an MDS approach based on a uniﬁed System of Security design Patterns (SoSPa). In SoSPa, security design patterns are collected, speciﬁed as reusable aspect models to form a coherent system of them that guides developers in systematically addressing multiple security concerns. SoSPa consists of not only interrelated security design patterns but also a reﬁnement process towards their application. We applied SoSPa to design the security of crisis management systems. The result shows that multiple security concerns in the case study have been addressed by systematically integrating diﬀerent security solutions.",
        "keywords": [],
        "authors": [
            "Phu H. Nguyen",
            "Koen Yskout",
            "Thomas Heyman",
            "Jacques Klein",
            "Riccardo Scandariato",
            "Yves Le Traon"
        ],
        "file_path": "data/models/models15/SoSPa A system of Security design Patterns for systematically engineering secure systems.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Performance Prediction upon Toolchain Migration in Model-Based Software",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Changing the development environment can have severe impacts on the system behavior such as the execution-time performance. Since it can be costly to migrate a software application, engineers would like to predict the performance parameters of the application under the new environment with as little effort as possible.\n\nIn this paper, we concentrate on model-driven development and provide a methodology to estimate the execution-time performance of application models under different toolchains. Our approach has low cost compared to the migration effort of an entire application. As part of the approach, we provide methods for characterizing model-driven applications, an algorithm for generating application-speciﬁc microbenchmarks, and results on using different methods for estimating the performance. In the work, we focus on SCADE as the development toolchain and use a Cruise Control and a Water Level application as case studies to conﬁrm the technical feasibility and viability of our technique.",
        "keywords": [
            "Model-based development",
            "Migration",
            "Automated Code Generation",
            "Estimation",
            "Prediction"
        ],
        "authors": [
            "Aymen Ketata",
            "Carlos Moreno",
            "Sebastian Fischmeister",
            "Jia Liang",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/models/models15/Performance prediction upon toolchain migration in model-based software.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Engineering Tagging Languages for DSLs",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "To keep a DSL clean, readable and reusable in different contexts, it is useful to deﬁne a separate tagging language. A tag model logically adds information to the tagged DSL model while technically keeping the artifacts separated. Using a generic tagging language leads to promiscuous tag models, whereas deﬁning a target DSL-speciﬁc tag language has a high initial overhead. This paper presents a systematic approach to deﬁne a DSL-speciﬁc tag language and a corresponding schema language, combining the advantages of both worlds: (a) the tag language speciﬁcally ﬁts to the DSL, (b) the artifacts are kept separated and enabling reuse with different tag decorations, (c) the tag language follows a deﬁned type schema, and (d) systematic derivation considerably reduces the effort necessary to implement the tag language. An example shows that it can at least partially be realized by a generator and applied for any kind of DSL.",
        "keywords": [
            "Software Engineering",
            "Modeling",
            "MDE",
            "GSE"
        ],
        "authors": [
            "Timo Greifenberg",
            "Markus Look",
            "Sebastian Roidl",
            "Bernhard Rumpe"
        ],
        "file_path": "data/models/models15/Engineering tagging languages for DSLs.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extracting Frame Conditions from Operation Contracts",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "In behavioral modeling, operation contracts deﬁned by pre- and postconditions describe the effects on model properties (i.e., model elements such as attributes, links, etc.) that are enforced by an operation. However, it is usually omitted which model properties should not be modiﬁed. Deﬁning so-called frame conditions can ﬁll this gap. But, thus far, these have to be deﬁned manually – a time-consuming task. In this work, we propose a methodology which aims to support the modeler in the deﬁnition of the frame conditions by extracting suggestions based on an automatic analysis of operation contracts provided in OCL. More precisely, the proposed approach performs a structural analysis of pre- and postconditions together with invariants in order to categorize which class and object properties are clearly “variable” or “unaffected” – and which are “ambiguous”, i.e. indeed require a more thorough inspection. The developed concepts are implemented as a prototype and evaluated by means of several example models known from the literature.",
        "keywords": [],
        "authors": [
            "Philipp Niemann",
            "Frank Hilken",
            "Martin Gogolla",
            "Robert Wille"
        ],
        "file_path": "data/models/models15/Extracting frame conditions from operation contracts.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "State Machine Antipatterns for UML-RT",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Software development guidelines are a set of rules which can help improve the quality of software. These rules are deﬁned on the basis of experience gained by the software development community over time. Software antipatterns are a powerful and effective form of guidelines used for the identiﬁcation of bad design choices and development practices that often lead to poor-quality software. This paper introduces a set of seven state machine antipatterns for the model-based development of real time embedded software systems. Each of these antipatterns is described with a pair of examples: one for the antipattern itself and a second one for improved, refactored solution.",
        "keywords": [],
        "authors": [
            "Tuhin Kanti Das",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models15/State machine antipatterns for UML-RT.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Textual Diagram Layout Language and Visualization Algorithm",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Graphical diagrams are an excellent source of information for understanding models. On the other hand, editing, storing and versioning models are more efficient in textual representations. In order to combine the advantages of these two representations, diagrams have to be generated from models defined in text. The generated diagrams are usually created by autolayout algorithms based on heuristics.\n\nIn this paper we argue that automatically laid out diagrams are not ideal. Instead, we propose a textual layout description language that allows users to define the arrangement of those diagram elements they consider important. The paper also presents algorithms that create diagrams according to the layout description and arrange the underspecified elements automatically.\n\nThe paper reports on the implementation of the proposed layout description language as an embedded language in Java. It is used to generate class and state machine diagrams compatible with the Papyrus UML editor.",
        "keywords": [],
        "authors": [
            "Balázs Gregorics",
            "Tibor Gregorics",
            "Gábor Ferenc Kovács",
            "András Dobreff",
            "Gergely Dévai"
        ],
        "file_path": "data/models/models15/Textual diagram layout language and visualization algorithm.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MODELS 2015 Organization",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models15/Contents.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reusable Event Types for Models at Runtime to Support the Examination of Runtime Phenomena",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Today’s software is getting more and more complex and harder to understand. Models help to organize knowledge and emphasize the structure of a software at a higher abstraction level. While the usage of model-driven techniques is widely adopted during software construction, it is still an open research topic if models can also be used to make runtime phenomena more comprehensible as well. It is not obvious which models are suitable for manual analysis and which model elements can be related to what type of runtime events. This paper proposes a collection of runtime event types that can be reused for various systems and meta-models. Based on these event types, information can be derived which help human observers to assess the current system state. Our approach is applied in a case study and evaluated regarding generalisability and completeness by relating it to two different meta-models.",
        "keywords": [
            "events",
            "examination",
            "models",
            "runtime"
        ],
        "authors": [
            "Michael Szvetits",
            "Uwe Zdun"
        ],
        "file_path": "data/models/models15/Reusable event types for models at runtime to support the examination of runtime phenomena.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistent Co-Evolution of Models and Transformations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Evolving metamodels are in the center of Model-Driven Engineering, necessitating the co-evolution of dependent artifacts like models and transformations. While model co-evolution has been extensively studied, transformation co-evolution has received less attention up to now. Current approaches for transformation co-evolution provide a ﬁxed, restricted set of metamodel (MM) changes, only. Furthermore, composite changes are treated as monolithic units, which may lead to inconsistent co-evolution for overlapping atomic changes and prohibits extensibility. Finally, transformation co-evolution is considered in isolation, possibly inducing inconsistencies between model and transformation co-evolution. To overcome these limitations, we propose a complete set of atomic MM changes being able to describe arbitrary MM evolutions. Reusability and extensibility are supported by means of change composition, ensuring an intra-artifact consistent co-evolution. Furthermore, each change provides resolution actions for both, models and transformations, ensuring an inter-artifact consistent co-evolution. Based on our conceptual approach, a prototypical implementation is presented.",
        "keywords": [],
        "authors": [
            "Angelika Kusel",
            "Jürgen Etzlstorfer",
            "Elisabeth Kapsammer",
            "Werner Retschitzegger",
            "Wieland Schwinger",
            "Johannes Schönböck"
        ],
        "file_path": "data/models/models15/Consistent co-evolution of models and transformations.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Based Framework for Probabilistic Simulation of Legal Policies",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Legal policy simulation is an important decision-support tool in domains such as taxation. The primary goal of legal policy simulation is predicting how changes in the law affect measures of interest, e.g., revenue. Currently, legal policies are simulated via a combination of spreadsheets and software code. This poses a validation challenge both due to complexity reasons and due to legal experts lacking the expertise to understand software code. A further challenge is that representative data for simulation may be unavailable, thus necessitating a data generator. We develop a framework for legal policy simulation that is aimed at addressing these challenges. The framework uses models for specifying both legal policies and the probabilistic characteristics of the underlying population. We devise an automated algorithm for simulation data generation. We evaluate our framework through a case study on Luxembourg’s Tax Law.",
        "keywords": [
            "Legal Policies",
            "Simulation",
            "UML Proﬁles",
            "Model-Driven Code Generation",
            "Probabilistic Data Generation"
        ],
        "authors": [
            "Ghanem Soltana",
            "Nicolas Sannier",
            "Mehrdad Sabetzadeh",
            "and Lionel C. Briand"
        ],
        "file_path": "data/models/models15/A model-based framework for probabilistic simulation of legal policies.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Statistical Analysis Approach to Assist Model Transformation Evolution",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Model Driven Engineering (MDE) is essentially based in metamodel deﬁnition, model edition and the speciﬁcation of model transformations (MT) among these. In many cases the development, evolution and adaptation of these transformations is still carried out without the support of proper methods and tools to reduce the effort and related costs to these activities. In this work, a novel model testing approach speciﬁcally designed to assist the engineer in model transformation evolution is presented. A statistical analysis of the actual behavior of the transformations is performed by means of the computation of well-known information extraction metrics. In order to assist the MT adaptation, a detailed interpretation of the possible results of those metrics is also presented. And ﬁnally, the results of applying this approach on a Model-Driven Reverse Engineering (MDRE) scenario deﬁned in the context of the MIGRARIA project are discussed.",
        "keywords": [
            "Model Transformation",
            "Model Transformation Evolution",
            "Model Transformation Testing",
            "Testing Oracle"
        ],
        "authors": [
            "Roberto Rodriguez-Echeverria",
            "Fernando Macias"
        ],
        "file_path": "data/models/models15/A statistical analysis approach to assist model transformation evolution.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Quick Fixing ATL Model Transformations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Abstract—The correctness of model transformations is key to\nobtain reliable MDE solutions. However, current transformation\ntools provide limited support to statically detect and correct\nerrors. This way, the identiﬁcation of errors and their correction\nare mostly manual activities. Our aim is to improve this situation.\nBased on a static analyser for ATL model transformations\nwhich we have previously built, we present a method and a system\nto propose quick ﬁxes for transformation errors. The analyser\nis based on a combination of program analysis and constraint\nsolving, and our quick ﬁx generation technique makes use of the\nanalyser features to provide a range of ﬁxes, notably some non-\ntrivial, transformation-speciﬁc ones. Our approach integrates\nseamlessly with the ATL editor. We provide an evaluation based\non an existing faulty transformation, and automatically generated\ntransformation mutants, showing overall good results.",
        "keywords": [
            "Model Transformation",
            "Transformation Static Analysis",
            "ATL",
            "Quick ﬁxes",
            "Veriﬁcation and Testing"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models15/Quick fixing ATL model transformations.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "A-posteriori Typing for Model-Driven Engineering",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model-Driven Engineering is founded on the ability to create and process models conformant to a meta-model. Hence, meta-model classes are used in two ways: as templates to create objects, and as classiﬁers for them. While these two aspects are inherently tied in most meta-modelling approaches, in this paper, we discuss the beneﬁts of their decoupling. Thus, we rely on standard mechanisms for object creation and propose a-posteriori typing as a means to reclassify objects and enable multiple, partial, dynamic typings. This approach enhances ﬂexibility, permitting unanticipated reutilization (as existing model management operations deﬁned for a meta-model can be reused with other models once they get reclassiﬁed), as well as model transformation by reclassiﬁcation. We show the underlying theory behind the introduced concepts, and illustrate its applicability using our METADEPTH meta-modelling tool.",
        "keywords": [
            "A-posteriori typing",
            "Model typing",
            "Partial typing",
            "Dynamic typing",
            "Flexible MDE"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/models/models15/A-posteriori typing for Model-Driven Engineering.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Situational Method for Semi-automated Enterprise Architecture Documentation (SoSyM Abstract)",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Large organizations critically rely on their IT infrastructure. So called Enterprise Architecture (EA) models are often created to understand how the IT supports the business and used to optimize the IT and align it with the business. However, the models grow very large and are hard to keep up-to-date. Current approaches focus on automated data collection to tackle this problem, which is not feasible in many situations. In this paper we present a semi-automated EA documentation method and tool support that tackles this problem and takes the organizational contexts into account.",
        "keywords": [],
        "authors": [
            "Matthias Farwick",
            "Christian M. Schweda",
            "Ruth Breu",
            "Inge Hanschke"
        ],
        "file_path": "data/models/models15/A situational method for semi-automated enterprise architecture documentation -SoSyM abstract-.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Identiﬁcation of Simulink Model Antipattern Instances using Model Clone Detection",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "One challenge facing the Model-Driven Engineering community is the need for model quality assurance. Speciﬁcally, there should be better facilities for analyzing models automat-ically. One measure of quality is the presence or absence of good and bad properties, such as patterns and antipatterns, respectively. We elaborate on and validate our earlier idea of detecting patterns in model-based systems using model clone detection by devising a Simulink antipattern instance detector. We chose Simulink because it is prevalent in industry, has mature model clone detection techniques, and interests our industrial partners. We demonstrate our technique using near-miss cross-clone detection to ﬁnd instances of Simulink antipatterns derived from the literature in four sets of public Simulink projects. We present our detection results, highlight interesting examples, and discuss potential improvements to our approach. We hope this work provides a ﬁrst step in helping practitioners improve Simulink model quality and further research in the area.",
        "keywords": [],
        "authors": [
            "Matthew Stephan",
            "James R. Cordy"
        ],
        "file_path": "data/models/models15/Identification of Simulink model antipattern instances using model clone detection.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automobile: Aircraft or Smartphone? Modeling Challenges and Opportunities in Automotive Systems",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Automotive systems are turning out to be one of the most complex consumer electronic systems being ever built. For the modern day users, they are products like smartphones and tablets but in size, complexity and quality and safety requirements they match if not exceed aircraft, and similar high integrity systems. Many of the major advances in Software engineering like model based development, platform based design and product line engineering have been introduced in the development of automotive electronic and software subsystems, which involve million lines of code and tens of electronic control units interconnected with multiple communication buses. This talk will highlight the challenges, current practices and new developments in the industry in building next generation automotive software from the modeling and analysis perspective. The challenges include traditional issues like system integration and feature interaction arising out of the federated development model, heterogeneity in subsystem behavior, time and space distributed development of software and the recent and rapidly increasing demand for advanced driver assistance features and system level requirements like fault tolerance and security. The talk attempts to outline a set of requirements for modeling from the perspective of system design and analysis. The talk will also touch upon some of the research and developments efforts currently ongoing within and with our external partners to meet these challenges.",
        "keywords": [],
        "authors": [
            "Ramesh S"
        ],
        "file_path": "data/models/models15/Automobile Aircraft or smartphone- Modeling challenges and opportunities in Automotive Systems -keynote-.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enriching Megamodel Management with Collection-Based Operators",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Megamodels are often used in MDE to describe collections of models and relationships between them. Typical collection-based operations – map, reduce, ﬁlter – cannot be applied directly to megamodels since these operators need to take relationships between models into consideration. In this paper, we propose adapted versions of these operators, demonstrating them on four megamodeling scenarios. We then analyze their applicability for handling industrial-sized megamodels. Finally, we report on a reference implementation of the operators and experimental results using it.",
        "keywords": [],
        "authors": [
            "Rick Salay",
            "Sahar Kokaly",
            "Alessio Di Sandro",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models15/Enriching megamodel management with collection-based operators.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling the Climate System: Is Model-Based Science Like Model-Based Engineering?",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Modern computational science is largely a model-building activity. At first sight, the models that scientists construct seem to differ radically from those used in model-based engineering. Scientists tend to build indicative ('how things are') models of the world using sets of continuous equations, while engineers tend to build optative ('how things should be') models of the world using structural and procedural abstractions. But a closer look reveals many fascinating similarities. In this talk, I will explore the relationship between the two types of modelling, drawing on my field studies of how climate modellers work. I'll begin with an overview of what a climate model is and how it is used. I'll then dive deeper into the engineering challenges of constructing a climate model, including the challenges of coupling disparate model components, dealing with model version-ing and model management issues, and the role that climate models play in enabling collaborative work. In the process, I hope to inspire people to explore how ideas from model-based software engineering might contribute to scientific mod-elling in general, and, more specifically, to the societal grand challenge of climate change.",
        "keywords": [],
        "authors": [
            "Steve Easterbrook"
        ],
        "file_path": "data/models/models15/Modelling the climate system Is model-based science like model-based engineering- -Keynote-.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Formalizing the ISO/IEC/IEEE 29119 Software Testing Standard",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in the literature and in the practice. However, all of the techniques have been developed using their own concepts and terminology of MBT, which are very often different than other techniques and at times have conflicting semantics. Moreover, while working on MBT projects with our industrial partners in the last several years, we were unable to find a unified way of defining MBT techniques based on standard terminology. To precisely define MBT concepts with the aim of providing common understanding of MBT terminology across techniques, we formalize a small subset of the recently released ISO/IEC/IEEE 29119 Software Testing Standard as a conceptual model (UML class diagrams) together with OCL constraints. The conceptual model captures all the necessary concepts based on the standard terminology that are mandatory or optional in the context of MBT techniques and can be used to define new MBT tools and techniques. To validate the conceptual model, we instantiated its concepts for various MBT techniques previously developed in the context of our industrial partners. Such instantiation automatically enforces the specified OCL constraints. This type of validation provided us feedback to further refine the conceptual model. Finally, we also provide our experiences and lessons learnt for such formalization and validation.",
        "keywords": [
            "Model-Based Testing",
            "ISO/IEC/IEEE 29119",
            "UML",
            "Test Case Generation",
            "Modeling Methodology"
        ],
        "authors": [
            "Shaukat Ali",
            "Tao Yue"
        ],
        "file_path": "data/models/models15/Formalizing the ISO-IEC-IEEE 29119 Software Testing Standard.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the Use of UML Documentation in Software Maintenance: Results from a Survey in Industry",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "This paper presents the findings of a survey on the use of UML in software maintenance, carried out with 178 professionals working on software maintenance projects in 12 different countries. As part of long-term research we are carrying out to investigate the benefits of using UML in software maintenance, the main objectives of this survey are: 1) to explore whether UML diagrams are being used in software industry maintenance projects; 2) to see what UML diagrams are the most effective for software maintenance; 3) to find out what the perceived benefits of using UML diagrams are; and 4) to contextualize the kind of companies that use UML documentation in software maintenance. Some complementary results based on the way the documentation is used (whether it is UML-based or not) during software maintenance are also presented.",
        "keywords": [
            "UML",
            "Software Maintenance",
            "Survey"
        ],
        "authors": [
            "Ana M. Fernández-Sáez",
            "DaniloCaivano",
            "Marcela Genero",
            "Michel R.V. Chaudron"
        ],
        "file_path": "data/models/models15/On the use of UML documentation in software maintenance Results from a survey in industry.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incremental Symbolic Execution of Evolving State Machines",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "This paper introduces two complementary techniques, memoization-based and dependency-based incremental symbolic execution, that aim to optimize the analysis of state machine models that undergo change. We implement the two proposed techniques on IBM Rhapsody Statecharts and present some evaluation results.",
        "keywords": [],
        "authors": [
            "Amal Khalil",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models15/Incremental symbolic execution of evolving state machines.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Framework for Relating Syntactic and Semantic Model Differences",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model differencing is an important activity in model-based development processes. Differences need to be detected, analyzed, and understood to evolve systems and explore alternatives.\n\nTwo distinct approaches have been studied in the literature: syntactic differencing, which compares the concrete or abstract syntax of models, and semantic differencing, which compares models in terms of their meaning. Syntactic differencing identifies change operations that transform the syntactical representation of one model to the syntactical representation of the other. However, it does not explain their impact on the meaning of the model. Semantic model differencing is independent of syntactic changes and presents differences as elements in the semantics of one model but not the other. However, it does not reveal the syntactic changes causing these semantic differences.\n\nWe define a language independent, abstract framework, which relates syntactic change operations and semantic difference witnesses. We formalize fundamental relations of necessary and sufficient sets of change operations and analyze their properties. We further demonstrate concrete instances of the framework for three different popular modeling languages, namely, class diagrams, activity diagrams, and feature models. The framework provides a novel foundation for combining syntactic and semantic differencing.",
        "keywords": [],
        "authors": [
            "Shahar Maoz and Jan Oliver Ringert"
        ],
        "file_path": "data/models/models15/A framework for relating syntactic and semantic model differences.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synthesizing Tests for Combinatorial Coverage of\nModal Scenario Speciﬁcations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Software-intensive systems often consist of many components that interact to fulﬁll complex functionality. Testing these systems is vital, preferably by a minimal set of tests that covers all relevant cases. The behavior is typically speciﬁed by scenarios that describe what the system may, must, or must not do. When designing tests, as in the design of the system itself, the challenge is to consider interactions of scenarios. When doing this manually, critical interactions are easily overlooked. Inspired by Combinatorial Test Design, which exploits that bugs are typically found by regarding the interaction of a small set of parameters, we propose a new test coverage criterion based on scenario interactions. Furthermore, we present a novel technique for automatically synthesizing from Modal Sequence Diagram speciﬁcations a minimal set of tests that ensures a maximal coverage of possible t-wise scenario interactions. The technique is evaluated on an example speciﬁcation from an industrial project.",
        "keywords": [],
        "authors": [
            "Valerio Panzica La Manna",
            "Itai Segall",
            "Joel Greenyer"
        ],
        "file_path": "data/models/models15/Synthesizing tests for combinatorial coverage of modal scenario specifications.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward Overcoming Accidental Complexity in Organisational Decision-Making",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "This paper takes a practitioner’s perspective on the problem of organisational decision-making. Industry practice follows a refinement based iterative method for organizational decision-making. However, existing enterprise modelling tools are not complete with respect to the needs of organizational decision-making. As a result, today, a decision maker is forced to use a chain of non-interoperable tools supporting paradigmatically diverse modelling languages with the onus of their co-ordinated use lying entirely on the decision maker. This paper argues the case for a model-based approach to overcome this accidental complexity. A bridge meta-model, specifying relationships across models created by individual tools, ensures integration and a method, describing what should be done when and how, and ensures better tool integration. Validation of the proposed solution using a case study is presented with current limitations and possible means of overcoming them outlined.",
        "keywords": [
            "Organizational decision making",
            "Enterprise modeling tools",
            "Meta modelling",
            "Method"
        ],
        "authors": [
            "Vinay Kulkarni",
            "Souvik Barat",
            "Tony Clark",
            "Balbir Barn"
        ],
        "file_path": "data/models/models15/Toward overcoming accidental complexity in organisational decision-making.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models15/Sponsors.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "FRAGMENTA: A Theory of Fragmentation for MDE",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model-Driven Engineering (MDE) promotes models throughout development. However, models may become large and unwieldy even for small to medium-sized systems. This paper tackles the MDE challenges of model complexity and scalability. It proposes FRAGMENTA, a theory of modular design that breaks down overall models into fragments that can be put together to build meaningful wholes, in contrast to classical MDE approaches that are essentially monolithic. The theory is based on an algebraic description of models, fragments and clusters based on graphs and morphisms. The paper’s novelties include: (i) a mathematical treatment of fragments and a seaming mechanism of proxies to enable inter-fragment referencing, (ii) fragmentation strategies, which prescribe a fragmentation structure to model instances, (iii) FRAGMENTA’s support for both top-down and bottom-up design, and (iv) our formally proved result that shows that inheritance hierarchies remain well-formed (acyclic) globally when fragments are composed provided some local fragment constraints are met.",
        "keywords": [
            "Model-driven engineering",
            "meta-modelling",
            "modularity",
            "graphs",
            "scalability",
            "model composition"
        ],
        "authors": [
            "Nuno Amálio",
            "Juan de Lara",
            "Esther Guerra"
        ],
        "file_path": "data/models/models15/Fragmenta A theory of fragmentation for MDE.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Automated Model Based Testing Approach for Platform Games",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Game development has recently gained a lot of momentum and is now a major software development industry. Platform games are being revived with both their 2D and 3D versions being developed. A major challenge faced by the industry is a lack of automated system-level approaches for game testing. Currently in most game development organizations, games are tested manually or using semi-automated techniques. Such testing techniques do not scale to the industry requirements where more systematic and repeatable approaches are required. In this paper we propose a model-based testing approach for automated black box functional testing of platform games. The paper provides a detailed modeling methodology to support automated system-level game testing. As part of the methodology, we provide guidelines for modeling the platform games for testing using our proposed game test modeling profile. We use domain modeling for representing the game structure and UML state machines for behavioral modeling. We present the details related to automated test case generation, execution, and oracle generation. We demonstrate our model-based testing approach by applying it on two cases studies, a widely referenced and open source implementation of Mario brothers game and an industrial case study of an endless runner game. The proposed approach was able to identify major faults in the open source game implementation. Our results showed that the proposed approach is practical and can be applied successfully on industrial games.",
        "keywords": [
            "Model based testing (MBT)",
            "game testing",
            "black box testing",
            "functional testing",
            "system-level testing",
            "and unified modeling language (UML)"
        ],
        "authors": [
            "Sidra Iftikhar",
            "Muhammad Zohaib Iqbal",
            "Muhammad Uzair Khan",
            "Wardah Mahmood"
        ],
        "file_path": "data/models/models15/An automated model based testing approach for platform games.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Pattern-Based Debugging of Declarative Models",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Pattern-based debugging compares the engineer’s model to a pre-computed library of patterns, and generates discriminating examples that help the engineer decide if the model’s constraints need to be strengthened or weakened. A number of tactics are used to help connect the generated examples to the text of the model. This technique augments existing example/counter-example generators and unsatisﬁable core analysis tools, to help the engineer better localize and understand defects caused by complete overconstraint, partial overconstraint, and underconstraint. The technique is applied to localizing, understanding, and ﬁxing a defect in an Alloy model of Dijkstra’s Dining Philosopher’s problem. Automating the search procedure remains as essential future work.",
        "keywords": [],
        "authors": [
            "Vajih Montaghami and Derek Rayside"
        ],
        "file_path": "data/models/models15/Pattern-based debugging of declarative models.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing the Communication Value of UML Models with Graphical Layers",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "UML is defined as a visual modeling language for specifying, constructing, and documenting software intensive systems. In that context, UML diagrams play a central role in the whole software engineering process, starting from early analysis, through implementation, to maintenance. Recent surveys of UML use in industry showed that software practitioners use it on a regular basis, and particularly for communication and as a mental-assist tool. However, they also pointed out the following weaknesses: the lack of context, graphical layout problems, and the language’s inadequacy as a facility for communication between technical teams and their clients. In this paper, we present a general approach that addresses these problems by enhancing the effectiveness of UML models as a communication vehicle. Our approach is based on expressing stakeholder-specific viewpoints through the use of secondary notations. This involves the use of auxiliary visual variables (e.g., color, position, size) that are not formally specified in UML. To that end, we extend the traditional concept of layer found in many graphical editors to UML diagram editors. FlipLayers is an implementation of our approach. It is in the form of a plugin for the Papyrus modeling environment. One scenario with several case studies is presented in the paper to demonstrate the benefits of our approach and also to illustrate how to express viewpoints with FlipLayers.",
        "keywords": [],
        "authors": [
            "Yosser El Ahmar",
            "Sébastien Gérard",
            "Cédric Dumoulin",
            "Xavier Le Pallec"
        ],
        "file_path": "data/models/models15/Enhancing the communication value of UML models with graphical layers.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Systematically Deriving Domain-Speciﬁc Transformation Languages",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Model transformations are helpful to evolve, refactor, refine and maintain models. While domain-specific languages are normally intuitive for modelers, common model transformation approaches (regardless of whether they transform graphical or textual models) are based on the modeling language’s abstract syntax requiring the modeler to learn the internal representation of the model to describe transformations. This paper presents a process that allows to systematically derive a textual domain-specific transformation language from the grammar of a given textual modeling language. As example, we apply this systematic derivation to UML class diagrams to obtain a domain-specific transformation language called CDTrans. CDTrans incorporates the concrete syntax of class diagrams which is already familiar to the modeler and extends it with a few transformation operators. To demonstrate the usefulness of the derived transformation language, we describe several refactoring transformations.",
        "keywords": [
            "Model transformation",
            "concrete syntax",
            "domain-specific",
            "language-specific",
            "systematic derivation",
            "generation"
        ],
        "authors": [
            "Katrin Hölldobler",
            "Bernhard Rumpe",
            "Ingo Weisemöller"
        ],
        "file_path": "data/models/models15/Systematically deriving domain-specific transformation languages.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "CDTrans"
        }
    },
    {
        "title": "Process Mining in Software Systems\nDiscovering Real-Life Business Transactions and Process Models from Distributed Systems",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "This paper presents a novel reverse engineering technique for obtaining real-life event logs from distributed systems. This allows us to analyze the operational processes of software systems under real-life conditions, and use process mining techniques to obtain precise and formal models. Hence, the work can be positioned in-between reverse engineering and process mining. We present a formal deﬁnition, implementation and an instrumentation strategy based the joinpoint-pointcut model. Two case studies are used to evaluate our approach. These concrete exam- ples demonstrate the feasibility and usefulness of our approach.",
        "keywords": [
            "Reverse Engineering",
            "Process Mining",
            "Event Log",
            "Distributed Systems",
            "Performance Analysis",
            "Process Discovery",
            "Joinpoint-Pointcut Model",
            "Aspect-Oriented Programming"
        ],
        "authors": [
            "Maikel Leemans (m.leemans@tue.nl) and Wil M. P. van der Aalst (w.m.p.v.d.aalst@tue.nl)"
        ],
        "file_path": "data/models/models15/Process mining in software systems Discovering real-life business transactions and process models from distributed systems.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Aalst",
            "Wil M. P. van der",
            "Ali",
            "Shaukat",
            "Almorsy",
            "Mohamed",
            "Amálio",
            "Nuno",
            "Atkinson",
            "Colin",
            "Auguston",
            "Mikhail",
            "Babau",
            "Jean-Philippe",
            "Barat",
            "Souvik",
            "Barn",
            "Balbir",
            "Bastarrica",
            "María Cecilia",
            "Basten",
            "Twan",
            "Breu",
            "Ruth",
            "Briand",
            "Lionel C.",
            "Burgueño",
            "Loli",
            "Caivano",
            "Danilo",
            "Chaudron",
            "Michel R. V.",
            "Chechik",
            "Marsha",
            "Chen",
            "Tieming",
            "Clark",
            "Tony",
            "Combemale",
            "Benoit",
            "Conte",
            "Tayana",
            "Cordy",
            "James R.",
            "Czarnecki",
            "Krzysztof",
            "Das",
            "Tuhin Kanti",
            "DeAntoni",
            "Julien",
            "De Lara",
            "Juan",
            "Dévai",
            "Gergely",
            "Diaz-Pace",
            "J. Andrés",
            "Dingel",
            "Juergen",
            "Di Sandro",
            "Alessio",
            "Dobreff",
            "András",
            "Dong",
            "Jin Song",
            "Drechsler",
            "Rolf",
            "Drira",
            "Khalil",
            "Dumoulin",
            "Cédric",
            "Easterbrook",
            "Steve",
            "Eder",
            "Klaus",
            "Eichler",
            "Cédric",
            "Elaasar",
            "Maged",
            "El Ahmar",
            "Yosser",
            "Etzlstorfer",
            "Jürgen",
            "Farwick",
            "Matthias",
            "Fernández-Sáez",
            "Ana M.",
            "Fischmeister",
            "Sebastian",
            "Fouché",
            "Alexis",
            "Fouquet",
            "Francois",
            "Garmendia",
            "Antonio",
            "Geilen",
            "Marc",
            "Genero",
            "Marcela",
            "Gérard",
            "Sébastien",
            "Gerbig",
            "Ralph",
            "Gogolla",
            "Martin",
            "Goknil",
            "Arda",
            "Greenyer",
            "Joel",
            "Gregorics",
            "Balázs",
            "Gregorics",
            "Tibor",
            "Greifenberg",
            "Timo",
            "Grieco",
            "Alfredo",
            "Grünbacher",
            "Paul",
            "Grundy",
            "John",
            "Guerra",
            "Esther",
            "Hajri",
            "Ines",
            "Hanschke",
            "Inge",
            "Hartmann",
            "Thomas",
            "Heyman",
            "Thomas",
            "Hilken",
            "Christoph",
            "Hilken",
            "Frank",
            "Hölldobler",
            "Katrin",
            "Iftikhar",
            "Sidra",
            "Iqbal",
            "Muhammad Zohaib",
            "Jacobs",
            "Johan",
            "Kapsammer",
            "Elisabeth",
            "Kerboeuf",
            "Mickaël",
            "Ketata",
            "Aymen",
            "Khalil",
            "Amal",
            "Khan",
            "Muhammad Uzair",
            "Kholkar",
            "Deepali",
            "Kienzle",
            "Jörg",
            "Klein",
            "Jacques",
            "Kokaly",
            "Sahar",
            "Kovács",
            "Gábor Ferenc",
            "Kˇrikava",
            "Filip",
            "Kühne",
            "Thomas",
            "Kulkarni",
            "Vinay",
            "Kusel",
            "Angelika",
            "Leemans",
            "Maikel",
            "Le Pallec",
            "Xavier",
            "Le Traon",
            "Yves",
            "Lettner",
            "Daniela",
            "Liang",
            "Jia",
            "Liu",
            "Yang",
            "Look",
            "Markus",
            "Lúcio",
            "Levi",
            "Lüddecke",
            "Daniel",
            "Macias",
            "Fernando",
            "Mahmood",
            "Wardah",
            "Mallet",
            "Frédéric",
            "Maoz",
            "Shahar",
            "Marcos",
            "Claudia",
            "Marczak",
            "Sabrina",
            "Martin",
            "Kevin J. M.",
            "Moawad",
            "Assaad",
            "Montaghami",
            "Vajih",
            "Monteil",
            "Thierry",
            "Moreno",
            "Carlos",
            "Murphy",
            "Gail",
            "Nain",
            "Gregory",
            "Nguyen",
            "Phu H.",
            "Nguyen",
            "Tuong Huan",
            "Niemann",
            "Philipp",
            "Noyrit",
            "Florian",
            "Oakes",
            "Bentley James",
            "Oran",
            "Ana Carolina",
            "Panzica La Manna",
            "Valerio",
            "Peleska",
            "Jan",
            "Perovich",
            "Daniel",
            "Pescador",
            "Ana",
            "Prähofer",
            "Herbert",
            "Przigoda",
            "Nils",
            "Rabelo",
            "Jacilane",
            "Rago",
            "Alejandro",
            "Rayside",
            "Derek",
            "Reniers",
            "Michel",
            "Retschitzegger",
            "Werner",
            "Ringert",
            "Jan Oliver",
            "Rodriguez-Echeverria",
            "Roberto",
            "Roidl",
            "Sebastian",
            "Rouvoy",
            "Romain",
            "Rumpe",
            "Bernhard",
            "S",
            "Ramesh",
            "Sabetzadeh",
            "Mehrdad",
            "Salay",
            "Rick",
            "Sánchez Cuadrado",
            "Jesús",
            "Sanden",
            "Bram van der",
            "Sannier",
            "Nicolas",
            "Scandariato",
            "Riccardo",
            "Schaefer",
            "Ina",
            "Schiffelers",
            "Ramon",
            "Schneider",
            "Jens",
            "Schönböck",
            "Johannes",
            "Schöttle",
            "Matthias",
            "Schweda",
            "Christian M.",
            "Schwinger",
            "Wieland",
            "Segall",
            "Itai",
            "Seidl",
            "Christoph",
            "Seinturier",
            "Lionel",
            "Silvestre",
            "Luis",
            "Simmonds",
            "Jocelyn",
            "Soltana",
            "Ghanem",
            "Song",
            "Songzheng",
            "Stephan",
            "Matthew",
            "Stephany",
            "Thierry",
            "Stolf",
            "Patricia",
            "Sun",
            "Jun",
            "Sunkle",
            "Sagar",
            "Szvetits",
            "Michael",
            "Troya",
            "Javier",
            "Valentim",
            "Natasha M. Costa",
            "Vallecillo",
            "Antonio",
            "Vallejo",
            "Paola",
            "Vara Larsen",
            "Matias Ezequiel",
            "Voeten",
            "Jeroen",
            "Weisemöller",
            "Ingo",
            "Wille",
            "Robert",
            "Wimmer",
            "Manuel",
            "Yskout",
            "Koen",
            "Yue",
            "Tao",
            "Zdun",
            "Uwe"
        ],
        "file_path": "data/models/models15/Author index.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Stream my Models: Reactive Peer-to-Peer Distributed Models@run.time",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The models@run.time paradigm promotes the use of models during the execution of cyber-physical systems to represent their context and to reason about their runtime behaviour. However, current modeling techniques do not allow to cope at the same time with the large-scale, distributed, and constantly changing nature of these systems. In this paper, we introduce a distributed models@run.time approach, combining ideas from reactive programming, peer-to-peer distribution, and large-scale models@run.time. We deﬁne distributed models as observable streams of chunks that are exchanged between nodes in a peer-to-peer manner. A lazy loading strategy allows to transparently access the complete virtual model from every node, although chunks are actually distributed across nodes. Observers and automatic reloading of chunks enable a reactive programming style. We integrated our approach into the Kevoree Modeling Framework and demonstrate that it enables frequently changing, reactive distributed models that can scale to millions of elements and several thousand nodes.",
        "keywords": [
            "Models@run.time",
            "Distributed models",
            "Reactive programming",
            "Asynchronous programming",
            "Peer-to-peer"
        ],
        "authors": [
            "Thomas Hartmann",
            "Assaad Moawad",
            "Francois Fouquet",
            "Gregory Nain",
            "Jacques Klein",
            "and Yves Le Traon"
        ],
        "file_path": "data/models/models15/Stream my models Reactive peer-to-peer distributed models-run.time.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Concern-Oriented Interfaces for Model-Based Reuse of APIs",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Reuse is essential in modern software engineering, but limited in the context of MDE by the poor availability of reusable models. On the other hand, reusable code artifacts such as frameworks and libraries are abundant. This paper presents an approach to raise reusable code artifacts to the modelling level by modelling their API using concern-oriented techniques, thus enabling their use in the context of MDE. Our API interface models contain additional information, such as the encapsulated features and their impacts, to assist the developer in the reuse process. Once he has speciﬁed his needs, the model interface exposes only the API elements relevant for this speciﬁc reuse at the model level, together with the required usage protocol. We show how this approach is applied by hand to model the interface of a small GUI framework and outline how we envision this process to be performed semi-automatically.",
        "keywords": [],
        "authors": [
            "Matthias Schöttle and Jörg Kienzle"
        ],
        "file_path": "data/models/models15/Concern-oriented interfaces for model-based reuse of APIs.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying Product Line Use Case Modeling in an Industrial Automotive Embedded System: Lessons Learned and a Reﬁned Approach",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "In this paper, we propose, apply, and assess Product line Use case modeling Method (PUM), an approach that supports modeling variability at different levels of granularity in use cases and domain models. Our motivation is that, in many software development environments, use case modeling drives interactions among stakeholders and, therefore, use cases and domain models are common practice for requirements elicitation and analysis. In PUM, we integrate and adapt existing product line extensions for use cases and introduce some template extensions for use case speciﬁcations. Variability is captured in use case diagrams while it is reﬂected at a greater level of detail in use case speciﬁcations. Variability in domain concepts is captured in domain models. PUM is supported by a tool relying on Natural Language Processing (NLP). We applied PUM to an industrial automotive embedded system and report lessons learned and results from structured interviews with experienced engineers.",
        "keywords": [],
        "authors": [
            "Ines Hajri",
            "Arda Goknil",
            "Lionel C. Briand",
            "Thierry Stephany"
        ],
        "file_path": "data/models/models15/Applying product line Use case modeling in an industrial automotive embedded system Lessons learned and a refined approach.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Beyond Discrete Modeling: A Continuous and Efﬁcient Model for IoT",
        "submission-date": "2015/MM",
        "publication-date": "2015/MM",
        "abstract": "Internet of Things applications analyze our past habits through sensor measures to anticipate future trends. To yield accurate predictions, intelligent systems not only rely on single numerical values, but also on structured models aggregated from different sensors. Computation theory, based on the discretization of observable data into timed events, can easily lead to millions of values. Time series and similar database structures can efﬁciently index the mere data, but quickly reach computation and storage limits when it comes to structuring and processing IoT data. We propose a concept of continuous models that can handle high-volatile IoT data by deﬁning a new type of meta attribute, which represents the continuous nature of IoT data. On top of traditional discrete object-oriented modeling APIs, we enable models to represent very large sequences of sensor values by using mathematical polynomials. We show on various IoT datasets that this signiﬁcantly improves storage and reasoning efﬁciency.",
        "keywords": [
            "IoT",
            "Continuous modeling",
            "Discrete modeling",
            "Polynomial",
            "Extrapolation",
            "Big Data"
        ],
        "authors": [
            "Assaad Moawad",
            "Thomas Hartmann",
            "Francois Fouquet",
            "Gregory Nain",
            "Jacques Klein",
            "and Yves Le Traon"
        ],
        "file_path": "data/models/models15/Beyond discrete modeling A continuous and efficient model for IoT.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Megamodel for Software Process Line Modeling and Evolution",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Companies formalize software processes as a way of organizing development projects. Since there are differences in project contexts, a one-size-fits-all approach does not work well in practice. Some companies use a family of a predeﬁned processes, but this approach has a high process maintenance cost. Instead, we deﬁne Software Process Lines (SPrL), where a general process with variability is tailored to project contexts. Model-Driven Engineering (MDE) provides a formal framework for deﬁning the models and transformations required for automated SPrL tailoring. However, this approach requires the deﬁnition and co-evolution of various types of models and tool support beyond the skills of process engineers, making the industrial adoption challenging. This paper shares our experience using a megamodeling approach to the development of the back-end of our toolset. The megamodel provides a uniform mechanism for process deﬁnition, variability, tailoring and evolution, and we hide the MDE complexity through a user-friendly front-end. We report the application of our approach at Mobius, a small Chilean software enterprise.",
        "keywords": [
            "Megamodel",
            "Software Process Line",
            "Variability"
        ],
        "authors": [
            "Jocelyn Simmonds",
            "Daniel Perovich",
            "Mar´ıa Cecilia Bastarrica and Luis Silvestre"
        ],
        "file_path": "data/models/models15/A megamodel for Software Process Line modeling and evolution.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Feature Modeling of Two Large-Scale Industrial Software Systems: Experiences and Lessons Learned",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Feature models are frequently used to capture the knowledge about conﬁgurable software systems and product lines. However, feature modeling of large-scale systems is challenging as many models are needed for diverse purposes. For instance, feature models can be used to reﬂect the perspectives of product management, technical solution architecture, or product conﬁg-uration. Furthermore, models are required at different levels of granularity. Although numerous approaches and tools are available, it remains hard to deﬁne the purpose, scope, and granularity of feature models. In this paper we thus present experiences of developing feature models for two large-scale industrial automation software systems. Speciﬁcally, we extended an existing feature modeling tool to support models for different purposes and at multiple levels. We report results on the characteristics and modularity of the feature models, including metrics about model dependencies. We further discuss lessons learned during the modeling process.",
        "keywords": [
            "feature modeling",
            "industrial software systems",
            "experience report"
        ],
        "authors": [
            "Daniela Lettner",
            "Klaus Eder",
            "Paul Grünbacher",
            "Herbert Prähofer"
        ],
        "file_path": "data/models/models15/Feature modeling of two large-scale industrial software systems Experiences and lessons learned.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Systematic Generation of Standard Compliant Tool Support of Diagrammatic Modeling Languages",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "In the Model-Driven Engineering community, the abstract syntax of modeling languages is usually defined and implemented using metamodeling techniques. However, it is not the case for the concrete syntax of graphical modeling languages. Indeed, this concern is mostly specified by informal means. This practice leaves considerable leeway in the implementation and raises several standards compliance issues. Hence, toolsmiths can only rely on their interpretation of the standard and lack of systematic way to build conforming tool support. In this context, a first normative specification of the concrete syntax of UML 2.5 has been recently released using Diagram Definition. In this paper, we propose an approach that uses those formal specifications to systematically generate modeling language tool support that guarantees compliance to standard notation. We assess the approach on a subset of the UML class diagram implemented within the open-source Papyrus tool.",
        "keywords": [
            "Diagrammatic languages",
            "standard-compliance",
            "tooling support",
            "MDE",
            "UML"
        ],
        "authors": [
            "Alexis Fouché",
            "Florian Noyrit",
            "Sébastien Gérard",
            "Maged Elaasar"
        ],
        "file_path": "data/models/models15/Systematic generation of standard compliant tool support of diagrammatic modeling languages.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Pattern-Based Development of Domain-Speciﬁc Modelling Languages",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Model-Driven Engineering (MDE) promotes the use of models to conduct all phases of software development in an automated way. Models are frequently deﬁned using Domain-Speciﬁc Modelling Languages (DSMLs), which many times need to be developed for the domain at hand. However, while constructing DSMLs is a recurring activity in MDE, there is scarce support for gathering, reusing and enacting knowledge for their design and implementation. This forces the development of every new DSML to start from scratch.\n\nTo alleviate this problem, we propose the construction of DSMLs and their modelling environments aided by patterns which gather knowledge of speciﬁc domains, design alternatives, concrete syntax, dynamic semantics and functionality for the modelling environment. They may have associated services, realized via components. Our approach is supported by a tool that enables the construction of DSMLs through the application of patterns, and synthesizes a graphical modelling environment according to them.",
        "keywords": [
            "Domain-Speciﬁc Modelling Languages",
            "Meta-Modelling",
            "Meta-Modelling Patterns",
            "Modelling Environments"
        ],
        "authors": [
            "Ana Pescador",
            "Antonio Garmendia",
            "Esther Guerra",
            "Jes´us S´anchez Cuadrado",
            "Juan de Lara"
        ],
        "file_path": "data/models/models15/Pattern-based development of Domain-Specific Modelling Languages.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Controlled Experiment with Usability Inspection Techniques Applied to Use Case Specifications: Comparing the MIT 1 and the UCE Techniques",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "A Use Case Model is composed of use cases that describe software functionalities through Use Case Specifications. The evaluation of the specifications that compose such a model can allow for an early identification of usability defects. We previously proposed MIT 1—Model Inspection Technique for Usability Evaluation that aims to support the identification of usability defects through the evaluation of use cases specifications. In this paper, we present the evaluation of this technique through a controlled experiment that measured its efficiency, effectiveness, perceived ease of use, and perceived usefulness when compared to the Use Case Evaluation (UCE) method. Our quantitative findings indicate that MIT 1 allows users to find more usability defects in less time than UCE. However, UCE was considered easiest to use and more useful than MIT 1, highlighting improvement needs for MIT 1.",
        "keywords": [
            "controlled experiment",
            "use case model",
            "use case specification",
            "early usability",
            "inspection",
            "empirical study"
        ],
        "authors": [
            "Natasha M. Costa Valentim",
            "Jacilane Rabelo",
            "Ana Carolina Oran",
            "Tayana Conte",
            "Sabrina Marczak"
        ],
        "file_path": "data/models/models15/A controlled experiment with Usability Inspection Techniques applied to Use Case Specifications comparing the MIT 1 and the UCE techniques.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Proceedings",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Timothy Lethbridge",
            "Jordi Cabot",
            "and Alexander Egyed"
        ],
        "file_path": "data/models/models15/Front cover.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Checking Concurrent Behavior in UML/OCL Models",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The Uniﬁed Modeling Language (UML) is a de-facto standard for software development and, together with the Object Constraint Language (OCL), allows for a precise description of a system prior to its implementation. At the same time, these descriptions can be employed to check the consistency and, hence, the correctness of a given UML/OCL model. In the recent past, numerous (automated) approaches have been proposed for this purpose. The behavior of the systems has usually been considered by means of sequence diagrams, state machines, and activity diagrams. But with the increasing popularity of design by contract, also composite structures, classes, and operations are frequently used to describe behavior in UML/OCL. However, for these description means no solution for the validation and veriﬁcation of concurrent behavior is available yet. In this work, we propose such a solution. To this end, we discuss the possible interpretations of “concurrency” which are admissible according to the common UML/OCL interpretation and, afterwards, propose a methodology which exploits solvers for SAT Modulo Theories (i. e., SMT solvers) in order to check the concurrent behavior of UML/OCL models. How to address the resulting problems is described and illustrated by means of a running example. Finally, the application of the proposed method is demonstrated.",
        "keywords": [],
        "authors": [
            "Nils Przigoda",
            "Christoph Hilken",
            "Robert Wille",
            "Jan Peleska",
            "Rolf Drechsler"
        ],
        "file_path": "data/models/models15/Checking concurrent behavior in UML-OCL models.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating Goal-Oriented and Use Case-Based Requirements Engineering: The Missing Link",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Combining goal-oriented and use case modeling has been shown as an effective method of requirements engineering. To ensure the quality of such modeled artifacts, a conceptual foundation is needed to govern the process of determining what types of artifacts to be modeled, and how they should be specified and analyzed for 3Cs problems (completeness, consistency and correctness). However, such a foundation is missing in current goal-use case integration approaches. In this paper, we present GUIMeta, a meta-model, to address this problem. GUIMeta consists of three layers. The artifact layer defines the semantics and classification of artifacts and their relationships. The specification layer offers specification rules for each artifact class. The ontology layer allows semantics to be integrated into the entire model. Our promising evaluation shows the suitability of GUIMeta in modeling goals and use cases.",
        "keywords": [
            "Goal and Use Case",
            "Meta-model",
            "Functional Grammar",
            "Ontology"
        ],
        "authors": [
            "Tuong Huan Nguyen",
            "John Grundy",
            "and Mohamed Almorsy"
        ],
        "file_path": "data/models/models15/Integrating goal-oriented and use case-based requirements engineering The missing link.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Timothy Lethbridge",
            "Jordi Cabot",
            "Alexander Egyed",
            "Yvan Labiche",
            "Gunter Mussbacher",
            "Ana Moreira",
            "Abdelwahab Hamou-Lhadj",
            "Emilio Insfran",
            "Vinay Kulkarni",
            "Omar Badreddin",
            "Benoit Baudry",
            "Benoit Combemale",
            "Tony Clark",
            "Arnon Sturm",
            "Marsha Chechik",
            "Dimitris Kolovos",
            "Martin Gogolla",
            "Mira Balaban",
            "Stéphane Somé",
            "Sahar Kokaly",
            "Michalis Famelis",
            "Manuel Wimmer",
            "Tian Zhang"
        ],
        "file_path": "data/models/models15/MODELS 2015 organization.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A Behavioral Coordination Operator Language (BCOoL)",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "The design of complex systems involves various, possibly heterogeneous, structural and behavioral models. In model-driven engineering, the coordination of behavioral models to produce a single integrated model is necessary to provide support for validation and veriﬁcation. Indeed, it allows system designers to understand and validate the global and emerging behavior of the system. However, the manual coordination of models is tedious and error-prone, and current approaches to automate the coordination are bound to a ﬁxed set of coordination patterns. In this paper, we propose a Behavioral Coordination Operator Language (B-COOL) to reify coordination patterns between speciﬁc domains by using coordination operators between the Domain-Speciﬁc Modeling Languages used in these domains. Those operators are then used to automate the coordination of models conforming to these languages. We illustrate the use of B-COOL with the deﬁnition of coordination operators between timed ﬁnite state machines and activity diagrams.",
        "keywords": [
            "Heterogeneous Modeling",
            "Coordination Languages",
            "DSMLs"
        ],
        "authors": [
            "Matias Ezequiel Vara Larsen",
            "Julien DeAntoni",
            "Benoit Combemale",
            "and Fr´ed´eric Mallet"
        ],
        "file_path": "data/models/models15/A Behavioral Coordination Operator Language -BCOoL-.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "B-COOL"
        }
    },
    {
        "title": "Models 2015 – the 18th instance of the International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "This volume contains the proceedings of Models 2015 – the 18th instance of the International Conference on Model Driven Engineering Languages and Systems. This year we received 216 abstract submissions that materialized in 172 papers, consisting of 132 technical papers (including 22 new ideas papers) and 40 in-practice papers. Of these, the Program Committee and Program Board accepted 35 foundations papers (26.5% acceptance rate) and 11 in-practice papers (28%).",
        "keywords": [],
        "authors": [
            "Tim Lethbridge",
            "Jordi Cabot and Alexander Egyed"
        ],
        "file_path": "data/models/models15/Message from the chairs.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modular Model-Based Supervisory Controller Design for Wafer Logistics in Lithography Machines",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Development of high-level supervisory controllers is an important challenge in the design of high-tech systems. It has become a significant issue due to increased complexity, combined with demands for verified quality, time to market, ease of development, and integration of new functionality. To deal with these challenges, model-based engineering approaches are suggested as a cost-effective way to support easy adaptation, validation, synthesis, and verification of controllers. This paper presents an industrial case study on modular design of a supervisory controller for wafer logistics in lithography machines. The uncontrolled system and control requirements are modeled independently in a modular way, using small, loosely coupled and minimally restrictive extended finite automata. The multiparty synchronization mechanism that is part of the specification formalism provides clear advantages in terms of modularity, traceability, and adaptability of the model. We show that being able to refer to variables and states of automata in guard expressions and state-based requirements, enabled by the use of extended finite automata, provides concise models. Additionally, we show how modular synthesis allows construction of local supervisors that ensure safety of parts of the system, since monolithic synthesis is not feasible for our industrial case.",
        "keywords": [],
        "authors": [
            "Bram van der Sanden",
            "Michel Reniers",
            "Marc Geilen",
            "Twan Basten",
            "Johan Jacobs",
            "Jeroen Voeten",
            "Ramon Schiffelers"
        ],
        "file_path": "data/models/models15/Modular model-based supervisory controller design for wafer logistics in lithography machines.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Unifying Approach to Connections for Multi-Level Modeling",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "Capturing relationships between concepts in a domain is as important as capturing the concepts themselves. Modeling languages reﬂect this by providing connections with rich semantics, such as associations and links, thus providing a key advantage over approaches that support relationships with simple references only. While connections for two-level modeling (e.g. in the UML) have enjoyed a stable design for a considerable time, the same cannot be said for connections in multi-level modeling languages. As interest in multi-level modeling grows, it is important to provide a comprehensive design for connections that not only adheres to multi-level principles such as level-agnosticism and explicit level organization, but also supports deep characterization, i.e., the ability to specify level content beyond one level boundary. In this paper we propose a unifying conceptual model for connections whose expressiveness and scalability does not come at the cost of concept proliferation.",
        "keywords": [],
        "authors": [
            "Colin Atkinson",
            "Ralph Gerbig",
            "Thomas K¨uhne"
        ],
        "file_path": "data/models/models15/A unifying approach to connections for multi-level modeling.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Employing Classifying Terms for Testing Model Transformations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "This contribution proposes a new technique for developing test cases for UML and OCL models. The technique is based on an approach that automatically constructs object models for class models enriched by OCL constraints. By guiding the construction process through so-called classifying terms, the built test cases in form of object models are classified into equivalence classes. A classifying term can be an arbitrary OCL term on the class model that calculates for an object model a characteristic value. From each equivalence class of object models with identical characteristic values one representative is chosen. The constructed test cases behave significantly different with regard to the selected classifying term. By building few diverse object models, properties of the UML and OCL model can be explored effectively. The technique is applied for automatically constructing relevant source model test cases for model transformations between a source and target metamodel.",
        "keywords": [],
        "authors": [
            "Martin Gogolla",
            "Antonio Vallecillo",
            "Loli Burgueño",
            "Frank Hilken"
        ],
        "file_path": "data/models/models15/Employing classifying terms for testing model transformations.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Identifying Duplicate Functionality in Textual Use Cases by Aligning Semantic Actions",
        "submission-date": "2014/08",
        "publication-date": "2015/00",
        "abstract": "Developing high-quality requirements specifications often demands a thoughtful analysis and an adequate level of expertise from analysts. Although requirements modeling techniques provide mechanisms for abstraction and clarity, fostering the reuse of shared functionality (e.g., via UML relationships for use cases), they are seldom employed in practice. A particular quality problem of textual requirements, such as use cases, is that of having duplicate pieces of functionality scattered across the specifications. Duplicate functionality can sometimes improve readability for end users, but hinders development-related tasks such as effort estimation, feature prioritization and maintenance, among others. Unfortunately, inspecting textual requirements by hand in order to deal with redundant functionality can be an arduous, time-consuming and error-prone activity for analysts. In this context, we introduce a novel approach called ReqAligner that aids analysts to spot signs of duplication in use cases in an automated fashion. To do so, ReqAligner combines several text processing techniques, such as a use-case-aware classifier and a customized algorithm for sequence alignment. Essentially, the classifier converts the use cases into an abstract representation that consists of sequences of semantic actions, and then these sequences are compared pairwise in order to identify action matches, which become possible duplications. We have applied our technique to five real-world specifications, achieving promising results and identifying many sources of duplication in the use cases.",
        "keywords": [],
        "authors": [
            "Alejandro Rago",
            "Claudia Marcos",
            "J. Andrés Diaz-Pace"
        ],
        "file_path": "data/models/models15/Identifying duplicate functionality in textual use cases by aligning semantic actions -SoSyM abstract-.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Regulatory Compliance: A Case Study of “Know Your Customer” Regulations",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Modern enterprises face an unprecedented regulatory regime. Industry governance, risk, and compliance (GRC) solutions are document-oriented and expert-driven. Formal compliance checking techniques in contrast attempt to provide ways for rigorous modeling and analysis of regulatory compliance but miss out on holistic GRC perspective due to missing integration between diverse set of (semi-) formal models. We show that streamlining regulatory compliance using multiple purposive models of various aspects of regulations, it is possible to leverage both the rigor of formal techniques and the holistic enterprise GRC perspective. Our contributions are twofold. First, we present a model-driven architecture based on a conceptual model of integrated GRC that is capable of addressing key challenges of regulatory compliance. Second, using Know Your Customer regulations in Indian context as a case study, we demonstrate the utility of this architecture. Initial results with KYC regulations are promising and point to further work in model-driven regulatory compliance.",
        "keywords": [],
        "authors": [
            "Sagar Sunkle",
            "Deepali Kholkar",
            "and Vinay Kulkarni"
        ],
        "file_path": "data/models/models15/Model-driven regulatory compliance A case study of -Know Your Customer- regulations.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving Reuse by means of Asymmetrical Model Migrations: An Application to the Orcc Case Study",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The legacy code of a tool handling domain speciﬁc data gathers valuable expertise. However in many cases, it must be rewritten to make it apply to structurally incompatible data. We investigate a co-evolution approach to avoid this update by making the call context meet the a legacy tool deﬁnition domain. The data conforming to the call context co-evolve into data conforming to the deﬁnition domain. Once processed by the tool, they can be put back into their original context thanks to a speciﬁc reverse transformation which enables the recovery of elements that had been initially removed. This approach is applied to Orcc, a compiler for dataﬂow applications. Orcc requires many common functions that are expected to be adapted to its own context. Our approach is an effective way to reuse them instead of rewriting them.",
        "keywords": [
            "TMM",
            "Refactoring",
            "Migration",
            "Application domain",
            "Legacy tool domain",
            "Co-evolution",
            "Reverse Migration"
        ],
        "authors": [
            "Paola Vallejo",
            "Mickaël Kerboeuf",
            "Kevin J. M. Martin",
            "Jean-Philippe Babau"
        ],
        "file_path": "data/models/models15/Improving reuse by means of asymmetrical model migrations An application to the Orcc case study.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhanced Graph Rewriting Systems for Complex Software Domains",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "Methodologies for correct by construction reconﬁgurations can efﬁciently solve consistency issues in dynamic software architecture. Graph-based models are appropriate for designing such architectures and methods. At the same time, they may be unﬁt to characterize a system from a non functional perspective. This stems from efﬁciency and applicability limitations in handling time-varying characteristics and their related dependencies. In order to lift these restrictions, an extension to graph rewriting systems is proposed herein. The suitability of this approach, as well as the restraints of currently available ones, are illustrated, analysed and experimentally evaluated with reference to a concrete example. This investigation demonstrates that the conceived solution can: (i) express any kind of algebraic dependencies between evolving requirements and properties; (ii) signiﬁcantly ameliorate the efﬁciency and scalability of system modiﬁcations with respect to classic methodologies; (iii) provide an efﬁcient access to attribute values; (iv) be fruitfully exploited in software management systems; (v) guarantee theoretical properties of a grammar, like its termination.",
        "keywords": [],
        "authors": [
            "Cédric Eichler",
            "Thierry Monteil",
            "Patricia Stolf",
            "Alfredo Grieco",
            "Khalil Drira"
        ],
        "file_path": "data/models/models15/Enhanced graph rewriting systems for complex software domains -SoSyM abstract-.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalizing and Verifying Stochastic System Architectures Using Monterey Phoenix",
        "submission-date": "2014/04",
        "publication-date": "2015/00",
        "abstract": "The analysis of software architecture plays an important role in understanding the system structures and facilitate proper implementation of user requirements. Despite its importance in the software engineering practice, the lack of formal description and veriﬁcation support in this domain hinders the development of quality architectural models. To tackle this problem, in this work, we develop an approach for modeling and verifying software architectures speciﬁed using Monterey Phoenix (MP) architecture description language. MP is capable of modeling system and environment behaviors based on event traces, as well as supporting different architecture composition operations and views. First, we formalize the syntax and operational semantics for MP; therefore, formal veriﬁcation of MP models is feasible. Second, we extend MP to support shared variables and stochastic characteristics, which not only increases the expressiveness of MP, but also widens the properties MP can check, such as quantitative requirements. Third, a dedicated model checker for MP has been implemented, so that automatic veriﬁcation of MP models is supported. Finally, several experiments are conducted to evaluate the applicability and efﬁciency of our approach.",
        "keywords": [],
        "authors": [
            "Songzheng Song",
            "Yang Liu",
            "Mikhail Auguston",
            "Jun Sun",
            "Jin Song Dong",
            "Tieming Chen"
        ],
        "file_path": "data/models/models15/Formalizing and verifying stochastic system architectures using Monterey Phoenix -SoSyM abstract-.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Software Supply Chains (Keynote)",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "It has long been desired to build software systems predominantly through the composition of existing software components. The need for such a production model is growing given the increasing use and reliance on software for almost everything we interact with from toasters to airplanes. For some kinds of systems, we have come a long way towards meeting the production via composition through the use of libraries, frameworks and plugin architectures. But, for other systems that require tight integrations of components produced by different suppliers, we are not yet able to reliably engineer a software supply chain. In this talk, I will outline some achievements in software supply chains and describe some of the challenges that need to be met to productively provide the systems of the future.",
        "keywords": [],
        "authors": [
            "Gail C. Murphy"
        ],
        "file_path": "data/models/models15/Software supply chains -keynote-.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Infrastructure as Runtime Models: Towards Model-Driven Resource Management",
        "submission-date": "2015/00",
        "publication-date": "2015/00",
        "abstract": "The importance of continuous delivery and the emergence of tools allowing to treat infrastructure configurations programmatically have revolutionized the way computing resources and software systems are managed. However, these tools keep lacking an explicit model representation of underlying resources making it difficult to introspect, verify or reconfigure the system in response to external events.\n\nIn this paper, we outline a novel approach that treats system infrastructure as explicit runtime models. A key benefit of using such models@run.time representation is that it provides a uniform semantic foundation for resources monitoring and reconfiguration. Adopting models at runtime allows one to integrate different aspects of system management, such as resource monitoring and subsequent verification into an unified view which would otherwise have to be done manually and require to use different tools. It also simplifies the development of various self-adaptation strategies without requiring the engineers and researchers to cope with low-level system complexities.",
        "keywords": [],
        "authors": [
            "Filip Kˇrikava",
            "Romain Rouvoy",
            "Lionel Seinturier"
        ],
        "file_path": "data/models/models15/Infrastructure as runtime models Towards Model-Driven resource management.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Facilitating Modeling and Simulation of Complex Systems Through Interoperable Software",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Incorporating models into the design, test, and monitoring processes of complex systems can reduce development costs and improve performance and reliability. However, developing appropriate models and calibrating these models with measurement data is often time consuming. The problems are magnified in cases of distributed systems and cyber-physical system. Additionally, real-time test and hardware-in-the-loop applications may require the partitioning of model components on heterogeneous targets with a combination of multi-core processors and field programmable gate arrays. There are various commercial and open-source software options available for developing models but multiple modeling tools may be required for a single application. \nThis talk will review current efforts to overcome these challenges in modeling and simulation. Several successful applications will be discussed as well. Research in this area is continuing and collaboration is a must.",
        "keywords": [],
        "authors": [
            "Dr. Jeannie Falcon"
        ],
        "file_path": "data/models/models17/Keynotes.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Systematic Mapping Study on Modeling for Industry 4.0",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Industry 4.0 is a vision of manufacturing in which smart, interconnected production systems optimize the complete value-added chain to reduce cost and time-to-market. At the core of Industry 4.0 is the smart factory of the future, whose successful deployment requires solving challenges from many domains. Model-based systems engineering (MBSE) is a key enabler for such complex systems of systems as can be seen by the increased number of related publications in key conferences and journals. This paper aims to characterize the state of the art of MBSE for the smart factory through a systematic mapping study on this topic. Adopting a detailed search strategy, 1466 papers were initially identiﬁed. Of these, 222 papers were selected and categorized using a particular classiﬁcation scheme. Hence, we present the concerns addressed by the modeling community for Industry 4.0, how these are investigated, where these are published, and by whom. The resulting research landscape can help to understand, guide, and compare research in this ﬁeld. In particular, this paper identiﬁes the Industry 4.0 challenges addressed by the modeling community, but also the challenges that seem to be less investigated.",
        "keywords": [],
        "authors": [
            "Andreas Wortmann",
            "Benoit Combemale",
            "Olivier Barais"
        ],
        "file_path": "data/models/models17/A Systematic Mapping Study on Modeling for Industry 4.0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bidirectional Transformations in the Large",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "The model-driven development of systems involves multiple models, metamodels and transformations, and relation-ships between them. A bidirectional transformation (bx) is usually deﬁned as a means of maintaining consistency between “two (or more)” models. This includes cases where one model may be generated from one or more others, as well as more complex (“symmetric”) cases where models record partially overlapping information. In recent years binary bx, those relating two models, have been extensively studied. Multiary1 bx, those relating more than two models, have received less attention. In this paper we consider how a multiary consistency relation may be deﬁned in terms of binary consistency relations, and how consistency restoration may be carried out on a network of models and relationships between them. We relate this to megamodelling and discuss further research that is needed.",
        "keywords": [],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/models/models17/Bidirectional Transformations in the Large.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Copyright notice.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-Driven Approach to Trace Checking of Pattern-based Temporal Properties",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Trace checking is a procedure for evaluating requirements over a log of events produced by a system. This paper deals with the problem of performing trace checking of temporal properties expressed in TemPsy, a pattern-based speciﬁcation language. The goal of the paper is to present a scalable and practical solution for trace checking, which can be used in contexts where relying on model-driven engineering standards and tools for property checking is a fundamental prerequisite. The main contributions of the paper are: a model-driven trace checking procedure, which relies on the efﬁcient mapping of temporal requirements written in TemPsy into OCL constraints on a meta-model of execution traces; the implementation of this trace checking procedure in the TEMPSY-CHECK tool; the evaluation of the scalability of TEMPSY-CHECK, applied to the veriﬁcation of real properties derived from a case study of our industrial partner, including a comparison with a state-of-the-art alternative technology based on temporal logic. The results of the evaluation show the feasibility of applying our model-driven approach for trace checking in realistic settings: TEMPSY-CHECK scales linearly with respect to the length of the input trace and can analyze traces with one million events in about two seconds.",
        "keywords": [],
        "authors": [
            "Wei Dou",
            "Domenico Bianculli",
            "Lionel Briand"
        ],
        "file_path": "data/models/models17/A Model-Driven Approach to Trace Checking of Pattern-Based Temporal Properties.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "2017 ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Heuristic-Based Recommendation for Metamodel - OCL Coevolution.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Component and Connector Views in Practice: An Experience Report",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Component and Connector (C&C) view speciﬁcations, with corresponding veriﬁcation and synthesis techniques, have been recently suggested as a means for formal yet intuitive structural speciﬁcation of C&C models. In this paper we report on our recent experience in applying C&C views in industrial practice, where we aimed to answer questions such as: could C&C views be practically used in industry, what are challenges of systems engineers that the use of C&C views could address, and what are some of the technical obstacles in bringing C&C views to the hands of systems engineers. We describe our experience in detail and discuss a list of lessons we have learned, including, e.g., a missing abstraction concept in C&C models and C&C views that we have identiﬁed and added to the views language and tool, that engineers can create graphical C&C views quite easily, and how veriﬁcation algorithms scale on real-size industry models. Furthermore, we report on the non-negligible technical effort needed to translate Simulink block diagrams to C&C models. We make all materials mentioned and used in our experience electronically available for inspection and further research.",
        "keywords": [
            "component and connector models",
            "Simulink",
            "architecture",
            "industrial case study"
        ],
        "authors": [
            "Vincent Bertram",
            "Shahar Maoz",
            "Jan Oliver Ringert",
            "Bernhard Rumpe",
            "Michael von Wenckstern"
        ],
        "file_path": "data/models/models17/Component and Connector Views in Practice An Experience Report.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Publisher-s information.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reusable Speciﬁcation Templates for Deﬁning Dynamic Semantics of DSLs",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Domain-Speciﬁc Languages (DSLs) are a central concept of Model Driven Engineering (MDE). They are considered to be very effective in software development and are being widely adopted by industry nowadays. A DSL is a programming language specialized to a speciﬁc application domain. This paper proposes a new method for deﬁning the dynamic semantics of DSLs using reusable speciﬁcation templates, bridging the gap between DSL concepts and execution platforms.",
        "keywords": [],
        "authors": [
            "Ulyana Tikhonova"
        ],
        "file_path": "data/models/models17/Reusable Specification Templates for Defining Dynamic Semantics of DSLs.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tool Support for Live Formal Veriﬁcation",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Despite an increasing interest from industry (e.g.,\nDO333 standard [1]), formal veriﬁcation is still not widely used\nin production for safety critical systems. This has been recognized\nfor a while and various causes have been identiﬁed, one of them\nbeing the lack for scalable and cost effective tools. Many such\ntools exist for formal veriﬁcation, but few of them are user-\nfriendly: using formal veriﬁcation generally still requires such\nan effort that the time spent on the tool prevents the integration\nof the method in an industrial setting. This paper presents a\ntool prototype aiming at supporting non-experts in using formal\nveriﬁcation. The tooling approach is meant to be cost effective\nand change-supportive: user-friendliness is designed not only for\nthe non-expert, but also to require minimum effort so that formal\nveriﬁcation is triggered even for the non-enthusiast who is not\nwilling to push a button. To do so, we trigger, in a background\ntask, pre-deﬁned formal veriﬁcation checks at (almost) every\nchange of the model. We only display error messages in case\nof problem: the user is not disturbed if no problem is detected.\nTo prevent checks to be triggered all the time, we decide to\nconsider only local analyses (i.e., only checks which do not\nrequire knowledge of elements in a remote position in the model).\nThis restricts the sort of formal veriﬁcation that we support,\nbut this is a conscious choice: our motto is ”Let us ﬁrst make\nbasic techniques very user-friendly; more powerful ones will be\nconsidered only when at least the basic techniques have proven\nto be accepted.”",
        "keywords": [
            "tool; cost-effective; formal veriﬁcation; user friendliness; scalable"
        ],
        "authors": [
            "Vincent Aravantinos",
            "Sudeep Kanav"
        ],
        "file_path": "data/models/models17/Tool Support for Live Formal Verification.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Partial Evaluation of OCL Expressions",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "In the academic literature, many uses of the Object Constraint Language (OCL) have been proposed. By contrast, the utilization of OCL in contemporary modelling tools lags behind, suggesting that leverage of OCL remains limited in practice. We consider this undeserved, and present a scheme for partially evaluating OCL expressions that allows one to capitalize on given OCL speciﬁcations for a wide array of purposes using a single implementation: a partial evaluator of OCL.",
        "keywords": [],
        "authors": [
            "Bastian Ulke",
            "Friedrich Steimann",
            "Ralf L¨ammel"
        ],
        "file_path": "data/models/models17/Partial Evaluation of OCL Expressions.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SQL-PL4OCL : An automatic code generator from OCL to SQL Procedural Language",
        "submission-date": "2017/05",
        "publication-date": "2017/05",
        "abstract": "Design models are widely spread as core artifacts in software engineering. Yet, a key problem is how to fulﬁll correctly these blueprint speciﬁcations when code components are developed. The best possible scenario occurs when a source modeling language can be perfectly linked to a target language of election. Namely, a well deﬁned mapping bridges the gap between the source and the target language. Otherwise, manual encoding of the system design is cumbersome and error prone. In this setting, we introduce a SQL-PL1 code generator for OCL expressions that, in contrast to other proposals, is able to map OCL iterate and iterator expressions thanks to our use of stored procedures. More in detail, our source language is the Object Constraint Language (OCL), which nowadays is an ISO standard used to express constraints and queries in a textual notation on UML models. Our target language is the procedural language (PL) extension to the Structured Query Language (SQL). SQL is a special-purpose programming language designed for managing data in relational database management systems (RDBMS). The purpose of PL for SQL is to combine database language and procedural programming language. Although SQL is also an ISO standard, different RDBMS implement certain syntactic variations to the standard SQL notation. Thus, we had to adapt the implementation of our mapping to each of them. As implementation targets we selected MariaDB, PostgreSQL, and MS SQL Server. MariaDB and PostgreSQL were selected because they are open source and widely used by developers. MS SQL server was selected to be able to compare evaluation time from open source to commercial RDBMS. A variety of applications arises for a mapping from OCL to SQL expressions. Among others, there are three prominent types. These are i) evaluation of OCL expressions (analysis queries and metrics) on large model’s instances, ii) identiﬁ-cation of constraints during data modeling that have to be checked as integrity constraints on actual data; iii) automatic code generation from models. Indeed, our implementation was used as a key component of a toolkit that automatically generated ready-to-deploy web applications for secure data management from design models. Our component mapped and evaluated OCL constraints speciﬁed within authorization policies. Our code generator is deﬁned recursively over the structure of OCL expressions and it is implemented in the SQL-PL4OCL tool that is publicly available at [1]. The seminal work of the mapping presented here can be found in [2], [3]. The key idea that enables the mapping from OCL iterator expressions to iterative stored procedures remains the same, but the work detailed in [4] introduces a novel mapping from OCL expressions to SQL-PL stored procedures. In the novel mapping we have taken design decisions which have facilitated the recursive deﬁnition of the code generator and simpliﬁed its deﬁnition. These decisions have also helped to signiﬁcantly decrease the time required for the evaluation of the code generated. Regarding semantics, the new mapping is able to deal properly with the three-valued evaluation semantics of OCL. In addition, our original work and implementation was intended only for the procedural extension of MySQL, while our new deﬁnition eased the implementation of the mapping into other relational database management systems. In turn, we can now evaluate the resulting code using different RDBMS, which permits us to widen our discussion regarding efﬁciency in terms of evaluation-time of the code produced by SQL-PL4OCL tool.",
        "keywords": [],
        "authors": [
            "Marina Egea",
            "Carolina Dania"
        ],
        "file_path": "data/models/models17/SQL-PL4OCL An Automatic Code Generator from OCL to SQL Procedural Language.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How is ATL Really Used? Language Feature Use in the ATL Zoo",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Studies of code repositories have long been used to understand the use of programming languages and to provide insight into how they should evolve. Such studies can highlight features that are rarely used and can safely be removed to simplify the language. Conversely, combinations of features that are frequently used together can be identified and possibly replaced with new features to improve the user experience. Unfortunately, this kind of research has not been as popular in Model Driven Development (MDD). More specifically, using repositories of model transformations (in any language) to understand how the features of these languages are used has not been investigated much, despite its potential benefits. In this paper, we study the use of the ATL model transformation language in an ATL transformation repository. We identify three research questions aimed at providing insight into how ATL’s features are actually used. Using the TXL source transformation language, we implement a parser-based analyzer to extract information from the ATL Zoo. We use this information to answer these research questions and provide additional observations based on manual inspection of ATL artifacts.",
        "keywords": [
            "Model transformations",
            "MDD",
            "ATL",
            "TXL"
        ],
        "authors": [
            "Gehan M. K. Selim",
            "James R. Cordy",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models17/How is ATL Really Used- Language Feature Use in the ATL Zoo.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "ATL, TXL"
        }
    },
    {
        "title": "Co-evolution of Meta-Modeling Syntax and Informal Semantics in Domain-Specific Modeling Environments - A Case Study of AUTOSAR",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "One domain-specific modeling environment is centered around a domain-specific meta-model which defines syntax (modeling elements, e.g., classes) for the domain models. However, in order for the system designers to be able to construct meaningful models, semantics of the domain-specific meta-model needs to be described as well. This semantics is often provided in a form of informal natural language specifications that contain a set of design requirements, each describing the intended use of one or more modeling elements. Intuitively, introduction of new concepts into the modeling environment is expected to require changes in both meta-modeling syntax and informal semantics in such a way that their co-evolution is highly correlated. In order to test this hypothesis, we analyzed the relation between added classes, attributes, and connectors, as meta-modeling syntax, and modified/added design requirements, as meta-modeling semantics, in a case study of the AUTOSAR meta-modeling environment. We found that new AUTOSAR concepts usually require both new modeling elements and new design requirements, but surprisingly adding more elements is not always followed by more requirements. This finding is also validated by the moderately strong correlation between the evolution of these two AUTOSAR meta-modeling artifacts (Spearman’s ρ 0,63 and Kendall’s τ 0,49). For system designers, this means that both meta-modeling syntax and informal semantics is important to be considered in the analysis of domain-specific meta-model evolution, but it may not be enough for understanding the use of all modeling elements. For designers responsible for the maintenance of domain-specific meta-models, this means that more effort shall be put into describing the semantics of all introduced modeling elements.",
        "keywords": [],
        "authors": [
            "Darko Durisic",
            "Corrado Motta",
            "Miroslaw Staron",
            "Matthias Tichy"
        ],
        "file_path": "data/models/models17/Co-Evolution of Meta-Modeling Syntax and Informal Semantics in Domain-Specific Modeling Environments - A Case Study of AUTOSAR.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Revisiting Visitors for Modular Extension of Executable DSMLs",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Executable Domain-Speciﬁc Modeling Languages (xDSMLs) are typically deﬁned by metamodels that specify their abstract syntax, and model interpreters or compilers that deﬁne their execution semantics. To face the proliferation of xDSMLs in many domains, it is important to provide language engineering facilities for opportunistic reuse, extension, and customization of existing xDSMLs to ease the deﬁnition of new ones. Current approaches to language reuse either require to anticipate reuse, make use of advanced features that are not widely available in programming languages, or are not directly applicable to metamodel-based xDSMLs. In this paper, we propose a new language implementation pattern, named REVISITOR, that enables independent extensibility of the syntax and semantics of metamodel-based xDSMLs with incremental compilation and without anticipation. We seamlessly implement our approach alongside the compilation chain of the Eclipse Modeling Framework, thereby demonstrating that it is directly and broadly applicable in various modeling environments. We show how it can be employed to incrementally extend both the syntax and semantics of the fUML language without requiring anticipation or re-compilation of existing code, and with acceptable performance penalty compared to classical handmade visitors.",
        "keywords": [],
        "authors": [
            "Manuel Leduc",
            "Thomas Degueule",
            "Benoit Combemale",
            "Tijs van der Storm",
            "Olivier Barais"
        ],
        "file_path": "data/models/models17/Revisiting Visitors for Modular Extension of Executable DSMLs.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Language Design with Intent",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Software languages have always been an essential component of model-driven engineering. Their importance and popularity has been on the rise thanks to language workbenches, language-oriented development and other methodologies that enable us to quickly and easily create new languages speciﬁc for each domain. Unfortunately, language design is largely a form of art and has resisted most attempts to turn it into a form of science or engineering. In this paper we borrow concepts, techniques and principles from the domain of persuasive technology, or wider yet, design with intent — which was developed as a way to inﬂuence users behaviour for social and environmental beneﬁt. Similarly, we claim, software language designers can make conscious choices in order to inﬂuence the behaviour of language users. The paper describes a process of extracting design components from 24 books of eight categories (dragon books, parsing techniques, compiler construction, compiler design, language implementa-tion, language documentation, programming languages, software languages), as well as from the original set of Design with Intent cards and papers on DSL design. The resulting language design card toolkit can be used by DSL designers to cover important design decisions and make them with more conﬁdence.",
        "keywords": [],
        "authors": [
            "Vadim Zaytsev"
        ],
        "file_path": "data/models/models17/Language Design with Intent.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "2017 ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems MODELS 2017",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Table of contents.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Abrahão, Silvia",
            "Faugère, Madeleine",
            "Agner, Luciane T. W.",
            "Fohler, Gerhard",
            "Al-Refai, Mohammed",
            "Fouquet, Francois",
            "Antkiewicz, Michal",
            "Ghosh, Sudipto",
            "Aravantinos, Vincent",
            "Giorgini, Paolo",
            "Barais, Olivier",
            "Greenyer, Joel",
            "Barner, Simon",
            "Guerra, Esther",
            "Batot, Edouard",
            "Gutjahr, Timo",
            "Beckmann, Martin",
            "Hartmann, Thomas",
            "Benelallam, Amine",
            "Hidaka, Sochiro",
            "Bergmann, Gábor",
            "Hutchesson, Stuart",
            "Bertram, Vincent",
            "Ipatiov, Alexandru",
            "Bianculli, Domenico",
            "Izquierdo, Javier Luis Cánovas",
            "Bourcier, Johann",
            "Jouault, Frédéric",
            "Bourdeleau, Francis",
            "Jürjens, Jan",
            "Briand, Lionel",
            "Kanav, Sudeep",
            "Burger, Erik",
            "Kessentini, Wael",
            "Cabot, Jordi",
            "Khalil, Maged",
            "Cazzola, Walter",
            "Klare, Heiko",
            "Chechik, Marsha",
            "Kokaly, Sahar",
            "Cheng, Betty",
            "Kolb, Bernd",
            "Clarisó, Robert",
            "Kolovos, Dimitrios S.",
            "Combemale, Benoit",
            "Kramer, Max",
            "Cordy, James R.",
            "Lämmel, Ralf",
            "Cosentino, Valerio",
            "Langhammer, Michael",
            "Cuadrado, Jesús Sánchez",
            "Le Traon, Yves",
            "Czarnecki, Krzysztof",
            "Leduc, Manuel",
            "Dania, Carolina",
            "Lethbridge, Timothy C.",
            "Dániel, Varró",
            "Liang, Jia Hui",
            "de Lara, Juan",
            "Maoz, Shahar",
            "Debreceni, Csaba",
            "Michalke, Vanessa N.",
            "Degueule, Thomas",
            "Migge, Jörn",
            "Denney, Ewen",
            "Moawad, Assaad",
            "Deursen, Arie van",
            "Motta, Corrado",
            "Diewald, Alexander",
            "Mouline, Ludovic",
            "Dingel, Juergen",
            "Murashkin, Alexandr",
            "Dou, Wei",
            "Pai, Ganesh",
            "Dummann, Kolja",
            "Paige, Richard",
            "Durisic, Darko",
            "Paige, Richard F.",
            "Eder, Johannes",
            "Palomares, Javier",
            "Egea, Marina",
            "Pech, Vaclav",
            "Ernadote, Dominique",
            "Pérez, Daniel Gracia",
            "Famelis, Michalis",
            "Pomerantz, Nitzan",
            "Fohler, Gerhard"
        ],
        "file_path": "data/models/models17/Author index.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "The Next Evolution of MDE: A Seamless Integration of Machine Learning into Domain Modeling",
        "submission-date": "2017/03",
        "publication-date": "2017/03",
        "abstract": "Advances in software and sensors have led to a new generation of systems which can help to minimize human intervention in critical infrastructures, like the power grid. However, they have mainly been designed to face predictable situations, in order to react, for example, to a critical over-load. This is called known domain knowledge. However, such systems have also to face events that are unpredictable at design time. For instance, the electric consumption of a house depends on the number of persons living there, their activities, weather conditions, used devices, and so forth. Despite such behaviour is unpredictable at design time, it is identiﬁable and a hypothesis about it can be already formulated and solved later by observing past situations, once data becomes available. Sutcliffe et al., [1] suggest to call this known unknown. Machine learning algorithms are designed to resolve these unknowns, using ﬁne- or coarse-grained learning. Coarse-grained learning means extracting the average behaviour of a large dataset. Conversely, ﬁne-grained learning means specializing learning algorithms only on speciﬁc elements. In cases where datasets are composed of independent and het-erogenous entities, which behave very differently, finding one coarse-grained common behaviour can be difﬁcult or even inappropriate. For example, considering smart grids, the daily consumption of a factory follows a very different pattern than the consumption of an apartment. Thus, coarse-grained learning alone, which is based on the “law of large numbers ”, can be inaccurate for such systems. Additionally, any data changes requires the whole learning process to be recomputed. Instead, following a divide and conquer strategy, learning on ﬁner granularities can be considerably more efﬁcient [2], [3]. In accordance to the pedagogical concept [4], we refer to small ﬁne-grained learning units as “micro learning”. However, applying micro learning on systems, such as the electric grid, can potentially lead to many ﬁne-grained learning units, that need to be combined and synchronised with domain data. Learning frameworks like TensorFlow fo-cus solely on the learning ﬂow without any relation to the domain model. Consequently, domain data and its structure is expressed in different models than learning tasks, using different languages and tools. This leads to a separation of domain data, knowledge, known unknowns, and associated learning methods. Therefore, an appropriate structure to model learning units and their relationships to domain knowledge is required. To tame such complexity, we propose to weave micro machine learning seamlessly into data modeling. Specifically, our approach aims at: (1) Structuring complex learning tasks with reusable, chainable, and independently computable micro learning units. (2) Seamlessly integrating behavioural models which are known at design time, behavioural models that need to be learned at runtime, and domain models using common modeling concepts. (3) Automating the mapping between the mathematical representation expected by a speciﬁc machine learning algorithm and the domain representation [5] and independently updating micro learning units to be fast enough for online learning. As a natural extension of model-driven engineering ap-proaches, we take advantage of relationships between domain data and behavioural elements (learned or known at design time) to implicitly deﬁne a ﬁne-grained mapping of learning units and domain data. We implemented and integrated our approach into the open-source modeling framework GreyCat, which is specifically designed for the requirements of CPSs and IoT. We evaluate our approach on a concrete smart grid case study and show that: (1) Micro machine learning for such scenarios can be more accurate than coarse-grained learning (2) Performance is fast enough to be used for real-time analytics. The full paper has been published in [6].",
        "keywords": [
            "Domain modeling",
            "Live learning",
            "Model-driven engineering",
            "Meta modeling",
            "Cyber-physical systems",
            "Smart grids"
        ],
        "authors": [
            "Thomas Hartmann",
            "Assaad Moawad",
            "Francois Fouquet",
            "and Yves Le Traon"
        ],
        "file_path": "data/models/models17/The Next Evolution of MDE A Seamless Integration of Machine Learning into Domain Modeling.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Translating target to source constraints in model-to-model transformations",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Model transformations are used to automate model manipulation in Model-Driven Engineering (MDE). In particular, model-to-model transformations produce target models (confor-mant to a target meta-model) from source ones (conformant to a source meta-model). While transformation correctness is crucial in MDE, developing transformations is error-prone due to the difficulty in testing them. This problem is further aggravated if the source and target meta-models contain OCL integrity constraints, as every transformed source model should satisfy the target integrity constraints.\n\nIn order to attack this problem, we present a novel method that translates target OCL constraints to the source meta-model using the transformation deﬁnition. This way, if a source model satisﬁes the advanced constraint, the transformed model will satisfy the target constraint. The method has been implemented for the ATL transformation language and integrated with the anATLyzer tool. We show its beneﬁts in combination with model ﬁnders, and the promising results of its validation using mutation techniques and transformations developed by third parties.",
        "keywords": [
            "Model-driven engineering; model transformations; integrity constraints; OCL; quality"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Esther Guerra",
            "Juan de Lara",
            "Robert Clarisó",
            "Jordi Cabot"
        ],
        "file_path": "data/models/models17/Translating Target to Source Constraints in Model-to-Model Transformations.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Removal of Redundant Elements within UML Activity Diagrams",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "As the complexity of systems continues to rise, the use of model-driven development approaches becomes more widely applied. Still, many created models are mainly used for documentation. As such, they are not designed to be used in following stages of development, but merely as a means of improved overview and communication. In an effort to use existing UML2 activity diagrams of an industry partner (Daimler AG) as a source for automatic generation of software artifacts, we discovered, that the diagrams often contain multiple instances of the same element. These redundant instances might improve the readability of a diagram. However, they complicate further approaches such as automated model analysis or traceability to other artifacts because mostly redundant instances must be handled as one distinctive element. In this paper, we present an approach to automatically remove redundant ExecutableNodes within activity diagrams as they are used by our industry partner. The removal is implemented by merging the redundant instances to a single element and adding additional elements to maintain the original behavior of the activity. We use reachability graphs to argue that our approach preserves the behavior of the activity. Additionally, we applied the approach to a real system described by 36 activity diagrams. As a result 25 redundant instances were removed from 15 affected diagrams.",
        "keywords": [],
        "authors": [
            "Martin Beckmann",
            "Vanessa N. Michalke",
            "Andreas Vogelsang",
            "Aaron Schlutter"
        ],
        "file_path": "data/models/models17/Removal of Redundant Elements within UML Activity Diagrams.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "An Empirical Study on the Maturity of the Eclipse Modeling Ecosystem",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Since the early days of Model-driven Engineering (MDE), our community has been discussing the reasons why MDE had not quickly became mainstream. It is now clear the answer is a mix of technical and social factors, but among the former, the lack of maturity of MDE tools is often mentioned. The goal of this paper is to explore the question of whether this lack of maturity is actually true. We do so by comparing the maturity of over a hundred modeling and non-modeling projects living together in the Eclipse ecosystem. In both cases, we use the word project to refer to a variety of tools, libraries and other artefacts to build and manipulate software components, either at the model or code level. Our maturity model is based on code-centric and community metrics that we evaluate on the repository data for both kinds of projects. Their incubation status is also considered in the assessment. Results show that there are indeed diﬀerences between modeling and non-modeling projects, though less than we expected when setting up the study. Moreover, while the incu-bation status clearly separates non-modeling projects, the same is not true for modeling projects which seem to remain much more stable across their lifespan. We believe our results help to have a better perspective on maturity of modeling support nowadays and provide ideas for further analysis towards their improvement.",
        "keywords": [],
        "authors": [
            "Javier Luis Cánovas Izquierdo",
            "Valerio Cosentino",
            "Jordi Cabot"
        ],
        "file_path": "data/models/models17/An Empirical Study on the Maturity of the Eclipse Modeling Ecosystem.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "DREAMS Toolchain: Model-Driven Engineering of Mixed-Criticality Systems",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Mixed-criticality systems (MCS) aim at boosting the integration density in safety-critical systems, resulting into efficient systems, while simultaneously providing increased performance. The DREAMS project provides a cross-domain architectural style for MCS based on networked, virtualized multi-cores controlled by hierarchical resource managers. However, the availability of a platform is only one side of the coin: deploying mixed-critical applications to shared resources typically requires design-time configurations (e.g., to ensure real-time constraints or separation constraints mandated by safety regulations). These configurations are the outcome of complex optimization problems which are intractable in a manual process that also hardly can guarantee the consistency of all deployable artefacts nor their traceability to the requirements. However, existing toolchains lack support for MCS integration, and particularly DREAMS’ advanced platform capabilities.\nWe present an integrated model-driven toolchain and the underlying metamodels covering all relevant aspects of MCS including applications, timing, platforms, deployments, configurations and annotations for extra-functional properties such as safety. The approach focuses on the left branch of the V-cycle, and ranges from product-line and design space exploration to resource allocation and configuration generation. We report on the integration of exploration tools and a reconfiguration graph synthesizer, and evaluate the resulting toolchains in two use cases consisting of a product-line of wind power control applications and an avionic subsystem respectively.",
        "keywords": [
            "Model-Driven Engineering",
            "Mixed-Criticalitity Systems",
            "Safety",
            "Resource Management",
            "Product-Lines",
            "Design Space Exploration"
        ],
        "authors": [
            "Simon Barner",
            "Alexander Diewald",
            "Jörn Migge",
            "Ali Syed",
            "Gerhard Fohler",
            "Madeleine Faugère",
            "Daniel Gracia Pérez"
        ],
        "file_path": "data/models/models17/DREAMS Toolchain Model-Driven Engineering of Mixed-Criticality Systems.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From Secure Business Process Modeling to Design-Level Security Veriﬁcation",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Tracing and integrating security requirements throughout the development process is a key challenge in security engineering. In socio-technical systems, security requirements for the organizational and technical aspects of a system are currently dealt with separately, giving rise to substantial misconceptions and errors. In this paper, we present a model-based security engineering framework for supporting the system design on the organizational and technical level. The key idea is to allow the involved experts to specify security requirements in the languages they are familiar with: business analysts use BPMN for procedural system descriptions; system developers use UML to design and implement the system architecture. Security requirements are captured via the language extensions SecBPMN2 and UMLsec. We provide a model transformation to bridge the conceptual gap between SecBPMN2 and UMLsec. Using UMLsec policies, various security properties of the resulting architecture can be veriﬁed. In a case study featuring an air trafﬁc management system, we show how our framework can be practically applied.",
        "keywords": [],
        "authors": [
            "Qusai Ramadan",
            "Mattia Salnitri",
            "Daniel Strüber",
            "Jan Jürjens",
            "Paolo Giorgini"
        ],
        "file_path": "data/models/models17/From Secure Business Process Modeling to Design-Level Security Verification.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven Development of Safety Architectures",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "We describe the use of model-driven development for safety assurance of a pioneering NASA ﬂight operation involving a ﬂeet of small unmanned aircraft systems (sUAS) ﬂying beyond visual line of sight. The central idea is to develop a safety architecture that provides the basis for risk assessment and visualization within a safety case, the formal justiﬁcation of acceptable safety required by the aviation regulatory authority. A safety architecture is composed from a collection of bow tie diagrams (BTDs), a practical approach to manage safety risk by linking the identiﬁed hazards to the appropriate mitigation measures. The safety justiﬁcation for a given unmanned aircraft system (UAS) operation can have many related BTDs. In practice, however, each BTD is independently developed, which poses challenges with respect to incremental development, maintaining consistency across different safety artifacts when changes occur, and in extracting and presenting stakeholder speciﬁc information relevant for decision making. We show how a safety architecture reconciles the various BTDs of a system, and, collectively, provide an overarching picture of system safety, by considering them as views of a uniﬁed model. We also show how it enables model-driven development of BTDs, replete with validations, transformations, and a range of views. Our approach, which we have implemented in our toolset, AdvoCATE, is illustrated with a running example drawn from a real UAS safety case. The models and some of the innovations described here were instrumental in successfully obtaining regulatory ﬂight approval.",
        "keywords": [
            "Bow tie diagram",
            "Model-driven development",
            "Safety architecture",
            "Safety case",
            "Transformation",
            "Unmanned aircraft systems",
            "Views"
        ],
        "authors": [
            "Ewen Denney",
            "Ganesh Pai",
            "and Iain Whiteside"
        ],
        "file_path": "data/models/models17/Model-Driven Development of Safety Architectures.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Raising Time Awareness in Model-Driven Engineering",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "The conviction that big data analytics is a key for the success of modern businesses is growing deeper, and the mobilisation of companies into adopting it becomes increasingly important. Big data integration projects enable companies to capture their relevant data, to efficiently store it, turn it into domain knowledge, and finally monetize it. In this context, historical data, also called temporal data, is becoming increasingly available and delivers means to analyse the history of applications, discover temporal patterns, and predict future trends. Despite the fact that most data that today’s applications are dealing with is inherently temporal current approaches, methodologies, and environments for developing these applications don’t provide sufficient support for handling time. We envision that Model-Driven Engineering (MDE) would be an appropriate ecosystem for a seamless and orthogonal integration of time into domain modelling and processing. In this paper, we investigate the state-of-the-art in MDE techniques and tools in order to identify the missing bricks for raising time-awareness in MDE and outline research directions in this emerging domain.",
        "keywords": [
            "Model-Driven Engineering",
            "Analytics",
            "Big Data",
            "Temporal Data",
            "Internet of Things"
        ],
        "authors": [
            "Amine Benelallam",
            "Thomas Hartmann",
            "Ludovic Mouline",
            "Francois Fouquet",
            "Johann Bourcier",
            "Olivier Barais",
            "and Yves Le Traon"
        ],
        "file_path": "data/models/models17/Raising Time Awareness in Model-Driven Engineering Vision Paper.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Additivity in Transformation Languages",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Some areas in computer science are characterized\nby a shared base structure for data artifacts (e.g., list, table,\ntree, graph, model), and dedicated languages for transforming\nthis structure. We observe that in several of these languages\nit is possible to identify a clear correspondence between some\nelements in the transformation code and the output they generate.\nConversely given an element in an output artifact it is often\npossible to immediately trace the transformation parts that are\nresponsible for its creation.\nIn this paper we formalize this intuitive concept by deﬁning\na property that characterizes several transformation languages\nin different domains. We name this property additivity: for a\ngiven ﬁxed input, the addition or removal of program elements\nresults in a corresponding addition or removal of parts of\nthe output. We provide a formal deﬁnition for additivity and\nargue that additivity enhances modularity and incrementality\nof transformation engineering activities, by enumerating a set\nof tasks that this property enables or facilitates. Then we\ndescribe how it is instantiated in some well-known transformation\nlanguages. We expect that the development of new formal results\non languages with additivity will beneﬁt from our deﬁnitions.",
        "keywords": [],
        "authors": [
            "Sochiro Hidaka",
            "Fr´ed´eric Jouault",
            "Massimo Tisi"
        ],
        "file_path": "data/models/models17/On Additivity in Transformation Languages.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Property-based Locking in Collaborative Modeling",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Large-scale model-driven engineering projects are carried out collaboratively. Enabling a high degree of concurrency is required to make the traditionally rigid development processes more agile. The increasing number of collaborators increases the probability of introducing conﬂicts which need to be resolved manually by the collaborators. In case of highly interdependent models, avoiding conﬂicts by the use of locks can save valuable time. However, traditional locking techniques such as fragment-based and object-based strategies may impose unnecessary restrictions on editing, which can decrease the efﬁciency of collaboration.\n\nIn this paper, we propose a property-based locking approach that generalizes traditional locking techniques, and further allows more ﬁne-grained locks in order to restrict modiﬁcations only when necessary. A lock is considered to be violated if a match appears or disappears for its associated graph pattern (formula), which captures the property of the model that the upcoming edit transaction can be freely executed. An initial evaluation has been carried out using a case study of the MONDO EU project.",
        "keywords": [],
        "authors": [
            "Csaba Debreceni",
            "G´abor Bergmann",
            "Istv´an R´ath",
            "D´aniel Varr´o"
        ],
        "file_path": "data/models/models17/Property-Based Locking in Collaborative Modeling.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Symbolic Execution for Realizability-Checking of Scenario-based Speciﬁcations",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Scenario-based speciﬁcation with the Scenario Modeling Language (SML) is an intuitive approach for formally specifying the behavior of reactive systems. SML is close to how humans conceive and communicate requirements, yet SML is executable and simulation and formal realizability checking can ﬁnd speciﬁcation ﬂaws early. The realizability checking complexity is, however, exponential in the number of scenarios and variables. Therefore algorithms relying on explicit-state exploration do not scale and, especially when speciﬁcations have message parameters and variables over large domains, fail to unfold their potential. In this paper, we present a technique for the symbolic execution of SML speciﬁcations that interprets integer message parameters and variables symbolically. It can be used for symbolic realizability checking and interactive symbolic simulation. We implemented the technique in SCENARIOTOOLS. Evaluation shows drastic performance improvements over the explicit-state approach for a range of examples. Moreover, sym- bolic checking produces more concise counter examples, which eases the comprehension of speciﬁcation ﬂaws.",
        "keywords": [],
        "authors": [
            "Joel Greenyer",
            "Timo Gutjahr"
        ],
        "file_path": "data/models/models17/Symbolic Execution for Realizability-Checking of Scenario-Based Specifications.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Title page iii.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Ecoreiﬁcation: Making Arbitrary Java Code Accessible to Metamodel-Based Tools",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Models are used in software engineering to describe parts of a system that are relevant for the computation of speciﬁc analyses, or the provision of speciﬁc functionality. Metamodeling languages such as Ecore make it possible to realize analyses and functionality with model-driven technology, such as transformation engines. If models conform to a metamodel that was expressed using Ecore, numerous Eclipse-based tools can be reused to directly analyze, display, or transform models. In many software projects, models are, however, realized with objects of plain-old Java classes rather than an explicit metamodel, so these popular tools cannot be used.\n\nIn this new ideas paper, we present an Ecoreiﬁcation approach, which can be used to automatically extract Ecore-conforming metamodels from Java code, and a code generator that combines the beneﬁts of both worlds. The resulting code can be used exactly as before, but it also uses the modeling infrastructure and implements all interfaces for Ecore-based tooling. This way, arbitrary non-standard models can be displayed and modiﬁed, for example using graphical Sirius editors, or transformed with well-proven transformation languages, such as QVT-O or ATL.",
        "keywords": [],
        "authors": [
            "Heiko Klare",
            "Erik Burger",
            "Max Kramer",
            "Michael Langhammer",
            "Timur Sa˘glam",
            "Ralf Reussner"
        ],
        "file_path": "data/models/models17/Ecoreification Making Arbitrary Java Code Accessible to Metamodel-Based Tools.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-O, ATL"
        }
    },
    {
        "title": "Experiences with Teaching MPS in Industry\nTowards Bringing Domain Speciﬁc Languages Closer to Practitioners",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "Domain speciﬁc languages (DSLs) bring substantial increase in productivity and quality and thus look very appealing to software engineering practitioners. Because language workbenches can drastically reduce the cost of building and maintaining DSLs and associated tooling, they catch the attention of technical leads and project managers in the industry. Effective use of language engineering technologies for software development requires specific knowledge about building DSLs in general and about language workbenches in particular. Practicing software engineers need to enrich their skills with a new software development approach and the supporting tools. In this paper we present our experiences with training and coaching software practitioners in developing domain specific languages and the associated tooling with Jetbrains’ Meta-Programming System. We distill the experience that we have gained over the last three years while running 16 trainings organized by three different organizations. The trainings were attended by over 50 developers, who work in different business domains and posses a wide variety of technical backgrounds, previous experiences and concrete needs. We present a set of challenges faced while teaching language engineering technologies in the industry. To address these challenges we developed a curriculum containing increasingly complex topics and an approach, which combines classical trainings with continuous coaching either remotely or on site. Based on our experience we distill a set of lessons learnt about the dissemination of language engineering technologies to practitioners. We identify several concrete needs which are key to broader adoption of language engineering in practice.",
        "keywords": [],
        "authors": [
            "Daniel Ratiu",
            "Vaclav Pech",
            "Kolja Dummann"
        ],
        "file_path": "data/models/models17/Experiences with Teaching MPS in Industry Towards Bringing Domain Specific Languages Closer to Practitioners.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bridging Proprietary Modelling and Open-Source Model Management Tools: The Case of PTC Integrity Modeller and Epsilon",
        "submission-date": "2017/00",
        "publication-date": "2017/00",
        "abstract": "While the majority of research on Model-Based Software Engineering revolves around open-source modelling frameworks such as EMF, the use of commercial and closed-source modelling tools such as RSA, Rhapsody, MagicDraw and PTC Integrity Modeller appears to be the norm in industry at present. This technical gap can prohibit industrial users from reaping the beneﬁts of state-of-the-art research-based tools in their practice. In this paper, we discuss an attempt to bridge a proprietary UML modelling tool (PTC Integrity Modeller), which is used for model-based development of safety-critical systems at Rolls-Royce, with an open-source family of languages for automated model management (Epsilon). We present the architecture of our solution, the challenges we encountered in developing it, and a performance comparison against the tool’s built-in scripting interface.",
        "keywords": [],
        "authors": [
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Dimitrios S. Kolovos",
            "Richard F. Paige",
            "Stuart Hutchesson"
        ],
        "file_path": "data/models/models17/Bridging Proprietary Modelling and Open-Source Model Management Tools The Case of PTC Integrity Modeller and Epsilon.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon"
        }
    },
    {
        "title": "A Fuzzy Logic Based Approach for Model-based Regression Test Selection",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Regression testing is performed to verify that previously developed functionality of a software system is not broken when changes are made to the system. Since executing all the existing test cases can be expensive, regression test selection (RTS) approaches are used to select a subset of them, thereby improving the efficiency of regression testing. Model-based RTS approaches select test cases on the basis of changes made to the models of a software system. While these approaches are useful in projects that already use model-driven development methodologies, a key obstacle is that the models are generally created at a high level of abstraction. They lack the information needed to build traceability links between the models and the coverage-related execution traces from the code-level test cases. \nIn this paper, we propose a fuzzy logic based approach named FLiRTS, for UML model-based RTS. FLiRTS automatically refines abstract UML models to generate multiple detailed UML models that permit the identification of the traceability links. The process introduces a degree of uncertainty, which is addressed by applying fuzzy logic based on the refinements to allow the classification of the test cases as retestable according to the probabilistic correctness associated with the used refinement. The potential of using FLiRTS is demonstrated on a simple case study. The results are promising and comparable to those obtained from a model-based approach (MaRTS) that requires detailed design models, and a code-based approach (DejaVu).",
        "keywords": [
            "fuzzy logic",
            "model-based testing",
            "regression test selection",
            "UML models"
        ],
        "authors": [
            "Mohammed Al-Refai\nWalter Cazzola\nSudipto Ghosh"
        ],
        "file_path": "data/models/models17/A Fuzzy Logic Based Approach for Model-Based Regression Test Selection.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "User Experience for Model-Driven Engineering: Challenges and Future Directions",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Since its infancy, Model Driven Engineering (MDE) research has primarily focused on technical issues. Although it is becoming increasingly common for MDE research papers to evaluate their theoretical and practical solutions, extensive usability studies are still uncommon. We observe a scarcity of User eXperience (UX)-related research in the MDE community, and posit that many existing tools and languages have room for improvement with respect to UX [26], [44], [37], where UX is a key focus area in the software development industry. We consider this gap a fundamental problem that needs to be addressed by the community if MDE is to gain widespread use. In this vision paper, we explore how and where UX fits into MDE by considering motivating use cases that revolve around different dimensions of integration: model integration, tool integration, and integration between process and tool support. Based on the literature and our collective experience in research and industrial collaborations, we propose future directions for addressing these challenges.",
        "keywords": [],
        "authors": [
            "Silvia Abrahão",
            "Francis Bordeleau",
            "Betty Cheng",
            "Sahar Kokaly",
            "Richard F. Paige",
            "Harald Störrle",
            "Jon Whittle"
        ],
        "file_path": "data/models/models17/User Experience for Model-Driven Engineering Challenges and Future Directions.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bringing DSE to life: exploring the design space of an industrial automotive use case",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "In order to cope with the rising complexity of today’s systems, model-based development of software-intensive embedded systems has become a de-facto standard in recent years. Such a development approach enables a variety of front-loading methods. Design space exploration is one of those techniques. However, in order to properly perform a valid exploration, a system model has to have a certain quality. This requires dedicated, meaningful models as an input according to well-known design principles, which entails the structuring of models according to different viewpoints and usage of dedicated models for each of these viewpoints.\n\nIn this work, we demonstrate how, based on an industrial application model represented in SysML, design space exploration methods can be efficiently applied to enable the synthesis of deployments from a logical (platform-independent) system models to technical (platform-specific) system models. More-over, we will demonstrate the applicability of this approach by a project conducted with Continental.",
        "keywords": [
            "design space exploration",
            "mbse",
            "deployment"
        ],
        "authors": [
            "Johannes Eder\nSergey Zverlov\nSebastian Voss\nMaged Khalil\nAlexandru Ipatiov"
        ],
        "file_path": "data/models/models17/Bringing DSE to Life Exploring the Design Space of an Industrial Automotive Use Case.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Transformations of Software Product Lines: A Generalizing Framework based on Category Theory",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.",
        "keywords": [],
        "authors": [
            "Gabriele Taentzer",
            "Rick Salay",
            "Daniel Strüber",
            "and Marsha Chechik"
        ],
        "file_path": "data/models/models17/Transformations of Software Product Lines A Generalizing Framework Based on Category Theory.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Software Product Lines with Design Choices: Reasoning about Variability and Design Uncertainty",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "When designing changes to a software product line (SPL), developers are faced with uncertainty about deciding among multiple possible SPL designs. Since each SPL design encodes a set of related products, dealing with multiple designs means that developers must reason about sets of sets of products. The additional degree of multiplicity is not well described by existing product line abstractions. In this paper, we propose an approach for dealing with design uncertainty within SPLs using a novel composition of variability modelling with an abstraction for capturing and managing design uncertainty. This allows developers to accurately describe the decisions involved in making changes to an SPL during the design stage and provides them with a framework for SPL design space exploration by analyzing and enforcing SPL properties.",
        "keywords": [],
        "authors": [
            "Michalis Famelis",
            "Julia Rubin",
            "Krzysztof Czarnecki",
            "Rick Salay",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models17/Software Product Lines with Design Choices Reasoning about Variability and Design Uncertainty.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Organization.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Why Is My Component and Connector Views Speciﬁcation Unsatisﬁable?",
        "submission-date": "2017/10",
        "publication-date": "2017/10",
        "abstract": "Component and connector (C&C) views speciﬁcations, with corresponding veriﬁcation and synthesis techniques, have been recently suggested as a means for formal yet intuitive structural speciﬁcation of component and connector models. One challenge for effective use of C&C views synthesis relates to the case where the speciﬁcation is unsatisﬁable.\n\nIn this work we present an approach to deal with unsatisﬁable C&C views speciﬁcations. First, we deﬁne a notion of a C&C views speciﬁcation core, a locally minimal unsatisﬁable subset of the views speciﬁcation. Second, based on the core, we generate explicit, concrete, structured natural-language report, which explains the cause of unsatisﬁability. Finally, we extend our work to support speciﬁcations with architecture styles, library components, and Boolean formulas beyond simple conjunctions.\n\nOur views core computation relies on a new translation to SAT, via Alloy, which is reﬁned enough to allow the extraction of detailed explanations. We implemented our work and evaluated it using 12 synthetic and real-world C&C views speciﬁcations. The evaluation examines the cost of the core computation and its effectiveness in reducing the size of the speciﬁcation.",
        "keywords": [],
        "authors": [
            "Shahar Maoz",
            "Nitzan Pomerantz",
            "Jan Oliver Ringert",
            "RaﬁShalom"
        ],
        "file_path": "data/models/models17/Why is My Component and Connector Views Specification Unsatisfiable-.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synthesis and Exploration of Multi-Level, Multi-Perspective Architectures of Automotive Embedded Systems",
        "submission-date": "2017/04",
        "publication-date": "2017/04",
        "abstract": "In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today’s engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multilevel, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of questions, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates relevant to their task at hand. Finally, using the modeling language Clafer, which is supported by automated backend reasoners, we show that it is possible to synthesize and explore optimal candidate architectures for two highly configurable automotive subsystems.",
        "keywords": [
            "Architecture Synthesis; Multi-Level Architectures; Multi-Perspective Architectures; EE Architecture; Architecture Optimization; Candidate Architectures; Early Design"
        ],
        "authors": [
            "Jordan A. Ross",
            "Alexandr Murashkin",
            "Jia Hui Liang",
            "Micha Antkiewicz",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/models/models17/Synthesis and Exploration of Multi-level- Multi-perspective Architectures of Automotive Embedded Systems -SoSYM Abstract-.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Title page i.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Survey of Tool Use in Modeling Education",
        "submission-date": "2016/12",
        "publication-date": "Not found",
        "abstract": "We present the results of a survey of tool use in software modeling education conducted from December 2016 to March 2017. The survey was conducted among 150 professors who taught modeling in 30 countries from all regions of the world. Professors reported using 32 modeling tools. Top motivations for choosing tools are simplicity of learning and installing, as well as the tools being free and supporting the most important notations. Top complaints about tools included not interacting with other tools, not supporting sufficient modeling aspects, and being complex to use. Seven of the tools were used by more than one professor as their main tools, and we analyzed these in more depth. Among these 7, lack of feedback about models emerged as another key weakness. The tools varied very considerably regarding which of these strengths and weaknesses they exhibited. The key lessons from the paper are a) that tool developers have many opportunities to improve their products, and b) that educators might benefit from introducing students to multiple different tools.",
        "keywords": [
            "modeling tool; survey; education"
        ],
        "authors": [
            "Luciane T. W. Agner",
            "Timothy C. Lethbridge"
        ],
        "file_path": "data/models/models17/A Survey of Tool Use in Modeling Education.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems (MODELS 2017)",
        "submission-date": "2017/01",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models17/Preface.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Managing Design-Time Uncertainty",
        "submission-date": "2017/03",
        "publication-date": "2017/03",
        "abstract": "Any software system is the accumulated result of many design decisions taken by its developers. During the course of development, however, developers are often uncertain about how to make these decisions. This uncertainty reﬂects lack of knowledge about the design of the system, rather than about the environment in which the system is intended to operate. It is therefore called design-time uncertainty, and is different from environmental uncertainty [1]. Addressing environmental uncertainty requires using strategies such as self-adaptation [2], which result in fully functional software systems, capable of operating under uncertain conditions, i.e., uncertainty-aware software. In contrast, design-time un- certainty (henceforth, also simply “uncertainty”) cannot be “coded away”. Rather, it must be tackled as part of the process of software development, i.e., using uncertainty-aware software development methodologies. Existing methodologies, languages and tools assume that their inputs do not contain any uncertainty. Thus, uncertainty is rendered an undesirable characteristic that developers should either avoid or remove altogether before resuming their work. This results in either costly delays or potentially premature – and therefore risky – resolutions of uncertainty as developers make provisional decisions and attempt to keep track of them in case they need to be undone. We present an alternative strategy: the explicit management of design time uncertainty as part of the course of software development [3]. Specifically, we build on previously published work for encoding alternative design decisions in partial models which can subsequently be used for tasks such as reasoning, re- finement and transformation [4]. These techniques had been implemented as partial model operators in MU-MMINT, an interactive modelling tool [5]. We combine these point solu- tions into a coherent, tool-supported methodology for tackling design-time uncertainty. This combination allows deferring the resolution of uncertainty for as long as necessary while the development work can continue.",
        "keywords": [],
        "authors": [
            "Michalis Famelis",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models17/Managing Design-Time Uncertainty.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Active Domain-Speciﬁc Languages: making every mobile user a modeller",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "Domain-speciﬁc languages (DSLs) are small languages tailored to a certain application area, like logistics, web application testing or smart city planning. Traditionally, the use of DSLs has been limited to a static setting in desktop or web editors. However, in this paper, we claim that DSLs can be central components of mobile collaborative applications. In our vision, graphical DSLs can be extended to make use of mobility and context, and integrate heterogeneous information gathered from open APIs. We call this new generation languages “active DSLs”. We foresee a range of scenarios where active DSLs can be useful. On the one hand, they can be used more ﬂexibly in remote locations by enabling local collaboration of several mobile devices using their short-range communication capabilities. On the other hand, they can be extended with contextual features like geolocation, allowing the integration of maps and geo-services within the DSL, or the DSL rendering customization in response to contextual information. Active DSLs can also retrieve information from open APIs, in which case, models deﬁned with the DSL become aggregators of heterogeneous data. In this paper, we explain our vision for active DSLs and the ﬁrst steps towards its realization in the DSL-comet tool. The tool permits creating and using mobile graphical DSLs on iOS devices, and their seamless use in desktop environments.",
        "keywords": [
            "active DSL; graphical modelling language; ﬂexible modelling; mobile application; API integration"
        ],
        "authors": [
            "Diego Vaquero-Melchor",
            "Javier Palomares",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models17/Active Domain-Specific Languages Making Every Mobile User a Modeller.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ontology-Based Pattern for System Engineering",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "System engineering is a multi-domain process that encompasses the design, realization, delivery, and management of complex systems or system of systems. The Model-Based System Engineering (MBSE) approach is commonly accepted by the system engineers community that depends up on the creation of centralized models to produce the expected deliverables. Standard metamodels such as UML, SysML, or NMM/NAF are typically used to describe the relevant concepts for these descriptive models. However, there is a need to also use domain speciﬁc languages (aka ontologies) to ease the communication between all the system engineering stakeholders.\nThe author proposed an approach in previous works to reconcile the usage of complex but necessary predeﬁned metamodels with dedicated ontologies. This solution speeds up the creation of model-based documents. However, the imple-mentation of such approach revealed that the modeling users are expecting a solution in-between the frozen metamodel and the speciﬁc ontology approach; a set of predeﬁned modeling features addressing recurrent engineering concerns completed by project speciﬁc concerns. Among the recurrent concerns there are the requirement elicitation, the functional analysis, the system interface deﬁnitions. . . .\nThis paper shows how this balance can be addressed through ontology-based patterns developed as modular mod-eling features blocks. Since these blocks are applied in the context of model-based system engineering we also named them MBSE Enablers. The paper proposes a solution to a new issue raised by this pattern reuse expectations; a dynamic mapping is required between the building blocks and the existing models. The proposed method is based on the category theory which brings a theoretical foundation to ensure models are correctly managed. The global idea of the extended approach is to speed up again the modeling tool customizations letting the system engineers focusing as far as possible on the systems to be designed.",
        "keywords": [
            "MBSE",
            "Model-Based System Engineering",
            "System Engineering",
            "Metamodel",
            "Ontology",
            "Dynamic Ontology Mapping",
            "Ontology Pattern"
        ],
        "authors": [
            "Dr Dominique Ernadote"
        ],
        "file_path": "data/models/models17/Ontology-Based Pattern for System Engineering.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models19/Steering Committee.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling approach and evaluation criteria for adaptable architectural runtime model instances",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "An architectural runtime model is a causally connected abstract representation of a system that allows monitoring the system and adapting its conﬁguration. Since systems are often constructed to operate continuously, the corresponding runtime model instances need to be long-living and available without interruptions. An interruption occurs if a model needs to be re-instantiated with a new version of the modeling language implementation to support other kinds of information. Adaptable runtime models instances can render such interruptions unnecessary and enable changing information demands at runtime. They support multiple abstraction levels for different model parts and allow adjusting over time which details of the system and its environment are represented. This helps to focus the attention for effective and efﬁcient decision making. In this vision paper we present the fundamental idea of a generic modeling language for adaptable architectural runtime model instances and propose requirements and quality characteristics as criteria for its evaluation.",
        "keywords": [],
        "authors": [
            "Thomas Brand and Holger Giese"
        ],
        "file_path": "data/models/models19/Modeling Approach and Evaluation Criteria for Adaptable Architectural Runtime Model Instances.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Based Resource Analysis and Synthesis of Service-Oriented Automotive Software Architectures",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Abstract—Context: Automotive software architectures describe distributed functionality through an interplay of software components. One drawback of today’s architectures is their strong integration into the onboard communication network based on predefined dependencies at design-time. To foster independence, the idea of service-oriented architecture (SOA) provides a suitable prospect as network communication is established dynamically at run-time. Aim: We target to provide a model-based design methodology for analysing and synthesising hardware resources of automotive service-oriented architectures. Approach: For the approach, we apply the concepts of design space exploration and simulation to analyse and synthesise deployment configurations at an early stage of development. Result: We present an architecture candidate for an example function from the domain of automated driving. Based on corresponding simulation results, we gained insights about the feasibility to implement this candidate within our currently considered next E/E architecture generation. Conclusion: The introduction of service-oriented architectures strictly requires early run-time assessments. In order to get there, the usage of models and model transformations depict reasonable ways by additionally accounting quality and development speed.",
        "keywords": [
            "Service-oriented architecture",
            "real-time behaviour",
            "model-based design",
            "automotive architectures"
        ],
        "authors": [
            "Philipp Obergfell",
            "Stefan Kugele",
            "Eric Sax"
        ],
        "file_path": "data/models/models19/Model-Based Resource Analysis and Synthesis of Service-Oriented Automotive Software Architectures.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards System-Level Testing with Coverage Guarantees for Autonomous Vehicles",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Since safety-critical autonomous vehicles need to interact with an immensely complex and continuously changing environment, their assurance is a major challenge. While systems engineering practice necessitates assurance on multiple levels, existing research focuses dominantly on component-level assurance while neglecting complex system-level trafﬁc scenarios. In this paper, we aim to address the system-level testing of the situation-dependent behavior of autonomous vehicles by combining various model-based techniques on different levels of abstraction. (1) Safety properties are continuously monitored in challenging test scenarios (obtained in simulators or ﬁeld tests) using graph query and complex event processing techniques. To precisely quantify the coverage of an existing test suite with respect regulations of safety standards, (2) we provide qualitative abstractions of causal, temporal, or geospatial data recorded in individual runs into situation graphs, which allows to systematically measure system-level situation coverage (on an abstract level) wrt. safety concepts captured by domain experts. Moreover, (3) we can systematically derive new challenging (abstract) situations which justiﬁably lead to runtime behavior which has not been tested so far by adapting consistent graph generation techniques, thus increasing situation coverage. Finally, (4) such abstract test cases are concretized so that they can be investigated in a real or simulated context.",
        "keywords": [
            "Model-based testing",
            "Autonomous vehicles",
            "Cyber-Physical Systems",
            "System-level testing",
            "Test coverage"
        ],
        "authors": [
            "Istv´an Majzik",
            "Oszk´ar Semer´ath",
            "Csaba Hajdu",
            "Krist´of Marussy",
            "Zolt´an Szatm´ari",
            "Zolt´an Micskei",
            "Andr´as V¨or¨os",
            "Aren A. Babikian and D´aniel Varr´o"
        ],
        "file_path": "data/models/models19/Towards System-Level Testing with Coverage Guarantees for Autonomous Vehicles.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "2019/00",
        "publication-date": "2019/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models19/Copyright notice.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Classiﬁcation of Metamodel Repositories: A Machine Learning Approach",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Manual classiﬁcation methods of metamodel repositories require highly trained personnel and the results are usually inﬂuenced by the subjectivity of human perception. Therefore, automated metamodel classiﬁcation is very desirable and stringent. In this work, Machine Learning techniques have been employed for metamodel automated classiﬁcation. In particular, a tool implementing a feed-forward neural network is introduced to classify metamodels. An experimental evaluation over a dataset of 555 metamodels demonstrates that the technique permits to learn from manually classiﬁed data and effectively categorize incoming unlabeled data with a considerably high prediction rate: the best performance comprehends 95.40% as success rate, 0.945 as precision, 0.938 as recall, and 0.942 as F1 score.",
        "keywords": [],
        "authors": [
            "Phuong T. Nguyen",
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Alfonso Pierantonio",
            "Ludovico Iovino"
        ],
        "file_path": "data/models/models19/Automated Classification of Metamodel Repositories A Machine Learning Approach.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guided Architecture Trade Space Exploration: Fusing Model Based Engineering & Design by Shopping",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Advances in model-based system engineering have greatly increased the predictive power of models and the analyses that can be run on them. At the same time, designs have become more modular and component-based. It can be difficult to manually explore all possible system designs due to the sheer number of possible architectures and configurations; design space exploration has arisen as a solution to this challenge.\nIn this work, we present the Guided Architecture Trade Space Explorer (GATSE), software which connects an existing model based engineering language (AADL) and tool (OSATE) to an existing design space exploration tool (ATSV). GATSE, AADL, and OSATE are all designed to be easily extended by users, which enables relatively straightforward domain-customizations. ATSV, combined with these customizations, lets system designers “shop” for candidate architectures and interactively explore the architectural trade space according to any quantifiable quality attribute or system characteristic. We evaluate GATSE according to an established framework for variable system architectures, and demonstrate its use on an avionics subsystem.",
        "keywords": [
            "Design Space Exploration",
            "Search-Based System Engineering",
            "Model-Based Engineering",
            "Guided Optimization"
        ],
        "authors": [
            "Sam Procter",
            "Lutz Wrage"
        ],
        "file_path": "data/models/models19/Guided Architecture Trade Space Exploration Fusing Model Based Engineering - Design by Shopping.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "RaM: Causally-connected and Requirements-aware Runtime Models using Bayesian Learning",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "A model at runtime can be defined as an abstract representation of a system, including its structure and behaviour, which exist alongside with the running system. Runtime models provide support for decision-making and reasoning based on design-time knowledge but, also based on information that may emerge at runtime and which was not foreseen before execution. A challenge that persists is the update of runtime models during the execution to support up-to-date information for reasoning and decision-making. New techniques based on machine learning (ML) and Bayesian Learning offer great potential to support the update of runtime models during execution. Runtime models can be updated using these new techniques to, therefore, offer better-informed decision-making based on evidence collected at runtime. The techniques we use in this paper are based on a novel implementation of Partially Observable Markov Decision Processes (POMDPs). In this paper, we demonstrate how given the requirements specification, a Requirements-aware runtime model based on POMDPs (RaM-POMDP) is defined. We study in detail the nature of such runtime models coupled with consideration of the Bayesian inference algorithms and tools that provide evidence of unexpected/surprising changes in the environment. We show how the RaM-POMDPs and the MAPE-K loop offer the basis of the software architecture presented and how the required casual connection of runtime models is realized. Specifically, we demonstrate how according to evidence of changes in the systems, collected by the monitoring infrastructure and using Bayesian inference, the runtime models are updated and inferred (i.e. the first aspect of the causal connection). We also demonstrate how the running system changes its runtime model, producing therefore the corresponding self-adaptations. These self-adaptations are reflected on the managed system (i.e. the second aspect of the causal connection) to better satisfy the requirements specifications and improve conformance to its service level agreements (SLAs). The experiments have been applied to a real case study for the networking application domain.",
        "keywords": [
            "Runtime models",
            "causal connection",
            "decision-making",
            "uncertainty",
            "POMDPs",
            "Bayesian inference/learning"
        ],
        "authors": [
            "Nelly Bencomo",
            "Luis H. Garcia-Paucar"
        ],
        "file_path": "data/models/models19/RaM Causally-Connected and Requirements-Aware Runtime Models using Bayesian Learning.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Alexander Pretschner",
            "Sebastian Voss",
            "Loli Burgueño"
        ],
        "file_path": "data/models/models19/Organizing Committee.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An LSTM-Based Neural Network Architecture for Model Transformations",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Model transformations are a key element in any model-driven engineering approach. But writing them is a time-consuming and error-prone activity that requires specific knowledge of the transformation language semantics. We propose to take advantage of the advances in Artificial Intelligence and, in particular Long Short-Term Memory Neural Networks (LSTM), to automatically infer model transformations from sets of input-output model pairs. Once the transformation mappings have been learned, the LSTM system is able to autonomously transform new input models into their corresponding output models without the need of writing any transformation-specific code. We evaluate the correctness and performance of our approach and discuss its advantages and limitations.",
        "keywords": [
            "MDE",
            "model transformations",
            "LSTM ANN"
        ],
        "authors": [
            "Loli Burgueño",
            "Jordi Cabot",
            "Sébastien Gérard"
        ],
        "file_path": "data/models/models19/An LSTM-Based Neural Network Architecture for Model Transformations.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based, Platform-independent Logging for Heterogeneous Targets",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "A recurring issue in generative approaches, in particular if they generate code for multiple target languages, is logging. How to ensure that logging is performed consistently for all the supported languages? How to ensure that the specific semantics of the source language, e.g. a modeling language or a domain-specific language, is reflected in the logs? How to expose logging concepts directly in the source language, so as to let developers specify what to log? This paper reports on our experience developing a concrete logging approach for ThingML, a textual modeling language built around asynchronous components, statecharts and a first-class action language, as well as a set of “compilers” targeting C, Go, Java and JavaScript.",
        "keywords": [],
        "authors": [
            "Brice Morin and Nicolas Ferry"
        ],
        "file_path": "data/models/models19/Model-Based- Platform-Independent Logging for Heterogeneous Targets.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CONDEnSe: Contract-Based Design Synthesis",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "It is difﬁcult to maintain consistency between artifacts produced during the development of mechatronic systems, and to ensure the successful integration of independently developed parts. The difﬁculty stems from the complex, multidisciplinary nature of the problem, with multiple artifacts produced by each engineering domain, throughout the design process, and across supplier chains.\n\nIn this work, we develop a methodology and a tool, CONDEnSe, that given a set of Assume/Guarantee (A/G) contracts that capture the system requirements, and a high-level decomposition of the system model, automatically generates design variants that respect the requirements and exports those variants to different engineering tools for analysis.\n\nOur methodology makes use of a contract-based design algebra to ensure that generated artifacts for all design variants are consistent by construction, even when the process is modularized and independently developed parts are only later integrated. In contrast with earlier work, our approach reduces the search space to models that comply with the captured design requirements.",
        "keywords": [
            "contracts",
            "synthesis",
            "mbse",
            "integration"
        ],
        "authors": [
            "C´esar Augusto Santos",
            "Amr Hany Saleh",
            "Tom Schrijvers",
            "Mike Nicolai"
        ],
        "file_path": "data/models/models19/CONDEnSe Contract Based Design Synthesis.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bootstrapping MDE Development from ROS\nManual Code - Part 2: Model Generation",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "In principle, Model-Driven Engineering (MDE) addresses central aspects of robotics software development. Domain experts could leverage the expressiveness of models; implementation details over different hardware could be handled by automatic code generation. In practice, most evidence points to manual code development as the norm, despite several MDE efforts in robotics. Possible reasons for this disconnect are the wide ranges of applications and target platforms making all-encompassing MDE IDEs hard to develop and maintain, with developers reverting to writing code manually. Acknowledging this, and given the opportunity to leverage a large corpus of open-source software widely adopted by the robotics community, we pursue modeling as a complement, rather than an alternative, to manually written code. Our previous work introduced metamodels to describe components, their interactions, and their resulting composition, as inspired by, but not limited to, the de-facto standard Robot Operating System (ROS). In this paper we put such metamodels into use through two contributions [1]. First, we automate the generation of models from manually written artifacts through extraction from source code and runtime system monitoring. Second, we make available an easy-to-use web infrastructure to perform the extraction, together with a growing database of models so generated. Our aim with this tooling, publicly available both as-a-service and as source code, is to lower the MDE barrier for practitioners and leverage models to 1) improve the understanding of manually written code; 2) perform correctness checks; and 3) systematize the deﬁnition and adoption of best practices through large-scale generation of models from existing code. A comprehensive example is provided as a walk-through for robotics software practitioners.",
        "keywords": [
            "ROS",
            "models",
            "development environments"
        ],
        "authors": [
            "Nadia Hammoudeh Garcia",
            "Ludovic Delval",
            "Mathias L¨udtke",
            "Andre Santos",
            "Bj¨orn Kahl and Mirko Bordignon"
        ],
        "file_path": "data/models/models19/Bootstrapping MDE Development from ROS Manual Code - Part 2 Model Generation.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain-Level Observation and Control for Compiled Executable DSLs",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Executable Domain-Specific Languages (DSLs) are commonly defined with either operational semantics (i.e., interpretation) or translational semantics (i.e., compilation). An interpreted DSL relies on domain concepts to specify the possible execution states and steps, which enables the observation and control of executions using the very same domain concepts. In contrast, a compiled DSL relies on a transformation to an arbitrarily different target language. This creates a conceptual gap, where the execution can only be observed and controlled through target domain concepts, to the detriment of experts or tools that only understand the source domain. To address this problem, we propose a language engineering architecture for compiled DSLs that enables the observation and control of executions using source domain concepts. The architecture requires the definition of the source domain execution steps and states, along with a feedback manager that translates steps and states of the target domain back to the source domain. We evaluate the architecture with two different compiled DSLs, and show that it does enable domain-level observation and control while increasing execution time by 2× in the worst observed case.",
        "keywords": [
            "Software Language Engineering",
            "Domain-Specific Languages",
            "Executable DSL",
            "Compilation",
            "Feedback"
        ],
        "authors": [
            "Erwan Bousse",
            "Manuel Wimmer"
        ],
        "file_path": "data/models/models19/Domain-Level Observation and Control for Compiled Executable DSLs.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Alferez",
            "Mauricio; Atlee",
            "Joanne M.; Auffinger",
            "Yuri; Babikian",
            "Aren A.; Bencomo",
            "Nelly; Besnard",
            "Valentin; Bordignon",
            "Mirko; Bousse",
            "Erwan; Brand",
            "Thomas; Briand",
            "Lionel; Brun",
            "Matthias; Bucchiarone",
            "Antonio; Búr",
            "Márton; Burdusel",
            "Alexandru; Burgueño",
            "Loli; Cabot",
            "Jordi; Cheng",
            "Betty; Cicchetti",
            "Antonio; de Lara",
            "Juan; Deval",
            "Ludovic; DeVries",
            "Byron; Dhaussy",
            "Philippe; Dingel",
            "Juergen; Di Rocco",
            "Juri; Di Ruscio",
            "Davide; Elkhatib",
            "Yehia; Eric",
            "Sax; Ferry",
            "Nicolas; Fouquet",
            "Francois; García-Domínguez",
            "Antonio; Garcia Paucar",
            "Luis H.; García-Paucar",
            "Luis Hernán; Gérard",
            "Sébastien; Ghezzi",
            "Carlo; Giese",
            "Holger; Goes",
            "Peter; Guerra",
            "Esther; Hajdu",
            "Csaba; Hammoudeh Garcia",
            "Nadia; Hany Saleh",
            "Amr; Hartmann",
            "Thomas; Hassan",
            "Ahmed E.; Hoyos Rodriguez",
            "Horacio; Hu",
            "Zhenjiang; Iovino",
            "Ludovico; Iqbal",
            "Muhammad Zohaib; John",
            "Stefan; Jouault",
            "Frédéric; Jumagaliyev",
            "Assylbek; Jürjens",
            "Jan; Kahl",
            "Björn; Khan",
            "Muhammad Uzair; Kneisel",
            "Peter; Kolovos",
            "Dimitris; Kusmenko",
            "Evgeny; Le Traon",
            "Yves; Lüdtke",
            "Mathias; Majzik",
            "István; Marcony",
            "Annapaola; Marussy",
            "Kristóf; Micskei",
            "Zoltán; Moawad",
            "Assaad; Morin",
            "Brice; Nguyen",
            "Phuong T.; Nickels",
            "Sebastian; Nicolai",
            "Mike; Oliva",
            "Gustavo Ansaldi; Paige",
            "Richard; Parra-Ullauri",
            "Juan Marcelo; Pastore",
            "Fabrizio; Pavlitskaya",
            "Svetlana; Peldszus",
            "Sven; Philipp",
            "Obergfell"
        ],
        "file_path": "data/models/models19/Author Index.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Pitfalls Analyzer: Quality Control for Model-Driven Data Science Pipelines",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Data science pipelines are a sequence of data processing steps that aim to derive knowledge and insights from raw data. Data science pipeline tools simplify the creation and automation of data science pipelines by providing reusable building blocks that users can drag and drop into their pipelines. Such a graphical, model-driven approach enables users with limited data science expertise to create complex pipelines. However, recent studies show that there exist several data science pitfalls that can yield spurious results and, consequently, misleading insights. Yet, none of the popular pipeline tools have built-in quality control measures to detect these pitfalls. Therefore, in this paper, we propose an approach called Pitfalls Analyzer to detect common pitfalls in data science pipelines. As a proof-of-concept, we implemented a prototype of the Pitfalls Analyzer for KNIME, which is one of the most popular data science pipeline tools. Our prototype is model-driven, since the detection of pitfalls is accomplished using pipelines that were created with KNIME building blocks. To showcase the effectiveness of our approach, we run our prototype on 11 pipelines that were created by KNIME experts for 3 Internet-of-Things (IoT) projects. The results indicate that our prototype ﬂags all and only those instances of the pitfalls that we were able to ﬂag while manually inspecting the pipelines.",
        "keywords": [
            "Data science pipelines",
            "model-driven engineering",
            "quality control",
            "data science pitfalls"
        ],
        "authors": [
            "Gopi Krishnan Rajbahadur",
            "Gustavo Ansaldi Oliva",
            "Ahmed E. Hassan",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models19/Pitfalls Analyzer Quality Control for Model-Driven Data Science Pipelines.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Using Models to Enable Compliance Checking Against the GDPR: An Experience Report",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Damiano Torre",
            "Ghanem Soltana",
            "Mehrdad Sabetzadeh",
            "Lionel C. Briand",
            "Yuri Auffinger",
            "and Peter Goes"
        ],
        "file_path": "data/models/models19/Table of contents.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Design of City Spaces via Bidirectional Transformations",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Technological advances enable new kinds of smart environments exhibiting complex behaviors; smart cities are a notable example. Smart functionalities heavily depend on space and need to be aware of entities typically found in the spatial domain, e.g. roads, intersections or buildings in a smart city. We advocate a model-based development, where the model of physical space, coming from the architecture and civil engineering disciplines, is transformed into an analyzable model upon which smart functionalities can be embedded. Such models can then be formally analyzed to assess a composite system design. We focus on how a model of physical space speciﬁed in the CityGML standard language can be transformed into a model amenable to analysis and how the two models can be automatically kept in sync after possible changes. This approach is essential to guarantee safe model-driven development of composite systems inhabiting physical spaces. We showcase transformations of real CityGML models in the context of scenarios concerning both design time and runtime analysis of space-dependent systems.",
        "keywords": [
            "Bidirectional Model Transformations",
            "Model-driven Engineering",
            "CityGML",
            "Cyber-physical spaces"
        ],
        "authors": [
            "Ennio Visconti",
            "Christos Tsigkanos",
            "Zhenjiang Hu",
            "Carlo Ghezzi"
        ],
        "file_path": "data/models/models19/Model-Driven Design of City Spaces via Bidirectional Transformations.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying MDD in the Content Management System Domain: Scenarios and Empirical Assessment",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Content Management Systems (CMSs) such as Joomla and WordPress dominate today’s web. Enabled by standardized extensions, administrators can build powerful web applications for diverse customer demands. However, developing CMS extensions requires sophisticated technical knowledge, and the highly schematic code structure of an extension gives rise to errors during typical development and migration scenarios. Model-driven development (MDD) seems to be a promising paradigm to address these challenges, however it has not found adoption in the CMS domain yet. Systematic evidence of the benefit of applying MDD in this domain could facilitate its adoption; however, an empirical investigation of this benefit is currently lacking.\n\nIn this paper, we present a mixed-method empirical investigation of applying MDD in the CMS domain, based on an interview suite, a controlled experiment, and a field experiment. We consider three scenarios of developing new (both independent and dependent) CMS extensions and of migrating existing ones to a new major platform version. The experienced developers in our interviews acknowledge the relevance of these scenarios and report on experiences that render them suitable candidates for a successful application of MDD. We found a particularly high relevance of the migration scenario. Our experiments largely confirm the potentials and limits of MDD as identified for other domains. In particular, we found a productivity increase up to factor 17 during the development of CMS extensions. Furthermore, our observations highlight the importance of good tooling that seamlessly integrates with already used tool environments and processes.",
        "keywords": [
            "Model-Driven Development",
            "Content Management Systems",
            "Empirical Assessment"
        ],
        "authors": [
            "Dennis Priefer",
            "Peter Kneisel",
            "Wolf Rost",
            "Daniel Str¨uber",
            "Gabriele Taentzer"
        ],
        "file_path": "data/models/models19/Applying MDD in the Content Management System Domain Scenarios and Empirical Assessment.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Alfonso Pierantonio",
            "Antonio Cicchetti",
            "Birgit Demuth",
            "Daniel Amyot",
            "Davide Di Ruscio",
            "Daniel Varro",
            "Dimitris Kolovos",
            "Don Batory",
            "Esther Guerra",
            "Fiona Polack",
            "Friedrich Steimann",
            "Gregor Engels",
            "Hong Mei",
            "Houari Sahraoui",
            "Ileana Ober",
            "Iris Reinhartz-Berger",
            "Jocelyn Simmonds",
            "Jörg Kienzle",
            "Manuel Wimmer",
            "Michalis Famelis",
            "Mira Balaban",
            "Nelly Bencomo",
            "Peter Clarke",
            "Pieter van Gorp",
            "Regina Hebig",
            "Thomas Kühne",
            "Timothy Lethbridge",
            "Yu Jiang"
        ],
        "file_path": "data/models/models19/Program Committee.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Modelling Language to Support the Evolution of Multi-Tenant Cloud Data Architectures",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Multi-tenant data architectures enable efﬁcient resource utilization in cloud applications, but are currently being implemented in industry and research using manual coding techniques that tend to be time consuming and error prone. We propose a novel domain-speciﬁc modeling language, CadaML, to automatically manage the development and evolution of cloud data architectures that (a) adopt multi-tenancy and/or (b) comprise of a combination of different storage solutions such as relational and non-relational databases, and blob storage. CadaML provides concepts and notations to support abstract modelling of a multi-tenant data architecture, and also provides tools to validate the data architecture and automatically produce application code. We rigorously evaluate CadaML through a user experiment where developers of various capabilities are asked to re-architect the data layer of an industrial business process analysis application. We observe that CadaML users required 3.5x less development time than manual coders. In addition to improved productivity, CadaML users highlighted other beneﬁts gained in terms of reliability of generated code and usability.",
        "keywords": [
            "Domain-Speciﬁc modeling",
            "Model-Driven Engineering",
            "Cloud Computing",
            "Multi-tenancy",
            "Software Evolution",
            "Code Generation"
        ],
        "authors": [
            "Assylbek Jumagaliyev",
            "Yehia Elkhatib"
        ],
        "file_path": "data/models/models19/A Modelling Language to Support the Evolution of Multi-tenant Cloud Data Architectures.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Automatic Generation of Atomic Consistency Preserving Search Operators for Search-Based Model Engineering",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Recently there has been increased interest in combining the fields of Model-Driven Engineering (MDE) and Search-Based Software Engineering (SBSE). Such approaches use meta-heuristic search guided by search operators (model mutators and sometimes breeders) implemented as model transformations. The design of these operators can substantially impact the effectiveness and efficiency of the meta-heuristic search. Currently, designing search operators is left to the person specifying the optimisation problem. However, developing consistent and efficient search-operator rules requires not only domain expertise but also in-depth knowledge about optimisation, which makes the use of model-based meta-heuristic search challenging and expensive. In this paper, we propose a generalised approach to automatically generate atomic consistency preserving search operators (aCPSOs) for a given optimisation problem. This reduces the effort required to specify an optimisation problem and shields optimisation users from the complexity of implementing efficient meta-heuristic search mutation operators. We evaluate our approach with a set of case studies, and show that the automatically generated rules are comparable to, and in some cases better than, manually created rules at guiding evolutionary search towards near-optimal solutions.",
        "keywords": [
            "model driven engineering",
            "search based optimisation",
            "search based software engineering"
        ],
        "authors": [
            "Alexandru Burdusel",
            "Steffen Zschaler",
            "Stefan John"
        ],
        "file_path": "data/models/models19/Automatic Generation of Atomic Consistency Preserving Search Operators for Search-Based Model Engineering.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards effective mutation testing for ATL",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "The correctness of model transformations is crucial to obtain high-quality solutions in model-driven engineering. Testing is a common approach to detect errors in transformations, which requires having methods to assess the effectiveness of the test cases and improve their quality. Mutation testing permits assessing the quality of a test suite by injecting artiﬁcial faults in the system under test. These emulate common errors made by competent developers and are modelled using mutation operators. Some researchers have proposed sets of mutation operators for transformation languages like ATL. However, their suitability for an effective mutation testing process has not been investigated, and there is no automated mechanism to generate test models that increase the quality of the tests. In this paper, we use transformations created by third parties to evaluate the effectiveness ATL mutation operators proposed in the literature, and other operators that we have devised based on empirical evidence on real errors made by developers. Likewise, we evaluate the effectiveness of commonly used test model generation techniques. For the cases in which a test suite does not detect an injected fault, we synthesize test models able to detect it. As a technical contribution, we make available a framework that automates this process for ATL.",
        "keywords": [
            "Model transformations",
            "Mutation testing",
            "ATL"
        ],
        "authors": [
            "Esther Guerra",
            "Jesus Sanchez Cuadrado",
            "Juan de Lara"
        ],
        "file_path": "data/models/models19/Towards Effective Mutation Testing for ATL.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Querying and annotating model histories with time-aware patterns",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Models are not static entities: they evolve over time due to changes. Changes may inadvertently and surprisingly violate constraints imposed. Therefore, the models need to be monitored for compliance. On the one hand, in traditional design-time applications, new and evolving requirements impose changes on a model over time. These changes may accidentally break design rules. Further, the growing complexity of the models may need to be tracked for manageability. On the other hand, newer applications use models at runtime; building runtime abstractions that are used to control a system. Adopters of these approaches will need to query the history of the system to check if the models evolved as expected, or to find out the reasons for a particular behavior. Changes over models at runtime are more frequent than changes over design models. To cover these demands, we argue that a flexible and scalable approach for querying the history of the models is needed to study the evolution and for compliance sake. This paper presents a set of extensions to a model query language inspired in the Object Constraint Language (the Epsilon Object Language) for traversing the history of a model, and for making temporal assertions that will allow the elicitation of historic information. As querying long histories may be costly, the paper presents an approach that annotates versions of interest as they are observed, in order to provide efficient recalls in possible future queries. The approach has been implemented in a model indexing tool, and is demonstrated through a case study from the autonomous and self-adaptive systems domain.",
        "keywords": [
            "Model querying",
            "model versioning",
            "temporal graph databases",
            "model indexing",
            "scalable model-driven engineering"
        ],
        "authors": [
            "Antonio García-Domínguez",
            "Nelly Bencomo",
            "Juan Marcelo Parra-Ullauri",
            "Luis Hernán García-Paucar"
        ],
        "file_path": "data/models/models19/Querying and Annotating Model Histories with Time-Aware Patterns.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Goal-Based Modeling and Analysis of Non-Functional Requirements",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Non-functional goals specify a quality attribute of the functional goals for the system-to-be (e.g., cost, performance, security, and safety). However, non-functional goals are often cross-cutting and do not naturally ﬁt within the default decomposition expressed by a functional goal model. Further, any functional mitigations that ensure the satisfaction of a non-functional goal, or occur in the event a non-functional goal is violated, are conditionally applicable to the remainder of the system-to-be. Rather than modeling non-functional goals and their associated mitigations as a part of the system-to-be goal model, we introduce a method of modeling and analyzing non-functional goals and their associated mitigation as separate models. We illustrate our approach by applying our method to model non-functional goals related to an industry-based automotive braking system and analyzing for non-functional violations.",
        "keywords": [
            "non-functional",
            "requirements",
            "goal model"
        ],
        "authors": [
            "Byron DeVries",
            "Betty H.C. Cheng"
        ],
        "file_path": "data/models/models19/Goal-Based Modeling and Analysis of Non-Functional Requirements.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Verifying and Monitoring UML Models with Observer Automata\nA Transformation-free Approach",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "The increasing complexity of embedded systems renders veriﬁcation of software programs more complex and may require applying monitoring and formal techniques, like model-checking. However, to use such techniques, system engineers usually need formal experts to express software requirements in a formal language. To facilitate the use of model-checking tools by system engineers, our approach consists of using a UML model interpreter with which the software requirements can directly be expressed as observer automata in UML as well. These observer automata are synchronously composed with the system, and can be used unchanged both for model veriﬁcation and runtime monitoring. Our approach has been evaluated on the user interface model of a cruise control system. The observer veriﬁcation results are in line with the veriﬁcation of equivalent LTL properties. The runtime overhead of the monitoring infrastructure is 6.5%, with only 1.2% memory overhead.",
        "keywords": [
            "Observer Automata",
            "Monitoring",
            "Model Interpretation",
            "Embedded Systems"
        ],
        "authors": [
            "Valentin Besnard\nCiprian Teodorov\nFrédéric Jouault\nMatthias Brun\nPhilippe Dhaussy"
        ],
        "file_path": "data/models/models19/Verifying and Monitoring UML Models with Observer Automata A Transformation-Free Approach.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Proceedings 2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Marouane Kessentini",
            "Tao Yue",
            "Alexander Pretschner",
            "Sebastian Voss",
            "Loli Burgueño"
        ],
        "file_path": "data/models/models19/Title page iii.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Focus+Context Approach to Alleviate Cognitive Challenges of Editing and Debugging UML Models",
        "submission-date": "2019/00",
        "publication-date": "2019/00",
        "abstract": "Model-Driven Engineering has been proposed to increase the productivity of developing a software system. Despite its beneﬁts, it has not been fully adopted in the software industry. Research has shown that modelling tools are amongst the top barriers for the adoption of MDE by industry. Recently, researchers have conducted empirical studies to identify the most-severe cognitive difﬁculties of modellers when using UML model editors. Their analyses show that users’ prominent challenges are in remembering the contextual information when performing a particular modelling task; and locating, understanding, and ﬁxing errors in the models. To alleviate these difﬁculties, we propose two Focus+Context user interfaces that provide enhanced cognitive support and automation in the user’s interaction with a model editor. Moreover, we conducted two empirical studies to assess the effectiveness of our interfaces on human users. Our results reveal that our interfaces help users 1) improve their ability to successfully fulﬁl their tasks, 2) avoid unnecessary switches among diagrams, 3) produce more error-free models, 4) remember contextual information, and 5) reduce time on tasks.",
        "keywords": [
            "User-Centric Software Development",
            "Empirical Study",
            "UML",
            "Modelling Tools",
            "Modelling Challenges"
        ],
        "authors": [
            "Parsa Pourali",
            "Joanne M. Atlee"
        ],
        "file_path": "data/models/models19/A Focus-Context Approach to Alleviate Cognitive Challenges of Editing and Debugging UML Models.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Exploiting Multi-Level Modelling for Designing and Deploying Gameful Systems",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Gamiﬁcation is increasingly used to build solutions for driving the behaviour of target users’ populations. Gameful systems are typically exploited to keep users’ involvement in certain activities and/or to modify an initial behaviour through game-like elements, such as awarding points, submitting challenges and/or fostering competition and cooperation with other players. Gamiﬁcation mechanisms are well-deﬁned and composed of different ingredients that have to be correctly amalgamated together; among these we ﬁnd single/multi-player challenges targeted to reach a certain goal and providing an adequate award for compensation. Since the current approaches are largely based on hand-coding/tuning, when the game grows in its complex- ity, keeping track of all the mechanisms and maintaining the implementation can become error-prone and tedious activities. In this paper, we describe a multi-level modelling approach for the deﬁnition of gamiﬁcation mechanisms, from their design to their deployment and runtime adaptation. The approach is implemented by means of JetBrains MPS, a text-based meta- modelling framework, and validated using two gameful systems in the Education and Mobility domains.",
        "keywords": [
            "Multi-Level Modelling",
            "Model-Driven Engineering",
            "MPS",
            "Gamiﬁcation Engine"
        ],
        "authors": [
            "Antonio Bucchiarone",
            "Antonio Cicchetti",
            "Annapaola Marconi"
        ],
        "file_path": "data/models/models19/Exploiting Multi-level Modelling for Designing and Deploying Gameful Systems.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On-the-ﬂy Translation and Execution of OCL-like Queries on Simulink Models",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "MATLAB/Simulink is a tool for dynamic system modelling. Model management languages such as OCL, ATL and the languages of the Epsilon platform tend to focus on the Eclipse Modelling Framework (EMF), a de facto standard for domain specific modelling. As Simulink models are built on an entirely different technical stack, the current solution to manipulate them using such languages requires their transformation into an EMF-compatible representation. This approach is expensive as the cost of the transformation can be crippling for large models, it requires the synchronisation of the native Simulink model and its EMF counterpart, and the EMF-representation may be an incomplete copy of the model. In this paper we propose an alternative approach that uses the MATLAB API to bridge Simulink models with existing model management languages that relies on the “on-the-ﬂy” translation of model management language constructs into MATLAB commands. Our approach eliminates the cost of the transformation and of the co-evolution of the EMF-compatible representation while enabling full access to the Simulink model details. We evaluate the performance of both approaches using a set of model validation constraints executed on a sample of the largest Simulink models available on GitHub. Our evaluation suggests that the translation approach can reduce the model validation time up to 80%.",
        "keywords": [
            "Eclipse Modelling Framework",
            "MATLAB Simulink",
            "Model Driven Engineering",
            "Epsilon"
        ],
        "authors": [
            "Beatriz A. Sanchez",
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Dimitris S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/models/models19/On-the-Fly Translation and Execution of OCL-Like Queries on Simulink Models.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-based Testing Approach for Cockpit Display Systems of Avionics",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Avionics are highly critical systems that require extensive testing governed by international safety standards. Cockpit Display Systems (CDS) are an essential component of modern aircraft cockpits and display information from the user application (UA) using various widgets. A signiﬁcant step in the testing of avionics is to evaluate whether these CDS are displaying the correct information. A common industrial practice is to manually test the information on these CDS by taking the aircraft into different scenarios during the simulation. Such testing is required very frequently and at various changes in the avionics. Given the large number of scenarios to test, manual testing of such behavior is a laborious activity. In this paper, we propose a model-based strategy for automated testing of the information displayed on CDS. Our testing approach focuses on evaluating that the information from the user applications is being displayed correctly on the CDS. For this purpose, we develop a proﬁle for capturing the details of different widgets of the display screens using models. The proﬁle is based on the ARINC 661 standard for Cockpit Display Systems. The expected behavior of the CDS visible on the screens of the aircraft is captured using constraints written in Object Constraint Language. We apply our approach on an industrial case study of a Primary Flight Display (PFD) developed for an aircraft. Our results showed that the proposed approach is able to automatically identify faults in the simulation of PFD. Based on the results, it is concluded that the proposed approach is useful in ﬁnding display faults on avionics CDS.",
        "keywords": [
            "Model-based Testing; Cockpit Display Systems; Safety-critical Systems; ARINC 661; Object Constraint Language (OCL)"
        ],
        "authors": [
            "Muhammad Zohaib Iqbal",
            "Hassan Sartaj",
            "Muhammad Uzair Khan",
            "Fitash Ul Haq",
            "Ifrah Qaisar"
        ],
        "file_path": "data/models/models19/A Model-Based Testing Approach for Cockpit Display Systems of Avionics.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Meta-Modelling Meta-Learning",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Although artiﬁcial intelligence and machine learning are currently extremely fashionable, applying machine learning on real-life problems remains very challenging. Data scientists need to evaluate various learning algorithms and tune their numerous parameters, based on their assumptions and experience, against concrete problems and training data sets. This is a long, tedious, and resource expensive task. Meta-learning is a recent technique to overcome, i.e. automate this problem. It aims at using machine learning itself to automatically learn the most appropriate algorithms and parameters for a machine learning problem. As it turns out, there are many parallels between meta-modelling—in the sense of model-driven engineering—and meta-learning. Both rely on abstractions, the meta data, to model a predeﬁned class of problems and to deﬁne the variabilities of the models conforming to this deﬁnition. Both are used to deﬁne the output and input relationships and then ﬁtting the right models to represent that behaviour. In this paper, we envision how a meta-model for meta-learning can look like. We discuss possible variabilities, for what types of learning it could be appropriate for, how concrete learning models can be generated from it, and how models can be ﬁnally selected. Last but not least, we discuss a possible integration into existing modelling tools.",
        "keywords": [
            "meta-learning",
            "meta-modelling",
            "AutoML",
            "modelling framework"
        ],
        "authors": [
            "Thomas Hartmann",
            "Assaad Moawad",
            "Cedric Schockaert",
            "Francois Fouquet",
            "Yves Le Traon"
        ],
        "file_path": "data/models/models19/Meta-Modelling Meta-Learning.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards WCET Estimation of Graph Queries@Run.time",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "Recent approaches in runtime monitoring and live data analytics have started to use expressive graph queries at runtime to capture and observe properties of interest at a high level of abstraction. However, in a critical context, such applications often require timeliness guarantees, which have not been investigated yet for query-based solutions due to limitations of existing static worst-case execution time (WCET) analysis techniques. One limitation is the lack of support for dynamic memory allocation, which is required by the dynamically evolving runtime models on which the queries are evaluated. Another open challenge is to compute WCET for asynchronously communicating programs such as distributed monitors. This paper introduces our vision about how to assess such timeliness properties and how to provide tight WCET estimates for query execution at runtime over a dynamic model. Furthermore, we present an initial solution that combines state-of-the-art parametric WCET estimations with model statistics and search plans of queries.",
        "keywords": [],
        "authors": [
            "Márton Búr",
            "Dániel Varró"
        ],
        "file_path": "data/models/models19/Towards WCET Estimation of Graph Queries-Run.time.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bridging the Gap between Requirements Modeling and Behavior-driven Development",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "Acceptance criteria (AC) are implementation agnostic conditions that a system must meet to be consistent with its requirements and be accepted by its stakeholders. Each acceptance criterion is typically expressed as a natural-language statement with a clear pass or fail outcome. Writing AC is a tedious and error-prone activity, especially when the requirements speciﬁcations evolve and there are different analysts and testing teams involved. Analysts and testers must iterate multiple times to ensure that AC are understandable and feasible, and accurately address the most important requirements and workﬂows of the system being developed. In many cases, analysts express requirements through models, along with natural language, typically in some variant of the UML. AC must then be derived by developers and testers from such models. In this paper, we bridge the gap between requirements models and AC by providing a UML-based modeling methodology and an automated solution to generate AC. We target AC in the form of Behavioral Speciﬁcations in the context of Behavioral-Driven Development (BDD), a widely used agile practice in many application domains. More specially we target the well-known Gherkin language to express AC, which then can be used to generate executable test cases. We evaluate our modeling methodology and AC generation solution through an industrial case study in the ﬁnancial domain. Our results suggest that (1) our methodology is feasible to apply in practice, and (2) the additional modeling effort required by our methodology is outweighed by the beneﬁts the methodology brings in terms of automated and systematic AC generation and improved model precision.",
        "keywords": [
            "Software testing",
            "BDD",
            "modeling",
            "requirements engineering",
            "text generation",
            "Gherkin",
            "and FinTech."
        ],
        "authors": [
            "Mauricio Alferez",
            "Fabrizio Pastore",
            "Mehrdad Sabetzadeh",
            "Lionel C. Briand",
            "Jean-Richard Riccardi"
        ],
        "file_path": "data/models/models19/Bridging the Gap between Requirements Modeling and Behavior-Driven Development.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MODELS 2019",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models19/Title page i.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Preface to the 22nd International ACM/IEEE Conference on Model Driven Engineering Languages and Systems",
        "submission-date": "2019/09",
        "publication-date": "2019/09",
        "abstract": "This document is a preface to the 22nd International ACM/IEEE Conference on Model Driven Engineering Languages and Systems (MODELS 2019), held September 15-20, 2019 in Munich, Germany. It provides information about the conference location, program, submission and acceptance rates, and acknowledges the contributions of various individuals and organizations involved in its organization.",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models19/Preface.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling and Training of Neural Processing Systems",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "The ﬁeld of deep learning has become more and more pervasive in the last years as we have seen varieties of problems being solved using neural processing techniques. Image analysis and detection, control, speech recognition, translation are only a few prominent examples tackled successfully by neural networks. Thereby, the discipline imposes a completely new problem solving paradigm requiring a rethinking of classical software development methods. The high demand for deep learning technology has led to a large amount of competing frameworks mostly having a Python interface - a quasi standard in the community. Although, existing tools often provide great ﬂexibility and high performance, they still lack to deliver a completely domain oriented problem view. Furthermore, using neural networks as reusable building blocks with clear interfaces in productive systems is still a challenge. In this work we propose a domain speciﬁc modeling methodology tackling design, training, and integration of deep neural networks. Thereby, we distinguish between three main modeling concerns: architecture, training, and data. We integrate our methodology in a component-based modeling toolchain allowing one to employ and reuse neural networks in large software architectures.",
        "keywords": [
            "deep learning",
            "neural networks",
            "model-driven software engineering"
        ],
        "authors": [
            "Evgeny Kusmenko",
            "Sebastian Nickels",
            "Svetlana Pavlitskaya",
            "Bernhard Rumpe",
            "Thomas Timmermanns"
        ],
        "file_path": "data/models/models19/Modeling and Training of Neural Processing Systems.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using Models to Enable Compliance Checking against the GDPR: An Experience Report",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "The General Data Protection Regulation (GDPR) harmonizes data privacy laws and regulations across Europe. Through the GDPR, individuals are able to better control their personal data in the face of new technological developments. While the GDPR is highly advantageous to individuals, complying with it poses major challenges for organizations that control or process personal data. Since no automated solution with broad industrial applicability currently exists for GDPR compliance checking, organizations have no choice but to perform costly manual audits to ensure compliance. In this paper, we share our experience building a UML representation of the GDPR as a ﬁrst step towards the development of future automated methods for assessing compliance with the GDPR. Given that a concrete implementation of the GDPR is affected by the national laws of the EU member states, GDPR’s expanding body of case law and other contextual information, we propose a two-tiered representation of the GDPR: a generic tier and a specialized tier. The generic tier captures the concepts and principles of the GDPR that apply to all contexts, whereas the specialized tier describes a speciﬁc tailoring of the generic tier to a given context, including the contextual variations that may impact the interpretation and application of the GDPR. We further present the challenges we faced in our modeling endeavor, the lessons we learned from it, and future directions for research.",
        "keywords": [
            "General Data Protection Regulation",
            "Regulatory Compliance",
            "UML",
            "OCL"
        ],
        "authors": [
            "Damiano Torre",
            "Ghanem Soltana",
            "Mehrdad Sabetzadeh",
            "Lionel C. Briand",
            "Yuri Aufﬁnger",
            "Peter Goes"
        ],
        "file_path": "data/models/models19/Using Models to Enable Compliance Checking Against the GDPR An Experience Report.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Secure Data-Flow Compliance Checks between Models and Code based on Automated Mappings",
        "submission-date": "2019/10",
        "publication-date": "2019/10",
        "abstract": "During the development of security-critical software, the system implementation must capture the security properties postulated by the architectural design. This paper presents an approach to support secure data-ﬂow compliance checks between design models and code. To iteratively guide the developer in discovering such compliance violations we introduce automated mappings. These mappings are created by searching for correspondences between a design-level model (Security Data Flow Diagram) and an implementation-level model (Program Model). We limit the search space by considering name similarities between model elements and code elements as well as by the use of heuristic rules for matching data-ﬂow structures. The main contributions of this paper are three-fold. First, the automated mappings support the designer in an early discovery of implementation absence, convergence, and divergence with respect to the planned software design. Second, the mappings also support the discovery of secure data-ﬂow compliance violations in terms of illegal asset ﬂows in the software implementation. Third, we present our implementation of the approach as a publicly available Eclipse plugin and its evaluation on ﬁve open source Java projects (including Eclipse secure storage).",
        "keywords": [
            "Security-by-design",
            "Security compliance",
            "Data Flow Diagram (DFD)",
            "Model-to-Model Transformation (M2M)"
        ],
        "authors": [
            "Sven Peldszus",
            "Katja Tuma",
            "Daniel Strüber",
            "Jan Jürjens",
            "Riccardo Scandariato"
        ],
        "file_path": "data/models/models19/Secure Data-Flow Compliance Checks between Models and Code Based on Automated Mappings.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Leveraging Model-Driven Technologies for JSON Artefacts: The Shipyard Case Study",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "With JSON’s increasing adoption, the need for structural constraints and validation capabilities led to JSON Schema, a dedicated meta-language to specify languages which are in turn used to validate JSON documents. Currently, the standardisation process of JSON Schema and the implementation of adequate tool support (e.g., validators and editors) are work in progress. However, the periodic issuing of newer JSON Schema drafts makes tool development challenging. Nevertheless, many JSON Schemas as language deﬁnitions exist, but JSON documents are still mostly edited in basic text-based editors. To tackle this challenge, we investigate in this paper how Model-Driven Engineering (MDE) methods for language engineering can help in this area. Instead of re-inventing the wheel of building up particular technologies directly for JSON, we study how the existing MDE infrastructures may be utilized for JSON. In particular, we present a bridge between the JSONware and Modelware technical spaces to exchange languages and documents. Based on this bridge, our approach supports language engineers, domain experts, and tool providers in editing, validating, and generating tool support with enhanced capabilities for JSON schemas and their documents. We evaluate our approach with Shipyard, a JSON Schema-based language for the workﬂow speciﬁcation for Keptn, an open-source tool for DevOps automation of cloud-native applications. The results of the case study show that proper editors and language evolution support from MDE can be reused and, at the same time, the surface syntax of JSON is maintained.",
        "keywords": [
            "JSON",
            "JSON Schema",
            "MDE",
            "DevOps",
            "Tool Interoperability"
        ],
        "authors": [
            "Alessandro Colantoni",
            "Antonio Garmendia",
            "Luca Berardinelli",
            "Manuel Wimmer",
            "Johannes Br¨auer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a250/349500a250.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Reinforcement Learning for In-Place Model Transformations",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Model-driven optimization has gained much interest in the last years which resulted in several dedicated extensions for in-place model transformation engines. The main idea is to exploit domain-speciﬁc languages to deﬁne models which are optimized by applying a set of model transformation rules. Objectives are guiding the optimization processes which are currently mostly realized by meta-heuristic searchers such as different kinds of Genetic Algorithms. However, meta-heuristic search approaches are currently challenged by reinforcement learning approaches for solving optimization problems.\n\nIn this new ideas paper, we apply for the ﬁrst time reinforce-ment learning for in-place model transformations. In particular, we extend an existing model-driven optimization approach with reinforcement learning techniques. We experiment with value-based and policy-based techniques. We investigate several case studies for validating the feasibility of using reinforcement learning for model-driven optimization and compare the perfor-mance against existing approaches. The initial evaluation shows promising results but also helped in identifying future research lines for the whole model transformation community.",
        "keywords": [
            "Model Transformations",
            "Reinforcement-Learning",
            "Model-based Optimization"
        ],
        "authors": [
            "Martin Eisenberg",
            "Hans-Peter Pichler",
            "Antonio Garmendia",
            "Manuel Wimmer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a082/349500a082.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Concept for a Qualiﬁable (Meta)-Modeling Framework Deployable in Systems and Tools of Safety-critical and Cyber-physical Environments",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "The development of cyber-physical systems can significantly benefit from domain-specific modeling and requires adequate (meta)-modeling frameworks. If such systems are designed for the safety-critical area, the systems must undergo qualiﬁcation processes deﬁned and monitored by a certiﬁcation authority. To use the resulting artifacts of modeling tools without further qualiﬁcation activities, the modeling tool must be additionally qualiﬁed. Tool qualiﬁcation has to be conducted by the tool user and can be assisted by the tool developer by providing qualiﬁcation artifacts. However, state-of-the-art domain-specific modeling frameworks barely support the user in the qualiﬁcation process, which results in an extensive manual effort. To reduce this effort and to avoid modeling constructs that can hardly be implemented in a qualiﬁable way, we propose the development of an open source (meta)-modeling framework that inherently considers qualiﬁcation issues. Based on the functionality of existing frameworks, we have identiﬁed components that necessarily need to be rethought or changed. This leads to the consideration of the following six cornerstones for our framework: (1) an essential meta-language, (2) a minimal runtime, (3) deterministic transformations, (4) a qualiﬁcation artifact generation, (5) a sophisticated visualization, and (6) a decoupled interaction of framework components. All these cornerstones consider the aspect of a safety-critical (meta)-modeling framework in their own manner. This combination leads to a holistic framework usable in the safety-critical system development domain.",
        "keywords": [],
        "authors": [
            "Vanessa Tietz",
            "Julian Schoepf",
            "Andreas Waldvogel",
            "Bjoern Annighoefer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a163/349500a163.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Zhenjiang Hu",
            "Tomoji Kishi",
            "Naoyasu Ubayashi",
            "Daniel Varro",
            "Shiva Nejati",
            "Xiaoxing Ma",
            "Huseyin Ergin",
            "Shaukat Ali"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z012/349500z012.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Execution Trace Analysis for a Precise Understanding of Latency Violations",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Despite the amount of proposed works for the veriﬁcation of diverse model properties, under-standing the root cause of latency requirements viola-tion in execution traces is still an open-issue especially for complex HW/SW system-level designs: is it due to an unfavorable real-time scheduling, to contentions on buses, to the characteristics of functional algorithms or hardware components? This identiﬁcation is particu-larly at stake when adding new features in a model, e.g., a new security countermeasure. The paper introduces PLAN, a new trace analysis technique whose objective is to classify execution transactions according to their impact on latency. To do so, we rely ﬁrst on a model transformation that builds up a dependency graph from an allocation model, thus including hardware and software aspects of a system model. Then, from this graph and an execution trace, our analysis can highlight how software or hardware elements contributed to the latency violation. The paper ﬁrst formalizes the problem before applying our approach to simulation traces of SysML models. A case study deﬁned in the AQUAS European project illustrates the interest of our approach.",
        "keywords": [
            "Embedded Systems",
            "Execution Trace Analysis",
            "Dependency Graph",
            "MBSE",
            "Timing analysis",
            "Simulation"
        ],
        "authors": [
            "Maysam Zoor",
            "Ludovic Apvrille",
            "Renaud Pacalet"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a123/349500a123.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Shaukat Ali",
            "Mira Balaban",
            "Olivier Barais",
            "Thorsten Berger",
            "Gábor Bergmann",
            "Marco Brambilla",
            "Loli Burgueño",
            "Michel Chaudron",
            "Juergen Dingel",
            "Jeff Gray",
            "Esther Guerra",
            "Xiao He",
            "Marianne Huchard",
            "Fuyuki Ishikawa",
            "Gabor Karsai",
            "Dimitris Kolovos",
            "Sébastien Mosser",
            "Ileana Ober",
            "Alfonso Pierantonio",
            "Gianna Reggio",
            "Riccardo Scandariato",
            "Andy Schürr",
            "Jocelyn Simmonds",
            "Daniel Strüber",
            "Matthias Tichy",
            "Massimo Tisi",
            "Antonio Vallecillo",
            "Andreas Wortmann",
            "Andrzej Wąsowski",
            "Tao Yue",
            "Steffen Zschaler"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z021/349500z021.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying Declarative Analysis to Software Product Line Models: An Industrial Study",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Software Product Lines (SPLs) are families of related software products developed from a common set of artifacts. Most existing analysis tools can be applied to a single product at a time, but not to an entire SPL. Some tools have been redesigned/re-implemented to support the kind of variability exhibited in SPLs, but this usually takes a lot of effort, and is error-prone. Declarative analyses written in languages like Datalog have been collectively lifted to SPLs in prior work [1], which makes the process of applying an existing declarative analysis to a product line more straightforward.\nIn this paper, we take an existing declarative analysis (behaviour alteration) and apply it to a set of automotive software product lines from General Motors. We discuss the design of the analysis pipeline used in this process, present its scalability results, and provide a means to visualize the analysis results for a subset of products ﬁltered by feature expression. We also reﬂect on some of the lessons learned throughout this project.",
        "keywords": [],
        "authors": [
            "Ramy Shahin",
            "Robert Hackman",
            "Rafael Toledo",
            "Ramesh S.",
            "Joanne M. Atlee",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a145/349500a145.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Simulation-Based Analysis for Multi-Robot Systems",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Multi-robot systems are increasingly deployed to provide services and accomplish missions whose complexity or cost is too high for a single robot to achieve on its own. Although multi-robot systems offer increased reliability via redundancy and enable the execution of more challenging missions, engineering these systems is very complex. This complexity affects not only the architecture modelling of the robotic team but also the modelling and analysis of the collaborative intelligence enabling the team to complete its mission. Existing approaches for the development of multi-robot applications do not provide a systematic mechanism for capturing these aspects and assessing the robustness of multi-robot systems. We address this gap by introducing ATLAS, a novel model-driven approach supporting the systematic robustness analysis of multi-robot systems in simulation. The ATLAS domain-speciﬁc language enables modelling the architecture of the robotic team and its mission, and facilitates the speciﬁcation of the team’s intelligence. We evaluate ATLAS and demonstrate its effectiveness on two oceanic exploration missions performed by a team of unmanned underwater vehicles developed using the MOOS-IvP robotic simulator.",
        "keywords": [
            "model-driven engineering",
            "robotics",
            "simulation"
        ],
        "authors": [
            "James Harbin",
            "Simos Gerasimou",
            "Nicholas Matragkas",
            "Athanasios Zolotas and Radu Calinescu"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a331/349500a331.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Monte Carlo Tree Search and\nGR(1) Synthesis for Robot Tasks Planning in\nAutomotive Production Lines",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "In automotive production cells, complex processes\ninvolving multiple robots must be optimized for cycle time.\nWe investigated using symbolic GR(1) controller synthesis for\nautomating multi-robot task planning. Given a speciﬁcation of\nthe order of tasks and states to avoid, often multiple valid\nstrategies can be computed; in many states there are multiple\nchoices to satisfy the speciﬁcation, such as choosing different\nrobots to perform a certain task. To determine the best choices\nunder the consideration of movement times and probabilities\nthat robots may be interrupted for repairs or corrections, we\ncombine the execution of the synthesized controller with Monte\nCarlo Tree Search (MCTS), a heuristic AI-planning technique.\nThe result is a model-at-run-time approach that we present by\nthe example of a multi-robot spot welding cell. We report on\nexperiments showing that the approach (1) can reduce cycle\ntimes by choosing time-efﬁcient movement sequences and (2)\ncan choose executions that react efﬁciently to interruptions by\nchoosing to delay tasks that, if an interruption of one robot\nshould occur later, can be reallocated to another robot. Most\ninterestingly, we found, however, that (3) in some cases there is a\nconﬂict between time-efﬁcient movement sequences and ones that\nmay react efﬁciently to probable future interruptions—and when\ninterruption probabilities are low, increasing the time allocated\nfor MCTS, i.e., increasing the number of sample simulations\nmade by MCTS, does not improve cycle time.",
        "keywords": [
            "Robot tasks planning",
            "Reactive systems",
            "Monte\nCarlo Tree Search"
        ],
        "authors": [
            "Eric Wete",
            "Joel Greenyer",
            "Andreas Wortmann",
            "Oliver Flegel",
            "Martin Klein"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a320/349500a320.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Silvia Abrahão",
            "Richard Paige",
            "Don Batory",
            "Benoit Baudry",
            "Nelly Bencomo",
            "Ruth Breu",
            "Jordi Cabot",
            "Silvia Ceballos"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z016/349500z016.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MoDALAS: Model-Driven Assurance for Learning-Enabled Autonomous Systems",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Increasingly, safety-critical systems include artificial intelligence and machine learning components (i.e., Learning-Enabled Components (LECs)). However, when behavior is learned in a training environment that fails to fully capture real-world phenomena, the response of an LEC to untrained phenomena is uncertain, and therefore cannot be assured as safe. Automated methods are needed for self-assessment and adaptation to decide when learned behavior can be trusted. This work introduces a model-driven approach to manage self-adaptation of a Learning-Enabled System (LES) to account for run-time contexts for which the learned behavior of LECs cannot be trusted. The resulting framework enables an LES to monitor and evaluate goal models at run time to determine whether or not LECs can be expected to meet functional objectives. Using this framework enables stakeholders to have more confidence that LECs are used only in contexts comparable to those validated at design time.",
        "keywords": [
            "goal-based modeling",
            "self-adaptive systems",
            "artificial intelligence",
            "machine learning",
            "models at run time",
            "cyber physical systems",
            "behavior oracles",
            "autonomous vehicles"
        ],
        "authors": [
            "Michael Austin Langford",
            "Kenneth H. Chan",
            "Jonathon Emil Fleck",
            "Philip K. McKinley",
            "Betty H.C. Cheng"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a182/349500a182.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Ali, Shaukat",
            "Amundson, Isaac",
            "Annighoefer, Bjoern",
            "Apvrille, Ludovic",
            "Aslam, Kousar",
            "Atlee, Joanne M.",
            "Babaei, Majid",
            "Babar, Junaid",
            "Belharbi, Khalid",
            "Bennett, Michael",
            "Berardinelli, Luca",
            "Bhatambrekar, Sachin",
            "Bittner, Paul Maximilian",
            "Boletsis, Costas",
            "Borum, Holger Stadel",
            "Bräeuer, Johannes",
            "Brown, Caroline",
            "Calinescu, Radu",
            "Carlson, Jan",
            "Chan, Kenneth H.",
            "Chechik, Marsha",
            "Cheng, Betty H.C.",
            "Chouki, Tibermacine",
            "Cicchetti, Antonio",
            "Ciccozzi, Federico",
            "Cofer, Darren",
            "Colantoni, Alessandro",
            "Combemale, Benoît",
            "Cooper, Justin",
            "Costa Seco, João",
            "Damasceno, Carlos Diego Nascimento",
            "David, Istvan",
            "De La Vega, Alfonso",
            "Dingel, Juergen",
            "Di Rocco, Juri",
            "Di Ruscio, Davide",
            "Di Sandro, Alessio",
            "Di Sipio, Claudio",
            "Dorofeev, Kirill",
            "Dumitrescu, Roman",
            "Ege, Florian",
            "Eisenberg, Martin",
            "Elyes, Cherfa",
            "Engels, Gregor",
            "Faridmoayer, Sogol",
            "Ferreira, Carla",
            "Fischbach, Jannik",
            "Fleck, Jonathon Emil",
            "Flegel, Oliver",
            "Galasso, Jessie",
            "Garcia-Ceja, Enrique",
            "Garmendia, Antonio",
            "Gerasimou, Simos",
            "Gorissen, Simon",
            "Greenyer, Joel",
            "Gross-Amblard, David",
            "Grunske, Lars",
            "Hackman, Robert",
            "Halvorsrud, Ragnhild",
            "Harbin, James",
            "Hardin, David",
            "Hernández López, José Antonio",
            "Hoyos Rodriguez, Horacio",
            "Jaskolka, Monika",
            "Jézéquel, Jean-Marc",
            "Jongeling, Robbert",
            "Kehrer, Timo",
            "Klein, Martin",
            "Kolovos, Dimitris",
            "Lago, Patricia",
            "Langford, Michael Austin",
            "Lawford, Mark",
            "Lofberg, Anders",
            "Lourenço, Hugo"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a343/349500a343.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A GNN-based Recommender System to Assist the Speciﬁcation of Metamodels and Models",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Nowadays, while modeling environments provide users with facilities to specify different kinds of artifacts, e.g., metamodels, models, and transformations, the possibility of learning from previous modeling experiences and being assisted during modeling tasks remains largely unexplored. In this paper, we propose MORGAN, a recommender system based on a graph neural network (GNN) to assist modelers in performing the speciﬁcation of metamodels and models. The (meta)model being speciﬁed, and the training data are encoded in a graph-based format by exploiting natural language processing (NLP) techniques. Afterward, a graph kernel function uses the extracted graphs to provide modelers with relevant recommendations to complete the partially speciﬁed (meta)models. We evaluated MORGAN on real-world datasets using various quality metrics, i.e., precision, recall, and F-measure. The experimental results are encouraging and demonstrate the feasibility of our tool to support modelers while specifying metamodels and models.",
        "keywords": [],
        "authors": [
            "Juri Di Rocco",
            "Claudio Di Sipio",
            "Davide Di Ruscio",
            "Phuong T. Nguyen"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a070/349500a070.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z023/349500z023.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Identifying manual changes to generated code: Experiences from the industrial automation domain",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "In this paper, we report on a case study in an industrial setting where code is generated from models, and, for various reasons, that generated code is then manually modified. To enhance the maintainability of both models and code, consistency between them is imperative. A first step towards establishing that consistency is to identify the manual changes that were made to the code after it was generated and deployed. Identifying the delta is not straightforward and requires pre-processing of the artifacts. The main mechanics driving our solution are higher-order transformations, which make the implementation scalable and robust to small changes in the modeling language. We describe the specific industrial setting of the problem, as well as the experiences and lessons learned from developing, implementing, and validating our solution together with our industrial partner.",
        "keywords": [
            "Model-based development",
            "round-trip engineering",
            "higher-order transformations",
            "domain-specific modeling languages",
            "industrial case study"
        ],
        "authors": [
            "Robbert Jongeling",
            "Sachin Bhatambrekar",
            "Anders Lofberg",
            "Antonio Cicchetti",
            "Federico Ciccozzi",
            "Jan Carlson"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a035/349500a035.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Based Development of Engine Control Systems: Experiences and Lessons Learnt",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Rolls-Royce Control Systems supplies engine control and monitoring systems for aviation applications, and is required to design, certify, and deliver these to the highest level of safety assurance. To allow Rolls-Royce to develop safe and robust systems, which continue to increase in complexity, model-based techniques are now a critical part of the software development process. In this paper, we discuss the experiences, challenges and lessons learnt when developing a bespoke domain-speciﬁc modelling workbench based on open-source modelling technologies including the Eclipse Modelling Framework (EMF), Xtext, Sirius and Epsilon. This modelling workbench will be used to architect and integrate the software for all future Rolls-Royce engine control and monitoring systems.",
        "keywords": [
            "Domain Speciﬁc Language",
            "Component Oriented Architecture",
            "Graphical Modelling Workbench",
            "Xtext",
            "Sirius",
            "EMF"
        ],
        "authors": [
            "Justin Cooper",
            "Alfonso de la Vega",
            "Richard Paige",
            "Dimitris Kolovos",
            "Michael Bennett",
            "Caroline Brown",
            "Beatriz Sanchez Pi˜na",
            "Horacio Hoyos Rodriguez"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a308/349500a308.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z004/349500z004.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Collaborative Model-Driven Software Engineering: A Systematic Update",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Current software engineering practices rely on highly heterogeneous and distributed teams working in a collaborative setting. Between 2013–2020, the publication output in the ﬁeld of collaborative Model-Driven Software Engineering (MDSE) has signiﬁcantly increased. However, the only systematic mapping study available is limited to studies published until 2015. In this paper, we provide an update on that study for the complementing 2016–2020 period, and report the latest results, challenges, and trends. Our analysis led to selecting 29 clusters of 54 new peer-reviewed publications on collaborative MDSE. Based on the novel developments in the ﬁeld, we have extended and improved the original classiﬁcation framework, making it applicable to recent and future research contributions on collaborative MDSE. The insights in this paper relate to the changing trends in the ﬁeld and present new relevant information.",
        "keywords": [
            "Model-driven engineering",
            "collaborative modeling",
            "systematic mapping study",
            "systematic update"
        ],
        "authors": [
            "Istvan David",
            "Kousar Aslam",
            "Sogol Faridmoayer",
            "Ivano Malavolta",
            "Eugene Syriani",
            "Patricia Lago"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a273/349500a273.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "24th International Conference on Model-Driven Engineering Languages and Systems",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z001/349500z001.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Patch Generation for Fixing Semantic Errors in ATL Transformation Rules",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "With the growing popularity of the MDE paradigm, model transformations are becoming more and more complex. ATL transformations, in particular, are error-prone due to the declarative nature of the language and the dependency towards the involved metamodels. To alleviate the burden of developers, we propose, in this paper, an approach for ﬁxing semantic errors in ATL transformation rules without predeﬁned patch templates for speciﬁc error types. In a ﬁrst step, our approach determines the rules that are likely to contain errors starting from the discrepancy between the expected and produced outputs of test cases. Then, a second step allows to generate candidate patches for these errors using a multiobjective optimization algorithm, guided by the same test cases. In a preliminary evaluation, we show that our approach can ﬁx most of the errors for transformations with one or two errors. For those with multiple errors, more iterations are necessary to ﬁx some of the errors.",
        "keywords": [
            "Model transformation",
            "Program repair",
            "Multi-objective optimization"
        ],
        "authors": [
            "Zahra VaraminyBahnemiry",
            "Jessie Galasso",
            "Khalid Belharbi",
            "Houari Sahraoui"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a013/349500a013.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Silvia Abrahão",
            "Joanne M. Atlee",
            "Nelly Bencomo",
            "Jordi Cabot",
            "Betty H.C. Cheng",
            "Benoit Combemale",
            "Davide Di Ruscio",
            "Gregor Engels",
            "Martin Gogolla",
            "Jörg Kienzle",
            "Ana Moreira",
            "Richard Paige",
            "Eugene Syriani",
            "Gabriele Taentzer",
            "Manuel Wimmer",
            "Juan de Lara"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z020/349500z020.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "DataTime: a Framework to smoothly Integrate Past, Present and Future into Models",
        "submission-date": "2021/09",
        "publication-date": "2021/09",
        "abstract": "Models at runtime have been initially investigated for adaptive systems. Models are used as a reﬂective layer of the current state of the system to support the implementation of a feedback loop. More recently, models at runtime have also been identiﬁed as key for supporting the development of full-ﬂedged digital twins. However, this use of models at runtime raises new challenges, such as the ability to seamlessly interact with the past, present and future states of the system. In this paper, we propose a framework called DataTime to implement models at runtime which capture the state of the system according to the dimensions of both time and space, here modeled as a directed graph where both nodes and edges bear local states (ie. values of properties of interest). DataTime provides a unifying interface to query the past, present and future (predicted) states of the system. This unifying interface provides i) an optimized structure of the time series that capture the past states of the system, possibly evolving over time, ii) the ability to get the last available value provided by the system’s sensors, and iii) a continuous micro-learning over graph edges of a predictive model to make it possible to query future states, either locally or more globally, thanks to a composition law. The framework has been developed and evaluated in the context of the Intelligent Public Transportation Systems of the city of Rennes (France). This experimentation has demonstrated how DataTime can deprecate the use of heterogeneous tools for managing data from the past, the present and the future, and facilitate the development of digital twins.",
        "keywords": [],
        "authors": [
            "Gauthier LYAN",
            "Jean-Marc J´EZ´EQUEL",
            "David GROSS-AMBLARD",
            "Benoˆıt COMBEMALE"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a134/349500a134.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "24th International Conference on Model-Driven Engineering Languages and Systems",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Zhenjiang Hu",
            "Tomoji Kishi",
            "Naoyasu Ubayashi",
            "Daniel Varro",
            "Shiva Nejati"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z003/349500z003.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Designing a Modeling Language for Customer Journeys: Lessons Learned from User Involvement",
        "submission-date": "2021/09",
        "publication-date": "2021/09",
        "abstract": "Although numerous methods have been formalized for handling the technical aspects of developing domain-speciﬁc modeling languages (DSMLs), user needs and usability aspects are often addressed in ad hoc manners and late in the development process. Working in this context, this paper presents the development of the customer journey modeling language (CJML), a DSML for modeling service processes from the end-user’s perspective. CJML targets a wide and heterogeneous group of users, making it especially challenging regarding usability. This paper describes how an industry-relevant DSML was systematically improved by using a variety of user-centered design techniques in close collaboration with the target group and how their feedback was used to reﬁne and evolve the syntax and semantics of CJML. We also suggest how a service-providing organization may beneﬁt from adopting CJML as a unifying language for documentation purposes, compliance analysis, and service innovation. Finally, we generalize the experience gained into lessons learned and methodological guidelines.",
        "keywords": [
            "DSML",
            "customer journey",
            "user involvement",
            "user-centered design"
        ],
        "authors": [
            "Ragnhild Halvorsrud",
            "Costas Boletsis",
            "Enrique Garcia-Ceja"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a239/349500a239.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Collaborative Software Modeling in Virtual Reality",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Modeling is a key activity in conceptual design and system design. Through collaborative modeling, end-users, stakeholders, experts, and entrepreneurs are able to create a shared understanding of a system representation. While the Uni-fied Modeling Language (UML) is one of the major conceptual modeling languages in object-oriented software engineering, more and more concerns arise from the modeling quality of UML and its tool-support. Among them, the limitation of the two-dimensional presentation of its notations and lack of natural collaborative modeling tools are reported to be signiﬁcant. In this paper, we explore the potential of using Virtual Reality (VR) technology for collaborative UML software design by comparing it with classical collaborative software design using conventional devices (Desktop PC / Laptop). For this purpose, we have developed a VR modeling environment that offers a natural collaborative modeling experience for UML Class Diagrams. Based on a user study with 24 participants, we have compared collaborative VR modeling with conventional modeling with regard to efﬁciency, effectiveness, and user satisfaction. Results show that the use of VR has some disadvantages concerning efﬁciency and effectiveness, but the user’s fun, the feeling of being in the same room with a remote collaborator, and the naturalness of collaboration were increased.",
        "keywords": [
            "Collaborative Modeling",
            "Virtual Reality",
            "UML"
        ],
        "authors": [
            "Enes Yigitbas",
            "Simon Gorissen",
            "Nils Weidmann",
            "Gregor Engels"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a261/349500a261.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards the Characterization of Realistic Model Generators using Graph Neural Networks",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "The automatic generation of software models is an important element in many software and systems engineering scenarios such as software tool certiﬁcation, validation of cyber-physical systems, or benchmarking graph databases. Several model generators are nowadays available, but the topic of whether they generate realistic models has been little studied. The state-of-the-art approach to check the realistic property in software models is to rely on simple comparisons using graph metrics and statistics. This generates a bottleneck due to the compression of all the information contained in the model into a small set of metrics. Furthermore, there is a lack of interpretation in these approaches since there are no hints of why the generated models are not realistic. Therefore, in this paper, we tackle the problem of assessing how realistic a generator is by mapping it to a classiﬁcation problem in which a Graph Neural Network (GNN) will be trained to distinguish between the two sets of models (real and synthetic ones). Then, to assess how realistic a generator is we perform the Classiﬁer Two-Sample Test (C2ST). Our approach allows for interpretation of the results by inspecting the attention layer of the GNN. We use our approach to assess four state-of-the-art model generators applied to three different domains. The results show that none of the generators can be considered realistic.",
        "keywords": [
            "Model generators",
            "Realistic models",
            "Graph neural networks",
            "Two-Sample Test"
        ],
        "authors": [
            "Jos´e Antonio Hern´andez L´opez",
            "Jes´us S´anchez Cuadrado"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a058/349500a058.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Designing Applied DSLs for Non-programming Experts in Evolving Domains",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Domain-speciﬁc languages (DSLs) have emerged as a plausible way for non-programming experts to efﬁciently express their domain knowledge. Recent DSL research has taken a technical perspective on how and why to create DSLs, resulting in a wealth of innovative tools, frameworks and technical approaches. Less attention has been paid to the design process. Namely, how can it ensure that the created DSL realises the expected beneﬁts? This paper seeks to answer this question when designing DSLs for highly specialised domains subject to resource constraints, an evolving application domain, and scarce user participation. We propose an iteration of alternating activities in a human-centred design method that seeks to minimise the need for expensive implementation and user involvement. The method moves from a low-validity exploration of highly diverse language designs towards a higher-validity exploration of more homogeneous designs. We give an in-depth case study of designing an actuarial DSL called MAL, or Management Action Language, which allows actuaries to model so-called future management actions in asset/liability projections in life insurance and pension companies. The proposed human-centred design method was synthesised from this case study, where we found it useful for iteratively identifying and removing usability problems during the design.",
        "keywords": [
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Human-centred design"
        ],
        "authors": [
            "Holger Stadel Borum",
            "Henning Niss",
            "Peter Sestoft"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a227/349500a227.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Repository Mining for Changes in Simulink Models",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Model-Based Development (MBD) is widely used for\nembedded controls development, with MATLAB/Simulink being\none of the most used environments in the automotive industry.\nSimulink models are the primary design artifact and as with all\nsoftware, must be constantly maintained and evolved over their\nlifetime. It is necessary to develop models that support likely\nchanges in order to assist with evolution/maintenance processes.\nIn order to do so, the types of frequently performed changes\nmust be understood and appropriate language mechanisms must\nbe available to support these changes. However, Simulink model\nchanges are currently not well understood. We analyze a real\nindustrial software repository of our industrial partner and its\nversion control system to provide insights into the likely changes\nfor Simulink. The intent with this analysis includes providing\nguidance on how Simulink is used in industrial practice and\nhow particular model changes can impact system evolution.",
        "keywords": [
            "Simulink",
            "model-based development",
            "model change",
            "repository mining",
            "software evolution",
            "version control system"
        ],
        "authors": [
            "Monika Jaskolka",
            "Vera Pantelic",
            "Alan Wassyng",
            "Mark Lawford",
            "Richard Paige"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a046/349500a046.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Lean Approach to Building Valid Model-Based Safety Arguments",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "In recent decades, cyber-physical systems developed using Model-Driven Engineering (MDE) techniques have become ubiquitous in safety-critical domains. Safety assurance cases (ACs) are structured arguments designed to comprehensively show that such systems are safe; however, the reasoning steps, or strategies, used in AC arguments are often informal and difficult to rigorously evaluate. Consequently, AC arguments are prone to fallacies, and unsafe systems have been deployed as a result of fallacious ACs. To mitigate this problem, prior work [32] created a set of provably valid AC strategy templates to guide developers in building rigorous ACs. Yet instantiations of these templates remain error-prone and still need to be reviewed manually. In this paper, we report on using the interactive theorem prover Lean to bridge the gap between safety arguments and rigorous model-based reasoning. We generate formal, model-based machine-checked AC arguments, taking advantage of the traceability between model and safety artifacts, and mitigating errors that could arise from manual argument assessment. The approach is implemented in an extended version of the MMINT-A model management tool [10]. Implementation includes a conversion of informal claims into formal Lean properties, decomposition into formal sub-properties and generation of correctness proofs. We demonstrate the applicability of the approach on two safety case studies from the literature.",
        "keywords": [],
        "authors": [
            "Torin Viger",
            "Logan Murphy",
            "Alessio Di Sandro",
            "Ramy Shahin",
            "Marsha Chechik"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a194/349500a194.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Restricted Natural Language and Model-based Adaptive Test Generation for Autonomous Driving",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "With the aim to reduce car accidents, autonomous driving attracted a lot of attentions these years. However, recently reported crashes indicate that this goal is far from being achieved. Hence, cost-effective testing of autonomous driving systems (ADSs) has become a prominent research topic. The classical model-based testing (MBT), i.e., generating test cases from test models followed by executing the test cases, is ineffective for testing ADSs, mainly because of the constant exposure to ever-changing operating environments, and uncertain internal behaviors due to employed AI techniques. Thus, MBT must be adaptive to guide test case generation based on test execution results in a step-wise manner. To this end, we propose a natural language and model-based approach, named LiveTCM, to automatically execute and generate test case speciﬁcations (TCSs) by interacting with an ADS under test and its environment. LiveTCM is evaluated with an open-source ADS and two test generation strategies: Deep Q-Network (DQN)-based and Random. Results show that LiveTCM with DQN can generate TCSs with 56 steps on average in 60 seconds, leading to 6.4 test oracle violations and covering 14 APIs per TCS on average.",
        "keywords": [
            "Natural Language and Model-based Testing",
            "Adaptive Test Generation",
            "Autonomous Driving"
        ],
        "authors": [
            "Yize Shi",
            "Chengjie Lu",
            "Man Zhang",
            "Huihui Zhang",
            "Tao Yue",
            "Shaukat Ali"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a101/349500a101.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OSTRICH - A Type-safe Template Language for Low-code Development",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Low-code platforms aim at allowing non-experts to develop complex systems and knowledgeable developers to improve their productivity in orders of magnitude. The greater gain comes from (re)using components developed by experts capturing common patterns across all layers of the application, from the user interface to the data layer and integration with external systems. Often, cloning sample code fragments is the only alternative in such scenarios, requiring extensive adaptation to reach the intended use. Such customization activities require deep knowledge outside of the comfort zone of low-code. To effectively speed up the reuse, composition, and adaptation of pre-deﬁned components, low-code platforms need to provide safe and easy-to-use language mechanisms.\nThis paper introduces OSTRICH, a strongly-typed rich tem-plating language for a low-code platform (OutSystems) that builds on metamodel annotations and allows the correct in-stantiation of templates. We conservatively extend the existing metamodel and ensure that the resulting code is always well-formed. The results we present include a novel type safety veriﬁ-cation of template deﬁnitions, and template arguments, providing model consistency across application layers. We implemented this template language in a prototype of the OutSystems platform and ported nine of the top ten most used sample code fragments, thus improving the reuse of professionally designed components.",
        "keywords": [
            "metamodel templating",
            "typechecking templates",
            "low-code",
            "development productivity",
            "model reuse"
        ],
        "authors": [
            "Hugo Lourenço",
            "Carla Ferreira",
            "João Costa Seco"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a216/349500a216.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "OSTRICH"
        }
    },
    {
        "title": "Scalable N-Way Model Matching using Multi-dimensional Search Trees",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Alexander Schultheiß",
            "Paul Maximilian Bittner",
            "Lars Grunske",
            "Thomas Thüm",
            "and Timo Kehrer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z005/349500z005.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Identifying Metamodel Inaccurate Structures During Metamodel/Constraint Co-Evolution",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Metamodels are subject to evolution over their lifetime. UML metamodel for instance evolved through different versions, ranging from 0.8 to 2.5 minors. These metamodels are sometimes accompanied with constraints deﬁned using OCL (Object Constraint Language). Many works in the literature developed methods for managing and assisting the co-evolution of metamodels and their constraints. These methods enable a developer to update, in an automated (or semi-automated) way, the constraints associated to a metamodel starting from the deltas identiﬁed between versions of this metamodel. In this work we complement this assistance by notifying the developer with potential inaccurate structures in the metamodel that may be introduced during evolution. We introduce in this paper an original evolution assistance method which focuses rather on the problem (notifying metamodel inaccurate structures) than on the solution (generating OCL constraints using patterns of them). The ultimate goal of this assistance is not only to enable the developer to complete existing/updated constraints with new ones, but also to accompany her/him to further check existing constraints and to test whether they still hold. A case study is presented to show the relevance of the method.",
        "keywords": [
            "Model-driven Engineering",
            "Metamodelling",
            "OCL",
            "Co-Evolution"
        ],
        "authors": [
            "Elyes Cherfa",
            "Soraya Mesli-Kesraoui",
            "Chouki Tibermacine",
            "Salah Sadou",
            "R´egis Fleurquin"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a024/349500a024.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Quality Guidelines for Research Artifacts in Model-Driven Engineering",
        "submission-date": "2021/00",
        "publication-date": "2021/00",
        "abstract": "Sharing research artifacts is known to help people to build upon existing knowledge, adopt novel contributions in practice, and increase the chances of papers receiving attention. In Model-Driven Engineering (MDE), openly providing research artifacts plays a key role, even more so as the community targets a broader use of AI techniques, which can only become feasible if large open datasets and conﬁdence measures for their quality are available. However, the current lack of common discipline-speciﬁc guidelines for research data sharing opens the opportunity for misunderstandings about the true potential of research artifacts and subjective expectations regarding artifact quality. To address this issue, we introduce a set of guidelines for artifact sharing specifically tailored to MDE research. To design this guidelines set, we systematically analyzed general-purpose artifact sharing practices of major computer science venues and tailored them to the MDE domain. Subsequently, we conducted an online survey with 90 researchers and practitioners with expertise in MDE. We investigated our participants’ experiences in developing and sharing artifacts in MDE research and the challenges encountered while doing so. We then asked them to prioritize each of our guidelines as essential, desirable, or unnecessary. Finally, we asked them to evaluate our guidelines with respect to clarity, completeness, and relevance. In each of these dimensions, our guidelines were assessed positively by more than 92% of the participants. To foster the reproducibility and reusability of our results, we make the full set of generated artifacts available in an open repository at https://mdeartifacts.github.io/",
        "keywords": [
            "Software artifacts",
            "Open Science",
            "Model-Driven Engineering",
            "Quality Management"
        ],
        "authors": [
            "Carlos Diego Nascimento Damasceno and Daniel Str¨uber"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a285/349500a285.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Scalable N-Way Model Matching Using Multi-Dimensional Search Trees",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Model matching algorithms are used to identify common elements in input models, which is a fundamental pre-condition for many software engineering tasks, such as merging software variants or views. If there are multiple input models, an n-way matching algorithm that simultaneously processes all models typically produces better results than the sequential application of two-way matching algorithms. However, existing algorithms for n-way matching do not scale well, as the computational effort grows fast in the number of models and their size. We propose a scalable n-way model matching algorithm, which uses multi-dimensional search trees for efficiently finding suitable match candidates through range queries. We implemented our generic algorithm named RaQuN (Range Queries on N input models) in Java, and empirically evaluate the matching quality and runtime performance on several datasets of different origin and model type. Compared to the state-of-the-art, our experimental results show a performance improvement by an order of magnitude, while delivering matching results of better quality.",
        "keywords": [
            "Model-driven engineering",
            "n-way model matching",
            "clone-and-own development",
            "software product lines",
            "multi-view integration",
            "variability mining."
        ],
        "authors": [
            "Alexander Schultheiß",
            "Paul Maximilian Bittner",
            "Lars Grunske",
            "Thomas Thüm and Timo Kehrer"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a001/349500a001.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Control Flow Analysis of Declarative Graph Transformations with Symbolic Execution",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "The declarative graph transformation language Henshin transforms instance models represented as graphs by applying a series of basic steps that match and replace structural patterns on parts of models. These simple transformation rules are then combined into control ﬂow constructs similar to those of imperative programming languages to create more complex transformations. However, defects in the structure of control ﬂow or in transformation rules might misschedule the application of operations, resulting in basic steps to be inapplicable or produce incorrect output. Understanding and ﬁxing these bugs is complicated by the fact that pattern matching in rules is non-deterministic. Moreover, some control ﬂow structures employ a nondeterministic choice of alternatives. This makes it challenging for developers to keep track of all the possible execution paths and interactions between them.\nFor conventional programming languages, techniques have been developed to execute a program symbolically. By abstracting over the concrete values of variables in any actual run, generalized knowledge is gained about the possible behavior of the program. This can be useful in understanding problems and fixing bugs. In this paper, we present an approach to symbolically execute graph transformations for a subset of Henshin, using symbolic path constraints based on the cardinalities of graph pattern occurrences in the model.",
        "keywords": [
            "model driven software engineering",
            "declarative graph transformations",
            "control ﬂow analysis",
            "symbolic execution"
        ],
        "authors": [
            "Florian Ege",
            "Matthias Tichy"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a156/349500a156.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Henshin"
        }
    },
    {
        "title": "Efﬁcient Replay-based Regression Testing for Distributed Reactive Systems in the Context of Model-driven Development",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "As software evolves, regression testing techniques are typically used to ensure the new changes are not adversely affecting the existing features. Despite recent advances, regression testing for distributed systems remains challenging and extremely costly. Existing techniques often require running a failing system several time before detecting a regression. As a result, conven-tional approaches that use re-execution without considering the inherent non-determinism of distributed systems, and providing no (or low) control over execution are inadequate in many ways. In this paper, we present MRegTest, a replay-based regression testing framework in the context of model-driven development to facilitate deterministic replay of traces for detecting regressions while offering sufﬁcient control for the purpose of testing over the execution of the changed system. The experimental results show that compared to the traditional approaches that annotate traces with timestamps and variable values MRegTest detects almost all regressions while reducing the size of the trace signiﬁcantly and incurring similar runtime overhead.",
        "keywords": [
            "MDD",
            "Distributed Systems",
            "Regression Testing"
        ],
        "authors": [
            "Majid Babaei",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a089/349500a089.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrated and Iterative Requirements Analysis and Test Speciﬁcation: A Case Study at Kostal",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Currently, practitioners follow a top-down approach in automotive development projects. However, recent studies have shown that this top-down approach is not suitable for the implementation and testing of modern automotive systems. Specifically, practitioners increasingly fail to specify requirements and tests for systems with complex component interactions (e.g., e-mobility systems). In this paper, we address this research gap and propose an integrated and iterative scenario-based technique for the speciﬁcation of requirements and test scenarios. Our idea is to combine both a top-down and a bottom-up integration strategy. For the top-down approach, we use a behavior-driven development (BDD) technique to drive the modeling of high-level system interactions from the user’s perspective. For the bottom-up approach, we discovered that natural language processing (NLP) techniques are suited to make textual speciﬁcations of existing components accessible to our technique. To integrate both directions, we support the joint execution and automated analysis of system-level interactions and component-level behavior. We demonstrate the feasibility of our approach by conducting a case study at Kostal (Tier1 supplier). The case study corroborates, among other things, that our approach supports practitioners in improving requirements and test speciﬁcations for integrated system behavior.",
        "keywords": [
            "Requirements Analysis",
            "Test Speciﬁcation",
            "Natural Language Processing",
            "Scenario-based Requirements Engineering",
            "Model-based Testing",
            "Scenario-based Testing"
        ],
        "authors": [
            "Carsten Wiecher",
            "Jannik Fischbach",
            "Joel Greenyer",
            "Andreas Vogelsang",
            "Carsten Wolff",
            "Roman Dumitrescu"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a112/349500a112.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Exploring Architectural Design Decisions in Industry 4.0: A Literature Review and Taxonomy",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Architectural design decisions, such as service de-\nployment and composition, plant layout synthesis, or production\nplanning, are an indispensable and overarching part of an\nindustrial manufacturing system design. In the fourth industrial\nrevolution (Industry 4.0), frequent production changes trigger\ntheir synthesis, and preferably optimization. Yet, knowledge\non architecture synthesis and optimization has been scattered\naround other domains, such as generic software engineering.\nWe take a step towards synthesizing current knowledge on\narchitectural design decisions in Industry 4.0. We developed a\ntaxonomy describing architectural models, design decisions, and\noptimization possibilities. The developed taxonomy serves as a\nguideline for comparing different possibilities (e.g., application\nof different optimization algorithms) and selecting appropriate\nones for a given context. Furthermore, we reviewed and mapped\n30 relevant research works to the taxonomy, identifying research\ntrends and gaps. We discuss interesting, and yet uncovered topics\nthat emerged from our review.",
        "keywords": [
            "architecture synthesis",
            "optimization",
            "taxonomy",
            "design space exploration",
            "model-based development",
            "Industry 4.0"
        ],
        "authors": [
            "Tarik Terzimehi´c",
            "Kirill Dorofeev",
            "Sebastian Voss"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a170/349500a170.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synthesizing Veriﬁed Components for Cyber Assured Systems Engineering",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Cyber-physical systems, such as avionics, must be tolerant to cyber-attacks in the same way they are tolerant to random faults: they either gracefully recover or safely shut down as requirements dictate. The DARPA Cyber Assured Systems Engineering program is developing tools for design, analysis, and veriﬁcation that enable systems engineers to design-in cyber-resiliency in a Model-Based Systems Engineering environment. This paper describes automated model transformations that introduce high-assurance cyber-resiliency components into a system, in particular ﬁlters and monitors that prevent malicious input and detect supply chain attacks, respectively. A formal speciﬁcation deﬁnes each high-assurance component, and is used to verify that the component addresses system level cyber requirements. Implementations for these high-assurance components are directly synthesized from their speciﬁcations, and are automatically proven to preserve the exact meaning of the speciﬁcations all the way down to the binary code level. The model transformations are integrated into the Open Source AADL Tool Environment (OSATE). The paper further reports on a case study applying security-enhancing model transformations to a UAV system that uses the Air Force Research Laboratory’s OpenUxAS services for route planning. In the case study, the model transformations add ﬁlters to guard against malformed input, as well as monitors to guard against ground station spooﬁng and malicious ﬂight plans from OpenUxAS.",
        "keywords": [],
        "authors": [
            "Eric Mercer",
            "Konrad Slind",
            "Isaac Amundson",
            "Darren Cofer",
            "Junaid Babar",
            "David Hardin"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a205/349500a205.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assessing the Usefulness of a Visual Programming IDE for Large-Scale Automation Software",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Industrial control applications are usually designed by domain experts instead of software engineers. These experts frequently use visual programming languages based on standards such as IEC 61131-3 and IEC 61499. The standards apply model-based engineering concepts to abstract from hardware and low-level communication. Developing industrial control software is challenging due the fact that such systems are usually one-of-a-kind systems that have to be maintained for many years. These challenges, together with the growing complexity of control software, require very usable model-based development environments for visual programming languages. However, so far only little empirical research exists on the practical usefulness of such environments, i.e., their usability and utility. In this paper, we discuss common control software maintenance tasks and tool capabilities based on existing research and show the realization of these capabilities in 4diac IDE. We ﬁrst performed a walkthrough of the demonstrated capabilities using the cognitive dimensions of notations framework from the ﬁeld of human-computer interaction. We then improved the tool and conducted a user study involving ten industrial automation engineers, who used 4diac IDE in a realistic control software maintenance scenario. Our ﬁndings demonstrate how the usefulness of IDEs can be successfully investigated using a multi-phase approach that includes a walkthrough and a user study. We discuss lessons learned and derive general implications with respect to large-scale applications for developers of IDEs that we deem applicable in the context of (visual) model-based engineering tools.",
        "keywords": [
            "Usefulness study",
            "Open source software",
            "IEC 61499",
            "Modeling tools",
            "Model-driven engineering"
        ],
        "authors": [
            "Bianca Wiesmayr",
            "Alois Zoitl",
            "Rick Rabiser"
        ],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500a297/349500a297.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Preface to the 24th International ACM/IEEE Conference on Model Driven Engineering Languages and Systems (MoDELS)",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "This paper is a preface to the 24th International ACM/IEEE Conference on Model Driven Engineering Languages and Systems (MoDELS). It describes the conference, its history, the challenges faced due to the COVID-19 pandemic, and the organization of the conference, including the review process and acceptance rates for the Foundations and Practice & Innovation tracks.",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models21/MODELS2021-vcEQLsxHBZ4L3NxxjGCoP/349500z010/349500z010.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MoDLF – A Model-Driven Deep Learning Framework for Autonomous Vehicle Perception (AVP)",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Modern vehicles are extremely complex embedded systems that integrate software and hardware from a large set of contributors. Modeling standards like EAST-ADL have shown promising results to reduce complexity and expedite system development. However, such standards are unable to cope with the growing demands of the automotive industry. A typical example of this phenomenon is autonomous vehicle perception (AVP) where deep learning architectures (DLA) are required for computer vision (CV) tasks like real-time object recognition and detection. However, existing modeling standards in the automotive industry are unable to manage such CV tasks at a higher abstraction level. Consequently, system development is currently accomplished through modeling approaches like EAST-ADL while DLA-based CV features for AVP are implemented in isolation at a lower abstraction level. This significantly compromises productivity due to integration challenges. In this article, we introduce MoDLF - A Model-Driven Deep learning Framework to design deep convolutional neural network (DCNN) architectures for AVP tasks. Particularly, Model Driven Architecture (MDA) is leveraged to propose a metamodel along with a conformant graphical modeling workbench to model DCNNs for CV tasks in AVP at a higher abstraction level. Furthermore, Model-To-Text (M2T) transformations are provided to generate executable code for MATLAB® and Python. The framework is validated via two case studies on benchmark datasets for key AVP tasks. The results prove that MoDLF effectively enables model-driven architectural exploration of deep convnets for AVP system development while supporting integration with renowned existing standards like EAST-ADL.",
        "keywords": [
            "Model-Driven Architecture",
            "Model transformation",
            "Low code",
            "Autonomous vehicles perception",
            "Deep learning",
            "Computer vision"
        ],
        "authors": [
            "Aon Safdar",
            "Farooque Azam",
            "Muhammad Waseem Anwar",
            "Usman Akram and Yawar Rasheed"
        ],
        "file_path": "data/models/models22/main/papers/p187-safdar.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incremental Causal Connection for Self-Adaptive Systems Based on Relational Reference Attribute Grammars",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Even though model-driven engineering reduces complexity during the development of self-adaptive systems and models@run.time enables using them during runtime, connecting models to different external systems still involves manual work. Those connections are essential to the complete system, as they enable external systems to react to changes in the internal model and vice versa. In our case, the model is based on Relational Reference Attribute Grammars, an extension of Attribute Grammars to enable conceptual models at runtime while retaining their benefits of modular specification and an incremental evaluation scheme. We present an approach to enable concise specification of the causal connection and needed transformations to match required formats or semantics. To show its applicability, a case study showing the coordination of multiple industrial robot arms using models is presented. We show that using our approach, connections can be specified more concisely while maintaining the same efficiency as hand-written code. The artefact comprising all source code and an executable version of the case studies is available at https://doi.org/10.5281/zenodo.7009758.",
        "keywords": [
            "Reference Attribute Grammar",
            "Cyber-physical System",
            "Causal Connection",
            "Models@run.time",
            "Model-Driven Software Engineering"
        ],
        "authors": [
            "René Schöne",
            "Johannes Mey",
            "Sebastian Ebert",
            "Sebastian Götz",
            "Uwe Aßmann"
        ],
        "file_path": "data/models/models22/main/papers/p1-schoene.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Feedback on the Formal Verification of UML Models in an Industrial Context: The Case of a Smart Device Life Cycle Management System",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "This paper presents experience feedback on how we managed to formally verify properties on semi-formal models of a Life Cycle Management System (LCMS) for smart devices. These devices are typically structured around a System on Chip (SoC), which can provide built-in hardware security. They can offer the possibility to make the deployment of Product-Service Systems (PSSs) to consumers easier, through traceability and collaborative consumption rule enforcement. A PSS is a business model in which products and services are tightly connected. One of the main advantages of such a PSS is that it optimizes product use, with a positive environmental impact. Associating the LCMS with a blockchain-based protocol makes it possible to avoid centralization. Semi-formal UML models of such a LCMS, as well as the informal properties it must comply with, were defined in order to explore its design space and evaluate the outcomes of specific design choices. However, the security of the LCMS implementation must be guaranteed, including protocols and architecture. For that purpose, these models and properties were later improved to be formally verifiable, which ensures the security of their implementation at the expense of added complexity. The verification was carried out using two available software tools: VerifPal for the protocol model, and AnimUML (developed by one of the authors) for the architecture model. This makes the procedure accessible for non-specialists in formal verification. Finally, our feedback on the whole process as well as on VerifPal is also provided.",
        "keywords": [
            "Formally verifiable models",
            "Formal verification tools",
            "UML",
            "Cryptographic protocol",
            "Life cycle management system"
        ],
        "authors": [
            "Maxime Méré",
            "Frédéric Jouault",
            "Loïc Pallardy",
            "Richard Perdriau"
        ],
        "file_path": "data/models/models22/main/papers/p121-mere.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling Program Verification Tools for Software Engineers",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "In software engineering, models are used for many different things. In this paper, we focus on program verification, where we use models to reason about the correctness of systems. There are many different types of program verification techniques which provide different correctness guarantees. We investigate the domain of program verification tools, and present a concise megamodel to distinguish these tools. We also present a data set of almost 400 program verification tools. This data set includes the category of verification tool according to our megamodel, practical information such as input/output format, repository links, and more. The categorisation enables software engineers to find suitable tools, investigate similar alternatives and compare them. We also identify trends for each level in our megamodel based on the categorisation. Our data set, publicly available at https://doi.org/10.4121/20347950, can be used by software engineers to enter the world of program verification and find a verification tool based on their requirements.",
        "keywords": [
            "Formal Methods; Program Verification; Megamodelling."
        ],
        "authors": [
            "Sophie Lathouwers and Vadim Zaytsev"
        ],
        "file_path": "data/models/models22/main/papers/p98-lathouwers.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Quantifying the Variability Mismatch Between Problem and Solution Space",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "A software product line allows to derive individual software products based on a configuration. As the number of configurations is an indicator for the general complexity of a software product line, automatic #SAT analyses have been proposed to provide this information. However, the number of configurations does not need to match the number of derivable products. Due to this mismatch, using the number of configurations to reason about the software complexity (i.e., the number of derivable products) of a software product line can lead to wrong assumptions during implementation and testing. How to compute the actual number of derivable products, however, is unknown. In this paper, we mitigate this problem and present a concept to derive a solution-space feature model which allows to reuse existing #SAT analyses for computing the number of derivable products of a software product line. We apply our concept to a total of 119 subsystems of three industrial software product lines. The results show that the derivation scales for real world software product lines and confirm the mismatch between the number of configurations and the number of products.",
        "keywords": [
            "Product lines",
            "variability mismatch",
            "solution-space analyses"
        ],
        "authors": [
            "Marc Hentze",
            "Chico Sundermann",
            "Thomas Thüm",
            "Ina Schaefer"
        ],
        "file_path": "data/models/models22/main/papers/p322-hentze.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Addressing the Uncertainty Interaction Problem in Software-intensive Systems: Challenges and Desiderata",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Software-intensive systems are increasingly used to support tasks that are typically characterized by high degrees of uncertainty. The modeling notations employed to design, verify, and operate such systems have increasingly started to capture different types of uncertainty, so that they can be explicitly considered when systems are developed and deployed. While these modeling paradigms consider different sources of uncertainty individually, these sources are rarely independent, and their interactions affect the achievement of system goals in subtle and often unpredictable ways. This vision paper describes the problem of uncertainty interaction in software-intensive systems, illustrating it on examples from relevant application domains. We then identify key open challenges and define desiderata that future modeling notations and model-driven engineering research should consider to address these challenges.",
        "keywords": [
            "Uncertainty interaction",
            "Modeling notations",
            "Patterns"
        ],
        "authors": [
            "Javier Cámara",
            "Radu Calinescu",
            "Betty H.C. Cheng",
            "David Garlan",
            "Bradley Schmerl",
            "Javier Troya",
            "Antonio Vallecillo"
        ],
        "file_path": "data/models/models22/main/papers/p24-camara.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Machine Learning-based Incremental Learning in Interactive Domain Modelling",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "In domain modelling, practitioners manually transform informal requirements written in natural language (problem descriptions) to more concise and analyzable domain models expressed with class diagrams. With automated domain modelling support using existing approaches, manual modifications may still be required in extracted domain models and problem descriptions to make them more accurate and concise. For example, educators teaching software engineering courses at universities usually use an incremental approach to build modelling exercises to restrict students in using intended modelling patterns. These modifications result in the evolution of domain modelling exercises over time. To assist practitioners in this evolution, a synergy between interactive support and automated domain modelling is required. In this paper, we propose a bot-assisted approach to allow practitioners perform domain modelling quickly and interactively. Furthermore, we provide an incremental learning strategy empowered by machine learning to improve the accuracy of the bot’s suggestions and extracted domain models by analyzing practitioners’ decisions over time. We evaluate the performance of our bot using test problem descriptions which shows that practitioners can expect to get useful support from the bot when applied to exercises of similar size and complexity, with precision, recall, and F2 scores over 85%. Finally, we evaluate our incremental learning strategy where we observe a reduction in the required manual modifications by 70% and an improvement of F2 scores of extracted domain models by 4.2% when using our proposed approach and learning strategy together.",
        "keywords": [
            "Domain Models",
            "Natural Language Processing (NLP)",
            "Machine Learning (ML)",
            "Bot",
            "Evolution",
            "Decisions",
            "Incremental Learning"
        ],
        "authors": [
            "Rijul Saini",
            "Gunter Mussbacher",
            "Jin L.C. Guo",
            "Jörg Kienzle"
        ],
        "file_path": "data/models/models22/main/papers/p176-saini.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Digital Twin as Risk-Free Experimentation Aid for Techno-socio-economic Systems",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Environmental uncertainties and hyperconnectivity force techno-socio-economic systems to introspect and adapt to succeed and survive. Current practices in decision-making are predominantly intuition-driven with attendant challenges for precision and rigor. We propose to use the concept of digital twins by combining results from Modelling & Simulation, Artificial Intelligence, and Control Theory to create a risk free ‘in silico’ experimentation aid to help: (i) understand why a system is the way it is, (ii) be prepared for possible outlier conditions, and (iii) identify plausible solutions for mitigating the outlier conditions in an evidence-backed manner. We use reinforcement learning to systematically explore the digital twin solution space. Our proposal is significant because it advances the effective use of digital twins to new problem domains that have new potential for impact. Our approach contributes an original meta model for simulatable digital twin of industry scale techno-socio-economic systems, agent-based implementation of the digital twin, and an architecture that serves as a risk-free experimentation aid to support simulation-based evidence-backed decision-making. We also discuss the rigor of our validation of the proposed approach and associated technology infrastructure through a representative sample of industry-scale real-world use cases.",
        "keywords": [
            "Digital Twin",
            "Decision Making",
            "Simulatable Model",
            "Agent Model"
        ],
        "authors": [
            "Souvik Barat",
            "Tony Clark",
            "Vinay Kulkarni",
            "Balbir Barn"
        ],
        "file_path": "data/models/models22/main/papers/p66-barat.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Checking Legal Contracts with SymboleoPC",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Legal contracts specify requirements for business transactions. As\nany other requirements specification, contracts may contain errors\nand violate properties expected by contracting parties. Symboleo\nwas recently proposed as a formal specification language for legal\ncontracts. This paper presents SymboleoPC, a tool for analyzing\nSymboleo contracts using model checking. It highlights the architec-\nture, implementation and testing of the tool, as well as a scalability\nevaluation with respect to the size of contracts and properties to\nbe checked through a series of experiments. The results suggest\nthat SymboleoPC can be usefully applied to the analysis of formal\nspecifications of contracts with real-life sizes and structures.",
        "keywords": [
            "Legal contracts",
            "smart contracts",
            "software requirements specifications",
            "formal specification languages",
            "model checking",
            "performance analysis",
            "nuXmv"
        ],
        "authors": [
            "Alireza Parvizimosaed",
            "Marco Roveri",
            "Aidin Rasti",
            "Daniel Amyot",
            "Luigi Logrippo",
            "John Mylopoulos"
        ],
        "file_path": "data/models/models22/main/papers/p278-parvizimosaed.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assisting in Requirements Goal Modeling: A Hybrid Approach based on Machine Learning and Logical Reasoning",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Goal modeling plays an imperative role in early requirements engineering, which has been investigated for decades. There have been many studies that show the usefulness of requirements goal models. However, the establishment of goal models is typically done manually, which is time-consuming and has a steep learning curve. In this paper, we propose a semi-automatic framework for constructing iStar models, which is a well-known goal modeling language. Specifically, we first investigate the practical needs of iStar modelers on the automation of iStar modeling by holding interviews, based on which we propose an interactive and iterative modeling process. Our proposal takes advantage of human decisions and artificial intelligence algorithms, respectively, aiming at achieving low modeling costs while maintaining the quality of models. We then propose a hybrid approach for automatically extracting goal model snippets from requirements text, which implements the automatic tasks of our proposed process. The proposed method combines logical reasoning with deep learning techniques so as to unleash the power of domain knowledge to assist with automation tasks. We have performed a series of experiments for evaluation. The experimental results show that our method achieves the F1-measure of 90.34% for actor entity extraction, 93.14% for intention entity extraction, and 83.18% for actor relation extraction, which can efficiently establish high-quality goal models. The artifacts are available at Zenodo1.",
        "keywords": [
            "goal modeling",
            "requirements engineering",
            "natural language processing",
            "machine learning"
        ],
        "authors": [
            "Qixiang Zhou",
            "Tong Li",
            "Yunduo Wang"
        ],
        "file_path": "data/models/models22/main/papers/p199-zhou.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic Test Amplification for Executable Models",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Behavioral models are important assets that must be thoroughly veri-\nfied early in the design process. This can be achieved with manually-\nwritten test cases that embed carefully hand-picked domain-specific\ninput data. However, such test cases may not always reach the de-\nsired level of quality, such as high coverage or being able to localize\nfaults efficiently. Test amplification is an interesting emergent ap-\nproach to improve a test suite by automatically generating new test\ncases out of existing manually-written ones. Yet, while ad-hoc test\namplification solutions have been proposed for a few programming\nlanguages, no solution currently exists for amplifying the test cases\nof behavioral models.\nIn this paper, we fill this gap with an automated and generic\napproach. Given an executable DSL, a conforming behavioral model,\nand an existing test suite, our approach generates new regression\ntest cases in three steps: (i) generating new test inputs by applying\na set of generic modifiers on the existing test inputs; (ii) running\nthe model under test with new inputs and generating assertions from\nthe execution traces; and (iii) selecting the new test cases that\nincrease the mutation score. We provide tool support for the approach\natop the Eclipse GEMOC Studio1 and show its applicability in an\nempirical study. In the experiment, we applied the approach to 71\ntest suites written for models conforming to two different DSLs, and\nfor 67 of the 71 cases, it successfully improved the mutation score\nbetween 3.17 % and 54.11 % depending on the initial setup.",
        "keywords": [
            "Test Amplification",
            "Regression Testing",
            "Executable Model",
            "Executable DSL"
        ],
        "authors": [
            "Faezeh Khorram",
            "Erwan Bousse",
            "Jean-Marie Mottu",
            "Gerson Sunyé",
            "Pablo Gómez-Abajo",
            "Pablo C. Cañizares",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models22/main/papers/p109-khorram.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Schema Inference for Multi-Model Data",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "The knowledge of a structural schema of data is a crucial aspect of most data management tasks. Unfortunately, in many real-world scenarios, the data is not accompanied by it, and schema-inference approaches need to be utilised.\nIn this paper, we focus on a specific and complex use case of multi-model data where several often contradictory features of the combined models must be considered. Hence, single-model approaches cannot be applied straightforwardly. In addition, the data often reach the scale of Big Data, and thus a scalable solution is inevitable. In our approach, we reflect all these challenges. In addition, we can also infer local integrity constraints as well as intra- and inter-model references. Last but not least, we can cope with cross-model data redundancy. Using a set of experiments, we prove the advantages of the proposed approach and we compare it with related work.",
        "keywords": [
            "schema inference",
            "multi-model data",
            "cross-model references",
            "data redundancy"
        ],
        "authors": [
            "Pavel Koupil",
            "Sebastián Hricko",
            "Irena Holubová"
        ],
        "file_path": "data/models/models22/main/papers/p13-koupli.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Finding with NEMO: A Recommender System to Forecast the Next Modeling Operations",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Nowadays, while modeling environments provide users with facilities to specify different kinds of artifacts, e.g., metamodels, models, and transformations, the possibility of learning from previous modeling experiences and being assisted during modeling tasks remains largely unexplored. In this paper, we propose NEMO, a recommender system based on an Encoder-Decoder neural network to assist modelers in performing model editing operations. NEMO learns from past modeling activities and performs predictions employing a deep learning technique. Such an algorithm has been successfully applied in machine translation to convert a text from a language to another foreign language and vice versa. An empirical evaluation on a dataset of BPMN change-based persistent model demonstrates that the technique permits learning from existing operations and effectively predicting the next editing operations with considerably high prediction accuracy. In particular, NEMO gets 0.977 as precision/recall and 0.992 as success rate score by the best performance.",
        "keywords": [],
        "authors": [
            "Juri Di Rocco",
            "Claudio Di Sipio",
            "Phuong T. Nguyen",
            "Davide Di Ruscio",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/models/models22/main/papers/p154-rocco.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Survey of Established Practices in the Life Cycle of Domain-Specific Languages",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Domain-specific languages (DSLs) have demonstrated their usefulness within many domains such as finance, robotics, and telecommunication. This success has been exemplified by the publication of a wide range of articles regarding specific DSLs and their merits in terms of improved software quality, programmer efficiency, security, etc. However, there is little public information on what happens to these DSLs after they are developed and published. The lack of information makes it difficult for a DSL practitioner or tool creator to identify trends, current practices, and issues within the field. In this paper, we seek to establish the current state of a DSL’s life cycle by analysing 30 questionnaire answers from DSL authors on the design and development, launch, evolution, and end of life of their DSL. On this empirical foundation, we make six recommendations to DSL practitioners, scholars, and tool creators on the subjects of user involvement in the design process, DSL evolution, and the end of life of DSLs.",
        "keywords": [
            "Domain-specific languages",
            "Survey"
        ],
        "authors": [
            "Holger Stadel Borum",
            "Christoph Seidl"
        ],
        "file_path": "data/models/models22/main/papers/p266-borum.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Nested OSTRICH: Hatching Compositions of Low-code Templates",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Low-code frameworks strive to simplify and speed-up application\ndevelopment. Native support for the reuse and composition of\nparameterised coarse-grain components (templates) is essential\nto achieve these goals. OSTRICH — a rich template language for\nthe OutSystems platform — was designed to simplify the use and\ncreation of such templates. However, without a built-in composition\nmechanism, OSTRICH templates are hard to create and maintain.\nThis paper presents a template composition mechanism and its\ntyping and instantiation algorithms for model-driven low-code de-\nvelopment environments. We evolve OSTRICH to support nested\ntemplates and allow the instantiation (hatching) of templates in\nthe definition of other templates. Thus, we observe a significant\nincrease code reuse potential, leading to a safer evolution of appli-\ncations. The present definition seamlessly extends the existing Out-\nSystems metamodel with template constructs expressed by model\nannotations that maintain backward compatibility with the existing\nlanguage toolchain. We present the metamodel, its annotations, and\nthe corresponding validation and instantiation algorithms. In par-\nticular, we introduce a type-based validation procedure that ensures\nthat using a template inside a template produces valid models.\nThe work is validated using the OSTRICH benchmark. Our proto-\ntype is an extension of the OutSystems IDE allowing the annotation\nof models and their use to produce new models. We also analyse\nwhich existing OutSystems sample screens templates can be im-\nproved by using and sharing nested templates.",
        "keywords": [],
        "authors": [
            "João Costa Seco",
            "Hugo Lourenço",
            "Joana Parreira",
            "Carla Ferreira"
        ],
        "file_path": "data/models/models22/main/papers/p210-seco.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "OSTRICH"
        }
    },
    {
        "title": "Verification of Railway Network Models with EVEREST",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Models – at different levels of abstraction and pertaining to different\nengineering views – are central in the design of railway networks, in\nparticular signalling systems. The design of such systems must fol-\nlow numerous strict rules, which may vary from project to project\nand require information from different views. This renders manual\nverification of railway networks costly and error-prone.\nThis paper presents EVEREST, a tool for automating the verifica-\ntion of railway network models that preserves the loosely coupled\nnature of the design process. To achieve this goal, EVEREST first\ncombines two different views of a railway network model – the\ntopology provided in signalling diagrams containing the functional\ninfrastructure, and the precise coordinates of the elements pro-\nvided in technical drawings (CAD) – in a unified model stored in the\nrailML standard format. This railML model is then verified against\na set of user-defined infrastructure rules, written in a custom modal\nlogic that simplifies the specification of spatial constraints in the\nnetwork. The violated rules can be visualized both in the signalling\ndiagrams and technical drawings, where the element(s) responsible\nfor the violation are highlighted.\nEVEREST is integrated in a long-term effort of EFACEC to im-\nplement industry-strong tools to automate and formally verify the\ndesign of railway solutions.",
        "keywords": [
            "railway engineering",
            "railway network model verification",
            "formal infrastructure rule specification",
            "railML"
        ],
        "authors": [
            "João Martins",
            "José M. Fonseca",
            "Rafael Costa",
            "José C. Campos",
            "Alcino Cunha",
            "Nuno Macedo",
            "José N. Oliveira"
        ],
        "file_path": "data/models/models22/main/papers/p345-martins.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Validating the Correctness of Reactive Systems Specifications Through Systematic Exploration",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. While the synthesized system is guaranteed to be correct w.r.t. the specification, the specification itself may be incorrect w.r.t. the engineers’ intention or w.r.t. the requirements or the environment in which the system should execute in. It thus requires validation. Combinatorial coverage (CC) is a well-known coverage criterion. Its rationale and key for effectiveness is the empirical observation that in many cases, the presence of a defect depends on the interaction between a small number of features of the system at hand. In this work we propose a validation approach for a reactive system specification, based on a systematic combinatorial exploration of the behaviors of a controller that was synthesized from it. Specifically, we present an algorithm to generate and execute a small scenario suite that covers all tuples of given variable value combinations over the reachable states of the controller. We have implemented our work in the Spectra synthesis en-vironment. We evaluated it over benchmarks from the literature using a mutation approach, specifically tailored for evaluating scenario suites of temporal specifications for reactive synthesis. The evaluation shows that for pairwise coverage, our CC algorithms are feasible and provide a 1.7 factor of improvement in mutation score compared to random scenario generation. We further report on a user study with students who have participated in a work-shop class at our university and have used our tool to validate their specifications. The user study results demonstrate the potential effectiveness of our work in helping engineers detect real bugs in the specifications they write.",
        "keywords": [],
        "authors": [
            "Dor Ma’ayan",
            "Shahar Maoz",
            "Roey Rozi"
        ],
        "file_path": "data/models/models22/main/papers/p132-maayan.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Solving the Instance Model-View Update Problem in AADL",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "The Architecture Analysis and Design Language (AADL) is a rich\nlanguage for modeling embedded systems through several con-\nstructs such as component extension and refinement to promote\nmodularity of component declarations. To ease processing AADL\nmodels, OSATE, the reference tool for AADL, defines another model\n(namely ‘instance’ model) computed from a base ‘declarative’ mod-\nels. An instance model is a simple object tree where all information\nfrom the declarative model is flattened so that tools can easily use\nthis information to analyze the system. However for modifications,\nthey have to make changes in the complex declarative model since\nthere is no automated backward transformation (deinstantiation)\nfrom instance to declarative models. Since the instance model is a\n‘view’ of the declarative model, this is a view-update problem. In\nthis paper, we propose the OSATE Declarative-Instance Mapping\nTool (OSATE-DIM1), an Eclipse plugin for deinstantiation of AADL\nmodels implementing a solution of this view-update problem. We\nevaluate OSATE-DIM with a benchmark of existing AADL model\nprocessing tools and verify the correctness of our deinstantiation\ntransformations. We also discuss how our approach could be use-\nful for decompilation of Object-Oriented languages’ intermediate\nrepresentations.",
        "keywords": [
            "Model-Driven Engineering",
            "Cyber-Physical Systems",
            "Embedded\nSystems",
            "View-Update Problem",
            "AADL"
        ],
        "authors": [
            "Rakshit Mittal",
            "Dominique Blouin",
            "Anish Bhobe",
            "Soumyadip Bandyopadhyay"
        ],
        "file_path": "data/models/models22/main/papers/p55-mittal.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editing Support for Software Languages: Implementation Practices in Language Server Protocols",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Effectively using software languages, be it programming or domain-specific languages, requires effective editing support. Modern IDEs, modeling tools, and code editors typically provide sophisticated support to create, comprehend, or modify instances—programs or models—of particular languages. Unfortunately, building such edit-ing support is challenging. While the engineering of languages is well understood and supported by modern model-driven tech-niques, there is a lack of engineering principles and best prac-tices for realizing their editing support. Especially domain-specific languages—often created by smaller organizations or individual developers, sometimes even for single projects—would benefit from better methods and tools to create proper editing support. We study practices for implementing editing support in 30 so-called language servers—implementations of the language server protocol (LSP). The latter is a recent de facto standard to realize editing support for languages, separated from the editing tools (e.g., IDEs or modeling tools), enhancing the reusability and quality of the editing support. Witnessing the LSP’s popularity—a whopping 121 language servers are in existence today—we take this opportunity to analyze the implementations of 30 language servers, some of which support multiple languages. We identify concerns that developers need to take into account when developing editing support, and we synthesize implementation practices to address them, based on a systematic analysis of the servers’ source code. We hope that our results shed light on an important technology for software language engineering, that facilitates language-oriented programming and systems development, including model-driven engineering.",
        "keywords": [
            "Language engineering",
            "code assistance",
            "source code editor",
            "implementation practices"
        ],
        "authors": [
            "Djonathan Barros",
            "Sven Peldszus",
            "Wesley K. G. Assunção",
            "Thorsten Berger"
        ],
        "file_path": "data/models/models22/main/papers/p232-barros.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Accelerating Similarity-Based Model Matching Using On-The-Fly Similarity Preserving Hashing",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Similarity-based model matching is the foundation of model versioning. It pairs model elements based on a distance metric (e.g., edit distance). Because it is expensive to calculate the distance between two elements, a similarity-based matcher usually suffers from performance issues when the model size increases. This paper proposes a hash-based approach to accelerate similarity-based model matching. Firstly, we design a novel similarity-preserving hash function that maps a model element to a 64-bit hash value. If two elements are similar, their hashes are also very close. Secondly, we propose a 3-layer index structure and a query algorithm to quickly filter out impossible candidates for the element to be matched based on their hashes. For the remaining candidates, we employ the classical similarity-based matching algorithm to determine the final matches. Our approach has been realized and integrated into EMF Compare. The evaluation results show that our hash function is effective to preserve the similarity between model elements and our matching approach reduces 16%–72% of time costs while assuring the matching results consistent with EMF Compare.",
        "keywords": [
            "model matching",
            "similarity-preserving hashing",
            "distance function",
            "model merging"
        ],
        "authors": [
            "Xiao He",
            "Letian Tang",
            "Yutong Li"
        ],
        "file_path": "data/models/models22/main/papers/p244-he.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Comprehensive Framework for the Analysis of Automotive Systems",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Analysis models, technologies and tools are extensively used in the automotive domain to validate and optimize the design and implementation of SW systems. This is especially true for modern systems including advanced autonomous (and complex) features. The range of analysis methods that can be applied is extremely wide and goes from functional correctness to functional safety to timing (and schedulability), security, and possibly even more. The AUTOSAR automotive standard has been defined with the purpose of standardizing the SW architecture of automotive systems and enable the construction of systems by composing SW components that are portable and abstract with respect to the underlying HW/SW platform. However, AUTOSAR was originally developed with portability of code in mind, and even if it quickly evolved to include a system-level modeling language (with its metamodel) and later extensions to deal with the needs of analysis methods (and tools), it is hardly comprehensive and still affected by several omissions and limitations. To fix the limitations with respect to timing and schedulability analysis Bosch developed the Amalthea (later App4MC) metamodel and tools. In Huawei, a more general (and ambitious) approach was undertaken to support not only timing analysis, but also model checking (or other types of formal verification), safety analysis and even design optimization. The approach is based on the concepts of a unified (modular) metamodel and a framework based on Eclipse to integrate analysis methods and tools. In this paper we describe the framework and the results obtained with respect to the objectives of functional verification and timing analysis.",
        "keywords": [
            "AUTOSAR SW Systems",
            "Model-Based Development",
            "Timing Analysis",
            "Formal Verification"
        ],
        "authors": [
            "Alessandro Cimatti",
            "Sara Corfini",
            "Luca Cristoforetti",
            "Marco Di Natale",
            "Alberto Griggio",
            "Stefano Puri",
            "Stefano Tonetta"
        ],
        "file_path": "data/models/models22/main/papers/p379-cimatti.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "System Architecture Synthesis for Performability by Logic Solvers",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "In model-based systems engineering, system architectures often have to make compromises to meet hard constraints of functional and extra-functional requirements while optimizing for a target objective. Design space exploration (DSE) techniques have been developed to automatically propose candidate architectures over an extremely large design and configuration space. (1) Meta-heuristic exploration algorithms are often used to provide practical, best-effort solutions for DSE, but they lack any guarantees of completeness or optimality. (2) Logic synthesis based approaches may offer strong theoretical guarantees, but frequently face scalability issues. In the paper, we propose two logic solver-based approaches to evaluate complex design spaces by using partial models in order to find an optimal solution with respect to performability objectives. One approach uses performability analysis as a post-filtering of valid system architecture candidates, while the other approach uses performability analysis for guiding the actual search over partial models. We evaluate both approaches on an interferometry mission architecture case study using view transformations for performability analysis and compare our approach with a well-known DSE framework based on meta-heuristic search.",
        "keywords": [
            "performability",
            "design-space exploration",
            "logic solver",
            "model generator",
            "partial models"
        ],
        "authors": [
            "Máté Földiák",
            "Kristóf Marussy",
            "Dániel Varró",
            "István Majzik"
        ],
        "file_path": "data/models/models22/main/papers/p43-foldiak.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Practical Multiverse Debugging through User-defined Reductions: Application to UML Models",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Multiverse debugging is an extension of classical debugging methods, particularly adapted to non-deterministic systems. Recently, a language-independent formalization was proposed. Moreover, multiverse debugging is particularly beneficial for specification and design languages, such as UML. However, this method suffers from scalability issues during breakpoint lookup. This problem arises due to the exhaustive exploration performed on the potentially infinite state-space of the system.\n\nIn this paper, we tackle this problem by introducing Reduced Multiverse Debugging, an extension proposing a way for the user to define reduction policies used during breakpoint lookup. We enrich the formalization of multiverse debugging with a modular breakpoint lookup strategy, which allows the integration of the reduction policy. We validate our approach by implementing a practical UML Statechart debugger in the AnimUML web framework. We show several ways the reduction can be applied, using methods such as predicate abstraction for breakpoint lookup on an infinite state-space, removing irrelevant variables, or creating classes of equivalent values. Moreover, we show the possibility to integrate probabilistic reduction strategies. Relying on hash collisions, these strategies can be iteratively refined to increase precision.",
        "keywords": [
            "multiverse debugging",
            "model analysis",
            "concurrency",
            "abstraction"
        ],
        "authors": [
            "Matthias Pasquier",
            "Ciprian Teodorov",
            "Frédéric Jouault",
            "Matthias Brun",
            "Luka Le Roux",
            "Loïc Lagadec"
        ],
        "file_path": "data/models/models22/main/papers/p87-pasquier.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Model-based Bias Mitigation in Machine Learning",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Models produced by machine learning are not guaranteed to be free from bias, particularly when trained and tested with data produced in discriminatory environments. The bias can be unethical, mainly when the data contains sensitive attributes, such as sex, race, age, etc. Some approaches have contributed to mitigating such biases by providing bias metrics and mitigation algorithms. The challenge is users have to implement their code in general/statistical programming languages, which can be demanding for users with little programming and fairness in machine learning experience. We present FairML, a model-based approach to facilitate bias measure- ment and mitigation with reduced software development effort. Our evaluation shows that FairML requires fewer lines of code to produce comparable measurement values to the ones produced by the baseline code.",
        "keywords": [
            "Model-Driven Engineering",
            "Generative Programming",
            "Bias Mitigation",
            "Bias Metrics",
            "Machine Learning"
        ],
        "authors": [
            "Alfa Yohannis and Dimitris Kolovos"
        ],
        "file_path": "data/models/models22/main/papers/p143-yohannis.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Advanced Visualization and Interaction in GLSP-based Web Modeling: Realizing Semantic Zoom and Off-Screen Elements",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Conceptual modeling is widely adopted in industrial practices, e.g., process, software, and systems modeling. Providing adequate and usable modeling tools is essential for the efficient adoption of modeling. Metamodeling platforms provide a rich set of functionalities and maturely realize state-of-the-art modeling tools. However, despite their maturity and stability, most of these platforms only slowly – if at all – leverage the full extent of functionalities and the ease of exploitation and integration enabled by web technologies. With the Graphical Language Server Protocol (GLSP), it is now possible to realize much richer, advanced opportunities for visualizing and interacting with conceptual models. This paper presents a concept and a prototypical implementation of two advanced model visualization and interaction functionalities with the Eclipse GLSP platform: Semantic Zoom and Off-Screen Elements. We believe such advanced functionalities pave the way for a prosperous modeling future and spark innovation in modeling tool development.",
        "keywords": [
            "Modeling tools",
            "Web Modeling",
            "Language Server Protocol",
            "Visualization"
        ],
        "authors": [
            "Giuliano De Carlo",
            "Philip Langer",
            "Dominik Bork"
        ],
        "file_path": "data/models/models22/main/papers/p221-carlo.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Symboleo2SC: From Legal Contract Specifications to Smart Contracts",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Smart contracts (SCs) are software systems that monitor and control the execution of legal contracts to ensure compliance with the contracts’ terms and conditions. They often exploit Internet-of-Things technologies to support their monitoring functions, and blockchain technology to ensure the integrity of their data. Ethereum and business blockchain platforms, such as Hyperledger Fabric, are popular choices for SC development. However, there is a gap in the knowledge of SCs between developers and legal experts. Symboleo is a formal specification language for legal contracts that was introduced to address this issue. Symboleo specifications directly encode legal concepts such as parties, obligations, and powers. In this paper, we propose a tool-supported method for translating Symboleo specifications into smart contracts. We have extended the current Symboleo IDE, implemented the ontology and semantics of Symboleo into a reusable library, and developed the Sym-boleo2SC tool to generate Hyperledger Fabric code exploiting this library. Symboleo2SC was evaluated with three sample contracts. The results shows that legal contract specifications in Symboleo can be fully converted to SCs for monitoring purposes. Moreover, Symboleo2SC helps simplify the SC development process, saves development effort, and helps reduce risks of coding errors.",
        "keywords": [
            "Smart contracts",
            "code generation",
            "blockchain",
            "domain-specific languages",
            "legal ontology"
        ],
        "authors": [
            "Aidin Rasti",
            "Daniel Amyot",
            "Alireza Parvizimosaed",
            "Marco Roveri",
            "Luigi Logrippo",
            "Amal Ahmed Anda",
            "John Mylopoulos"
        ],
        "file_path": "data/models/models22/main/papers/p300-rasti.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Symboleo"
        }
    },
    {
        "title": "A Domain-Specific Language for Simulation-Based Testing of IoT Edge-to-Cloud Solutions",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "The Internet of things (IoT) is increasingly prevalent in domains such as emergency response, smart cities and autonomous vehicles. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. In this paper, we propose a domain-specific language (DSL) for generating edge-to-cloud simulators. An edge-to-cloud simulator executes the functionality of a large array of edge devices that communicate with cloud applications. Our DSL, named IoTECS, is the result of a collaborative project with an IoT analytics company, Cheetah Networks. The industrial use case that motivates IoTECS is ensuring the scalability of cloud applications by putting them under extreme loads from IoT devices connected to the edge. We implement IoTECS using Xtext and empirically evaluate its usefulness. We further reflect on the lessons learned.",
        "keywords": [
            "Domain-Specific Languages",
            "IoT",
            "Simulation",
            "Stress Testing",
            "Xtext"
        ],
        "authors": [
            "Jia Li",
            "Shiva Nejati",
            "Mehrdad Sabetzadeh",
            "Michael McCallen"
        ],
        "file_path": "data/models/models22/main/papers/p367-li.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bug Localization in Game Software Engineering: Evolving Simulations to Locate Bugs in Software Models of Video Games",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Video games have characteristics that differentiate their development and maintenance from classic software development and maintenance. These differences have led to the coining of the term Game Software Engineering to name the emerging subfield that intersects Software Engineering and video games. One of these differences is that video game developers perceive more difficulties than other non-game developers when it comes to locating bugs. Our work proposes a novel way to locate bugs in video games by means of evolving simulations. As the baseline, we have chosen BLiMEA, which targets classic software engineering and uses bug reports and the defect localization principle to locate bugs. We also include Random Search as a sanity check in the evaluation. We evaluate the approaches in a commercial video game (Kromaia). The results for F-measure range from 46.80%. to 70.28% for five types of bugs. Our approach improved the results of the baseline by 20.29% in F-measure. To the best of our knowledge, this is the first approach that is designed specifically for bug localization in video games. A focus group with professional video game developers has confirmed the acceptance of our approach. Our approach opens a new research direction for bug localization for both game software engineering and possibly classic software engineering.",
        "keywords": [
            "Bug Localization",
            "Video Games",
            "Search-Based Software Engineering",
            "Model-Driven Engineering"
        ],
        "authors": [
            "Rodrigo Casamayor",
            "Lorena Arcega",
            "Francisca Pérez",
            "Carlos Cetina"
        ],
        "file_path": "data/models/models22/main/papers/p356-casamayor.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Declarative Modelling Framework for the Deployment and Management of Blockchain Applications",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "The deployment and management of Blockchain applications require non-trivial efforts given the unique characteristics of their infrastructure (i.e., immutability) and the complexity of the software systems being executed. The operation of Blockchain applications is still based on ad-hoc solutions that are error-prone, difficult to maintain and evolve, and do not manage their interactions with other infrastructures (e.g., a Cloud backend). This paper proposes KATENA, a framework for the deployment and management of Blockchain applications. In particular, it focuses on applications that are compatible with Ethereum, a popular general-purpose Blockchain technology. KATENA provides i) a metamodel for defining Blockchain applications, ii) a set of processes to automate the deployment and management of defined models, and iii) an implementation of the approach based on TOSCA, a standard language for Infrastructure-as-Code, and xOpera, a TOSCA-compatible orchestrator. To evaluate the approach, we applied KATENA to model and deploy three real-world Blockchain applications, and showed that our solution reduces the amount of code required for their operations up to 82.7%.",
        "keywords": [
            "blockchain",
            "dApp",
            "decentralized applications",
            "orchestration",
            "devops",
            "infrastructure-as-code",
            "iac",
            "smart contract",
            "ethereum",
            "TOSCA",
            "deployment"
        ],
        "authors": [
            "Luciano Baresi",
            "Giovanni Quattrocchi",
            "Damian Andrew Tamburri",
            "Luca Terracciano"
        ],
        "file_path": "data/models/models22/main/papers/p311-baresi.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reactive Links Across Multi-Domain Engineering Models",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "As the engineering world moves towards collaborative model-driven development, it is becoming increasingly difficult to keep all model artifacts synchronized and consistent across a myriad of tools and domains. The existing literature proposes a variety of solutions, from passive trace links to computing change propagation paths. However, these solutions require manual propagation and the use of a limited set of tools, while also lacking the efficiency and granularity required during the development of complex systems. To overcome these limitations, this paper proposes a solution based on reactive propagation links between property values across multi-domain models managed in different tools. As opposed to the traditional passive links, the propagation links automatically react to changes during engineering to assure the synchronization and consistency of the models. The feasibility and performance of our solution were evaluated in two practical scenarios. We identified a set of change propagation cases, all of which could be resolved using our solution, while also rendering a great improvement in terms of efficiency as compared to manual propagation. The contribution of our solution to the state of the practice is to enhance the engineering process by reducing the burden of manually keep-ing models synchronized, eliminating inconsistencies that can be originated in artifacts managed in a variety of tool from different domains.",
        "keywords": [],
        "authors": [
            "Cosmina Cristina Rat,iu",
            "Wesley K. G. Assunção",
            "Rainer Haas",
            "Alexander Egyed"
        ],
        "file_path": "data/models/models22/main/papers/p76-tatiu.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Influence of Software Design Representation on the Design Communication of Teams with Diverse Personalities",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Software is the main driver of added-value in many of the systems that surround us. While its complexity is increasing, so is the diversity of systems driven by software. To meet the challenges emerging from this combination, it is necessary to mobilize increasingly large and heterogeneous multidisciplinary teams, comprising software experts, as well as experts from various domains related to the systems driven by software. Hence, the quality of communication about software between stakeholders of different domains and with different personalities is becoming a key issue for successfully engineering software-intensive systems. The goal of this study, thus, is to investigate the effect of the representation of software design models on the communication of design decisions between stakeholders with diverse personality traits. As a result, this study finds that graphical representations of software design models are better than textual representations in enhancing the communication and increasing the productivity of stakeholders with diverse personalities.",
        "keywords": [
            "Software Engineering",
            "Software Design",
            "Human Aspects",
            "Personality Traits",
            "Communication"
        ],
        "authors": [
            "Rodi Jolak",
            "Maxime Savary-Leblanc",
            "Manuela Dalibor",
            "Juraj Vincur",
            "Regina Hebig",
            "Xavier Le Pallec",
            "Michel Chaudron",
            "Sébastien Gérard",
            "Ivan Polasek",
            "and Andreas Wortmann"
        ],
        "file_path": "data/models/models22/main/papers/p255-jolak.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Predicate Abstractions for Smart Contract Validation",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Smart contracts are immutable programs deployed on the blockchain that can manage significant assets. Because of this, verification and validation of smart contracts is of vital importance. Indeed, it is industrial practice to hire independent specialized companies to audit smart contracts before deployment. Auditors typically rely on a combination of tools and experience but still fail to identify problems in smart contracts before deployment, causing significant losses. In this paper, we propose using predicate abstraction to construct models which can be used by auditors to explore and validate smart contact behaviour at the function call level by proposing predicates that expose different aspects of the contract. We propose predicates based on requires clauses and enum-type state variables as a starting point for contract validation and report on an evaluation on two different benchmarks.",
        "keywords": [],
        "authors": [
            "Javier Godoy",
            "Juan Pablo Galeotti",
            "Diego Garbervetsky",
            "Sebastian Uchitel"
        ],
        "file_path": "data/models/models22/main/papers/p289-godoy.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Precomputing Reconfiguration Strategies based on Stochastic Timed Game Automata",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Many modern software systems continuously reconfigure themselves to (self-)adapt to ever-changing environmental contexts. Selecting presumably best-fitting next configurations is, however, very challenging, depending on functional and non-functional criteria like real-time constraints as well as inherently uncertain future contexts which makes greedy one-step decision heuristics ineffective. In addition, the computational overhead caused by reconfiguration planning at run-time should not outweigh its benefits. On the other hand, completely pre-planning reconfiguration decisions at design time is also infeasible due to the lack of knowledge about the context behavior. In this paper, we propose a game-theoretic setting for precomputing reconfiguration decisions under partially uncertain real-time behavior. We employ stochastic timed game automata as reconfiguration model to derive winning strategies which enable the first player (the system) to make fast look-ups for presumably best-fitting reconfiguration decisions satisfying the second player (the context). To cope with the high computational complexity of finding winning strategies, our tool implementation1 utilizes the statistical model-checker Uppaal Stratego to approximate near-optimal solutions. In our evaluation, we investigate efficiency/effectiveness trade-offs by considering a real-world example consisting of a reconfigurable robot support system for the construction of aircraft fuselages.",
        "keywords": [
            "Stochastic Timed Game Automata",
            "Proactive Self-Adaptation",
            "Strategy Synthesis",
            "Statistical Model-Checking"
        ],
        "authors": [
            "Hendrik Göttmann",
            "Birte Caesar",
            "Lasse Beers",
            "Malte Lochau",
            "Andy Schürr",
            "Alexander Fay"
        ],
        "file_path": "data/models/models22/main/papers/p31-goettmann.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Machine Learning Methods for Model Classification: A Comparative Study",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "In the quest to reuse modeling artifacts, academics and industry have proposed several model repositories over the last decade. Different storage and indexing techniques have been conceived to facilitate searching capabilities to help users find reusable artifacts that might fit the situation at hand. In this respect, machine learning (ML) techniques have been proposed to categorize and group large sets of modeling artifacts automatically. This paper reports the results of a comparative study of different ML classification techniques employed to automatically label models stored in model repositories. We have built a framework to systematically compare different ML models (feed-forward neural networks, graph neural networks, 𝑘−nearest neighbors, support version machines, etc.) with varying model encodings (TF-IDF, word embeddings, graphs and paths). We apply this framework to two datasets of about 5,000 Ecore and 5,000 UML models. We show that specific ML models and encodings perform better than others depending on the characteristics of the available datasets (e.g., the presence of duplicates) and on the goals to be achieved.",
        "keywords": [
            "Model classification",
            "Model-Driven Engineering",
            "Machine learning"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Riccardo Rubei",
            "Jesús Sánchez Cuadrado",
            "Davide di Ruscio"
        ],
        "file_path": "data/models/models22/main/papers/p165-lopez.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modular Language Product Lines: A Graph Transformation Approach",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "Modelling languages are intensively used in paradigms like model-driven engineering to automate all tasks of the development process. These languages may have variants, in which case the need arises to deal with language families rather than with individual languages. However, specifying the syntax and semantics of each language variant separately is costly, hinders reuse across variants, and may yield inconsistent semantics between variants.\nTo attack this problem, we propose a novel, modular way to describe product lines of modelling languages. Our approach is compositional, enabling the incremental definition of language families by means of modules comprising meta-model fragments, graph transformation rules, and rule extensions. Language variants are configured by selecting the desired modules, which entails the composition of a language meta-model and a set of rules defining its semantics. This paper describes a theory able to check consistent semantics among all languages within the family, an implementation as an Eclipse plugin, and an evaluation reporting drastic specification size reduction w.r.t. an enumerative approach.",
        "keywords": [
            "Model-driven engineering",
            "Graph transformation",
            "Product lines",
            "Software language engineering"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra",
            "Paolo Bottoni"
        ],
        "file_path": "data/models/models22/main/papers/p334-lara.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EvoSL: A Large Open-Source Corpus of Changes in Simulink Models & Projects",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Having readily available corpora is crucial for performing replication, reproduction, extension, and verification studies of existing research tools and techniques. MATLAB/Simulink is a de-facto standard tool in several safety-critical industries for system modeling and analysis, compiling models to code, and deploying code to embedded hardware. There is no commonly used corpus for large-scale model change studies because there is no readily available corpus. EvoSL is the first large corpus of Simulink projects that includes model and project changes and allows redistribution. EvoSL is available under a permissive open-source license and contains its collection and analysis tools. Using a subset of EvoSL, we replicated a case study of model changes on a single closed-source industrial project.",
        "keywords": [
            "reproducibility",
            "replication",
            "Simulink",
            "open science",
            "Simulink model changes",
            "corpus",
            "evolution"
        ],
        "authors": [
            "Sohil Lal Shrestha",
            "Alexander Boll",
            "Shafiul Azam Chowdhury",
            "Timo Kehrer",
            "Christoph Csallner"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a273/248000a273.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MODELS 2023",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "This document is the preface to the MODELS 2023 conference. It details the conference's history, location, submission process, acceptance rates, and program highlights.",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z010/248000z010.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Experience in Specializing a Generic Realization Language for SPL Engineering at Airbus",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "In software product line (SPL) engineering, feature models are the de facto standard for modeling variability. A user can derive products out of a base model by selecting features of interest. Doing it automatically, however, requires a realization model, which is a description of how a base model should be modiﬁed when a given feature is selected/unselected. A realization model then necessarily depends on the base metamodel, asking for ad hoc solutions that have ﬂourished in recent years. In this paper, we propose Greal, a generic solution to this problem in the form of (1) a generic declarative realization language that can be automatically composed with one or more base metamodels to yield a domain-speciﬁc realization language and (2) a product derivation algorithm applying a realization model to a base model and a resolved model to yield a derived product. We describe how, on top of Greal, we specialized a realization language to support both positive and negative variability, ﬁt the syntax and semantics of the targeted language (BPMN) and take into account modeling practices at Airbus. We report on lessons learned of applying this approach on Program Development Plans based on business process models and discuss open problems.",
        "keywords": [],
        "authors": [
            "Damien Foures",
            "Mathieu Acher",
            "Olivier Barais",
            "Benoit Combemale",
            "Jean-Marc J´ez´equel",
            "J¨org Kienzle"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a319/248000a319.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Greal"
        }
    },
    {
        "title": "Automated Domain Modeling with Large Language Models: A Comparative Study",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Domain modeling is an essential part of software engineering and serves as a way to represent and understand the concepts and relationships in a problem domain. Typically, software engineers interpret the problem description written in natural language and manually translate it into a domain model. Domain modeling can be time-consuming and highly depends on the expertise of software engineers. Recently, Large Language Models (LLMs) have exhibited remarkable ability in language understanding, generation, and reasoning. In this paper, we conduct a comprehensive, comparative study of using LLMs for fully automated domain modeling. We assess two powerful LLMs, GPT3.5 and GPT4, employing various prompt engineering techniques on a data set containing ten diverse domain modeling examples with reference solutions created by modeling experts. Our findings reveal that while LLMs demonstrate impressive domain understanding capabilities, they are still impractical for full automation, with the top-performing LLM achieving F1 scores of 0.76 for class generation, 0.61 for attribute generation, and 0.34 for relationship generation. Moreover, the F1 score is characterized by higher precision and lower recall; thus, domain elements retrieved by LLMs are often reliable, but there are many missing elements. Furthermore, modeling best practices are rarely followed in auto-generated domain models. Our data set and evaluation provide a valuable baseline for future research in automated LLM-based domain modeling.",
        "keywords": [
            "domain modeling",
            "large language models",
            "few-shot learning",
            "chain-of-thought prompting",
            "prompt engineering"
        ],
        "authors": [
            "Kua Chen",
            "Yujing Yang",
            "Boqi Chen",
            "Jos´e Antonio Hern´andez L´opez",
            "Gunter Mussbacher",
            "D´aniel Varr´o"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a162/248000a162.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated Grading of Use Cases",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Use cases (UCs) play a crucial role in software engineering courses, with students frequently using them in assignments, projects, and exams. However, as the number of students enrolling in Computer Science and Software Engineering programs continues to rise, manual grading of these models is becoming increasingly time-consuming. While automated grading tools for class diagrams exist, the automation of grading use case models has received limited attention.\nThis paper proposes an approach for automatically grading use cases. To compare a student’s solution to the teacher’s solution our approach uses structural matching, syntactic and semantic word matching, natural language processing for sentence matching, and ﬂattening of use case hierarchies. The grading algorithm is evaluated on three actual undergraduate and graduate assignments that involve modeling a Gas Station ﬁll-up scenario, a Supermarket checkout scenario, as well as the interactions involved in playing the board game Elfenroads. The results show that with some tuning our automatically determined grades lie within 7% difference of the instructor’s manual grades.",
        "keywords": [
            "use cases",
            "automated grading",
            "model comparison"
        ],
        "authors": [
            "Mohsen Hosseinibaghdadabadi",
            "Omar Alam",
            "Nicolas Almerge",
            "Jörg Kienzle"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a106/248000a106.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OCL Rebuilt, From the Ground Up",
        "submission-date": "2023/09",
        "publication-date": "2023/09",
        "abstract": "The Object Constraint Language (OCL) serves the expression of complex conditions and queries over UML-based models in an object-oriented style. We note that OCL’s grounding in object-orientation leads to a number of issues, including subtle inconsistencies and unsafe navigation. To address these issues, we present OCL♯, a new formal foundation for OCL with borrowings from Alloy. We provide OCL♯’s syntax and semantics, prove type safety, and present a prototype implementation.",
        "keywords": [
            "OCL",
            "semantics",
            "relational language",
            "Alloy"
        ],
        "authors": [
            "Friedrich Steimann",
            "Robert Clarisó",
            "Martin Gogolla"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a194/248000a194.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MODELS 2023",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Antonio Cicchetti",
            "Alfonso Pierantonio",
            "Federico Ciccozzi",
            "Thomas Kühne",
            "Gabriele Taentzer",
            "Davide Di Ruscio",
            "Leen Lambers",
            "Hugo Bruneliere",
            "Fiona Polack",
            "Nelly Bencomo",
            "Sebastian Götz",
            "Ivano Malavolta",
            "Judith Michael",
            "Juri Di Rocco",
            "Matthias Tichy",
            "Jan Carlson",
            "Maria Spichkova"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z012/248000z012.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating Testing into the Alloy Model Development Workﬂow",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Software models help improve the reliability of software systems: models can convey requirements, and can analyze design and implementation properties. A key strength of Alloy, a commonly used modeling language, is the Alloy Analyzer toolset. The Analyzer is an automated analysis engine that searches for all valid instances, which are assignments to the sets of the model such that all executed formulas hold, up to a user-provided scope. Unfortunately, despite the Analyzer, writing correct models remains a difﬁcult and error-prone task. To address this, a unit testing framework, AUnit, was created for Alloy. Since then, several traditional imperative testing practices, including mutation testing, fault localization and repair, have been established for Alloy models. Prior work has introduced the feasibility of these approaches and produced command line prototype tools. This paper highlights the effort to translate these research products into the Analyzer, the main model development tool for Alloy, to produce one consolidated integrated development environment that provides robust testing support.",
        "keywords": [
            "Alloy",
            "SAT Solver",
            "Software Testing"
        ],
        "authors": [
            "Allison Sullivan"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a117/248000a117.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Richard Paige",
            "Daniel Varró",
            "Silvia Abrahão",
            "Don Batory",
            "Nelly Bencomo",
            "Jordi Cabot",
            "Marsha Chechik",
            "Juergen Dingel",
            "Alexander Egyed",
            "Jeff Gray",
            "Øystein Haugen",
            "Zhenjiang Hu",
            "Marouane Kessentini",
            "Jörg Kienzle",
            "Thomas Kühne",
            "Vinay Kulkarni",
            "Timothy C. Lethbridge",
            "Shiva Nejati",
            "Kathy Park",
            "Alfonso Pierantonio",
            "Alexander Pretschner",
            "Houari Sahraoui",
            "Wolfram Schulte",
            "Eugene Sirjani",
            "Gabriele Taentzer",
            "Manuel Wimmer",
            "Andrzej Wąsowski",
            "Tao Yue",
            "Juan de Lara"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z014/248000z014.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Word Embeddings for Model-Driven Engineering",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Model-Driven Engineering practitioners have to deal\nwith the construction of modelling evironments by devising meta-\nmodels, grammars, editors, etc. One of the goals of the application\nof Machine Learning to MDE is to use ML algorithms to assist\nthe MDE expert in these tasks. These algorithms cannot directly\nreceive raw models or meta-models as input, but they typically\nhave to be transformed into a numeric representation, i.e., a\nvector. In this context, a common approach is to use pre-trained\nWord Embeddings, which deﬁne mapping functions that associate\nwords to semantic vectors. However, current word embeddings\nare trained with general texts and lack the technical words which\ntypically arise in the modelling domain. To tackle this issue, we\nhave collected a corpus of modelling texts from well-known mod-\nelling venues, and we have trained two types of word embedding\nmodels. The resulting embeddings (named WordE4MDE) are\nspecialised to address ML tasks in the MDE domain. We have\nperformed an extensive evaluation using the Ecore models of\nthe ModelSet dataset and two state-of-the-art word embeddings\n(GloVe and Word2Vec) as baselines. We show that WordE4MDE\noutperforms these two baselines in three meta-modelling tasks,\nnamely meta-model classiﬁcation, meta-model clustering, and\nmeta-model concept recommendation. WordE4MDE embeddings\nare available at https://github.com/models-lab/worde4mde and\ncan be loaded using standard Python libraries for their use in\nML pipelines.",
        "keywords": [
            "Model-Driven Engineering",
            "Machine Learning",
            "Word Embeddings"
        ],
        "authors": [
            "Jos´e Antonio Hern´andez L´opez",
            "Carlos Dur´a",
            "Jes´us S´anchez Cuadrado"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a151/248000a151.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Approach for Knowledge-Based Engineering of Industrial Digital Twins",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Digital twins are heralding a new paradigm in the process and manufacturing industries by providing near real-time decisions for a range of problems. Engineering digital twin solutions is a knowledge and effort intensive activity. Currently, this is not an easily reproducible process. For each type of industry and for each specific plant, the digital twin design and development process has to start all over, and the effort needs to be reinvested. This is not a scalable proposition. To address this, we need a systematic approach that captures and reuses knowledge such that each new problem is solved significantly more efficiently than the previous problems. We propose a knowledge modeling and implementation methodology to systematically model and capture knowledge pertaining to the industrial manufacturing plant domain, problem domain and solution domain, and a knowledge guided process that reasons with this knowledge to provide intelligent decision support in the design and development of digital twin-based solutions for problems in manufacturing industries.",
        "keywords": [
            "Digital Twins",
            "Model Driven Engineering",
            "Knowledge Modeling",
            "Industry 4.0"
        ],
        "authors": [
            "Sushant Vale",
            "Sreedhar Reddy",
            "Sivakumar Subramanian",
            "Subhrojyoti Roy Chowdhury",
            "Sri Harsha Nistala",
            "Anirudh Deodhar",
            "Venkatraman Runkan"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a013/248000a013.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lessons Learned Building a Tool for Workﬂow+",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "As automotive manufacturers continue to release more advanced autonomous features, the effort required to ensure safety is increasing. This is a result of the growing complexity of automotive systems, and the increased level of safety assurance required for higher levels of autonomy. The Workﬂow+ model-based framework was developed in response to these challenges, to provide a way for safety assurance to be developed rigorously and with automated tool support. In this paper we discuss our experiences and lessons learned while developing Eclipse-based tooling for Workﬂow+ during a collaborative research project with a large automotive OEM.",
        "keywords": [
            "Model-Based Safety Assurance",
            "Change Impact Analysis",
            "Tool Development"
        ],
        "authors": [
            "Nicholas Annable",
            "Thomas Chiang",
            "Mark Lawford",
            "Richard F. Paige",
            "Alan Wassyng"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a140/248000a140.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Experimental Evaluation of Conformance Testing Techniques in Active Automata Learning",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "Active automata learning is a technique for dynamically learning finite state machine models of black-box systems. Conformance testing is a well-known bottleneck during learning. While multiple conformance testing techniques (CTTs) for Finite State Machines have been proposed, there is a lack of empirical studies that assess the effects of these CTTs during learning. In this work, we compare the performance of eight different CTTs (W, Wp, HSI, H-ADS, H, SPY, SPY-H, I-ADS) while learning 46 models from different communication protocols. Moreover, we propose APFDL as a metric for characterizing the efficiency of automata learning experiments in terms of fault detection capacity. This metric allows identifying CTTs with a lower total cost regarding the number of symbols and resets and a higher rate of state discovery during learning. Our results indicate that while the total cost entailed by CTTs in learning tends to be negligible, we found a significant difference in fault detection rate in learning. Nevertheless, the differences in fault detection rates become negligible when CTTs are applied in randomized mode. These findings reveal the positive role that randomness can have in improving learning efficiency, despite compromising test completeness.",
        "keywords": [],
        "authors": [
            "Bharat Garhewal",
            "Carlos Diego N. Damasceno"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a217/248000a217.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An extended model-based characterization of fine-grained access control for SQL queries",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "In the context of a project in model-driven security that focuses on the development of model-driven techniques for building secure data-centric (web) applications, we extend, in three (inter-related) dimensions, a recently proposed model-based characterization of fine-grained access control (FGAC) authorization for SQL queries. First, we extend the FGAC policies’ underlying data models by considering association-classes. Secondly, we extend the FGAC policies’ security modeling language by considering, as protected resources, the classes and the (explicit and implicit) associations introduced by the association-classes. Thirdly, we extend the clauses that define whether a user is authorized by an FGAC policy to execute a SQL query, to cover the case of queries retrieving information related to the association-classes. To illustrate our extensions and to demonstrate their applicability, we provide examples of authorization decisions for SQL queries with respect to FGAC policies. The artefact comprising the implementation of this model-based characterization and an executable version of the example is available at https://doi.org/10.5281/zenodo.8176237.",
        "keywords": [
            "Model-Driven Security",
            "Fine-Grained Access Control",
            "Secure Database Queries",
            "SecureUML"
        ],
        "authors": [
            "Hoang Nguyen",
            "Manuel Clavel"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a095/248000a095.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Manual Abstraction in the Wild: A Multiple-Case Study on OSS Systems’ Class Diagrams and Implementations",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Models are a useful tool for software design, analysis, and to support the onboarding of new maintainers. However, these beneﬁts are often lost over time, as the system implementation evolves and the original models are not updated. Reverse engineering methods and tools could help to keep models and implementation code in sync; however, automatically reverse-engineered models are typically not abstract and contain extensive information that prevents understanding. Recent advances in AI-based content generation make it likely that we will soon see reverse engineering tools with support for human-grade abstraction. To inform the design and validation of such tools, we need a principled understanding of what manual abstraction is, a question that has received little attention in the literature so far. Towards this goal, in this paper, we present a multiple-case study of model-to-code differences, investigating ﬁve substantial open-source software projects retrieved via repository mining. To explore characteristics of model-to-code differences, we, all in all, manually matched 466 classes, 1352 attributes, and 2634 operations from source code to 338 model elements (classes, attributes, operations, and relationships). These mappings precisely capture the differences between a provided class diagram design and implementation codebase. Studying all differences in detail allowed us to derive a taxonomy of difference types and to provide a sorted list of cases corresponding to the identiﬁed types of differences. As we discuss, our contributions pave the way for improved reverse engineering methods and tools, new mapping rules for model-to-code consistency checks, and guidelines for avoiding over-abstraction and over-speciﬁcation during design.",
        "keywords": [
            "software design",
            "modeling"
        ],
        "authors": [
            "Wenli Zhang",
            "Weixing Zhang",
            "Daniel Str¨uber",
            "Regina Hebig"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a036/248000a036.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mutation Testing for Temporal Alloy Models",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Writing declarative models has numerous beneﬁts, ranging from automated reasoning and correction of design-level properties before systems are built, to automated testing and debugging of their implementations after they are built. Alloy is a declarative modeling language that is well-suited for verifying system designs. A key strength of Alloy is its scenario-ﬁnding toolset, the Analyzer, which allows users to explore all valid scenarios that adhere to the model’s constraints up to a user-provided scope. Despite the Analyzer, writing correct Alloy models remains a difﬁcult task, partly due to Alloy’s expressive operators, which allow for succinct formulations of complex properties but can be difﬁcult to reason over manually. To further add to the complexity, Alloy’s grammar was recently expanded to support linear temporal logic, increasing both the expressibility of Alloy as well as the burden for accurately expressing properties. To address this, this paper presents μAlloyτ, an extension to Alloy’s mutation testing framework that accounts for the newly introduced temporal logic, including updating μAlloyτ’s test generation capability to produce temporal test cases. Experimental results reveal μAlloyτ is efﬁcient at generating and checking mutations and μAlloyτ’s automatically generated tests are effective at detecting faulty temporal models.",
        "keywords": [
            "Alloy",
            "Mutation Testing",
            "Test Generation"
        ],
        "authors": [
            "Ana Jovanovic",
            "Allison Sullivan"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a228/248000a228.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SkeMo: Sketch Modeling for Real-Time Model Component Generation",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Software modeling is a powerful tool in the design and implementation of high-quality software systems. Models can be used from high-level design to formal code generation, with various applications in between. Often, software models are initially created informally, by sketching on a whiteboard or paper during the early design phase of the system, and eventually converted into formal models using advanced modeling tools. The formalization of sketches into actual model components can be time-consuming, error-prone, and laborious. To address these shortcomings, we present SkeMo, an environment for real-time model component generation from sketch-based inputs. We curated a sketch dataset of 3000 images of various class diagram components and implemented a powerful Convolution Neural Network to classify input sketches as model components. We integrated our sketch classiﬁer into an existing web-based model editor and added a touch interface to support sketch-based modeling. We evaluated the SkeMo environment in two ways: through ten-fold cross-validation of the image classiﬁer and collection of metrics and feedback from a 20-participant user study. Based on our results, sketch-based modeling demonstrates signiﬁcant promise as an intuitive interface that is both easy to use and allows for faster model creation among most users.",
        "keywords": [
            "model-driven software engineering",
            "machine learning",
            "deep neural network",
            "convolution neural network",
            "image recognition",
            "sketch recognition",
            "class diagrams",
            "classiﬁers",
            "interface design",
            "touch interface",
            "collaborative modeling",
            "assistive modeling",
            "user studies"
        ],
        "authors": [
            "Alisha Sharma Chapai and Eric J. Rapos"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a173/248000a173.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Digital Twins for Cyber-Biophysical Systems: Challenges and Lessons Learned",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Digital twinning is gaining popularity in domains outside of traditional engineered systems, including cyber-physical systems (CPS) with biological modalities, or cyber-biophysical systems (CBPS) in short. While digital twinning has well-established practices in CPS settings, it raises special challenges in the context of CBPS. In this paper, we identify such challenges and lessons learned through an industry case of a digital twin for CBPS in controlled environment agriculture.",
        "keywords": [
            "controlled environment agriculture",
            "industry",
            "model-driven",
            "report",
            "simulation"
        ],
        "authors": [
            "Istvan David",
            "Pascal Archambault",
            "Quentin Wolak",
            "Cong Vinh Vu",
            "Timoth´e Lalonde",
            "Kashif Riaz",
            "Eugene Syriani",
            "Houari Sahraoui"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a001/248000a001.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z004/248000z004.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Rapid-Prototyping and Early Validation of Software Models through Uniform Integration of Hardware",
        "submission-date": "2023/09",
        "publication-date": "2023/09",
        "abstract": "Model-driven software engineering (MDSE) uses software models to make the complexity of cyber-physical and mechatronic systems (CPMS) manageable. For the validation of CPMS software models, closed-loop simulations are typically used. Since the system’s environment is part of the simulation, the software model is directly affected by the surroundings, which enables a more realistic evaluation. In early development phases, the expected target hardware platform for these software models is usually not considered, although such CPMS have a strong hardware dependency. This paper outlines a novel approach to couple these software models with hardware systems to improve the quality of these models and shorten the development cycle. The presented method allows the evaluation of functional and non-functional requirements. For this, a new in-the-loop concept is introduced where the hardware access is transparently performed using a remote procedure call mechanism. Moreover, the achieved modeling language and tool independence makes the novel approach suitable for various applications. The provided evaluation is based on two distinct modeling languages and tools to demonstrate the feasibility and performance of the new concept.",
        "keywords": [
            "model-driven software engineering",
            "model-in-the-loop",
            "cyber-physical systems",
            "remote procedure call"
        ],
        "authors": [
            "Michael Uelschen\nMarco Schaarschmidt\nJannis Budde"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a250/248000a250.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Proceedings ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems MODELS 2023",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z003/248000z003.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "184\n2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS)",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a184/248000a184.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Acher",
            "Mathieu\nAlam",
            "Omar\nAlmerge",
            "Nicolas\nAnnable",
            "Nicholas\nArchambault",
            "Pascal\nAtlee",
            "Joanne M.\nBagherzadeh",
            "Mojtaba\nBarais",
            "Olivier\nBarat",
            "Souvik\nBarkowsky",
            "Matthias\nBeermann",
            "Laura\nBenzarti",
            "Imen\nBerger",
            "Bernhard J.\nBoll",
            "Alexander\nBork",
            "Dominik\nBudde",
            "Jannis\nCabot",
            "Jordi\nChen",
            "Boqi\nChen",
            "Kua\nChen",
            "Xiang\nChiang",
            "Thomas\nChowdhury",
            "Shafiul Azam\nChrszon",
            "Philipp\nClarisó",
            "Robert\nClavel",
            "Manuel\nCombemale",
            "Benoit\nCsallner",
            "Christoph\nDarif",
            "Ikram\nDavid",
            "Istvan\nde Lara",
            "Juan\nDíez",
            "Pablo\nDingel",
            "Juergen\nDurá",
            "Carlos\nDutta",
            "Jaya\nEl Boussaidi",
            "Ghizlane\nFelderer",
            "Michael\nFischer",
            "Philipp M.\nFoures",
            "Damien\nGarhewal",
            "Bharat\nGerndt",
            "Andreas\nGiese",
            "Holger\nGogolla",
            "Martin\nGuerra",
            "Esther\nHamann",
            "Arne\nHebig",
            "Regina\nHeldal",
            "Rogardt\nHendriks",
            "Dennis\nHernández López",
            "José Antonio\nHosseinibaghdadabadi",
            "Mohsen\nIovino",
            "Ludovico\nJézéquel",
            "Jean-Marc\nJongeling",
            "Robbert\nJovanovic",
            "Ana\nKahani",
            "Nafiseh\nKehrer",
            "Timo\nKienzle",
            "Jörg\nKotte",
            "Oliver\nKpodjedo",
            "Sègla\nKulkarni",
            "Vinay\nLalonde",
            "Timothé\nLawford",
            "Mark\nLima",
            "Keila\nMartínez-Lasaca",
            "Francisco\nMaurer",
            "Paulina\nMussbacher",
            "Gunter\nN. Damasceno",
            "Carlos Diego\nNguyễn",
            "Hoàng\nNistala",
            "Sri Harsha\nOortwijn",
            "Wytse"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a331/248000a331.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Leveraging modeling concepts and techniques to address challenges in network management",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Managing a large enterprise network is a challenging task that involves configuring and monitoring a large number of networking devices from different vendors. To simplify network management, modeling techniques have been extensively applied to model network configurations and monitoring data. The most recent proposed solution in this context are OpenConﬁg models, which enable vendor-neutral automation. However, adopting networking models requires significant effort and cooperation from various stakeholders. The focus of this paper is to explore the challenges associated with adopting networking models, specifically OpenConﬁg models, from three primary viewpoints: network engineers, internet service/content providers, and networking software/hardware vendors. We also discuss possible solutions via application of software modeling techniques to aid in the successful adoption of networking models.",
        "keywords": [
            "OpenConﬁg",
            "YANG",
            "Model-based Network Management",
            "NETCONF",
            "gNMI"
        ],
        "authors": [
            "Naﬁseh Kahani",
            "Mojtaba Bagherzadeh",
            "Reza Ahmadi",
            "Juergen Dingel"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a055/248000a055.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Digital Twins for Cyber-Biophysical Systems: Challenges and Lessons Learned",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Istvan David",
            "Pascal Archambault",
            "Quentin Wolak",
            "Cong Vinh Vu",
            "Timothé Lalonde",
            "Kashif Riaz",
            "Eugene Syriani",
            "Houari Sahraoui"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z005/248000z005.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Marine Data Observability using KPIs: An MDSE Approach",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "The 2023 climate change report states that the current temperature rise has led to recurring and hazardous weather events, devastating communities and the planet. Ocean observation systems and marine data generated by them are crucial for predicting these extreme events, understanding the ecosystem states, and regulating marine industries. Many regional and global initiatives have been supporting the collection and sharing of more data, filling gaps in ocean observation. However, some challenges can impact the quality of marine data at different points of data delivery pipelines: from acquisition and transmission at the Internet-of-Underwater-Things (IoUT) level up to storage and sharing. IoUT devices can have challenges due to limited battery, rough underwater terrain, error-prone wireless underwater communication, or low communication bandwidth to the cloud. Thus, mechanisms must be put in place to allow monitoring of data quality throughout the delivery pipeline, to optimize the usage of data and improve decision-making based on the data. This study explores observation of marine data quality on a data platform using Key Performance Indicators (KPIs). We have created a model of the platform and specified KPIs. Both are fulfilled by platform-collected data quality metrics, with the purpose to infer the state of the data in the platform over different periods. Our results show that the model-based implementation is able to function as a semantic translator between a metric monitoring toolkit and the platform objectives, integrating it into an observable subsystem for the overall middleware data platform.",
        "keywords": [
            "data observability",
            "data quality",
            "smart ocean",
            "MDSE"
        ],
        "authors": [
            "Keila Lima",
            "Ludovico Iovino",
            "Maria Teresa Rossi",
            "Rogardt Heldal",
            "Tosin Daniel Oyetoyan",
            "Martina De Sanctis"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a024/248000a024.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Sensemaking Strategies: Exploiting Meta-Model Patterns to Understand Large Models",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "The increasing popularity of model-based and low-code platforms has raised the need to understand large models – especially in industrial settings. However, current approaches mainly rely on graph-based visual metaphors, which do not scale well with large model sizes. To address this issue, we introduce model sensemaking strategies: purposeful model visualisations based on alternative visual metaphors. We define them as reusable patterns that yield tailored visualisations when applied to meta-models. This paper presents a catalogue of domain-specific and domain-agnostic sensemaking strategies, and a recommender that suggests suitable strategies for a given meta-model. To showcase the framework’s applicability, we have implemented some of these strategies in Dandelion, an industrial, low-code graphical language workbench for the cloud. Using this platform, we have evaluated the effectiveness of the strategies to visualise large industrial models by the UGROUND company.",
        "keywords": [
            "model sensemaking strategies",
            "large model visualisation",
            "model-driven engineering",
            "low-code platforms"
        ],
        "authors": [
            "Francisco Mart´ınez-Lasaca",
            "Pablo D´ıez",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a261/248000a261.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incremental Model Transformations with Triple Graph Grammars for Multi-version Models",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Like conventional software projects, projects in model-driven software engineering require adequate management of multiple versions of development artifacts, importantly allowing living with temporary inconsistencies. In previous work, we have introduced multi-version models for model-driven software engineering, which allow checking well-formedness and finding merge conflicts for multiple versions of a model at once. However, also for multi-version models, situations where different artifacts, that is, different models, are linked via automatic model transformations have to be handled.\n\nIn this paper, we propose a technique for jointly handling the transformation of multiple versions of a source model into corresponding versions of a target model, which enables the use of a more compact representation that may afford improved execution time of both the transformation and further analysis operations. Our approach is based on the well-known formalism of triple graph grammars and the aforementioned encoding of model version histories called multi-version models. In addition to batch transformation of an entire model version history, the technique also covers incremental synchronization of changes in the framework of multi-version models.\n\nWe show the correctness of our approach with respect to the standard semantics of triple graph grammars and conduct an empirical evaluation to investigate the performance of our technique regarding execution time and memory consumption. Our results indicate that the proposed technique affords lower memory consumption and may improve execution time for batch transformation of large version histories, but can also come with computational overhead in unfavorable cases.",
        "keywords": [
            "Multi-version Models",
            "Triple Graph Grammars",
            "Incremental Model Transformation"
        ],
        "authors": [
            "Matthias Barkowsky",
            "Holger Giese"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a296/248000a296.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-Driven Prompt Engineering",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Generative artificial intelligence (AI) systems are capable of synthesizing complex content such as text, source code or images according to the instructions described in a natural language prompt. The quality of the output depends on crafting a suitable prompt. This has given rise to prompt engineering, the process of designing natural language prompts to best take advantage of the capabilities of generative AI systems.\nThrough experimentation, the creative and research communities have created guidelines and strategies for creating good prompts. However, even for the same task, these best practices vary depending on the particular system receiving the prompt. Moreover, some systems offer additional features using a custom platform-speciﬁc syntax, e.g., assigning a degree of relevance to speciﬁc concepts within the prompt.\nIn this paper, we propose applying model-driven engineering to support the prompt engineering process. Using a domain-speciﬁc language (DSL), we deﬁne platform-independent prompts that can later be adapted to provide good quality outputs in a target AI system. The DSL also facilitates managing prompts by providing mechanisms for prompt versioning and prompt chaining. Tool support is available thanks to a Langium-based Visual Studio Code plugin.",
        "keywords": [
            "prompt engineering",
            "model-driven engineering",
            "domain-speciﬁc language",
            "generative AI",
            "large language models"
        ],
        "authors": [
            "Robert Clarisó\nJordi Cabot"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a047/248000a047.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic Security-Flaw Detection\nReplication and Comparison",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Threat Modeling is an essential step in secure\nsoftware system development. It is a manual, attacker-centric\napproach for identifying architecture-level security flaws during\nthe planning phase of software systems. In the last years,\nacademia presented two methods to automate threat detection\nthat do not focus on a particular class of security flaws but offer\ngeneral-purpose means to describe security flaws.\nThis paper compares both approaches on an equal data\nfoundation that was published with one of the approaches. There-\nfore, we specify a model-to-model transformation for converting\nbetween the approaches to allow this conceptual replication.\nAdditionally, we provide security flaw patterns for the second\napproach that any user of the approach can use. We then\nreplicate the detection with the second security flaw detection\napproach to compare both approaches. We focus our analysis\non differences between automation-specific and approach-specific\nfinding misclassifications on identifying whether some flaws are\nharder to find with an automated approach than others.\nWe find that missed flaws usually stem from the imprecise\ndefinition of security flaws, while incorrectly identified flaws\nare approach-dependent. Despite that, both approaches perform\nsimilarly. The knowledge base, the transformation scripts and the\nevaluation script are publicly available to support the research\ncommunity.",
        "keywords": [
            "threat modeling",
            "dataflow diagrams",
            "security\nflaw detection",
            "automation",
            "interoperability",
            "comparison"
        ],
        "authors": [
            "Bernhard J. Berger\nChristina Plump"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a084/248000a084.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-driven and Template-based Approach for Requirements Speciﬁcation",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Requirements speciﬁcation and veriﬁcation play an important role in the certiﬁcation of safety-critical software (SCS). These activities are costly and error-prone because SCS exhibit a high number of requirements and most SCS manufacturers are still using natural language to specify these requirements. On one hand, natural language can introduce ambiguity and inconsistency. On the other hand, formal languages add an overhead to the requirements speciﬁcation because of their complexity. Controlled Natural Languages (CNLs) ﬁll these gaps by offering a middle-ground solution, although not yet well adopted by the industry. In this paper, we introduce an approach that combines CNLs and model-driven engineering (MDE) for requirements speciﬁcation. The approach was proposed to support an industrial partner in the certiﬁcation process of a SCS. Our approach uses templates and relies on two types of models: models that specify the templates, and a model of the domain of the system at hand. Using models of the templates enables to automate some requirements analysis tasks. Using a domain model allows the auto-completion and veriﬁcation of requirements speciﬁed using the templates. We implemented the approach and validated it using three case studies and more than a thousand requirements. We observed that our approach and underlying templates are applicable across domains and that the templates yield requirements with better quality in terms of necessity, ambiguity, completeness, singularity, and veriﬁability.",
        "keywords": [
            "Model-driven engineering",
            "Requirements engineering",
            "Requirements speciﬁcation",
            "Controlled natural language",
            "Requirement templates",
            "Safety critical systems",
            "Domain models"
        ],
        "authors": [
            "Ikram Darif",
            "Cristiano Politowski",
            "Ghizlane El Boussaidi",
            "Imen Benzarti and S`egla Kpodjedo"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a239/248000a239.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Proceedings ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems MODELS 2023",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z001/248000z001.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "gLTSdiff: A Generalized Framework for Structural Comparison of Software Behavior",
        "submission-date": "2023/00",
        "publication-date": "2023/00",
        "abstract": "State machine models, such as labeled transition systems and (extended) finite automata, can be structurally compared, for instance to find potential behavioral regressions in new software versions, to evaluate the accuracy of different model learning algorithms, and to fingerprint software for security applications. The state-of-the-art LTSDiff structural comparison algorithm has limited assumptions, making it broadly applicable. However, representation-specific information is not taken into account, requiring adaptations to prevent sub-optimal or even invalid results. We propose gLTSdiff, which generalizes and extends LTSDiff, allowing a wide range of state machine models to be compared, by recursively comparing the structure of state and transition labels. Additional challenges that we faced while applying LTSDiff in industrial practice are also addressed by gLTSdiff, as it rewrites undesired difference patterns, supports comparison of any number of input models, and allows for an effort/quality trade-off. gLTSdiff is implemented as an extensible open source library for structural model comparison. Using multiple large-scale industrial and open source case studies, we evaluate both its practical value and its various improvements.",
        "keywords": [],
        "authors": [
            "Dennis Hendriks",
            "Wytse Oortwijn"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a285/248000a285.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Not found",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z017/248000z017.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Developing and Operating GLSP-based Web Modeling Tools: Lessons Learned from BIGUML",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "The development of web-based modeling tools still poses significant challenges for developers. The Graphical Language Server Platform (GLSP) reduced some of these challenges by providing the necessary frameworks to efficiently create web modeling tools. However, more knowledge and experience are required regarding developing GLSP-based web modeling tools. This paper discusses the challenges and lessons learned after working with GLSP and realizing several GLSP-based modeling tools. More concretely, experiences, concepts, steps to be followed to develop and operate a GLSP-based web modeling tool, and the advantages and disadvantages of working with GLSP are discussed. As a proof of concept, we will report on the realization of a GLSP-based UML editor called BIGUML. Through BIGUML, we show that our procedure and the reference architecture we developed resulted in a scalable and flexible GLSP-based web modeling tool. The lessons learned, the procedural approach, the reference architecture, and the critical reflection on the challenges and opportunities of using GLSP provide valuable insights to the community and shall ease the decision of whether or not to use GLSP for future tool development projects.",
        "keywords": [
            "Modeling tool",
            "GLSP",
            "web modeling",
            "lessons learned",
            "LSP",
            "eclipse"
        ],
        "authors": [
            "Haydar Metin",
            "Dominik Bork"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a129/248000a129.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Variability-aware Neo4j for Analyzing a Graphical Model of a Software Product Line",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "Comprehensive analysis of a software product line (SPL) is expensive because the number of products to be analyzed is exponential in the number of the SPL’s features. To compensate, we analyze a model of the SPL rather than the source code, thereby reducing the size of the artifact under analysis. In this paper, we facilitate SPL analysis by lifting the Neo4j query engine to apply to an SPL model, so that a Neo4j query returns variability-aware results that cover all the SPL’s products. We used the lifted Neo4j to analyze five nontrivial SPLs (with respect to dataflows, control-flows, component interactions, etc.) and found that the overhead for returning variability-aware results for the full SPL, versus the results for just one product, ranges from 1.88% to 456%. In comparison to related work V-Souffl´e (a lifted Datalog engine), lifted Neo4j is able to report complete path results whereas V-Souffl´e reports only endpoints of paths. When both analyzers report the same results (e.g., endpoints of paths), lifted Neo4j is usually more efficient.",
        "keywords": [
            "Graphical software models",
            "Software product line models",
            "Lifted analyses",
            "Neo4j"
        ],
        "authors": [
            "Xiang Chen",
            "David R. Cheriton",
            "Joanne M. Atlee"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a307/248000a307.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Timing-Aware Software-in-the-Loop Simulation of Automotive Applications with FMI 3.0",
        "submission-date": "2023/10",
        "publication-date": "2023/10",
        "abstract": "In embedded real-time automotive systems, Software-in-the-Loop (SiL) represents the state-of-the-art for testing software code at design time. SiL environments focus on testing the functional software behaviour, typically neglecting the timing non-idealities introduced by the target embedded hardware. This separation of concerns prevents a credible virtual testing and validation of time-critical systems. In this paper, we propose an industry-viable modular co-simulation architecture based on the Functional Mock-up Interface (FMI) 3.0 standard, coupling timing simulation and functional simulations to obtain a timing-aware functional simulation of automotive applications. The proposed method enables an evaluation of the behaviour of functional software on the target hardware earlier in the development process. Also, our solution allows for the co-simulation of submodels generated with different tools, with minimal modifications required. Ultimately, this approach enables front-loading of development efforts, leading to reduced costs and time to market. A case study is presented to show a detailed examination of the proposed architecture.",
        "keywords": [
            "Software-in-the-Loop",
            "timing-aware simulation",
            "Functional Mock-up Interface",
            "Discrete-Event Co-Simulation"
        ],
        "authors": [
            "Srivathsan Ravi",
            "Laura Beermann",
            "Oliver Kotte",
            "Paolo Pazzaglia",
            "Mythreya Vinnakota",
            "Dirk Ziegenbein",
            "Arne Hamann"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a062/248000a062.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applicability of Model Checking for Verifying Spacecraft Operational Designs",
        "submission-date": "2023/03",
        "publication-date": "2023/03",
        "abstract": "Guaranteeing safety and correctness is one of the main objectives during the development of space systems. This is a challenging task, since many different engineering disciplines are involved in the development and the constituent parts of a spacecraft are highly interconnected and interdependent. Increasingly, formal methods such as model checking are applied to verify safety-critical parts of spacecraft designs and also implementation, since they may prove the absence of design errors. Generally, a major challenge for adopting model checking into the design process is its scalability. Usually, the whole state space of a system, which grows exponentially with, e.g., the number of parallel processes, must be explored.\nIn this paper, we consider operational designs of spacecraft as they may occur during early development phases and systematically evaluate the scalability of model checking for verifying such models. For this, we created an arbitrarily scalable operational design describing the mode management of a satellite. Transformations of the models into the modeling languages of different model-checking tools enables a comparative scalability study of various model-checking algorithms. The evaluation shows promising results for symbolic model-checking approaches. A comparatively low analysis time and memory usage suggest that model checking for early operational designs can be incorporated into existing design processes.",
        "keywords": [
            "Aerospace",
            "Formal Models",
            "Formal Methods",
            "Model Checking"
        ],
        "authors": [
            "Philipp Chrszon",
            "Paulina Maurer",
            "George Saleip",
            "Sascha M¨uller",
            "Philipp M. Fischer",
            "Andreas Gerndt",
            "Michael Felderer"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a206/248000a206.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Uncertainty-aware consistency checking in industrial settings",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "In this work, we explore how we can assist engineers in managing, in a lightweight way, both consistency and design uncertainty during the creation and maintenance of models and other development artifacts. We propose annotating degrees of doubt to indicate design uncertainties on elements of development artifacts. To combine multiple opinions, we use the fusion operators of subjective logic. We show how these annotations can be used to identify, prioritize, and resolve uncertainty and inconsistency. To do so, we identify the types of design uncertainty and inconsistency to be addressed in two concrete industrial settings and show a prototype implementation of our approach to calculating the uncertainty and inconsistency in these cases. We show how making design uncertainty explicit could be used to tolerate inconsistencies with high uncertainty, prioritize inconsistencies with low associated uncertainty, and uncover previously hidden potential inconsistencies.",
        "keywords": [
            "Uncertainty",
            "Consistency management",
            "Model-Based Development"
        ],
        "authors": [
            "Robbert Jongeling",
            "Antonio Vallecillo"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000a073/248000a073.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MODELS 2023",
        "submission-date": "Not found",
        "publication-date": "Not found",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Mohammed Rizwan Ali",
            "Oussama Ben Sghaier",
            "Paul Bittner",
            "Beatriz Cabrero-Daniel",
            "Robert Clarisó",
            "Renzo Degiovanni",
            "Khanh-Hoang Doan",
            "Flo Drux",
            "Sebastian Ehmes",
            "Josselin Enet",
            "Lars Fritsche",
            "Sandra Greiner",
            "Hendrik Göttmann",
            "Liping Han",
            "Alexander Hellwig",
            "José Antonio Hernández López",
            "Matthieu Jimenez",
            "Aton Kamanda",
            "Hendrik Kausch",
            "Faezeh Khorram",
            "Yves Kirschner",
            "Max Kratz",
            "Tim Kräuter",
            "Lars König",
            "Louis-Edouard Lafontant",
            "Sami Lazreg",
            "Alexander Lieb",
            "Lukas Netz",
            "Bentley Oakes",
            "Mathias Pfeiffer",
            "Sreedhar Reddy",
            "Lucas Sakizloglou",
            "Alexander Schultheiss",
            "Mazyar Seraj",
            "Karsten Sohr",
            "Max Stachon"
        ],
        "file_path": "data/models/models23/MODELS2023-5xtY4b9zCM5QafXrwgQTzw/248000z015/248000z015.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EditQL: A Textual Query Language for Evolving Models",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Context: Technically sophisticated systems are the result of the\njoint work of several domain experts. However, the more people\ncollaborate, the more important it becomes to make the model evo-\nlution and its single edit operations accessible and comprehensible\nfor involved stakeholders. Objective: We developed the textual and\nsemantic aware query language EditQL. It enables domain experts\nto search for model versions, changes, and causing edit operations\nwithin a model’s edit history. Based on an operation-based ver-\nsioning system, the query language covers both edit operations\nand all model states. Method: We systematically elaborate the re-\nquirements of a query language for edit histories. Based on this,\nwe present a DSL integrated into an existing modeling tool. We\nconducted a mixed-methods usability study with 15 participants\nin which they had to answer various questions about a model’s\nevolution using EditQL. Results: All participants agreed on the\nusefulness of the query language, particularly the possibility of\nquerying for semantic changes in the model. The measured System\nUsability Scale (SUS) scores range from OK to good. In addition,\nwe identified a set of possible improvements. Conclusion: The study\nconfirmed that EditQL and the underlying concepts are suitable\ntools to help domain experts understand the evolution of a model.",
        "keywords": [
            "versioning",
            "operation-based",
            "query language",
            "model evolution",
            "usability study",
            "collaboration"
        ],
        "authors": [
            "Jakob Pietron",
            "Benedikt Jutz",
            "Alexander Raschke",
            "Matthias Tichy"
        ],
        "file_path": "data/models/models24/3640310.3674101.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Automated Test Scenario Generation for Assuring COLREGs Compliance of Autonomous Surface Vehicles",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "International maritime traffic is controlled by collision-avoidance regulations (COLREGs) with 41 standardized rules describing how a vessel should navigate in the proximity of other vessels. Since some rules can be overridden by human judgement when resolving critical encounters of vessels, justifying COLREGs compliance has become a significant challenge in the increasing presence of autonomous surface vehicles (ASVs) operated without (or with only remote) human control. This paper provides a high-level framework and long-term research agenda towards the automated synthesis of test scenarios to assure COLREGs compliance for ASVs by exploiting various model-driven engineering techniques. By adapting ideas from testing self-driving cars, we envisage a multi-layered test scenario generation approach involving functional, logical and concrete scenarios. In the current paper, we demonstrate how functional scenarios of COLREGs situations between given vessels can be precisely formalized by using metamodels, domain-specific graph models and first-order logic graph constraints. By using automated model generation techniques, we derive a complete set of functional-level test scenarios, which includes all possible COLREGs situations that may arise between given vessels. As initial result, we provide several dangerous situations involving only three vessels where a potential collision may occur even when all vessels follow the COLREGs, which showcases that some COLREGs rules need further clarification for the safe regulation of ASVs.",
        "keywords": [
            "autonomous surface vehicles",
            "COLREGs",
            "test scenario generation",
            "consistent model generation",
            "qualitative abstraction"
        ],
        "authors": [
            "Ulf Kargén",
            "Dániel Varró"
        ],
        "file_path": "data/models/models24/3640310.3674098.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Languages for Automotive Digital Twins: A Survey Among the German Automotive Industry",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "The demand for digital twins and suitable modeling techniques in the automotive industry is increasing rapidly. Yet, there is no common understanding of digital twins in automotive, nor are there modeling techniques established to create automotive digital twins. Recent studies on digital twins focus on the analysis of the literature on digital twins for automotive or in general and, thus, neglect the industrial perspective of automotive practitioners. To mitigate this gap between scientific literature and the industrial perspective, we conducted a questionnaire survey among experts in the German automotive industry to identify i) the desired purposes for and capabilities of digital twins and ii) the modeling techniques related to engineering and operating digital twins across the phases of automotive development. To this end, we contacted 189 members of the Software-Defined Car research project and received 96 responses. The results show that digital twins are considered most useful in the usage and support phase of automotive development, representing vehicles as-operated. Moreover, simulation models, source code, and business process models are currently considered the most important models to be integrated into a digital twin alongside the associated, established tools.",
        "keywords": [
            "modeling languages",
            "digital twins",
            "automotive",
            "survey"
        ],
        "authors": [
            "Dominik Fuchß",
            "Thomas Kühn",
            "Dirk Neumann",
            "Christer Neimöck",
            "Jérôme Pfeiffer",
            "Robin Liebhart",
            "Christian Seiler",
            "Anne Koziolek",
            "Andreas Wortmann"
        ],
        "file_path": "data/models/models24/3640310.3674100.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Meta-Modelling Kindness",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Kindness is a psycho-social phenomenon that is also recognized as\nan important pro-social behaviour. The use of digital technology\nprovides opportunities to promote kindness in various ways, such\nas in social media campaigns and online communities. In princi-\npale, software engineers are well positioned to develop automated\nsystems that can facilitate software-mediated kindness. However,\nin practice, incorporating kindness concerns explicitly in the de-\nvelopment and use of software systems is challenging: kindness is\nhighly context dependent, affected by a range of factors such as\nintentions and opportunity.\n\nIn this paper, we explore systematic ways in which kindness\nconcerns can be considered by software engineers. We propose a\nnovel meta-model that captures essential entities and relations as-\nociated with kindness. The meta-model enables the representation\nof possible instances or opportunities for performing acts of kind-\nness, by considering the actors involved (such as giver, receiver, and\nobserver), their psychological and social attributes that promote\nkindness (such as emotional states and social relatedness), the acts\nneeded to fulfil kindness opportunities (such as motivation, ability,\nand timeliness), and other contextual factors (such as location and\ntime). Our meta-model is demonstrated through two software ap-\nplication scenarios that enable charitable donations and kindness in\nbusiness. Overall, our proposal offers a first, tentative, but concrete\nstep towards enabling kind computing, and promoting kindness in\nsoftware systems.",
        "keywords": [
            "Kindness",
            "Meta-Modelling",
            "Software Engineering",
            "Kind Computing"
        ],
        "authors": [
            "Faeq Alrimawi",
            "Bashar Nuseibeh"
        ],
        "file_path": "data/models/models24/3640310.3674095.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "AlloyASG: Alloy Predicate Code Representation as a Compact Structurally Balanced Graph",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Writing declarative models has numerous benefits, ranging from automated reasoning and correction of design-level properties to automated testing and debugging of system implementations. Unfortunately, the model itself needs to be correct to gain these benefits. Alloy is a commonly used modeling language that has several existing efforts to repair faulty models automatically. Currently, these efforts are search-based methods that use an Abstract Syntax Tree (AST) representation of the model and do not scale, as ASTs suffer from exponential growth in their data size due to duplicate nodes. To address this issue, we introduce a novel code representation schema, Complex Structurally Balanced Abstract Semantic Graph (CSBASG), which represents code as a complex-weighted directed graph that lists a semantic element as a node in the graph and ensures its structural balance for almost finitely enumerable code segments. We evaluate the efficiency of our CSBASG representation for Alloy models in terms of it’s compactness compared to ASTs, and we explore if a CSBASG can ease the process of comparing two Alloy predicates. Lastly, we identify several future applications of CSBASG, including Alloy code generation and automated repair.",
        "keywords": [],
        "authors": [
            "Guanxuan Wu",
            "Allison Sullivan"
        ],
        "file_path": "data/models/models24/3640310.3674088.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mutation Testing of Java Bytecode: A Model-Driven Approach",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Mutation testing is an approach to checking the robustness of test suites. The program code is slightly changed by mutations to inject bugs. A test suite is robust enough if it finds such bugs. Mutation testing tools typically integrate sets of mutation operators such as, for example, swapping arithmetic operators; modern tools typically work with compiled code such as Java bytecode. The mutations must be defined in such a way that the mutated program can still be loaded and executed. The results of mutation tests depend directly on the possible mutations. More advanced mutations and even domain-specific mutations can pose another challenge to the test suite. Since the classical, non-model-based mutation testing tools do not support the specification of advanced mutation operators well, we propose a model-driven approach where mutations of Java bytecode can be flexibly defined by model transformation. Our approach also provides advanced mutation operators for modifying object-oriented structures, Java-specific properties and API method calls, making it the only mutation testing tool for Java bytecode that supports such mutations. To further improve the effectiveness of mutation testing, mutants are generated only for bytecode that is covered by tests. Our approach is implemented in the MMT tool. It has been evaluated against non-model-based mutation testing tools for its ability to generate mutants close to real bugs. The experiments make use of Defects4J, a well-established collection of real-world Java projects with reproducible bugs.",
        "keywords": [
            "Mutation testing",
            "Java bytecode",
            "Model transformation"
        ],
        "authors": [
            "Christoph Bockisch",
            "Deniz Eren",
            "Sascha Lehmann",
            "Daniel Neufeld",
            "Gabriele Taentzer"
        ],
        "file_path": "data/models/models24/3640310.3674103.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EpiMDE: A Model-Driven Engineering Platform for Epidemiological Modeling",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Modeling is a critical step in studying epidemics. It allows us to better understand and predict the progression of a disease, design interventions such as vaccination, and assess their impact. Current epidemics are modeled using compartmental and mathematical models. While these are enough to achieve the primary goal of modeling, they suffer from shortcomings with respect to communicating and sharing the models, comparison and validation, and reproducibility. In this work, we propose the use of model-driven software engineering principles, to better represent disease models and facilitate the model management operations. We present an extensible metamodel for epidemics and an integrated development environment to allow epidemiologists to create and manage their models and simulations. We present the use of our platform on a COVID-19 model, where we show that the resulting model is more concise yet structurally and functionally equivalent to the original.",
        "keywords": [
            "epidemiological modeling",
            "integrated development environment",
            "reproducibility",
            "extensibility",
            "metamodeling"
        ],
        "authors": [
            "Bruno Curzi-Laliberté",
            "Marios Fokaefs",
            "Michalis Famelis",
            "Mohammad Hamdaqa"
        ],
        "file_path": "data/models/models24/3640310.3674104.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Requirement-Driven Generation of Distributed Ledger Architectures",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Cross-organizational, blockchain-based distributed ledger networks in general, and those based on Hyperledger Fabric in particular, have an architecture which can be adapted to specific application requirements. However, network design can be a particularly challenging task, as the connection between architectural and deployment decisions and extra-functional properties can be subtle and the requirements may contradict each other, requiring trade-offs. In this paper, we propose a model-based distributed ledger architecture design approach which enables expert exploration of design options. We capture key requirements and define architecture fragments using partial modelling. We enumerate qualitatively different architectural candidates by graph generation. We evaluate and rank order candidates in logic solver tooling. As a result, our approach provides generative architectures for distributed ledger networks by enabling efficient exploration of design alternatives.",
        "keywords": [
            "Model Generation",
            "Partial Modelling",
            "Blockchain",
            "HyperLedger Fabric",
            "Design-space Exploration",
            "Generative Architecture"
        ],
        "authors": [
            "Noor Mohammed Sabr Al-Gburi",
            "András Földvári",
            "Kristóf Marussy",
            "Oszkár Semeráth",
            "Imre Kocsis"
        ],
        "file_path": "data/models/models24/3640310.3674097.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tree-Based versus Hybrid Graphical-Textual Model Editors: An Empirical Study of Testing Specifications",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Tree-based model editors and hybrid graphical-textual model editors have advantages and limitations when editing domain mod-els. Data is displayed hierarchically in tree-based model editors, whereas hybrid graphical-textual model editors capture high-level domain concepts graphically and low-level domain details textu-ally. We conducted an empirical user study with 22 participants to evaluate the implicit assumption of system modellers that hybrid notations are superior, and to investigate the tradeoffs between the default EMF-based tree model editor and a Sirius/Xtext-based hybrid model editor. The results of the user study indicate that users largely prefer the hybrid editor and are more confident with hybrid notations for understanding the meaning of conditions. Further-more, we found that the tree editor provided superior performance for analysing ordered lists of model elements, whereas activities requiring the comprehension or modelling of complex conditions were carried out faster through the hybrid editor.",
        "keywords": [
            "Hybrid Notations",
            "Model Editors",
            "Fuzz Testing",
            "Empirical Study"
        ],
        "authors": [
            "Ionut Predoaia",
            "James Harbin",
            "Simos Gerasimou",
            "Christina Vasiliou",
            "Dimitris Kolovos",
            "Antonio García-Domínguez"
        ],
        "file_path": "data/models/models24/3640310.3674102.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Towards Runtime Monitoring for Responsible Machine Learning using Model-driven Engineering",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Machine learning (ML) components are used heavily in many current software systems, but developing them responsibly in practice remains challenging. ‘Responsible ML’ refers to developing, deploying and maintaining ML-based systems that adhere to human-centric requirements, such as fairness, privacy, transparency, safety, accessibility, and human values. Meeting these requirements is essential for maintaining public trust and ensuring the success of ML-based systems. However, as changes are likely in production environments and requirements often evolve, design-time quality assurance practices are insufficient to ensure such systems’ responsible behavior. Runtime monitoring approaches for ML-based systems can potentially offer valuable solutions to address this problem. Many currently available ML monitoring solutions overlook human-centric requirements due to a lack of awareness and tool support, the complexity of monitoring human-centric requirements, and the effort required to develop and manage monitors for changing requirements. We believe that many of these challenges can be addressed by model-driven engineering. In this new ideas paper, we present an initial meta-model, model-driven approach, and proof of concept prototype for runtime monitoring of human-centric requirements violations, thereby ensuring responsible ML behavior. We discuss our prototype, current limitations and propose some directions for future work.",
        "keywords": [
            "Runtime monitoring",
            "Responsible ML",
            "Human-centric requirements",
            "Machine learning components",
            "Model-driven engineering"
        ],
        "authors": [
            "Hira Naveed",
            "John Grundy",
            "Chetan Arora",
            "Hourieh Khalajzadeh",
            "Omar Haggag"
        ],
        "file_path": "data/models/models24/3640310.3674092.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Text2VQL: Teaching a Model Query Language to Open-Source Language Models with ChatGPT",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "While large language models (LLMs) like ChatGPT has demonstrated impressive capabilities in addressing various software engineering tasks, their use in a model-driven engineering (MDE) context is still in an early stage. Since the technology is proprietary and accessible solely through an API, its use may be incompatible with the strict protection of intellectual properties in industrial models. While there are open-source LLM alternatives, they often lack the power of proprietary models and require extensive data fine-tuning to realize their full potential. Furthermore, open-source datasets tailored for MDE tasks are scarce, posing challenges for training such models effectively.\n\nIn this work, we introduce Text2VQL, a framework that generates graph queries captured in the VIATRA Query Language (VQL) from natural language specifications using open-source LLMs. Initially, we create a high-quality synthetic dataset comprising pairs of queries and their corresponding natural language descriptions using ChatGPT and VIATRA parser. Leveraging this dataset, we use parameter-efficient tuning to specialize three open-source LLMs, namely, DeepSeek Coder 1b, DeepSeek Coder 7b, and CodeLlama 7b for VQL query generation. Our experimental evaluation demonstrates that the fine-tuned models outperform the base models in query generation, highlighting the usefulness of our synthetic dataset. Moreover, one of the fine-tuned models achieves performance comparable to ChatGPT.",
        "keywords": [
            "large language model (LLM)",
            "model query language",
            "query generation",
            "VIATRA Query Language (VQL)",
            "ChatGPT"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Máté Földiák",
            "Dániel Varró"
        ],
        "file_path": "data/models/models24/3640310.3674091.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "true",
            "language": "VIATRA Query Language (VQL)"
        }
    },
    {
        "title": "A Comparative Analysis of Energy Consumption Between Visual Scripting models and C++ in Unreal Engine: Raising Awareness on the importance of Green MDD",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Video game engines are used in most modern video games because they simplify and speed up development. In addition, some of the most popular engines, such as Unreal Engine 5 (UE5), also integrate visual scripting tools. Visual scripting in UE5, through Blueprints, is a model-driven development approach that replaces text code, like C++, with a visual language of interconnected nodes representing functions and data flows, forming a flowchart-like logic diagram. This approach simplifies game development by abstracting complex code into intuitive, visual models, enabling creators to construct and iterate game components without extensive programming knowl-edge. Although Blueprint models usually decrease the complexity of implementing components, thus accelerating the development, they might lead to less energy-efficient runtime performance than C++. In this work, we evaluate the energy consumption of three relevant video game components (health system management, in-puts processing, and collections operations for an inventory), each implemented with Blueprint models and C++. The results show that the energy consumption per frame when using C++ is up to 48% lower than when using Blueprint models. The combination of artistic and technical profiles in video game developments has favoured the adoption of Blueprint models. However, there is a lack of works analyzing the energy consumption. Until this work, there was no evidence that the success of models for developing video games, like the one under study in this work, was accompanied by a cost in energy consumption for certain situations. Given the huge popularity of video games, this cost in energy might reach up to the equivalent of the energy consumption of 28 million European households.",
        "keywords": [
            "Energy consumption",
            "Video Games",
            "Green software",
            "Green Video Games",
            "Software sustainability",
            "Game Engines",
            "Unreal Engine",
            "Soft-ware Models",
            "Visual Scripting",
            "Blueprints",
            "C++",
            "Game Software Engineering"
        ],
        "authors": [
            "Javier Verón",
            "Carlos Pérez",
            "Coral Calero",
            "MªÁngeles Moraga",
            "Francisca Pérez",
            "Carlos Cetina"
        ],
        "file_path": "data/models/models24/3640310.3674099.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A DSL for Testing LLMs for Fairness and Bias",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose Lang-BiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach.",
        "keywords": [
            "Model-Driven Engineering",
            "Domain-Specific Language",
            "Testing",
            "Ethics",
            "Bias",
            "Red Teaming",
            "Large Language Models"
        ],
        "authors": [
            "Sergio Morales",
            "Robert Clarisó",
            "Jordi Cabot"
        ],
        "file_path": "data/models/models24/3640310.3674093.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "10 years of Model Federation with Openflexo: Challenges and Lessons Learned",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "In the context of complex system development, heterogeneous\nmodeling responds to the need to integrate several domains. This\nneed requires the use of the most appropriate formalism and tooling\nfor each domain to be efficient. Model federation promotes the\nsemantic interoperability of heterogeneous models by providing the\nmeans to reify correspondences between different model elements,\nadd custom behaviors and bridge the gap between technological\nspaces. As such, it can be used as an infrastructure to address many\ndifferent system engineering problems. This is what we have been\ndoing for over a decade, as part of a close collaboration between\na small software engineering startup and academia. This paper\nreports on this experience.\nConcretely, we discuss the context, ambitions, and challenges\nthat led to the inception of our practice of model federation, and we\npresent five use cases experiences, stemming from real industrial\nand academic needs, and elaborate on lessons learned. In addition,\nwe also report on challenges and lessons learned regarding the\ndevelopment and maintenance of a model-driven model federation\ntool, the Openflexo framework. Finally, we set up a road map for\nthe future of model federation and Openflexo.",
        "keywords": [
            "Model federation",
            "Model management",
            "Experience report"
        ],
        "authors": [
            "Jean-Christophe Bach",
            "Antoine Beugnard",
            "Joël Champeau",
            "Fabien Dagnat",
            "Sylvain Guérin",
            "Salvador Martínez"
        ],
        "file_path": "data/models/models24/3640310.3674084.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Partial Bidirectionalization of Model Transformation Languages",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "While most model-transformation languages in Model-Driven Engineering are unidirectional, bidirectionality is valuable when artifacts need two-way synchronization. Although several bidirectional transformation engines have been developed, their behavior is generally considered more difficult to formulate and predict compared to the unidirectional case. In the bidirectionalization approach, users write the forward direction of their transformations in the same unidirectional language they are used to, and obtain a system that (besides performing the complete forward transformation) can automatically propagate in the backward direction the target updates. When possible, full bidirectionalization is desirable, but far from trivial.\n\nIn this paper we propose a partial bidirectionalization approach, by partial compilation of a unidirectional language into a bidirectional language, and coupled execution of the two language engines. Forward transformation is still complete, whereas the target updates that can be back-propagated are deletions and modifications of a well-defined part of the target model. While the extent of the bidirectionalization depends on the two coupled systems, in this paper we provide a general combination scheme and we briefly discuss its well-behavedness. Then we use our technique to bidirectionalize the ATL model-transformation language on top of the GRoundTram bidirectional graph-transformation system.",
        "keywords": [
            "Model Transformation",
            "Bidirectional Transformation",
            "Bidirectionalization",
            "Runtime Interoperation",
            "Transformation Engines"
        ],
        "authors": [
            "Soichiro Hidaka and Massimo Tisi"
        ],
        "file_path": "data/models/models24/3640310.3674083.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL, GRoundTram"
        }
    },
    {
        "title": "Automated Derivation of UML Sequence Diagrams from User Stories: Unleashing the Power of Generative AI vs. a Rule-Based Approach",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "User stories are informal, non-technical descriptions of features from a user’s perspective that guide collaboration and iterative development in Agile projects. However, ambiguities in user stories can lead to miscommunication among stakeholders. Design models, such as UML sequence diagrams, are essential for enhancing communication, clarifying system behavior, and improving the development process. This paper presents an automated approach for generating behavioral models specifically sequence diagrams from natural language requirements expressed as user stories. We also investigate the effectiveness of a Large Language Model (LLM) in using generative AI for this task. By applying our approach and ChatGPT to two benchmark datasets with the same set of user stories, we generated corresponding sequence diagrams for comparison. Expert evaluations in Software Engineering reveal that our approach effectively produces relevant, simplified diagrams for straightforward user stories, whereas the LLM tends to create more complex diagrams that sometimes go beyond the simplicity of the original user stories.",
        "keywords": [
            "User Story",
            "Sequence Diagram",
            "Generative Model",
            "Large Language Model",
            "Model Generation",
            "Natural Language Processing",
            "Rule-based approach"
        ],
        "authors": [
            "Munima Jahan",
            "Mohammad Mahdi Hassan",
            "Reza Golpayegani",
            "Golshid Ranjbaran",
            "Chanchal Roy",
            "Banani Roy",
            "Kevin Schneider"
        ],
        "file_path": "data/models/models24/3640310.3674081.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "AutoMW: Model-based Automated Medical Writing",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Medical Writing is an art of writing scientific documents which includes regulatory and research-related content. To obtain approval for marketing new medicines, pharmaceutical companies are obligated to provide drug authorities with a huge volume of documents related to clinical trials. Creating these clinical trial documents is a time, effort, and skill-intensive process as the required information exists in fragmented form distributed across various information sources. To overcome these challenges in medical writing, we propose Automated Medical Writing tool (AutoMW).  AutoMW enables the digitalization of information from different sources of information using a meta-model-based approach and leverages these models for the automated generation of clinical trial documents as per the regulatory authority document templates. This paper describes the approach and illustrates its utility and efficacy in real-world clinical trial application of two use cases - breast cancer, and diabetes.",
        "keywords": [
            "MDE",
            "Medical Writing",
            "Automated Content Generation",
            "NLP",
            "Clinical Trial Documentation"
        ],
        "authors": [
            "Asha Rajbhoj",
            "Ajim Pathan",
            "Tanay Sant",
            "Vinay Kulkarni",
            "Padmalata Nistala",
            "Rajesh Pandey",
            "Sabarinathan Narasimhan",
            "Geetha Thiagarajan"
        ],
        "file_path": "data/models/models24/3640310.3674096.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing Automata Learning with Statistical Machine Learning: A Network Security Case Study",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Intrusion detection systems are crucial for network security. Verification of these systems is complicated by various factors, including the heterogeneity of network platforms and the continuously changing landscape of cyber threats. In this paper, we use automata learning to derive state machines from network-traffic data with the objective of supporting behavioural verification of intrusion detection systems. The most innovative aspect of our work is addressing the inability to directly apply existing automata learning techniques to network-traffic data due to the numeric nature of such data. Specifically, we use interpretable machine learning (ML) to partition numeric ranges into intervals that strongly correlate with a system’s decisions regarding intrusion detection. These intervals are subsequently used to abstract numeric ranges before automata learning. We apply our ML-enhanced automata learning approach to a commercial network intrusion detection system developed by our industry partner, RabbitRun Technologies. Our approach results in an average 67.5% reduction in the number of states and transitions of the learned state machines, while achieving an average 28% improvement in accuracy compared to using expertise-based numeric data abstraction. Furthermore, the resulting state machines help practitioners in verifying system-level security requirements and exploring previously unknown system behaviours through model checking and temporal query checking. We make our implementation and experimental data available online.",
        "keywords": [
            "State-machine learning; Intrusion detection; Decision trees; Denial of Service (DoS) attacks; Model checking; Query checking."
        ],
        "authors": [
            "Negin Ayoughi",
            "Shiva Nejati",
            "Mehrdad Sabetzadeh",
            "Patricio Saavedra"
        ],
        "file_path": "data/models/models24/3640310.3674087.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Advancing Domain-Specific High-Integrity Model-Based Tools: Insights and Future Pathways",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Rolls-Royce Control Systems supplies engine control and monitoring systems for aviation applications, and is required to design, certify, and deliver these with the highest level of safety assurance. To allow Rolls-Royce to develop these systems, which continue to increase in complexity, model-based techniques are now a critical part of the software development process. At MODELS 2021 we presented early experiences with using and maintaining a bespoke domain-specific modelling workbench based on open-source modelling technologies, including the Eclipse Modelling Framework (EMF), Xtext, Sirius, and Epsilon. In this paper, we build on our previous paper with further insights, new challenges and lessons learnt as we have advanced and matured our domain-specific solution. We also discuss our experiences with moving towards web based modelling tools based on open-source technologies including Sirius Web, Eclipse GLSP and Eclipse Theia. Rolls-Royce intends to use a selection of these technologies to build a web-based modelling workbench, which will be used to architect and integrate the software for future Rolls-Royce engine control and monitoring systems in a collaborative way.",
        "keywords": [
            "Domain specific languages",
            "component oriented architecture",
            "web based modelling",
            "GLSP",
            "EMF"
        ],
        "authors": [
            "Qurat ul ain Ali",
            "Dimitris Kolovos",
            "Antonio Garcia-Dominguez",
            "Michael Bennett",
            "Joe Newton",
            "Piotr Zacharzewski"
        ],
        "file_path": "data/models/models24/3640310.3674094.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Product Lines of Graphical Modelling Languages",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Modelling languages are essential in many disciplines to express\nknowledge in a precise way. Furthermore, some domains require\nfamilies of notations (rather than individual languages) that account\nfor variations of a language. Some examples of language families\ninclude those to define automata, Petri nets, process models or\nsoftware architectures. Several techniques have been proposed to\nengineer families of languages, but they often neglect the language’s\nconcrete syntax, especially if it is graphical.\nTo fill this gap, we propose a modular method to build product\nlines of graphical modelling languages. Language features are de-\nfined in modules, which comprise both the abstract and graphical\nconcrete syntax of the feature. A language variant is selected by\nchoosing a valid configuration of modules, from which the abstract\nand concrete syntax of the variant is synthesised. Our approach per-\nmits composition and overriding of graphical elements (e.g., symbol\nstyles, visualisation layers), the injection of pre-defined graphi-\ncal styles into language families (e.g., to obtain a high-intensity\ncontrast variant for accessibility), and the analysis of graphical con-\nflicts at the product line level. We report on an implementation atop\nEclipse/Sirius, and demonstrate its benefits by an evaluation which\nshows a substantial specification size reduction of our product line\nmethod with respect to a case-by-case specification approach.",
        "keywords": [
            "Software Language Engineering",
            "Model-driven Engineering",
            "Graphical Concrete Syntax",
            "Product Lines"
        ],
        "authors": [
            "Antonio Garmendia",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/models/models24/3640310.3674082.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "AI-Driven Consistency of SysML Diagrams",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Graphical modeling languages, expected to simplify systems analysis and design, present a challenge in maintaining consistency across their varied views. Traditional rule-based methods for ensuring consistency in languages like UML often fall short in addressing complex semantic dimensions. Moreover, the integration of Large Language Models (LLMs) into Model Driven Engineering (MDE) introduces additional consistency challenges, as LLM’s limited output contexts requires the integration of responses. This paper presents a new framework that automates the detection and correction of inconsistencies across different views, leveraging formally defined rules and incorporating OpenAI’s GPT, as implemented in TTool. Focusing on the consistency between use case and block diagrams, the framework is evaluated through its application to three case studies, highlighting its potential to significantly enhance consistency management in graphical modeling.",
        "keywords": [],
        "authors": [
            "Bastien Sultan",
            "Ludovic Apvrille"
        ],
        "file_path": "data/models/models24/3640310.3674079.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ModelMate: A recommender for textual modeling languages based on pre-trained language models",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Current DSL environments lack smart editing facilities intended\nto enhance modeler productivity and cannot keep pace of current\ndevelopments of integrated development environments based on AI.\nIn this paper, we propose an approach to address this shortcoming\nthrough a recommender system specifically tailored for textual\nDSLs based on the fine-tuning of pre-trained language models. We\nidentify three main tasks: identifier suggestion, line completion,\nand block completion, which we implement over the same fine-\ntuned model and we propose a workflow to apply these tasks to\nany textual DSL. We have evaluated our approach with different\npre-trained models for three DSLs: Emfatic, Xtext and a DSL to\nspecify domain entities, showing that the system performs well\nand provides accurate suggestions. We compare it against existing\napproaches in the feature name recommendation task showing that\nour system outperforms the alternatives. Moreover, we evaluate\nthe inference time of our approach obtaining low latencies, which\nmakes the system adequate for live assistance. Finally, we contribute\na concrete recommender, named ModelMate, which implements\nthe training, evaluation and inference steps of the workflow as well\nas providing integration into Eclipse-based textual editors.",
        "keywords": [
            "Recommendation",
            "Meta-modeling",
            "Model-Driven Engineering",
            "Machine learning"
        ],
        "authors": [
            "Carlos Durá Costa",
            "José Antonio Hernández López",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/models/models24/3640310.3674089.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extensions and Scalability Experiments of a Generic Model-Driven Architecture for Variability Model Reasoning",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Until recently, the state-of-the-art of Software Product Line (SPL) configuration and verification automation consisted of a collection of ad-hoc approaches tightly coupling a single input Variability Modeling Language (VML) with a single constraint solver. To remedy this situation, a novel generic model-driven architecture was then proposed that enables using a variety of VMLs and solvers. The key ideas of this proposal were (a) the use of a standard logical language (CLIF) as a pivot between VMLs and solvers, and (b) the use of a standard data exchange format (JSON) to explicilty and declaratively specify the abstract syntax and semantics of the VMLs to be used in an SPL engineering project and the automated reasoning task to be performed by the solvers.\nIn this article, we overcome the limitations of this initial proposal in three key ways: (1) we add the ability to reason on textual or hybrid VMLs, rather than only on diagrammatic VMLs, enhancing the versatility of the architecture on the input side; (2) we enable the use of solvers from a third paradigm, enhancing the versatility of the architecture on the output side; and, (3) we present the results of scalability performance experiments of an implementation of this architecture. These results have been achieved without signifi-cantly altering the architecture, demonstrating its agnosticism with respect to specific VMLs and solvers. It also shows that it can under-lie the implementation of practical variability reasoning tools that scale up to real sized variability model analysis and configuration needs.",
        "keywords": [
            "Software Product Lines",
            "Automated Reasoning",
            "Generic Architecture",
            "Configuration Automation"
        ],
        "authors": [
            "Camilo Correa Restrepo",
            "Jacques Robin",
            "Raul Mazo"
        ],
        "file_path": "data/models/models24/3640310.3674090.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward Intelligent Generation of Tailored Graphical Concrete Syntax",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "In model-driven engineering, the concrete syntax of a domain-specific modeling language (DSML) is fundamental as it constitutes the primary point of interaction between the user and the DSML. Nevertheless, the conventional one-size-fits-all approach to concrete syntax often undermines the effectiveness of DSMLs, as it fails to accommodate the diverse constraints and specific requirements inherent to diverse users and usage contexts. Such shortcomings can lead to a significant decline in the performance, usability, and efficiency of DSMLs. This vision paper proposes a conceptual framework to generate concrete syntax intelligently. Our framework considers multiple concerns of users and aims to align the concrete syntax with the context of the DSML usage. Additionally, we detail a baseline process to employ our framework in practice, leveraging large language models to expedite the generation of tailored concrete syntax. We illustrate the potential of our vision with two concrete examples and discuss the shortcomings and research challenges of current intelligent generation techniques.",
        "keywords": [
            "Domain-specific Modeling Languages",
            "Concrete Syntax",
            "Artificial Intelligence",
            "Large Language Models"
        ],
        "authors": [
            "Meriem Ben Chaaben",
            "Oussama Ben Sghaier",
            "Mouna Dhaouadi",
            "Nafisa Elrasheed",
            "Ikram Darif",
            "Imen Jaoua",
            "Bentley Oakes",
            "Eugene Syriani",
            "Mohammad Hamdaqa"
        ],
        "file_path": "data/models/models24/3640310.3674085.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model Everything but with Intellectual Property Protection — The Deltachain Approach",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Many organizations are involved in the development of complex systems, e.g., cyber-physical systems. Organizations work collabo-ratively to describe these systems, using models, which are devel-oped using multiple languages and tools. The models may contain intellectual property that must be protected from other parties, including other contributors. To enable the ongoing exchange of models and to ensure intellectual property protection, our new idea is to use encrypted deltas, i.e., arbitrary changes made to a model. These encrypted deltas are stored on a chain, which we call Deltachain. Encryption enables free exchange of the Deltachain, e.g., on third-party commercial file storage servers. Collaborators involved in the development of the model can access the encrypted Deltachain, decrypt the parts to which they have access, and then work with those decrypted parts which are created by applying the deltas. Subsequently, the collaborators can encrypt their deltas to the model parts and append the encrypted deltas to the Deltachain. Our vision is the use of this Deltachain by collaborating organiza-tions as a single source of truth.",
        "keywords": [
            "Collaborative Software Engineering",
            "Model-Driven Engineering",
            "Cross-Organisational Collaboration",
            "Data Structures",
            "Applied Cryp-tography",
            "Deltachain"
        ],
        "authors": [
            "Thomas Weber",
            "Sebastian Weber"
        ],
        "file_path": "data/models/models24/3640310.3674086.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model aesthetics",
        "submission-date": "2005/04",
        "publication-date": "2005/04",
        "abstract": "We thank the readers that commented on our editorial on model quality in issue 2004-3, and we strongly encourage readers to send comments to us on the editorials and the papers published in SoSyM (our email addresses are included at the end of this editorial). Some comments pointed out that the study of model quality addresses a wide-ranging set of questions and concerns. Model quality is not only concerned with how faithfully a model describes desired properties of the real world or system; it should also be concerned with innate attributes that affect qualities such as analyzability, understandability, and evolvability: Is the model readable? Is the model ambiguous? Is the model concise and complete? Is the model unnecessarily redundant? It may be possible to identify “model smells” (similar to “code smells”) that provide indicators of a model’s quality. We encourage researchers in this area to submit high quality papers on this topic to SoSyM. Papers that describe the results of empirical studies on model quality and aesthetics are especially encouraged.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-005-0081-6.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A metamodel for the compact but lossless exchange of execution traces",
        "submission-date": "2009/11",
        "publication-date": "2010/10",
        "abstract": "Understanding the behavioural aspects of a software system can be made easier if efﬁcient tool support is provided. Lately, there has been an increase in the number of tools for analysing execution traces. These tools, however, have different formats for representing execution traces, which hinders interoperability and limits reuse and sharing of data. To allow for better synergies among trace analysis tools, it would be beneﬁcial to develop a standard format for exchanging traces. In this paper, we present a graph-based format, called compact trace format (CTF), which we hope will lead the way towards such a standard. CTF can model traces generated from a variety of programming languages, including both object-oriented and procedural ones. CTF is built with scalability in mind to overcome the vast size of most interesting traces. Indeed, the design of CTF is based on the idea that call trees can be transformed into more compact ordered acyclic directed graphs by representing similar subtrees only once. CTF is also supported by our trace anal-ysis tool SEAT (Software Exploration and Analysis Tool).",
        "keywords": [
            "Metamodelling",
            "Exchange format",
            "Execution traces",
            "Dynamic analysis"
        ],
        "authors": [
            "Abdelwahab Hamou-Lhadj",
            "Timothy C. Lethbridge"
        ],
        "file_path": "data/sosym-all/s10270-010-0180-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Runtime veriﬁcation of component-based systems in the BIP framework with formally-proved sound and complete instrumentation",
        "submission-date": "2012/03",
        "publication-date": "2013/04",
        "abstract": "Veriﬁcation of component-based systems still suffers from limitations such as state space explosion since a large number of different components may interact in a heterogeneous environment. These limitations entail the need for complementary veriﬁcation methods such as run-time veriﬁcation. Runtime veriﬁcation is a dynamic analysis technique and is prone to scalability. In this paper, we integrate runtime veriﬁcation into the BIP (Behavior, Interaction and Priority) framework. BIP is a powerful and expressive component-based framework for the formal construction of heterogeneous systems. Our method augments BIP systems with monitorsto check speciﬁcations at runtime. This method has been implemented in RV-BIP, a prototype tool that we used to validate the whole approach on a robotic application.",
        "keywords": [
            "Runtime veriﬁcation",
            "Component-based systems",
            "Instrumentation",
            "Formal methods"
        ],
        "authors": [
            "Yliès Falcone",
            "Mohamad Jaber",
            "Thanh-Hung Nguyen",
            "Marius Bozga",
            "Saddek Bensalem"
        ],
        "file_path": "data/sosym-all/s10270-013-0323-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework for qualitative assessment of domain-speciﬁc languages",
        "submission-date": "2013/03",
        "publication-date": "2013/11",
        "abstract": "Domain-speciﬁc languages (DSLs) are used for improving many facets of software development, but whether and to what extent this aim is achieved is an important issue that must be addressed. This paper presents a proposal for a Framework for Qualitative Assessment of DSLs (FQAD). FQAD is used for determining the perspective of the evaluator, understanding the goal of the assessment and selecting fundamental DSL quality characteristics to guide the evaluator in the process. This framework adapts and integrates the ISO/IEC 25010:2011 standard, CMMI maturity level evaluationapproachandthescalingapproachusedinDESMETinto a perspective-based assessment. A detailed list of domain-speciﬁc language quality characteristics is elaborated, and a novel assessment method is proposed. Two case studies through which FQAD is matured and evaluated are reported. The case studies have shown that stakeholders ﬁnd the FQAD process beneﬁcial.",
        "keywords": [
            "Domain-speciﬁclanguages",
            "Qualitymeasures",
            "Qualitative assessment",
            "ISO/IEC 25010",
            "CMMI"
        ],
        "authors": [
            "Gökhan Kahraman",
            "Semih Bilgen"
        ],
        "file_path": "data/sosym-all/s10270-013-0387-8.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Adding higher-level semantics to Functional Mock-up Units for easier, faster, and more robust co-simulation connections",
        "submission-date": "2023/06",
        "publication-date": "2025/01",
        "abstract": "The task of interfacing sub-simulators in a co-simulation often remains difﬁcult, tedious, and prone to error. Here, we describe how this process, and the validation of the resulting interface connections, can be made simpler, faster, and more reliable. Especially when, as is often the case, several individuals or teams collaborate on modeling and simulating a full system. This is achieved by grouping functional mock-up interface (FMI) variables into types close to the relevant engineering domains using semantics formalized as an ontology. Several beneﬁts, we argue, are gained from this: clearer communication, increased validation potential, and reduced number of interface connections to deal with. The validity and the limitations of our approach are demonstrated with a detailed case study of a real maritime system: A dynamic positioning (DP) hardware controller connected to several independent co-simulation models via 156 variables in 78 connections. The proposed solution greatly reduces the complexities and the error potential related to interfacing such systems. All results including reference implementations are openly available through the open simulation platform.",
        "keywords": [
            "Functional Mock-up Interface",
            "Co-Simulation",
            "Simulator Interface",
            "Ontology"
        ],
        "authors": [
            "Martin Rindarøy",
            "Håvard Nordahl",
            "Severin Sadjina",
            "Stian Skjong",
            "Marianne Hagaseth"
        ],
        "file_path": "data/sosym-all/s10270-024-01244-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Variability extraction and modeling for product variants",
        "submission-date": "2014/10",
        "publication-date": "2016/01",
        "abstract": "Fast-changing hardware and software technologies in addition to larger and more specialized customer bases demand software tailored to meet very diverse requirements. Software development approaches that aim at capturing this diversity on a single consolidated platform often require large upfront investments, e.g., time or budget. Alternatively, companies resort to developing one variant of a software product at a time by reusing as much as possible from already-existing product variants. However, identifying and extracting the parts to reuse is an error-prone and inefﬁcient task compounded by the typically large number of product variants. Hence, more disciplined and systematic approaches are needed to cope with the complexity of developing and maintainingsetsofproductvariants.Suchapproachesrequire detailed information about the product variants, the features they provide and their relations. In this paper, we present an approach to extract such variability information from product variants. It identiﬁes traces from features and feature interactions to their implementation artifacts, and computes their dependencies. This work can be useful in many scenarios ranging from ad hoc development approaches such as clone-and-own to systematic reuse approaches such as software product lines. We applied our variability extraction approach to six case studies and provide a detailed evaluation. The results show that the extracted variability information is consistent with the variability in our six case study systems given by their variability models and available product variants.",
        "keywords": [
            "Feature",
            "Trace",
            "Product variant",
            "Variability",
            "Dependency"
        ],
        "authors": [
            "Lukas Linsbauer",
            "Roberto Erick Lopez-Herrejon",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-015-0512-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving the quality of use case models using antipatterns",
        "submission-date": "2008/05",
        "publication-date": "2009/02",
        "abstract": "Use case (UC) modeling is a popular requirements modeling technique. While these models are simple to create and read; this simplicity is often misconceived, leading practitioners to believe that creating high quality models is straightforward. Therefore, many low quality models that are inconsistent, incorrect, contain premature restrictive design decision and contain ambiguous information are produced. To combat this problem of creating low quality UC models, this paper presents a new technique that utilizes antipatterns as a mechanism for remedying quality problems in UC models. The technique, supported by the tool ARBIUM, provides a framework for developers to define antipatterns. The feasibility of the approach is demonstrated by applying it to a real-world system. The results indicate that applying the technique improves the overall quality and clarity of UC models.",
        "keywords": [
            "Use cases",
            "Antipatterns",
            "UML",
            "Use case modeling qualityattributes",
            "OCL"
        ],
        "authors": [
            "Mohamed El-Attar",
            "James Miller"
        ],
        "file_path": "data/sosym-all/s10270-009-0112-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on SEFM’22",
        "submission-date": "2024/03",
        "publication-date": "2024/04",
        "abstract": "This special section contains revised and extended versions of selected papers from SEFM’22, the 20th International Conference on Software Engineering and Formal Methods, held in Berlin, Germany, on September 28–30, 2022. The SEFM conference series aims to bring together researchers and practitioners from academia, industry and government, to advance the state of the art in formal methods, to facilitate their uptake in the software industry, and to encourage their integration within practical software engineering methods and tools.",
        "keywords": [],
        "authors": [
            "Bernd-Holger Schlingloﬀ",
            "Ming Chai"
        ],
        "file_path": "data/sosym-all/s10270-024-01174-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating process model collections",
        "submission-date": "2014/11",
        "publication-date": "2015/10",
        "abstract": "Business process management plays an important role in the management of organizations. More and more organizations describe their operations as business processes. It is common for organizations to have collections of thousands of business processes, but for reasons of confidentiality these collections are often not, or only partially, available to researchers. On the other hand, research on techniques for managing process model collections, such as techniques for process retrieval, requires large collections for evaluation purposes. Therefore, this paper proposes a technique to generate such collections of process models, based on the properties of real-world collections. Where existing techniques focus on the structure of the process models, the technique proposed in this paper also generates task labels that consists of words from real-life task labels and considers semantic information of node and edge types. We evaluate our technique by applying it to generate two synthetic collections of process models of over 60,000 and over 2,000 models, respectively. We show that the generated synthetic collections have similar properties to the original collections. To the best of our knowledge, this is the first technique that can generate synthetic BPMN models, thus enabling experimentation with process collections that have laboratory-set quantitative parameters and qualitative properties that are based on real-world process model collections.",
        "keywords": [
            "Synthetic process models",
            "Process model generation",
            "Process model similarity"
        ],
        "authors": [
            "Zhiqiang Yan",
            "Remco Dijkman",
            "Paul Grefen"
        ],
        "file_path": "data/sosym-all/s10270-015-0497-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design and evaluation of a collaborative UML modeling environment in virtual reality",
        "submission-date": "2022/03",
        "publication-date": "2022/11",
        "abstract": "Modeling is a key activity in conceptual design and system design. Through collaborative modeling, end-users, stakeholders, experts, and entrepreneurs are able to create a shared understanding of a system representation. While the Uniﬁed Modeling Language (UML) is one of the major conceptual modeling languages in object-oriented software engineering, more and more concerns arise from the modeling quality of UML and its tool-support. Among them, the limitation of the two-dimensional presentation of its notations and lack of natural collaborative modeling tools are reported to be signiﬁcant. In this paper, we explore the potential of using virtual reality (VR) technology for collaborative UML software design by comparing it with classical collaborative software design using conventional devices (desktop PC/laptop). For this purpose, we have developed a VR modeling environment that offers a natural collaborative modeling experience for UML Class Diagrams. Based on a user study with 24 participants, we have compared collaborative VR modeling with conventional modeling with regard to efﬁciency, effectiveness, and user satisfaction. Results show that the use of VR has some disadvantages concerning efﬁciency and effectiveness, but the user’s fun, the feeling of being in the same room with a remote collaborator, and the naturalness of collaboration were increased.",
        "keywords": [
            "Collaborative modeling",
            "Virtual reality",
            "UML"
        ],
        "authors": [
            "Enes Yigitbas",
            "Simon Gorissen",
            "Nils Weidmann",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-022-01065-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "aCHAT-WF: Generating conversational agents for teaching business process models",
        "submission-date": "2020/11",
        "publication-date": "2021/10",
        "abstract": "This paper proposes a general approach for using conversational interfaces such as chatbots to offer adaptive learning of\nbusiness processes in an environment involving different actors. Adaptivity concerns both the content being proposed, the\nsequence of learning items, and the way the conversation is conducted. The original approach allows the development\nof sustainable chatbots and empowers various non-technical actors (authors, teachers, publishers, and learners) to control\nthe chatbot features directly. The aCHAT-WF framework (adaptive CHATbot for WorkFlows), proposed in this paper for\nmanaging conversational interfaces, conceptually represents all the aspects related to a conversation about business processes,\nwith different facets for the user, the conversation ﬂow, and the conversation contents, combining them to obtain a ﬂexible\ninteraction with the user. The paper focuses on the different preparation phases for instructional material based on Business\nProcess Modeling Notation (BPMN) models, separating the different roles involved in the construction of a chatbot for teaching\nbusiness processes and with the possibility of deﬁning different styles for the interaction with the users. The proposed method\nis conﬁguration-driven, to facilitate the separation of the different aspects of the control of the interaction and the delivery of\ncontents.",
        "keywords": [
            "Chatbot",
            "Business process",
            "BPMN",
            "Digital transformation",
            "Conﬁguration driven",
            "Educational conversational agent"
        ],
        "authors": [
            "Donya Rooein",
            "Devis Bianchini",
            "Francesco Leotta",
            "Massimo Mecella",
            "Paolo Paolini",
            "Barbara Pernici"
        ],
        "file_path": "data/sosym-all/s10270-021-00925-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A formal approach to finding inconsistencies in a metamodel",
        "submission-date": "2018/12",
        "publication-date": "2021/01",
        "abstract": "Checking the consistency of a metamodel involves finding a valid metamodel instance that provably meets the set of constraints that are defined over the metamodel. These constraints are often specified in Object Constraint Language. Often, a metamodel is inconsistent due to conflicts among the constraints. Existing approaches and tools are typically incapable of pinpointing the conflicting constraints, and this makes it difficult for users to debug and fix their metamodels. In this paper, we present a formal approach for locating conflicting constraints in inconsistent metamodels. Our approach has four distinct features: (1) users can rank individual metamodel features using their own domain-specific knowledge, (2) we transform these ranked features to a weighted maximum satisfiability modulo theories problem and solve it to compute the set of maximum achievable features, (3) we pinpoint the conflicting constraints by solving the set cover problem using a novel algorithm, and (4) we have implemented our approach into a fully automated tool called MaxUSE. Our evaluation results, using our assembled set of benchmarks, demonstrate the scalability of our work and that it is capable of efficiently finding conflicting constraints.",
        "keywords": [
            "Metamodel",
            "Conflicts",
            "SMT"
        ],
        "authors": [
            "Hao Wu",
            "Marie Farrell"
        ],
        "file_path": "data/sosym-all/s10270-020-00849-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A situational method for semi-automated Enterprise Architecture Documentation",
        "submission-date": "2013/08",
        "publication-date": "2014/04",
        "abstract": "The business capabilities of modern enterprises crucially rely on the enterprises’ information systems and underlying IT infrastructure. Hence, optimization of the business-IT alignment is a key objective of Enterprise Architecture Management (EAM). To achieve this objective, EAM creates, maintains and analyzes a model of the current state of the Enterprise Architecture. This model covers different concepts reflecting both the business and the IT perspective and has to be constantly maintained in response to ongoing transformations of the enterprise. In practice, EA models grow large and are difficult to maintain, since many stakeholders from various backgrounds have to contribute architecture-relevant information. EAM literature and two practitioner surveys conducted by the authors indicate that EA model maintenance, in particular the manual documentation activities, poses one of the biggest challenges to EAM in practice. Current research approaches target the automation of the EA documentation based on specific data sources. These approaches, as our systematic literature review showed, do not consider enterprise specificity of the documentation context or the variability of the data sources from organization to organization. The approach presented in this article specifically accounts for these factors and presents a situational method for EA documentation. It builds on four process-supported documentation techniques which can be selected, composedandappliedtodesignanorganization-speciﬁcdoc-umentation process. The techniques build on a meta-model for EA documentation, which is implemented in an EA-repository prototype that supports the conﬁguration and execution of the documentation techniques. We applied our documentation method assembly process at a German insurance company and report the ﬁndings from this case study in particular regarding practical applicability and usability of our approach.",
        "keywords": [
            "Enterprise Architecture",
            "Documentation",
            "Maintenance",
            "Model",
            "Automation",
            "Situational method"
        ],
        "authors": [
            "Matthias Farwick",
            "Christian M. Schweda",
            "Ruth Breu",
            "Inge Hanschke"
        ],
        "file_path": "data/sosym-all/s10270-014-0407-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Hybrid co-simulation: it’s about time",
        "submission-date": "2016/02",
        "publication-date": "2017/11",
        "abstract": "Model-based design methodologies are commonly used in industry for the development of complex cyber-physical systems (CPSs). There are many different languages, tools, and formalisms for model-based design, each with its strengths and weaknesses. Instead of accepting some weaknesses of a particular tool, an alternative is to embrace heterogeneity, and to develop tool integration platforms and protocols to leverage the strengths from different environments. A fairly recent attempt in this direction is the functional mock-up interface (FMI) standard that includes support for co-simulation. Although this standard has reached acceptance in industry, it provides only limited support for simulating systems that mix continuous and discretebehavior, whicharetypical of CPS. This paper identiﬁes the representation of time as a key problem, because the FMI representation does not support well the discrete events that typically occur at the cyber-physical boundary. We analyze alternatives for representing time in hybrid co-simulation and conclude that a superdense model of time using integers only solves many of these problems. We show how an execution engine can pick an adequate time resolution, and how disparities between time representations internal to co-simulated components and the resulting effects of time quantization can be managed. We propose a concrete extension to the FMI standard for supporting hybrid co-simulation that includes integer time, automatic choice of time resolution, and the use of absent signals. We explain how these extensions can be implemented modularly within the frameworks of existing simulation environments.",
        "keywords": [
            "Co-simulation",
            "Functional mock-up interface",
            "Time"
        ],
        "authors": [
            "Fabio Cremona",
            "Marten Lohstroh",
            "David Broman",
            "Edward A. Lee",
            "Michael Masin",
            "Stavros Tripakis"
        ],
        "file_path": "data/sosym-all/s10270-017-0633-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to the theme issue on model-driven service engineering",
        "submission-date": "2013/07",
        "publication-date": "2013/07",
        "abstract": "During the last years, Model-Driven Engineering (MDE) has started to have a direct inﬂuence in other research ﬁelds. Under the light of the premise that everything is a model, coined by Jean Bézivin in 2004 and adopted by the MDE community since then, practitioners from other areas have discovered that they were able to express their problems in terms of models and then take advantage of MDE techniques to solve them—or at least simplify them, either by increasing the level of automation or by raising the abstraction level at which solutions are planned and developed. The scope varies widely, from generic domains, like Web Engineering (see the previous SoSyM Theme Issue on Model-Driven Web Engi- neering), to more speciﬁc ones, like DB schema matching or domotics. Out of doubt, one of the areas that has more decisively beneﬁted from MDE advances has been Service Engineer- ing. It aims at bringing together the beneﬁts of Service Orientation and Business Process Management, therefore making the most of Service Orientation to help organizations deliver sustainable business value with increased agility and cost effectiveness.",
        "keywords": [],
        "authors": [
            "Juan Manuel Vara",
            "Mike Papazoglou",
            "Il-Yeol Song"
        ],
        "file_path": "data/sosym-all/s10270-013-0368-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial for the SoSyM issue 2014/04",
        "submission-date": "2014/09",
        "publication-date": "2014/09",
        "abstract": "In recent years, SoSyM has had a pretty large pipeline of papers to be published. This resulted in long hard copy publication turnaround times for accepted SoSyM papers. In order to facilitate more timely publication of accepted papers, we requested Springer to increase the number of pages per issue. Springer agreed to signiﬁcantly increase the number of pages per issue for 2014. Speciﬁcally, they agreed to increase the number of pages in the ﬁrst two issues of 2014 to 448 pages, and the last two issues of 2014 to 304 pages. Springer is also looking at increasing the number of pages published in 2015 to further reduce the pipeline. We are very grateful to the Springer staff for their willingness to take concrete steps to reduce the publication pipeline.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-014-0434-0.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Modeling and simulation of the IEEE 802.11e wireless protocol with hidden nodes using Colored Petri Nets",
        "submission-date": "2019/05",
        "publication-date": "2020/07",
        "abstract": "Wireless technologies are continuously evolving, including features such as the extension to mid- and long-range commu-nications and the support of an increasing number of devices. However, longer ranges increase the probability of suffering from hidden terminal issues. In the particular case of Wireless Local Area Networks (WLANs), the use of Quality of Service (QoS) mechanisms introduced in IEEE 802.11e compromises scalability, exacerbates the hidden node problem, and creates congestion as the number of users and the variety of services in the network grow. In this context, this paper presents a configurable Colored Petri Net (CPN) model for the IEEE 802.11e protocol with the aim of analyzing the QoS support in mid- and long-range WLANs The CPN model covers the behavior of the protocol in the presence of hidden nodes to examine the performance of the RTS/CTS exchange in scenarios where the QoS differentiation may involve massive collision chains and high delays. Our CPN model sets the basis for further exploring the performance of the various mechanisms defined by the IEEE 802.11 standard. We then use this CPN model to provide a comprehensive study of the effectiveness of this protocol by using the simulation and monitoring capabilities of CPN Tools.",
        "keywords": [
            "IEEE 802.11",
            "QoS",
            "Colored Petri Nets",
            "Simulation",
            "Performance",
            "Hidden terminal"
        ],
        "authors": [
            "Estefanía Coronado",
            "Valentín Valero",
            "Luis Orozco-Barbosa",
            "María-Emilia Cambronero",
            "Fernando L. Pelayo"
        ],
        "file_path": "data/sosym-all/s10270-020-00817-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multilevel modeling of geographic information systems based on international standards",
        "submission-date": "2020/06",
        "publication-date": "2021/07",
        "abstract": "Even though different applications based on Geographic Information Systems (GIS) provide different features and functions, they all share a set of common concepts (e.g., spatial data types, operations, services), a common architecture, and a common set of technologies. Furthermore, common structures appear repeatedly in different GIS, although they have to be specialized in speciﬁc application domains. Multilevel modeling is an approach to model-driven engineering (MDE) in which the number of metamodel levels is not ﬁxed. This approach aims at solving the limitations of a two-level metamodeling approach, which forces the designer to include all the metamodel elements at the same level. In this paper, we address the application of multilevel modeling to the domain of GIS, and we evaluate its potential beneﬁts. Although we do not present a complete set of models, we present four representative scenarios supported by example models. One of them is based on the standards deﬁned by ISO TC/211 and the Open Geospatial Consortium. The other three are based on the EU INSPIRE Directive (territory administration, spatial networks, and facility management). These scenarios show that multilevel modeling can provide more beneﬁts to GIS modeling than a two-level metamodeling approach.",
        "keywords": [
            "Model-driven engineering",
            "Multilevel software modeling",
            "Geographic information systems"
        ],
        "authors": [
            "Suilen H. Alvarado",
            "Alejandro Cortiñas",
            "Miguel R. Luaces",
            "Oscar Pedreira",
            "Angeles S. Places"
        ],
        "file_path": "data/sosym-all/s10270-021-00901-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling dynamic structures",
        "submission-date": "2020/04",
        "publication-date": "2020/04",
        "abstract": "David Harel once said during a talk, “Bridges are made to stand and software is there to do.” This is a very appropriate analogy, because it shows that software is about behavior. As a consequence, many software modeling techniques supported by languages also allow the description of behavior. However, behavior usually is embedded in some structure. In object-oriented systems, this is typically the object, where a system is composed of many object instances, with the class as the describing artifact that deﬁnes the blueprint. In many forms of complex or distributed systems, the notion of “component” or “assembly” is also used in various forms to describe structure.",
        "keywords": [],
        "authors": [
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-020-00793-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Conﬁguring use case models in product families",
        "submission-date": "2016/01",
        "publication-date": "2016/06",
        "abstract": "In many domains such as automotive and avion- ics, the size and complexity of software systems is quickly increasing. At the same time, many stakeholders tend to be involved in the development of such systems, which typi- cally must also be conﬁgured for multiple customers with varying needs. Product Line Engineering (PLE) is therefore an inevitable practice for such systems. Furthermore, because in many areas requirements must be explicit and traceabil- ity to them is required by standards, use cases and domain models are common practice for requirements elicitation and analysis. In this paper, based on the above observations, we aim at supporting PLE in the context of use case-centric development. Therefore, we propose, apply, and assess a use case-driven conﬁguration approach which interactively receives conﬁguration decisions from the analysts to gen- erate product-speciﬁc (PS) use case and domain models. Our approach provides the following: (1) a use case-centric product line modeling method (PUM), (2) automated, inter- active conﬁguration support based on PUM, and (3) an automatic generation of PS use case and domain models from Product Line (PL) models and conﬁguration decisions. The approach is supported by a tool relying on Natural Language Processing (NLP) and integrated with an industrial requirements management tool, i.e., IBM DOORS. We successfully applied and evaluated our approach to an industrial case study in the automotive domain, thus showing evidence that the approach is practical and beneﬁcial to capture variability at the appropriate level of granularity and to conﬁgure PS use case and domain models in industrial settings.",
        "keywords": [
            "Product line engineering",
            "Use case-driven development",
            "Conﬁguration",
            "Natural language processing",
            "Consistency checking"
        ],
        "authors": [
            "Ines Hajri",
            "Arda Goknil",
            "Lionel C. Briand",
            "Thierry Stephany"
        ],
        "file_path": "data/sosym-all/s10270-016-0539-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A theme section on the central role of modeling in designing and explaining data-driven systems and software",
        "submission-date": "2023/10",
        "publication-date": "2023/11",
        "abstract": "Following the stimulating 10th International Conference on Model and Data Engineering (MEDI 2021), we are excited to announce our proposal to edit a \"Theme Sec-tion\" for the International Journal on Software and Systems Modeling (SoSyM). This theme section will be dedicated to showcasing the recent results from the Data and Models communities, with a particular emphasis on the signiﬁcance of the 10th edition of MEDI.",
        "keywords": [],
        "authors": [
            "Christian Attiogbé",
            "Sadok Ben Yahia",
            "Ladjel Bellatreche"
        ],
        "file_path": "data/sosym-all/s10270-023-01133-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synthesis of veriﬁable concurrent Java components from formal models",
        "submission-date": "2016/03",
        "publication-date": "2017/02",
        "abstract": "Concurrent systems are hard to program, and ensuring quality by means of traditional testing techniques is often very hard as errors may not show up easily and reproducing them is hard. In previous work, we have advocated a model-driven approach to the analysis and design of concurrent, safety-critical systems. However, to take full advantage of these techniques, they must be supported by code generation schemes for concrete programming languages. Ideally, this translation should be traceable, automated and should supporttheveriﬁcationofthegeneratedcode.Inourwork,we consider the problem of generating a concurrent Java component from a high-level model of inter-process interaction (i.e., communication + synchronization). We call our formalism shared resources. From the model, which can be represented in mathematical notation or written as a Java interface annotated using an extension of JML, a Java component can be obtained by a semiautomatic translation. We describe how to obtain shared memory (using a priority monitors library) and message passing (using the JCSP library) implementations. Focusing on inter-process interaction for formal development is justiﬁed by several reasons, e.g., mathematical models are language-independent and allow to analyze certain concurrency issues, such as deadlocks or liveness properties prior to code generation. Also, the Java components produced from the shared resource model will contain all the concurrency-related language constructs, which are often responsible for many of the errors in concurrent software. We follow a realistic approach where the translation is semiautomatic (schemata for code generation) and the programmer still retains the power of coding or modifying parts of the code for the resource. The code thus obtained is JML-annotated Java with proof obligations that help with code traceability and veriﬁcation of safety and liveness properties. As the code thus obtained is not automatically correct, there is still the need to verify its conformance to the original specs. We illustrate the methodology by developing a concurrent control system and verifying the code obtained using the KeY program veriﬁcation tool. We also show how KeY can be used to ﬁnd errors resulting from a wrong use of the templates.",
        "keywords": [
            "CSP",
            "JCSP",
            "KeY",
            "Java",
            "JML",
            "Shared resources",
            "Veriﬁcation",
            "Model-driven",
            "Concurrency",
            "Message passing"
        ],
        "authors": [
            "Julio Mariño",
            "Raúl N. N. Alborodo",
            "Lars-Åke Fredlund",
            "Ángel Herranz"
        ],
        "file_path": "data/sosym-all/s10270-017-0581-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Petri nets in systems biology",
        "submission-date": "2014/01",
        "publication-date": "2014/06",
        "abstract": "Petri nets are used in many areas. This article discusses the application of Petri nets in systems biology. Using an example from biochemistry, concepts for the automatic decomposition of biochemical systems are introduced. The article focuses on those concepts that fulﬁll steady-state conditions. Interestingly, all the concepts are based on minimal,semi-positivetransitioninvariants.Thearticledescribes, which new deﬁnitions for network decomposition can be derived and how they can be interpreted in the context of biology. This is illustrated with the example of the citric acid cycle, for which a new metabolic pathway could be predicted with the help of such an analysis.",
        "keywords": [
            "Petri net",
            "Systems biology",
            "T-invariant",
            "MCT-set",
            "T-cluster",
            "Citrate cycle"
        ],
        "authors": [
            "Ina Koch"
        ],
        "file_path": "data/sosym-all/s10270-014-0421-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Developing BP-driven web applications through the use of MDE techniques",
        "submission-date": "2009/11",
        "publication-date": "2010/10",
        "abstract": "Model driven engineering (MDE) is a suitable approach for performing the construction of software systems (in particular in the Web application domain). There are different types of Web applications depending on their purpose (i.e., document-centric, interactive, transactional, workﬂow/business process-based, collaborative, etc). This work focusses on business process-based Web applications in order to be able to understand business processes in a broad sense, from the lightweight business processes already addressed by existing proposals to long-running asynchro- nous processes. This work presents a MDE method for the construction of systems of this type. The method has been designed in two steps following the MDE principles. In the ﬁrst step, the system is represented by means of models in a technology-independent manner. These models capture the different aspects of Web-based systems (these aspects refer to behaviour, structure, navigation, and presentation issues). In the second step, the model transformations (both model-to-model and model-to-text) are applied in order to obtain the ﬁnal system in terms of a speciﬁc technology. In addition, a setofEclipse-basedtoolshasbeendevelopedtoprovideauto- mation in the application of the proposed method in order to validate the proposal.",
        "keywords": [
            "Web engineering",
            "Model driven engineering",
            "Business processes"
        ],
        "authors": [
            "Victoria Torres",
            "Pau Giner",
            "Vicente Pelechano"
        ],
        "file_path": "data/sosym-all/s10270-010-0177-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based ﬂeet deployment in the IoT–edge–cloud continuum",
        "submission-date": "2021/02",
        "publication-date": "2022/05",
        "abstract": "With the increasing computing and networking capabilities, IoT devices and edge gateways have become part of a larger IoT–edge–cloud computing continuum, where processing and storage tasks are distributed across the whole network hierarchy, not concentrated only in the cloud. At the same time, this also introduced continuous delivery practices to the development of software components for network-connected gateways and sensing/actuating nodes. These devices are placed on end users’ premises and are characterized by continuously changing cyber-physical contexts, forcing software developers to maintain multiple application versions and frequently redeploy them on a distributed ﬂeet of devices with respect to their current contexts. Doing this correctly and efﬁciently goes beyond manual capabilities and requires an intelligent and reliable automated solution. This paper describes a model-based approach to automatically assigning multiple software deployment plans to hundreds of edge gateways and connected IoT devices implemented in collaboration with a smart healthcare application provider. From a platform-speciﬁc model of an existing edge computing platform, we extract a platform-independent model that describes a list of target devices and a pool of available deployment plans. Next, we use constraint solving to automatically assign deployment plans to devices at once with respect to their speciﬁc contexts. The result is transformed back into the platform-speciﬁc model and includes a suitable deployment plan for each device, which is then consumed by our engine to deploy software components not only on edge gateways but also on their downstream IoT devices with constrained resources and connectivity. We validate the approach with a ﬂeet deployment prototype integrated into a DevOps toolchain used by the partner application provider. Initial experiments demonstrate the viability of the approach and its usefulness in supporting DevOps for edge and IoT software development.",
        "keywords": [
            "Software deployment",
            "IoT",
            "Model-based software engineering",
            "Device ﬂeet",
            "DevOps",
            "Constraint solving"
        ],
        "authors": [
            "Hui Song",
            "Rustem Dautov",
            "Nicolas Ferry",
            "Arnor Solberg",
            "Franck Fleurey"
        ],
        "file_path": "data/sosym-all/s10270-022-01006-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the special issue on the 18th international conference on model driven engineering languages and systems (MODELS’15)",
        "submission-date": "2016/12",
        "publication-date": "2017/01",
        "abstract": "MODELS is the premier conference series for model-based software and systems engineering. It has been established in 1998 and has since been covering all aspects of modeling, from languages and methods to tools and applications. MODELS’15 was the 18th edition of the conference. It took place in Ottawa, Canada from September 27 to October 2, 2015. MODELS’15 challenged the modeling community to promote the magic of modeling by solidifying and extending the foundations and successful applications of modeling in areas such as business information and embedded systems, but also by exploring the use of modeling for new and emerging systems, paradigms, and challenges including cyber-physical systems, cloud computing, services, social media, big data, security, and open source. This challenge resulted in 216 abstract submissions that materialized in 172 papers, consisting of 132 technical papers (including 22 new ideas papers) and 40 in-practice papers. Of these, the Program Committee and Program Board accepted 35 foundations papers (26.5% acceptance rate) and 11 in-practice papers (28% acceptance rate). The program also included a diverse group of keynote talks, including presentations on climate models, automotive models, and software supply chains. Out of the accepted papers, we invited the best ones for this special issue. This invitation was based on a careful evaluation of all papers by the Program Board and Program Committee. The authors of these best papers were then asked to prepare a substantial improved and extended version for this special issue. Each article underwent a full journal review process and authors received anonymous feedback in two rounds of reviewing from three expert reviewers.",
        "keywords": [],
        "authors": [
            "Jordi Cabot",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-017-0577-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "WESSBAS: extraction of probabilistic workload speciﬁcations for load testing and performance prediction—a model-driven approach for session-based application systems",
        "submission-date": "2015/07",
        "publication-date": "2016/10",
        "abstract": "Thespeciﬁcationofworkloadsisrequiredinorder\nto evaluate performance characteristics of application sys-\ntemsusingloadtestingandmodel-basedperformancepredic-\ntion. Deﬁning workload speciﬁcations that represent the real\nworkload as accurately as possible is one of the biggest chal-\nlenges in both areas. To overcome this challenge, this paper\npresents an approach that aims to automate the extraction and\ntransformation of workload speciﬁcations for load testing\nand model-based performance prediction of session-based\napplication systems. The approach (WESSBAS) comprises\nthree main components. First, a system- and tool-agnostic\ndomain-speciﬁc language (DSL) allows the layered mod-\neling of workload speciﬁcations of session-based systems.\nSecond, instances of this DSL are automatically extracted\nfrom recorded session logs of production systems. Third,\nthese instances are transformed into executable workload\nspeciﬁcations of load generation tools and model-based per-\nformance evaluation tools. We present transformations to the\ncommon load testing tool Apache JMeter and to the Palla-\ndio Component Model. Our approach is evaluated using the\nindustry-standard benchmark SPECjEnterprise2010 and the\nWorld Cup 1998 access logs. Workload-speciﬁc characteris-\ntics (e.g., session lengths and arrival rates) and performance\ncharacteristics (e.g., response times and CPU utilizations)\nshow that the extracted workloads match the measured work-\nloads with high accuracy.",
        "keywords": [
            "Workload speciﬁcations",
            "Load testing",
            "Performance prediction",
            "Performance models"
        ],
        "authors": [
            "Christian Vögele",
            "André van Hoorn",
            "Eike Schulz",
            "Wilhelm Hasselbring",
            "Helmut Krcmar"
        ],
        "file_path": "data/sosym-all/s10270-016-0566-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Speciﬁcation-driven model transformation testing",
        "submission-date": "2012/10",
        "publication-date": "2013/08",
        "abstract": "Testing model transformations poses several challenges, among them the automatic generation of appropriate input test models and the speciﬁcation of oracle functions. Most approaches for the generation of input models ensure a certain coverage of the source meta-model or the transformation implementation code, whereas oracle functions are frequently deﬁned using query or graph languages. However, these two tasks are usually performed independently regardless of their common purpose, and sometimes, there is a gap between the properties exhibited by the generated input models and those considered by the transformations. Recently, we proposed a formal speciﬁcation language for the declarative formulation of transformation properties (bymeansofinvariants,pre-,andpostconditions)fromwhich we generated partial oracle functions used for transformation testing. Here, we extend the usage of our speciﬁcation language for the automated generation of input test models by SAT solving. The testing process becomes more intentional because the generated models ensure a certain coverage of the transformation requirements. Moreover, we use the same speciﬁcation to consistently derive both the input test models and the oracle functions. A set of experiments is presented, aimed at measuring the efﬁcacy of our technique.",
        "keywords": [
            "Model transformation",
            "Model transformation speciﬁcation",
            "Model transformation testing",
            "Model ﬁnding",
            "Test oracle"
        ],
        "authors": [
            "Esther Guerra",
            "Mathias Soeken"
        ],
        "file_path": "data/sosym-all/s10270-013-0369-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Modeling ecosystems of reference frameworks for assurance: a case on privacy impact assessment regulation and guidelines",
        "submission-date": "2022/02",
        "publication-date": "2022/11",
        "abstract": "To assure certain critical quality properties (e.g., safety, security, or privacy), supervisory authorities and industrial associations provide reference frameworks such as standards or guidelines that in some cases are enforced (e.g., regulations). Given the pace at which both technical advancements and risks appear, there is an increase in the number of reference frameworks. As several frameworks might apply for same systems, certain overlaps appear (e.g., regulations for different countries where the system will operate, or generic standards in conjunction with more concrete standards for a given industrial sector or system type). We propose the use of modelling for alleviating the complexity of these reference frameworks ecosystems, and we provide a tool-supported method to create them for the beneﬁt of different stakeholders. The case study is based on privacy data protection, and more concretely on privacy impact assessment processes. The European GDPR regulates the movement and processing of personal data, and, contrary to available software engineering privacy guidelines, articles in legal texts are usually difﬁcult to translate to the underlying processes, artefacts and roles that they refer to. To facilitate the mutual comprehension of legal experts and engineers, in this work we investigate how mappings can be created between these two domains of expertise. Notably, we rely on modelling as a central point. We modelled the legal requirements of the GDPR on data protection impact assessments, and then, we selected the ISO/IEC 29134, a mainstream engineering guideline for privacy impact assessment, and, taking a concrete sector as example, the EU Smart Grid Data Protection Impact Assessment template. The OpenCert tool was used for providing technical support to both the modelling and the creation of the mapping models in a systematic way. We provide a qualitative evaluation from legal experts and privacy engineering practitioners to report on the beneﬁts and limitations of this approach.",
        "keywords": [
            "Modelling",
            "OpenCert",
            "Reference frameworks",
            "Privacy",
            "GDPR",
            "ISO 29134",
            "Smart grid",
            "Privacy impact assessment"
        ],
        "authors": [
            "Alejandra Ruiz",
            "Yod-Samuel Martin",
            "Jabier Martinez",
            "Jacobo Quintans",
            "Guillaume Mockly",
            "Amelie Gyrard",
            "Tommaso Crepax"
        ],
        "file_path": "data/sosym-all/s10270-022-01061-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Probabilistic modelling and veriﬁcation using RoboChart and PRISM",
        "submission-date": "2020/11",
        "publication-date": "2021/10",
        "abstract": "RoboChart is a timed domain-speciﬁc language for robotics, distinctive in its support for automated veriﬁcation by model checking and theorem proving. Since uncertainty is an essential part of robotic systems, we present here an extension to RoboChart to model uncertainty using probabilism. The extension enriches RoboChart state machines with probability through a new construct: probabilistic junctions as the source of transitions with a probability value. RoboChart has an accompanying tool, called RoboTool, for modelling and veriﬁcation of functional and real-time behaviour. We present here also an automatic technique, implemented in RoboTool, to transform a RoboChart model into a PRISM model for veriﬁcation. We have extended the property language of RoboTool so that probabilistic properties expressed in temporal logic can be written using controlled natural language.",
        "keywords": [
            "State machines",
            "Formal semantics",
            "Model transformation",
            "PRISM",
            "Probabilistic model checking",
            "Domain-speciﬁc language for robotics"
        ],
        "authors": [
            "Kangfeng Ye",
            "Ana Cavalcanti",
            "Simon Foster",
            "Alvaro Miyazawa",
            "Jim Woodcock"
        ],
        "file_path": "data/sosym-all/s10270-021-00916-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Simplifying autonomic enterprise Java Bean applications via model-driven engineering and simulation",
        "submission-date": "2006/02",
        "publication-date": "2007/05",
        "abstract": "The goal of autonomic computing is to reduce the conﬁguration, operational, and maintenance costs of distributed applications by enabling them to self-manage, self-heal, and self-optimize. This paper provides two contributions to the Model-Driven Engineering (MDE) of autonomic computing systems using Enterprise Java Beans (EJBs). First, we describe the structure and functionality of an MDE tool that visually captures the design of EJB applications, their quality of service (QoS) requirements, and the adaptations applied to their EJBs. Second, the paper describes how MDE tools can be used to generate code to simulate adaptive systems for veriﬁcation and plug EJBs into a Java component framework that provides runtime adaptation capabilities.",
        "keywords": [
            "Autonomic Computing",
            "Model-Driven Engineering",
            "Enterprise Java Beans"
        ],
        "authors": [
            "Jules White",
            "Douglas C. Schmidt",
            "Aniruddha Gokhale"
        ],
        "file_path": "data/sosym-all/s10270-007-0057-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model modularity for reuse, libraries and composition: symbol management is key",
        "submission-date": "2024/06",
        "publication-date": "2024/06",
        "abstract": "It is insightful to observe the similarities between the ways that programs are decomposed into sets of individual and reusable artifacts and the corresponding ways that models are (or should be?) deﬁned. Modularity and encapsulation were introduced in the earlier work on Algebraic Datatypes and then Parnas pointed out the important mechanism of mod-ularity with his famous paper about “On the Criteria to be Used in Decomposing Systems into Modules.” Dijkstra also discussed modularity by popularizing the term “Separation of Concerns.” Most modern programming languages provide mechanisms for modularity and developers use the concepts of class, module, and assembly in their general programming repertoire. It is a core principle that the internal realization of implementation detail is secretly encapsulated and can only be accessed by an explicitly deﬁned and exported interface. Thisinterfaceisdeﬁnedbydeﬁnitiveprogrammingelements, which are given a human-readable name, such as class name “Person” or method name “getAge()”. Modularity has several beneﬁts, according to Parnas, including: • When the underlying secret is changed, an implementation can be exchanged and evolved without the need to modify the dependent code because the deﬁned interface remains untouched. This supports software changeability while minimizing global impacts across the code base. • Code can be developed independently within the same project, which supports parallel development by a team of software engineers. This also supports the integration and reuse of external and open-source libraries. Since McIlroy’s paper on reusability in the late 1960s, the general reusability of library code and frameworks has greatly increased programmer efﬁciency. • Modularity also helps with the comprehensibility of the program code by allowing each module to be reasoned about locally without a deep need to understand all of the inner details of other parts of a project. On the contrary, modern modeling languages and their tools do not natively provide a deep collection of reusable libraries. One key problem is the lack of well-understood, encapsulating interfaces of the deﬁned models. So far models have often been used to describe the interface of an underlying component, but the concept that a model itself has an interface and an encapsulated “body” is still unfamiliar. Only a few works from the community promote this idea, such as the concept of symbol tables for models, potentially aggregated to a model type or requirement model. This idea also has been further explored with different levels of interfaces, such as the interfaces for variability, customization, and use (VCU). Moreover, the availability of interfaces on models means that there must exist an efﬁcient management of symbols. Regarding symbols, we mean named elements that modelers deﬁne inside a model that are allowed to be referenced from outside, using the deﬁned name and signature (as providedbysymboltables,modeltypes,orrequirementmodels). This includes classes, states, activities, pins, ports, methods, attributes, variables, and many more potential kinds of symbols. Symbols are a well elaborated concept in programming languages (e.g., symbol tables explicitly store symbols in separate or joint artifacts). But symbol management is often absent in modeling languages and thus not very well accommodated in most tools. Most modern UML and SysML tools do not manage symbols explicitly, but store an integrated large model, where elements are directly connected, even though models provide explicitly deﬁned names for their",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01190-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The technological landscape of collaborative model-driven software engineering",
        "submission-date": "2023/10",
        "publication-date": "2025/02",
        "abstract": "Collaborative technologies are continuously evolving to address existing problems and introduce innovative features for enhancing collaboration in the landscape of model-driven software engineering (MDSE). Different collaborative MDSE technologies (CMTs) provide different solutions to facilitate collaboration, making it hard for practitioners to choose the technology that best suits their needs. This study aims to investigate the landscape of CMTs and to provide a list of recommended technologies tailored to speciﬁc use case scenarios in the context of MDSE. We compiled a comprehensive list of CMTs using a systematic search complemented with snowballing, investigating both academic and grey literature. The technologies were selected through a set of inclusion and exclusion criteria and eventually analyzed through an in-depth analysis focusing on model management, collaboration, and communication. The ﬁndings of our study reveal that the current landscape of CMTs is characterized by a relatively narrow range of capabilities offered by different technologies. Consequently, practitioners often have to become proﬁcient in combining several different technologies in order to meet their needs. While various CMTs offer distinct collaboration approaches, the current landscape could be richer in terms of capabilities. Our research provides a comprehensive description of recommended CMTs, enabling practitioners to make informed decisions and improve collaboration in their MDSE processes.",
        "keywords": [
            "Collaborative modeling",
            "Model-driven software engineering",
            "Collaborative modeling technologies"
        ],
        "authors": [
            "Abhishek Choudhury",
            "Ivano Malavolta",
            "Federico Ciccozzi",
            "Kousar Aslam",
            "Patricia Lago"
        ],
        "file_path": "data/sosym-all/s10270-025-01274-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mashup of metalanguages and its implementation in the Kermeta language workbench",
        "submission-date": "2012/01",
        "publication-date": "2013/06",
        "abstract": "With the growing use of domain-speciﬁc languages (DSL) in industry, DSL design and implementa-tion goes far beyond an activity for a few experts only and becomes a challenging task for thousands of software engi-neers. DSL implementation indeed requires engineers to care for various concerns, from abstract syntax, static semantics, behavioral semantics, to extra-functional issues such as run-time performance. This paper presents an approach that uses one metalanguage per language implementation concern. We show that the usage and combination of those metalanguages is simple and intuitive enough to deserve the term mashup. We evaluate the approach by completely implementing the non-trivial fUML modeling language, a semantically sound and executable subset of the Uniﬁed Modeling Language (UML).",
        "keywords": [
            "DSL design and Implementation",
            "Model-driven engineering",
            "Software language engineering"
        ],
        "authors": [
            "Jean-Marc Jézéquel",
            "Benoit Combemale",
            "Olivier Barais",
            "Martin Monperrus",
            "François Fouquet"
        ],
        "file_path": "data/sosym-all/s10270-013-0354-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Kermeta"
        }
    },
    {
        "title": "The KeY tool\nIntegrating object oriented design and formal veriﬁcation",
        "submission-date": "2002/12",
        "publication-date": "2004/04",
        "abstract": "KeY is a tool that provides facilities for formal\nspeciﬁcation and veriﬁcation of programs within a com-\nmercial platform for UML based software development.\nUsing the KeY tool, formal methods and object-oriented\ndevelopment techniques are applied in an integrated man-\nner. Formal speciﬁcation is performed using the Object\nConstraint Language (OCL), which is part of the UML\nstandard. KeY provides support for the authoring and for-\nmal analysis of OCL constraints. The target language of\nKeY based development is Java Card DL, a proper sub-\nset of Java for smart card applications and embedded\nsystems. KeY uses a dynamic logic for Java Card DL to\nexpress proof obligations, and provides a state-of-the-art\ntheorem prover for interactive and automated veriﬁca-\ntion. Apart from its integration into UML based software\ndevelopment, a characteristic feature of KeY is that for-\nmal speciﬁcation and veriﬁcation can be introduced in-\ncrementally.",
        "keywords": [
            "Object-oriented design",
            "Formal speciﬁcation",
            "Formal veriﬁcation",
            "UML",
            "OCL",
            "Design patterns",
            "Java"
        ],
        "authors": [
            "Wolfgang Ahrendt",
            "Thomas Baar",
            "Bernhard Beckert",
            "Richard Bubel",
            "Martin Giese",
            "Reiner H¨ahnle",
            "Wolfram Menzel",
            "Wojciech Mostowski",
            "Andreas Roth",
            "Steﬀen Schlager",
            "Peter H. Schmitt"
        ],
        "file_path": "data/sosym-all/s10270-004-0058-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design notations for secure software: a systematic literature review",
        "submission-date": "2014/07",
        "publication-date": "2015/08",
        "abstract": "In the past 10years, the research community has produced a signiﬁcant number of design notations to represent security properties and concepts in a design artifact. These notations are aimed at documenting and analyzing security in a software design model. The fragmentation of the research space, however, has resulted in a complex tangle of different techniques. Hence, practitioners are confronted with the challenging task of scouting the right approach from a multitude of proposals. Similarly, it is hard for researchers to keep track of the synergies among the existing notations, in order to identify the existing opportunities for original contributions. This paper presents a systematic literature review that inventorizes the existing notations and provides an in-depth, comparative analysis for each.",
        "keywords": [
            "Security",
            "Notation",
            "Software design",
            "Empirical study"
        ],
        "authors": [
            "Alexander van den Berghe",
            "Riccardo Scandariato",
            "Koen Yskout",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-015-0486-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ModelXGlue: a benchmarking framework for ML tools in MDE",
        "submission-date": "2023/05",
        "publication-date": "2024/06",
        "abstract": "The integration of machine learning (ML) into model-driven engineering (MDE) holds the potential to enhance the efﬁciency of modelers and elevate the quality of modeling tools. However, a consensus is yet to be reached on which MDE tasks can derive substantial beneﬁts from ML and how progress in these tasks should be measured. This paper introduces ModelXGlue, a dedicated benchmarking framework to empower researchers when constructing benchmarks for evaluating the application of ML to address MDE tasks. A benchmark is built by referencing datasets and ML models provided by other researchers, and by selecting an evaluation strategy and a set of metrics. ModelXGlue is designed with automation in mind and each component operates in an isolated execution environment (via Docker containers or Python environments), which allows the execution of approaches implemented with diverse technologies like Java, Python, R, etc. We used ModelXGlue to build reference benchmarks for three distinct MDE tasks: model classiﬁcation, clustering, and feature name recommendation. To build the benchmarks we integrated existing third-party approaches in ModelXGlue. This shows that ModelXGlue is able to accommodate heterogeneous ML models, MDE tasks and different technological requirements. Moreover, we have obtained, for the ﬁrst time, comparable results for these tasks. Altogether, it emerges that ModelXGlue is a valuable tool for advancing the understanding and evaluation of ML tools within the context of MDE.",
        "keywords": [
            "Benchmarking",
            "Machine Learning",
            "Model-Driven Engineering"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Jesús Sánchez Cuadrado",
            "Riccardo Rubei",
            "Davide Di Ruscio"
        ],
        "file_path": "data/sosym-all/s10270-024-01183-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reproducible execution of POSIX programs with DiOS",
        "submission-date": "2020/03",
        "publication-date": "2020/10",
        "abstract": "In this paper, we describe DiOS, a lightweight model operating system, which can be used to execute programs that make use of POSIX APIs. Such executions are fully reproducible: running the same program with the same inputs twice will result in two exactly identical instruction traces, even if the program uses threads for parallelism. DiOS is implemented almost entirely in portable C and C++: although its primary platform is DiVM, a veriﬁcation-oriented virtual machine, it can be conﬁgured to also run in KLEE, a symbolic executor. Finally, it can be compiled into machine code to serve as a user-mode kernel. Additionally, DiOS is modular and extensible. Its various components can be combined to match both the capabilities of the underlying platform and to provide services required by a particular program. Components can be added to cover additional system calls or APIs or removed to reduce overhead. The experimental evaluation has three parts. DiOS is ﬁrst evaluated as a component of a program veriﬁcation platform based on DiVM. In the second part, we consider its portability and modularity by combining it with the symbolic executor KLEE. Finally, we consider its use as a standalone user-mode kernel.",
        "keywords": [
            "Software veriﬁcation",
            "Operating systems",
            "POSIX",
            "Reproducibility",
            "C/C++"
        ],
        "authors": [
            "Petr Roˇckai",
            "Zuzana Baranová",
            "Jan Mrázek",
            "Katarína Kejstová",
            "Jiˇríí Barnat"
        ],
        "file_path": "data/sosym-all/s10270-020-00837-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Basic problems in multi-view modeling",
        "submission-date": "2016/02",
        "publication-date": "2017/12",
        "abstract": "Modeling all aspects of a complex system within a single model is a difﬁcult, if not impossible, task. Multi-view modeling is a methodology where different aspects of the system are captured by different models, or views. A key question then is consistency: if different views of a system have some degree of overlap, how can we guarantee that they are consistent, i.e., that they do not contradict each other? In this paper we formulate this and other basic problems in multi-view modeling within an abstract formal framework. We then instantiate this framework onto several discrete system settings: languages and automata over ﬁnite and inﬁnite words, and symbolic transition systems; and study how checking view consistency and other problems can be solved in these settings.",
        "keywords": [
            "Formal methods",
            "System modeling",
            "Views",
            "Veriﬁcation",
            "Synthesis",
            "Consistency",
            "Automata",
            "Symbolic transition systems",
            "Projection",
            "Inverse projection"
        ],
        "authors": [
            "Jan Reineke",
            "Christos Stergiou",
            "Stavros Tripakis"
        ],
        "file_path": "data/sosym-all/s10270-017-0638-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based simulation of legal policies: framework, tool support, and validation",
        "submission-date": "2016/01",
        "publication-date": "2016/07",
        "abstract": "Simulation of legal policies is an important decision-support tool in domains such as taxation. The primary goal of legal policy simulation is predicting how changes in the law affect measures of interest, e.g., revenue. Legal policy simulation is currently implemented using a combination of spreadsheets and software code. Such a direct implementation poses a validation challenge. In particular, legal experts often lack the necessary software background to review complex spreadsheets and code. Consequently, these experts currently have no reliable means to check the correctness of simulations against the requirements envisaged by the law. A further challenge is that representative data for simulation may be unavailable, thus necessitating a data generator. A hard-coded generator is difﬁcult to build and validate. We develop a framework for legal policy simulation that is aimed at addressing the challenges above. The framework uses models for specifying both legal policies and the probabilistic characteristics of the underlying population. We devise an automated algorithm for simulation data generation. We evaluate our framework through a case study on Luxembourg’s Tax Law.",
        "keywords": [
            "Legal policies",
            "Simulation",
            "UML profiles",
            "Model-driven code generation",
            "Probabilistic data generation"
        ],
        "authors": [
            "Ghanem Soltana",
            "Nicolas Sannier",
            "Mehrdad Sabetzadeh",
            "Lionel C. Briand"
        ],
        "file_path": "data/sosym-all/s10270-016-0542-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "FlexiSketch: a lightweight sketching and metamodeling approach for end-users",
        "submission-date": "2016/09",
        "publication-date": "2017/09",
        "abstract": "Engineers commonly use paper and whiteboards to sketch and discuss ideas in early phases of requirements elicitation and software modeling. These physical media foster creativity because they are quick to use and do not restrict in any way the form in which content can be drawn. If the sketched information needs to be reused later on, however, engineers have to spend extra effort for preserving the information in a form that can be processed by a software modeling tool. While saving information in a machine-readable way comes for free with formal software modeling tools, they typically anticipate the use of speciﬁc, predeﬁned modeling languages and therefore hamper creativity. To combine the advantages of informal and formal tools, we have developed a ﬂexible tool-supported modeling approach that augments a sketching environment with lightweight metamodeling capabilities. Users can create their own modeling languages by deﬁning sketched constructs on demand and export model sketches as semiformal models. In this article, we ﬁrst give an overview of FlexiSketch and then focus on an evaluation of our approach with two studies conducted with both novice modelers and experienced practitioners. Our goal was to ﬁnd out how well modelers manage to use our lightweight metamodeling mechanisms, and how they build notations collaboratively. Results show that experienced modelers adopt our approach quickly, while novices have difﬁculties to distinguish between the model and meta-model levels and would beneﬁt from additional guidance and user awareness features. The lessons learned from our studies can serve as advice for similar ﬂexible modeling approaches.",
        "keywords": [
            "Requirements engineering",
            "Tool",
            "Sketching",
            "Ad hoc modeling",
            "Notation deﬁnition",
            "End-user metamodeling",
            "Lightweight metamodeling",
            "Collaborative metamodeling",
            "Evaluation"
        ],
        "authors": [
            "Dustin Wüest",
            "Norbert Seyff",
            "Martin Glinz"
        ],
        "file_path": "data/sosym-all/s10270-017-0623-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Diagrammatic physical robot models",
        "submission-date": "2022/11",
        "publication-date": "2025/03",
        "abstract": "Simulation is a favoured technique in robotics. It is, however, costly, in terms of development time, and its usability is limited by the lack of standardisation and portability of simulators. We present RoboSim, a diagrammatic tool-independent domain-specific language to model robotic platforms and their controllers. It can be regarded as a profile of UML/SysML enriched with time primitives, differential equations, and a mathematical semantics. Our previous work on RoboSim described a notation to specify control software. In this paper, we present a novel notation to describe physical models: block diagrams that can be linked to the platform-independent software model to characterise how services required by the software are realised by actuators and sensors. Behaviours are specified by differential equations, and simulations and mathematical models of the whole system can be generated automatically. Our main contributions are a modular and extensible diagrammatic notation that supports the explicit specification of physical behaviours; a set of validation rules that identify well-formed models; a model-to-model transformation from RoboSim to an input format accepted by several simulators; and a formal semantics for mathematical reasoning.",
        "keywords": [
            "Simulation",
            "Veriﬁcation",
            "SDF",
            "Hybrid models",
            "Diagrammatic models"
        ],
        "authors": [
            "Alvaro Miyazawa",
            "Sharar Ahmadi",
            "Ana Cavalcanti",
            "James Baxter",
            "Mark Post",
            "Pedro Ribeiro",
            "Jon Timmis",
            "Thomas Wright"
        ],
        "file_path": "data/sosym-all/s10270-025-01270-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Practitioners’ experiences with model-driven engineering: a meta-review",
        "submission-date": "2021/10",
        "publication-date": "2022/07",
        "abstract": "The Object Management Group introduced the Model-Driven Architecture in 2001. Since then, the research community has embraced model-driven engineering (MDE), but to a lesser extent than practitioners had hoped. A good awareness of practitioners’ challenges, particularly with modeling, is required to ensure the relevance of a research agenda. Therefore, this study conducts a meta-review on the state of practice in using modeling languages for software engineering over the last ﬁve years using Kitchenham’s guidelines. This study serves as an orientation within the research ﬁeld and a basis for further research. It contributes to the literature by focusing on publications discussing the practical use of modeling languages and the beneﬁts and problems perceived by practitioners. The main ﬁnding of this review is that practitioners beneﬁt from MDE in the following ways: it is beneﬁcial for several stakeholders; it saves cost; it is easy to use; it improves productivity, quality, and understanding of the system; and it provides support for software development activities. However, practitioners continue to face several serious challenges. The most frequently reported issues are the missing tool functionalities. Many studies have found that adhering to the Physics of Notation principles would improve modeling languages. Other ﬁndings include that modeling is mostly used for documentation and requirements elicitation, and UML is the most often used.",
        "keywords": [
            "Model-driven engineering",
            "Modeling in practice",
            "Systematic literature review",
            "Meta-review",
            "UML",
            "BPMN",
            "Conceptual modeling"
        ],
        "authors": [
            "Charlotte Verbruggen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-022-01020-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Modeling Object-Oriented Information Systems",
        "submission-date": "2001/08",
        "publication-date": "2004/04",
        "abstract": "Object-Oriented Information System (OOIS) is an information system that employs object-oriented technologies in system design and implementation. Recent research advances and industrial innovations in distributed system modeling and Internet applications have enabled OOIS design and implementation to be carried out on the basis of new technologies and platforms. This special section on Modeling Object-Oriented Information Systems presents readers with a set of best papers selected from the 7th International Conference on OOIS. Reviews of theories and applications of OOIS’s are also provided for predicating trends in OOIS modeling.",
        "keywords": [
            "System modeling",
            "Information systems",
            "Object orientation",
            "Foundations",
            "Architectures",
            "Distributed objects",
            "Patterns",
            "Trends"
        ],
        "authors": [
            "Yingxu Wang",
            "Shushma Patel"
        ],
        "file_path": "data/sosym-all/s10270-004-0053-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling and enforcing invariants of dynamic software architectures",
        "submission-date": "2008/10",
        "publication-date": "2010/04",
        "abstract": "In this paper, we propose an “end-to-end” approach that supports dynamic reconﬁguration of software architectures taking advantage of graphical modeling, formal methods and aspect-oriented programming. There are three ingredients of the proposal. The speciﬁcation end of the solution is covered by a new UML proﬁle enabling to specify the desired architectural style (model), its invariants and the intended reconﬁguration operations. In order to verify the consistency of the model and the preservation of the invariants aftereveryreconﬁguration,weautomaticallygenerateformal speciﬁcations in Z notation from the deﬁned model. At the runtime enforcing end of the solution, we propose to encode the enforcement logic as aspect in the AspectJ language. The third important ingredient that makes our approach end-to-end is the automatic translation of formal speciﬁcations into aspect-based enforcement code.",
        "keywords": [
            "Software architecture",
            "UML proﬁle",
            "Formal speciﬁcation and veriﬁcation",
            "Aspect-oriented programming",
            "Runtime enforcement"
        ],
        "authors": [
            "Slim Kallel",
            "Mohamed Hadj Kacem",
            "Mohamed Jmaiel"
        ],
        "file_path": "data/sosym-all/s10270-010-0162-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mutation testing with hyperproperties",
        "submission-date": "2020/02",
        "publication-date": "2021/04",
        "abstract": "We present a new method for model-based mutation-driven test case generation. Mutants are generated by making small syntactical modiﬁcations to the model or source code of the system under test. A test case kills a mutant if the behavior of the mutant deviates from the original system when running the test. In this work, we use hyperproperties—which allow to express relations between multiple executions—to formalize different notions of killing for both deterministic as well as non-deterministic models. The resulting hyperproperties are universal in the sense that they apply to arbitrary reactive models and mutants. Moreover, an off-the-shelf model checking tool for hyperproperties can be used to generate test cases. Furthermore, we propose solutions to overcome the limitations of current model checking tools via a model transformation and a bounded SMT encoding. We evaluate our approach on a number of models expressed in two different modeling languages by generating tests using a state-of-the-art mutation testing tool.",
        "keywords": [],
        "authors": [
            "Andreas Fellner",
            "Mitra Tabaei Befrouei",
            "Georg Weissenbacher"
        ],
        "file_path": "data/sosym-all/s10270-020-00850-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial for the Speccial Issue UML 2001 Conference",
        "submission-date": "2002/12",
        "publication-date": "2002/12",
        "abstract": "This paper is an editorial for a special issue of SoSyM focusing on the UML 2001 conference. It discusses the aims and scope of SoSyM, the importance of UML in the software development community, and the context of the UML 2001 conference within the evolution of the UML standard.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-002-0007-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSyM reﬂections: the 2021 \"state of the journal\" report",
        "submission-date": "2022/02",
        "publication-date": "2022/02",
        "abstract": "The inaugural issue of each SoSyM volume-year is usually written during a time of personal and professional reﬂection. Over the past 12 months, researchers from all disciplines continued to experience the need for ﬂexibility as the COVID-19 pandemic extended its impact across the globe through many variants. This has forced us all to adapt in many ways such that the idea of “virtual everything” permeates our daily discussions. The software and systems modeling community continued to thrive and exhibited a great degree of ﬂexibil-ity and increased production in the overall research output, despite the limitations for in-person collaborations. In fact, for many years, the ﬁrst issue of each SoSyM volume-year has reported growth in submissions and other important met-rics. The same is true for the 2021 publication year of SoSyM.",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-022-00979-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tradeoffs in modeling performance of highly conﬁgurable software systems",
        "submission-date": "2016/07",
        "publication-date": "2018/02",
        "abstract": "Modeling the performance of a highly conﬁgurable software system requires capturing the inﬂuences of its conﬁguration optionsandtheirinteractionsonthesystem’sperformance.Performance-inﬂuencemodelsquantifytheseinﬂuences,explaining this way the performance behavior of a conﬁgurable system as a whole. To be useful in practice, a performance-inﬂuence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively inﬂuence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large conﬁguration space can be described accurately only with large models and signiﬁcant resource investment. By means of 10 real-world highly conﬁgurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-inﬂuence model can ﬁt different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more conﬁguration options have only a minor inﬂuence on the prediction error and that ignoring them when learning a performance-inﬂuence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on speciﬁc regions of the conﬁguration space and ﬁnd a sweet spot between accuracy and effort. We further analyzed the causes for the conﬁguration options and their interactions having the observed inﬂuences on the systems’ performance. We were able to identify several patterns across subject systems, such as dominant conﬁguration options and data pipelines, that explain the inﬂuences of highly inﬂuential conﬁguration options and interactions, and give further insights into the domain of highly conﬁgurable systems.",
        "keywords": [
            "Performance-inﬂuence models",
            "Highly conﬁgurable software systems",
            "Performance prediction",
            "Feature interactions",
            "Variability",
            "Software product lines",
            "Machine learning"
        ],
        "authors": [
            "Sergiy Kolesnikov",
            "Norbert Siegmund",
            "Christian Kästner",
            "Alexander Grebhahn",
            "Sven Apel"
        ],
        "file_path": "data/sosym-all/s10270-018-0662-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On applying residual reasoning within neural network veriﬁcation",
        "submission-date": "2023/02",
        "publication-date": "2023/11",
        "abstract": "As neural networks are increasingly being integrated into mission-critical systems, it is becoming crucial to ensure that they meet various safety and liveness requirements. Toward, that end, numerous complete and sound veriﬁcation techniques have been proposed in recent years, but these often suffer from severe scalability issues. One recently proposed approach for improving the scalability of veriﬁcation techniques is to enhance them with abstraction/reﬁnement capabilities: instead of verifying a complex and large network, abstraction allows the veriﬁer to construct and then verify a much smaller network, and the correctness of the smaller network immediately implies the correctness of the original, larger network. One shortcoming of this scheme is that whenever the smaller network cannot be veriﬁed, the veriﬁer must perform a reﬁnement step, in which the size of the network being veriﬁed is increased. The veriﬁer then starts verifying the new network from scratch—effectively “forgetting” its earlier work, in which the smaller network was veriﬁed. Here, we present an enhancement to abstraction-based neural network veriﬁcation, which uses residual reasoning: a process where information acquired when verifying an abstract network is utilized in order to facilitate the veriﬁcation of reﬁned networks. At its core, the method enables the veriﬁer to retain information about parts of the search space in which it was determined that the reﬁned network behaves correctly, allowing the veriﬁer to focus on areas of the search space where bugs might yet be discovered. For evaluation, we implemented our approach as an extension to the Marabou veriﬁer and obtained highly promising results.",
        "keywords": [
            "Neural networks",
            "Veriﬁcation",
            "Abstraction reﬁnement",
            "Residual reasoning",
            "Incremental reasoning"
        ],
        "authors": [
            "Yizhak Yisrael Elboher",
            "Elazar Cohen",
            "Guy Katz"
        ],
        "file_path": "data/sosym-all/s10270-023-01138-w.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "An ontology-based approach to engineering ethicality requirements",
        "submission-date": "2022/11",
        "publication-date": "2023/07",
        "abstract": "In a world where Artiﬁcial Intelligence (AI) is pervasive, humans may feel threatened or at risk by giving up control to machines. In this context, ethicality becomes a major concern to prevent AI systems from being biased, making mistakes, or going rogue. Requirements Engineering (RE) is the research area that can exert a great impact in the development of ethical systems by design. However, proposing concepts, tools and techniques that support the incorporation of ethicality into the software development processes as explicit requirements remains a great challenge in the RE ﬁeld. In this paper, we rely on Ontology-based Requirements Engineering (ObRE) as a method to elicit and analyze ethicality requirements (‘Ethicality requirements’ is adopted as a name for the class of requirements studied in this paper by analogy to other quality requirements studied in software engineering, such as usability, reliability, and portability, etc. The use of this term (as opposed to ‘ethical requirements’) highlights that they represent requirements for ethical systems, analogous to how ‘trustworthiness requirements’ represent requirements for trustworthy systems. To put simply: the predicates ‘ethical’ or ‘trustworthy’ are not meant to be predicated over the requirements themselves). ObRE applies ontological analysis to ontologically unpack terms and notions that are referred to in requirements elicitation. Moreover, this method instantiates the adopted ontology and uses it to guide the requirements analysis activity. In a previous paper, we presented a solution concerning two ethical principles, namely Beneﬁcence and Non-maleﬁcence. The present paper extends the previous work by targeting two other important ethicality principles, those of Explicability and Autonomy. For each of these new principles, we do ontological unpacking of the relevant concepts, and we present requirements elicitation and analysis guidelines, as well as examples in the context of a driverless car case. Furthermore, we validate our approach by analysing the requirements elicitation made for the driverless car case in contrast with a similar case, and by assessing our method’s coverage w.r.t European Union guidelines for Trustworthy AI.",
        "keywords": [
            "Requirements elicitation and analysis",
            "Ontological analysis",
            "Foundational ontologies",
            "Ethicality requirements",
            "Ethical systems"
        ],
        "authors": [
            "Renata Guizzardi",
            "Glenda Amaral",
            "Giancarlo Guizzardi",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-023-01115-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Event-driven grammars: relating abstract and concrete levels of visual languages",
        "submission-date": "2005/02",
        "publication-date": "2007/03",
        "abstract": "In this work we introduce event-driven grammars, a kind of graph grammars that are especially suited for visual modelling environments generated by meta-modelling. Rules in these grammars may be triggered by user actions (such as creating, editing or connecting elements) and in their turn may trigger other user-interface events. Their combination with triple graph transformation systems allows constructing and checking the consistency of the abstract syntax graph while the user is building the concrete syntax model, as well as managing the layout of the concrete syntax representation. As an example of these concepts, we show the deﬁnition of a modelling environment for UML sequence diagrams. A discussion is also presented of methodological aspects for the generation of environments for visual languages with multiple views, its connection with triple graph grammars, the formalization of the latter in the double pushout approach and its extension with an inheritance concept.",
        "keywords": [
            "Graph Grammars",
            "Triple Graph Transformation",
            "Meta-Modelling",
            "Visual Languages",
            "Consistency",
            "UML"
        ],
        "authors": [
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-007-0051-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Merging of EMF models",
        "submission-date": "2011/03",
        "publication-date": "2012/10",
        "abstract": "Inadequate version control for models significantly impedes the application of model-driven software development. In particular, sophisticated support for merging model versions is urgently needed. In this paper, we present a formal approach to both two- and three-way merging of models in the EMF framework. The approach may be applied to instances of arbitrary Ecore models. We specify context-free as well as context-sensitive rules for model merging which both detect and resolve merge conflicts. Based on these rules, a merge algorithm is developed which produces a consistent model from consistent input models. The merge algorithm does neither assume unique object identifiers, nor does it require change logs. In contrast, it relies on matchings among the input models which identify common elements (state-based approach). The requirements imposed on these matchings are reduced to a minimum, e.g., there are no restrictions on the relative positions of matched elements. Altogether, the merge algorithm is widely applicable, preserves consistency, and offers advanced features, such as merging of ordered collections in the presence of arbitrary moves and handling of context-sensitive conflicts which are hard to detect and to resolve.",
        "keywords": [
            "EMF models",
            "Three-way merging",
            "Version control"
        ],
        "authors": [
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-012-0279-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "DEPS: a model- and property-based language for system synthesis problems",
        "submission-date": "2022/11",
        "publication-date": "2023/10",
        "abstract": "DEPS (design problem speciﬁcation) is a new modeling language designed to pose and solve system design problems. DEPS addresses problems of sizing, conﬁguration, resource allocation and of architecture generation for systems. Unlike system modeling languages, which are dedicated to the representation of a deﬁned system for evaluation or analysis, we propose a problem modeling language for representing the design problem with a view to its automatic resolution. Compared with other declarative problem modeling languages, DEPS is a declarative structured and property-based language that combinesstructuralmodelingfeaturesspeciﬁctoobject-orientedlanguageswithproblemspeciﬁcationfeaturesfromconstraint programming. The mathematical nature of the problems is described by formal properties encapsulated in models organized according to the architecture of the studied system. The main features of the language are presented in details and are illustrated with examples in different domains. An integrated modeling and solving environment called DEPS Studio allows the designer to express its models in DEPS, to compile the models and to compute automatically the solutions. The validation of the approach is done through two case studies. Finally, we will conclude with the studies and developments in progress which will be integrated into the next version of DEPS Studio.",
        "keywords": [
            "Model-based system synthesis",
            "Speciﬁcation",
            "Design problem modeling language",
            "Problem solving",
            "Constraint programming"
        ],
        "authors": [
            "Pierre-Alain Yvars",
            "Laurent Zimmer"
        ],
        "file_path": "data/sosym-all/s10270-023-01129-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reusable model transformations",
        "submission-date": "2009/11",
        "publication-date": "2010/11",
        "abstract": "Model transformations written for an input metamodel may often apply to other metamodels that share similar concepts. For example, a transformation written to refactor Java models can be applicable to refactoring UML class diagrams as both languages share concepts such as classes,methods,attributes,andinheritance.Derivingmotivation from this example, we present an approach to make model transformations reusable such that they function correctly across several similar metamodels. Our approach relies on these principal steps: (1) We analyze a transformation to obtain an effective subset of used concepts. We prune the input metamodel of the transformation to obtain an effec- tive input metamodel containing the effective subset. The effective input metamodel represents the true input domain of transformation. (2) We adapt a target input metamodel by weaving it with aspects such as properties derived from the effective input metamodel. This adaptation makes the tar- get metamodel a subtype of the effective input metamodel. The subtype property ensures that the transformation can process models conforming to the target input metamodel without any change in the transformation itself. We validate our approach by adapting well known refactoring transfor- mations (Encapsulate Field, Move Method, and Pull Up Method) written for an in-house domain-speciﬁc modeling language (DSML) to three different industry standard meta- models (Java, MOF, and UML).",
        "keywords": [
            "Adaptation",
            "Aspect weaving",
            "Genericity",
            "Metamodel pruning",
            "Model typing",
            "Model transformation",
            "Refactoring"
        ],
        "authors": [
            "Sagar Sen",
            "Naouel Moha",
            "Vincent Mahé",
            "Olivier Barais",
            "Benoit Baudry",
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-010-0181-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated generation of consistent models using qualitative abstractions and exploration strategies",
        "submission-date": "2021/02",
        "publication-date": "2021/09",
        "abstract": "Automatically synthesizing consistent models is a key prerequisite for many testing scenarios in autonomous driving to ensure a designated coverage of critical corner cases. An inconsistent model is irrelevant as a test case (e.g., false positive); thus, each synthetic model needs to simultaneously satisfy various structural and attribute constraints, which includes complex geometric constraints for trafﬁc scenarios. While different logic solvers or dedicated graph solvers have recently been developed, they fail to handle either structural or attribute constraints in a scalable way. In the current paper, we combine a structural graph solver that uses partial models with an SMT-solver and a quadratic solver to automatically derive models which simultaneously fulﬁll structural and numeric constraints, while key theoretical properties of model generation like completeness or diversity are still ensured. This necessitates a sophisticated bidirectional interaction between different solvers which carry out consistency checks, decision, unit propagation, concretization steps. Additionally, we introduce custom exploration strategies to speed up model generation. We evaluate the scalability and diversity of our approach, as well as the inﬂuence of customizations, in the context of four complex case studies.",
        "keywords": [
            "Model generation",
            "Partial model",
            "Graph solver",
            "SMT-solver",
            "Numeric solver",
            "Exploration strategy",
            "Test scenario synthesis"
        ],
        "authors": [
            "Aren A. Babikian",
            "Oszkár Semeráth",
            "Anqi Li",
            "Kristóf Marussy",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-021-00918-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Conceptual distance of models and languages",
        "submission-date": "2019/05",
        "publication-date": "2019/05",
        "abstract": "Conceptual distance (i.e., measurement of the distance between two sets of concepts) had its roots in linguistics as the semantic distance problem. In the linguistics context, conceptual distance provides a metric of the difficulty in understanding a topic across different disciplines or subject areas. However, it seems that there is no commonly agreed and well-founded theory that allows measurement of the distance between two sets of concepts. We believe that it is worthwhile to measure the conceptual distance between models, and also between languages.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-019-00734-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A graph grammar-based formal validation of object-process diagrams",
        "submission-date": "2010/02",
        "publication-date": "2011/04",
        "abstract": "Two basic requirements from a system’s conceptual model are correctness and comprehensibility. Most modeling methodologies satisfy only one of these apparently contradicting requirements, usually comprehensibility, leaving aside problems of correctness and ambiguousness that are associated with expressiveness. Some formal modeling languages do exist, but in these languages a complete model of a complex system is fairly complicated to understand. Object-process methodology (OPM) is a holistic systems modeling methodology that combines the two major aspects of a system—structure and behavior—in one model, providing mechanisms to manage the complexity of the model using refinement-abstraction operations, which divide a complex system into many interconnected diagrams. Although the basic syntax and semantics of an OPM model are defined, they are incomplete and leave room for incorrect or ambiguous models. This work advances the formal definition of OPM by providing a graph grammar for creating and checking OPM diagrams. The grammar provides a validation methodology of the semantic and syntactic correctness of a single object-process diagram.",
        "keywords": [
            "Formal system model",
            "Object-process modeling",
            "Graph transformation",
            "Model veriﬁcation"
        ],
        "authors": [
            "Arieh Bibliowicz",
            "Dov Dori"
        ],
        "file_path": "data/sosym-all/s10270-011-0201-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Corpus-based analysis of domain-speciﬁc languages",
        "submission-date": "2012/03",
        "publication-date": "2013/06",
        "abstract": "As more domain-speciﬁc languages (DSLs) are\ndesigned and developed, the need to evaluate these languages\nbecomes an essential part of the overall DSL life cycle.\nCorpus-based analysis can serve as an evaluation mecha-\nnism to identify characteristics of the language after it has\nbeen deployed by looking at how end-users employ it in\npractice. This analysis that is based on actual usage of the\nlanguage brings a new perspective which can be considered\nby a language engineer when working toward improving\nthe language. In this paper, we describe our utilization of\ncorpus-based analysis techniques and exemplify them on the\nevaluation of the Puppet and ATL DSLs. We also outline an\nEclipse plug-in, which is a generic corpus-based DSL analy-\nsis tool that can accommodate the evaluation of different\nDSLs.",
        "keywords": [
            "Domain-speciﬁc languages",
            "DSL",
            "Corpus",
            "Analysis",
            "ATL",
            "Puppet"
        ],
        "authors": [
            "Robert Tairas",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-013-0352-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model hybridization: towards a unifying theory for inductive and deductive reasoning",
        "submission-date": "2024/12",
        "publication-date": "2024/12",
        "abstract": "Whilesoftwareisrevolutionizingthemodernworld,software engineering practices must keep pace accordingly. Modern software-based systems operate under rapidly changing conditions and face ever-increasing uncertainty. These dynamics demand accelerated adaptability, or more precisely, temporal adaptability—the ability to adapt not only to a ﬁxed set of requirements but also to an evolving sequence of variable requirements, which are often increasingly driven by newly incoming usage and context data. This adaptability corresponds to how humans handle uncertainty in dynamic environments by continuously updating their mental models to accommodate new information. As a result, the traditional boundary between development-time and operations-time is blurring. \nTo address such a pressing context, current development processes often implement agile methodologies that increase the release frequency to leverage the available runtime and telemetry data, eventually driving the overall roadmap. These continuous engineering processes promise tremendous potential for gaining insights, optimizing operations, and improving decision-making. This is true in the software industry, but also in the broader scope of cyber-physical systems accompanied by so-called digital twins, across various industries, including manufacturing, health-care, transportation, and more. Recent developments show that MDE can—or maybe even must—play a central role in systematically leveraging the runtime and telemetry data to cope with this new temporal adaptability, and many researchers from the MDE community have investigated MDE technology to provide a smooth continuum between development and operations.\nIn the future, predictive capabilities in the context of continuous engineering will be leveraged for dynamically evolving ecosystems to address challenges such as sustainability at a much more complex scale. To this aim, techniques for the coordinated use of heterogeneous descriptive, predictive, and prescriptive models need to be elaborated, and the propagation of uncertainty investigated, leading towards the deﬁnition of a unifying theory for inductive and deductive reasoning.\nAt the heart and soul of the combination of inductive and deductive reasoning is the need for model hybridization. With this term, we mean that there is an urgent need and opportunity for a systematic approach to combine heterogeneous models, such as architectural models, continuous and discrete-event behavioral models, and statistical models (e.g., machine learning models).",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01249-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Flexible and configurable verification policies with Omnibus",
        "submission-date": "2006/04",
        "publication-date": "2007/06",
        "abstract": "The three main assertion-based verification approaches are: run-time assertion checking (RAC), extended static checking (ESC) and full formal verification (FFV). Each approach offers a different balance between rigour and ease of use, making them appropriate in different situations. Our goal is to explore the use of these approaches together in a flexible way, enabling an application to be broken down into parts with different reliability requirements and different verification approaches used in each part. We explain the benefits of using the approaches together, present a set of guidelines to avoid potential conflicts and give an overview of how the Omnibus IDE provides support for the full range of assertion-based verification approaches within a single tool.",
        "keywords": [],
        "authors": [
            "Thomas Wilson",
            "Savi Maharaj",
            "Robert G. Clark"
        ],
        "file_path": "data/sosym-all/s10270-007-0060-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "State-based versus event-based speciﬁcations for information systems: a comparison of B and EB3",
        "submission-date": "2005/05",
        "publication-date": "2005/05",
        "abstract": "This paper compares two formal methods, B and eb3, for specifying information systems. These two methods are chosen as examples of the state-based paradigm and the event-based paradigm, respectively. The paper considers four viewpoints: functional behav-ior expression, validation, veriﬁcation, and evolution. Issues in expressing event ordering constraints, data in-tegrity constraints, and modularity are thereby consid-ered. A simple case study is used to illustrate the compar-ison, namely, a library management system. Two equiva-lent speciﬁcations are presented using each method. The paper concludes that B and eb3 are complementary. The former is better at expressing complex ordering and static data integrity constraints, whereas the latter provides a simpler, modular, explicit representation of dynamic constraints that are closer to the user’s point of view, while providing loosely coupled deﬁnitions of data attributes. The generality of these results from the state-based paradigm and the event-based paradigm per-spective are discussed.",
        "keywords": [
            "State-based paradigm",
            "Event-based paradigm",
            "eb3",
            "B",
            "Process algebra",
            "Information system",
            "Formal speciﬁcation"
        ],
        "authors": [
            "Benoˆıt Fraikin",
            "Marc Frappier",
            "R´egine Laleau"
        ],
        "file_path": "data/sosym-all/s10270-005-0083-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A feature-based survey of model view approaches",
        "submission-date": "2017/01",
        "publication-date": "2017/09",
        "abstract": "When dealing with complex systems, information is very often fragmented across many different models expressed within a variety of (modeling) languages. To provide the relevant information in an appropriate way to different kinds of stakeholders, (parts of) such models have to be combined and potentially revamped by focusing on concerns of particular interest for them. Thus, mechanisms to deﬁne and compute views over models are highly needed. Several approaches have already been proposed to provide (semi)automated support for dealing with such model views. This paper provides a detailed overview of the current state of the art in this area. To achieve this, we relied on our own experiences of designingandapplyingsuchsolutions inorder to conduct a literature review on this topic. As a result, we discuss the main capabilities of existing approaches and pro- pose a corresponding research agenda. We notably contribute a feature model describing what we believe to be the most important characteristics of the support for views on models. We expect this work to be helpful to both current and poten- tial future users and developers of model view techniques, as well as to any person generally interested in model-based software and systems engineering.",
        "keywords": [
            "Modeling",
            "Viewpoint",
            "View",
            "Model",
            "Survey"
        ],
        "authors": [
            "Hugo Bruneliere",
            "Erik Burger",
            "Jordi Cabot",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-017-0622-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Testing models and model transformations using classifying terms",
        "submission-date": "2016/01",
        "publication-date": "2016/12",
        "abstract": "This paper proposes the use of equivalence partitioning techniques for testing models and model transformations. In particular, we introduce the concept of classifying terms, which are general OCL terms on a class model enriched with OCL constraints. Classifying terms permit deﬁning equivalence classes, in particular for partitioning the source and target model spaces of the transformation, deﬁning for each class a set of equivalent models with regard to the transformation. Using these classes, a model validator tool is able to automatically construct object models for each class, which constitute relevant test cases for the transformation. We show how this approach of guiding the construction of test cases in an orderly, systematic and efﬁcient manner can be effectively used in combination with Tracts for testing both directional and bidirectional model transformations and for analyzing their behavior.",
        "keywords": [
            "Model transformations",
            "Contract-based speciﬁcations",
            "Equivalence partitioning"
        ],
        "authors": [
            "Frank Hilken",
            "Martin Gogolla",
            "Loli Burgueño",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-016-0568-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reusable speciﬁcation templates for deﬁning dynamic semantics of DSLs",
        "submission-date": "2016/10",
        "publication-date": "2017/03",
        "abstract": "In the context of model-driven engineering, the dynamic (execution) semantics of domain-speciﬁc languages (DSLs) is usually not speciﬁed explicitly and stays (hard) coded in model transformations and code generation. This poses challenges such as learning, debugging, understanding, maintaining, and updating a DSL. Facing the lack of supporting tools for specifying the dynamic semantics of DSLs (or programming languages in general), we propose to specify the architecture and the detailed design of the software that implements the DSL, rather than requirements for the behavior expected from DSL programs. To compose such a speciﬁcation, we use speciﬁcation templates that capture softwaredesignsolutionstypicalforthe(application)domain of the DSL. As a result, on the one hand, our approach allows for an explicit and clear deﬁnition of the dynamic semantics of a DSL, supports separation of concern and reuse of typical design solutions. On the other hand, we do not introduce (yet another) speciﬁcation formalism, but we base our approach on an existing formalism and apply its extensive tool support for veriﬁcation and validation to the dynamic semantics of a DSL.",
        "keywords": [
            "Domain-speciﬁclanguage",
            "Dynamicsemantics",
            "Speciﬁcation template",
            "Generic programming",
            "Aspect-oriented programming"
        ],
        "authors": [
            "Ulyana Tikhonova"
        ],
        "file_path": "data/sosym-all/s10270-017-0590-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Constructing and verifying a robust Mix Net using CSP",
        "submission-date": "2013/11",
        "publication-date": "2015/06",
        "abstract": "A Mix Net is a cryptographic protocol that unlinksthecorrespondencebetweenitsinputsanditsoutputs. Inthispaper,weformallyanalyseaMixNetusingtheprocess algebra CSP and its associated model checker FDR. The protocol that we verify removes the reliance on a Web Bul- letin Board: rather than communicating via a Web Bulletin Board, the protocol allows the mix servers to communicate directly, exchanging signed messages and maintaining their own records of the messages they have received. Mix Net analyses in the literature are invariably focused on safety properties; important liveness properties, such as deadlock freedom, are wholly neglected. This is an unhappy omission, however, since a Mix Net that produces no results is of little use. In contrast, we verify here that the Mix Net is guaranteed to terminate, with each honest mix server outputting the decrypted vector of plaintexts alongside a chain proving that each re-encryption/permutation and partial decryption operation was performed correctly, under the assumption that there is an honest majority of them acting according to the protocol.",
        "keywords": [
            "Mix Nets",
            "Formal methods",
            "Model checking",
            "CSP",
            "FDR"
        ],
        "authors": [
            "Efstathios Stathakidis",
            "David M. Williams",
            "James Heather"
        ],
        "file_path": "data/sosym-all/s10270-015-0474-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The complexities of the satisﬁability checking problems of feature diagram sublanguages",
        "submission-date": "2022/02",
        "publication-date": "2022/10",
        "abstract": "It is well-known that the satisﬁability problem of feature diagrams (FDs) is computationally hard. This paper examines the complexities of the satisﬁability problems of sixteen FD sublanguages, i.e., languages that only permit a subset of the syntactic modeling elements of general FDs. Previous work neglected a detailed examination of the complexities of the satisﬁability problems of FD sublanguages, although results in this area could lead to the development of efﬁcient satisﬁability checking procedures for the FDs of speciﬁc sublanguages. This paper contributes to ﬁll this gap by analyzing the complexities of the satisﬁability problems of sixteen FD sublanguages that differ in whether they allow or-groups, xor-groups, excludes constraints, and requires constraints. The results of this paper show that the satisﬁability problem is NP-complete for eight of these sublanguages, is solvable in linear time for two of these sublanguages, and is trivial for the remaining six sublanguages in the sense that every FD of these sublanguages is satisﬁable. The results are extended to feature model sublanguages with complex cross-tree constraints by extending FD sublanguages that have satisﬁability problems, which are solvable in polynomial time. The feature model sublanguages also have satisﬁability problems that are solvable in polynomial time.",
        "keywords": [
            "Feature diagram",
            "Feature model",
            "Satisﬁability",
            "Analysis",
            "Semantics",
            "Complexity",
            "NP-complete"
        ],
        "authors": [
            "Oliver Kautz"
        ],
        "file_path": "data/sosym-all/s10270-022-01048-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Variability testing in the wild: the Drupal case study",
        "submission-date": "2014/10",
        "publication-date": "2015/04",
        "abstract": "Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in singlefeaturesand160faultstriggeredbytheinteractionofupto four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.",
        "keywords": [
            "Test case prioritization",
            "Test case selection",
            "Variability-intensive systems",
            "Automated testing",
            "Non-functional properties",
            "Variability testing"
        ],
        "authors": [
            "Ana B. Sánchez",
            "Sergio Segura",
            "José A. Parejo",
            "Antonio Ruiz-Cortés"
        ],
        "file_path": "data/sosym-all/s10270-015-0459-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Decision-making under uncertainty: be aware of your priorities",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Self-adaptive systems (SASs) are increasingly leveraging autonomy in their decision-making to manage uncertainty in their operating environments. A key problem with SASs is ensuring their requirements remain satisﬁed as they adapt. The trade-off analysis of the non-functional requirements (NFRs) is key to establish balance among them. Further, when performing the trade-offs it is necessary to know the importance of each NFR to be able to resolve conﬂicts among them. Such trade-off analyses are often built upon optimisation methods, including decision analysis and utility theory. A problem with these techniques is that they use a single-scalar utility value to represent the overall combined priority for all the NFRs. However, this combined scalar priority value may hide information about the impacts of the environmental contexts on the individual NFRs’ priorities, which may change over time. Hence, there is a need for support for runtime, autonomous reasoning about the separate priority values for each NFR, while using the knowledge acquired based on evidence collected. In this paper, we propose Pri-AwaRE, a self-adaptive architecture that makes use of Multi-Reward Partially Observable Markov Decision Process (MR-POMDP) to perform decision-making for SASs while offering awareness of NFRs’ priorities. MR-POMDP is used as a priority-aware runtime speciﬁcation model to support runtime reasoning and autonomous tuning of the distinct priority values of NFRs using a vector-valued reward function. We also evaluate the usefulness of our Pri-AwaRE approach by applying it to two substantial example applications from the networking and IoT domains.",
        "keywords": [
            "Self-Adaptive systems",
            "Priorities",
            "Non-functional requirements",
            "Decision-making"
        ],
        "authors": [
            "Huma Samin",
            "Nelly Bencomo",
            "Peter Sawyer"
        ],
        "file_path": "data/sosym-all/s10270-021-00956-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An improved approach on the model checking for an agent-based simulation system",
        "submission-date": "2019/06",
        "publication-date": "2020/06",
        "abstract": "Model checking is an effective way to verify behaviours of an agent-based simulation system. Three behaviours are analysed: operational, control, and global behaviours. Global behaviours of a system emerge from operational behaviours of local components regulated by control behaviours of the system. The previous works principally focus on verifying the system from the operational point of view (operational behaviour). The satisfaction of the global behaviour of the system conforming to the control behaviour has not been investigated. Thus, in this paper, we propose a more complete approach for verifying global and operational behaviours of systems. To do so, these three behaviours are ﬁrstly formalized by automata-based techniques. The meta-transformation between automata theories and Kripke structure is then provided, in order to illustrate the feasibility for the model transformation between the agent-based simulation model and Kripke structure-based model. Then, a mapping between the models is proposed. Subsequently, the global behaviour of the system is veriﬁed by the properties extracted from the control behaviour and the operational behaviour is checked by general system performance properties (e.g. safety, deadlock freedom). Finally, a case study on the simulation system for aircraft maintenance has been carried out. A counterexample of signals sending between Flight agent and Plane agent has been produced by NuSMV model checker. Modiﬁcations for the NuSMV model and agent-based simulation model have been performed. The experiment results show that 9% out of 19% of ﬂights have been changed to be serviceable.",
        "keywords": [
            "Model checking",
            "Agent-based simulation system",
            "Global behaviours and operational behaviours",
            "Model transformation"
        ],
        "authors": [
            "Yinling Liu",
            "Tao Wang",
            "Haiqing Zhang",
            "Vincent Cheutet"
        ],
        "file_path": "data/sosym-all/s10270-020-00807-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic veriﬁcation of behavior preservation at the transformation level for relational model transformation",
        "submission-date": "2017/07",
        "publication-date": "2018/12",
        "abstract": "The correctness of model transformations is a crucial element for model-driven engineering of high-quality software. In particular, behavior preservation is an important correctness property avoiding the introduction of semantic errors during the model-driven engineering process. Behavior preservation veriﬁcation techniques show some kind of behavioral equivalence or reﬁnement between source and target model of the transformation. Automatic tool support is available for verifying behavior preservation at the instance level, i.e., for a given source and target model speciﬁed by the model transformation. However, until now there is no sound and automatic veriﬁcation approach available at the transformation level, i.e., for all source and target models. In this article, we extend our results presented in earlier work (Giese and Lambers, in: Ehrig et al (eds) Graph transformations, Springer, Berlin, 2012) and outline a new transformation-level approach for the sound and automatic veriﬁcation of behavior preservation captured by bisimulation resp. simulation for outplace model transformations speciﬁed by triple graph grammars and semantic deﬁnitions given by graph transformation rules. In particular, we ﬁrst show how behavior preservation can be modeled in a symbolic manner at the transformation level and then describe that transformation-level veriﬁcation of behavior preservation can be reduced to invariant checking of suitable conditions for graph transformations. We demonstrate that the resulting checking problem can be addressed by our own invariant checker for an example of a transformation between sequence charts and communicating automata.",
        "keywords": [
            "Relational model transformation",
            "Formal veriﬁcation of behavior preservation",
            "Behavioral equivalence and reﬁnement",
            "Bisimulation and simulation",
            "Graph transformation",
            "Triple graph grammars",
            "Invariant checking"
        ],
        "authors": [
            "Johannes Dyck",
            "Holger Giese",
            "Leen Lambers"
        ],
        "file_path": "data/sosym-all/s10270-018-00706-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Can explainable artiﬁcial intelligence support software modelers in model comprehension?",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "As software systems become increasingly complex, the application of artiﬁcial intelligence (AI) in software engineering is gaining relevance. However, a critical gap exists in the understanding and interpretation of AI-driven decision-making processes, especially in areas intrinsically linked to human expertise, such as software modeling. This paper proposes an exploratory study on the feasibility, efﬁcacy, and relevance of eXplainable Artiﬁcial Intelligence (XAI) techniques within this context. The application of machine learning (ML) to software models is relatively recent, so efforts such as the ModelSet dataset are crucial for deriving effective training data for ML models. In addition, predictive software modeling tasks present some unusual requirements, such as the need for multi-class and multi-label approaches that are not as commonly investigated from an XAI perspective. In fact, the adoption of XAI in software modeling has been barely explored and possibly requires adapted methodologies. Our approach encompasses an in-depth examination of explanations generated by ﬁve XAI techniques that evaluate feature contributions globally for ML models and locally for speciﬁc predictions. This could help software modelers understand, for example, why a model is classiﬁed in a particular domain. Additionally, our study includes a survey conducted among software modelers to capture how explanations support their decision-making, to evaluate the perceived level of agreement between different XAI techniques, and to identify current limitations. We argue that XAI can improve the transparency and trustworthiness of the decision-making process for software modelers, thereby fostering a deeper understanding of intricate modeling tasks.",
        "keywords": [
            "Software modeling",
            "Machine learning",
            "Explainable artiﬁcial intelligence"
        ],
        "authors": [
            "Francisco Javier Alcaide",
            "José Raúl Romero",
            "Aurora Ramírez"
        ],
        "file_path": "data/sosym-all/s10270-024-01251-4.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Epsilon Flock: a model migration language",
        "submission-date": "2011/03",
        "publication-date": "2012/11",
        "abstract": "Model-driven engineering introduces additional challenges for controlling and managing software evolution. Today, tools exist for generating model editors and for managing models with transformation, validation, merging and weaving. There is limited support, however, for model migration—a development activity in which instance models areupdatedinresponsetometamodelevolution.Inthispaper, we propose conservative copy—a style of model transforma-tion that we believe is well-suited to model migration—and Epsilon Flock—a compact model-to-model transformation language tailored for model migration. The proposed struc-tures are evaluated by comparing the conciseness of model migration strategies written in different styles of transfor-mation language, using several examples of evolution taken from UML and the graphical modelling framework.",
        "keywords": [
            "Model migration",
            "Metamodel evolution",
            "Model transformation"
        ],
        "authors": [
            "Louis M. Rose",
            "Dimitrios S. Kolovos",
            "Richard F. Paige",
            "Fiona A. C. Polack",
            "Simon Poulding"
        ],
        "file_path": "data/sosym-all/s10270-012-0296-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon Flock"
        }
    },
    {
        "title": "A descriptive study of assumptions in STRIDE security threat modeling",
        "submission-date": "2020/10",
        "publication-date": "2021/11",
        "abstract": "Security threat modeling involves the systematic elicitation of plausible threat scenarios, and leads to the identiﬁcation and articulation of the security requirements in the early stages of software development. Although they are an important source of architectural knowledge, assumptions made in this context are in practice left implicit or at best, documented informally in an unstructured textual format. As guidelines and best practices are lacking, the nature, purpose and impact of assumptions made in this context is generally not well understood. We present a descriptive study of in total 640 textual assumptions made in 96 STRIDE threat models of the same system. The study mainly focuses on the diversity in how assumptions are used in practice, in terms of (i) the role or function of these assumptions in the threat modeling process, (ii) the degree of coupling between the assumptions and the system under analysis, and (iii) the extent to which these assumptions are exclusively speciﬁc to security. We observe large differences on all three investigated aspects: practitioners use the mechanism of assumption-making for diverse purposes, but predominantly to exclude certain threats from further analysis, i.e. to scope the analysis effort by steering it away from threat scenarios that are considered less relevant up front. Based on our ﬁndings, we argue against the exclusive use of Data Flow Diagrams as the main basis for threat analysis, and in favor of integrating more expressive attacker and trust models which can co-evolve with the threat model and the system.",
        "keywords": [
            "Threat modeling",
            "Security architecture",
            "Secure development life-cycle (SDLC)",
            "STRIDE",
            "Security assumptions",
            "Architecture knowledge management"
        ],
        "authors": [
            "Dimitri Van Landuyt",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-021-00941-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Jidoka: automation with a human touch",
        "submission-date": "2024/01",
        "publication-date": "2024/12",
        "abstract": "Organisations often ﬁnd themselves trapped in a costly cycle of replacing legacy systems. Jidoka is a software development methodology that aims to break this cycle and for organisations to mature into a mode of continuous modernisation. Jidoka, which loosely translates to automation with a human touch, offers several strategies for developers to modernise legacy systems onto their preferred technology stack, ensuring that quality, security, and other compliance is met. The paper presents state-of-practice information on how the process emerged through the use of Model-Driven Engineering (MDE), DevOps, and lessons learned using other software development methodologies across 100+ projects.",
        "keywords": [
            "Software development methodologies",
            "Model-driven engineering",
            "DevOps",
            "Software evolution",
            "Legacy system modernisation",
            "Empirical"
        ],
        "authors": [
            "Eban Escott"
        ],
        "file_path": "data/sosym-all/s10270-024-01256-z.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Verifying consistency of software product line architectures with product architectures",
        "submission-date": "2022/05",
        "publication-date": "2023/07",
        "abstract": "There has been increasing interest in modeling software product lines (SPLs) using architecture description languages (ADLs). However, sometimes it is required to reverse engineer an SPL architecture from a set of product architectures. This procedure needs to be performed manually as currently does not exist tool support to automate this task. In this case, verifying consistency between the product architectures and the reverse engineered SPL architecture is still a challenge; particularly, verifying component interconnection aspects of product architectures with respect to the commonality and variability of an SPL architecture represented in an ADL. Current approaches are unable to detect whether the component interconnections in a product architecture have inconsistencies with the component interconnections deﬁned by the SPL architecture. To tackle these shortcomings, we developed the Ontology-based Product Architecture Veriﬁcation (OntoPAV) framework. OntoPAV relies on the ontology formalism to capture the commonality and variability of SPLs architectures. Reasoning engines are employed to automatically identify component interconnection inconsistencies among SPL and product architectures. Our evaluation results show that our veriﬁer has a high accuracy for detecting consistency errors and that it scales linearly for architectures from 1000 to 5000 architecture elements.",
        "keywords": [
            "Software product lines",
            "Software architecture",
            "SPL Veriﬁcation",
            "Architecture veriﬁcation",
            "Ontologies",
            "Model-driven engineering"
        ],
        "authors": [
            "Hector A. Duran-Limon",
            "Perla Velasco-Elizondo",
            "Manuel Mora",
            "Maria E. Meda-Campana",
            "Karina Aguilar",
            "Martha Hernandez-Ochoa",
            "Leonardo Soto Sumuano"
        ],
        "file_path": "data/sosym-all/s10270-023-01114-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic generation of basic behavior schemas from UML class diagrams",
        "submission-date": "2008/03",
        "publication-date": "2008/12",
        "abstract": "The speciﬁcation of a software system must include all relevant static and dynamic aspects of the domain. Dynamic aspects are usually speciﬁed by means of a behavioral schema consisting of a set of system operations that the user may execute to update the system state. To be useful, such a set must be complete (i.e. through these operations, users should be able to modify the population of all elements in the class diagram) and executable (i.e. for each opera-tion, there must exist a system state over which the oper-ation can be successfully applied). A manual speciﬁcation of these operations is an error-prone and time-consuming activity. Therefore, the aim of this paper is to present a strat-egy for the automatic generation of a basic behavior schema. Operations in the schema are drawn from the static aspects of the domain as deﬁned in the UML class diagram and take into account possible dependencies among them to ensure the completeness and executability of the operations. We believe our approach is especially useful in a Model-Driven Devel-opment setting, where the full implementation of the system is derived from its speciﬁcation. In this context, our approach facilitates the definition of the behavioral speciﬁcation and ensures its quality obtaining, as a result, an improved code generation phase.",
        "keywords": [
            "Behavior schema",
            "Operation",
            "Structural event",
            "Class diagram",
            "UML",
            "OCL"
        ],
        "authors": [
            "Manoli Albert",
            "Jordi Cabot",
            "Cristina Gómez",
            "Vicente Pelechano"
        ],
        "file_path": "data/sosym-all/s10270-008-0108-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The design space of multi-language development environments",
        "submission-date": "2012/11",
        "publication-date": "2013/09",
        "abstract": "Non-trivial software systems integrate many artifacts expressed in multiple modeling and programming languages. However, even though these artifacts heavily depend on each other, existing development environments do not sufficiently support handling relations between artifacts in different languages. By means of a literature survey, tool prototyping, and experiments, we study the design space of multi-language development environments (MLDEs)—tools that consider cross-language relations as first artifacts. We ask: What is the state of the art in the MLDE space? What are the design choices and challenges faced by tool builders? To what extent are MLDEs desired by users, and what aspects of MLDEs are particularly helpful? Our main conclusions are that (a) cross-language relations are ubiquitous and troublesome in multi-language systems, (b) users highly appreciate cross-language support mechanisms of MLDEs, and (c) generic MLDEs clearly advance the state of the art in tooling for language integration. The technical artifacts resulting from this study include a feature model of the MLDE design space, a data set of harvested cross-language relations in a case study system (JTrac) and two MLDE prototypes, TexMo and Coral, that implement two radically different choices in the design space.",
        "keywords": [
            "Multi-language development environment",
            "Multi-modeling",
            "Cross-language relations"
        ],
        "authors": [
            "Rolf-Helge Pfeiffer",
            "Andrzej Wa˛sowski"
        ],
        "file_path": "data/sosym-all/s10270-013-0376-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling learning technology systems as business systems",
        "submission-date": "2002/09",
        "publication-date": "2003/06",
        "abstract": "The design of Learning Technology Systems, and the Software Systems that support them, is largely conducted on an intuitive, ad hoc basis, thus resulting in inefficient systems that defectively support the learning process. There is now justifiable, increasing effort in formalizing the engineering of Learning Technology Systems in order to achieve better learning effectiveness as well as development efficiency. This paper presents such an approach for designing Learning Technology Systems and their most popular specialization, the Web-based Learning Systems, by modeling them as business systems, using business-modeling methods. The aim is to provide an in-depth analysis and comprehension of the Learning Technology Systems and Web-based Learning Systems’ domain, that can be used for improving the systems themselves, as well as for building the supporting software systems. Our work is based upon the Learning Technology Systems Architecture standard of IEEE LTSC, on the empirical results of designing Web-based Learning Systems for university courses and on the practices of the Rational Unified Process and the Unified Modeling Language.",
        "keywords": [
            "Business model",
            "Learning technology system",
            "Unified modeling language",
            "Rational unified process",
            "Web-based learning systems",
            "Open and distance learning",
            "e-learning",
            "Learning technology systems architecture"
        ],
        "authors": [
            "Paris Avgeriou",
            "Symeon Retalis",
            "Nikolaos Papaspyrou"
        ],
        "file_path": "data/sosym-all/s10270-003-0022-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Context-driven process discovery: enhancing process flow interpretability with contextualized activity hierarchies",
        "submission-date": "2024/11",
        "publication-date": "Not found",
        "abstract": "Analyzing business processes is important for organizations aiming to optimize operations and identify inefficiencies. Traditional discovered process models often lack sufficient contextual depth, limiting the interpretability and actionability of the revealed activity process flows. This paper addresses the challenge of balancing interpretability with complexity in discovered process models by introducing a new context-driven method, namely Contextualized Activity hierarchies for Process discovery (CARPI). CARPI consists of a detailed five-step process to identify, extract, and integrate meaningful contextual variables into core activities flows in the process to enhance model clarity and decision-making support. We implement and validate this method using a real-world case study in manufacturing and the BPI Challenge 2017 dataset, demonstrating how the integration of relevant contextual variables refines process models to make activity flows more interpretable and actionable. This contribution advances the field of process mining by offering a clear and structured method to enrich process models with important context variables, laying the foundation for more insightful and effective business process management and improvement.",
        "keywords": [
            "Process mining",
            "Contextualization",
            "Activity specialization",
            "Context aggregation",
            "Sensor data",
            "Activity labeling",
            "Industrial case study"
        ],
        "authors": [
            "Zahra Ahmadi",
            "Jochen De Weerdt",
            "Estefanía Serral Asensio"
        ],
        "file_path": "data/sosym-all/s10270-025-01313-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Softw Syst Model (2003) 2: 3–4",
        "submission-date": "2003/02",
        "publication-date": "2003/02",
        "abstract": "In nearly every ﬁeld of computer science, models and modeling play an important role. Many researchers concentrate their research on modeling issues in their respective ﬁelds, for example in requirements, databases, workﬂow management or Petri Nets. However, these researchers are typically rooted in the research communities of their speciﬁc sub-domains. Modeling, being a cross-cutting issue, has no research community of its own. Consequently, researchers are frequently not aware of modeling research and results outside their own sub-domain. In 1997, a couple of German-speaking modeling researchers became aware of this problem and developed the idea of bringing together researchers from various ﬁelds of computer science who share a common interest in modeling problems.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-003-0016-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ontological Evaluation of the UML Using the Bunge–Wand–Weber Model",
        "submission-date": "2002/03",
        "publication-date": "2002/09",
        "abstract": "An ontological model of information systems, the Bunge–Wand–Weber (BWW) model, is used to analyse and evaluate the Uniﬁed Modeling Language (UML) as a language for representing concrete problem domains. As a result, each relevant and major UML construct becomes more precisely deﬁned in terms of the phenomena in and aspects of the problem domain it represents. The analysis and evaluation shows that many of the central UML constructs are well matched with the BWW-model, but also suggests several concrete improvements to the UML-metamodel.Newmetaclassesareproposedtodistin-guish between (physically) impossible and (humanly) disallowed events, based on UML-exceptions. New abstract metaclasses are proposed for static and behavioural constraints, behaviours and static behaviours, as well as bindingrelationshipsandcoupledevents.Newmeta-subclasses ofUML-objects,-classes,-typesand-relationshipsarepro-posed to make the UML more orthogonal,and a new deﬁ-nition is proposed for UML-responsibilities. The analysis also shows that the constructs in the UML must play sev-eral roles simultaneously, supporting representation both of the problem domain, of the development artifacts and of the proposed software or information system, while ﬁtting together as a tightly integrated, well-deﬁned language.",
        "keywords": [
            "Object-oriented analysis",
            "Problem domain representation",
            "Ontological analysis and evaluation",
            "Uniﬁed Modeling Language (UML)",
            "The Bunge",
            "Wand",
            "Weber model (BWW)"
        ],
        "authors": [
            "Andreas L. Opdahl",
            "Brian Henderson-Sellers"
        ],
        "file_path": "data/sosym-all/s10270-002-0003-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Implementing associations: UML 2.0 to Java 5",
        "submission-date": "2005/05",
        "publication-date": "2006/07",
        "abstract": "A signiﬁcant current software engineering problem is the conceptual mismatch between the abstract concept of an association as found in modelling languages such as UML and the lower level expressive facilities available in object-oriented languages such as Java. This paper introduces some code generation patterns that aid the production of Java based implementations from UML models. The work is motivated by a project to construct model driven development tools in support of the construction of embedded systems. This involves the speciﬁcation and implementation of a number of meta-models (or models of languages). Many current UML oriented tools provide code generation facilities, in particular the generation of object-oriented code from class diagrams. However, many of the more complex aspects of class diagrams, such as qualiﬁed associations are not supported. In addition, several concepts introduced in UML version 2.0 are also not supported.The aim of the work presented in this paper is to develop a number of code generation patterns that allow us to support the automatic generation of Java code from UML class diagrams that support these new and complex association concepts. These patterns signiﬁcantly improve the code generation abilities of UML tools, providing a useful automation facility that bridges the gap between the concept of an association and lower level object-oriented programming languages.",
        "keywords": [
            "UML",
            "Java",
            "Association",
            "Property",
            "Code Generation"
        ],
        "authors": [
            "D. Akehurst",
            "G. Howells",
            "K. McDonald-Maier"
        ],
        "file_path": "data/sosym-all/s10270-006-0020-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Visual notations in container orchestrations: an empirical study with Docker Compose",
        "submission-date": "2021/07",
        "publication-date": "2022/09",
        "abstract": "Container orchestration tools supporting infrastructure-as-code allow new forms of collaboration between developers and operatives. Still, their text-based nature permits naive mistakes and is more difﬁcult to read as complexity increases. We can ﬁnd few examples of low-code approaches for deﬁning the orchestration of containers, and there seems to be a lack of empirical studies showing the beneﬁts and limitations of such approaches. We hypothesize that a complete visual notation for Docker-based orchestrations could reduce the effort, the error rate, and the development time. Therefore, we developed a tool featuring such a visual notation for Docker Compose conﬁgurations, and we empirically evaluated it in a controlled experiment with novice developers. The results show a signiﬁcant reduction in development time and error-proneness when deﬁning Docker Compose ﬁles, supporting our hypothesis. The participants also thought the prototype easier to use and useful, and wanted to use it in the future.",
        "keywords": [
            "Container orchestrations",
            "Infrastructure as code",
            "Empirical study",
            "Visual programming",
            "Docker",
            "Docker Compose"
        ],
        "authors": [
            "Bruno Piedade",
            "João Pedro Dias",
            "Filipe F. Correia"
        ],
        "file_path": "data/sosym-all/s10270-022-01027-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the reuse and recommendation of model refactoring specifications",
        "submission-date": "2011/03",
        "publication-date": "2012/04",
        "abstract": "Refactorings can be used to improve the structure of software artefacts while preserving the semantics of the encapsulated information. Various types of refactorings have been proposed and implemented for programming languages (e.g., Java or C#). With the advent of Model-Driven Software Development (MDSD), a wealth of modelling languages rises and the need for restructuring models similar to programs has emerged. Since parts of these modelling languages are often very similar, we consider it beneficial to reuse the core transformation steps of refactorings across languages. In this sense, reusing the abstract transformation steps and the abstract participating elements suggests itself. Previous work in this field indicates that refactorings can be specified generically to foster their reuse. However, existing approaches can handle certain types of modelling languages only and solely reuse refactorings once per language. In this paper, a novel approach based on role models to specify generic refactorings is presented. Role models are suitable for this problem since they support declaration of roles which have to be played in a certain context. Assigned to generic refactoring, contexts are different refactorings and roles are the participating elements. We discuss how this resolves the limitations of previous works, as well as how specific refactorings can be defined as extensions to generic ones. The approach was implemented in our tool Refactory based on the Eclipse Modeling Framework (EMF) and evaluated using multiple modelling languages and refactorings. In addition, this paper investigates on the recommendation of refactoring specifications. This is motivated by the fact that language designers have many possibilities to enable refactorings in their modelling languages with regard to the language structures. To overcome this problem and to support language designers in deciding which refactorings to enable, we propose a solution and a prototypical implementation.",
        "keywords": [
            "Generic model refactoring",
            "Role-based refactoring",
            "Refactoring reuse",
            "Refactoring recommendation",
            "Role modelling"
        ],
        "authors": [
            "Jan Reimann",
            "Mirko Seifert",
            "Uwe Aßmann"
        ],
        "file_path": "data/sosym-all/s10270-012-0243-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dynamic Meta Modeling with time: Specifying the semantics of multimedia sequence diagrams",
        "submission-date": "2003/02",
        "publication-date": "2004/04",
        "abstract": "The Uniﬁed Modeling Langugage (UML) of-fers diﬀerent diagram types to model the behavior of software systems. In some domains like embedded real-time systems or multimedia systems, it is necessary to in-clude speciﬁcations of time in behavioral models since the correctness of these applications depends on the fulﬁll-ment of temporal requirements in addition to functional requirements. UML thus already incorporates language features to model time and temporal constraints. Such model elements must have an equivalent in the semantic domain.\nWe have proposed Dynamic Meta Modeling (DMM), an approach based on graph transformation, as a means for specifying operational semantics of dynamic UML di-agrams. In this article, we extend this approach to also account for time by extending the semantic domain to timed graph transformation. This enables us to deﬁne the operational semantics of UML diagrams with time speci-ﬁcations. As an example, we provide semantics for special sequence diagrams from the domain of multimedia application modeling.",
        "keywords": [
            "Formal semantics",
            "Meta modeling",
            "UML extensions",
            "Graph transformation",
            "Time",
            "Multimedia",
            "Sequence diagram"
        ],
        "authors": [
            "Jan Hendrik Hausmann",
            "Reiko Heckel",
            "Stefan Sauer"
        ],
        "file_path": "data/sosym-all/s10270-003-0045-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Meet OCL♯, a relational object constraint language",
        "submission-date": "2024/04",
        "publication-date": "2025/03",
        "abstract": "At its core, OCL as currently deﬁned is a ﬁrst-order functional language: its expressions evaluate to single values, with collections accounting for multitudes of values, and special values null and invalid for partiality. By contrast, the data model providing the context of OCL expressions is inherently relational: the associations of UML class diagrams are essentially relations, with uniqueness and order designators extending expressiveness to ordered multirelations. As a result, OCL suffers from a functional/relational impedance mismatch, which is only superﬁcially addressed by its navigation shorthands. At the same time, OCL is inherently unsafe: expressions containing subexpressions evaluating to null may be invalid, translating to a runtime error in programming languages. We address this situation by turning OCL into a relational language that retains most of OCL’s original syntax and semantics, yet revises its fundamental design decisions that lead up to the noted problems. In particular, our version of OCL, which we call OCL♯, is type-safe.",
        "keywords": [
            "Speciﬁcation language",
            "Relational language",
            "Collections",
            "Type safety"
        ],
        "authors": [
            "Friedrich Steimann",
            "Robert Clarisó",
            "Martin Gogolla"
        ],
        "file_path": "data/sosym-all/s10270-025-01286-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSyM reﬂections: the 2018 “State of the Journal” report",
        "submission-date": "2019/01",
        "publication-date": "2019/01",
        "abstract": "We are delighted to kick off another volume of SoSyM, with this first issue of 2019! The “health” of SoSyM continues to be in good shape, with evidence of its growing popularity and impact. The statistics reported in the next section indicate that we have a steady rise in the number of incoming and also accepted papers. Therefore, we are proud to announce that beginning with this issue, we will have an increase from four to six issues per year. This will allow us to grow the overall number of pages published each year.",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-019-00715-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Synchrony and asynchrony in conformance testing",
        "submission-date": "2012/03",
        "publication-date": "2013/01",
        "abstract": "We present and compare different notions of\nconformance testing based on labeled transition systems.\nWe formulate and prove several theorems which enable\nusing synchronous conformance testing techniques such as\ninput–output conformance testing (ioco) in order to test\nimplementationsonlyaccessiblethroughasynchronouscom-\nmunication channels. These theorems deﬁne when the syn-\nchronous test cases are sufﬁcient for checking all aspects of\nconformance that are observable by asynchronous interac-\ntion with the implementation under test.",
        "keywords": [
            "Conformance testing",
            "ioco",
            "Asynchronous conformance testing",
            "Queue context",
            "Internal choice implementation"
        ],
        "authors": [
            "Neda Noroozi",
            "Ramtin Khosravi",
            "Mohammad Reza Mousavi",
            "Tim A. C. Willemse"
        ],
        "file_path": "data/sosym-all/s10270-012-0302-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assessing the testing skills transfer of model-based testing on testing skill acquisition",
        "submission-date": "2023/04",
        "publication-date": "2024/01",
        "abstract": "When creating a software model, it is necessary that it accurately captures the desired behaviour, while at the same time ensuring that any undesired behaviour is excluded. On the one hand, formal veriﬁcation tools can be used to check the internal consistency of a software system, ensuring that the behaviour of one software component does not contradict another. On the other hand, software testing is essential to check the external validity of the model more comprehensively. Unfortunately, software testing is often overlooked in curricula, resulting in graduates with inadequate software testing skills for industry. Software testing tools such as TesCaV can be used to help teachers teach software testing topics in a non-intrusive and less time-consuming way. Previous research has shown that TesCaV is easy to use and that novice users produce better quality software tests when using TesCaV. However, it has remained unclear whether learners retain the skills they gain from using TesCaV even when the tool is not offered for help. In order to understand the positive effect of TesCaV on learners’ software testing skills, this study conducted an experiment with 45 participants. The experiment used a pretest-treatment-posttest design. The results show that participants feel equally conﬁdent about the completeness of their test coverage, even though they identify more test cases. It is concluded that for course design, a capsule such as TesCaV can help students to understand the full complexity of software testing and help them to be more systematic in their approach.",
        "keywords": [
            "Model-based testing",
            "Model-driven engineering",
            "TesCaV",
            "MERODE"
        ],
        "authors": [
            "Felix Cammaerts",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-023-01141-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correct-by-construction requirement decomposition",
        "submission-date": "2024/05",
        "publication-date": "2025/03",
        "abstract": "In systems engineering, accurately decomposing requirements is crucial for creating well-deﬁned and manageable system\ncomponents, particularly in safety-critical domains. Despite the critical need, rigorous, top-down methodologies for effectively\nbreaking down complex requirements into precise, actionable sub-requirements are scarce, especially compared to the wealth\nof bottom-up veriﬁcation techniques. Addressing this gap, we introduce a formal decomposition for contract-based design that\nguarantees the correctness of decomposed requirements if speciﬁc conditions are met. Our (semi-)automated methodology\naugments contract-based design with reachability analysis and constraint programming to systematically identify, verify, and\nvalidate sub-requirements representable by continuous bounded sets—continuous relations between real-valued inputs and\noutputs. We demonstrate the efﬁcacy and practicality of a correct-by-construction approach through a comprehensive case\nstudy on a cruise control system, highlighting how our methodology improves the interpretability, tractability, and veriﬁability\nof system requirements.",
        "keywords": [
            "Contract-based design",
            "Requirement decomposition",
            "Constraint programming"
        ],
        "authors": [
            "Minghui Sun",
            "Georgios Bakirtzis",
            "Hassan Jafarzadeh",
            "Cody Fleming"
        ],
        "file_path": "data/sosym-all/s10270-025-01291-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Code generation by model transformation: a case study in transformation modularity",
        "submission-date": "2008/11",
        "publication-date": "2009/11",
        "abstract": "The realization of model-driven software development requires effective techniques for implementing code generators for domain-speciﬁc languages. This paper identifies techniques for improving separation of concerns in the implementation of generators. The core technique is code generation by model transformation, that is, the generation of a structured representation (model) of the target program instead of plain text. This approach enables the transformation of code after generation, which in turn enables the extension of the target language with features that allow better modularity in code generation rules. The technique can also be applied to ‘internal code generation’ for the translation of high-level extensions of a DSL to lower-level constructs within the same DSL using model-to-model transformations. This paper refines our earlier description of code generation by model transformation with an improved architecture for the composition of model-to-model normalization rules, solving the problem of combining type analysis and transformation. Instead of coarse-grained stages that alternate between normalization and type analysis, we have developed a new style of type analysis that can be integrated with normalizing transformations in a fine-grained manner. The normalization strategy has a simple extension interface and integrates non-local, context-sensitive transformation rules. We have applied the techniques in a realistic case study of domain-speciﬁc language engineering, i.e. the code generator for WebDSL, using Stratego, a high-level transformation language that integrates model-to-model, model-to-code, and code-to-code transformations.",
        "keywords": [
            "Transformation",
            "Transformation engineering",
            "Term rewriting",
            "Webapplication DSL",
            "Combination of analysis and transformation"
        ],
        "authors": [
            "Zef Hemel",
            "Lennart C. L. Kats",
            "Danny M. Groenewegen",
            "Eelco Visser"
        ],
        "file_path": "data/sosym-all/s10270-009-0136-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Stratego"
        }
    },
    {
        "title": "Using DevOps toolchains in Agile model-driven engineering",
        "submission-date": "2021/11",
        "publication-date": "2022/05",
        "abstract": "For model-driven engineering (MDE) to become more Agile, the community needs to embrace development and operations (DevOps) practices. One of the core practices of DevOps is the use of pipelines to enable CI/CD to make teams more Agile and break down the barriers between development and operations with faster deployments. Current MDE tooling is not designed at its core to participate in DevOps pipelines. Consequently this makes the adoption of MDE in industry more difﬁcult. In this article, we cover an industrial experience report describing how we enabled our pipelines using DevOps and MDE.",
        "keywords": [
            "DevOps",
            "CI/CD",
            "Ant",
            "EMF",
            "Eclipse",
            "Agile",
            "Model-driven engineering",
            "MDE"
        ],
        "authors": [
            "Jörn Guy Süß",
            "Samantha Swift",
            "Eban Escott"
        ],
        "file_path": "data/sosym-all/s10270-022-01003-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the beneﬁts of ﬁle-level modularity for EMF models",
        "submission-date": "2018/12",
        "publication-date": "2020/06",
        "abstract": "Model-driven development (MDD) tools based on the Eclipse Modeling Framework (EMF) typically store all elements\nin a model in a single ﬁle which arguably is one of the main reasons why these tools do not scale well and cannot take\nadvantage of existing code versioning systems and other related facilities such as Git and Make. In this work, we describe an\napproach for storing models in multiple ﬁles. We argue that EMF-based MDD tools can beneﬁt signiﬁcantly from this ﬁle-level\nmodularity not only by improving the performance and scalability of basic model operations, but also by simplifying many\nmodel management activities through the use of existing code versioning systems and build automation tools. We introduce\na domain-speciﬁc language that allows deﬁning, at the metamodel level: (1) the mapping between models’ elements and\nthe ﬁle structure for model storage and (2) the dependencies between model elements that affect the code generation and\ncompilation (if the integration with code-based tools is required). Our suite then generates an API and scripts to provide\nsupport for ﬁle-level modularity and facilitate using code-based versioning and build tools. We have used our DSL in the\ncontext of Papyrus-RT, an MDD tool for real-time and embedded software, and show how ﬁle-level modularity can (1)\nsubstantially improve performance and scalability of load and save operations, (2) enable collaborative model development,\nand (3) facilitate MDD-speciﬁc activities such as model comparison and incremental code generation. Our implementation\nand the models used for evaluation are publicly available.",
        "keywords": [
            "Model management",
            "Model versioning",
            "Model comparison",
            "Incremental code generation",
            "Build automation",
            "UML-RT"
        ],
        "authors": [
            "Karim Jahed\nMojtaba Bagherzadeh\nJuergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-020-00804-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ChronoSphere: a graph-based EMF model repository for IT landscape models",
        "submission-date": "2018/04",
        "publication-date": "2019/03",
        "abstract": "IT Landscape models are representing the real-world IT infrastructure of a company. They include hardware assets such as physical servers and storage media, as well as virtual components like clusters, virtual machines and applications. These models are a critical source of information in numerous tasks, including planning, error detection and impact analysis. The responsible stakeholders often struggle to keep such a large and densely connected model up-to-date due to its inherent size and complexity, as well as due to the lack of proper tool support. Even though modeling techniques are very suitable for this domain, existing tools do not offer the required features, scalability or ﬂexibility. In order to solve these challenges and meet the requirements that arise from this application domain, we combine domain-driven modeling concepts with scalable graph-based repository technology and a custom language for model-level queries. We analyze in detail how we synthesized these requirements from the application domain and how they relate to the features of our repository. We discuss the architecture of our solution which comprises the entire data management stack, including transactions, queries, versioned persistence and metamodel evolution. Finally, we evaluate our approach in a case study where our open-source repository implementation is employed in a production environment in an industrial context, as well as in a comparative benchmark with an existing state-of-the-art solution.",
        "keywords": [
            "Model-driven engineering",
            "Model repositories",
            "Versioning",
            "Graph database",
            "IT landscape"
        ],
        "authors": [
            "Martin Haeusler",
            "Thomas Trojer",
            "Johannes Kessler",
            "Matthias Farwick",
            "Emmanuel Nowakowski",
            "Ruth Breu"
        ],
        "file_path": "data/sosym-all/s10270-019-00725-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Speciﬁcation and analysis of legal contracts with Symboleo",
        "submission-date": "2020/11",
        "publication-date": "2022/10",
        "abstract": "Legal contracts specify the terms and conditions—in essence, requirements—that apply to business transactions. This paper proposes a formal speciﬁcation language for legal contracts, called Symboleo, where contracts consist of collections of obligations and powers that deﬁne a legal contract’s compliant executions. Symboleo offers execution time operations such as subcontracting, assignment, and substitution. Its formal semantics is deﬁned in terms of logical axioms on statecharts that describe the lifetimes of contracts, obligations, and powers. We have implemented two tools to support the analysis of contract speciﬁcations. One is a conformance validation tool that enables checking that a speciﬁcation is consistent with the expectations of contracting parties. The other tool enables model-checking of desired contract properties, expressed in temporal logic. We envision Symboleo with its associated tools as enablers for the formal veriﬁcation of contracts to detect requirements-level issues. Our proposal includes an evaluation through the speciﬁcation of two real life-inspired contracts.",
        "keywords": [
            "Legal contracts",
            "Software requirements speciﬁcations",
            "Formal speciﬁcation languages",
            "Model checking",
            "nuXmv",
            "Smart contracts"
        ],
        "authors": [
            "Alireza Parvizimosaed",
            "Sepehr Shariﬁ",
            "Daniel Amyot",
            "Luigi Logrippo",
            "Marco Roveri",
            "Aidin Rasti",
            "Ali Roudak",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-022-01053-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the use of large language models in model-driven engineering",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Model-driven engineering (MDE) has seen signiﬁcant advancements with the integration of machine learning (ML) and deep learning techniques. Building upon the groundwork of previous investigations, our study provides a concise overview of current large language models (LLMs) applications in MDE, emphasizing their role in automating tasks like model repository classiﬁcation and developing advanced recommender systems. The paper also outlines the technical considerations for seamlessly integrating LLMs in MDE, offering a practical guide for researchers and practitioners. Looking forward, the paper proposes a focused research agenda for the future interplay of LLMs and MDE, identifying key challenges and opportunities. This concise roadmap envisions the deployment of LLM techniques to enhance the management, exploration, and evolution of modeling ecosystems. Moreover, we also discuss the adoption of LLMs in various domains by means of model-driven techniques and tools, i.e., MDE for supporting LLMs. By offering a compact exploration of LLMs in MDE, this paper contributes to the ongoing evolution of MDE practices, providing a forward-looking perspective on the transformative role of large language models in software engineering and model-driven practices.",
        "keywords": [
            "LLMs",
            "Generative AI",
            "Model-Driven Engineering"
        ],
        "authors": [
            "Juri Di Rocco\nDavide Di Ruscio\nClaudio Di Sipio\nPhuong T. Nguyen\nRiccardo Rubei"
        ],
        "file_path": "data/sosym-all/s10270-025-01263-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Detection and resolution of conﬂicting change operations in version management of process models",
        "submission-date": "2011/03",
        "publication-date": "2011/12",
        "abstract": "Version management of process models requires\nthat different versions of process models are integrated by\napplying change operations. Conﬂict detection between indi-\nvidually applied change operations and conﬂict resolution\nsupport are integral parts of version management. For con-\nﬂict detection it is utterly important to compute a precise set\nof conﬂicts, since the minimization of the number of detected\nconﬂicts also reduces the overhead for merging different pro-\ncess model versions. As not every syntactic conﬂict leads to\na conﬂict when taking into account model semantics, a com-\nputation of conﬂicts solely on the syntax leads to an unnec-\nessary high number of conﬂicts. Moreover, even the set of\nprecisely computed conﬂicts can be extensive and their res-\nolution means a significant workload for a user. As a con-\nsequence, adequate support is required that guides a user\nthrough the resolution process and suggests possible reso-\nlution strategies for individual conﬂicts. In this paper, we\nintroduce the notion of syntactic and semantic conﬂicts for\nchange operations of process models. We provide a method\nhow to efﬁciently compute conﬂicts precisely, using a term\nformalization of process models and consider the subsequent\nresolution of the detected conﬂicts based on different strat-",
        "keywords": [
            "Business process model",
            "Versionmanagement",
            "Conﬂict detection",
            "Conﬂict resolution"
        ],
        "authors": [
            "Christian Gerth",
            "Jochen M. Küster",
            "Markus Luckey",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-011-0226-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automating the development of API-based generators using code idioms mining",
        "submission-date": "2024/03",
        "publication-date": "2025/04",
        "abstract": "API-Based Generators (ABGs) allow practitioners to achieve the advantages of Model-Driven Engineering (MDE) without\nmaking signiﬁcant changes to their current workﬂow or the architecture of their solution. However, using ABGs that are deﬁned\nin terms of the syntax rules of a target language can be cumbersome and impractical. Additionally, manually developing an\nABG based on higher-level concepts already used in an existing solution is labor-intensive and time-consuming. This paper\nintroduces a new approach called Semi-automatic API-based Generators Development (SAGED) to expedite the creation\nof ABGs customized for MDE development of existing solutions. SAGED accomplishes this by (i) providing insight into\ncode idioms that could be considered good candidates for code generation, as they frequently appear in the codebase, and\n(ii) automating the creation of a code generation API for an ABG, deﬁned in terms of the identiﬁed code idioms. The\nSAGED approach relies on mining code idioms from existing, unlabeled source code based on a machine learning technique\ncalled the nonparametric Bayesian Probabilistic Tree Substitution Grammar (PTSG). The main contributions of this paper\ninclude the introduction of the SAGED approach, an explanation and optimization of the Type-Based MCMC as a method for\napproximating the nonparametric Bayesian PTSG, and the development of an open-source inference core for implementing the\ninference method in different programming languages. Furthermore, the paper presents a solution for implementing SAGED\nin the C# programming language, along with case studies that demonstrate its effectiveness.",
        "keywords": [
            "API-Based Generators",
            "Machine Learning",
            "Code Idioms Mining"
        ],
        "authors": [
            "Nenad Todorovi´c",
            "Aleksandar Luki´c",
            "Nikola Todorovi´c",
            "Bojana Dragaš",
            "Gordana Milosavljevi´c"
        ],
        "file_path": "data/sosym-all/s10270-025-01296-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design for service compatibility\nBehavioural compatibility checking and diagnosis",
        "submission-date": "2011/03",
        "publication-date": "2012/01",
        "abstract": "Service composition is a recent ﬁeld that has seen\na ﬂurry of different approaches proposed towards the goal\nof ﬂexible distributed heterogeneous interoperation of soft-\nware systems, usually based on the expectation that such\nsystems must be derived from higher-level models rather\nthan be coded at low level. In practice, achieving service\ninteroperability nonetheless continues to require significant\nmodelling approach at multiple abstraction levels, and exist-\ning formal approaches typically require the analysis of the\nglobal space of joint executions of interacting services. Based\non our earlier work on providing locally checkable consis-\ntency rules for guaranteeing the behavioural consistency of\ninheritance hierarchies, a model-driven approach for creating\nconsistent service orchestrations is proposed. Service execu-\ntion and interaction is represented with a high-level model in\nterms of extended Petri net notation; formal criteria are pro-\nvided for service consistency that can be checked in terms of\nlocal model properties, and give a multi-step design approach\nfor developing services that are guaranteed to be interoper-\nable. Finally, it is outlined how the presented results can be\ncarried over and applied to modelling processes using the\nBusiness Process Modelling Notation (BPMN).",
        "keywords": [
            "Service compatibility",
            "Petri net",
            "BPMN",
            "Business process modelling",
            "Behavior diagrams",
            "Consistency rules"
        ],
        "authors": [
            "Georg Grossmann",
            "Michael Schreﬂ",
            "Markus Stumptner"
        ],
        "file_path": "data/sosym-all/s10270-012-0229-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An empirical approach toward the resolution of conﬂicts in goal-oriented models",
        "submission-date": "2014/08",
        "publication-date": "2015/04",
        "abstract": "One signiﬁcant problem requirements engineers have to cope with is the management of unclear requirements, ambiguities, and conﬂicts that may arise between stakeholders. Such issues may be desirable since they may allow for further elicitation of requirements that would have been missed otherwise. Goal models capture the objectives and other intentions of different stakeholders, together with theirrelationships.Theycanbeusedtoreﬁneunclearrequire-ments and to detect conﬂicts and ambiguities early during model validation. However, resolving such ambiguities and conﬂicts is key for the successful implementation of the goal models. In this paper, we propose a novel approach to validate models in the Goal-oriented Requirement Language and resolve conﬂicts between the perspectives of intervening stakeholders (and especially between stakeholders of a given group). Our approach is based on a statistical analy-sis of empirical data that we collect from surveys designed for each group of stakeholders. We apply concept analysis in order to ﬁx goal-model artifacts that are subject to con-ﬂict. We illustrate our approach using a case study of a goal model describing the involvement of undergraduate students in university research activities.",
        "keywords": [
            "Goal model",
            "Conﬂict resolution",
            "GRL",
            "Statistical analysis",
            "Empirical analysis",
            "Concept analysis"
        ],
        "authors": [
            "Jameleddine Hassine",
            "Daniel Amyot"
        ],
        "file_path": "data/sosym-all/s10270-015-0460-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A simple game-theoretic approach to checkonly QVT Relations",
        "submission-date": "2009/12",
        "publication-date": "2011/03",
        "abstract": "The QVT Relations (QVT-R) transformation language allows the definition of bidirectional model transformations, which are required in cases where two (or more) models must be kept consistent in the face of changes to either or both. A QVT-R transformation can be used either in check-only mode, to determine whether a target model is consistent with a given source model, or in enforce mode, to change the target model. A precise understanding of checkonly mode transformations is prerequisite to a precise understanding of enforce mode transformations, and this is the focus of this paper. In order to give semantics to checkonly QVT-R transformations, we need to consider the overall structure of the transformation as given by when and where clauses, and the role of trace classes. In the standard, the semantics of QVT-R are given both directly, and by means of a translation to QVT Core, a language which is intended to be simpler. In this paper, we argue that there are irreconcilable differences between the intended semantics of QVT-R and those of QVT Core, so that no translation from QVT-R to QVT Core can be semantics-preserving, and hence no such translation can be helpful in defining the semantics of QVT-R. Treating QVT-R directly, we propose a simple game-theoretic semantics. We demonstrate its behaviour on examples and show how it can be used to prove an example result comparing two QVT-R transformations. We demonstrate that consistent models may not possess a single trace model whose objects can be read as traceability links in either direction. We briefly discuss the effect of variations in the rules of the game, to elucidate some design choices available to the designers of the QVT-R language.",
        "keywords": [
            "Bidirectional model transformation",
            "QVT Relations",
            "QVT Core",
            "Games",
            "Semantics",
            "Consistency checking"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-011-0198-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT Relations, QVT Core"
        }
    },
    {
        "title": "Modeling safety and airworthiness (RTCA DO-178B) information: conceptual model and UML proﬁle",
        "submission-date": "2009/01",
        "publication-date": "2010/06",
        "abstract": "Several safety-related standards exist for developing and certifying safety-critical systems. System safety assessments are common practice and system certiﬁcation according to a standard requires submitting relevant system safety information to appropriate authorities. The RTCA DO-178B standard is a software quality assurance, safety-related standard for the development of software aspects of aerospace systems. This research introduces an approach to improve communication and collaboration among safety engineers, software engineers, and certiﬁcation authorities in the context of RTCA DO-178B. This is achieved by utilizing a Uniﬁed Modeling Language (UML) proﬁle that allows software engineers to model safety-related concepts and properties in UML, the de facto software modeling standard. A conceptual meta-model is deﬁned based on RTCA DO-178B, and then a corresponding UML proﬁle, which we call Safe-UML, is designed to enable its precise modeling. We show how SafeUML improves communication by, for example, allowing monitoring implementation of safety requirements during the development process, and supporting system certiﬁcation per RTCA DO-178B. This is enabled through automatic generation of safety and certiﬁcation-related information from UML models. We validate this approach through a case study on developing an aircraft’s navigation controller subsystem.",
        "keywords": [
            "UML",
            "UML proﬁle",
            "Conceptual model",
            "Meta-model",
            "Airworthiness",
            "RTCA DO-178B",
            "Safety",
            "Safety-critical",
            "Safety assessment",
            "Certiﬁcation"
        ],
        "authors": [
            "Gregory Zoughbi",
            "Lionel Briand",
            "Yvan Labiche"
        ],
        "file_path": "data/sosym-all/s10270-010-0164-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using structural decomposition and reﬁnements for deep modeling of software architectures",
        "submission-date": "2017/11",
        "publication-date": "2018/10",
        "abstract": "Traditional metamodeling in two levels gets to its limits when model elements of a domain should be described as instances of other model elements. For example in architecture description languages, components may be instances of their component types. Although workarounds to model such instance relations between model elements exist, they require many validation constraintsandimplyacumbersomeinterface.Toobtainmoreelegantmetamodels,deepmodelingseekswaystorepresentnon-transitive instantiation chains directly. However, existing concepts cannot be applied in some situations we refer as composite instantiation patterns. Further, these concepts make existing technologies for model transformation and analysis obsolete as these languages have to be adapted. In this paper, we present an approach to realize deep modeling through structural decomposition and reﬁnements that can be implemented as a noninvasive extension to EMOF-like meta-metamodels. As a consequence, existing tools need not be adapted and composite instantiation patterns are fully supported. We validate our concept by creating a deep modeling architecture description language based on the Palladio Component Model and demonstrate its advantages by modeling a synthetic web application. We show that existing tools for incremental model analysis can be reused and manifest several orders of speedup for a synthetic example analysis.",
        "keywords": [
            "Deep modeling",
            "Structural decomposition",
            "Reﬁnements",
            "NMeta",
            "Modeling language"
        ],
        "authors": [
            "Georg Hinkel"
        ],
        "file_path": "data/sosym-all/s10270-018-0701-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correct development of real time and embedded systems",
        "submission-date": "2008/03",
        "publication-date": "2008/03",
        "abstract": "This issue is special in that for the ﬁrst time in the 7-year history of SoSyM, a special section is dedicated to the outcome of a major research project. The OMEGA project was funded by the European Union (EU) and brought together experts in the ﬁeld of embedded and real-time systems. A major result of the project is a UML proﬁle called “OMEGA UML proﬁle”. This proﬁle provides concepts and modeling constructs for specifying real-time constraints.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-008-0087-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The RALph miner for automated discovery and veriﬁcation of resource-aware process models",
        "submission-date": "2018/12",
        "publication-date": "2020/08",
        "abstract": "Automated process discovery is a technique that extracts models of executed processes from event logs. Logs typically include information about the activities performed, their timestamps and the resources that were involved in their execution. Recent approaches to process discovery put a special emphasis on (human) resources, aiming at constructing resource-aware process models that contain the inferred resource assignment constraints. Such constraints can be complex and process discovery approaches so far have missed the opportunity to represent expressive resource assignments graphically together with process models. A subsequent veriﬁcation of the extracted resource-aware process models is required in order to check the proper utilisation of resources according to the resource assignments. So far, research on discovering resource-aware process models has assumed that models can be put into operation without modiﬁcation and checking. Integrating resource mining and resource-aware process model veriﬁcation faces the challenge that different types of resource assignment languages are used for each task. In this paper, we present an integrated solution that comprises (i) a resource mining technique that builds upon a highly expressive graphical notation for deﬁning resource assignments; and (ii) automated model-checking support to validate the discovered resource-aware process models. All the concepts reported in this paper have been implemented and evaluated in terms of feasibility and performance.",
        "keywords": [
            "Model checking",
            "Organisational mining",
            "Process mining",
            "Process veriﬁcation",
            "RALph",
            "Resource assignment",
            "Resource mining"
        ],
        "authors": [
            "Cristina Cabanillas",
            "Lars Ackermann",
            "Stefan Schönig",
            "Christian Sturm",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-020-00820-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Stateful component-based performance models",
        "submission-date": "2012/02",
        "publication-date": "2013/04",
        "abstract": "The accuracy of performance-prediction models\nis crucial for widespread adoption of performance prediction\nin industry. One of the essential accuracy-inﬂuencing aspects\nofsoftwaresystemsisthedependenceofsystembehaviouron\na conﬁguration, context or history related state of the system,\ntypically reﬂected with a (persistent) system attribute. Even\nin the domain of component-based software engineering, the\npresence of state-reﬂecting attributes (the so-called inter-\nnal states) is a natural ingredient of the systems, implying\nthe existence of stateful services, stateful components and\nstateful systems as such. Currently, there is no consensus on\nthe deﬁnition or method to include state-related information\nin component-based prediction models. Besides the task to\nidentify and localise different types of stateful information\nacross component-based software architecture, the issue is\nto balance the expressiveness and complexity of prediction\nmodels via an effective abstraction of state modelling. In\nthis paper, we identify and classify stateful information in\ncomponent-based software systems, study the performance\nimpact of the individual state categories, and discuss the costs\nof their modelling in terms of the increased model size. The\nobservations are formulated into a set of heuristics-guiding\nsoftwareengineersinstatemodelling.Finally,practicaleffect\nof state modelling on software performance is evaluated on a\nreal-world case study, the SPECjms2007 Benchmark. The\nobserved deviation of measurements and predictions was\nsigniﬁcantly decreased by more precise models of stateful\ndependencies.",
        "keywords": [
            "Stateful components",
            "Performance prediction",
            "Prediction accuracy"
        ],
        "authors": [
            "Lucia Happe",
            "Barbora Buhnova",
            "Ralf Reussner"
        ],
        "file_path": "data/sosym-all/s10270-013-0336-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The 2012 “State of the Journal” Report",
        "submission-date": "2013/01",
        "publication-date": "2013/01",
        "abstract": "SoSyM continues to experience an increasing number of submissions. In 2012, 308 new manuscripts were submitted, of which 49 % were reviewed for regular issues, while the other 51 % were submitted for special or theme sections. The average number of days from submission to a ﬁnal decision (that is, a ﬁnal accept or reject) was 136 days in 2012. This is 12 days less than the average in 2011. The number of months to online publication of an accepted paper has continued to drop as well, and is now between 3 and 4 weeks.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Geri Georg",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-012-0310-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Rigorous identiﬁcation and encoding of trace-links in model-driven engineering",
        "submission-date": "2009/01",
        "publication-date": "2010/03",
        "abstract": "Model-driven engineering (MDE) involves the construction and manipulation of many models of different kinds in an engineering process. In principle, models can be used in the product engineering lifecycle in an end-to-end manner for representing requirements, designs and implementations, and assisting in deployment and maintenance. The manipulations applied to models may be manual, but they can also be automated—for example, using model transformations, code generation, and validation. To enhance automated analysis, consistency and coherence of models used in an MDE process, it is useful to identify, establish and maintain trace-links between models. However, the breadth and scope of trace-links that can be used in MDE is substantial, and managing trace-link information can be very complex. In this paper, we contribute to managing the complexity of traceability information in MDE in two ways: ﬁrstly, we demonstrate how to identify the different kinds of trace-links that may appear in an end-to-end MDE process; secondly, we describe a rigorous approach to deﬁning semantically rich trace-links between models, where the models themselves may be constructed using diverse modelling languages. The definition of rich trace-links allows us to use tools to maintain and analyse traceability relationships.",
        "keywords": [
            "Traceability",
            "Semantics",
            "Classiﬁcations",
            "Identiﬁcation"
        ],
        "authors": [
            "Richard F. Paige",
            "Nikolaos Drivalos",
            "Dimitrios S. Kolovos",
            "Kiran J. Fernandes",
            "Christopher Power",
            "Goran K. Olsen",
            "Steffen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-010-0158-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Feature Nets: behavioural modelling of software product lines",
        "submission-date": "2013/12",
        "publication-date": "2015/06",
        "abstract": "Software product lines (SPLs) are diverse systems that are developed using a dual engineering process: (a) family engineering defines the commonality and variability among all members of the SPL, and (b) application engineering derives specific products based on the common foundation combined with a variable selection of features. The number of derivable products in an SPL can thus be exponential in the number of features. This inherent complexity poses two main challenges when it comes to modelling: firstly, the formalism used for modelling SPLs needs to be modular and scalable. Secondly, it should ensure that all products behave correctly by providing the ability to analyse and verify complex models efficiently. In this paper, we propose to integrate an established modelling formalism (Petri nets) with the domain of software product line engineering. To this end, we extend Petri nets to Feature Nets. While Petri nets provide a framework for formally modelling and verifying single software systems, Feature Nets offer the same sort of benefits for software product lines. We show how SPLs can be modelled in an incremental, modular fashion using Feature Nets, provide a Feature Nets variant that supports modelling dynamic SPLs, and propose an analysis method for SPL modelled as Feature Nets. By facilitating the construction of a single model that includes the various behaviours exhibited by the products in an SPL, we make a significant step towards efficient and practical quality assurance methods for software product lines.",
        "keywords": [
            "Behavioural modelling",
            "Software product lines",
            "Petri nets",
            "Variability"
        ],
        "authors": [
            "Radu Muschevici",
            "José Proença",
            "Dave Clarke"
        ],
        "file_path": "data/sosym-all/s10270-015-0475-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven assessment of system dependability",
        "submission-date": "2007/03",
        "publication-date": "2008/03",
        "abstract": "Designers of complex real-time systems need to address dependability requirements early on in the development process. This paper presents a model-based approach that allows developers to analyse the dependability of use cases and to discover more reliable and safe ways of designing the interactions of the system with the environment. The hardware design and the dependability of the hardware to be used also needs to be considered. We use a probabilistic extension of statecharts to formally model the interaction requirements deﬁned in the use cases. The model is then evaluated analytically based on the success and failure probabilities of events. The analysis may lead to further reﬁnement of the use cases by introducing detection and recovery measures to ensure dependable system interaction. A visual modelling environment for our extended statecharts formalism support-ing automatic probability analysis has been implemented in AToM3, A Tool for Multi-formalism and Meta-Modelling. Our approach is illustrated with an elevator control system case study.",
        "keywords": [
            "Dependability",
            "Use cases",
            "Reliability",
            "Safety",
            "Requirements"
        ],
        "authors": [
            "Sadaf Mustaﬁz",
            "Ximeng Sun",
            "Jörg Kienzle",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-008-0084-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "BPMN-E2: a BPMN extension for an enhanced workﬂow description",
        "submission-date": "2016/08",
        "publication-date": "2018/03",
        "abstract": "This paper discusses a business process model and notation (BPMN) extension that includes new elements designed to improve its expressiveness. In previous works, different shortcomings concerning the BPMN language were detected. As a result, a set of requirements to overcome these issues was collected and used to guide this work. The proposed extension supports the representation of information commonly used by experts in the hazard analysis and critical control points domain, usually expressed in natural language, in a machine-understandable fashion. To take full advantage of the features introduced in this BPMN extension, tools such as ProM can be easily upgraded with appropriate plugins to support the new elements. In this line, an advanced conformance checking plugin was developed for process mining on BPMN models. A real-world example of use showing the beneﬁts of applying the new elements is also discussed. This proposal paves the way for novel advanced analysis mechanisms for traceability systems.",
        "keywords": [
            "BPMN",
            "HACCP",
            "Quality controls",
            "Conformance checking",
            "Process monitoring",
            "Process mining",
            "Enhanced expressiveness",
            "Time constraints",
            "Activity effect",
            "Parenteral nutrition"
        ],
        "authors": [
            "Mateo Ramos-Merino",
            "Juan M. Santos-Gago",
            "Luis M. Álvarez-Sabucedo",
            "Victor M. Alonso-Roris",
            "Javier Sanz-Valero"
        ],
        "file_path": "data/sosym-all/s10270-018-0669-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special Section of BPMDS 2022—reﬂections on interactions and responsibility in a digitized business processes ecosystem",
        "submission-date": "2024/10",
        "publication-date": "2024/11",
        "abstract": "The Business Process Modeling, Development and Support (BPMDS) working conference series has served as a meeting place for researchers and practitioners in business process modeling, development, and support since 1998. This special section follows the 23rd edition of the BPMDS series, organized in conjunction with CAISE 2022, which focused on the theme of ‘Reﬂections on human-human interaction and responsibility in a virtual environment.’",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-024-01240-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Empirical assessment of two approaches for specifying software product line use case scenarios",
        "submission-date": "2014/10",
        "publication-date": "2015/05",
        "abstract": "Modularity beneﬁts, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those beneﬁts have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the beneﬁts of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The ﬁrst uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario speciﬁcations. After evaluating these approaches through the speciﬁcations of several systems, we ﬁnd out that MSVCM reduces feature scatteringandimprovesscenariocohesion.Theseresultssug- gest that evolving a product line speciﬁcation using MSVCM requiresonlylocalizedchanges.Ontheotherhand,theresults of six experiments reveal that MSVCM requires more time to derive the product line speciﬁcations and, contrasting with the modularity results, reduces the time to evolve a prod- uct line speciﬁcation only when the subjects have been well trained and are used to the task of evolving product line spec- iﬁcations.",
        "keywords": [
            "Usage scenarios",
            "Requirements engineering",
            "Software modularity",
            "Software product lines",
            "Experimentation in software engineering"
        ],
        "authors": [
            "Rodrigo Bonifácio",
            "Paulo Borba",
            "Cristiano Ferraz",
            "Paola Accioly"
        ],
        "file_path": "data/sosym-all/s10270-015-0471-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synthesis of test scenarios using UML activity diagrams",
        "submission-date": "2008/04",
        "publication-date": "2009/10",
        "abstract": "Often system developers follow Uniﬁed Modeling Language (UML) activity diagrams to depict all possible ﬂows of controls commonly known as scenarios of use cases. Hence, an activity diagram is treated as a useful design artifact to identify all possible scenarios and then check faults in scenarios of a use case. However, identiﬁcation of all possible scenarios and then testing with activity diagrams is a challenging task because several control ﬂow constructs and their nested combinations make path identiﬁcation difﬁcult. In this paper, we address this problem and propose an approach to identify all scenarios from activity diagrams and use them to test use cases. The proposed approach is based on the classiﬁcation of control constructs followed by a transformation approach which takes into account any combination of nested structures and transforms an activity diagram into a model called Intermediate Testable Model (ITM). We use ITM to generate test scenarios. With our approach it is possible to generate more scenarios than the existing work. Further, the proposed approach can be directly carried out using design models without any addition of testability information unlike the existing approaches.",
        "keywords": [
            "UML",
            "Software testing",
            "Model-based testing",
            "Activity diagram",
            "Test case generation"
        ],
        "authors": [
            "Ashalatha Nayak",
            "Debasis Samanta"
        ],
        "file_path": "data/sosym-all/s10270-009-0133-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework for evaluating tool support for co-evolution of modeling languages, tools and models",
        "submission-date": "2024/02",
        "publication-date": "2024/10",
        "abstract": "We present a framework for evaluating language workbenches’ capabilities for co-evolution of graphical modeling languages, modeling tools and models. As with programming, maintenance tasks such as language reﬁnement and enhancement typically account for more work than the initial development phase. Modeling languages have the added challenge of keeping tools and existing models in step with the evolving language. As domain-speciﬁc modeling languages and tools have started to be used widely, thanks to reports of signiﬁcant productivity improvements, some language workbench users have indeed reported problems with co-evolution of tools and models. Our tool-agnostic evaluation framework aims to cover changes across the whole language deﬁnition: the abstract syntax, concrete syntax, and constraints. Change impact is assessed for knock-on effects within the language deﬁnition, the modeling tools, semantics via generators, and existing models. We demonstrate the viability of the framework by evaluating MetaEdit+, EMF/Sirius and Jjodel, providing a detailed evaluation process for others to repeat with their tools. The results of the evaluation show differences among the tools: from editors not opening correctly or at all, through highlighting of items requiring manual intervention, to fully automatic updates of languages, models and editors. We call for industry to evaluate their tool choices with the framework, tool developers to extend their tool support for co-evolution, and researchers to reﬁne the evaluation framework and evaluations presented.",
        "keywords": [
            "Domain-speciﬁc modeling",
            "Domain-speciﬁc language",
            "Evolution",
            "Maintenance",
            "Metamodel evolution",
            "Model evolution"
        ],
        "authors": [
            "Juha-Pekka Tolvanen",
            "Steven Kelly",
            "Juri Di Rocco",
            "Alfonso Pierantonio",
            "Giordano Tinella"
        ],
        "file_path": "data/sosym-all/s10270-024-01218-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Analyzing the impact of human errors on interactive service robotic scenarios via formal veriﬁcation",
        "submission-date": "2022/06",
        "publication-date": "2023/08",
        "abstract": "Developing robotic applications with human–robot interaction for the service sector raises a plethora of challenges. In these settings, human behavior is essentially unconstrained as they can stray from the plan in numerous ways, constituting a critical source of uncertainty for the outcome of the robotic mission. Application designers require accessible and reliable frameworks to address this issue at an early development stage. We present a model-driven framework for developing interactive service robotic scenarios, allowing designers to model the interactive scenario, estimate its outcome, deploy the application, and smoothly reconﬁgure it. This article extends the framework compared to previous works by introducing an analysis of the impact of human errors on the mission’s outcome. The core of the framework is a formal model of the agents at play—the humans and the robots—and the robotic mission under analysis, which is subject to statistical model checking to estimate the mission’s outcome. The formal model incorporates a formalization of different human erroneous behaviors’ phenotypes, whose likelihood can be tuned while conﬁguring the scenario. Through scenarios inspired by the healthcare setting, the evaluation highlights how different conﬁgurations of erroneous behavior impact the veriﬁcation results and guide the designer toward the mission design that best suits their needs.",
        "keywords": [
            "Human",
            "robot interaction",
            "Human errors",
            "Service robotics",
            "Formal veriﬁcation",
            "Formal modeling",
            "Stochastic Hybrid Automata",
            "Statistical model checking"
        ],
        "authors": [
            "Livia Lestingi",
            "Andrea Manglaviti",
            "Davide Marinaro",
            "Luca Marinello",
            "Mehrnoosh Askarpour",
            "Marcello M. Bersani",
            "Matteo Rossi"
        ],
        "file_path": "data/sosym-all/s10270-023-01125-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The concepts of Petri nets",
        "submission-date": "2014/01",
        "publication-date": "2014/08",
        "abstract": "“Everything is connected to everything else.” An attempt to\nfind the origins of this piece of lore with a search engine\nwill produce sources from esoterism, Buddhism, quantum\nmechanics and many more completely different areas. For\ncomputer scientists and engineers, this sentence is a chal-\nlenge. With only a few components, such a system is man-\nageable. However, if the system grows and each new com-\nponent is connected to each existing one, the number of con-\nnections grows quadratically. Such systems will soon be too\nlarge to manage. In fact, large technical systems are generally\nconstructed differently: They consist of subsystems (compo-\nnents), each of which is directly connected to only a few other\nsubsystems. In this way, too, everything can be connected\nto everything else, but many connections are only indirect,\nmediated by other subsystems. The currently most striking\nexample of such a connected system is probably the Internet.\nTo design a system, analyze it or present it to other people,\nit is modeled. Since the 1950s, primarily automata models\nhave been used to model the behavior of technical systems. A\nclassic automaton models a system as a monolithic block. A\nglobal state indicates the current condition of each element at\na given point in time. A step transforms one global state into\nanother global state. Such a global perception is unsuitable\nfor large systems with many components because the number\nof global states often grows exponentially with the number\nof elements. No one, for example, would even attempt to\nconceive of the Internet as an automaton, which proceeds in\na sequence of steps from one global state to the next. This is\nwhy modeling is often actually limited to individual system\ncomponents, while the interaction with other components is\ndiscussed elsewhere, or is even neglected.\nAlready in the early 1960s, these practices led C. A. Petri\nto look for an alternative to automata that would put the\nlocal interaction of components at the center of modeling\n[10]. It was a bold idea, in a time when informatics sys-\ntems were not yet that large. Today, the idea is not that sur-\nprising anymore because the complex connection structures\nbetween components have a much greater inﬂuence on the",
        "keywords": [
            "Modeling language",
            "Petri nets",
            "Carl Adam\nPetri",
            "Modularity",
            "Locality"
        ],
        "authors": [
            "Jörg Desel",
            "Wolfgang Reisig"
        ],
        "file_path": "data/sosym-all/s10270-014-0423-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling for sustainability: Sustainable Development Goals (SDG) of the United Nations",
        "submission-date": "2024/07",
        "publication-date": "2024/07",
        "abstract": "Our world has pressing problems. We are glad that the United Nations has undertaken the effort to develop a framework for deﬁning the most pressing Sustainable Development Goals (SDGs) and how they should be addressed. Their website (https://sdgs.un.org/goals) summarizes the 17 goals for sustainable development as follows: This list of SDGs was adopted by all United Nations Member States in 2015. It also embodies an urgent call for action by all countries to help end poverty and other deprivations, improve health and education, reduce inequality, spur economic growth, tackle climate change, and preserve our oceans and forests.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01196-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Embedding domain-speciﬁc modelling languages in Maude speciﬁcations",
        "submission-date": "2011/01",
        "publication-date": "2012/02",
        "abstract": "We propose a formal approach for the defini-tion and analysis of domain-speciﬁc modelling languages (dsml). The approach uses standard model-driven engineering artifacts for deﬁning a language’s syntax (using metamodels) and its operational semantics (using model transformations). We give formal meanings to these artifacts by translating them to the Maude language: metamodels and models are mapped to equational speciﬁcations, and model transformations are mapped to rewrite rules between such speciﬁcations, which are also expressible in Maude due to Maude’s reﬂective capabilities. These mappings provide us, on the one hand, with abstract definitions of the mde concepts used for deﬁning dsml, which naturally capture their intended meanings; and, on the other hand, with equivalent executable definitions, which can be directly used by Maude for formal veriﬁcation. We also study a notion of operational semantics-preserving model transformations, which are model transformations between two dsml that ensure that each execution of a transformed instance is matched by an execution of the original instance. We propose a semi-decision procedure, implemented in Maude, for checking the semantics-preserving property. We also show how the procedure can be adapted for tracing ﬁnite executions of the transformed instance back to matching executions of the original one. The approach is illustrated on xspem, a language for describing the execution of activities constrained by time, precedence, and resource availability.",
        "keywords": [
            "Domain-speciﬁc languages",
            "Algebraic speciﬁcations",
            "Formal veriﬁcation",
            "Maude"
        ],
        "authors": [
            "Vlad Rusu"
        ],
        "file_path": "data/sosym-all/s10270-012-0232-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Maude"
        }
    },
    {
        "title": "Connecting software build with maintaining consistency between models: towards sound, optimal, and ﬂexible building from megamodels",
        "submission-date": "2019/07",
        "publication-date": "2020/03",
        "abstract": "Software build systems tackle the problem of building software from sources in a way which is sound (when a build completes successfully, the relations between the generated and source ﬁles are as speciﬁed) and optimal (only genuinely required rebuilding steps are done). In this paper, we explain and exploit the connection between software build and the megamodel consistency problem. The model-driven development of systems involves multiple models, metamodels and transformations. Transformations—which may be bidirectional—specify, and provide means to enforce, desired “consistency” relationships between models. We can describe the whole conﬁguration using a megamodel. As development proceeds, and various models are modiﬁed, we need to be able to restore consistency in the megamodel, so that the consequences of decisions ﬁrst recorded in one model are appropriately reﬂected in the others. At the same time, we need to minimise the amount of recomputation needed; in particular, we would like to avoid reapplying a transformation when no relevant changes have occurred in the models it relates. The megamodel consistency problem requires ﬂexibility beyond what is found in conventional software build, because different results are obtained depending on which models are allowed to be modiﬁed and on the order and directionoftransformationapplication.Inthispaper,weproposeusinganorientationmodeltomakeimportantchoicesexplicit. We show how to extend the formalised build system pluto to provide a means of restoring consistency in a megamodel, that is, in appropriate senses, ﬂexible, sound and optimal.",
        "keywords": [
            "Megamodel",
            "Build system",
            "Model transformation",
            "Bidirectionality",
            "Orientation model"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-020-00788-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On enterprise coherence governance with GEA: a 15-year co-evolution of practice and theory",
        "submission-date": "2022/02",
        "publication-date": "2022/10",
        "abstract": "General Enterprise Architecting (GEA) is an enterprise architecture method which has emerged out of a need in practice, and has been developed and matured over the past 15 years. The GEA method differs from other enterprise architecture approaches in that it has a strong focus on enterprise coherence and the explicit governance thereof. This focus followed from the observed need to move beyond the Business-IT alignment and ‘Business-to-IT’ stack thinking that is embodied in most of the existing enterprise architecture approaches. The main objective of this paper is to report, and reﬂect on, the development of the GEA method (so-far), which involved a co-evolution between theory and practice. In doing so, we also present core elements of (the current version of) GEA, and illustrate these in terms of a real-world (social housing) case. We will, furthermore, also discuss some of the lessons learned in applying GEA across different organizations.",
        "keywords": [
            "Enterprise architecture"
        ],
        "authors": [
            "Henderik A. Proper",
            "Roel Wagter",
            "Joost Bekel"
        ],
        "file_path": "data/sosym-all/s10270-022-01059-0.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "An overview of model checking practices on veriﬁcation of PLC software",
        "submission-date": "2013/11",
        "publication-date": "2014/12",
        "abstract": "Programmablelogiccontrollers(PLCs)areheav-ily used in industrial control systems, because of their high capacity of simultaneous input/output processing capabili-ties. Characteristically, PLC systems are used in mission crit-ical systems, and PLC software needs to conform real-time constraints in order to work properly. Since PLC program-ming requires mastering low-level instructions or assembly like languages, an important step in PLC software produc-tion is modelling using a formal approach like Petri nets or automata. Afterward, PLC software is produced semiau-tomatically from the model and reﬁned iteratively. Model checking, on the other hand, is a well-known software veriﬁ-cation approach, where typically a set of timed properties are veriﬁed by exploring the transition system produced from the software model at hand. Naturally, model checking is applied in a variety of ways to verify the correctness of PLC-based software. In this paper, we provide a broad view about the difﬁculties that are encountered during the model checking process applied at the veriﬁcation phase of PLC software pro-duction. We classify the approaches from two different per-spectives: ﬁrst, the model checking approach/tool used in the veriﬁcation process, and second, the software model/source code and its transformation to model checker’s speciﬁcation language. In a nutshell, we have mainly examined SPIN, SMV, and UPPAAL-based model checking activities and model construction using Instruction Lists (and alike), Func-tion Block Diagrams, and Petri nets/automata-based model construction activities. As a result of our studies, we provide a comparison among the studies in the literature regarding various aspects like their application areas, performance con-siderations, and model checking processes. Our survey can be used to provide guidance for the scholars and practitioners planning to integrate model checking to PLC-based software veriﬁcation activities.",
        "keywords": [
            "Model checking",
            "Programmable logic controllers",
            "Program veriﬁcation"
        ],
        "authors": [
            "Tolga Ovatman",
            "Atakan Aral",
            "Davut Polat",
            "Ali Osman Ünver"
        ],
        "file_path": "data/sosym-all/s10270-014-0448-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling with Gentleman: a web-based projectional editor",
        "submission-date": "2023/11",
        "publication-date": "2024/10",
        "abstract": "Domain-speciﬁc modeling (DSM) makes software more accessible and inclusive, leveraging the expertise and knowledge of\nvarious experts instead of relying solely on technical experts. However, the widespread adoption of DSM is hampered due to poor tool support and modeling languages. In recent years, projectional editing has proven to be a promising approach to\ncreating and manipulating domain-speciﬁc languages. This editing approach contrasts with traditional parser-based editing by allowing user edits to directly modify the abstract syntax tree. It supports various notations, enabling more intuitive lan-\nguages and facilitating language extension and composition. However, current projectional editing solutions are heavyweight,\nplatform-speciﬁc, and hard to integrate with. We aim to provide better support for this paradigm and make modeling more accessible to domain experts. Thus, we present Gentleman, a lightweight web-based projectional editor. With Gentleman,\nusers can create models with simple structures and manipulate them with user-friendly projections. We evaluate Gentleman through a user study, demonstrating its ability to create and manipulate models effectively.",
        "keywords": [
            "Projectional editing",
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Language workbench"
        ],
        "authors": [
            "Louis-Edouard Lafontant\nEugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-024-01219-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling in advanced systems engineering",
        "submission-date": "2022/03",
        "publication-date": "2022/03",
        "abstract": "Previous editorials highlighted the use of models and the activity of modeling as an integral part of almost all development activities for complex systems. Complexity is an omnipresent challenge, for software, for mechanical systems with intelligence (e.g., autonomous cars, robots, healthcare gadgets or airplanes), for production plants, and also for biological systems (e.g., advanced medicine, human cells, organs, and even full organisms), that will be engineered with increased frequency in the future. “Systems of systems” are typically confederated and collaborating ensembles of systems that were originally designed individually, but through new opportunities that were not originally envisioned, are now required to tightly cooperate. Examples of such systems include telecommunications services, multiple integrated web services across the internet, energy networks and also the city networks of collaborating smart buildings and transportation infrastructure. \nAll of the examples listed previously need models. During development of these initially independent systems, models are often deﬁned before the system exists, requiring explicit modeling activities to explicate requirements on the system to document multiple design decisions and alternatives. This is an entirely different process from how models come to life when using data science techniques to extract models (e.g., of behavior or typical dynamic conﬁgurations) from observations of existing systems. These other kinds of extracted models are helpful when optimizing existing systems and processes, but are not intended for the original design (which may have occurred by nature). However, it is our belief that even though the process for model creation and use is differ- ent across various domains, the same underlying modeling paradigms and concepts can often be incorporated. As a consequence, these models could also be adopted in simi- lar modeling languages.",
        "keywords": [],
        "authors": [
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-00999-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A method for transforming knowledge discovery metamodel to ArchiMate models",
        "submission-date": "2020/08",
        "publication-date": "2021/08",
        "abstract": "Enterprise architecture has become an important driver to facilitate digital transformation in companies, since it allows to manage IT and business in a holistic and integrated manner by establishing connections among technology concerns and strategical/motivational ones. Enterprise architecture modelling is critical to accurately represent business and their IT assets in combination. This modelling is important when companies start to manage their enterprise architecture, but also when it is remodelled so that the enterprise architecture is realigned in a changing world. Enterprise architecture is commonly modelled by few experts in a manual way, which is error-prone and time-consuming and makes continuous realignment difficult. In contrast, other enterprise architecture modelling proposal automatically analyses some artefacts like source code, databases, services, etc. Previous automated modelling proposals focus on the analysis of individual artefacts with isolated transformations toward ArchiMate or other enterprise architecture notations and/or frameworks. We propose the usage of Knowledge Discovery Metamodel (KDM) to represent all the intermediate information retrieved from information systems’ artefacts, which is then transformed into ArchiMate models. Thus, the core contribution of this paper is the model transformation between KDM and ArchiMate metamodels. The main implication of this proposal is that ArchiMate models are automatically generated from a common knowledge repository. Thereby, the relationships between different-nature artefacts can be exploited to get more complete and accurate enterprise architecture representations.",
        "keywords": [
            "Enterprise architecture",
            "ArchiMate",
            "Knowledge discovery metamodel",
            "Model transformation",
            "MDE",
            "ATL"
        ],
        "authors": [
            "Ricardo Pérez‑Castillo",
            "Andrea Delgado",
            "Francisco Ruiz",
            "Virginia Bacigalupe",
            "Mario Piattini"
        ],
        "file_path": "data/sosym-all/s10270-021-00912-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "On the adoption of blockchain for business process monitoring",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Being the blockchain and distributed ledger technologies particularly suitable to create trusted environments where participants do not trust each other, business process management represents a proper setting in which these technologies can be adopted. In this direction, current research work primarily focuses on blockchain-oriented business process design, or on execution engines able to enact processes through smart contracts. Conversely, less attention has been paid to study if and how blockchains can be beneﬁcial to business process monitoring. This work aims to ﬁll this gap by (1) providing a reference architecture for enabling the adoption of blockchain technologies in business process monitoring solutions, (2) deﬁning a set of relevant research challenges derived from this adoption, and (3) discussing the current approaches to address the aforementioned challenges.",
        "keywords": [
            "Blockchain",
            "Distributed ledger technology",
            "Business process management",
            "Software architectures",
            "Business process monitoring"
        ],
        "authors": [
            "Claudio Di Ciccio",
            "Giovanni Meroni",
            "Pierluigi Plebani"
        ],
        "file_path": "data/sosym-all/s10270-021-00959-x.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Models as the subject of research",
        "submission-date": "2019/08",
        "publication-date": "2019/08",
        "abstract": "It is said that in the times before Plato, mathematics was only a vehicle to solve practical problems. Plato himself, however, identified mathematics as a pure subject of research, with the observation that the beauty of mathematical theories was of deep intrinsic value to be developed and refined independent of practical application. This spawned mathematics as a field of research, which also led to many more practical applications as a result.\nComputer science has a much younger history. Some early approaches to general-purpose programming (e.g., Charles Babbage’s mechanical general-purpose computer in the 1800s, the Analytical Engine, and Ada Lovelace’s first algorithms) are the most well-known examples. However, computer science (or better: informatics) really emerged in the twentieth century. Informatics started as a tool to help with other science and mathematics investigations. When the increasing complexity became clear, informatics became the target of its own research area with beautiful results in various subdomains.\nModeling, however, is much older. Conceptual models are omnipresent in philosophy, physics, chemistry, and many other inquiry-based disciplines. Physical manifestations of models were used frequently in building construction. The term “model” had an early historical connection to the construction of churches, where a model was a 1:10 reduced wooden version of the church to be built. From the great inventor Da Vinci, we still have many drawings that model surprising and smart machines—even though many of them have never been built. The authors of this editorial, however, are not aware of the topic of modeling as its own subject or research area in these earlier centuries.\nThe concept of a model needed more formalization when a specific semantics was required, such as modeling of systems and software. The most widely used definition of “model” was coined by Stachowiak only in 1973. Programming theory and semantics definitions needed a precisely defined notion of a well-formed piece of code. This notion also carried over to other forms of digitally communicated models. This includes Petri Nets in their various forms, automata and statecharts, class diagrams, action languages, and other forms of modeling languages. The formal methods domain (including logic) and the programming language domain (in particular, compiler construction) in informatics were the earliest to put some focus on the value of models and explicitly defined modeling languages. Software engineers and database administrators also relied upon models in various representations, focusing on the practical use of restricted forms of models. Interestingly, modeling tools have been a core product offered by commercial vendors since the beginning of the software industry. In fact, the first software product sold independently of a hardware package was Autoflow, which was a flowchart modeling tool developed in 1964 by Martin Goetz of Applied Data Research.\nThe use of models was widely discussed in the software engineering domain during the 1980s and 1990s, where a variety of different modeling languages (e.g., Booch’s clouds and Rumbaugh’s object models) were adopted. Around 1994, the overlap of common concepts across the various modeling languages pointed to the need for a unification of the different languages. The “methods wars”, which were intensely discussed at the OOPSLA conferences during the 1990s, were finally resolved into a standardization effort, first called the “Unified Method” and later leading to the “Unified Modeling Language” (UML). During this crucial period of unification, it became clear that defining such a standard would not be an easy task. A research community emerged that became interested in studying models and the UML as a core research subject area of its own.\nIn 1998, Jean Bézivin and Pierre-Alain Muller organized the first UML workshop, “The Unified Modeling Language. «UML»’98: Beyond the Notation” in Mulhouse, France,",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-019-00751-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multi-dimensional multi-level modeling",
        "submission-date": "2020/05",
        "publication-date": "2022/01",
        "abstract": "The growth of multi-level modeling has resulted in an increase of level-organization alternatives which signiﬁcantly differ from each other with respect to their underlying foundations and the well-formedness rules they enforce. Alternatives substantially divergewithrespect tohowlevel boundaries shouldgovern instance-of relationships, what modelingmechanisms theyemploy, and what modeling principles they establish. In this article, I analyze how a number of multi-level modeling approaches deal with certain advanced modeling scenarios. In particular, I identify linear domain metamodeling, i.e., the requirement that all domain-induced instance-of relationships align with a single global level-hierarchy, as a source of accidental complexity. I propose a novel multi-dimensional multi-level modeling approach based on the notion of orthogonal ontological classiﬁcation that supports modeling of domain scenarios with minimal complexity while supporting separation of concerns and sanity-checking to avoid inconsistent modeling choices.",
        "keywords": [
            "Multi-level modeling",
            "Level",
            "well-formedness",
            "Sanity-checking",
            "Ontological classiﬁcation",
            "Multi-dimensional"
        ],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-021-00951-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven engineering with domain-speciﬁc meta-modelling languages",
        "submission-date": "2012/10",
        "publication-date": "2013/07",
        "abstract": "Domain-speciﬁc modelling languages are normally deﬁned through general-purpose meta-modelling languages like the MOF. While this is satisfactory for many model-driven engineering (MDE) projects, several researchers have identiﬁed the need for domain-speciﬁc meta-modelling (DSMM) languages. These provide customised domain-speciﬁc meta-modelling primitives aimed at the deﬁnition of modelling languages for a speciﬁc domain, as well as the construction of meta-model families. Unfortunately, current approaches to DSMM rely on ad hoc methods which add unnecessary complexity to the realization of DSMM in practice. Hence, the goal of this paper is to simplify the deﬁnition and usage of DSMM languages. For this purpose, we apply multi-level meta-modelling for the systematic engineering of DSMM architectures. Our method integrates techniques to control the meta-modelling primitives offered to the users of the DSMM languages, provides a ﬂexible approach to deﬁne textual concrete syntaxes for DSMM languages, and extends existing model management languages (for model-to-model transformation, in-place transformation and code generation) to work in a multi-level setting, thus enabling the practical use of DSMM in MDE. As a proof of concept, we report on a working implementation of these ideas in the MetaDepth tool.",
        "keywords": [
            "Model-driven engineering",
            "Multi-level meta-modelling",
            "Domain-speciﬁc meta-modelling",
            "Textual concrete syntax",
            "MetaDepth"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-013-0367-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Broadened support for software and system model interchange",
        "submission-date": "2017/06",
        "publication-date": "2019/04",
        "abstract": "Although sound performance analysis theories and techniques exist, they are not widely used because they require extensive expertise in performance modeling and measurement. The overall goal of our work is to make performance modeling more accessible by automating much of the modeling effort. We have proposed a model interoperability framework that enables performance models to be automatically exchanged among modeling (and other) tools. The core of the framework is a set of model interchange formats (MIF): a common representation for data required by performance modeling tools. Our previous research developed a representation for system performance models (PMIF) and another for software performance models (S-PMIF), both based on the Queueing Network Modeling (QNM) paradigm. In order to manage the research scope and focus on model interoperability issues, the initial MIFs were limited to QNMs that can be solved by efﬁcient, exact solution algorithms. The overall model interoperability approach has now been demonstrated to be viable. This paper broadens the scope of PMIF and S-PMIF to represent models that can be solved with additional methods such as analytical approximations or simulation solutions. It presents the extensions considered, describes the extended meta-models, and provides veriﬁcation with examples and a case study.",
        "keywords": [
            "Performance modeling",
            "Interoperability",
            "Model interchange formats",
            "Software performance engineering (SPE)",
            "Queueing networks",
            "Software performance models"
        ],
        "authors": [
            "Catalina M. Lladó",
            "Connie U. Smith"
        ],
        "file_path": "data/sosym-all/s10270-019-00728-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An executable object-oriented semantics and its application to ﬁrewall veriﬁcation",
        "submission-date": "2008/07",
        "publication-date": "2010/04",
        "abstract": "This paper presents a formal executable semantics of object-oriented models. We made it possible to conduct both simulation and theorem proving on the semantics by implementing it within the expressive intersection of the functional programming language ML and the theorem prover HOL. In this paper, we present the definition and implementation of the semantics. We also present a prototype veriﬁcation tool ObjectLogic which supports simulation and theorem proving on the semantics. As a case study, we show the veriﬁcation of a practical ﬁrewall system.",
        "keywords": [
            "Object-Oriented",
            "Theorem proving",
            "Simulation",
            "HOL",
            "ML"
        ],
        "authors": [
            "Kenro Yatake",
            "Takuya Katayama"
        ],
        "file_path": "data/sosym-all/s10270-010-0160-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating the comprehension of means-ends maps",
        "submission-date": "2017/11",
        "publication-date": "2018/09",
        "abstract": "Information and software systems development is rapidly changing due to exponential technology development. This acceleration is also impacting other technology or engineering domains. Thus, there is a need to identify problems and their solutions, and to reason about new options so as to better arrive at the right decision of which technology or solution should be adopted among various alternatives. In this paper, we argue that such know-how information can be mapped, to ease such tasks. In particular, we examine the hypothesis that know-how mapping, using an approach we call Means-Ends Map (ME-MAP), facilitates analysis in technological domains. We design a controlled experiment to assess the comprehension of ME maps with that of textual summaries in two different domains. We find that subjects exploring a domain using ME maps were able to better identify solutions and better understand the tradeoffs among alternative solutions. Furthermore, these subjects gained that understanding faster compared to those using textual summaries.",
        "keywords": [
            "Knowledge mapping",
            "Literature review",
            "ME-MAP",
            "Evaluation",
            "Controlled experiment"
        ],
        "authors": [
            "Jumana Nassour",
            "Michael Elhadad",
            "Arnon Sturm",
            "Eric Yu"
        ],
        "file_path": "data/sosym-all/s10270-018-0691-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying CSP ∥B to information systems",
        "submission-date": "2005/02",
        "publication-date": "2007/04",
        "abstract": "CSP ∥B is a formal approach which combines state and event-based descriptions of a system. It enables the automatic veriﬁcation of dynamic properties using model checking techniques. In this paper we identify a variation on the standard CSP ∥B architecture so that it is more appli-cable to support the speciﬁcation of information systems. We specify a library system using this new architecture. We examine several safety and liveness requirements and dem-onstrate that we can compositionally verify them using FDR. If a property fails to model check we identify an abstrac-tion technique which enables us to pinpoint the cause of the failure.",
        "keywords": [
            "CSP",
            "B",
            "Information systems",
            "Combining formalisms",
            "Compositional veriﬁcation"
        ],
        "authors": [
            "Neil Evans",
            "Helen Treharne",
            "Régine Laleau",
            "Marc Frappier"
        ],
        "file_path": "data/sosym-all/s10270-007-0048-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing remaining time prediction in business processes by considering system-level and resource-level inter-case features",
        "submission-date": "2023/04",
        "publication-date": "2025/01",
        "abstract": "Accurately predicting the remaining time of running cases is crucial for effective scheduling in business processes. To achieve accurate predictions, it is essential to consider the conditions and behavior of organizational resources, which signiﬁcantly impact process completion times. Interactions between different cases can change resource conditions, so inter-case features created by these interactions should be considered in remaining time prediction. These inter-case features can be considered at both the system and resource levels. While past studies have largely focused on system-level features, they have often neglected the impact of resource-level features. This research investigates the effect of inter-case features on various prediction models by developing a conceptual framework that extracts open cases and resource multitasking as indicators of system and resource workloads, along with resource experience features, from event logs. Using four regression algorithms, four bucketing methods, and ﬁve encoding techniques, the study applies predictive process monitoring models to eight real-world datasets. The ﬁndings indicate that incorporating inter-case features generally improves prediction accuracy, particularly in processes with high resource and system workloads. However, the optimal model conﬁguration for the highest accuracy varies across datasets. Although inter-case features enhance prediction accuracy, they also increase both ofﬂine and online execution times. This study underscores the importance of considering both system and resource workloads for accurate remaining time prediction and provides a framework to guide the integration of inter-case features.",
        "keywords": [
            "Predictive process monitoring",
            "Remaining time prediction",
            "Inter-case features",
            "Open cases",
            "Resource multitasking"
        ],
        "authors": [
            "Reza Aalikhani",
            "Mohammad Fathian",
            "Mohammad Reza Rasouli"
        ],
        "file_path": "data/sosym-all/s10270-025-01267-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Are models better read on paper or on screen? A comparative study",
        "submission-date": "2021/04",
        "publication-date": "2022/01",
        "abstract": "Is it really better to print everything, including software models, or is it better to view them on screen? With the ever\nincreasing complexity of software systems, software modeling is integral to software development. Software models facilitate\nand automate many activities during development, such as code and test case generation. However, a core goal of software\nmodeling is to communicate and collaborate. Software models are presented to team members on many mediums and two of\nthe most common mediums are paper and computer screens. Reading from paper or screen is ostensibly considered to have the\nsame effect on model comprehension. However, the literature on text reading has indicated that the reading experiences can\nbe very different which in turn effects various metrics related to reader performance. This paper reports on an experiment that\nwas conducted to investigate the effect of reading software models on paper in comparison with reading them on a computer\nscreen with respect to cognitive effectiveness. Cognitive effectiveness here refers to the ease by which a model reader can read\na model. The experiment used a total of 74 software engineering students as subjects. The experiment results provide strong\nevidence that displaying diagrams on a screen allows subjects to read them quicker. There is also evidence that indicates that\non screen viewing induces fewer reading errors.",
        "keywords": [
            "Paper-based reading",
            "Screen-based reading use case diagrams",
            "Feature diagrams",
            "Student-based experiments",
            "Controlled experiment",
            "Model comprehension",
            "Model representation"
        ],
        "authors": [
            "Mohamed El-Attar"
        ],
        "file_path": "data/sosym-all/s10270-021-00966-y.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Linear parallel algorithms to compute strong and branching bisimilarity",
        "submission-date": "2022/04",
        "publication-date": "2022/12",
        "abstract": "We present the ﬁrst parallel algorithms that decide strong and branching bisimilarity in linear time. More precisely, if a transition system has n states, m transitions and |Act| action labels, we introduce an algorithm that decides strong bisimilarity in O(n + |Act|) time on max(n, m) processors and an algorithm that decides branching bisimilarity in O(n + |Act|) time using up to max(n2, m, |Act|n) processors.",
        "keywords": [
            "Strong bisimulation",
            "Branching bisimulation",
            "RCPP",
            "Parallel algorithms",
            "PRAM"
        ],
        "authors": [
            "Jan Martens",
            "Jan Friso Groote",
            "Lars B. van den Haak",
            "Pieter Hijma",
            "Anton Wijs"
        ],
        "file_path": "data/sosym-all/s10270-022-01060-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Proﬁling the publish/subscribe paradigm for automated analysis using colored Petri nets",
        "submission-date": "2018/04",
        "publication-date": "2019/01",
        "abstract": "UML sequence diagrams are used to graphically describe the message interactions between the objects participating in a certain scenario. Combined fragments extend the basic functionality of UML sequence diagrams with control structures, such as sequences, alternatives, iterations, or parallels. In this paper, we present a UML proﬁle to annotate sequence diagrams with combined fragments to model timed Web services with distributed resources under the publish/subscribe paradigm. This proﬁle is exploited to automatically obtain a representation of the system based on Colored Petri nets using a novel model-to-model (M2M) transformation. This M2M transformation has been speciﬁed using QVT and has been integrated in a new add-on extending a state-of-the-art UML modeling tool. Generated Petri nets can be immediately used in well-known Petri net software, such as CPN Tools, to analyze the system behavior. Hence, our model-to-model transformation tool allows for simulating the system and ﬁnding design errors in early stages of system development, which enables us to ﬁx them at these early phases and thus potentially saving development costs.",
        "keywords": [
            "UML 2.5",
            "Distributed resources",
            "Publish/Subscribe",
            "Automated analysis",
            "WSRF",
            "WSN",
            "Colored Petri nets",
            "CPN tools"
        ],
        "authors": [
            "Abel Gómez",
            "Ricardo J. Rodríguez",
            "María-Emilia Cambronero",
            "Valentín Valero"
        ],
        "file_path": "data/sosym-all/s10270-019-00716-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Remarks on Egon Börger: “Approaches to model business processes: a critical analysis of BPMN, workﬂow patterns and YAWL, SOSYM 11:305–318”",
        "submission-date": "2012/10",
        "publication-date": "2013/01",
        "abstract": "Egon Börger (SOSYM, 11, pp. 305–318, 2012) challenges the concepts of BPMN, workﬂow patterns and YAWL as useful contributions to the modeling of business processes.IshowthathemisjudgestheroleofBPMN,YAWL and similar techniques in the modeling of business processes. In particular he mistakes YAWL’s formal basis, i.e. Petri nets. Börger furthermore suggests evaluation criteria for business process modeling tools. I argue that his criteria overemphasize some less important aspects, while ignoring some decisive ones.",
        "keywords": [
            "Business process modelling",
            "BPMN",
            "YAWL",
            "Petri nets",
            "Evaluation criteria for tools"
        ],
        "authors": [
            "Wolfgang Reisig"
        ],
        "file_path": "data/sosym-all/s10270-012-0306-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Specifying business rules in object-oriented analysis",
        "submission-date": "2003/12",
        "publication-date": "2004/08",
        "abstract": "A major purpose of analysis is to represent precisely all relevant facts, as they are observed in the external world. A substantial problem in object-oriented analysis is that most modelling languages are more suit- able to build computational models than to develop con- ceptual models. It is a rather blind assumption that con- cepts that are convenient for design can also be applied during analysis. Preconditions, postconditions and in- variants are typical examples of concepts with blurred se- mantics. At the level of analysis they are most often used to specify business rules. This paper introduces proper concepts for modelling business rules and speciﬁes their semantics.",
        "keywords": [
            "Object-oriented analysis",
            "Constraints",
            "Business rules",
            "Uniﬁed modeling language",
            "Object Constraint Language"
        ],
        "authors": [
            "Frank Devos",
            "Eric Steegmans"
        ],
        "file_path": "data/sosym-all/s10270-004-0064-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven development approach for context-aware systems",
        "submission-date": "2015/04",
        "publication-date": "2016/10",
        "abstract": "The widespread usage of various types of computer devices with different platform characteristics created a need for new methods and tools to support the development of context-aware applications capable of dynamically adapting themselves to context changes. In this paper, we present a new model-based approach that addresses the development of context-aware applications from both the theoretical and practical perspectives and that supports all development phases of context-aware systems. On the one hand, we describe how our approach is applied to dynamically capture, observe the change of the context and notify the system at runtime. On the other hand, we show how our approach is used by programmers to develop a context- aware application.",
        "keywords": [
            "Context modeling",
            "Application adaptation",
            "Context-aware application development",
            "Model-driven development"
        ],
        "authors": [
            "Imen Jaouadi",
            "Raoudha Ben Djemaa",
            "Hanêne Ben-Abdallah"
        ],
        "file_path": "data/sosym-all/s10270-016-0550-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using DAG transformations to verify Euler/Venn homogeneous and Euler/Venn FOL heterogeneous rules of inference",
        "submission-date": "2003/01",
        "publication-date": "2004/04",
        "abstract": "In this paper we will present a graph-transformation based method for the veriﬁcation of heterogeneous ﬁrst order logic (FOL) and Euler/Venn proofs. In previous work, it has been shown that a special collection of directed acyclic graphs (DAGs) can be used interchangeably with Euler/Venn diagrams in reasoning processes. Thus, proofs which include Euler/Venn diagrams can be thought of as proofs with DAGs where steps involving only Euler/Venn diagrams can be treated as particular DAG transformations. Here we will show how the characterization of these manipulations can be used to verify Euler/Venn proofs. Also, a method for verifying the use of heterogeneous Euler/Venn and FOL reasoning rules will be presented that is also based upon DAG transformations.",
        "keywords": [
            "Euler and Venn diagrams",
            "Diagrammatic reasoning",
            "Graph transformation",
            "Proof veriﬁcation"
        ],
        "authors": [
            "Nik Swoboda",
            "Gerard Allwein"
        ],
        "file_path": "data/sosym-all/s10270-003-0044-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "In search of eﬀective design abstractions",
        "submission-date": "2004/02",
        "publication-date": "2004/02",
        "abstract": "The perceived popularity of the modeling languages such as the UML may lead some to believe that there is wide-spread appreciation of the value of modeling in the software development industry. Informal polls in North American trade journals seem to indicate otherwise. Use of the UML seems to be limited to the use of use cases for requirements and class diagrams for graphically representing programs. The polls also seem to indicate that awareness of the Object Management Group’s Model Driven Architecture (MDA) is not widespread. As MDA is currently the most widespread model-based development approach this strongly indicates that much needs to be done to convince practicing developers of the value of model-driven development approaches in general. But, we expect to see in 2004 a surge in modeling tools in particular those that claim to be “MDA-Compliant”. Tool support is essential to realizing the vision of model-driven development, but use of tools without a solid understanding of the principles and methods they support can lead to failed projects and faulty perceptions of model-driven development. In this editorial we reﬂect on the value of the abstraction principle in software development and on how modeling approaches can support this principle.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-004-0052-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ceaml: A novel modeling language for enabling cloud and edge continuum orchestration",
        "submission-date": "2023/10",
        "publication-date": "2024/11",
        "abstract": "Cloud modeling languages (CMLs) are utilized to depict and describe Cloud infrastructures and services in a manner that is easily comprehensible and manipulable by Cloud orchestration systems. These languages play a pivotal role in the domain of Cloud and Edge computing, as they enable the deployment of applications and services that can be dynamically scaled and adapted to evolving workloads. In this paper, a novel CML is presented that encompasses the ability to represent runtime adaptation through an event-driven syntax that relies on Quality of Experience conditions. In addition to introducing the modeling language, this paper also explains how it was applied in a system to deploy and adapt applications within Kubernetes at runtime.",
        "keywords": [
            "Cloud modeling language",
            "Application model",
            "Cloud",
            "Edge",
            "Orchestration",
            "Runtime adaptation",
            "Workflows"
        ],
        "authors": [
            "Ioannis Korontanis",
            "Antonios Makris",
            "Konstantinos Tserpes"
        ],
        "file_path": "data/sosym-all/s10270-024-01222-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The uncertainty interaction problem in self-adaptive systems",
        "submission-date": "2022/04",
        "publication-date": "2022/08",
        "abstract": "The problem of mitigating uncertainty in self-adaptation has driven much of the research proposed in the area of software engineering for self-adaptive systems in the last decade. Although many solutions have already been proposed, most of them tend to tackle speciﬁc types, sources, and dimensions of uncertainty (e.g., in goals, resources, adaptation functions) in isolation. A special concern are the aspects associated with uncertainty modeling in an integrated fashion. Different uncertainties are rarely independent and often compound, affecting the satisfaction of goals and other system properties in subtle and often unpredictable ways. Hence, there is still limited understanding about the speciﬁc ways in which uncertainties from various sources interact and ultimately affect the properties of self-adaptive, software-intensive systems. In this SoSym expert voice, we introduce the Uncertainty Interaction Problem as a way to better qualify the scope of the challenges with respect to representing different types of uncertainty while capturing their interaction in models employed to reason about self-adaptation. We contribute a characterization of the problem and discuss its relevance in the context of case studies taken from two representative application domains. We posit that the Uncertainty Interaction Problem should drive future research in software engineering for autonomous and self-adaptive systems, and therefore, contribute to evolving uncertainty modeling towards holistic approaches that would enable the construction of more resilient self-adaptive systems.",
        "keywords": [
            "Uncertainty",
            "Modeling",
            "Self-adaptation",
            "Assurances"
        ],
        "authors": [
            "Javier Cámara",
            "Javier Troya",
            "Antonio Vallecillo",
            "Nelly Bencomo",
            "Radu Calinescu",
            "Betty H. C. Cheng",
            "David Garlan",
            "Bradley Schmerl"
        ],
        "file_path": "data/sosym-all/s10270-022-01037-6.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Modelling in low-code development: a multi-vocal systematic review",
        "submission-date": "2021/07",
        "publication-date": "2022/01",
        "abstract": "In 2014, a new software development approach started to get a foothold: low-code development. Already from its early\ndays, practitioners in software engineering have been showing a rapidly growing interest in low-code development. In 2021\nonly, the revenue of low-code development technologies reached 13.8 billion USD. Moreover, the business success of low-\ncode development has been sided by a growing interest from the software engineering research community. The model-driven\nengineering community has shown a particular interest in low-code development due to certain similarities between the two. In\nthis article, we report on the planning, execution, and results of a multi-vocal systematic review on low-code development, with\nspecial focus to its relation to model-driven engineering. The review is intended to provide a structured and comprehensive\nsnapshot of low-code development in its peak of inﬂated expectations technology adoption phase. From an initial set of\npotentially relevant 720 peer-reviewed publications and 199 grey literature sources, we selected 58 primary studies, which\nwe analysed according to a meticulous data extraction, analysis, and synthesis process. Based on our results, we tend to frame\nlow-code development as a set of methods and/or tools in the context of a broader methodology, often being identiﬁed as\nmodel-driven engineering.",
        "keywords": [
            "Low-code development",
            "Modelling",
            "Model-driven engineering",
            "Systematic review"
        ],
        "authors": [
            "Alessio Bucaioni",
            "Antonio Cicchetti",
            "Federico Ciccozzi"
        ],
        "file_path": "data/sosym-all/s10270-021-00964-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correctly defined concrete syntax",
        "submission-date": "2007/03",
        "publication-date": "2008/07",
        "abstract": "Due to their complexity, the syntax of modern modeling languages is preferably defined in two steps. The abstract syntax identifies all modeling concepts whereas the concrete syntax should clarify how these concepts are rendered by graphical and/or textual elements. While the abstract syntax is often defined in form of a metamodel, there does not exist such standard format yet for concrete syntax definitions. The diversity of definition formats—ranging from EBNF grammars to informal text—is becoming a major obstacle for advances in modeling language engineering, including the automatic generation of editors. In this paper, we propose a uniform format for concrete syntax definitions. Our approach captures both textual and graphical model representations and even allows to assign more than one rendering to the same modeling concept. Consequently, following our approach, a model can have multiple, fully equivalent representations, but—in order to avoid ambiguities when reading a model representation—two different models should always have distinguishable representations. We call a syntax definition correct, if all well-formed models are represented in a non-ambiguous way. As the main contribution of this paper, we present a rigorous analysis technique to check the correctness of concrete syntax definitions.",
        "keywords": [
            "Visual languages",
            "Concrete syntax",
            "Metamodeling",
            "OCL",
            "Triple-Graph-Grammars (TGGs)"
        ],
        "authors": [
            "Thomas Baar"
        ],
        "file_path": "data/sosym-all/s10270-008-0086-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Run-time threat models for systematic and continuous risk assessment",
        "submission-date": "2023/09",
        "publication-date": "2024/12",
        "abstract": "Threat modeling involves systematically assessing the likelihood and potential impact of diverse security threat scenarios. Existing threat modeling approaches and tools act at the level of a software architecture or design (e.g., a data ﬂow diagram), at the level of abstract system elements. These approaches, however, do not allow more in-depth analysis that takes into account concrete instances and conﬁgurations of these elements. This lack of expressiveness—as threats that require articulation at the level of instances cannot be expressed nor managed properly—hinders systematic risk calculation—as risks cannot be expressed and estimated in terms of instance-level properties. In this paper, we present a novel threat modeling approach that supports modeling complex systems at two distinct levels: (i) the design model deﬁnes the classes and entity types in the system, and (ii) the instance model speciﬁes concrete instances and their properties. This innovation allows systematically calculating broader risk estimates at the design level, yet also performing more reﬁned analysis in terms of more precise risk values at the instance level. Moreover, the ability to assess instance-level risks serves as an enabler for run-time continuous threat and risk (re-)assessment, and risk-adaptive security in general. We evaluate this approach in a prototype and through simulation of the dynamics of a realistic IoT-based system, a smart trafﬁc application that involves vehicles and other infrastructural elements such as smart trafﬁc lights. In these efforts, we demonstrate the practical feasibility of the approach, and we quantify the performance cost of maintaining a threat model at run-time, taking into account the time to perform risk assessment.",
        "keywords": [
            "Threat modeling",
            "risk assessment",
            "digital twin",
            "security-by-design"
        ],
        "authors": [
            "Stef Verreydt",
            "Dimitri Van Landuyt",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-024-01242-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal detection of feature interactions with logic programming and LOTOS",
        "submission-date": "2005/12",
        "publication-date": "2005/12",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Nicolas Gorse",
            "Luigi Logrippo",
            "Jacques Sincennes"
        ],
        "file_path": "data/sosym-all/s10270-005-0104-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MOOGLE: a metamodel-based model search engine",
        "submission-date": "2009/06",
        "publication-date": "2010/07",
        "abstract": "Models are becoming increasingly important in the software development process. As a consequence, the number of models being used is increasing, and so is the need for efﬁcient mechanisms to search them. Various existing search engines could be used for this purpose, but they lack features to properly search models, mainly because they are strongly focused on text-based search. This paper presents Moogle,amodelsearchenginethatusesmetamodelinginfor- mation to create richer search indexes and to allow more complex queries to be performed. The paper also presents the results of an evaluation of Moogle, which showed that the metamodel information improves the accuracy of the search.",
        "keywords": [
            "Model-driven development",
            "Model search",
            "Model reuse"
        ],
        "authors": [
            "Daniel Lucrédio",
            "Renata P. de M. Fortes",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-010-0167-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multi-objective exploration of architectural designs by composition of model transformations",
        "submission-date": "2016/03",
        "publication-date": "2017/01",
        "abstract": "Designing software architectures and optimizing them based on extra-functional properties (EFPs) require to identify appropriate design decisions and to apply them on valid architectural elements. Software designers have to check whether the resulting architecture fulﬁlls the requirements and how it positively improves (possibly con-ﬂicting) EFPs. In practice, they apply well-known solu-tions such as design patterns manually. This is time-consuming, error-prone, and possibly sub-optimal. Well-established approaches automate the search of the design space for an optimal solution. They are based model-driven engineering techniques that formalized design decisions as model transformations and architectural elements as compo-nents. Using multi-objective optimizations techniques, they explore the design space by randomly selecting a set of com-ponents and applying to them variation operators that include a ﬁxed set of predeﬁned design decisions. In this work, we claim that the design space exploration requires to reason on both architectural components as well as model transforma-tions. More speciﬁcally, we focus on possible instantiations of model transformations materialized as the application of model transformation alternatives on a set of architectural components. This approach was prototyped in RAMSES, a model transformation and code generation framework. Experimental results show the capability of our approach (i) to combine evolutionary algorithms and model transforma-tion techniques to explore efﬁciently a set of architectural alternatives with conﬂicting EFPs, (ii) to instantiate, and select transformation instances that generate architectures satisfying stringent structural constraints, and (iii) to explore design spaces by chaining more than one transformation. In particular, we evaluated our approach on EFPs, architectures, and design alternatives inspired from the railway industry by chaining model transformations dedicated to implement safety design patterns and software components allocation on a multi-processor hardware platform.",
        "keywords": [
            "Component-based software engineering",
            "Model transformations composition",
            "Design space exploration",
            "Rule-based transformation languages",
            "AADL models",
            "Extra-functional properties",
            "Multiple objectives evolutionary algorithms",
            "NSGA-II",
            "SAT solvers",
            "Linear programming"
        ],
        "authors": [
            "Smail Rahmoun",
            "Asma Mehiaoui-Hamitou",
            "Etienne Borde",
            "Laurent Pautet",
            "Elie Soubiran"
        ],
        "file_path": "data/sosym-all/s10270-017-0580-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "A process mining-based analysis of business process work-arounds",
        "submission-date": "2013/10",
        "publication-date": "2014/06",
        "abstract": "Business process work-arounds are speciﬁc\nforms of incompliant behavior, where employees inten-\ntionally decide to deviate from the required procedures\nalthough they are aware of them. Detecting and understand-\ning the work-arounds performed can guide organizations in\nredesigning and improving their processes and support sys-\ntems. Existing process mining techniques for compliance\nchecking and diagnosis of incompliant behavior rely on the\navailable information in event logs and emphasize techno-\nlogical capabilities for analyzing this information. They do\nnot distinguish intentional incompliance and do not address\nthe sources of this behavior. In contrast, the paper builds on\na list of generic types of work-arounds found in practice and\nexplores whether and how they can be detected by process\nmining techniques. Results obtained for four work-around\ntypes in ﬁve real-life processes are reported. The remaining\ntwo types are not reﬂected in events logs and cannot be cur-\nrently detected by process mining. The detected work-around\ndata are further analyzed for identifying correlations between\nthe frequency of speciﬁc work-around types and properties of\nthe processes and of speciﬁc activities. The analysis results\npromote the understanding of work-around situations and\nsources.",
        "keywords": [
            "Business process work-arounds",
            "Process mining",
            "Compliance checking"
        ],
        "authors": [
            "Nesi Outmazgin",
            "Pnina Soffer"
        ],
        "file_path": "data/sosym-all/s10270-014-0420-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic security-ﬂaw detection - towards a fair evaluation and comparison",
        "submission-date": "2024/05",
        "publication-date": "2025/05",
        "abstract": "Threat Modeling is an essential step in secure software system development. It is a (so far) manual, attacker-centric approach for identifying architecture-level security ﬂaws during the planning phase of software systems. In recent years, academia has presented ideas to automate threat detection that do not focus on a particular class of security ﬂaws but offer means of pattern-based security ﬂaw descriptions. However, comparing presented ideas (tools) for automated threat detection contains the potential for unwilling bias or restricted information content. In this work, we investigate the process of comparing automatic security ﬂaw detection tools, clarify common pitfalls during this process, and propose a fair, reproducible, and informative comparison approach to be used as a community standard. We additionally discuss the necessary steps for the community to effectively implement this approach and support improved comparisons and evaluations in the future. We use a previously published case study to determine problems with current comparison techniques and classify different levels of comparison to be used for future reference as our main contribution. As a consequence, we propose using a model-based approach for specifying security ﬂaws and apply an existing natural language-based catalogue to this model-based approach. Furthermore, we introduce an inspection process model (for providing a standard to specify ﬁndings of a threat detection process) to streamline the evaluation and comparisons of automatic security ﬂaw detection tools. We provide an exemplary evaluation of this detection guideline and inspection process model along the lines of both automatic approaches from the original case study. All artefacts of the work are publicly available to support the research community and to create a common baseline for future tool comparisons.",
        "keywords": [
            "Threat modeling",
            "Dataﬂow diagrams",
            "Security ﬂaw detection",
            "Automation",
            "Interoperability",
            "Comparison"
        ],
        "authors": [
            "Bernhard J. Berger",
            "Christina Plump"
        ],
        "file_path": "data/sosym-all/s10270-025-01300-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Six years of modeling in SoSyM",
        "submission-date": "2007/11",
        "publication-date": "2007/11",
        "abstract": "Already 6 years? Wow. Time is passing quickly. Since we began, the International Journal on Software and Systems Modeling (SoSyM) is now a well established journal with many quality papers communicating research and experience related to building and using models in the development of software-based systems. Time to rethink the form of publication and what can be improved—Feedback on the Journal as well as our considerations is welcome.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-007-0070-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An efﬁcient line-based approach for resolving merge conﬂicts in XMI-based models",
        "submission-date": "2021/07",
        "publication-date": "2022/03",
        "abstract": "Conﬂicts in software artefacts can appear during collaborative development through version control systems. When these\nconﬂicts happen in XMI models, the conﬂict sections generated by diff programs break the XMI serialisation and compromise\nthe ability to use model editors that assume well-formedness of this serialisation. While these conﬂict sections already mark\nthe conﬂicting lines of the model, current tools for conﬂict resolution in models ignore them and instead load the different\nversions of a model from the repository, over which they perform a full and costly comparison that re-identiﬁes the conﬂicts.\nWe present a novel approach that prevents this repetition of work by directly parsing XMI-based models with conﬂict sections,\nwhich allows for a targeted analysis of only the lines of the model that have been detected to be in conﬂict by the version control\nsystem. We have implemented this approach in the Peacemaker tool, which can load XMI models with conﬂict sections,\ncompute and display conﬂicts at the model level, and provide appropriate resolution actions. Compared with state-of-the-art\nmodel comparison tools with support for conﬂict resolution, Peacemaker is able to identify the vast majority of conﬂicts\nin models while reducing the required time by up to 60%. The small subset of non-identiﬁed conﬂicts does not introduce\nissues into the models, e.g. there is no loss of model information, and the resulting models after line-merging these conﬂicts\nconform to their metamodels.",
        "keywords": [
            "Model-driven engineering",
            "Version control systems",
            "Conﬂict resolution"
        ],
        "authors": [
            "Alfonso de la Vega",
            "Dimitris Kolovos"
        ],
        "file_path": "data/sosym-all/s10270-022-00976-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML-Based Approach to System Testing",
        "submission-date": "2002/02",
        "publication-date": "2002/09",
        "abstract": "System testing is concerned with testing an entire system based on its speciﬁcations. In the context of object-oriented, UML development, this means that system test requirements are derived from UML analysis artifacts such as use cases, their corresponding sequence and collaboration diagrams, class diagrams, and possibly Object Constraint Language (OCL) expressions across all these artifacts. Our goal here is to support the derivation of functional system test requirements, which will be transformed into test cases, test oracles, and test drivers once we have detailed design information. In this paper, we describe a methodology in a practical way and il- lustrate it with an example. In this context, we address testability and automation issues, as the ultimate goal is to fully support system testing activities with high- capability tools.",
        "keywords": [
            "Testing of object-oriented systems",
            "System testing",
            "UML",
            "Use Cases",
            "Sequence Diagrams",
            "Testability"
        ],
        "authors": [
            "Lionel Briand",
            "Yvan Labiche"
        ],
        "file_path": "data/sosym-all/s10270-002-0004-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic maintenance of association invariants",
        "submission-date": "2006/04",
        "publication-date": "2008/03",
        "abstract": "Many approaches to software speciﬁcation and design make use of invariants: constraints whose truth is preserved under operations on a system or component. Object modelling involves the definition of association invariants: constraints upon the sets of links corresponding to particular associations, most often concerning type, multiplicity, or symmetry. This paper shows how the definitions of operations may be extended to take account of association invariants, so that they may be properly considered when the operations are implemented. It introduces a formal, object-based modelling notation in which the process of extension and implementation, and thus the maintenance of association invariants, can be automated, making it easier to produce correct implementations of an object-oriented design.",
        "keywords": [
            "Integrity",
            "Maintenance",
            "Association",
            "Invariants",
            "Model",
            "Completion"
        ],
        "authors": [
            "James Welch",
            "David Faitelson",
            "Jim Davies"
        ],
        "file_path": "data/sosym-all/s10270-008-0085-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Fair treatment of evaluations in reviews",
        "submission-date": "2008/06",
        "publication-date": "2008/06",
        "abstract": "As scientists, we understand and appreciate the value of evaluating the results of our research effort. As software engineers, we are painfully aware of the difﬁcult challenges we must address when attempting to rigorously evaluate the methods, techniques, tools, languages, and other artifacts that we produce. The pressing problems that we tackle in the software and system modeling research domain can be classiﬁed as “wicked problems”: we learn more about the nature of the problems we tackle through experimentation with proposed solutions. Rigorous evaluation of these solutions invariably entails costly and lengthy experimentation in industrial contexts. Experiments that seek to evaluate solutions based on novel or radically different ideas are particularly difﬁcult to sell to potential industrial partners because the risks are not well-understood by all involved. Even with committed industrial partners, the wide variations in industrial development environments makes it difﬁcult (if not foolhardy) to extrapolate the results beyond the speciﬁc industries. Despite the difficulties, there is no getting away from the reality that evaluation is key to developing progressively better solutions to wicked problems. As researchers, we must evaluate the products of our research. The responsibilities of manuscript authors with respect to the evaluation content are not the focus of this editorial; there are many published high quality articles on this topic. Rather, this editorial focuses on the responsibilities of reviewers when it comes to commenting on the evaluation content of submitted journal papers.",
        "keywords": [],
        "authors": [
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-008-0096-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lightweight string reasoning in model ﬁnding",
        "submission-date": "2012/11",
        "publication-date": "2013/04",
        "abstract": "Models play a key role in assuring software quality in the model-driven approach. Precise models usually require the deﬁnition of well-formedness rules to specify constraints that cannot be expressed graphically. The Object Constraint Language (OCL) is a de-facto standard to deﬁne such rules. Techniques that check the satisﬁability of such models and ﬁnd corresponding instances of them are important in various activities, such as model-based testing and validation. Several tools for these activities have been developed, but to our knowledge, none of them supports OCL string operations on scale that is sufficient for, e.g., model-based testing. As, in contrast, many industrial models do contain such operations, there is evidently a gap. We present a lightweight solver that is specifically tailored to generate large solutions for tractable string constraints in model ﬁnding, and that is suited to directly express the main operations of the OCL datatype String. It is based on constraint logic programming (CLP) and constraint handling rules, and can be seamlessly combined with other constraint solvers in CLP. We have integrated our solver into the EMFtoCSP model ﬁnder, and we show that our implementation efficiently solves several common string constraints on large instances.",
        "keywords": [
            "Model instantiation",
            "OCL",
            "String constraints",
            "Constraint logic programming",
            "Constraint handling rules"
        ],
        "authors": [
            "Fabian Büttner",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-013-0332-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the theme issue on performance modeling",
        "submission-date": "2012/08",
        "publication-date": "Not found",
        "abstract": "The increasing complexity of modern computer systems demands new and efficient performance modeling techniques to aid developers in producing the most efficient system for a desired task. Performance evaluation seeks to understand and explain system performance. It should be an integral component of the development process from the beginning to reduce the costs required to fix problems late in the development cycle. Indeed, decisions made early in the process will directly impact the quality of the final product. The model-based development paradigm, in contrast, advocates developing systems starting from models—at multiple levels of abstraction—that express domain-specific concepts precisely and intuitively while supporting automated manipulation and transformation. The goal of this theme issue is to illuminate the deep relationship between computer systems performance evaluation and system modeling. ",
        "keywords": [],
        "authors": [
            "David J. Lilja",
            "Raffaela Mirandola"
        ],
        "file_path": "data/sosym-all/s10270-012-0269-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A formal approach to model refactoring and model refinement",
        "submission-date": "2005/02",
        "publication-date": "2006/08",
        "abstract": "Model-driven engineering is an emerging software engineering approach that relies on model transformation. Typical kinds of model transformations are model refinement and model refactoring. Whenever such a transformation is applied to a consistent model, we would like to know whether the consistency is preserved by the transformation. Therefore, in this article, we formally define and explore the relation between behaviour inheritance consistency of a refined model with respect to the original model, and behaviour preservation of a refactored model with respect to the original model. As it turns out, there is a strong similarity between these notions of behaviour consistency and behaviour preservation. To illustrate this claim, we formalised the behaviour specified by UML 2.0 sequence and protocol state machine diagrams. We show how the reasoning capabilities of description logics, a decidable fragment of first-order logic, can be used in a natural way to detect behaviour inconsistencies. These reasoning capabilities can be used in exactly the same way to detect behaviour preservation violations during model refactoring. A prototype plug-in in a UML CASE tool has been developed to validate our claims.",
        "keywords": [
            "Model-driven engineering",
            "UML 2.0",
            "Description logics",
            "Model refinement",
            "Model refactoring",
            "Behaviour preservation"
        ],
        "authors": [
            "Ragnhild Van Der Straeten",
            "Viviane Jonckers",
            "Tom Mens"
        ],
        "file_path": "data/sosym-all/s10270-006-0025-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Industry 4.0 as a Cyber-Physical System study",
        "submission-date": "2015/01",
        "publication-date": "2015/10",
        "abstract": "Advances in computation and communication are taking shape in the form of the Internet of Things, Machine-to-Machine technology, Industry 4.0, and Cyber-Physical Systems (CPS). The impact on engineering such systems is a new technical systems paradigm based on ensembles of collaborating embedded software systems. To successfully facilitate this paradigm, multiple needs can be identified along three axes: (i) online configuring an ensemble of systems, (ii) achieving a concerted function of collaborating systems, and (iii) providing the enabling infrastructure. This work focuses on the collaborative function dimension and presents a set of concrete examples of CPS challenges. The examples are illustrated based on a pick and place machine that solves a distributed version of the Towers of Hanoi puzzle. The system includes a physical environment, a wireless network, concurrent computing resources, and computational functionality such as, service arbitration, various forms of control, and processing of streaming video. The pick and place machine is of medium-size complexity. It is representative of issues occurring in industrial systems that are coming online. The entire study is provided at a computational model level, with the intent to contribute to the model-based research agenda in terms of design methods and implementation technologies necessary to make the next generation systems a reality.",
        "keywords": [
            "Cyber-Physical Systems",
            "Industry 4.0",
            "Modeling and simulation",
            "Industrial practice"
        ],
        "authors": [
            "Pieter J. Mosterman",
            "Justyna Zander"
        ],
        "file_path": "data/sosym-all/s10270-015-0493-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Growing the UML",
        "submission-date": "2002/12",
        "publication-date": "2002/12",
        "abstract": "The entire history of software engineering can be characterized as an increase in levels of abstraction. This paper discusses the growing complexity of software and its impact on cost, and explores technologies that are on the watch list for the next 3-5 years.",
        "keywords": [],
        "authors": [
            "Grady Booch"
        ],
        "file_path": "data/sosym-all/s10270-002-0013-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Genericity for model management operations",
        "submission-date": "2010/11",
        "publication-date": "2011/06",
        "abstract": "Models are the core assets in model-driven engineering, and are therefore subject to all kind of manipulations, such as refactorings, animations, transformations into other languages, comparisons and merging. This set of model-related activities is known as model management. Even though many languages and approaches have been proposed for model management, most of them are type-centric, specific to concrete meta-models, and hence leading to specifications with a low level of abstraction and difficult to be reused in practice. In this paper, we introduce ideas from generic programming into model management to raise the level of abstraction of the specifications of model manipulations and facilitate their reuse. In particular we adopt generic meta-model concepts as an intermediate, abstract meta-model over which model management specifications are defined. Such meta-model concepts are mapped to concrete meta-models, so that specifications can be applied to families of meta-models satisfying the concept requirements. As a proof of concept, we show the implementation of these ideas using the Eclipse Modeling Framework and the Epsilon family of languages for model management.",
        "keywords": [
            "Model management",
            "Genericity",
            "Reusability",
            "Epsilon",
            "Eclipse Modelling Framework"
        ],
        "authors": [
            "Louis Rose",
            "Esther Guerra",
            "Juan de Lara",
            "Anne Etien",
            "Dimitris Kolovos",
            "Richard Paige"
        ],
        "file_path": "data/sosym-all/s10270-011-0203-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon"
        }
    },
    {
        "title": "Formalizing and verifying stochastic system architectures using Monterey Phoenix",
        "submission-date": "2013/10",
        "publication-date": "2014/04",
        "abstract": "The analysis of software architecture plays an important role in understanding the system structures and facilitate proper implementation of user requirements. Despite its importance in the software engineering practice, the lack of formal description and veriﬁcation support in this domain hinders the development of quality architectural models. To tackle this problem, in this work, we develop an approach for modeling and verifying software architectures speciﬁed using Monterey Phoenix (MP) architecture description language. MP is capable of modeling system and environment behaviors based on event traces, as well as supporting different architecture composition operations and views. First, we formalize the syntax and operational semantics for MP; therefore, formal veriﬁcation of MP models is feasible. Second, we extend MP to support shared variables and stochastic characteristics, which not only increases the expressiveness of MP, but also widens the properties MP can check, such as quantitative requirements. Third, a dedicated model checker for MP has been implemented, so that automatic veriﬁcation of MP models is supported. Finally, several experiments are conducted to evaluate the applicability and efﬁciency of our approach",
        "keywords": [
            "Model checking",
            "Stochastic system architecture",
            "Monterey Phoenix"
        ],
        "authors": [
            "Songzheng Song",
            "Jiexin Zhang",
            "Yang Liu",
            "Mikhail Auguston",
            "Jun Sun",
            "Jin Song Dong",
            "Tieming Chen"
        ],
        "file_path": "data/sosym-all/s10270-014-0411-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An enterprise architecture framework for multi-attribute information systems analysis",
        "submission-date": "2011/09",
        "publication-date": "2012/11",
        "abstract": "Enterprise architecture is a model-based IT and business management discipline. Enterprise architecture analysis concerns using enterprise architecture models for analysis of selected properties to provide decision support. This paper presents a framework based on the ArchiMate metamodel for the assessment of four properties, viz., application usage, system availability, service response time and data accuracy. The framework integrates four existing meta-models into one and implements these in a tool for enterprise architecture analysis. The paper presents the overall metamodel and four viewpoints, one for each property. The underlying theory and formalization of the four viewpoints is presented. In addition to the tool implementation, a running example as well as guidelines for usage makes the viewpoints easily applicable.",
        "keywords": [
            "Enterprise architecture",
            "Enterprise architecture analysis",
            "Enterprise architecture tool",
            "Data accuracy",
            "Technology usage",
            "Service availability",
            "Service response time"
        ],
        "authors": [
            "Per Närman",
            "Markus Buschle",
            "Mathias Ekstedt"
        ],
        "file_path": "data/sosym-all/s10270-012-0288-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Measuring and achieving test coverage of attack simulations extended version",
        "submission-date": "2021/10",
        "publication-date": "2022/09",
        "abstract": "Designing secure and reliable systems is a difﬁcult task. Threat modeling is a process that supports the secure design of systems by easing the understanding of the system’s complexity, as well as identifying and modeling potential threats. These threat models can serve as input for attack simulations, which are used to analyze the behavior of attackers within the system. To ensure the correct functionality of these attack simulations, automated tests are designed that check if an attacker can reach a certain point in the threat model. Currently, there is no way for developers to estimate the degree to which their tests cover the attack simulations and, thus, they cannot determine the quality of their tests. To resolve this shortcoming, we analyze structural testing methods from the software engineering domain and transfer them to the threat modeling domain by following an action design research approach. Further, we develop a ﬁrst prototype, which is able to assess the test coverage in an automated way and provide a first approach to achieve full coverage. This will enable threat modeler to determine the quality of their tests and, simultaneously, increase the quality of the threat models.",
        "keywords": [
            "Threat modeling",
            "Attack simulations",
            "Testing",
            "Test coverage"
        ],
        "authors": [
            "Simon Hacks",
            "Linus Persson",
            "Nicklas Hersén"
        ],
        "file_path": "data/sosym-all/s10270-022-01042-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML 2002",
        "submission-date": "2002/00",
        "publication-date": "2003/09",
        "abstract": "This paper provides an overview of the UML 2002 conference, its submissions, accepted papers, and the topics covered. It highlights the conference as a leading forum for researchers and practitioners in UML and model-driven engineering, and discusses the evolution of UML towards better integration with other OMG technologies.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-003-0030-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Use cases – Yesterday, today, and tomorrow",
        "submission-date": "2004/07",
        "publication-date": "2004/07",
        "abstract": "To my knowledge, no other software engineering language construct as signiﬁcant as use cases has been adopted so quickly and so widely among practitioners. I believe this is because use cases play a role in so many diﬀerent aspects of software engineering. Although I ﬁrst used the term in 1986, I had actually been working on and evolving the concept of use cases since 1967. So many people have asked me how I came up with this concept that I decided to write this article to explain the origins and evolution of use cases. I’ll also summarize what they have helped us achieve so far, and then suggest a few improvements for the future.",
        "keywords": [
            "Use case",
            "Use case driven development",
            "History of use cases",
            "Extension use cases",
            "Inclusion use cases",
            "Roles of use cases",
            "Use cases are early aspects",
            "Use case fragments"
        ],
        "authors": [
            "Ivar Jacobson"
        ],
        "file_path": "data/sosym-all/s10270-004-0060-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal reconﬁguration model for cloud resources",
        "submission-date": "2021/01",
        "publication-date": "2022/03",
        "abstract": "The execution context of the cloud composite services is dynamically and rapidly changing. In the cloud environment, the service demands can increase/decrease in a restrained time interval. Due to this fact, cloud composite services have to evolve continuously by scaling up/down their capacity to handle new demands. Scaling up consists in making a component larger or faster to handle a greater load. Scaling down is the reverse of Scaling up and it is the situation of reducing component capacity when the load decreases. Dynamic adaptation mechanisms must be in place to take into account the evolution of the execution context and environment. In this paper, we propose a new Event-B formal model to manage the dynamic reconﬁguration of composite services in the cloud context. The proposed approach sets up the required reconﬁguration mechanisms and takes into account the coordination between the different cloud computing levels: Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). The proposed model contains four abstraction levels and implements the scaling mechanisms at each abstraction level. The model consistency has been proved thanks to the Event-B dedicated tools.",
        "keywords": [
            "Cloud environment",
            "Dynamic reconﬁguration",
            "Elasticity coordination",
            "Formal approach",
            "Event-B"
        ],
        "authors": [
            "Aida Lahouij",
            "Lazhar Hamel",
            "Mohamed Graiet"
        ],
        "file_path": "data/sosym-all/s10270-022-00990-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Case-based exploration of bidirectional transformations in QVT Relations",
        "submission-date": "2015/07",
        "publication-date": "2016/05",
        "abstract": "QVT Relations (QVT-R), a standard issued by the Object Management Group, is a language for the declarative speciﬁcation of model transformations. This paper focuses on a particularly interesting feature of QVT-R: the declarative speciﬁcation of bidirectional transformations. Rather than writing two unidirectional transformations separately, a transformation developer may provide a single relational speciﬁcation which may be executed in both directions. This approach saves speciﬁcation effort and ensures the consistency of forward and backward transformations. This paper explores QVT-R’s support for bidirectional model transformations through a spectrum of transformation cases. The transformation cases vary with respect to several factors such as the size of the transformation deﬁnition or the relationships between the metamodels for source and target models. The cases are solved in QVT-R, but may be applied to other bidirectional transformation languages, as well; thus, they may be used as a benchmark for comparing bidirectional transformation languages. In our work, we focus on the following research questions: functionality of bidirectional transformations in terms of relations between source and target models, solvability (which problems may be solved by a single relational speciﬁcation of a bidirectional transformation), variability (does a bidirectional transformation contain varying elements, i.e., elements being speciﬁc to one direction), comprehensibility (referring to the ease of understanding and constructing QVT-R transformations), and the semantic soundness of bidirectional transformations written in QVT-R.",
        "keywords": [
            "Model-driven software engineering",
            "Bidirectional model transformations",
            "QVT Relations"
        ],
        "authors": [
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-016-0527-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT Relations"
        }
    },
    {
        "title": "The many meanings of UML 2 Sequence Diagrams: a survey",
        "submission-date": "2009/07",
        "publication-date": "2010/04",
        "abstract": "Scenario languages are widely used in software development. Typical usage scenarios, forbidden behaviors, test cases, and many more aspects can be depicted with graphical scenarios. Scenario languages were introduced into the Uniﬁed Modeling Language (UML) under the name of Sequence Diagrams. The 2.0 version of UML changed Sequence Diagrams significantly and the expressiveness of the language was highly increased. However, the complexity of the language (and the diversity of the goals Sequence Diagrams are used for) yields several possible choices in its semantics. This paper collects and categorizes the semantic choices in the language, surveys the formal semantics proposed for Sequence Diagrams, and presents how these approaches handle the various semantic choices.",
        "keywords": [
            "UML",
            "Sequence diagrams",
            "Semantics"
        ],
        "authors": [
            "Zoltán Micskei",
            "Hélène Waeselynck"
        ],
        "file_path": "data/sosym-all/s10270-010-0157-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Least-change bidirectional model transformation with QVT-R and ATL",
        "submission-date": "2013/09",
        "publication-date": "2014/11",
        "abstract": "QVT Relations (QVT-R) is the standard lan-guage proposed by the OMG to specify bidirectional model transformations. Unfortunately, in part due to ambigui-ties and omissions in the original semantics, acceptance and development of effective tool support have been slow. Recently, the checking semantics of QVT-R has been clar-iﬁed and formalized. In this article, we propose a QVT-R tool that complies to such semantics. Unlike any other exist-ing tool, it also supports meta-models enriched with OCL constraints (thus avoiding returning ill-formed models) and proposes an alternative enforcement semantics that works according to the simple and predictable “principle of least change.” The implementation is based on an embedding of bothQVT-RtransformationsandUMLclassdiagrams(annotated with OCL) in Alloy, a lightweight formal speciﬁcation language with support for automatic model ﬁnding via SAT solving. We also show how this technique can be applied to bidirectionalize ATL, a popular (but unidirectional) model transformation language.",
        "keywords": [
            "Model transformation",
            "Bidirectional transformation",
            "Least-change principle",
            "QVT-R",
            "ATL",
            "Alloy"
        ],
        "authors": [
            "Nuno Macedo",
            "Alcino Cunha"
        ],
        "file_path": "data/sosym-all/s10270-014-0437-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-R, ATL"
        }
    },
    {
        "title": "Event-B patterns and their tool support",
        "submission-date": "2010/06",
        "publication-date": "2011/01",
        "abstract": "Event-B has given developers the opportunity\nto construct models of complex systems that are correct-\nby-construction. However, there is no systematic approach,\nespecially in terms of reuse, which could help with the con-\nstruction of these models. We introduce the notion of design\npatterns within the framework of Event-B to shorten this\ngap. Our approach preserves the correctness of the models,\nwhich is critical in formal methods and also reduces the prov-\ning effort. Within our approach, an Event-B design pattern is\njust another model devoted to the formalisation of a typical\nsub-problem. As a result, we can use patterns to construct a\nmodel which can subsequently be used as a pattern to con-\nstruct a larger model. We also present the interaction between\ndevelopers and the tool support within the associated RODIN\nPlatform of Event-B. The approach has been applied success-\nfully to some medium-size industrial case studies.",
        "keywords": [
            "Event-B",
            "Formal methods",
            "Design patterns",
            "Formal modelling",
            "Model reuse"
        ],
        "authors": [
            "Thai Son Hoang",
            "Andreas Fürst",
            "Jean-Raymond Abrial"
        ],
        "file_path": "data/sosym-all/s10270-010-0183-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EMF-Syncer: scalable maintenance of view models over heterogeneous data-centric software systems at run time",
        "submission-date": "2022/05",
        "publication-date": "2023/06",
        "abstract": "Withtheincreasingpresenceofcyber-physicalsystems(CPSs),likeautonomousvehiclesystemsanddigitaltwins,thefutureof\nsoftware engineering is predicated on the importance of designing and developing data-centric software systems that can adapt\nintelligently at run time. CPSs consist of complex heterogeneous software components. Model-driven engineering advocates\nusing software models to tame such complexity, capturing the relevant design concerns of such systems at different levels of\nabstraction. Yet most of the existing CPSs are engineered without considering MDE practices and tools, facing fundamental\nchallenges when working with data: monitoring the program data at run time, syncing updates between program and model,\ndealing with heterogeneous data sources, and representing such observed data at run time to facilitate automated analysis. In\nthis work, we introduce the notion of view models to explicitly represent parts of the domain knowledge implicitly embedded\nin the source code of a software system. This notion is equipped with a scalable bidirectional syncing mechanism that extracts\nview model instances from program snapshots at run time. The syncing mechanism is proposed from a conceptual point of\nview, independently of speciﬁc implementations and supports incremental view model update and view model maintenance.\nWe show how this syncing mechanism is ﬂexible enough to facilitate the non-intrusive adoption of MDE technology over\nexisting MDE-agnostic heterogeneous data-centric systems. We study the run-time cost implied by the EMF- Syncer ,\nthe tool implementing this syncing mechanism for Java applications and view models atop the eclipse modeling framework\n(EMF) when executing data analytic and transformation tasks over large volumes of data in the presence of data updates at run\ntime. An empirical evaluation of the EMF- Syncer has been conducted with an industry-targeted benchmark for decision\nsupport systems, analyzing performance and scalability. The novel syncing mechanism enables new opportunities to adopt\nMDE technology in heterogeneous data-centric systems.",
        "keywords": [
            "Model-driven engineering",
            "Models@runtime",
            "Roundtrip synchronization",
            "View update problem"
        ],
        "authors": [
            "Artur Boronat"
        ],
        "file_path": "data/sosym-all/s10270-023-01111-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lazy model checking for recursive state machines",
        "submission-date": "2022/06",
        "publication-date": "2024/03",
        "abstract": "Recursive state machines (RSMs) are state-based models for procedural programs with wide-ranging applications in program veriﬁcation and interprocedural analysis. Model-checking algorithms for RSMs and related formalisms have been intensively studied in the literature. In this article, we devise a new model-checking algorithm for RSMs and requirements in computation tree logic (CTL) that exploits the compositional structure of RSMs by ternary model checking in combination with a lazy evaluation scheme. Speciﬁcally, a procedural component is only analyzed in those cases in which it might inﬂuence the satisfaction of the CTL requirement. We implemented our model-checking algorithms and evaluate them on randomized scalability benchmarks and on an interprocedural data-ﬂow analysis of Java programs, showing both practical applicability and signiﬁcant speedups in comparison to state-of-the-art model-checking tools for procedural programs.",
        "keywords": [
            "Model checking",
            "Lazy veriﬁcation",
            "Interprocedural static analysis",
            "Recursive state machines",
            "Computation tree logic"
        ],
        "authors": [
            "Clemens Dubslaff",
            "Patrick Wienhöft",
            "Ansgar Fehnker"
        ],
        "file_path": "data/sosym-all/s10270-024-01159-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Report on the State of the SoSyM Journal end of 2022",
        "submission-date": "2023/02",
        "publication-date": "2023/02",
        "abstract": "SoSyM continued to expand in many ways over the past\nyear. Modeling remains a challenging topic, in terms of both\nresearch investigations and practical applications. While a\ngrowing number of industrial projects are collecting impor-\ntant experiences about the application of models in various\nforms, they are also identifying new and challenging ques-\ntions that need to be addressed and solved. This holds for\nmodels applied to traditional software development, models\nthat assist in the design of cyber physical systems, and models\nthat offer support for systems modeling, in general. A com-\npletely new chapter has been opened with the use of models\nnot only for system development, but also for the investiga-\ntion of their accompanying digital twins, as addressed in a\nrecent editorial.",
        "keywords": [],
        "authors": [
            "Stéphanie Challita",
            "Benoit Combemale",
            "Huseyin Ergin",
            "JeﬀGray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-023-01085-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving the accuracy of UML metamodel extensions by introducing induced associations",
        "submission-date": "2006/07",
        "publication-date": "2007/07",
        "abstract": "In the process of extending the UML metamodel for a speciﬁc domain, the metamodel speciﬁer introduces frequently some metaassociations at MOF level M2 with the aim that they induce some speciﬁc associations at MOF level M1. For instance, if a metamodel for software process modelling states that a “Role” is responsible for an “Arti-fact”, we can interpret that its speciﬁer intended to model two aspects: (1) the implications of this metaassociation at level M1 (e.g., the speciﬁc instance of Role “TestEngineer” is responsible for the speciﬁc instance of Artifact “TestPlans”); and (2) the implications of this metaassociation at level M0 (e.g., “John Doe” is the responsible test engineer for elaborating the test plans for the package “Foo”). Unfortunately, the second aspect is often not enforced by the metamodel and, as a result, the models which are deﬁned as its instances may not incorporate it. This problem, consequence of the so-called “shallow instantiation” in Atkinson and Kühne (Procs.UML’01,LNCS2185,Springer,2001),preventsthese models from being accurate enough in the sense that they do not express all the information intended by the metamodel speciﬁer and consequently do not distinguish metaassocia-tions that induce associations at M1 from those that do not. In this article we introduce the concept of induced association that may come up when an extension of the UML metamodel is developed. The implications that this concept has both in the extended metamodel and in its instances are discussed. We also present a methodology to enforce that M1 models incorporate the associations induced by the metamodel which they are instances from. Next, as an example of application we present a quality metamodel for software artifacts which makes intensive use of induced associations. Finally, we introduce a software tool to assist the development of quality models as correct instantiations of the metamodel, assuring the proper application of the induced associations as required by the metamodel.",
        "keywords": [
            "Metamodelling",
            "MOF",
            "Shallow instantiation",
            "UML extension",
            "Software quality",
            "Metaassociations"
        ],
        "authors": [
            "Xavier Burgués",
            "Xavier Franch",
            "Josep M. Ribó"
        ],
        "file_path": "data/sosym-all/s10270-007-0062-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Transactional execution of hierarchical reconﬁgurations in cyber-physical systems",
        "submission-date": "2016/03",
        "publication-date": "2017/02",
        "abstract": "Cyber-physical systems reconﬁgure the structure of their software architecture, e.g., to avoid hazardous situations and to optimize operational conditions like their energy consumption. These reconﬁgurations have to be safe so that the systems protect their users or environment against harm-ful conditions or events while changing their structure. As software architectures are typically built on components, reconﬁguration actions need to take into account the component structure. This structure should support vertical composition to enable hierarchically encapsulated components. While many reconﬁguration approaches for cyber-physical and embedded real-time systems allow the use of hierarchi-cally embedded components, i.e., vertical composition, none of them offers a modeling and veriﬁcation solution to take hierarchical composition, i.e., encapsulation, into account thus limiting reuse and compositional veriﬁcation. In this paper, we present an extension to our existing modeling language, MechatronicUML, to enable safe hierarchical reconﬁgurations. The three extensions are (a) an adapted variant of the 2-phase-commit protocol to initiate recon-ﬁgurations that maintain component encapsulation, (b) the integration of feedback controllers during reconﬁguration, and(c)averiﬁcationapproachbasedon(timed)modelcheck-ing for instances of our model. We illustrate our approach on a case study in the area of smart railway systems by show-ing two different use cases of our approach. We show that using our approach the systems can be easily designed to reconﬁgure safely.",
        "keywords": [
            "CPS",
            "Safe reconﬁguration",
            "Correctness-by-construction",
            "Runtime reconﬁguration",
            "Component model",
            "Reconﬁguration behavior",
            "Feedback controller exchange",
            "Transactions",
            "Atomicity",
            "Consistency",
            "Isolation",
            "Timed model checking"
        ],
        "authors": [
            "Christian Heinzemann",
            "Steffen Becker",
            "Andreas Volk"
        ],
        "file_path": "data/sosym-all/s10270-017-0583-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A data-driven approach for constructing multilayer network-based service ecosystem models",
        "submission-date": "2021/09",
        "publication-date": "2022/08",
        "abstract": "Services are ﬂourishing drastically both on the Internet and in the real world. In addition, services have become much more interconnected to facilitate transboundary business collaboration to create and deliver distinct new values to customers. Various service ecosystems come into being and are increasingly becoming a focus in both research and practice. However, due to the lack of widely recognized service ecosystem models and sufﬁcient real data for constructing such models, existing studies on service ecosystems are limited to a very narrow scope and cannot effectively guide the design, optimization, and evolution of service ecosystems. In this paper, we ﬁrst propose a multilayer network-based service ecosystem model (MSEM), which covers a variety of service-related elements, including stakeholders, channels, functional and nonfunctional features, and domains, and more importantly, structural and evolutionary relations between them. “Events” are introduced to describe the triggers of service ecosystem evolution. Then, we propose a data-driven approach for constructing MSEM from public media news and external data sources. Experiments conducted on real news corpora show that compared with other approaches, our approach can construct large-scale models for real-world service ecosystems with lower cost and higher efﬁciency.",
        "keywords": [
            "Service ecosystem",
            "Multilayer knowledge graph",
            "Service-related event",
            "Event mining",
            "Model construction",
            "Evolution"
        ],
        "authors": [
            "Mingyi Liu",
            "Zhiying Tu",
            "Xiaofei Xu",
            "Zhongjie Wang",
            "Yan Wang"
        ],
        "file_path": "data/sosym-all/s10270-022-01029-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An intermediate metamodel with scenarios and resources for generating performance models from UML designs",
        "submission-date": "2005/02",
        "publication-date": "2006/08",
        "abstract": "Performance analysis of a software speciﬁ- cation in a language such as UML can assist a design team in evaluating performance-sensitive design deci- sions and in making design trade-offs that involve per- formance. Annotations to the design based on the UML Proﬁle for Schedulability, Performance and Time pro- vide necessary information such as workload parame- ters for a performance model, and many different kinds of performance techniques can be applied. The Core Scenario Model (CSM) described here provides a meta- model for an intermediate form which correlates mul- tiple UML diagrams, extracts the behaviour elements with the performance annotations, attaches important resource information that is obtained from the UML, and supports the creation of many different kinds of per- formance models. Models can be made using queueing networks, layered queues, timed Petri nets, and it is proposed to develop the CSM as an intermediate language for all performance formalisms. This paper deﬁnes the CSM and describes how it resolves questions that arise in performance model-building.",
        "keywords": [
            "Uniﬁed modeling language",
            "Performance evaluation",
            "Scenarios",
            "Model transformations"
        ],
        "authors": [
            "Dorin B. Petriu",
            "Murray Woodside"
        ],
        "file_path": "data/sosym-all/s10270-006-0026-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Designing secure business processes with SecBPMN",
        "submission-date": "2014/10",
        "publication-date": "2015/10",
        "abstract": "Modern information systems are increasingly large and consist of an interplay of technical components and social actors (humans and organizations). Such interplay threatens the security of the overall system and calls for veriﬁcation techniques that enable determining compliance with security policies. Existing veriﬁcation frameworks either have a limited expressiveness that inhibits the speciﬁcationofreal-worldrequirementsorrelyonformallanguages that are difﬁcult to use for most analysts. In this paper, we overcome the limitations of existing approaches by presenting the SecBPMN framework. Our proposal includes: (1) the SecBPMN-ml modeling language, a security-oriented extension of BPMN for specifying composite information systems; (2) the SecBPMN-Q query language for representing security policies; and (3) a query engine that enables checking SecBPMN-Q policies against SecBPMN-ml speciﬁcations. We evaluate our approach by studying its understandability and perceived complexity with experts, running scalability analysis of the query engine, and through an application to a large case study concerning air trafﬁc management.",
        "keywords": [
            "Information systems",
            "Security policies",
            "BPMN",
            "Compliance"
        ],
        "authors": [
            "Mattia Salnitri",
            "Fabiano Dalpiaz",
            "Paolo Giorgini"
        ],
        "file_path": "data/sosym-all/s10270-015-0499-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A survey of traceability in requirements engineering and model-driven development",
        "submission-date": "2009/01",
        "publication-date": "2009/12",
        "abstract": "Traceability—the ability to follow the life of software artifacts—is a topic of great interest to software developers in general, and to requirements engineers and model-driven developers in particular. This article aims to bring those stakeholders together by providing an overview of the current state of traceability research and practice in both areas. As part of an extensive literature survey, we identify commonalities and differences in these areas and uncover several unresolved challenges which affect both domains. A good common foundation for further advances regarding these challenges appears to be a combination of the formal basis and the automated recording opportunities of MDD on the one hand, and the more holistic view of traceability in the requirements engineering domain on the other hand.",
        "keywords": [
            "Requirements engineering",
            "Model-driven engineering",
            "Model-driven development",
            "Traceability"
        ],
        "authors": [
            "Stefan Winkler",
            "Jens von Pilgrim"
        ],
        "file_path": "data/sosym-all/s10270-009-0145-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Typing artifacts in megamodeling",
        "submission-date": "2009/11",
        "publication-date": "2011/02",
        "abstract": "Model management is essential for coping with the complexity introduced by the increasing number and varied nature of artifacts involved in model-driven engineering-based projects. Global model management (GMM) addresses this issue by enabling the representation of artifacts, particularly transformation composition and execution, within a model called a megamodel. Type information about artifacts can be used for preventing type errors during execution. Built on our previous work, in this paper we present the core elements of a type system for GMM that improves its original typing approach and enables both typechecking and type inference on artifacts within a megamodel. This type system is able to deal with non-trivial situations such as the use of higher order transformations. We also present a prototypical implementation of such a type system.",
        "keywords": [
            "Model transformation",
            "Type system",
            "Megamodeling"
        ],
        "authors": [
            "Andrés Vignaga",
            "Frédéric Jouault",
            "María Cecilia Bastarrica",
            "Hugo Brunelière"
        ],
        "file_path": "data/sosym-all/s10270-011-0191-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assessing event correlation in non-process-aware information systems",
        "submission-date": "2011/10",
        "publication-date": "2012/09",
        "abstract": "Many present-day companies carry out a huge amount of daily operations through the use of their information systems without ever having done their own enterprise modeling. Business process mining is a well-proven solution which is used to discover the underlying business process models that are supported by existing information systems. Business process discovery techniques employ event logs as input, which are recorded by process-aware information systems. However, a wide variety of traditional information systems do not have any in-built mechanisms with which to collect events (representing the execution of business activities). Various mechanisms with which to collect events from non-process-aware information systems have been proposed in order to enable the application of process mining techniques to traditional information systems. Unfortunately, since business processes supported by traditional information systems are implicitly defined, correlating events into the appropriate process instance is not trivial. This challenge is known as the event correlation problem. This paper presents an adaptation of an existing event correlation algorithm and incorporates it into a technique in order to collect event logs from the execution of traditional information systems. The technique first instruments the source code to collect events together with some candidate correlation attributes. Based on several well-known design patterns, the technique provides a set of guidelines to support experts when instrumenting the source code. The event correlation algorithm is subsequently applied to the data set of events to discover the best correlation conditions, which are then used to create event logs. The technique has been semi-automated to facilitate its validation through an industrial case study involving a writer management system and a healthcare evaluation system. The study demonstrates that the technique is able to discover an appropriate correlation set and obtain well-formed event logs, thus enabling business process mining techniques to be applied to traditional information systems.",
        "keywords": [
            "Business process mining",
            "Event correlation",
            "Event model",
            "Case study"
        ],
        "authors": [
            "Ricardo Pérez-Castillo",
            "Barbara Weber",
            "Ignacio García-Rodríguez de Guzmán",
            "Mario Piattini",
            "Jakob Pinggera"
        ],
        "file_path": "data/sosym-all/s10270-012-0285-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The evolution of modeling research challenges",
        "submission-date": "2013/05",
        "publication-date": "2013/05",
        "abstract": "In 2007 ICSE hosted a track called “Future of Software Development” (FOSD). We were invited to write and present a paper on the future of modeling for the track. The result-ing paper [1] described the state of modeling research, iden-tiﬁed some major challenges and proposed a research road map. Parts of this road map are currently being explored, and progress has been made in addressing some of the challenges we identiﬁed. However, there is still signiﬁcant research “to be done” with respect to the challenges outlined in that paper. It is not our intent to discuss the progress the community has made with respect to the road map in this editorial (our apolo-gies for deﬂating expectations in this regard; an editorial is simply not the place for such discussions). Rather, we would like to use this editorial to stimulate discussions around some of the challenges that have arisen since we wrote that paper.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-013-0346-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Artefacts in software engineering: a fundamental positioning",
        "submission-date": "2018/05",
        "publication-date": "2019/01",
        "abstract": "Artefacts play a vital role in software and systems development processes. Other terms like documents, deliverables, or work products are widely used in software development communities instead of the term artefact. In the following, we use the term ‘artefact’ including all these other terms. Despite its relevance, the exact denotation of the term ‘artefact’ is still not clear due to a variety of different understandings of the term and to a careless negligent usage. This often leads to approaches being grounded in a fuzzy, unclear understanding of the essential concepts involved. In fact, there does not exist a common terminology. Therefore, it is our goal that the term artefact be standardised so that researchers and practitioners have a common understanding for discussions and contributions. In this position paper, we provide a positioning and critical reﬂection upon the notion of artefacts in software engineering at different levels of perception and how these relate to each other. We further contribute a metamodel that provides a description of an artefact that is independent from any underlying process model. This metamodel deﬁnes artefacts at three levels. Abstraction and reﬁnement relations between these levels allow correlating artefacts to each other and deﬁning the notion of related, reﬁned, and equivalent artefacts. Our contribution shall foster the long overdue and too often underestimated terminological discussion on what artefacts are to provide a common ground with clearer concepts and principles for future software engineering contributions, such as the design of artefact-oriented development processes and tools.",
        "keywords": [
            "Software engineering artefacts",
            "Metamodelling",
            "Propaedeutics",
            "Syntax of artefacts",
            "Semantics of artefacts",
            "Equivalence of artefacts"
        ],
        "authors": [
            "Daniel Méndez Fernández",
            "Wolfgang Böhm",
            "Andreas Vogelsang",
            "Jakob Mund",
            "Manfred Broy",
            "Marco Kuhrmann",
            "Thorsten Weyer"
        ],
        "file_path": "data/sosym-all/s10270-019-00714-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From well-formedness to meaning preservation: model refactoring for almost free",
        "submission-date": "2012/04",
        "publication-date": "2013/04",
        "abstract": "Modelling languages such as the UML specify well-formedness as constraints on models. For the refactoring of a model to be correct, it must take these constraints into account and check that they are still satisﬁed after the refactoring has been performed—if not, execution of the refactoring must be refused. By replacing constraint checking with constraint solving, we show how the role of constraints can be lifted from permitting or denying a tentative refactoring to computing additional model changes required for the refactoring to be executable. Thus, to the degree that the semantics of a modelling language is speciﬁed using constraints, refactorings based on these constraints are guaranteed to be meaning preserving. To be able to exploit constraints available in the form of a language’s well-formedness rules for refactoring, we present a mapping from these rules to the constraint rules required by constraint-based refactoring. Where there are no gaps between well-formedness and (static) semantics of a modelling language, these mappings enable structural refactorings of models at no extra cost; where there are, we identify ways of detecting and ﬁlling the gaps.",
        "keywords": [
            "Model refactoring",
            "Constraints"
        ],
        "authors": [
            "Friedrich Steimann"
        ],
        "file_path": "data/sosym-all/s10270-013-0314-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the assessment of generative AI in modeling tasks: an experience report with ChatGPT and UML",
        "submission-date": "2023/03",
        "publication-date": "2023/05",
        "abstract": "Most experts agree that large language models (LLMs), such as those used by Copilot and ChatGPT, are expected to revo-lutionize the way in which software is developed. Many papers are currently devoted to analyzing the potential advantages and limitations of these generative AI models for writing code. However, the analysis of the current state of LLMs with respect to software modeling has received little attention. In this paper, we investigate the current capabilities of ChatGPT to perform modeling tasks and to assist modelers, while also trying to identify its main shortcomings. Our ﬁndings show that, in contrast to code generation, the performance of the current version of ChatGPT for software modeling is limited, with various syntactic and semantic deﬁciencies, lack of consistency in responses and scalability issues. We also outline our views on how we perceive the role that LLMs can play in the software modeling discipline in the short term, and how the modeling community can help to improve the current capabilities of ChatGPT and the coming LLMs for software modeling.",
        "keywords": [
            "Large language models",
            "ChatGPT",
            "Software models",
            "Modeling languages",
            "UML"
        ],
        "authors": [
            "Javier Cámara",
            "Javier Troya",
            "Lola Burgueño",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-023-01105-5.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Example-driven meta-model development",
        "submission-date": "2013/06",
        "publication-date": "2013/12",
        "abstract": "The intensive use of models in model-driven engineering (MDE) raises the need to develop meta-models with different aims, such as the construction of textual and visual modelling languages and the speciﬁcation of source and target ends of model-to-model transformations. While domain experts have the knowledge about the concepts of the domain, they usually lack the skills to build meta-models. Moreover, meta-models typically need to be tailored according to their future usage and speciﬁc implementation platform, which demands knowledge available only to engineers with great expertise in speciﬁc MDE platforms. These issues hinder a wider adoption of MDE both by domain experts and software engineers. In order to alleviate this situation, we propose an interactive, iterative approach to meta-model construction, enabling the speciﬁcation of example model fragments by domain experts, with the possibility of using informal drawing tools like Dia or yED. These fragments can be annotated with hints about the intention or needs for certain elements. A meta-model is then automatically induced, which can be refactored in an interactive way, and then compiled into an implementation meta-model using proﬁles and patterns for different platforms and purposes. Our approach includes the use of a virtual assistant, which provides suggestions for improving the meta-model based on well-known refactorings, and a validation mode, enabling the validation of the meta-model by means of examples.",
        "keywords": [
            "Meta-modelling",
            "Domain-speciﬁc modelling languages",
            "Interactive meta-modelling",
            "Meta-model induction",
            "Example-driven modelling",
            "Meta-model design exploration",
            "Meta-model validation"
        ],
        "authors": [
            "Jesús J. López-Fernández",
            "Jesús Sánchez Cuadrado",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-013-0392-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Playground for multi-level modeling constructs",
        "submission-date": "2020/05",
        "publication-date": "2021/08",
        "abstract": "In recent years, multi-level modeling has become more and more popular. It is mainly due to the fact that multi-level modeling aims to reduce or even totally eliminate any accidental complexity inadvertently created as by-product in traditional model design. Moreover, besides reducing model complexity, multi-level modeling also improves on general comprehension of models. The key enablers of multi-level modeling are the concepts of clabjects and deep instantiation. The latter is often governed by the potency notion, of which many different interpretations and variations emerged over the years. However, there exist also some approaches that disregard the potency notion. Thus, multi-level modeling approaches tend to take advantage of different theoretical and practical backgrounds. In this paper, we propose a unifying framework, the Multi-Level Modeling Playground (MLMP), which is a validating modeling environment for multi-level modeling research. The MLMP environment is based on our multi-layer modeling framework (the Dynamic Multi-Layer Algebra), which provides useful mechanisms to validate different multi-level modeling constructs. Since beyond the structure also the well-formedness rules of the modeling constructs can be speciﬁed, our proposed MLMP environment delivers several practical beneﬁts: i) well-formedness is always veriﬁed, ii) multi-level constructs can be experimented with independently of any concrete tool chains, and iii) relationships (i.e., correlations or exclusions) between different multi-level constructs can be easily investigated in practice. Also, the capability of the environment is demonstrated via complete examples inspired by state-of-the-art research literature.",
        "keywords": [
            "Multi-level modeling",
            "Potency notion",
            "Clabject",
            "Level-blind",
            "Scientiﬁc experimentation",
            "Modular playground"
        ],
        "authors": [
            "Ferenc A. Somogyi",
            "Gergely Mezei",
            "Zoltán Theisz",
            "Sándor Bácsi",
            "Dániel Palatinszky"
        ],
        "file_path": "data/sosym-all/s10270-021-00900-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing automated network function onboarding through language extension and code refactoring",
        "submission-date": "2024/09",
        "publication-date": "2025/06",
        "abstract": "The management of modern communication networks requires sophisticated, specialized tooling that, akin to an operating system, provides an appropriately abstracted view of the network and its resources together with a broad and extensible range of functionality to observe, analyze, and change different aspects of the network such as software-deﬁned networking. This research explores the process of extending network management platforms, focusing speciﬁcally on a management platform in use at TELUS, a leading Canadian communication and IT company. The TELUS Intelligent Network Automation and Analytics (TINAA) is a model-driven control and management ecosystem. TINAA uses MDE techniques and YANG, a data modeling language widely used in the networking domain, to facilitate the implementation of new network functions. More speciﬁcally, to add a network function to TINAA, the following process is used: (1) The function is described using YANG. (2) Skeleton code is generated from these models. (3) The skeleton code is modiﬁed and completed as appropriate by a developer. In particular, we demonstrate how YANG’s language extension mechanism, combined with code clone detection and removal, can capture essential information at the model level and generate cleaner, more readable skeleton code, signiﬁcantly simplifying the manual code completion process. We evaluate our techniques using the L3VPN Network Model (L3NM), a widely adopted model for managing and conﬁguring Layer 3 VPN services. We identify general lessons learned of interest to both the networking and the MDE community.",
        "keywords": [
            "Model-based engineering",
            "Model-driven network automation",
            "NetOps",
            "Project scaffolding",
            "Network function virtualization",
            "RESTful API speciﬁcation",
            "Software-deﬁned networking (SDN)",
            "YANG",
            "Business Process Modeling Notation (BPMN)",
            "Code clones",
            "Refactoring"
        ],
        "authors": [
            "Hesham Elabd",
            "Juergen Dingel",
            "Tung Fai Lau",
            "Ali Tizghadam"
        ],
        "file_path": "data/sosym-all/s10270-025-01311-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal foundation of consistent EMF model transformations by algebraic graph transformation",
        "submission-date": "2009/06",
        "publication-date": "2011/03",
        "abstract": "Model transformation is one of the key activities in model-driven software development. An increasingly popular technology to deﬁne modeling languages is provided by the Eclipse Modeling Framework (EMF). Several EMF model transformation approaches have been developed, focusing on different transformation aspects. To validate model transformations with respect to functional behavior and correctness, a formal foundation is needed. In this paper, we deﬁne consistent EMF model transformations as a restricted class of typed graph transformations usingnodetypeinheritance.ContainmentconstraintsofEMF model transformations are translated to a special kind of graph transformation rules such that their application leads to consistent transformation results only. Thus, consistent EMF model transformations behave like algebraic graph transformations and the rich theory of algebraic graph transformation can be applied to these EMF model transformations to show functional behavior and correctness. Furthermore, we propose parallel graph transformation as a suitable framework for modeling EMF model transformations with multi-object structures. Rules extended by multi-object structures can specify a ﬂexible number of recurring structures. The actual number of recurring structures is dependent on the application context of such a rule. We illustrate our approach by selected refactorings of simpliﬁed statechart models. Finally, we discuss the implementation of our concepts in a tool environment for EMF model transformations.",
        "keywords": [
            "Consistent EMF models",
            "Model transformation",
            "Graph transformation",
            "Rule amalgamation"
        ],
        "authors": [
            "Enrico Biermann",
            "Claudia Ermel",
            "Gabriele Taentzer"
        ],
        "file_path": "data/sosym-all/s10270-011-0199-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "In memory of Robert B. France, Co-Founder and Editor-in-Chief of SoSyM from 1999 to 2015",
        "submission-date": "2015/04",
        "publication-date": "2015/04",
        "abstract": "The SoSyM team has been devastated to learn about the passing of Prof. Robert B. France, on the evening of Sunday, February 15th, 2015. His passing was painless, after a battle against cancer. He was 54 years old.",
        "keywords": [],
        "authors": [
            "Marsha Chechik",
            "Geri Georg",
            "Martin Gogolla",
            "Jean-Marc Jezequel",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-015-0461-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SQL-PL4OCL: an automatic code generator from OCL to SQL procedural language",
        "submission-date": "2016/11",
        "publication-date": "2017/05",
        "abstract": "In this paper, we introduce a SQL-PL code generator for OCL expressions that, in contrast to other proposals, is able to map OCL iterate and iterator expressions thanks to our use of stored procedures. We explain how the mapping presented here introduces key differences to the previous version of our mapping in order to (i) simplify its deﬁnition, (ii) improve the evaluation time of the resulting code, and (iii) consider OCL three-valued evaluation semantics. Moreover, we have implemented our mapping to target several relational database management systems, i.e., MySQL, MariaDB, PostgreSQL, and SQL server, which allows us to widen its usability and to benchmark the evaluation time of the SQL-PL code produced.",
        "keywords": [
            "OCL",
            "UML",
            "SQL",
            "Stored procedures",
            "Code generator"
        ],
        "authors": [
            "Marina Egea",
            "Carolina Dania"
        ],
        "file_path": "data/sosym-all/s10270-017-0597-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Performance models of storage contention in cloud environments",
        "submission-date": "2011/07",
        "publication-date": "2012/03",
        "abstract": "We propose simple models to predict the performance degradation of disk requests due to storage device contention in consolidated virtualized environments. Model parameters can be deduced from measurements obtained inside Virtual Machines (VMs) from a system where a single VM accesses a remote storage server. The parameterized model can then be used to predict the effect of storage contention when multiple VMs are consolidated on the same server. We first propose a trace-driven approach that evaluates a queueing network with fair share scheduling using simulation. The model parameters consider Virtual Machine Monitor level disk access optimizations and rely on a calibration technique. We further present a measurement-based approach that allows a distinct characterization of read/write performance attributes. In particular, we define simple linear prediction models for I/O request mean response times, throughputs and read/write mixes, as well as a simulation model for predicting response time distributions. We found our models to be effective in predicting such quantities across a range of synthetic and emulated application workloads.",
        "keywords": [
            "Performance modeling",
            "Virtualization",
            "Storage"
        ],
        "authors": [
            "Stephan Kraft",
            "Giuliano Casale",
            "Diwakar Krishnamurthy",
            "Des Greer",
            "Peter Kilpatrick"
        ],
        "file_path": "data/sosym-all/s10270-012-0227-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Formal synthesis of application and platform behaviors of embedded software systems",
        "submission-date": "2011/01",
        "publication-date": "2013/05",
        "abstract": "Two main embedded software components, application software and platform software, i.e., the real-time operating system (RTOS), interact with each other in order to achieve the functionality of the system. However, they are so different in behaviors that one behavior modeling language is not sufﬁcient to model both styles of behaviors and to reason about the characteristics of their individual behaviors as well as their parallel behavior and interaction properties. In this paper, we present a formal approach to the synthesis of the application software and the RTOS behavior models. In this approach, each of them is modeled with its adequate model-ing language and then is composed into a system model for analysis. Moreover, this paper also presents a consistent way of analyzing the application software with respect to both functional requirements and timing requirements. To show the effectiveness of the approach, a case study is conducted, where ARINC 653 and its application are modeled and veriﬁed against timing requirements. Using our approach, application software can be constructed as a behavioral model independently from a speciﬁc platform and can be veriﬁed against various platforms and timing constraints in a formal way.",
        "keywords": [
            "Embedded software systems",
            "Real-time operating systems",
            "Model-driven development",
            "Statecharts",
            "TRoS",
            "Formal methods and engineering"
        ],
        "authors": [
            "Jinhyun Kim",
            "Inhye Kang",
            "Jin-Young Choi",
            "Insup Lee",
            "Sungwon Kang"
        ],
        "file_path": "data/sosym-all/s10270-013-0342-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Aspects of abstraction in software development",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "Abstraction is a fundamental tool of human thought in every context. This essay briefly reviews some manifestations of abstraction in everyday life, in engineering and mathematics, and in software and system development. Vertical and horizontal abstraction are distinguished and characterised. The use of vertical abstraction in top-down and bottom-up program development is discussed, and also, the use of horizontal abstraction in one very different approach to program design. The ubiquitous use of analogical models in software is explained in terms of analytical abstractions. Some aspects of the practical use of abstraction in the development of computer-based systems are explored. The necessity of multiple abstractions is argued from the essential nature of abstraction, which by definition focuses on some concerns at the expense of discarding others. Finally, some general recommendations are offered for a consciously thoughtful use of abstraction in software development.",
        "keywords": [
            "Abstraction",
            "Analogic model",
            "Bottom-up design",
            "Grounded abstraction",
            "Free abstraction",
            "Horizontal abstraction",
            "Monsters",
            "Refinement",
            "Theory",
            "Top-down design",
            "Vertical abstraction"
        ],
        "authors": [
            "Michael Jackson"
        ],
        "file_path": "data/sosym-all/s10270-012-0259-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Strategic business modeling: representation and reasoning",
        "submission-date": "2011/11",
        "publication-date": "2012/10",
        "abstract": "Business intelligence (BI) offers tremendous potential for business organizations to gain insights into their day-to-day operations, as well as longer term opportunities and threats. However, most of today’s BI tools are based on models that are too much data-oriented from the point of view of business decision makers. We propose an enterprise modeling approach to bridge the business-level understand-ing of the enterprise with its representations in databases and data warehouses. The business intelligence model (BIM) offers concepts familiar to business decision making—such as goals, strategies, processes, situations, inﬂuences, and indicators. Unlike many enterprise models which are meant to be used to derive, manage, or align with IT system imple-mentations, BIM aims to help business users organize and make sense of the vast amounts of data about the enterprise and its external environment. In this paper, we present core BIM concepts, focusing especially on reasoning about situ-ations, inﬂuences, and indicators. Such reasoning supports strategic analysis of business objectives in light of current enterprise data, allowing analysts to explore scenarios and ﬁnd alternative strategies. We describe how goal reasoning techniquesfromconceptualmodelingandrequirementsengi-neering have been applied to BIM. Techniques are also pro-vided to support reasoning with indicators linked to business metrics, including cases where speciﬁcations of indicators are incomplete. Evaluation of the proposed modeling and reasoning framework includes an on-going prototype imple-mentation, as well as case studies.",
        "keywords": [
            "Business intelligence",
            "Business model",
            "Conceptual modeling languages",
            "Inﬂuence diagrams",
            "Goal",
            "Situation",
            "Key performance indicators",
            "Strategic planning"
        ],
        "authors": [
            "Jennifer Horkoff",
            "Daniele Barone",
            "Lei Jiang",
            "Eric Yu",
            "Daniel Amyot",
            "Alex Borgida",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-012-0290-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating instance models from meta models",
        "submission-date": "2007/04",
        "publication-date": "2008/07",
        "abstract": "Meta modeling is a wide-spread technique to deﬁne visual languages, with the UML being the most pro-minent one. Despite several advantages of meta modeling such as ease of use, the meta modeling approach has one disadvantage: it is not constructive, i.e., it does not offer a direct means of generating instances of the language. This disadvantage poses a severe limitation for certain applica-tions. For example, when developing model transformations, it is desirable to have enough valid instance models available for large-scale testing. Producing such a large set by hand is tedious. In the related problem of compiler testing, a string grammar together with a simple generation algorithm is typically used to produce words of the language automatically. In this paper, we introduce instance-generating graph grammars for creating instances of meta models, thereby overcoming the main deﬁcit of the meta modeling approach for deﬁning languages.",
        "keywords": [
            "Meta model",
            "UML",
            "Graph grammar",
            "Instance generation"
        ],
        "authors": [
            "Karsten Ehrig",
            "Jochen Malte Küster",
            "Gabriele Taentzer"
        ],
        "file_path": "data/sosym-all/s10270-008-0095-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "What is a process model composed of? A systematic literature review of meta-models in BPM",
        "submission-date": "2019/09",
        "publication-date": "2021/01",
        "abstract": "Business process modelling languages typically enable the representation of business process models by employing (graphical) symbols. These symbols can vary depending upon the verbosity of the language, the modelling paradigm, the focus of the language and so on. To make explicit different constructs and rules employed by a speciﬁc language, as well as bridge the gap across different languages, meta-models have been proposed in the literature. These meta-models are a crucial source of knowledge on what state-of-the-art literature considers relevant to describe business processes. The goal of this work is to provide the ﬁrst extensive systematic literature review (SLR) of business process meta-models. This SLR aims to answer research questions concerning: (1) the kind of meta-models proposed in the literature, (2) the recurring constructs they contain, (3) their purposes and (4) their evaluations. The SRL was performed manually considering papers automatically retrieved from reference paper repositories as well as proceedings of the main conferences in the Business Process Management research area. Sixty-ﬁve papers were selected and evaluated against four research questions. The results indicate the existence of a reasonable body of work conducted in this speciﬁc area, but not a full maturity. In particular, in answering the research questions several challenges have (re-)emerged for the Business Process Community, concerning: (1) the type of elements that constitute a Business Process and their meaning, (2) the absence of a (or several) reference meta-model(s) for the community, (3) the purpose for which meta-models are introduced in the literature and (4) a framework for the evaluation of the meta-models themselves. Moreover, the classiﬁcation framework devised to answer the four research questions can provide a reference structure for future descriptive categorizations.",
        "keywords": [
            "Business Process Modelling",
            "Conceptual Modelling",
            "Meta-Models"
        ],
        "authors": [
            "Greta Adamo",
            "Chiara Ghidini",
            "Chiara Di Francescomarino"
        ],
        "file_path": "data/sosym-all/s10270-020-00847-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Handling nonconforming individuals in search-based model-driven engineering: nine generic strategies for feature location in the modeling space of the meta-object facility",
        "submission-date": "2019/02",
        "publication-date": "2021/03",
        "abstract": "Lately, the model-driven engineering community has been paying more attention to the techniques offered by the search-based software engineering community. However, even though the conformance of models and metamodels is a topic of great interest for the modeling community, the works that address model-related problems through the use of search metaheuristics are not taking full advantage of the strategies for handling nonconforming individuals. The search space can be huge when searching in model artifacts (magnitudes of around 10150 for models of 500 elements). By handling the nonconforming individuals, the search space can be drastically reduced. In this work, we present a set of nine generic strategies for handling nonconforming individuals that are ready to be applied to model artifacts. The strategies are independent from the application domain and only include constraints derived from the meta-object facility. In addition, we evaluate the strategies with two industrial case studies using an evolutionary algorithm to locate features in models. The results show that the use of the strategies presented can reduce the number of generations needed to reach the solution by 90% of the original value. Generic strategies such as the ones presented in this work could lead to the emergence of more complex ﬁtness functions for searches in models or even new applications for the search metaheuristics in model-related problems.",
        "keywords": [
            "Model-driven engineering (MDE)",
            "Search-based software engineering (SBSE)",
            "Feature location (FL)",
            "Evolutionary algorithm (EA)"
        ],
        "authors": [
            "Jaime Font",
            "Lorena Arcega",
            "Øystein Haugen",
            "Carlos Cetina"
        ],
        "file_path": "data/sosym-all/s10270-021-00870-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal veriﬁcation of QVT transformations for code generation",
        "submission-date": "2012/03",
        "publication-date": "2013/06",
        "abstract": "We present a formal calculus for operational QVT. The calculus is implemented in the interactive theorem prover KIV and allows to prove properties of QVT transformations for arbitrary meta models. Additionally, we present a framework for provably correct Java code generation. The framework uses a meta model for a Java abstract syntax tree as the target of QVT transformations. This meta model is mapped to a formal Java semantics in KIV. This makes it possible to formally prove (interactively) with the QVT calculus that a transformation always generates a Java model (i.e. a program) that is type correct and has certain semanti-cal properties. The Java model can be used to generate source code by a model-to-text transformation or byte code directly.",
        "keywords": [
            "Correctness of model transformations",
            "QVT",
            "Formal veriﬁcation",
            "Interactive theorem proving"
        ],
        "authors": [
            "Kurt Stenzel",
            "Nina Moebius",
            "Wolfgang Reif"
        ],
        "file_path": "data/sosym-all/s10270-013-0351-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Polymorphic scenario-based speciﬁcation models: semantics and applications",
        "submission-date": "2010/02",
        "publication-date": "2010/07",
        "abstract": "We present polymorphic scenarios, a generalization of a UML2-compliant variant of Damm and Harel’s live sequence charts (LSC) in the context of object-orientation. Polymorphic scenarios are visualized using (modal) sequence diagrams where lifelines may represent classes and interfaces rather than concrete objects. Their semantics takes advantage of inheritance and interface realization to allow the speciﬁcation of most expressive, succinct, and reusable universal and existential inter-object scenarios for object-oriented system models. We motivate the use of polymorphic scenarios, formally deﬁne their trace-based semantics, and present their application for scenario-based testing and execution, as implemented in the S2A compiler developed at the Weizmann Institute of Science. We further discuss advanced semantic issues arising from the use of scenarios in a polymorphic setting, suggest possible extensions, present a UML proﬁle to support polymorphic scenarios, consider the application of the polymorphic semantics to other variants of scenario-based speciﬁcation languages, and position our work in the broader context of behavioral subtyping.",
        "keywords": [
            "Live sequence charts",
            "UML interactions",
            "Sequence diagrams",
            "Polymorphism",
            "Scenario-based modeling",
            "Behavioral subtyping"
        ],
        "authors": [
            "Shahar Maoz"
        ],
        "file_path": "data/sosym-all/s10270-010-0168-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special issue on MODELS 2011",
        "submission-date": "2013/03",
        "publication-date": "2013/03",
        "abstract": "For the past 14 years, the MODELS conference has been the premier venue for the exchange of innovative ideas and experiences of model-based approaches in the development of complex systems. The conference series covers all aspects of model-based development for software and systems engineering, including modeling languages, methods, tools, and their applications. MODELS is universally recognized as one of the top conferences in software engineering research with an acceptance rate averaging 20% in recent years. In 16–21 October 2011, the MODELS conference was held in Wellington, New Zealand. The conference received 167 papers of which 34 were accepted for presentation by the programme committee, resulting in an acceptance rate of roughly 20%. Research in software and system modeling is now a relatively mature ﬁeld.",
        "keywords": [],
        "authors": [
            "Jon Whittle",
            "Tony Clark"
        ],
        "file_path": "data/sosym-all/s10270-013-0331-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven engineering for digital twins: a systematic mapping study",
        "submission-date": "2023/09",
        "publication-date": "2025/03",
        "abstract": "Digital twins (DTs) are proliferating in a multitude of domains, including agriculture, automotive, avionics, logistics, manu-facturing, medicine, smart homes, etc. As domain experts and software experts both have to contribute to the engineering of effective DTs, several model-driven engineering (MDE) approaches have been recently proposed to ease the design, develop-ment, and operation of DTs. However, the diversity of domains in which MDE is currently applied to DTs, as well as the diverse landscape of DTs and MDE applications to DTs, makes it challenging for researchers and practitioners to get an overview of what techniques and artifacts are already applied in this context. In this paper, we shed light on the aforementioned aspects by performing a systematic mapping study on the application of MDE automation techniques, i.e., model-to-model transfor-mation, code generation, and model interpretation, in the context of DTs as well as on the characteristics of DTs including the twinned systems to which these techniques are applied in different domains. We systematically retrieved a set of 189 unique publications, of which 66 were selected for further investigation in this paper. Our results indicate that the distribution of employed MDE techniques (136 applications of automation techniques) is balanced between the different techniques, but there are signiﬁcant variations for different DT types. With respect to the different domains, we found that even though applications are available in many domains, a small number of domains currently dominate applications of MDE to DTs, i.e., more than half of included papers are in the manufacturing and transportation domains.",
        "keywords": [
            "Model-driven engineering",
            "Digital twin",
            "Digital twin engineering",
            "Digital twin modeling languages",
            "Literature review"
        ],
        "authors": [
            "Daniel Lehner",
            "Jingxi Zhang",
            "Jérôme Pfeiffer",
            "Sabine Sint",
            "Ann-Kathrin Splettstößer",
            "Manuel Wimmer",
            "Andreas Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-025-01264-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introductory paper",
        "submission-date": "2004/11",
        "publication-date": "2004/11",
        "abstract": "A lot of problems which occur when developing (embed-ded) systems are currently caused by highly decoupled development activities. In the major disciplines such as mechanical engineering or function development, moreover, complex activities, e.g. geometric modelling, requirement management or system architecture develop-ment are performed. Specialised tools are in place for each of the various activities, most of them commercial oﬀthe shelf (COTS) tools. The variety of activities carried out by COTS tools as well as their lack of interoperability, discourage co-operation within development teams. This separation causes high development costs due to long feedback cycles and investments bound to tool-related development results. Quality is reduced by gaps within the development processes used and low interaction between development disciplines. As part of the solution, system development must be supported by a new, sophisticated means of integrating existing tools.",
        "keywords": [],
        "authors": [
            "Andy Schürr",
            "Heiko Dörr"
        ],
        "file_path": "data/sosym-all/s10270-004-0069-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Holistic data-driven requirements elicitation in the big data era",
        "submission-date": "2021/04",
        "publication-date": "2021/10",
        "abstract": "Digital transformation stimulates continuous generation of large amounts of digital data, both in organizations and in society\nat large. As a consequence, there have been growing efforts in the Requirements Engineering community to consider digital\ndata as sources for requirements acquisition, in addition to human stakeholders. The volume, velocity and variety of the data\nmake requirements discovery increasingly dynamic, but also unstructured and complex, which current elicitation methods\nare unable to consider and manage in a systematic and efﬁcient manner. We propose a framework, in the form of a conceptual\nmetamodel and a method, for continuous and automated acquisition, analysis and aggregation of heterogeneous digital sources\nthat aims to support data-driven requirements elicitation and management. The usability of the framework is partially validated\nby an in-depth case study from the business sector of video game development.",
        "keywords": [
            "Data-driven requirements engineering",
            "Big data",
            "Requirements modeling",
            "Machine learning",
            "Natural language processing"
        ],
        "authors": [
            "Aron Henriksson",
            "Jelena Zdravkovic"
        ],
        "file_path": "data/sosym-all/s10270-021-00926-6.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Advanced testing and debugging support for reactive executable DSLs",
        "submission-date": "2021/08",
        "publication-date": "2022/09",
        "abstract": "ExecutableDomain-SpeciﬁcLanguages (xDSLs) allowthedeﬁnitionandtheexecutionof behavioral models. Somebehavioral models arereactive, meaningthat duringtheir execution, theyaccept external events andreact byexposingevents totheexternal environment. Since complex interaction may occur between the reactive model and the external environment, they should be tested as early as possible to ensure the correctness of their behavior. In this paper, we propose a set of generic testing facilities for reactive xDSLs using the standardized Test Description Language (TDL). Given a reactive xDSL, we generate a TDL library enabling the domain experts to write and run event-driven TDL test cases for conforming reactive models. To further support the domain expert, the approach integrates interactive debugging to help in localizing defects, and mutation analysis to measure the quality of test cases. We evaluate the level of genericity of the approach by successfully writing, executing, and analyzing 247 event-driven TDL test cases for 70 models conforming to two different reactive xDSLs.",
        "keywords": [
            "Reactive executable DSL",
            "Testing",
            "Test description language",
            "Debugging",
            "Mutation analysis"
        ],
        "authors": [
            "Faezeh Khorram",
            "Erwan Bousse",
            "Jean-Marie Mottu",
            "Gerson Sunyé"
        ],
        "file_path": "data/sosym-all/s10270-022-01025-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain object hierarchies inducing multi-level models",
        "submission-date": "2020/05",
        "publication-date": "2022/03",
        "abstract": "Conceptual modeling of domain object hierarchies, such as product hierarchies or organization hierarchies, is difﬁcult due to the intricate nature of nonphysical domain objects organized in such hierarchies. Modeling domain object hierarchies as part-whole hierarchies covers their hierarchical structure, yet to capture their meaning, part-whole hierarchies have to be combined with specialization and multi-level instantiation. To this end we introduce the deep domain object (DDO) multi-level modeling pattern and approach. With the DDO approach, subclasses and metaclasses are induced by and integrated with the part-whole hierarchy. The approach is aligned with the multi-level theory (MLT) and formalized by a metamodel and a set of deductive rules implemented in F-Logic. The proof-of-concept prototype is used for automated application of the pattern and for querying induced multi-level models.",
        "keywords": [
            "Conceptual modeling",
            "Multi-level modeling",
            "Metamodeling",
            "Part-whole",
            "Generalization",
            "Abstraction",
            "Concretization"
        ],
        "authors": [
            "Bernd Neumayr\nMichael Schreﬂ"
        ],
        "file_path": "data/sosym-all/s10270-022-00973-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating the accessibility of a PoN-enabled misuse case notation by the red–green colorblind community",
        "submission-date": "2021/06",
        "publication-date": "2022/03",
        "abstract": "In 2015, an improved version of the misuse case modeling notation designed using the Physics of Notations (PoN) framework was proposed. Empirical data support that the new notation is more cognitively effective than the original notation. The new notation makes use of color, in particular, red and green, meaning that red–green colorblind community will not be able to view the notation as designed and intended. The cognitive effectiveness of the red–green deﬁcient (RGD) version of the new notation in comparison to the original notation is unknown. The PoN outlines a number of principles that can be satisﬁed with or without the use of color, but would the deﬁciency of color may be so inhibiting that the cognitive effectiveness superiority of the PoN-enabled misuse case notation becomes diminished? Perhaps the original use case notation would be more cognitively effective for red–green colorblind users. An empirical study using 84 IT (Information Technology) professionals is conducted to assess the cognitive effectiveness of the RGD version of the PoN-enabled misuse case notation in comparison to the original misuse case notation. The experiment data are analyzed for any statistically signiﬁcant ﬁndings. The quantitative and qualitative results of the experiment indicate that the RGD version of PoN-enabled misuse case notation maintains its cognitive effectiveness superiority over the original notation. The results also show that the subjects are divided in their opinions with respect to the aesthetic appeal of the two notations. Adhering to the complete set of principles outlined in the PoN has allowed the new notation to maintain its cognitive effectiveness superiority over the original notation despite a curtailed color perspective.",
        "keywords": [
            "Misuse case notation",
            "Visual syntax",
            "Cognitive effectiveness",
            "UML",
            "Red",
            "green colorblindness"
        ],
        "authors": [
            "Mohamed El-Attar"
        ],
        "file_path": "data/sosym-all/s10270-022-00992-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on model transformation",
        "submission-date": "2013/10",
        "publication-date": "2015/01",
        "abstract": "Modeling is a key element in reducing the complexity of the development and maintenance of software systems. Software engineering paradigms like model-driven engineering (MDE) consider models as the primary elements in the software construction process. In this setting, model transformations are essential for elevating models from documentation elements to first-class artifacts of the development. Model transformation includes model-to-text transformation to generate code from models, text-to-model transformations to parse textual representations to model representations, model extraction to derive higher-level models from legacy code, and model-to-model transformations to normalize, weave, optimize, simulate, and refactor models, as well as to translate between modeling languages.",
        "keywords": [],
        "authors": [
            "Zhenjiang Hu",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-013-0388-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Least privilege analysis in software architectures",
        "submission-date": "2010/11",
        "publication-date": "2011/11",
        "abstract": "Due to the lack of both precise definitions and effective software engineering methodologies, security design principles are often neglected by software architects, resulting in potentially high-risk threats to systems. This work lays the formal foundations for understanding the security design principle of least privilege in software architectures and provides a technique to identify violations against this principle. The technique can also be leveraged to analyze violations against the security design principle of separation of duties. The proposed approach is supported by tools and has been validated in four case studies, two of which are presented in detail in this paper.",
        "keywords": [
            "Security analysis",
            "Least privilege",
            "Software architecture"
        ],
        "authors": [
            "Koen Buyens",
            "Riccardo Scandariato",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-011-0218-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MoDALAS: addressing assurance for learning-enabled autonomous systems in the face of uncertainty",
        "submission-date": "2022/03",
        "publication-date": "2023/03",
        "abstract": "Increasingly, safety-critical systems include artiﬁcial intelligence and machine learning components (i.e., learning-enabled components (LECs)). However, when behavior is learned in a training environment that fails to fully capture real-world phenomena, the response of an LEC to untrained phenomena is uncertain and therefore cannot be assured as safe. Automated methods are needed for self-assessment and adaptation to decide when learned behavior can be trusted. This work introduces a model-driven approach to manage self-adaptation of a learning-enabled system (LES) to account for run-time contexts for which the learned behavior of LECs cannot be trusted. The resulting framework enables an LES to monitor and evaluate goal models at run time to determine whether or not LECs can be expected to meet functional objectives and enables system adaptation accordingly. Using this framework enables stakeholders to have more conﬁdence that LECs are used only in contexts comparable to those validated at design time.",
        "keywords": [
            "Goal-based modeling",
            "Self-adaptive systems",
            "Artiﬁcial intelligence",
            "Machine learning",
            "Models at run time",
            "Cyber physical systems",
            "Behavior oracles",
            "Autonomous vehicles"
        ],
        "authors": [
            "Michael Austin Langford",
            "Kenneth H. Chan",
            "Jonathon Emil Fleck",
            "Philip K. McKinley",
            "Betty H.C. Cheng"
        ],
        "file_path": "data/sosym-all/s10270-023-01090-9.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Theme section of BPMDS’2014: the human perspective in business processes",
        "submission-date": "2016/11",
        "publication-date": "2017/01",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a two-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of Information Systems theory. This theme section follows the 15th edition of the BPMDS (Business Process Modeling, Development, and Support) series, organized in conjunction with CAISE’14, which was held in Thessaloniki, Greece, June 2014. BPMDS’2014 received 48 submissions from 23 countries, and 20 papers were selected and published in Springer LNBIP 175 volume.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-016-0570-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Holistic security requirements analysis for socio-technical systems",
        "submission-date": "2015/10",
        "publication-date": "2016/09",
        "abstract": "Security has been a growing concern for large organizations, especially ﬁnancial and governmental institutions, as security breaches in the systems they depend on have repeatedlyresultedinbillionsofdollarsinlossesperyear,and this cost is on the rise. A primary reason for these breaches is that the systems in question are “socio-technical” a mix of people, processes, technology, and infrastructure. However, suchsystemsaredesignedinapiecemealratherthanaholistic fashion, leaving parts of the system vulnerable. To tackle this problem, we propose a three-layer security analysis framework consisting of a social layer (business processes, social actors), a software layer (software applications that support the social layer), and an infrastructure layer (physical and technological infrastructure). In our proposal, global security requirements lead to local security requirements, cutting across conceptual layers, and upper-layer security analysis inﬂuences analysis at lower layers. Moreover, we propose a set of analytical methods and a systematic process that together drive security requirements analysis across the three layers. To support analysis, we have deﬁned corresponding inference rules that (semi-)automate the analysis, helping to deal with system complexity. A prototype tool has been implemented to support analysts throughout the analysis process. Moreover, we have performed a case study on a real-world smart grid scenario to validate our approach.",
        "keywords": [
            "Security requirements",
            "Goal model",
            "Enterprise architecture",
            "Socio-technical system",
            "Security pattern"
        ],
        "authors": [
            "Tong Li",
            "Jennifer Horkoff",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-016-0560-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Road to a reactive and incremental model transformation platform: three generations of the VIATRA framework",
        "submission-date": "2016/03",
        "publication-date": "2016/05",
        "abstract": "The current release of VIATRA provides open-\nsource tool support for an event-driven, reactive model\ntransformation engine built on top of highly scalable incre-\nments graph queries for models with millions of elements\nand advanced features such as rule-based design space\nexploration complex event processing or model obfuscation.\nHowever, the history of the VIATRA model transformation\nframework dates back to over 16years. Starting as an early\nacademic research prototype as part of the M.Sc project of\nthe the ﬁrst author it ﬁrst evolved into a Prolog-based engine\nfollowed by a family of open-source projects which by now\nmatured into a component integrated into various industrial\nand open-source tools and deployed over multiple technolo-\ngies. This invited paper brieﬂy overviews the evolution of\nthe VIATRA/IncQuery family by highlighting key features\nand illustrating main transformation concepts along an open\ncase study inﬂuenced by an industrial project.",
        "keywords": [
            "Model transformations",
            "Incremental evaluation",
            "Reactive transformations",
            "Graph queries"
        ],
        "authors": [
            "Dániel Varró",
            "Gábor Bergmann",
            "Ábel Hegedüs",
            "Ákos Horváth",
            "István Ráth",
            "Zoltán Ujhelyi"
        ],
        "file_path": "data/sosym-all/s10270-016-0530-4.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "IncQuery"
        }
    },
    {
        "title": "A novel model-based testing approach for software product lines",
        "submission-date": "2013/11",
        "publication-date": "2016/02",
        "abstract": "Model-based testing relies on a model of the sys-tem under test. FineFit is a framework for model-based testing of Java programs. In the FineFit approach, the model is expressed by a set of tables based on Parnas tables. A software product line is a family of programs (the products) with well-deﬁned commonalities and variabilities that are developed by (re)using common artifacts. In this paper, we address the issue of using the FineFit approach to support the development of correct software product lines. We specify a software product line as a speciﬁcation product line where each product is a FineFit speciﬁcation of the corresponding software product. The main challenge is to concisely specify the software product line while retaining the readability of the speciﬁcation of a single system. To address this, we used delta-oriented programming, a recently proposed ﬂexible approach for implementing software product lines, and developed: (1) delta tables as a means to apply the delta-oriented programming idea to the speciﬁcation of software product lines; and (2) DeltaFineFit as a novel model-based testing approach for software product lines.",
        "keywords": [
            "Java",
            "Alloy",
            "Software product line",
            "Delta-oriented programming",
            "Model-based testing",
            "Reﬁnement"
        ],
        "authors": [
            "Ferruccio Damiani",
            "David Faitelson",
            "Christoph Gladisch",
            "Shmuel Tyszberowicz"
        ],
        "file_path": "data/sosym-all/s10270-016-0516-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic generation of built-in contract test drivers",
        "submission-date": "2011/06",
        "publication-date": "2012/09",
        "abstract": "Automatic generation of platform-independent and -dependent built-in contract test drivers that check pair-wise interactions between client and server components is presented, focusing on the built-in contract testing (BIT) method and the model-driven testing approach. Components are speciﬁed by UML diagrams that deﬁne the contract between client and server, independent of a speciﬁc platform. MDA approaches are applied to formalize and perform automatic transformations from a platform-independent model to a platform-independent test architecture according to a BIT proﬁle. The test architecture is mapped to Java platform models and then to test code. All these transformations are speciﬁed by a set of transformation rules written in the Atlas Transformation Language (ATL) that are automatically performed by the ATL engine. The solution named the MoBIT tool is applied to case studies in order to investigate the expected beneﬁts and challenges to be faced.",
        "keywords": [
            "Model-driven testing",
            "Built-in contract testing",
            "MDA"
        ],
        "authors": [
            "Everton L. G. Alves",
            "Patricia D. L. Machado",
            "Franklin Ramalho"
        ],
        "file_path": "data/sosym-all/s10270-012-0282-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Improving the SAT modulo ODE approach to hybrid systems analysis by combining different enclosure methods",
        "submission-date": "2012/03",
        "publication-date": "2012/11",
        "abstract": "Aiming at automatic veriﬁcation and analy- sis techniques for hybrid discrete-continuous systems, we present a novel combination of enclosure methods for ordi- nary differential equations (ODEs) with the iSAT solver for large Boolean combinations of arithmetic constraints. Improving on our previous work, the contribution of this paper lies in combining iSAT with VNODE-LP, as a state-of-the-art interval solver for ODEs, and with bracketing systems, which exploit monotonicity properties allowing to ﬁnd enclosures for problems that VNODE-LP alone cannot enclose tightly. We apply the combined iSAT-ODE solver to the analysis of a variety of non-linear hybrid systems by solving predicative encodings of reachability properties and of an inductive stability argument, and evaluate the impact of the different enclosure methods, decision heuristics and their combination. Our experiments include classic benchmarks A preliminary version of this paper appeared in [6]. This work has been supported by the German Research Council DFG within SFB/TR 14 “http://www.avacs.org”, by the French National Research Agency under contract ANR 2011 INS 006 04 “http://projects.laas.fr/ ANR-MAGIC-SPS”, and by the Natural Sciences and Engineering Research Council of Canada. from the literature, as well as a newly-designed conveyor belt system that combines hybrid behavior of parallel com- ponents, a slip-stick friction model with non-linear dynamics and ﬂow invariants and several dimensions of parameteriza- tion. In the paper, we also present and evaluate an extension of VNODE-LP tailored to its use as a deduction mechanism within iSAT-ODE, to allow fast re-evaluations of enclosures over arbitrary subranges of the analyzed time span.",
        "keywords": [
            "Analysis of hybrid discrete-continuous systems",
            "Satisﬁability modulo theories",
            "Enclosure methods for ODEs",
            "Bracketing systems"
        ],
        "authors": [
            "Andreas Eggers",
            "Nacim Ramdani",
            "Nedialko S. Nedialkov",
            "Martin Fränzle"
        ],
        "file_path": "data/sosym-all/s10270-012-0295-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Safety behavior abstraction and model evolution in autonomous driving",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "In the autonomous driving systems (ADSs) literature, most existing approaches primarily focus on identifying driving scenar-ios, which is challenged by the reality that real-world driving scenarios are countless and unpredictable, and it is impossible to have a comprehensive set of driving scenarios to demonstrate the test efﬁciency in covering all possible situations ADSs might face. To address these challenges, one fundamental step is to abstract complex ADS behaviors, e.g., (semi-)automatically derive a holistic view of how an ADS behaves under its driving environment with high-level representations, such as prior-knowledge-based models. Therefore, in this paper, we propose a novel Risk-basEd Model comprehension and Evolution approach for autonomous Driving sYstems, named REMEDY, which facilitates the development of such models and enables automated model evolution (i.e., discovering and extracting ADS behaviors and their interactions with the environment) via model execution and simulation with the autonomous driving simulator CARLA. To enable efﬁcient model evolution, we also equipped REMEDY with a risk-based strategy using Q-Learning, which is empirically evaluated by comparing it with three baselines (i.e., a random strategy, a coverage-based greedy strategy, and DeepCollision—a state-of-the-art approach). Results show that REMEDY is capable of discovering new and diverse behaviors, and the risk-based strategy is efﬁcient in discovering risky ADS behaviors.",
        "keywords": [
            "Autonomous driving",
            "Model evolution",
            "Model execution"
        ],
        "authors": [
            "Chao Tan",
            "Tiexin Wang",
            "Man Zhang",
            "Tao Yue"
        ],
        "file_path": "data/sosym-all/s10270-024-01261-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Communicating the variability of a software-product family to customers",
        "submission-date": "2002/10",
        "publication-date": "2003/02",
        "abstract": "Variability is a central concept in software product family development. Variability empowers constructive reuse and facilitates the derivation of different, customer specific products from the product family. If many customer specific requirements can be realised by exploiting the product family variability, the reuse achieved is obviously high. If not, the reuse is low. It is thus important that the variability of the product family is adequately considered when eliciting requirements from the customer.\n\nIn this paper we sketch the challenges for requirements engineering for product family applications. More precisely we elaborate on the need to communicate the variability of the product family to the customer. We differentiate between variability aspects which are essential for the customer and aspects which are more related to the technical realisation and need thus not be communicated to the customer. Motivated by the successful usage of use cases in single product development we propose use cases as communication medium for the product family variability. We discuss and illustrate which customer relevant variability aspects can be represented with use cases, and for which aspects use cases are not suitable. Moreover we propose extensions to use case diagrams to support an intuitive representation of customer relevant variability aspects.",
        "keywords": [
            "Variability",
            "Product family",
            "UML",
            "Use cases",
            "Requirements engineering"
        ],
        "authors": [
            "G¨unter Halmans",
            "Klaus Pohl"
        ],
        "file_path": "data/sosym-all/s10270-003-0019-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Meta3: a code generator framework for domain-speciﬁc languages",
        "submission-date": "2017/04",
        "publication-date": "2018/03",
        "abstract": "In software development, domain-speciﬁc languages (DSLs) are often applied for speciﬁc or repetitive tasks. For executable DSLs, a model interpreter can be developed to run DSL programs. Nevertheless, it is more widespread to generate code in a general-purpose programming language. A properly chosen DSL expresses the original problem more naturally for both domain and information technology experts, and thus, this approach makes the whole development process, especially requirements engineering and requirements analysis, more efﬁcient and less prone to human errors. There are code generator frameworks and so-called language workbenches available that make the development of code generators for DSLs easier. In this paper, we report on our own code generator framework, called Meta3. Meta3 is based on our code generator development experience. We believe that this experience report will be useful for developers of code generators and language workbenches interested in building more ﬂexible and robust code generators as well as better tools that support the construction of the latter.",
        "keywords": [
            "Domain-speciﬁc modeling",
            "Code generation",
            "Architecture"
        ],
        "authors": [
            "Gábor Kövesdán",
            "László Lengyel"
        ],
        "file_path": "data/sosym-all/s10270-018-0673-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ontologies for ﬁnding journalistic angles",
        "submission-date": "2019/10",
        "publication-date": "2020/06",
        "abstract": "Journalism relies more and more on information and communication technology (ICT). ICT-based journalistic knowledge platforms continuously harvest potentially news-relevant information from the Internet and make it useful for journalists. Because information about the same event is available from different sources and formats vary widely, knowledge graphs are emerging as a preferred technology for integrating, enriching, and preparing information for journalistic use. The paper explores how journalistic knowledge graphs can be augmented with support for news angles, which can help journalists to detect newsworthy events and make them interesting for the intended audience. We argue that ﬁnding newsworthy angles on news-related information is an important example of a topical problem in information science: that of detecting interesting events and situations in big data sets and presenting those events and situations in interesting ways.",
        "keywords": [
            "Computational journalism",
            "Data journalism",
            "Journalistic knowledge platforms",
            "News angles",
            "Knowledge graphs",
            "Ontology"
        ],
        "authors": [
            "Andreas L. Opdahl",
            "Bjørnar Tessem"
        ],
        "file_path": "data/sosym-all/s10270-020-00801-w.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A UML-based quantitative framework for early prediction of resource usage and load in distributed real-time systems",
        "submission-date": "2006/06",
        "publication-date": "2008/08",
        "abstract": "This paper presents a quantitative framework for early prediction of resource usage and load in distributed real-time systems (DRTS). The prediction is based on an analysis of UML 2.0 sequence diagrams, augmented with timing information, to extract timed-control ﬂow information. It is aimed at improving the early predictability of a DRTS by offering a systematic approach to predict, at the design phase, system behavior in each time instant during its execution. Since behavioral models such as sequence diagrams are available in early design phases of the software life cycle, the framework enables resource analysis at a stage when design decisions are still easy to change. Though we provide a general framework, we use network trafﬁc as an example resource type to illustrate how the approach is applied. We also indicate how usage and load analysis of other types of resources (e.g., CPU and memory) can be performed in a similar fashion. A case study illustrates the feasibility of the approach.",
        "keywords": [
            "Resource usage prediction",
            "Load analysis",
            "Load forecasting",
            "Resource overuse detection",
            "Real-time systems",
            "Distributed systems",
            "UML"
        ],
        "authors": [
            "Vahid Garousi",
            "Lionel C. Briand",
            "Yvan Labiche"
        ],
        "file_path": "data/sosym-all/s10270-008-0099-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synthesizing object life cycles from business process models",
        "submission-date": "2013/11",
        "publication-date": "2014/05",
        "abstract": "Uniﬁed modeling language (UML) activity diagrams can model the ﬂow of stateful business objects among activities, implicitly specifying the life cycles of those objects. The actual object life cycles are typically expressed in UML state machines. The implicit life cycles in UML activity diagrams need to be discovered in order to derive the actual object life cycles or to check the consistency with an existing life cycle. This paper presents an automated approach for synthesizing a UML state machine modeling the life cycle of an object that occurs in different states in a UML activity diagram. The generated state machines can contain parallelism, loops, and cross-synchronization. The approach makes life cycles that have been modeled implicitly in activity diagrams explicit. The synthesis approach has been implemented using a graph transformation tool and has been applied in several case studies.",
        "keywords": [
            "Process models",
            "State machines",
            "UML",
            "Model transformation"
        ],
        "authors": [
            "Rik Eshuis",
            "Pieter Van Gorp"
        ],
        "file_path": "data/sosym-all/s10270-014-0406-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Software engineering and formal methods: SEFM 2019 special section",
        "submission-date": "2021/02",
        "publication-date": "2021/03",
        "abstract": "This special section of Software and Systems Modeling contains extended versions of selected papers from the 17th International Conference on Software Engineering and Formal Methods (SEFM), which was held in Oslo, Norway, in September 2019. SEFM 2019 was the seventeenth edition of an annual series of conferences that aims at bringing together leading researchers and practitioners from academia and industry to advance the state of the art in formal methods, to facilitate their uptake in the software industry, and to encourage their integration within practical software engineering methods and tools.",
        "keywords": [],
        "authors": [
            "Peter Csaba Ölveczky",
            "Gwen Salaün"
        ],
        "file_path": "data/sosym-all/s10270-021-00874-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Efﬁcient parallel reasoning on fuzzy goal models for run time requirements veriﬁcation",
        "submission-date": "2015/07",
        "publication-date": "2016/09",
        "abstract": "As software applications become highly inter-connected in dynamically provisioned platforms, they form the so-called systems-of-systems. Therefore, a key issue that arises in such environments is whether speciﬁc requirements are violated, when these applications interact in new unforeseen ways as new resources or system components are dynamically provisioned. Such environments require the continuous use of frameworks for assessing compliance against speciﬁc mission critical system requirements. Such frameworks should be able to (a) handle large requirements models, (b) assess system compliance repeatedly and frequently using events from possibly high velocity and high frequency data streams, and (c) use models that can reﬂect the vagueness that inherently exists in big data event collection and in modeling dependencies between components of complex and dynamically re-conﬁgured systems. In this paper, we introduce a framework for run time reasoning over medium and large-scale fuzzy goal models, and we propose a process which allows for the parallel evaluation of such models. The approach has been evaluated for time and space performance on large goal models, exhibiting that in a simulation environment, the parallel reasoning process offers signiﬁcant performance improvement over a sequential one.",
        "keywords": [
            "Goal models reasoning",
            "Fuzzy reasoning",
            "Parallel reasoning",
            "Software systems",
            "Veriﬁcation"
        ],
        "authors": [
            "George Chatzikonstantinou",
            "Kostas Kontogiannis"
        ],
        "file_path": "data/sosym-all/s10270-016-0562-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing classic transformation languages to support multi-level modeling",
        "submission-date": "2012/10",
        "publication-date": "2013/10",
        "abstract": "As practical tools for disciplined multi-level modeling have begun to mature, the problem of supporting simple and efﬁcient transformations to-and-from multi-level models to facilitate interoperability has assumed grow-ing importance. The challenge is not only to support efﬁ-cient transformations between multi-level models, but also between multi-level and two-level model content represented in traditional modeling infrastructures such as the UML and programming languages. Multi-level model content can already be accessed by traditional transformation languages such as ATL and QVT, but in a way that is blind to the onto-logical classiﬁcation information they contain. In this paper, we present an approach for making rule-based transforma-tion languages “multi-level aware” so that the semantics of ontological classiﬁcation as well as linguistic classiﬁcation can be exploited when writing transformations.",
        "keywords": [
            "Multi-level transformation",
            "Orthogonal classiﬁcation architecture",
            "Ontological classiﬁcation",
            "Linguistic classiﬁcation"
        ],
        "authors": [
            "Colin Atkinson",
            "Ralph Gerbig",
            "Christian Vjekoslav Tunjic"
        ],
        "file_path": "data/sosym-all/s10270-013-0384-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL, QVT"
        }
    },
    {
        "title": "A framework for conceptual characterization of ontologies and its application in the cybersecurity domain",
        "submission-date": "2021/03",
        "publication-date": "2022/07",
        "abstract": "Organizations are actively seeking efﬁcient solutions for the management and protection of their assets. However, Cyber-security is a vast and complex domain, especially for large enterprises because it requires an interdisciplinary approach. Knowledge Graphs are one of the mechanisms that organizations use to explore security among assets and possible attacks. The grounding of concepts is fundamental to implementing Knowledge Graphs, and it is one of the most relevant ontology applications. Therefore, Cybersecurity Ontologies have emerged as an important research subject. The ﬁrst contribution of this paper is a search for previously existing works that have deﬁned Cybersecurity Ontologies. We found twenty-eight ontologies in this search. Based on this result, we propose a Cybersecurity Terminological Validation and a Framework for Classifying Ontologies. Then, we provide a cross-analysis of these two proposals and present a proposal of best practices for improving the ontological approach in the cybersecurity domain. We also discuss the impact of this proposal with regard to the Ontology Engineering process. Our goal is to provide a solution that meets the organization’s needs in terms of Cybersecurity and to contribute to Ontology Engineering research.",
        "keywords": [
            "Conceptual modeling",
            "Ontology classiﬁcation",
            "Cybersecurity ontology",
            "Ontology"
        ],
        "authors": [
            "Beatriz Franco Martins",
            "Lenin Javier Serrano Gil",
            "José Fabián Reyes Román",
            "José Ignacio Panach",
            "Oscar Pastor",
            "Moshe Hadad",
            "Benny Rochwerger"
        ],
        "file_path": "data/sosym-all/s10270-022-01013-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Pragmatic reuse for DSML development\nComposing a DSL for hybrid CPS modeling",
        "submission-date": "2019/10",
        "publication-date": "2020/10",
        "abstract": "By bridging the semantic gap, domain-speciﬁc language (DSLs) serve an important role in the conquest to allow domain\nexperts to model their systems themselves. In this publication we present a case study of the development of the Continuous\nREactive SysTems language (CREST), a DSL for hybrid systems modeling. The language focuses on the representation\nof continuous resource ﬂows such as water, electricity, light or heat. Our methodology follows a very pragmatic approach,\ncombining the syntactic and semantic principles of well-known modeling means such as hybrid automata, data-ﬂow languages\nand architecture description languages into a coherent language. The borrowed aspects have been carefully combined and\nformalised in a well-deﬁned operational semantics. The DSL provides two concrete syntaxes: CREST diagrams, a graphical\nlanguage that is easily understandable and serves as a model basis, and crestdsl, an internal DSL implementation that\nsupports rapid prototyping—both are geared towards usability and clarity. We present the DSL’s semantics, which thoroughly\nconnect the various language concerns into an executable formalism that enables sound simulation and formal veriﬁcation in\ncrestdsl, and discuss the lessons learned throughout the project.",
        "keywords": [
            "Cyber-physical systems",
            "Domain-speciﬁc language",
            "Modeling",
            "Simulation",
            "Veriﬁcation"
        ],
        "authors": [
            "Stefan Klikovits",
            "Didier Buchs"
        ],
        "file_path": "data/sosym-all/s10270-020-00831-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Object-oriented design: A goal-driven and pattern-based approach",
        "submission-date": "2006/09",
        "publication-date": "2007/07",
        "abstract": "In recent years, the inﬂuences of design patterns on software quality have attracted increasing attention in the area of software engineering, as design patterns encapsulate valuable knowledge to resolve design problems, and more importantly to improve the design quality. One of the key challenges in object-oriented design is how to apply appropriate design patterns during the system development. In this paper, design pattern is analyzed from different perspectives to see how it can facilitate design activities, handle non-functional requirement, solve design problems and resolve design conﬂicts. Based on the analysis, various kinds of applicability of design patterns are explored and integrated with a goal-driven approach to guiding developers to construct the object-oriented design model in a systematic manner. There are three beneﬁts to the proposed approach: making it easy to meet requirements, helping resolve design conﬂicts, and facilitating improvement of the design quality.",
        "keywords": [
            "Design pattern",
            "Object-oriented design",
            "Software quality",
            "Goal-driven requirement engineering"
        ],
        "authors": [
            "Nien-Lin Hsueh",
            "Jong-Yih Kuo",
            "Ching-Chiuan Lin"
        ],
        "file_path": "data/sosym-all/s10270-007-0063-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Software engineering and formal methods",
        "submission-date": "2008/06",
        "publication-date": "2008/06",
        "abstract": "Formal methods, i.e., technologies for the formal description, construction, analysis, and validation of software—mostly based on logics and formal reasoning—have matured and can be expected to complement and partly replace traditional software engineering methods in the future. The field has outgrown the area of academic case studies, and industry is showing serious interest. The challenge now is to scale up the application of formal methods in software industry and to encourage their integration with practical engineering methods. This challenge is met by the conference series Software Engineering and Formal Methods (SEFM) (http://sefm.iist.unu.edu), which aims to bring together practitioners and researchers from academia, industry and government to advance the state of the art in formal methods and their integration in the software development process.",
        "keywords": [],
        "authors": [
            "Bernhard Aichernig",
            "Bernhard Beckert"
        ],
        "file_path": "data/sosym-all/s10270-008-0091-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Broadcast psi-calculi with an application to wireless protocols",
        "submission-date": "2012/03",
        "publication-date": "2013/11",
        "abstract": "Psi-calculi is a parametric framework for the extensions of pi-calculus, with arbitrary data structures and logical assertions for facts about data. In this paper we add primitives for broadcast communication in order to model wireless protocols. The additions preserve the purity of the psi-calculi semantics, and we formally prove the standard congruence and structural properties of bisimilarity. We demonstrate the expressive power of broadcast psi-calculi by modelling the wireless ad hoc routing protocol LUNAR and verifying a basic reachability property.",
        "keywords": [
            "Psi-calculus",
            "Broadcast communication",
            "Bisimulation",
            "Ad hoc routing protocol"
        ],
        "authors": [
            "Johannes Borgström",
            "Shuqin Huang",
            "Magnus Johansson",
            "Palle Raabjerg",
            "Björn Victor",
            "Johannes Åman Pohjola",
            "Joachim Parrow"
        ],
        "file_path": "data/sosym-all/s10270-013-0375-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the theme section on Trends in Enterprise Architecture Research",
        "submission-date": "2024/03",
        "publication-date": "2024/04",
        "abstract": "This theme section on Trends in Enterprise Architecture Research originated at the 2022 TEAR workshop. Together with a general call for papers, an invitation to submit papers to this theme section was extended to the authors of the TEAR 2022 workshop papers. The international TEAR series brings together Enterprise Architecture (EA) researchers and provides a forum to present EA research results and to discuss future EA research directions. We received a total of 11 intentions to submit, from which six submissions materialized and went into review. After a thorough reviewing process, two papers were finally selected for inclusion into the theme section.",
        "keywords": [],
        "authors": [
            "Sybren de Kinderen",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-024-01165-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Model-based approach for the synthesis of software to firmware adapters for use with automatically generated components",
        "submission-date": "2015/05",
        "publication-date": "2016/05",
        "abstract": "This paper presents the MDE process in use at Elettronica SpA (ELT) for the development of complex embedded systems integrating software and firmware. The process is based on the adoption of SysML as the system-level modeling language and the use of Simulink for the refinement of selected subsystems. Implementations are generated automatically for both the software (C++ code) and firmware parts, and communication adapters are automatically generated from SysML using a dedicated profile and open-source tools for modeling and code generation. The process starts from a SysML system model, developed according to the platform-based design paradigm, in which a functional model of the system is paired to a model of the executionplatform.SubsystemsarereﬁnedasSimulinkmod- els or hand-coded in C++. An implementation for Simulink models is generated as software code or firmware on FPGA. Based on the SysML system architecture specification, our framework drives the generation of Simulink models with consistent interfaces, allows the automatic generation of the communication code among all subsystems (including the HW–FW interface code). In addition, it provides for the automatic generation of connectors for system-level simulation and of test harnesses and mockups to ease the integration and verification stage. We provide early results on the time savings obtained by using these technologies in the development process.",
        "keywords": [
            "Systemengineering",
            "Model-drivenarchitecture",
            "Model-based design",
            "Platform-based design",
            "Automatic code generation"
        ],
        "authors": [
            "Marco Di Natale",
            "David Perillo",
            "Francesco Chirico",
            "Andrea Sindico",
            "Alberto Sangiovanni-Vincentelli"
        ],
        "file_path": "data/sosym-all/s10270-016-0534-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Systematic stereotype usage",
        "submission-date": "2003/08",
        "publication-date": "2003/08",
        "abstract": "As one of the UML’s main extension mechan-isms, stereotypes play a crucial role in the UML’s ability to serve a wide and growing base of users. However, the precise meaning of stereotypes and their intended mode of use has never been entirely clear and has even gener-ated signiﬁcant debate among experts. Two basic ways of using UML stereotypes have been observed in practice: One to support the classiﬁcation of classes as a means of emulating metamodel extensions, the other to support the classiﬁcation of objects as a means of assigning them certain properties. In this paper we analyze these two recognized stereotype usage scenarios and explain the rationale for explicitly identifying a third form of usage sce-nario. We propose some notational concepts which could be used to explicitly distinguish the three usage scenar-ios and provide heuristics as to when each should be used. Finally, we conclude by proposing enhancements to the UML which could support all three forms cleanly and concisely.",
        "keywords": [
            "UML extension mechanism",
            "Stereotypes",
            "Classiﬁcation",
            "Transitive properties"
        ],
        "authors": [
            "Colin Atkinson",
            "Thomas K¨uhne",
            "Brian Henderson-Sellers"
        ],
        "file_path": "data/sosym-all/s10270-003-0027-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward a methodology for case modeling",
        "submission-date": "2018/11",
        "publication-date": "2019/11",
        "abstract": "Case management is increasingly used to capture and enact ﬂexible, knowledge-intensive processes in organizations. None of\ntheexistingcasemanagement approaches provides amethodologyfor casemodel elicitationandmodeling. Inthis contribution,\nthree modeling methods for fragment-based case management are presented: one which focuses on the control-ﬂow view, \nthe process-ﬁrst method, one which has a data-centric view, the object lifecycle-ﬁrst method, and one which focuses on \nthe goals of a case, the goals-ﬁrst method. Following the design science process, each of the three methods was evaluated in\ntwo case modeling workshops with two different stakeholder groups (PhD students and secretaries), resulting in a total of\nsix workshops. All participants were novices in case management and most of them as well in process modeling. The results\nindicate that the process-ﬁrst method can be quickly learned by novices and it might be useful for scenarios where the focus\nis on the main process with some degree of ﬂexibility. The object lifecycle-ﬁrst method yields more ﬂexible and consistent\ncase models, but requires a higher initial modeling effort, as the lifecycle of the main case object has to be designed ﬁrst. The\ngoals-ﬁrst method leads to a detailed and consistent case model and additionally provides, by means of the deﬁned goals, a\nchecklist what needs to be done for a case. This method requires in addition to the process modeling notation another model\ntype, the goal hierarchy, and therefore is less suited for novice modelers, as found by the workshop results.",
        "keywords": [
            "Case management",
            "Goal modeling",
            "Object lifecycle",
            "Process elicitation",
            "Process modeling",
            "t.BPM"
        ],
        "authors": [
            "Marcin Hewelt",
            "Luise Pufahl",
            "Sankalita Mandal",
            "Felix Wolﬀ",
            "Mathias Weske"
        ],
        "file_path": "data/sosym-all/s10270-019-00766-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain speciﬁc modeling",
        "submission-date": "2005/01",
        "publication-date": "2005/01",
        "abstract": "Looking at other engineering disciplines, it is evident that modeling is a vital and important part of the development of complex artifacts. Good modeling techniques provide support for the separation of concerns principle, rigorous analysis of designs, and for structuring construction ac-tivities. Models can play a similar role in the development of software-based systems. Furthermore, the software de-velopment activity has a characteristic not present in physical engineering: deliverable products (software sys-tems) can be generated from models (an observation made by Bran Selic, IBM/Rational at an MDA summer school). This characteristic can and should be exploited in our quest to make software development an engineering activity.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-005-0078-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The 9th annual state of SoSyM report",
        "submission-date": "2010/11",
        "publication-date": "2010/11",
        "abstract": "Another year has gone by and we are happy to report that the International Journal on Software and Systems Modeling (SoSyM) continues to do well in terms of a steady stream of submissions, quality of published papers, and average review turnaround time. Of particular significance is the first release of SoSyM’s impact factor, which, notably, is 1.533. As we have done on previous anniversaries, we take this opportunity to give a “state of the journal” report and to acknowledge the reviewers, editors, and publication staff that have contributed to the journal’s very good performance in the past year.",
        "keywords": [],
        "authors": [
            "Marco Aiello",
            "Ian Alexander",
            "Daniel Amyot",
            "Sven Apel",
            "Joao Araujo",
            "Colin Atkinson",
            "Joanne Atlee",
            "Orlando Avila",
            "Franck Barbier",
            "Luciano Baresi",
            "Balbir Barn",
            "Benoit Baudry",
            "Jorn Bettin",
            "Brian Blake",
            "Mireille Blay-Fornarino",
            "Christoph Bockisch",
            "Peter Bollen",
            "Behzad Bordbar",
            "Artur Boronat",
            "Jonathan Bowen",
            "Gyrd Brandeland",
            "Christoph Brandt",
            "Lionel Briand",
            "Phil Brooke",
            "Achim D. Brucker",
            "Jordi Cabot",
            "Artur Caetano",
            "Daniela Cancila",
            "Sven Castelyn",
            "Walter Cazzola",
            "María Victoria Cengarle",
            "Joanna Chimiak-Opoka",
            "Wei-Ngan Chin",
            "Tony Clark",
            "Manuel Clavel",
            "Sholom Cohen",
            "Philippe Collet",
            "Sara Comai",
            "Vittorio Cortellessa",
            "Fabio Costa",
            "Jorge R. Cuellar",
            "Guglielmo De Angelis",
            "Juan de Lara",
            "Olga De Troyer",
            "Serge Demeyer",
            "Ewen Denney",
            "Dirk Deridder",
            "Davide Di Ruscio",
            "Ada Diaconescu",
            "Oscar Díaz",
            "Juergen"
        ],
        "file_path": "data/sosym-all/s10270-010-0182-8.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Special Section of BPMDS’2016: Business Processes in a Connected World",
        "submission-date": "2018/04",
        "publication-date": "2018/05",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a 2-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of Information Systems theory. The goals, format, and history of BPMDS can be found on the web site http://www.bpmds.org/. This special section follows the 17th edition of the BPMDS (Business Process Modeling, Development, and Support) series, organized in conjunction with CAISE’16, which was held in Ljubljana, Slovenia, June 2016. BPMDS’2016 received 48 submissions from 31 countries, and 19 papers were selected and published in Springer LNBIP 248 volume. The focus theme of BPMDS 2016 was “Business Processes in a Connected World.” It embraced three areas of research, covering different aspects.",
        "keywords": [],
        "authors": [
            "Rainer Schmidt",
            "Ilia Bider"
        ],
        "file_path": "data/sosym-all/s10270-018-0677-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Report on the state of the SoSyM journal (2023 summary)",
        "submission-date": "2024/02",
        "publication-date": "2024/02",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Stéphanie Challita",
            "Benoit Combemale",
            "Huseyin Ergin",
            "JeﬀGray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-024-01152-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven development of mobile applications for Android and iOS supporting role-based app variability",
        "submission-date": "2015/04",
        "publication-date": "2016/09",
        "abstract": "Rapidly increasing numbers of applications and users make the development of mobile applications to one of the most promising ﬁelds in software engineering. Due to short time to market, differing platforms, and fast emerging technologies, mobile application development faces typical challenges where model-driven development (MDD) can help. We present a modeling language and an infrastructure for the MDD of native apps in Android and iOS. Our approach allows a ﬂexible app development on differ-ent abstraction levels: compact modeling of standard app elements such as standard data management and increas-ingly detailed modeling of individual elements to cover, for example, speciﬁc behavior. Moreover, a kind of variability modeling is supported such that mobile apps with variants can be developed. We demonstrate our MDD approach with several apps including a conference app, a museum guide with augmented reality functionality, and a SmartPlug.",
        "keywords": [
            "Model-driven development",
            "Mobile applica-tion",
            "Android",
            "iOS"
        ],
        "authors": [
            "Steffen Vaupel",
            "Gabriele Taentzer",
            "René Gerlach",
            "Michael Guckert"
        ],
        "file_path": "data/sosym-all/s10270-016-0559-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Developing conﬁgurations and solutions for logical puzzles with UML and OCL",
        "submission-date": "2024/03",
        "publication-date": "2025/02",
        "abstract": "Logical puzzles can be important factors for the development of rational analysis and application capabilities for pupils and students. Therefore, logical puzzles can also take a prominent supporting role in computer science education. This contribution proposes a UML class model with accompanying OCL constraints for developing logical puzzles. The class model acts as a metamodel for the description of the basic puzzle organization and the logical clues presented to the learners. The constraints express, for example, statements about uniqueness of solutions, and the degree of puzzle complexity may be tuned by appropriate model elements. Given a puzzle speciﬁcation which simply comprises the domain elements of the puzzle plus constraints, our implementation uses a UML and OCL solver to construct a puzzle instance (i.e., a set of clues and solutions) automatically. The puzzle is made playable by a graphical user interface. We have validated this approach for developing puzzles by building several puzzles from the literature of increasing complexity and performed a student survey.",
        "keywords": [
            "Logical puzzle",
            "Formal method",
            "UML",
            "OCL",
            "Metamodel"
        ],
        "authors": [
            "Martin Gogolla",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-025-01272-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Unifying nominal and structural typing",
        "submission-date": "2017/03",
        "publication-date": "2018/02",
        "abstract": "In this article, I argue for a typing scheme for modeling that uniﬁes the hitherto separated approaches of nominal and structural typing. Both these approaches have their respective advantages and disadvantages, and I suggest a unifying approach that provides one with the best of both worlds on demand. The ultimate goal is to make a contribution toward removing the gulf currently running through the modeling community that is created by the differences between explanatory and constructive modeling with their dependence on structural and nominal typing, respectively. To this end, I ﬁrst characterize the typing disciplines underlying these different schools of thought, then identify their respective trade-offs, subsequently observe what aspects of these rather different typing approaches are compatible with each other and which are inherently incompatible, and ﬁnally propose a scheme that supports ﬂuid transitioning between the approaches.",
        "keywords": [
            "Explanatory modeling",
            "Constructive modeling",
            "Uniﬁcation",
            "Ontologies",
            "Conceptual models",
            "Structural typing",
            "Nominal typing"
        ],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-018-0660-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards a model-driven approach for multiexperience AI-based user interfaces",
        "submission-date": "2021/06",
        "publication-date": "Not found",
        "abstract": "Software systems start to include other types of interfaces beyond the “traditional” Graphical-User Interfaces (GUIs). In particular, Conversational User Interfaces (CUIs) such as chat and voice are becoming more and more popular. These new types of interfaces embed smart natural language processing components to understand user requests and respond to them. To provide an integrated user experience all the user interfaces in the system should be aware of each other and be able to collaborate. This is what is known as a multiexperience User Interface. Despite their many beneﬁts, multiexperience UIs are challenging to build. So far CUIs are created as standalone components using a platform-dependent set of libraries and technologies. This raises signiﬁcant integration, evolution and maintenance issues. This paper explores the application of model-driven techniques to the development of software applications embedding a multiexperience User Interface. We will discuss how raising the abstraction level at which these interfaces are deﬁned enables a faster development and a better deployment and integration of each interface with the rest of the software system and the other interfaces with whom it may need to collaborate. In particular, we propose a new Domain Speciﬁc Language (DSL) for specifying several types of CUIs and show how this DSL can be part of an integrated modeling environment able to describe the interactions between the modeled CUIs and the other models of the system (including the models of the GUI). We will use the standard Interaction Flow Modeling Language (IFML) as an example “host” language.",
        "keywords": [
            "Multiexperience development platform (MXDP)",
            "Model-driven development (MDD)",
            "bots",
            "Conversational user interface (CUI)"
        ],
        "authors": [
            "Elena Planas",
            "Gwendal Daniel",
            "Marco Brambilla",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-021-00904-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Systematizing modeler experience (MX) in model-driven engineering success stories",
        "submission-date": "2024/03",
        "publication-date": "2024/07",
        "abstract": "Modeling is often associated with complex and heavy tooling, leading to a negative perception among practitioners. However,\nalternative paradigms, such as everything-as-code or low-code, are gaining acceptance due to their perceived ease of use.\nThis paper explores the dichotomy between these perceptions through the lens of “modeler experience” (MX). MX includes\nfactors such as user experience, motivation, integration, collaboration and versioning, and language complexity. We examine\nthe relationships between these factors and their impact on different modeling usage scenarios. Our ﬁndings highlight the\nimportance of considering MX when understanding how developers interact with modeling tools and the complexities of\nmodeling and associated tooling.",
        "keywords": [
            "MDE",
            "User experience",
            "UX"
        ],
        "authors": [
            "Reyhaneh Kalantari",
            "Julian Oertel",
            "Joeri Exelmans",
            "Satrio Adi Rukmono",
            "Vasco Amaral",
            "Matthias Tichy",
            "Katharina Juhnke",
            "Jan-Philipp Steghöfer",
            "Silvia Abrahão"
        ],
        "file_path": "data/sosym-all/s10270-024-01194-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SimIMA: a virtual Simulink intelligent modeling assistant",
        "submission-date": "2022/06",
        "publication-date": "2023/03",
        "abstract": "Intelligent virtual model assistance is a key challenge in cultivating model-driven engineering proliferation and growth. Such assistance will help improve the quality of software models, support education for students learning modeling, and lower the entry barriers to new modelers. We present SimIMA, an intelligent modeling assistant for Simulink, which is an extremely popular modeling language in both industry and academia. SimIMA provides modelers with two different forms of data-driven guidance using a knowledge base of conﬁgurable repositories and sources. The ﬁrst form of guidance, SimGESTION, suggests to modelers single-step operations they can perform on their models as they edit them in their modeling environment. These suggestions are based on the machine learning technique of ensemble learning through association rule mining and frequency classiﬁcation. The second form of guidance, SimXAMPLE, presents modelers with similar/related Simulink systems for modelers to either insert directly into their environments or to view for inspiration. SimXAMPLE accomplishes this through model clone detection. To validate SimIMA, we conduct experiments using an established, open, and curated large set of Simulink models coming from a variety of application domains. Our results show that both of SimIMA’s forms of guidance are inferring the appropriate model and element suggestions given SimIMA’s knowledge base and that SimIMA is both scalable and efﬁcient. Through our evaluation, SimIMA demonstrates a prediction accuracy of 78.86% for block-level suggestions and 82.04% for full system suggestions.",
        "keywords": [],
        "authors": [
            "Bhisma Adhikari",
            "Eric J. Rapos",
            "Matthew Stephan"
        ],
        "file_path": "data/sosym-all/s10270-023-01093-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Systematic review of matching techniques used in model-driven methodologies",
        "submission-date": "2019/03",
        "publication-date": "2019/11",
        "abstract": "In model-driven methodologies, model matching is the process of ﬁnding a matching pair for every model element between two or more software models. Model matching is an important task as it is often used while differencing and merging models, which are key processes in version control systems. There are a number of different approaches to model matching, with most of them focusing on different goals, i.e., the accuracy of the matching process, or the generality of the algorithm. Moreover, there exist algorithms that use the textual representations of the models during the matching process. We present a systematic literature review that was carried out to obtain the state-of-the-art of model matching techniques. The search process was conducted based on a well-deﬁned methodology. We have identiﬁed a total of 3274 non-duplicate studies, out of which 119 have been included as primary studies for this survey. We present the state-of-the-art of model matching, highlighting the differences between different matching techniques, mainly focusing on text-based and graph-based algorithms. Finally, the main open questions, challenges, and possible future directions in the ﬁeld of model matching are discussed, also including topics like benchmarking, performance and scalability, and conﬂict handling.",
        "keywords": [
            "Model matching",
            "Model comparison",
            "Model differencing",
            "Version control",
            "Text-based modeling",
            "Systematic literature review"
        ],
        "authors": [
            "Ferenc Attila Somogyi",
            "Mark Asztalos"
        ],
        "file_path": "data/sosym-all/s10270-019-00760-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Taming uncertainty with MDE: an historical perspective",
        "submission-date": "2024/03",
        "publication-date": "2024/10",
        "abstract": "Uncertainty in Informatics can stem from various sources, whether ontological (inherent unpredictability, such as aleatory\nfactors) or epistemic (due to insufﬁcient knowledge). Effectively handling uncertainty, encompassing both ontological and\nepistemic aspects, to create predictable systems is a key objective for a signiﬁcant portion of the software engineering\ncommunity, particularly within the model-driven engineering (MDE) realm. Numerous techniques have been proposed over\nthe years, leading to evolving trends in model-based software development paradigms. This paper revisits the history of MDE,\naiming to pinpoint the primary aspects of uncertainty that these paradigms aimed to tackle upon their introduction. Our claim\nis that MDE progressively came after more and more aspects of uncertainty, up to the point that it could now help fully\nembrace it.",
        "keywords": [
            "MDE",
            "Uncertainty",
            "MDA",
            "SPL",
            "Aleatory"
        ],
        "authors": [
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-024-01227-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the special section of the 20th International Conference on Model-Driven Engineering Languages and Systems (MODELS’17)",
        "submission-date": "2019/12",
        "publication-date": "2020/01",
        "abstract": "Since its inception in 1998, MODELS has been the premier conference series for model-based software and systems engineering, covering all aspects of modeling, from languages and methods to tools and applications. MODELS’17 represented the 20th edition of the conference and was held in Austin, Texas, USA, during the week of September 17–22, 2017.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Vinay Kulkarni"
        ],
        "file_path": "data/sosym-all/s10270-020-00774-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on MODELS 2012",
        "submission-date": "2014/06",
        "publication-date": "2014/06",
        "abstract": "In this special section, we are happy to present three papers that resulted from invitations we made to the authors of some of the best papers presented at MODELS 2012. MODELS 2012 was the ACM/IEEE 15th International Conference on Model Driven Engineering Languages and Systems and took place in Innsbruck, Austria, from September 30 to October 5, 2012. The conference has established itself as one of the key international venues for the presentation of scientiﬁc results in the domain of model-driven engineering and related top- ics such as software modeling and model transformation.",
        "keywords": [],
        "authors": [
            "Jürgen Kazmeier",
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-014-0418-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Current trends in digital twin development, maintenance, and operation: an interview study",
        "submission-date": "2023/05",
        "publication-date": "2024/04",
        "abstract": "Digital twins (DTs) are often deﬁned as a pairing of a physical entity and a corresponding virtual entity (VE), mimicking certain aspects of the former depending on the use-case. In recent years, this concept has facilitated numerous use-cases ranging from design to validation and predictive maintenance of large and small high-tech systems. Various heterogeneous cross-domain models are essential for such systems, and model-driven engineering plays a pivotal role in the design, development, and maintenance of these models. We believe models and model-driven engineering play a similarly crucial role in the context of a VE of a DT. Due to the rapidly growing popularity of DTs and their use in diverse domains and use-cases, the methodologies, tools, and practices for designing, developing, and maintaining the corresponding VEs differ vastly. To better understand these differences and similarities, we performed a semi-structured interview research with 19 professionals from industry and academia who are closely associated with different lifecycle stages of digital twins. In this paper, we present our analysis and ﬁndings from this study, which is based on seven research questions. In general, we identiﬁed an overall lack of uniformity in terms of the understanding of digital twins and used tools, techniques, and methodologies for the development and maintenance of the corresponding VEs. Furthermore, considering that digital twins are software intensive systems, we recognize a signiﬁcant growth potential for adopting more software engineering practices, processes, and expertise in various stages of a digital twin’s lifecycle.",
        "keywords": [
            "Digital twin",
            "Semi-structured interview",
            "Digital twin trends"
        ],
        "authors": [
            "Hossain Muhammad Muctadir",
            "David A. Manrique Negrin",
            "Raghavendran Gunasekaran",
            "Loek Cleophas",
            "Mark van den Brand",
            "Boudewijn R. Haverkort"
        ],
        "file_path": "data/sosym-all/s10270-024-01167-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Searching textual and model-based process descriptions based on a uniﬁed data format",
        "submission-date": "2016/12",
        "publication-date": "2017/12",
        "abstract": "Documenting business processes using process models is common practice in many organizations. However, not all process information is best captured in process models. Hence, many organizations complement these models with textual descriptions that specify additional details. The problem with this supplementary use of textual descriptions is that existing techniques for automatically searching process repositories are limited to process models. They are not capable of taking the information from textual descriptions into account and, therefore, provide incomplete search results. In this paper, we address this problem and propose a technique that is capable of searching textual as well as model-based process descriptions. It automatically extracts activity-related and behavioral information from both descriptions types and stores it in a uniﬁed data format. An evaluation with a large Austrian bank demonstrates that the additional consideration of textual descriptions allows us to identify more relevant processes from a repository.",
        "keywords": [
            "Integrated Process Search",
            "Uniﬁed Data Format",
            "Textual Process Descriptions"
        ],
        "authors": [
            "Henrik Leopold",
            "Han van der Aa",
            "Fabian Pittke",
            "Manuel Raﬀel",
            "Jan Mendling",
            "Hajo A. Reijers"
        ],
        "file_path": "data/sosym-all/s10270-017-0649-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Managing design-time uncertainty",
        "submission-date": "2016/10",
        "publication-date": "2017/03",
        "abstract": "Managing design-time uncertainty, i.e., uncertainty that developers have about making design decisions, requires creation of “uncertainty-aware” software engineering methodologies. In this paper, we propose a methodological approach for managing uncertainty using partial models. To this end, we identify the stages in the lifecycle of uncertainty-related design decisions and characterize the tasks needed to manage it. We encode this information in the Design-Time Uncertainty Management (DeTUM) model. We then use the DeTUM model to create a coherent, tool-supported methodology centred around partial model management. We demonstrate the effectiveness and feasibility of our methodology through case studies.",
        "keywords": [
            "Software methodology",
            "Software modelling",
            "Software design",
            "Design space management",
            "Uncertainty"
        ],
        "authors": [
            "Michalis Famelis",
            "Marsha Chechik"
        ],
        "file_path": "data/sosym-all/s10270-017-0594-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A comparative study of students and professionals in syntactical \nmodel comprehension experiments",
        "submission-date": "2018/03",
        "publication-date": "2019/04",
        "abstract": "Empirical evaluations can be conducted with students or professionals as subjects. Students are much more accessible than \nprofessionals and they are inexpensive, allowing a greater number of empirical studies to be conducted. Professionals are pre-\nferred over students due to concerns regarding the external validity of student-based experiments. Professionals are believed \nto perform differently, most likely better than students. But with respect to evaluating the cognitive effectiveness of software \nengineering notations, are professionals really better? The literature has suggested that the presentation of information is just \nas critical as the content it conveys, hence necessitating this type of empirical studies. If professionals are not much better \nthan students, then such important finding can be a springboard to many much-needed empirical evaluations in this field. In \nthis paper, we report on an experiment that compare the performances of professionals and students with respect to syntac-\ntical model comprehension, which is a core factor for evaluating the cognitive effectiveness of notations. The experiment \ninvolved two groups of professionals and two student groups, totaling 74 professionals and 75 students. The results of the \nexperiment indicate that students can be used as an adequate replacement to professionals in such type of empirical studies.",
        "keywords": [
            "Statechart modeling",
            "Use case modeling",
            "Student-based experiments",
            "Professional-based experiments"
        ],
        "authors": [
            "Mohamed El‑Attar"
        ],
        "file_path": "data/sosym-all/s10270-019-00720-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guaranteed master for interval-based cosimulation",
        "submission-date": "2020/02",
        "publication-date": "2021/01",
        "abstract": "In this paper, we tackle the problem of guaranteed simulation of cyber-physical systems, an important model for current engineering systems. Their is always increasing complexity which leads to models of higher and higher dimensions, yet typically involving multiple subsystems or even multiple physics. Given this modularity, we more precisely explore cosimulation of such dynamical systems, with the aim of reaching higher dimensions of the simulated systems. In this paper, we present a guaranteed interval-based approach for cosimulation of continuous time systems. We propose an algorithm which first proves the existence and returns an enclosure of global solutions, using only local computations. This mitigates the curse of dimensionality faced by global (guaranteed) integration methods. Local computations are then realized with a safe estimate of the other sub-systems until the next macro-step. We increase the accuracy of the approach by using an interval extrapolation of the state of the other sub-systems. We finally propose some possible further improvements including adaptive macro-step size. Our method is fully guaranteed, taking into account all possible sources of error. It is implemented in a C++ prototype relying on the DynIbex library, and we illustrate our approach on multiple examples of the literature.",
        "keywords": [
            "Cosimulation",
            "Guaranteed simulation",
            "Integration methods"
        ],
        "authors": [
            "Adrien Le Coënt",
            "Julien Alexandre dit Sandretto",
            "Alexandre Chapoutot"
        ],
        "file_path": "data/sosym-all/s10270-020-00858-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SAMM: an architecture modeling methodology for ship command and control systems",
        "submission-date": "2012/12",
        "publication-date": "2014/01",
        "abstract": "Ship command and control systems (SCCSs) are composed of large-scale, complex, real-time and software-intensive systems that complete tasks collaboratively. Open architecture has been introduced to design the architecture of SCCSs and has been reﬁned into functional architecture (FA) and technical architecture (TA) to meet architectural requirements such as adapting fast-speed functional and technical changes. Thereby, specifying the architecture of SCCSs, based on FA and TA, becomes a key issue for stakeholders of the domain. In this paper, we propose an architecture modeling methodology (named as SAMM) for describing the architecture of SCCSs. SAMM is derived by following a systematic and generic framework—modeling Goal, domain-speciﬁc Conceptual model, architecture View-point, and architecture description Language (GCVL), which guides domain experts to devise domain-speciﬁc architecture modeling methodologies of large-scale software-intensive systems. SAMM contains three viewpoints and 22 models, and a UML/SysML-based architecture description language. An industrial application of SAMM, along with the subsequent application of the derived SAMM architecture model (i.e., a deployed SCCS prototype) was conducted to evaluate SAMM. A questionnaire-based survey was also conducted to subjectively evaluate whether SAMM meets the modeling goals and its applicability. Results show that SAMM meets all modeling goals and is easy to apply.",
        "keywords": [
            "Architecture modeling",
            "Viewpoint",
            "UML",
            "SysML",
            "Ship command and control systems"
        ],
        "authors": [
            "Zhiqiang Fan",
            "Tao Yue",
            "Li Zhang"
        ],
        "file_path": "data/sosym-all/s10270-013-0393-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal validation of domain-speciﬁc languages with derived features and well-formedness constraints",
        "submission-date": "2014/09",
        "publication-date": "2015/07",
        "abstract": "Despite the wide range of existing tool support, constructing a design environment for a complex domain-speciﬁc language (DSL) is still a tedious task as the large number of derived features and well-formedness constraints complementing the domain metamodel necessitate special handling. Such derived features and constraints are frequently deﬁned by declarative techniques (such graph patterns or OCL invariants). However, for complex domains, derived features and constraints can easily be formalized incorrectly resulting in inconsistent, incomplete or ambigu-ous DSL speciﬁcations. To detect such issues, we propose an automated mapping of EMF metamodels enriched with derived features and well-formedness constraints captured as graph queries in EMF- IncQuery or (a subset of) OCL invariants into an effectively propositional fragment of ﬁrst-order logic which can be efﬁciently analyzed by back-end reasoners. On the conceptual level, the main added value of our encoding is (1) to transform graph patterns of the EMF- IncQuery framework into FOL and (2) to introduce approximations for complex language features (e.g., transi-tive closure or multiplicities) which are not expressible in FOL. On the practical level, we identify and address relevant challenges and scenarios for systematically validating DSL speciﬁcations. Our approach is supported by a tool, and it will be illustrated on analyzing a DSL in the avionics domain. We also present initial performance experiments for the validation using Z3 and Alloy as back-end reasoners.",
        "keywords": [
            "Language validation",
            "Derived features",
            "Partial snapshots",
            "Model queries",
            "SMT solvers"
        ],
        "authors": [
            "Oszkár Semeráth",
            "Ágnes Barta",
            "Ákos Horváth",
            "Zoltán Szatmári",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-015-0485-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework to specify system requirements using natural interpretation of UML/MARTE diagrams",
        "submission-date": "2016/03",
        "publication-date": "2017/03",
        "abstract": "The ever-increasing design complexity of embedded systems is constantly pressing the demand for more abstract design levels and possible methods for automatic veriﬁcation and synthesis. Transforming a text-based user requirements document into semantically sound models is always difﬁcult and error-prone as mostly these requirements are vague and improperly documented. This paper presents a framework to specify textual requirements graphically in standard modeling formalisms like uml and marte in the form of temporal and logical patterns. The underlying formal semantics of these graphical models allow to eliminate ambiguity in speciﬁcations and automatic design veriﬁcation at different abstraction levels using these patterns. The semantics of these operators/patterns are presented formally as state automatons and a comparison is made to the existing ccsl relational operators. To reap the beneﬁts of mde, a software plugin TemLoPAC is presented as part of the framework to transform the graphical patterns into ccsl and Verilog-based observers.",
        "keywords": [
            "FSL",
            "Graphical properties",
            "UML",
            "MARTE",
            "CCSL",
            "Modeling",
            "Embedded systems"
        ],
        "authors": [
            "Aamir M. Khan",
            "Frédéric Mallet",
            "Muhammad Rashid"
        ],
        "file_path": "data/sosym-all/s10270-017-0588-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "CCSL"
        }
    },
    {
        "title": "EDITORIAL",
        "submission-date": "2025/05",
        "publication-date": "2025/05",
        "abstract": "As we present this issue of the Journal of Software and Systems Modeling (SoSyM), we take a moment to acknowledge and celebrate the remarkable contributions of Professor Antonio Vallecillo. After decades of dedication to the field, Antonio is now retiring, leaving behind an impressive amount of results in software and systems modeling. He has been a cornerstone of our journal, serving as an editor almost from the very beginning of SoSyM’s 25 year journey. He was always helpful as editor, advisor, and friend. His expertise, vision, and continuous commitment have significantly shaped not only the special section presented in this issue, but also the broader research community.",
        "keywords": [],
        "authors": [
            "Antonio Vallecillo",
            "Stéphanie Challita",
            "Marsha Chechik",
            "Benoit Combemale",
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-025-01290-5.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "VbTrace: using view-based and model-driven development to support traceability in process-driven SOAs",
        "submission-date": "2009/01",
        "publication-date": "2009/11",
        "abstract": "In process-driven, service-oriented architectures, there are a number of important factors that hinder the trace-ability between design and implementation artifacts. First of all, there are no explicit links between process design and implementation languages not only due to the differences of syntax and semantics but also the differences of granularity. The second factor is the complexity caused by tangled process concerns that multiplies the difﬁculty of analyzing and understanding the trace dependencies. Finally, there is a lack of adequate tool support for establishing and maintaining the trace dependencies between process designs and implementations. We present in this article a view-based, model-driven traceability approach that tackles these chal-lenges. Our approach supports (semi-)automatically eliciting and (semi-)formalizing trace dependencies among process development artifacts at different levels of granularity and abstraction. A proof-of-concept tool support has been real-ized, and its functionality is illustrated via an industrial case study.",
        "keywords": [
            "Software traceability",
            "View-based",
            "Model-driven",
            "Process-driven SOA",
            "Tool support"
        ],
        "authors": [
            "Huy Tran",
            "Uwe Zdun",
            "Schahram Dustdar"
        ],
        "file_path": "data/sosym-all/s10270-009-0137-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Event-driven temporal models for explanations - ETeMoX: explaining reinforcement learning",
        "submission-date": "2021/04",
        "publication-date": "2021/12",
        "abstract": "Modern software systems are increasingly expected to show higher degrees of autonomy and self-management to cope with uncertain and diverse situations. As a consequence, autonomous systems can exhibit unexpected and surprising behaviours. This is exacerbated due to the ubiquity and complexity of Artiﬁcial Intelligence (AI)-based systems. This is the case of Reinforcement Learning (RL), where autonomous agents learn through trial-and-error how to ﬁnd good solutions to a problem. Thus, the underlying decision-making criteria may become opaque to users that interact with the system and who may require explanations about the system’s reasoning. Available work for eXplainable Reinforcement Learning (XRL) offers different trade-offs: e.g. for runtime explanations, the approaches are model-speciﬁc or can only analyse results after-the-fact. Different from these approaches, this paper aims to provide an online model-agnostic approach for XRL towards trustworthy and understandable AI. We present ETeMoX, an architecture based on temporal models to keep track of the decision-making processes of RL systems. In cases where the resources are limited (e.g. storage capacity or time to response), the architecture also integrates complex event processing, an event-driven approach, for detecting matches to event patterns that need to be stored, instead of keeping the entire history. The approach is applied to a mobile communications case study that uses RL for its decision-making. In order to test the generalisability of our approach, three variants of the underlying RL algorithms are used: Q-Learning, SARSA and DQN. The encouraging results show that using the proposed conﬁgurable architecture, RL developers are able to obtain explanations about the evolution of a metric, relationships between metrics, and were able to track situations of interest happening over time windows.",
        "keywords": [
            "Temporal models",
            "Complex event processing",
            "Artiﬁcial intelligence",
            "Explainable reinforcement learning",
            "Event-driven monitoring"
        ],
        "authors": [
            "Juan Marcelo Parra-Ullauri",
            "Antonio García-Domínguez",
            "Nelly Bencomo",
            "Changgang Zheng",
            "Chen Zhen",
            "Juan Boubeta-Puig",
            "Guadalupe Ortiz",
            "Shufan Yang"
        ],
        "file_path": "data/sosym-all/s10270-021-00952-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extract, model, reﬁne: improved modelling of program veriﬁcation tools through data enrichment",
        "submission-date": "2023/05",
        "publication-date": "2025/01",
        "abstract": "In software engineering, models are used for many different things. In this paper, we focus on program veriﬁcation, where we use models to reason about the correctness of systems. There are many different types of program veriﬁcation techniques which provide different correctness guarantees. We investigate the domain of program veriﬁcation tools and present a concise megamodel to distinguish these tools. We also present a data set of 400+ program veriﬁcation tools. This data set includes the category of veriﬁcation tool according to our megamodel, practical information such as input/output format, repository links and more. The practical information, such as last commit date, is kept up to date through the use of APIs. Moreover, part of the data extraction has been automated to make it easier to expand the data set. The categorisation enables software engineers to ﬁnd suitable tools, investigate alternatives and compare tools. We also identify trends for each level in our megamodel. Our data set, publicly available at https://doi.org/10.4121/20347950, can be used by software engineers to enter the world of program veriﬁcation and ﬁnd a veriﬁcation tool based on their requirements. This paper is an extended version of https://doi.org/10.1145/3550355.3552426.",
        "keywords": [
            "Program veriﬁcation",
            "Megamodelling",
            "Data enrichment",
            "Data extraction"
        ],
        "authors": [
            "Sophie Lathouwers",
            "Yujie Liu",
            "Vadim Zaytsev"
        ],
        "file_path": "data/sosym-all/s10270-024-01232-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Stress-testing remote model querying APIs for relational and graph-based stores",
        "submission-date": "2016/11",
        "publication-date": "2017/06",
        "abstract": "Recent research in scalable model-driven engineering now allows very large models to be stored and queried. Due to their size, rather than transferring such models over the network in their entirety, it is typically more efficient to access them remotely using networked services (e.g. model repositories, model indexes). Little attention has been paid so far to the nature of these services, and whether they remain responsive with an increasing number of concurrent clients. This paper extends a previous empirical study on the impact of certain key decisions on the scalability of concurrent model queries on two domains, using an Eclipse Connected Data Objects model repository, four configurations of the Hawk model index and a Neo4j-based configuration of the NeoEMF model store. The study evaluates the impact of the network protocol, the API design, the caching layer, the query language and the type of database and analyses the reasons for their varying levels of performance. The design of the API was shown to make a bigger difference compared to the network protocol (HTTP/TCP) used. Where available, the query-specific indexed and derived attributes in Hawk outperformed the comprehensive generic caching in CDO. Finally, the results illustrate the still ongoing evolution of graph databases: two tools using different versions of the same backend had very different performance, with one slower than CDO and the other faster than it.",
        "keywords": [
            "Model persistence",
            "Remote model querying",
            "NoSQL storage",
            "Relational databases",
            "API design",
            "Stress testing",
            "Collaborative modelling"
        ],
        "authors": [
            "Antonio Garcia-Dominguez",
            "Konstantinos Barmpis",
            "Dimitrios S. Kolovos",
            "Ran Wei",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-017-0606-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Simulation and analysis of MultEcore multilevel models based on rewriting logic",
        "submission-date": "2020/05",
        "publication-date": "2021/11",
        "abstract": "Multilevel modelling (MLM) approaches make it possible for designers and modellers to work with an unlimited number of abstraction levels when specifying domain-speciﬁc modelling languages (DSMLs). In this paper, we present a functional infrastructure that allows modellers to deﬁne the structure and the operational semantics of multilevel modelling hierarchies, enabling simulation and analysis. Using the MultEcore tool, one can design and distribute the models that compose the language family in a multilevel hierarchy, and specify their behaviour by means of multilevel transformation, so-called multilevel coupled model transformations. We give a rewrite logic semantics to MultEcore’s MLM, on which we have based our automated transformation from MultEcore to the rewriting logic language Maude. Then, we rely on Maude to execute MultEcore models and to support analysis techniques, like reachability analysis, bounded and unbounded model checking of invariants and LTL formulas on systems with both ﬁnite and inﬁnite reachable state spaces using equational abstraction. We illustrate our developed techniques on a DSML family for Petri nets.",
        "keywords": [
            "Multilevel modelling",
            "Domain-speciﬁc modelling languages",
            "Model transformations",
            "Veriﬁcation",
            "Rewriting logic",
            "Maude"
        ],
        "authors": [
            "Alejandro Rodríguez",
            "Francisco Durán",
            "Lars Michael Kristensen"
        ],
        "file_path": "data/sosym-all/s10270-021-00947-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Maude"
        }
    },
    {
        "title": "Büchi automata for modeling component connectors",
        "submission-date": "2009/03",
        "publication-date": "2010/03",
        "abstract": "Reo is an exogenous coordination language for component connectors extending data ﬂow networks with synchronization and context-dependent behavior. The ﬁrst proposed formalism to capture the operational semantics of Reo is called constraint automaton. In this paper, we propose another operational model of Reo based on Büchi automata in which port synchronization is modeled by records labeling the transitions, whereas context dependencies are stored in the states. It is shown that constraint automata can be recast into our proposed Büchi automata of records. Also, we provide a composition operator which models the joining of two connectors and show that it can be obtained by using two standard operators: alphabet extension and automata product. Our semantics has the advantage over previous models in that it is based on standard automata theory, so that existing theories and tools can be easily reused. Moreover, it is the ﬁrst formal model addressing all of Reo’s features: synchronization, mutual exclusion, hiding, and context-dependency.",
        "keywords": [
            "Büchi automata",
            "Component connector",
            "Reo",
            "Constraint automata"
        ],
        "authors": [
            "Mohammad Izadi",
            "Marcello Bonsangue",
            "Dave Clarke"
        ],
        "file_path": "data/sosym-all/s10270-010-0152-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction: A framework for embedded software portability and veriﬁcation: from formal models to low-level code",
        "submission-date": "2024/04",
        "publication-date": "2024/04",
        "abstract": "In the original publication the paper mentions \"BlindReviewOS\" in several locations. The real name of the software is \"SmartOS\".\nThis has been corrected in the original publication.",
        "keywords": [],
        "authors": [
            "Renata Martins Gomes",
            "Bernhard Aichernig",
            "Marcel Baunach"
        ],
        "file_path": "data/sosym-all/s10270-024-01166-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Theme section on modeling and sustainability",
        "submission-date": "2025/02",
        "publication-date": "2025/03",
        "abstract": "Sustainability is becoming a key property of modern systems. Widespread digitalization and the accelerating pathways of digital technology have opened unprecedented opportunities for humankind to build more efﬁcient, robust, and performant systems. Unfortunately, the beneﬁts of elaborate digital technology come at the cost of an increased environmental footprint, unclear social impact, and increased maintenance efforts. Current systems’ engineering practices fall short of adequately addressing these sustainability challenges due to the highly multi-systemic and stratiﬁed nature of sustainability and need to be revised appropriately. Modeling offers numerous beneﬁts in understanding and assessing the sustainability properties of engineered systems. Modeling languages and tools support subject matter experts in expressing their views—process models allow for reasoning about trade-offs across the end-to-end systems’ engineering process, while runtime models allow for controlling engineering endeavors for sustainability. These are just a few of the many ways to support sustainability ambitions by modeling. This theme section invited researchers and practitioners in various areas of modeling and simulation to spotlight their recent innovative research results. We hope that this theme section will serve as an informative piece for anyone interested in modeling and sustainability, as well as motivate fellow researchers to devote their valuable time to the growing problem of unsustainable practices in technology and engineering.",
        "keywords": [],
        "authors": [
            "Istvan David",
            "Ankica Bariši´c",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-025-01279-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML 3.0 and the future of modeling",
        "submission-date": "2003/05",
        "publication-date": "2004/02",
        "abstract": "The major revision work for UML 2.0 is complete, and it is now an OMG Final Adopted Speciﬁcation. This is a good time to reﬂect on UML’s future, and the future of model-driven development.",
        "keywords": [],
        "authors": [
            "Cris Kobryn"
        ],
        "file_path": "data/sosym-all/s10270-004-0051-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Heap-abstraction for an object-oriented calculus with thread classes",
        "submission-date": "2006/02",
        "publication-date": "2007/08",
        "abstract": "This paper formalizes an open semantics for a calculus featuring thread classes, where the environment, consisting in particular of an overapproximation of the heap topology, is abstractly represented. From an observational point of view, considering classes as part of a component makes instantiation a possible interaction between component and environment or observer. For thread classes it means that a component may create external activity, which influences what can be observed. The fact that cross-border instantiation is possible requires that the connectivity of the objects needs to be incorporated into the semantics. We extend our prior work not only by adding thread classes, but also in that thread names may be communicated, which means that the semantics needs to account explicitly for the possible acquaintance of objects with threads. We show soundness of the abstraction.",
        "keywords": [
            "Class-based OO languages",
            "Thread-based concurrency",
            "Open systems",
            "Formal semantics",
            "Heap abstraction",
            "Observable behavior"
        ],
        "authors": [
            "Erika Ábrahám",
            "Andreas Grüner",
            "Martin Steffen"
        ],
        "file_path": "data/sosym-all/s10270-007-0065-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Five years of modeling in SoSyM",
        "submission-date": "2006/11",
        "publication-date": "2006/11",
        "abstract": "Since its inception in 2002 the International Journal on Software and Systems Modeling (SoSyM) has become a major source of quality papers describing research and experience related to building and using models in the development of software-based systems. Papers published in SoSyM cover many aspects of software development; from early business requirements modeling and analysis through system architecting to quality management, maintenance and evolution of software. However, in practice, models are primarily used in two ways: as informal descriptions of concepts to facilitate discussion (e.g., using the UML as a sketching notation), and as bases for generating code (e.g., generating code using model frameworks). Research on model-driven development (MDD) indicates that models can be better leveraged during development. However, there is a need to perform more foundational research and empirical studies to fully understand how the MDD vision of software development can be realized. The journal will continue to play a vital role in nurturing and advancing high quality research in MDD through the publication of results from high quality empirical studies, and of successful or highly promising approaches that address various aspects of MDD.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-006-0035-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Agile model-based system development",
        "submission-date": "2018/08",
        "publication-date": "2018/08",
        "abstract": "Our journal is called “Software and Systems Modeling,” but when we look at most of the papers that have been published in SoSyM recently, the focus is much more on software than on other kinds of systems. It is worth a fresh look at how other engineering disciplines use models. In fact, modeling is at the heart of almost any engineering discipline. Thus, it is not surprising that our engineering colleagues have developed a detailed portfolio of modeling techniques to describe their systems in various perspectives, viewpoints, and abstractions. Furthermore, a plethora of tools has been developed to assist practitioners with each of the individual modeling languages. Colleagues from traditional engineering disciplines model many more aspects of their systems than software developers. However, the state-of-the-art in systems modeling has several challenges, where each modeling aspect and view is often assisted by an individual modeling and analysis tool. Data exchange between the tools is complicated, even though mostly automated, but suffers from concerns about robustness, completeness of the mappings between the models, as well as regular version upgrades of tools. A second problem is that these mappings between models are not easy to standardize, because different projects use the same modeling languages in different forms (semantics), which enforces configurable mappings or individually developed translations per project. We have discussed these aspects on modeling partially in previous editorials (please see http://www.sosym.org/editorials/). An important aspect of a modeling toolchain concerns the “length” of the toolchain—the longer the toolchain, the more information that may be added along the toolchain, potentially reducing the agility of the development process. Agility is a well-known software development technique that dominates smaller- and medium-sized software projects. Many projects have demonstrated that an agile, lean development process has merits in terms of cost reduction, quality, and time to market. Driven by the large software companies in Silicon Valley that have a development and innovation pace far beyond traditional engineering, there are currently several attempts to adapt agility to systems development, with examples including smartphones, autonomous and electrified cars, and also medical devices and home automation. Therefore, it is worthwhile to revisit the most important ingredients required by an agile process, which are: Lean processes with as little documentation overhead as possible, Immediate feedback as and early as possible, Feedback on all activities, Automation of many development tasks, Many, small iterations, Self-responsibility for developers to do whatever seems best at the current situation. Traditional engineers use system models to design and understand the product through analysis and automation of engineering tasks. Therefore, models should be more than just documentation artifacts. Each model that captures a perspective of the project should either become a part of the product construction or should be used for automated validation and test quality management. Although synthesizing software products from models is a common practice in the software engineering domain, physical products can also be produced (e.g., with appropriate 3D printers). In systems modeling, simulations of the physical product in various abstractions and for various validation and testing purposes can be a core benefit of modeling. Simulations help to understand product sustainability (e.g., deterioration), usability of a product in its context (e.g., autonomous driving), and many additional aspects that are more difficult to explore as “what-if” analyses without modeling support. The benefit of models and simulation occurs when validation steps are available on very early versions of a model and not only on a complete",
        "keywords": [],
        "authors": [
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-018-0694-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated resolution of connector architectures using constraint solving (ARCAS method)",
        "submission-date": "2011/08",
        "publication-date": "2012/09",
        "abstract": "In current software systems, connectors play an important role by encapsulating the communication and coordination logic. Since they share common patterns (ele-ments) depending on characteristics of the connections, the elements can be predeﬁned and reused. A method of connector implementation based on a composition of predeﬁned elementsnaturallycomprisestwosteps:resolutionofthecon-nector architecture, and creation of the actual connector code based on the architecture. However, manual resolution of a connector architecture is very difﬁcult due to the number of factors to be considered. Thus, the challenge is to come up with an automated method, able to address all the important factors. In this paper, we present a method for automated res-olutionofconnectorarchitecturesbasedonconstraintsolving techniques. We exploit a propositional logic with relational calculus for deﬁning a connector theory, a constraint speciﬁ-cation reﬂecting both the predeﬁned parts and the important resolution factors, and employ a constraint solver to ﬁnd a suitable connector architecture as a model of the theory. As a proof of the concept, we show how the theory can be captured in the Alloy language and resolved via the Alloy Analyzer.",
        "keywords": [
            "Software architecture",
            "Software connectors",
            "Constraint solving",
            "Middleware-based connectors",
            "Connector theory",
            "Alloy"
        ],
        "authors": [
            "Jaroslav Keznikl",
            "Tomáš Bureš",
            "František Plášil",
            "Petr Hnˇetynka"
        ],
        "file_path": "data/sosym-all/s10270-012-0274-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Accelerating similarity-based model matching using dual hashing",
        "submission-date": "2023/04",
        "publication-date": "2024/04",
        "abstract": "Similarity-based model matching is the cornerstone of model versioning. It pairs model elements based on a distance metric (e.g., edit distance). However, calculating the distances between elements is computationally expensive. Consequently, a similarity-based matcher typically suffers from performance issues when the model size increases. Based on observation, there are two main causes of the high computation cost: (1) when matching an element p, the matcher calculates the distance between p and every candidate element q, despite the obvious dissimilarity between p and q; (2) the matcher always calculates the distance between p and q′, even though q and q′ are very similar and the distance between p and q is already known. This paper proposes a dual-hash-based approach, which employs two entirely different hashing techniques—similarity-preserving hashing and integrity-based hashing—to accelerate similarity-based model matching. With similarity-preserving hashing, our approach can quickly ﬁlter out the dissimilar candidate elements according to their similarity hashes computed using our similarity-preserving hash function, which maps an element to a 64-bit binary hash. With integrity-based hashing, our approach can cache and reuse computed distance values by associating them with the checksums of model elements. We also propose an index structure to facilitate hash-based model matching. Our approach has been implemented and integrated into EMF Compare. We evaluate our approach using open-source Ecore and UML models. The results show that our hash function is effective in preserving the similarity between model elements and our matching approach reduces time costs by 20–88% while assuring the matching results consistent with EMF Compare.",
        "keywords": [
            "Model matching",
            "Similarity-preserving hashing",
            "Integrity-based hashing",
            "Edit distance"
        ],
        "authors": [
            "Xiao He",
            "Yi Liu",
            "Huihong He"
        ],
        "file_path": "data/sosym-all/s10270-024-01173-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The 8th Annual State of SoSyM Report",
        "submission-date": "2009/11",
        "publication-date": "2009/11",
        "abstract": "Another year has gone by and we are happy to report that the International Journal on Software and Systems Modeling (SoSyM) is doing well. As we have done on previous anniversaries, we take this opportunity to give a “state of the journal” report and to acknowledge the reviewers, editors, and publication staff who have contributed to the journal’s very good performance in the past year.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-009-0143-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Introduction",
        "submission-date": "2003/09",
        "publication-date": "2005/05",
        "abstract": "Speciﬁcations of complex systems are usually based on either states or actions (events). Some people believe that the ﬁrst step in system development should be the identiﬁcation of the right global state structure. Other people start characterizing the system by describing its interactions with the environment, that is, by identifying event patterns (e.g. use cases), without worrying, at least initially, about the shape of the state. Formal speciﬁcation approaches such as Abstract State Machines, B, CSP, LOTOS, Predicate/Transition Nets, Statecharts, TLA, Z, imply a considerable bias towards one of these two ways of conceiving system behaviour.",
        "keywords": [],
        "authors": [
            "Tommaso Bolognesi",
            "John Derrick"
        ],
        "file_path": "data/sosym-all/s10270-005-0082-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven design space exploration for multi-robot systems in simulation",
        "submission-date": "2022/04",
        "publication-date": "2022/10",
        "abstract": "Multi-robot systems are increasingly deployed to provide services and accomplish missions whose complexity or cost is too high for a single robot to achieve on its own. Although multi-robot systems offer increased reliability via redundancy and enable the execution of more challenging missions, engineering these systems is very complex. This complexity affects not only the architecture modelling of the robotic team but also the modelling and analysis of the collaborative intelligence enabling the team to complete its mission. Existing approaches for the development of multi-robot applications do not provide a systematic mechanism for capturing these aspects and assessing the robustness of multi-robot systems. We address this gap by introducing ATLAS, a novel model-driven approach supporting the systematic design space exploration and robustness analysis of multi-robot systems in simulation. The ATLAS domain-speciﬁc language enables modelling the architecture of the robotic team and its mission and facilitates the speciﬁcation of the team’s intelligence. We evaluate ATLAS and demonstrate its effectiveness in three simulated case studies: a healthcare Turtlebot-based mission and two unmanned underwater vehicle missions developed using the Gazebo/ROS and MOOS-IvP robotic platforms, respectively.",
        "keywords": [
            "MRS",
            "Multi-robot systems",
            "Model-driven engineering",
            "MDE",
            "Simulation",
            "Design-space exploration"
        ],
        "authors": [
            "James Harbin",
            "Simos Gerasimou",
            "Nicholas Matragkas",
            "Thanos Zolotas",
            "Radu Calinescu",
            "Misael Alpizar Santana"
        ],
        "file_path": "data/sosym-all/s10270-022-01041-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated conceptual model clustering: a relator-centric approach",
        "submission-date": "2021/03",
        "publication-date": "2021/09",
        "abstract": "In recent years, there has been a growing interest in the use of reference conceptual models to capture information about complex and sensitive business domains (e.g., ﬁnance, healthcare, space). These models play a fundamental role in different types of critical semantic interoperability tasks. Therefore, domain experts must be able to understand and reason with their content. In other words, these models need to be cognitively tractable. This paper contributes to this goal by proposing a model clustering technique that leverages on the rich semantics of ontology-driven conceptual models (ODCM). In particular, we propose a formal notion of Relational Context to guide the automated clusterization (or modular breakdown) of conceptual models. Such Relational Contexts capture all the information needed for understanding entities “qua players of roles” in the scope of an objectiﬁed (reiﬁed) relationship (relator). The paper also presents computational support for automating the identiﬁcation of Relational Contexts and this modular breakdown procedure. Finally, we report the results of an empirical study assessing the cognitive effectiveness of this approach.",
        "keywords": [
            "Ontology-driven conceptual modeling",
            "Complexity management in conceptual modeling",
            "Conceptual model clustering",
            "OntoUML"
        ],
        "authors": [
            "Giancarlo Guizzardi",
            "Tiago Prince Sales",
            "João Paulo A. Almeida",
            "Geert Poels"
        ],
        "file_path": "data/sosym-all/s10270-021-00919-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synthesis and exploration of multi-level, multi-perspective architectures of automotive embedded systems",
        "submission-date": "2016/08",
        "publication-date": "2017/04",
        "abstract": "In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today’s engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multi-level, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of question templates, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates relevant to their task at hand. Finally, using the modeling language Clafer, which is supported by automated backend reasoners, we show that it is possible to synthesize and explore optimal candidate architectures for two highly configurable automotive sub-systems.",
        "keywords": [
            "Architecture synthesis",
            "Multi-level architectures",
            "Multi-perspective architectures",
            "E/E architecture",
            "Architecture optimization",
            "Candidate architectures",
            "Early design"
        ],
        "authors": [
            "Jordan A. Ross",
            "Alexandr Murashkin",
            "Jia Hui Liang",
            "Michał Antkiewicz",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-017-0592-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven performance prediction of systems of systems",
        "submission-date": "2015/07",
        "publication-date": "2016/07",
        "abstract": "Systems of systems exhibit characteristics that pose difficulty in modelling and predicting their overall performance capabilities, including the presence of operational independence, emergent behaviour, and evolutionary development. When considering systems of systems within the autonomous defence systems context, these aspects become increasingly critical, as constraints on the performance of the final system are typically driven by hard constraints on space, weight and power. System execution modelling languages and tools permit early prediction of the performance of model-driven systems; however, the focus to date has been on understanding the performance of a model rather than determining whether it meets performance requirements, and only subsequently carrying out analysis to reveal the causes of any requirement violations. Moreover, such an analysis is even more difficult when applied to several systems cooperating to achieve a common goal—a system of systems. In this article, we propose an integrated approach to performance prediction of model-driven real-time embedded defence systems and systems of systems. Our architectural prototyping system supports a scenario-driven experimental platform for evaluating model suitability within a set of deployment and real-timeperformanceconstraints.Wepresentanoverviewof our performance prediction system, demonstrating the integration of modelling, execution and performance analysis, and discuss a case study to illustrate our approach.",
        "keywords": [
            "Performance prediction",
            "Systems of systems",
            "Model-driven engineering",
            "System execution modelling"
        ],
        "authors": [
            "Katrina Falkner",
            "Claudia Szabo",
            "Vanea Chiprianov",
            "Gavin Puddy",
            "Marianne Rieckmann",
            "Dan Fraser",
            "Cathlyn Aston"
        ],
        "file_path": "data/sosym-all/s10270-016-0547-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Use, potential, and showstoppers of models in automotive requirements engineering",
        "submission-date": "2017/09",
        "publication-date": "2018/05",
        "abstract": "Several studies report that the use of model-centric methods in the automotive domain is widespread and offers several beneﬁts. However, existing work indicates that few modelling frameworks explicitly include requirements engineering (RE), and that natural language descriptions are still the status quo in RE. Therefore, we aim to increase the understanding of current and potential future use of models in RE, with respect to the automotive domain. In this paper, we report our ﬁndings from a multiple-case study with two automotive companies, collecting interview data from 14 practitioners. Our results show that models are used for a variety of different purposes during RE in the automotive domain, e.g. to improve communication and to handle complexity. However, these models are often used in an unsystematic fashion and restricted to few experts. A more widespread use of models is prevented by various challenges, most of which align with existing work on model use in a general sense. Furthermore, our results indicate that there are many potential beneﬁts associated with future use of models during RE. Interestingly, existing research does not align well with several of the proposed use cases, e.g. restricting the use of models to informal notations for communication purposes. Based on our ﬁndings, we recommend a stronger focus on informal modelling and on using models for multi-disciplinary environments. Additionally, we see the need for future work in the area of model use, i.e. information extraction from models by non-expert modellers.",
        "keywords": [
            "Modelling",
            "MDE",
            "MBE",
            "Requirements engineering",
            "Empirical research",
            "Case study",
            "Automotive"
        ],
        "authors": [
            "Grischa Liebel",
            "Matthias Tichy",
            "Eric Knauss"
        ],
        "file_path": "data/sosym-all/s10270-018-0683-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML vs. classical vs. rhapsody statecharts: not all models are created equal",
        "submission-date": "2006/01",
        "publication-date": "2007/01",
        "abstract": "State machines, represented by statecharts or state machine diagrams, are an important formalism for behavioural modelling. According to the research literature, the most popular statechart formalisms appear to be Classical, UML, and that implemented by Rhapsody. These three formalisms seem to be very similar; however, there are several key syntactic and semantic differences. These differences are enough that a model written in one formalism could be ill-formed in another formalism. Worse, a model from one formalism might actually be well-formed in another, but be interpreted differently due to the semantic differences. This paper summarizes the results of an informal comparative study of these three formalisms with the help of several illustrativeexamples. Wepresent aclassiﬁcationof thediffer-ences according to the nature of potential problems caused by each difference. In addition, for each differ-ence we discuss how translation between formalisms can be achieved, if at all.",
        "keywords": [],
        "authors": [
            "Michelle L. Crane",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-006-0042-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "VPM: A visual, precise and multilevel metamodeling framework for describing mathematical domains and UML",
        "submission-date": "2003/02",
        "publication-date": "2003/08",
        "abstract": "As UML 2.0 is evolving into a family of languages with individually speciﬁed semantics, there is an increasing need for automated and provenly correct model transformations that (i) assure the integration of local views (diﬀerent diagrams) of the system into a consistent global view, and, (ii) provide a well-founded mapping from UML models to diﬀerent semantic domains (Petri nets, Kripke automaton, process algebras, etc.) for formal analysis purposes as foreseen, for instance, in submissions for the OMG RFP for Schedulability, Per-formance and Time. However, such transformations into diﬀerent semantic domains typically require the deep understanding of the underlying mathematics, which hinders the use of formal speciﬁcation techniques in industrial applications. In the paper, we propose a multilevel metamodeling technique with precise static and dynamic semantics (based on a reﬁnement calculus and graph transformation) where the structure and operational semantics of mathematical models can be deﬁned in a UML notation without cumbersome mathematical formulae.",
        "keywords": [
            "Metamodeling",
            "Formal semantics",
            "Reﬁnement",
            "Model transformation",
            "Graph transformation"
        ],
        "authors": [
            "D´aniel Varr´o",
            "Andr´as Pataricza"
        ],
        "file_path": "data/sosym-all/s10270-003-0028-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Universal conceptual modeling: principles, beneﬁts, and an agenda for conceptual modeling research",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "The paper proposes universal conceptual modeling, conceptual modeling that strives to be as general-purpose as possible and accessible to anyone, professionals and non-experts alike. The idea of universal conceptual modeling is meant to catalyze new thinking in conceptual modeling and be used to evaluate and develop conceptual modeling solutions, such as modeling languages, approaches for requirements elicitation, or modeling tools. These modeling solutions should be usable by as many people and design agents as possible and for as many purposes as possible, aspiring to the ideals of universal conceptual model-ing. We propose foundations of universal conceptual modeling in the form of six principles: ﬂexibility, accessibility, ubiquity, minimalism, primitivism, and modularity. We then demonstrate the utility of these principles to evaluate existing conceptual modeling languages and understand conceptual modeling practices. Finally, we propose future research opportunities meant to realize the ideals of universal conceptual modeling.",
        "keywords": [
            "Universal conceptual modeling",
            "Conceptual modeling",
            "Conceptual modeling foundations",
            "Universal modeling language",
            "Datish",
            "RDF",
            "Inclusive modeling",
            "Universal design"
        ],
        "authors": [
            "Roman Lukyanenko",
            "Binny M. Samuel",
            "Jeﬀrey Parsons",
            "Veda C. Storey",
            "Oscar Pastor",
            "Araz Jabbari"
        ],
        "file_path": "data/sosym-all/s10270-024-01207-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evolution styles: foundations and models for software architecture evolution",
        "submission-date": "2011/03",
        "publication-date": "2012/11",
        "abstract": "As new market opportunities, technologies, platforms, and frameworks become available, systems require large-scale and systematic architectural restructuring to accommodate them. Today’s architects have few techniques to help them plan this architecture evolution. In particular, they have little assistance in planning alternative evolution paths, trading off various aspects of the different paths, or knowing best practices for particular domains. In this paper, we describe an approach for planning and reasoning about architecture evolution. Our approach focuses on providing architects with the means to model prospective evolution paths and supporting analysis to select among these candidate paths. To demonstrate the usefulness of our approach, we show how it can be applied to an actual architecture evolution. In addition, we present some theoretical results about our evolution path constraint specification language.",
        "keywords": [
            "Software architecture"
        ],
        "authors": [
            "Jeffrey M. Barnes",
            "David Garlan",
            "Bradley Schmerl"
        ],
        "file_path": "data/sosym-all/s10270-012-0301-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A reference architecture for the development of GLSP-based web modeling tools",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "Web-based modeling tools provide unprecedented opportunities for the realization of modern, powerful, and usable diagram editors running in the cloud. The development of such tools, however, still poses signiﬁcant challenges for developers. The graphical language server platform (GLSP) aims to reduce some of these challenges by providing the necessary frameworks to efﬁciently create web modeling tools. However, realizing modeling tools with GLSP remains challenging and not much support for interested tool developers is provided yet. This paper discusses these challenges and lessons learned after working with GLSP and realizing several GLSP-based modeling tools. We present experiences, concepts, and a reusable reference architecture to develop and operate GLSP-based web modeling tools. As a proof of concept, we report on the realization of a GLSP-based UML editor called bigUML. Through bigUML, we show that our procedure and the reference architecture we developed resulted in a scalable and ﬂexible GLSP-based web modeling tool for the UML. The lessons learned, the procedural approach, the reference architecture, and the critical reﬂection on the challenges and opportunities of using GLSP provide valuable insights to the community and shall ease the decision of whether or not to use GLSP for future tool development projects. With this paper, we publicly release a reference implementation of our architecture.",
        "keywords": [
            "UML",
            "Software modeling",
            "GLSP",
            "Modeling tool",
            "Web modeling",
            "LSP"
        ],
        "authors": [
            "Haydar Metin",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-024-01257-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Software engineering methods in other engineering disciplines",
        "submission-date": "2018/04",
        "publication-date": "2018/04",
        "abstract": "Software engineers are often told from experts in systems development that they “should develop their software in the same systematic and predictable way as other engineers (e.g., mechanical engineers)”. If we trace Software Engineering back to the 1968 NATO Science Committee [1] conference in Garmisch, Germany, organized by F. L. Bauer, then the discipline of Software Engineering is approaching 50 years old. Yet, Software Engineering is not a discipline that always develops products using processes similar to traditional mechanical engineering methods. Software developers have their own portfolio of methods, ranging from heavyweight documentation-oriented approaches to the much more beloved agile and light-weight methods. Even though there is always potential for optimization, agile techniques have reached a level where software developers can produce reliable products using cost efficient, relatively predictable and controllable processes. What seems to be even more important is that the way of developing software is tightly connected to innovative processes that not only lead to novel ideas about how to implement software, but also to new ideas about potential features and services that can be integrated into software solutions. The tight coupling of software development and innovation is closely related to the strong connection between requirements elicitation and direct implementation in agile processes, where the same stakeholders are responsible for the innovations of development. Furthermore, the invention of new control and data structures in object-oriented development can drive the innovation of new features. Companies in Silicon Valley have used this mood of innovation effectively. They have integrated software services into their core business model, which has led to innovative thinking and effective, agile development of products in other domains (e.g., medical services, autonomous driving, electric cars, finance). Traditional companies based on fundamental engineering processes may feel threatened by the radical change and fast rate of innovation that the new technologies offer. There are challenges in training classical engineers to adopt a new form of project organization, where the responsibilities are given to the developers much more than to the management hierarchies. However, this mindset is necessary to improve innovation. Traditional engineering-based companies, as well as other increasingly software-intensive companies, are now trying to catch up. Many good examples in these areas give us hope that change is possible. In systems that require expertise from multiple areas of engineering,traditional engineers are becoming more open to software engineering practices. Furthermore, software development methodologies are being adopted more frequently into traditional engineering practice. In a recent and well-known German TV Show [2], a highly esteemed expert in mechanical and production engineering, Günther Schuh, was asked why he and his team were able to produce the “StreetScooter” electric car so quickly. Schuh said, “Because they have used computer science methods”. In particular, Schuh mentioned agility as a key catalyst to the increased speed of development. The portfolio of methods that software engineering has developed over the last two decades is effective and efficient in multiple engineering domains, allowing innovative products to be created at a quick pace. After 50 years, Software Engineering has found its toolset of methods, languages, concepts and techniques that allow software developers to create various forms of software products, services and embedded software, which in turn enables various new business concepts. One of our main problems now is to teach these practices in appropriate forms to many more computer scientists and other engineers, such that there are enough people available to deliver new innovative projects in the future.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-018-0674-5.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Improving manual reviews in function‑centered engineering of embedded systems using a dedicated review model",
        "submission-date": "2017/08",
        "publication-date": "2019/02",
        "abstract": "In model-based engineering of embedded systems, manual validation activities such as reviews and inspections are needed to ensure that the system under development satisfies the stakeholder intentions. During the engineering process, changes in the stakeholder intentions typically trigger revisions of already developed and documented engineering artifacts including requirements and design specifications. In practice, changes in stakeholder intentions are often not immediately perceived and not properly documented. Moreover, they are quite often not consistently incorporated into all relevant engineering artifacts. In industry, typically manual reviews are executed to ensure that the relevant stakeholder intentions are adequately considered in the engineering artifacts. In this article, we introduce a dedicated review model to aid the reviewer in conducting manual reviews of behavioral requirements and functional design specification—two core artifacts in function-centered engineering of embedded software. To investigate whether the proposed solution is beneficial we conducted controlled experiments showing that the use of the dedicated review model can significantly increase the effectiveness and efficiency of manual reviews. Additionally, the use of the dedicated review model leads to significantly more confident decisions of the reviewers and is perceived by the reviewers as significantly more supportive compared with reviews without the dedicated review model.",
        "keywords": [
            "Behavioral requirements",
            "Embedded software",
            "Functional design",
            "Review model",
            "Requirements engineering",
            "Perspective-based review",
            "Model transformations"
        ],
        "authors": [
            "Marian Daun",
            "Thorsten Weyer",
            "Klaus Pohl"
        ],
        "file_path": "data/sosym-all/s10270-019-00723-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Contents for a Model-Based Software Engineering Body of Knowledge",
        "submission-date": "2019/05",
        "publication-date": "2019/07",
        "abstract": "Although Model-Based Software Engineering (MBE) is a widely accepted Software Engineering (SE) discipline, no agreed-upon core set of concepts and practices (i.e., a Body of Knowledge) has been deﬁned for it yet. With the goals of characterizing the contents of the MBE discipline, promoting a global consistent view of it, clarifying its scope with regard to other SE disciplines, and deﬁning a foundation for the development of educational curricula on MBE, this paper proposes the contents for a Body of Knowledge for MBE. We also describe the methodology that we have used to come up with the proposed list of contents, as well as the results of a survey study that we conducted to sound out the opinion of the community on the importance of the proposed topics and their level of coverage in the existing SE curricula.",
        "keywords": [
            "Model-Based Software Engineering",
            "Body of Knowledge",
            "Core concepts",
            "Education"
        ],
        "authors": [
            "Loli Burgueño",
            "Federico Ciccozzi",
            "Michalis Famelis",
            "Gerti Kappel",
            "Leen Lambers",
            "Sebastien Mosser",
            "Richard F. Paige",
            "Alfonso Pierantonio",
            "Arend Rensink",
            "Rick Salay",
            "Gabriele Taentzer",
            "Antonio Vallecillo",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-019-00746-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling NASA swarm-based systems: using agent-oriented software engineering and formal methods",
        "submission-date": "2009/09",
        "publication-date": "2009/10",
        "abstract": "The need to collect new data and perform new science is causing the complexity of NASA missions to continually increase. This complexity needs to be controlled via new technological advancements and balanced with a reduction in mission and operation costs. Planned and hypothesized missions involve self-management, biological-inspiration based on swarms, and autonomous operation as a means of achieving these goals. We consider a tailored software engineering approach to developing such systems based on agent-oriented software engineering and formal methods. We report on advances in modeling, implementing, and testing NASA swarm-based concept missions.",
        "keywords": [
            "Swarms",
            "Emergent behavior",
            "Agent-oriented software engineering",
            "Formal methods"
        ],
        "authors": [
            "Joaquin Peña",
            "Christopher A. Rouff",
            "Mike Hinchey",
            "Antonio Ruiz-Cortés"
        ],
        "file_path": "data/sosym-all/s10270-009-0135-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A Multi-Paradigm Modelling approach to live modelling",
        "submission-date": "2018/02",
        "publication-date": "2018/10",
        "abstract": "To develop complex systems and tackle their inherent complexity, (executable) modelling takes a prominent role in the development cycle. But whereas good tool support exists for programming, tools for executable modelling have not yet reached the same level of functionality and maturity. In particular, live programming is seeing increasing support in programming tools, allowing users to dynamically change the source code of a running application. This signiﬁcantly reduces the edit–compile–debug cycle and grants the ability to gauge the effect of code changes instantly, aiding in debugging and code comprehension in general. In the modelling domain, however, live modelling only has limited support for a few formalisms. In this paper, we propose a Multi-Paradigm Modelling approach to add liveness to modelling languages in a generic way, which is reusable across multiple formalisms. Live programming concepts and techniques are transposed to (domain-speciﬁc) executable modelling languages, clearly distinguishing between generic and language-speciﬁc concepts. To evaluate our approach, live modelling is implemented for three modelling languages, for which the implementation of liveness substantially differs. For all three cases, the exact same structured process was used to enable live modelling, which only required a “sanitization” operation to be deﬁned.",
        "keywords": [
            "Live programming",
            "Live modelling",
            "Debugging",
            "Multi-Paradigm Modelling"
        ],
        "authors": [
            "Yentl Van Tendeloo",
            "Simon Van Mierlo",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-018-0700-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consolidation of database check constraints",
        "submission-date": "2016/07",
        "publication-date": "2017/11",
        "abstract": "Independent modeling of various modules of an information system (IS), and consequently database subschemas, may result in formal or semantic conﬂicts between the modules being modeled. Such conﬂicts may cause collisions between the integrated database schema of a whole IS and the modeled subschemas. In our previous work, we have proposed criteria and algorithms for identifying and resolving such conﬂicts so as to provide a consolidation of database subschemas with the integrated database schema with respect to various database concepts, such as domains, relation schemes, primary key constraints and referential integrity constraints. In this paper, we propose a new approach and algorithms for identifying conﬂicts and testing consolidation of subschemas with the integrated database schema against check constraints. The proposed approach is based on satisﬁability modulo theory (SMT) solvers. Hereby, we propose the integration of SMT solvers into our MDSD tool, aimed at supporting a database schema integration process.",
        "keywords": [
            "Database subschema consolidation",
            "Check constraint collision",
            "Implication problem",
            "SMT solver"
        ],
        "authors": [
            "Nikola Obrenovi´c",
            "Ivan Lukovi´c",
            "Sonja Risti´c"
        ],
        "file_path": "data/sosym-all/s10270-017-0637-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scalable modeling technologies in the wild: an experience report on wind turbines control applications development",
        "submission-date": "2019/02",
        "publication-date": "2020/01",
        "abstract": "Scalability in modeling has many facets, including the ability to build larger models and domain-speciﬁc languages (DSLs) efﬁciently. With the aim of tackling some of the most prominent scalability challenges in model-based engineering (MBE), the MONDO EU project developed the theoretical foundations and open-source implementation of a platform for scalable modeling and model management. The platform includes facilities for building large graphical DSLs, for splitting large models into sets of smaller interrelated fragments, to index large collections of models to speed-up their querying, and to enable the collaborative construction and reﬁnement of complex models, among other features. This paper reports on the tools provided by MONDO that Ikerlan, a medium-sized technology center which in the last decade has embraced the MBE paradigm, adopted in order to improve their processes. This experience produced as a result a set of model editors and related technologies that fostered collaboration and scalability in the development of wind turbine control applications. In order to evaluate the beneﬁts obtained, an on-site evaluation of the tools was performed. This evaluation shows that scalable MBE technologies give new growth opportunities to small- and medium-sized organizations.",
        "keywords": [
            "Model-based engineering (MBE)",
            "Scalability",
            "Domain-speciﬁc graphical modeling languages",
            "Collaborative modeling",
            "Model indexing",
            "Experience report"
        ],
        "authors": [
            "Abel Gómez",
            "Xabier Mendialdua",
            "Konstantinos Barmpis",
            "Gábor Bergmann",
            "Jordi Cabot",
            "Xabier de Carlos",
            "Csaba Debreceni",
            "Antonio Garmendia",
            "Dimitrios S. Kolovos",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-020-00776-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalizing the structural semantics of domain-speciﬁc modeling languages",
        "submission-date": "2007/04",
        "publication-date": "2008/12",
        "abstract": "Model-based approaches to system design are now widespread and successful. These approaches make extensive use of model structure to describe systems using domain-speciﬁc abstractions, to specify and implement model transformations, and to analyze structural properties of models. In spite of its general importance the structural semantics of modeling languages are not well-understood. In this paper we develop the formal foundations for the structural semantics of domain-speciﬁc modeling languages (DSML), including the mechanisms by which metamodels specify the structural semantics of DSMLs. Additionally, we show how our formalization can complement existing tools, and how it yields algorithms for the analysis of DSMLs and model transformations.",
        "keywords": [
            "Model-based Design",
            "Domain-speciﬁc modeling languages",
            "Structural semantics",
            "Metamodeling",
            "Formal logic",
            "Horn logic"
        ],
        "authors": [
            "Ethan Jackson",
            "Janos Sztipanovits"
        ],
        "file_path": "data/sosym-all/s10270-008-0105-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A systematic literature review of cross-domain model consistency checking by model management tools",
        "submission-date": "2019/09",
        "publication-date": "2020/10",
        "abstract": "Objective The goal of this study is to identify gaps and challenges related to cross-domain model management focusing on consistency checking. Method We conducted a systematic literature review. We used the keyword-based search on Google Scholar, and we identiﬁed 618 potentially relevant studies; after applying inclusion and exclusion criteria, 96 papers were selected for further analysis. Results The main ﬁndings/contributions are: (i) a list of available tools used to support model management; (ii) 40% of the tools can provide consistency checking on models of different domains and 25% on models of the same domain, and 35% do not provide any consistency checking; (iii) available strategies to keep the consistency between models of different domains are not mature enough; (iv) most of the tools that provide consistency checking on models of different domains can only capture up to two inconsistency types; (v) the main challenges associated with tools that manage models on different domains are related to interoperability between tools and the consistency maintenance. Conclusion The results presented in this study can be used to guide new research on maintaining the consistency between models of different domains. Example of further research is to investigate how to capture the Behavioral and Reﬁnement inconsistency types. This study also indicates that the tools should be improved in order to address, for example, more kinds of consistency check.",
        "keywords": [
            "Model management",
            "Systems engineering",
            "Model-based systems engineering"
        ],
        "authors": [
            "Weslley Torres",
            "Mark G. J. van den Brand",
            "Alexander Serebrenik"
        ],
        "file_path": "data/sosym-all/s10270-020-00834-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An approach to clone detection in sequence diagrams and its application to security analysis",
        "submission-date": "2015/08",
        "publication-date": "2016/09",
        "abstract": "Duplication in software systems is an important issue in software quality assurance. While many methods for software clone detection in source code and structural models have been described in the literature, little has been done on similarity in the dynamic behaviour of interactive systems. In this paper, we present an approach to identifying near-miss interaction clones in reverse-engineered UML sequence diagrams. Our goal is to identify patterns of interaction (“conversations”) that can be used to characterize and abstract the run-time behaviour of web applications and other interactive systems. In order to leverage existing robust near-miss code clone technology, our approach is text-based, working on the level of XMI, the standard interchange serialization for UML. Clone detection in UML behavioural models, such as sequence diagrams, presents a number of challenges— first, it is not clear how to break a continuous stream of interaction between lifelines (representing the objects or actors in the system) into meaningful conversational units. Second, unlike programming languages, the XMI text representation for UML is highly non-local, using attributes to reference-related elements in the model file remotely. In this work, we use a set of contextualizing source transformations on the XMI text representation to localize related elements, exposing the hidden hierarchical structure of the model and allowing us to granularize behavioural interactions into conversational units. Then we adapt NICAD, a robust near-miss code clone detection tool, to help us identify conversational clones in reverse-engineered behavioural models. These conversational clones are then analysed to find worrisome interactions that may indicate security access violations.",
        "keywords": [
            "Model clone detection",
            "Model based security analysis"
        ],
        "authors": [
            "Manar H. Alalﬁ",
            "Elizabeth P. Antony",
            "James R. Cordy"
        ],
        "file_path": "data/sosym-all/s10270-016-0557-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Contract-based veriﬁcation of discrete-time multi-rate Simulink models",
        "submission-date": "2013/11",
        "publication-date": "2015/06",
        "abstract": "This paper presents an approach to modular contract-based veriﬁcation of discrete-time multi-rate Simulink models. The veriﬁcation approach uses a translation of Simulink models to sequential programs that can then be veriﬁed using traditional software veriﬁcation techniques. Automatic generation of the proof obligations needed for veriﬁcation of correctness with respect to contracts, and automatic proofs are also discussed. Furthermore, the paper provides detailed discussions about the correctness of each step in the veriﬁcation process. The veriﬁcation approach is demonstrated on a case study involving control software for prevention of pressure peaks in hydraulics systems.",
        "keywords": [
            "Automated veriﬁcation",
            "Reﬁnement",
            "Control systems",
            "SMT solving",
            "Synchronous data ﬂow"
        ],
        "authors": [
            "Pontus Boström",
            "Jonatan Wiik"
        ],
        "file_path": "data/sosym-all/s10270-015-0477-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bridging the chasm between MDE and the world of compilation",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "Modeling and transforming have always been the cornerstones of software system development, albeit often investigated by different research communities. Modeling addresses how information is represented and processed, while transformation cares about what the results of processing this information are. To address the growing complexity of software systems, model-driven engineering (MDE) leverages domain-speciﬁc languages to deﬁne abstract models of systems and automated methods to process them. Meanwhile, compiler technology mostly concentrates on advanced techniques and tools for program transformation. For this, it has developed complex analyses and transformations (from lexical and syntactic to semantic analyses, down to platform-speciﬁcoptimizations).Thesetwocommunitiesappeartoday quite complementary and are starting to meet again in the software language engineering (SLE) ﬁeld. SLE addresses all the stages of a software language lifecycle, from its deﬁnition to its tooling. In this article, we show how SLE can lean on the expertise of both MDE and compiler research communities and how each community can bring its solutions to the other one. We then draw a picture of the current state of SLE and of the challenges it has still to face.",
        "keywords": [
            "Model-driven engineering (MDE)",
            "Domain-speciﬁc language (DSL)",
            "Compilation",
            "Intermediate representation (IR)",
            "Software language engineering (SLE)"
        ],
        "authors": [
            "Jean-Marc Jézéquel",
            "Benoit Combemale",
            "Steven Derrien",
            "Clément Guy",
            "Sanjay Rajopadhye"
        ],
        "file_path": "data/sosym-all/s10270-012-0266-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model for dynamic reconﬁguration in service-oriented architectures",
        "submission-date": "2010/11",
        "publication-date": "2012/02",
        "abstract": "The importance of modelling the dynamic characteristics of the architecture of software systems has long been recognised. However, the nature of the dynamics of service-oriented applications goes beyond what is currently addressed by architecture description languages (ADLs). At the heart of the service-oriented approach is the logical separation between the service need and the need-fulﬁllment mechanism, i.e., the provision of the service: the binding between the requester and the provider is deferred to run time and established at the instance level, i.e., each time the need for the service arises. As a consequence, computation in the context of service-oriented architectures transforms not only the states of the components that implement applications but also the conﬁgurations of those applications. In this paper, we present a model for dynamic reconﬁguration that is general enough to support the definition of ADLs that are able to address the full dynamics of service-oriented applications. As an instance of the model, we present a simple service-oriented ADL derived from the modelling language srmlthat we developed in the Sensoria project.",
        "keywords": [
            "Software architecture",
            "Service-oriented computing",
            "Dynamic formal modelling"
        ],
        "authors": [
            "José Luiz Fiadeiro",
            "Antónia Lopes"
        ],
        "file_path": "data/sosym-all/s10270-012-0236-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Application and evaluation of interlinked approaches for modeling changing capabilities",
        "submission-date": "2023/03",
        "publication-date": "2024/05",
        "abstract": "The nature of modern organizations needs to be increasingly adaptive, since they are dealing with a constant demand to respond to stimuli derived from the dynamic environments they operate in. Changing their capabilities is a common response, and this makes capability management a vital aspect of organizational survivability. To date, there are no approaches specifically designed to address this specific situation. KYKLOS and Compass are two interlinked approaches of different complexity, a DSMLandacanvas,developedtosupportcapabilitychange.Asrecentlydevelopedmethods,theylackedformaldemonstration and evaluation; therefore, the goal of this article is to present the demonstration and evaluation of the two approaches by their stakeholders, in particular, business and modeling experts. A case study in a Swedish company in the ERP system consulting domain that is undergoing changes in its sales and consulting capabilities related to evolving customer requirements has been used to demonstrate and evaluate the two approaches. The process consisted of two evaluation cycles. The first cycle concerned KYKLOS and used two categories of evaluators, the business experts and the modeling experts. While the modeling experts evaluated positively the method, the business experts had difficulties associated with its ease of use and adoption. This resulted in the development of Compass, which was evaluated by business experts during the second evaluation cycle. Compass was evaluated more positively in terms of the difficult aspects, but the challenge is ongoing and motivates further future research.",
        "keywords": [
            "Capability management",
            "Enterprise modeling",
            "DSML",
            "Method evaluation",
            "Change management"
        ],
        "authors": [
            "Georgios Koutsopoulos",
            "Anna Andersson",
            "Janis Stirna",
            "Martin Henkel"
        ],
        "file_path": "data/sosym-all/s10270-024-01181-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Module superimposition: a composition technique for rule-based model transformation languages",
        "submission-date": "2008/11",
        "publication-date": "2009/10",
        "abstract": "As the application of model transformation becomes increasingly commonplace, the focus is shifting from model transformation languages to the model transformations themselves. The properties of model transformations, such as scalability, maintainability and reusability, have become important. Composition of model transformations allows for the creation of smaller, maintainable and reusable transformation definitions that together perform a larger transformation. This paper focuses on composition for two rule-based model transformation languages: the ATLAS Transformation Language (ATL) and the QVT Relations language. We propose a composition technique called module superimposition that allows for extending and overriding rules in transformation modules. We provide executable semantics as well as a concise and scalable implementation of module superimposition based on ATL.",
        "keywords": [
            "Software engineering",
            "Model driven engineering",
            "Model transformation"
        ],
        "authors": [
            "Dennis Wagelaar",
            "Ragnhild Van Der Straeten",
            "Dirk Deridder"
        ],
        "file_path": "data/sosym-all/s10270-009-0134-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL, QVT Relations"
        }
    },
    {
        "title": "SoSyM reflections: the 2020 \"State of the Journal\" report",
        "submission-date": "2021/02",
        "publication-date": "2021/02",
        "abstract": "When writing the “2019 State of the Journal Report” at this same time last year, we could not have predicted the global changes that would beset us with so many challenges from the emergence of the COVID-19 pandemic. Some of us lost loved ones, friends, and colleagues, while the entire world adapted to working from home, participating in remote classrooms, and adopting safety precautions as a new means of daily life. The research community was also affected, with the cessation of travel leading to virtual conferences. Thanks to the heroic efforts of many conference chairs1 adapting to the quick pace of change, scientific discussions contin-ued, but sometimes in a less personable form. Journals also experienced changes with submissions on the rise, but fewer reviewers available to assist with the evaluation because of personal challenges faced by many.\nYet, in the presence of a global pandemic, scientific con-tributions continued in the software and systems modeling community. The number of SoSyM submissions over the past year saw an increase, while the general health of the journal remains strong. Measures put in place recently, such as the second year of moving to six issues per year, have helped to reduce the time to publication significantly. The Open Access movement is also progressing, with Springer announcing new initiatives to make SoSyM publications more accessible to a broader community of researchers. The rest of this editorial summarizes the progress made by the journal during this unprecedented situation.",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-021-00871-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An integrated multi-level modeling approach for industrial-scale data interoperability",
        "submission-date": "2015/02",
        "publication-date": "2016/04",
        "abstract": "Multi-level modeling is currently regaining attention in the database and software engineering community with different emerging proposals and implementations. One driver behind this trend is the need to reduce model complexity, a crucial aspect in a time of analytics in Big Data that deal with complex heterogeneous data structures. So far no standard exists for multi-level modeling. Therefore, different formalization approaches have been proposed to address multi-level modeling and veriﬁcation in different frameworks and tools. In this article, we present an approach that integrates the formalization, implementation, querying, andveriﬁcationofmulti-levelmodels.Theapproachhasbeen evaluated in an open-source F-Logic implementation and applied in a large-scale data interoperability project in the oil and gas industry. The outcomes show that the framework is adaptable to industry standards, reduces the complexity of speciﬁcations, and supports the veriﬁcation of standards from a software engineering point of view.",
        "keywords": [
            "Multi-level modeling",
            "Interoperability",
            "Multi-level model reasoning",
            "F-Logic",
            "Multi-level model querying",
            "Multi-level model veriﬁcation"
        ],
        "authors": [
            "Muzaffar Igamberdiev",
            "Georg Grossmann",
            "Matt Selway",
            "Markus Stumptner"
        ],
        "file_path": "data/sosym-all/s10270-016-0520-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "gLTSdiff: a generalized framework for structural comparison of software behavior",
        "submission-date": "2024/04",
        "publication-date": "Not found",
        "abstract": "Structural comparison of state machine models – such as labeled transition systems and (extended) ﬁnite automata – is used for numerous applications, such as ﬁnding potential behavioral regressions in new software versions, evaluating the accuracy of different model learning algorithms, and ﬁngerprinting software for security applications. The state-of-the-art LTSDiff structural comparison algorithm has limited assumptions, making it broadly applicable. However, representation-speciﬁc information is not taken into account, requiring adaptations to prevent sub-optimal or even invalid results. We introduce gLTSdiff, which generalizes and extends LTSDiff, allowing a wide range of state machine models to be compared, by recursively comparing over the structure of state and transition labels. Additional challenges that we faced while applying LTSDiff in industrial practice are also addressed by gLTSdiff, as it rewrites undesired difference patterns, supports comparison of any number of input models, and allows for an effort/quality trade-off. We formally deﬁne gLTSdiff, and make it available as an extensible open source library for structural model comparison. Using multiple large-scale industrial and open source case studies, we evaluate both its practical value and its various improvements.",
        "keywords": [
            "Software behavior",
            "State machines",
            "Structural comparison",
            "Industrial application"
        ],
        "authors": [
            "Dennis Hendriks",
            "Wytse Oortwijn"
        ],
        "file_path": "data/sosym-all/s10270-024-01239-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Testing concurrent user behavior of synchronous web applications with Petri nets",
        "submission-date": "2016/09",
        "publication-date": "2018/02",
        "abstract": "Web applications are now used in every aspect of our lives to manage work, provide products and services, read email, and provide entertainment. The software technologies used to build web applications provide features that help designers provide flexible functionality, but that are challenging to model and test. In particular, the network-based request-response model of programming means that web applications are inherently “stateless” and implicitly concurrent. They are stateless because a new network connection is made for each request (for example, when a user clicks a submit button). Thus, the server does not, by default, recognize multiple requests from the same user. Web applications are also concurrent because multiple users can use the same web application at the same time, creating contention for the same resources. Unfortunately, most web application testing does not adequately evaluate these aspects of web applications, leaving many software faults in deployed web applications. Part of this problem is because most traditional software modeling tools (such as UML) do not have built-in support for the stateless and concurrent aspects of web applications. This research project uses a novel model that is based on Petri nets to describe certain aspects of the behavior of web applications. This paper makes several contributions. We present a novel technique to design tests from this model that explicitly tests concurrency in web applications. We present novel coverage criteria that are defined on the Petri net model. We present results from an empirical study of 18 web applications with 343 components and 30,186 lines of code, followed by a case study on a large industrial web application. The tests found significantly more faults than traditional requirements-based tests, with fewer tests.",
        "keywords": [
            "Web applications",
            "Model-based testing",
            "Test criteria",
            "Petri nets"
        ],
        "authors": [
            "Jeff Offutt",
            "Sunitha Thummala"
        ],
        "file_path": "data/sosym-all/s10270-018-0655-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A dependability profile within MARTE",
        "submission-date": "2009/01",
        "publication-date": "2009/08",
        "abstract": "The importance of assessing software non-functional properties (NFP) beside the functional ones is well accepted in the software engineering community. In particular, dependability is a NFP that should be assessed early in the software life-cycle by evaluating the system behaviour under different fault assumptions. Dependability-speciﬁc modeling and analysis techniques include for example Failure Mode and Effect Analysis for qualitative evaluation, stochastic Petri nets for quantitative evaluation, and fault trees for both forms of evaluation. Uniﬁed Modeling Language (UML) may be specialized for different domains by using the proﬁle mechanism. For example, the MARTE proﬁle extends UML with concepts for modeling and quantitative analysis of real-time and embedded systems (more speciﬁcally, for schedulability and performance analysis). This paper proposes to add to MARTE a proﬁle for dependability analysis and modeling (DAM). A case study of an intrusion-tolerant message service will offer insight on how the MAR-TE-DAM proﬁle can be used to derive a stochastic Petri net model for performance and dependability assessment.",
        "keywords": [],
        "authors": [
            "Simona Bernardi",
            "José Merseguer",
            "Dorina C. Petriu"
        ],
        "file_path": "data/sosym-all/s10270-009-0128-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section of BPMDS’2020 business process management meets data",
        "submission-date": "2022/03",
        "publication-date": "2022/04",
        "abstract": "The business process modeling, development and support (BPMDS) working conference series serves as a meeting place for researchers and practitioners in the areas of Business Process Modeling, Development and Support. This special section follows the 21st edition of the BPMDS series, organized in conjunction with CAiSE’20, which was held online in Grenoble, France, June 2020. BPMDS’2020 received 30 submissions from 19 countries, and 13 papers were selected and published in Springer LNBIP 387 volume. The theme of BPMDS’2020: ‘BPM meets Data’ follows the emergence of data science as a prominent area, and is thus investigating various aspects of the relations between processes and data.",
        "keywords": [],
        "authors": [
            "Pnina Soffer",
            "Selmin Nurcan"
        ],
        "file_path": "data/sosym-all/s10270-022-00997-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Live process modeling with the BPMN Sketch Miner",
        "submission-date": "2021/02",
        "publication-date": "2022/06",
        "abstract": "BPMN Sketch Miner is a modeling environment for generating visual business process models starting from constrained\nnatural language textual input. Its purpose is to support business process modelers who need to rapidly sketch visual BPMN\nmodels during interviews and design workshops, where participants should not only provide input but also give feedback\non whether the sketched visual model represents accurately what has been described during the discussion. In this article,\nwe present a detailed description of the BPMN Sketch Miner design decisions and list the different control ﬂow patterns\nsupported by the current version of its textual DSL. We also summarize the user study and survey results originally published\nin MODELS 2020 concerning the tool usability and learnability and present a new performance evaluation regarding the\nvisual model generation pipeline under actual usage conditions. The goal is to determine whether it can support a rapid\nmodel editing cycle, with live synchronization between the textual description and the visual model. This study is based on\na benchmark including a large number of models (1350 models) exported by users of the tool during the year 2020. The\nmain results indicate that the performance is sufﬁcient for a smooth live modeling user experience and that the end-to-end\nexecution time of the text-to-model-to-visual pipeline grows linearly with the model size, up to the largest models (with 195\nlines of textual description) found in the benchmark workload.",
        "keywords": [
            "Business Process Model and Notation (BPMN)",
            "Process mining",
            "Domain-speciﬁc languages",
            "Performance\nevaluation"
        ],
        "authors": [
            "Ana Ivanchikj",
            "Souhaila Serbout",
            "Cesare Pautasso"
        ],
        "file_path": "data/sosym-all/s10270-022-01009-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "GReTL: an extensible, operational, graph-based transformation language",
        "submission-date": "2011/04",
        "publication-date": "2012/05",
        "abstract": "This article introduces the graph-based transformation language GReTL. GReTL is operational, and transformations are either speciﬁed in plain Java using the GReTL API or in a simple domain-speciﬁc language. GReTL follows the conception of incrementally constructing the target meta-model together with the target graph. When creating a new metamodel element, a set-based semantic expression is speciﬁed that describes the set of instances that have to be created in the target graph. This expression is deﬁned as a query on the source graph. GReTL is a kernel language consisting of a minimal set of operations, but it is designed for being extensible. Custom higher-level operations can be built on top of the kernel operations easily. After a description of the foundations of GReTL, its most important elements are introduced along with a transformation example in the ﬁeld of metamodel integration. Insights into the design of the GReTL API are given, and a convenience copy operation is implemented to demonstrate GReTL’s extensibility.",
        "keywords": [
            "Model transformation",
            "Graph transformation",
            "Metamodel merging"
        ],
        "authors": [
            "Jürgen Ebert",
            "Tassilo Horn"
        ],
        "file_path": "data/sosym-all/s10270-012-0250-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "GReTL"
        }
    },
    {
        "title": "Supporting the reconciliation of models of object behaviour",
        "submission-date": "2002/09",
        "publication-date": "2004/04",
        "abstract": "This paper presents Reconciliation+, a method which identifies overlaps between models of software systems behaviour expressed as UML object interaction diagrams (i.e., sequence and/or collaboration diagrams), checks whether the overlapping elements of these models satisfy specific consistency rules and, in cases where they violate these rules, guides software designers in handling the detected inconsistencies. The method detects overlaps between object interaction diagrams by using a probabilistic message matching algorithm that has been developed for this purpose. The guidance to software designers on when to check for inconsistencies and how to deal with them is delivered by enacting a built-in process model that specifies the consistency rules that can be checked against overlapping models and different ways of handling violations of these rules. Reconciliation+ is supported by a toolkit. It has also been evaluated in a case study. This case study has produced positive results which are discussed in the paper.",
        "keywords": [
            "Consistency management",
            "Software design models",
            "Object interaction diagrams"
        ],
        "authors": [
            "George Spanoudakis",
            "Hyoseob Kim"
        ],
        "file_path": "data/sosym-all/s10270-004-0054-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The KlaperSuite framework for model-driven reliability analysis of component-based systems",
        "submission-date": "2012/02",
        "publication-date": "2013/03",
        "abstract": "Automatic prediction tools play a key role in enabling the application of non-functional requirements analysis,tosimplifytheselectionandtheassemblyofcomponentsforcomponent-basedsoftwaresystems,andinreducing the need for strong mathematical skills for software designers. By exploiting the paradigm of Model-Driven Engineering (MDE), it is possible to automatically transform design models into analytical models, thus enabling formal property veriﬁcation. MDE is the core paradigm of the Klaper-Suite framework presented in this paper, which exploits the KLAPER pivot language to ﬁll the gap between design and analysis of component-based systems for reliability properties. KlaperSuite is a family of tools empowering designers with the ability to capture and analyze quality of service viewsoftheirsystems,bybuildingaone-clickbridgetowards a number of established veriﬁcation instruments. In this arti-cle, we concentrate on the reliability-prediction capabilities of KlaperSuite and we evaluate them with respect to several case studies from literature and industry.",
        "keywords": [
            "Model-driven engineering",
            "Reliability analysis",
            "Component-based systems"
        ],
        "authors": [
            "Andrea Ciancone",
            "Mauro Luigi Drago",
            "Antonio Filieri",
            "Vincenzo Grassi",
            "Heiko Koziolek",
            "Raffaela Mirandola"
        ],
        "file_path": "data/sosym-all/s10270-013-0334-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "KLAPER"
        }
    },
    {
        "title": "Repository mining for changes in Simulink and Stateﬂow models",
        "submission-date": "2022/04",
        "publication-date": "2023/06",
        "abstract": "Model-Based Development (MBD) is widely used for embedded controls development, with MATLAB/Simulink/Stateﬂow being one of the most used development environments in the automotive industry. Simulink/Stateﬂow models are the primary design artifacts in automotive controls MBD development, and these models must be maintained over their lifetime. We achieve this in traditional software designs through the use of information hiding. It is thus beneﬁcial to develop these models so that they facilitate likely changes that do not adversely impact the quality of the design. In order to do so, the types of frequently performed changes must be understood and appropriate language mechanisms must be available to support these changes. While some work has been done to analyze changes in Simulink/Stateﬂow models, a much deeper understanding is needed. We leveraged an extraordinary opportunity of having access to a comprehensive industrial software repository and its associated version control system to gain insight into likely changes for Simulink/Stateﬂow in automotive controls development. This analysis provides accurate feedback on actual changes made over many years to Simulink/Stateﬂow models, and classiﬁes changes to suggest how particular model changes can impact system evolution.",
        "keywords": [
            "Simulink",
            "Stateﬂow",
            "Model-Based Development",
            "Model change",
            "Repository mining",
            "Software evolution",
            "Version control system"
        ],
        "authors": [
            "Monika Jaskolka",
            "Vera Pantelic",
            "Alan Wassyng",
            "Richard F. Paige",
            "Mark Lawford"
        ],
        "file_path": "data/sosym-all/s10270-023-01113-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Interface protocol inference to aid understanding legacy software components",
        "submission-date": "2019/06",
        "publication-date": "2020/06",
        "abstract": "High-tech companies are struggling today with the maintenance of legacy software. Legacy software is vital to many organizations as it contains the important business logic. To facilitate maintenance of legacy software, a comprehensive understanding of the software’s behavior is essential. In terms of component-based software engineering, it is necessary to completely understand the behavior of components in relation to their interfaces, i.e., their interface protocols, and to preserve this behavior during the maintenance activities of the components. For this purpose, we present an approach to infer the interface protocols of software components from the behavioral models of those components, learned by a blackbox technique called active (automata) learning. To validate the learned results, we applied our approach to the software components developed with model-based engineering so that equivalence can be checked between the learned models and the reference models, ensuring the behavioral relations are preserved. Experimenting with components having reference models and performing equivalence checking builds confidence that applying active learning technique to reverse engineer legacy software components, for which no reference models are available, will also yield correct results. To apply our approach in practice, we present an automated framework for conducting active learning on a large set of components and deriving their interface protocols. Using the framework, we validated our methodology by applying active learning on 202 industrial software components, out of which, interface protocols could be successfully derived for 156 components within our given time bound of 1h for each component.",
        "keywords": [
            "Active automata learning",
            "Interface protocols",
            "Learning framework",
            "Equivalence oracles"
        ],
        "authors": [
            "Kousar Aslam",
            "Loek Cleophas",
            "Ramon Schiﬀelers",
            "Mark van den Brand"
        ],
        "file_path": "data/sosym-all/s10270-020-00809-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest Editorial to the Special Issue on MoDELS 2006",
        "submission-date": "2006/10",
        "publication-date": "2008/08",
        "abstract": "This issue of “Software and Systems Modeling” and the following one are devoted to selected papers of the ninth MODELS Conference held in Genoa, Italy, from October 1–6, 2006. The conference has established itself as one of the key international venues for the presentation of scientific results in the domain of model-driven engineering and related topics such as software modeling and model transformation.",
        "keywords": [],
        "authors": [
            "Oscar Nierstrasz",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-008-0100-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for EMMSAD’2017 special section",
        "submission-date": "2018/08",
        "publication-date": "2018/08",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) series has produced 22 events, associated with CAiSE (Conference on Advanced Information Systems Engineering), from 1996 to 2017. From 2009, EMMSAD has become a 2-day working conference. The topics addressed by the EMMSAD series focus on modeling methods for software systems, enterprises, and business processes, as well as their evaluation through a variety of empirical and non-empirical approaches. The aims, topics, and history of EMMSAD can be found on the Web site at http://www.emmsad.org/. This special section follows the 22nd edition of the EMM-SAD series, organized in conjunction with CAiSE’17 at Essen, Germany, June 2017. A total of 25 submissions from 18 countries were received, and 9 full papers and 2 short papers were selected and published in Springer LNBIP 287 volume. These papers referred to: (1) modeling approaches that support decision making, business process or behavior speciﬁcation, and evolving contexts of enterprise modeling and cloud computing; and (2) evaluation and comparison of modeling languages and methods.",
        "keywords": [],
        "authors": [
            "Iris Reinhartz-Berger",
            "Wided Guédria",
            "Palash Bera"
        ],
        "file_path": "data/sosym-all/s10270-018-0693-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Conﬂuence of aspects for sequence diagrams",
        "submission-date": "2009/07",
        "publication-date": "2011/09",
        "abstract": "The last decade has seen several aspect language proposals for UML 2 sequence diagrams. Aspects allow the modeler to deﬁne crosscutting concerns of sequence diagrams and to have these woven with the sequence diagrams of a so-called base model, in order to create a woven model. In a real-world scenario, there may be multiple aspects applicable to the same base model. This raises the need to analyse the set of aspects to identify possible aspect interactions (dependencies and conﬂicts) between applications of aspects. We call a set of aspects terminating if they may not be applied inﬁnitely many times for any given base model. Furthermore, we call a set of terminating aspects conﬂuent, if they, for any given base model, always yield the same ﬁnal result regardless of the order in which they are applied. Since conﬂuence must hold for any base model, this is a much stronger result than many of the current approaches that have addressed detection of aspect interactions limited to a speciﬁc base model. Our aspects are speciﬁed using standard sequence diagrams with some extensions. In this paper, we present a conﬂuence theory specialized for our highly expressive aspect language. For the most expressive aspects, we prove that conﬂuence is undecidable. For another class of aspects with considerable expressiveness, we prescribe an algorithm to check conﬂuence. This algorithm is based on what we call an extended critical pair analysis. These results are useful both for modelers and researchers working with sequence diagram aspects and for researchers wanting to establish a conﬂuence theory for other aspect-oriented modelling or model transformation approaches.",
        "keywords": [
            "Aspect",
            "Weave",
            "Conﬂuence",
            "Aspect interaction",
            "Aspect interference",
            "Graph transformation",
            "Critical pair",
            "UML",
            "Sequence diagram"
        ],
        "authors": [
            "Roy Grønmo",
            "Ragnhild Kobro Runde",
            "Birger Møller-Pedersen"
        ],
        "file_path": "data/sosym-all/s10270-011-0212-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Understanding MDE projects: megamodels to the rescue for architecture recovery",
        "submission-date": "2018/12",
        "publication-date": "2019/07",
        "abstract": "Conventional wisdom on Model-Driven Engineering (MDE) suggests that this software discipline is the key to achieve superior automation, whether it be refactoring, simulation, or code generation. However, the diversity of employed languages and technologies blurs the picture making it difﬁcult to analyze existing MDE-based projects in order to retrieve architectural information to foster a better understanding about the rationale behind them. Thus, the ability of carefully analyzing projects to identify their components and their interrelationships is key to obtain representations at a higher level of abstraction that can support reuse processes. In this paper, a megamodel-based approach to the reverse engineering of model-driven projects is proposed in order to leverage the representation of the involved technologies and assets. An automated recovery technique implemented by the MDEprofiler infrastructure is presented and illustrated by analyzing community projects in terms of basic MDE artifacts (such as models and metamodels) and the usage of common technologies such as model transformations and code generators.",
        "keywords": [
            "Megamodeling",
            "Reverse engineering",
            "Architecture recovery",
            "MDE",
            "Code generator",
            "Model transformation"
        ],
        "authors": [
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Johannes Härtel",
            "Ludovico Iovino",
            "Ralf Lämmel",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-019-00748-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Hardware architecture exploration: automatic exploration of distributed automotive hardware architectures",
        "submission-date": "2019/08",
        "publication-date": "2020/03",
        "abstract": "As the engineering of distributed embedded systems is getting more and more complex, due to increasingly sophisticated functionalities demanding more and more powerful hardware, model-based development of software-intensive embedded systems has become a de facto standard in recent years. Among other advantages, it enables design space exploration methods allowing for frontloading techniques which support a system architect already at early stages of development. In this paper, we want to present an approach which is capable of automatically generating automotive E/E architectures (electric/electronic architecture; in-car network of processing units and buses). Based on the concept of viewpoints, we will introducededicatedtechnicalmeta-models,alanguagetoformallydescribeahardwarearchitectureexplorationproblemandan automatic exploration approach using satisﬁability modulo theories. We will furthermore introduce a dedicated methodology and show how an exploration integrates into a system development process. In the end, we will evaluate our approach by applying it to an industrial use case provided by Continental.",
        "keywords": [
            "E/E architecture",
            "Design space exploration",
            "Automotive"
        ],
        "authors": [
            "Johannes Eder",
            "Sebastian Voss",
            "Andreas Bayha",
            "Alexandru Ipatiov",
            "Maged Khalil"
        ],
        "file_path": "data/sosym-all/s10270-020-00786-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using DSLs to manage consistency in long-lived enterprise language speciﬁcations",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Modern enterprise systems are likely to have a very long life. Their speciﬁcations therefore need to employ mechanisms that allow them to evolve during their lifetime; where they exploit generic components, these must be adaptable for use in novel situations. The paper looks at some of the issues that arise from this requirement, and how the exploitation of domain-speciﬁc language technologies in the tool-chain can assist in maintaining consistency of the speciﬁcation as a whole. First, it reviews the ﬁnal state of the family of standards supporting the ODP Enterprise Language, which is intended to handle this kind of application. In particular, it looks at the way the framework for deﬁning policies can be used to accommodate changing requirements during the lifetime of an evolving system. It also looks at the way the idea of deontic tokens enables factoring out of the management of obligations from the basic behaviour of interacting system components. It then proposes a roadmap for building tools that can be used to unify the constraints from different areas of concern into a single speciﬁcation. The approach taken is to exploit the power of domain-speciﬁc languages (DSLs) to allow designers in the various areas of concern to provide their input in terms natural to them. Finally, it looks at the way this approach promotes the establishment of a robust tool-chain capable of handling the evolution and scalability of enterprise systems. The paper uses a running example from the e-health domain to show how speciﬁc areas identiﬁed in the e-health standards can lead to language deﬁnitions, and so to tooling, that can be used to manage uniﬁed, system-wide speciﬁcations.",
        "keywords": [
            "Open Distributed Processing",
            "Domain Speciﬁc Languages",
            "Policies",
            "Deontic Tokens"
        ],
        "authors": [
            "Peter Linington",
            "Zoran Milosevic",
            "Akira Tanaka",
            "Igor Dejanovi´c"
        ],
        "file_path": "data/sosym-all/s10270-024-01243-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Nesting in Euler Diagrams: syntax, semantics and construction",
        "submission-date": "2003/10",
        "publication-date": "2003/10",
        "abstract": "This paper considers the notion of nesting in Euler diagrams, and how nesting affects the interpretation and construction of such diagrams. After setting up the necessary definitions for concrete Euler diagrams (drawn in the plane) and abstract diagrams (having just formal structure), the notion of nestedness is defined at both concrete and abstract levels. The concept of a dual graph is used to give an alternative condition for a drawable abstract Euler diagram to be nested. The natural progression to the diagram semantics is explored and we present a “nested form” for diagram semantics. We describe how this work supports tool-building for diagrams, and how effective we might expect this support to be in terms of the proportion of nested diagrams.",
        "keywords": [
            "Visual formalisms",
            "Formal methods",
            "Euler diagrams",
            "Nested Euler diagrams",
            "Diagrammatic reasoning"
        ],
        "authors": [
            "Jean Flower",
            "John Howse",
            "John Taylor"
        ],
        "file_path": "data/sosym-all/s10270-003-0036-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Weaving variability into domain metamodels",
        "submission-date": "2010/02",
        "publication-date": "2010/12",
        "abstract": "Domain-speciﬁc modeling languages (DSMLs) are the essence of MDE. A DSML describes the concepts of a particular domain in a metamodel, as well as their rela- tionships. Using a DSML, it is possible to describe a wide range of different models that often share a common base and vary on some parts. On the one hand, some current approaches tend to distinguish the variability language from the DSMLs themselves, implying greater learning curve for DSMLs stakeholders and a significant overhead in product line engineering. On the other hand, approaches integrat- ing variability in DSMLs lack generality and tool support. Communicated by Andy Schuerr and Bran Selic. We argue that aspect-oriented modeling techniques enabling ﬂexible metamodel composition and results obtained by the software product line community to manage and resolve variability form the pillars for a solution for integrating variability into DSMLs. In this article, we consider vari- ability as an independent and generic aspect to be woven into the DSML. In particular, we detail how variability is woven and how to perform product line derivation. We vali- date our approach through the weaving of variability into two different metamodels: Ecore—widely used for DSML def- inition—and SmartAdapters, our aspect model weaver. These results emphasize how new abilities of the language can be provided by this means.",
        "keywords": [
            "Domain speciﬁc languages",
            "Model weaving",
            "Variability and software product lines"
        ],
        "authors": [
            "Gilles Perrouin",
            "Gilles Vanwormhoudt",
            "Brice Morin",
            "Philippe Lahire",
            "Olivier Barais",
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-010-0186-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The 2014 “State of the Journal” report",
        "submission-date": "2015/01",
        "publication-date": "2015/01",
        "abstract": "SoSyM continues to experience a high number of submissions. In 2014, 295 manuscripts were submitted, of which 69% were reviewed for regular issues, while the other 31% were submitted for special or theme sections or the industry voice column. The average number of days from submission to a final decision (that is, a final accept or reject) was 221days.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Geri Georg",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-014-0452-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling and enforcing access control policies in conversational user interfaces",
        "submission-date": "2022/11",
        "publication-date": "2023/11",
        "abstract": "Conversational user interfaces (CUIs), such as chatbots, are becoming a common component of many software systems. Although they are evolving in many directions (such as advanced language processing features, thanks to new AI-based developments), less attention has been paid to access control and other security concerns associated with CUIs, which may pose a clear risk to the systems they interface with. In this paper, we apply model-driven techniques to model and enforce access-control policies in CUIs. In particular, we present a fully ﬂedged framework to integrate the role-based access-control (RBAC) protocol into CUIs by: (1) modeling a set of access-control rules to specify permissions over the bot resources using a domain-speciﬁc language that tailors core RBAC concepts to the CUI domain; and (2) describing a mechanism to show the feasibility of automatically generating the infrastructure to evaluate and enforce the modeled access control policies at runtime.",
        "keywords": [
            "Model-driven engineering",
            "Conversational user interfaces",
            "CUIs",
            "Access-control",
            "RBAC"
        ],
        "authors": [
            "Elena Planas",
            "Salvador Martínez",
            "Marco Brambilla",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-023-01131-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Experience with model-based performance, reliability, and adaptability assessment of a complex industrial architecture",
        "submission-date": "2011/07",
        "publication-date": "2012/09",
        "abstract": "In this paper, we report on our experience with the application of validated models to assess performance, reliability, and adaptability of a complex mission critical system that is being developed to dynamically monitor and control the position of an oil-drilling platform. We present real-time modeling results that show that all tasks are schedulable. We performed stochastic analysis of the distribution of task execution time as a function of the number of system interfaces. We report on the variability of task execution times for the expected system conﬁgurations. In addition, we have executed a system library for an important task inside the performance model simulator. We report on the measured algorithm convergence as a function of the number of vessel thrusters. We have also studied the system architecture adaptability by comparing the documented system architecture and the implemented source code. We report on the adaptability ﬁndings and the recommendations we were able to provide to the system’s architect. Finally, we have developed models of hardware and software reliability. We report on hardware and software reliability results based on the evaluation of the system architecture.",
        "keywords": [
            "Performance",
            "Reliability",
            "Adaptability"
        ],
        "authors": [
            "Daniel Dominguez Gouvêa",
            "Cyro de A. Assis D. Muniz",
            "Gilson A. Pinto",
            "Alberto Avritzer",
            "Rosa Maria Meri Leão",
            "Edmundo de Souza e Silva",
            "Morganna Carmem Diniz",
            "Vittorio Cortellessa",
            "Luca Berardinelli",
            "Julius C. B. Leite",
            "Daniel Mossé",
            "Yuanfang Cai",
            "Michael Dalton",
            "Lucia Happe",
            "Anne Koziolek"
        ],
        "file_path": "data/sosym-all/s10270-012-0264-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A taxonomy of tool-related issues affecting the adoption of model-driven engineering",
        "submission-date": "2014/09",
        "publication-date": "2015/08",
        "abstract": "Although poor tool support is often blamed for the low uptake of model-driven engineering (MDE), recent studies have shown that adoption problems are as likely to be down to social and organizational factors as with tooling issues. This article discusses the impact of tools on MDE adoption and practice and does so while placing tooling within a broader organizational context. The article revisits previous data on MDE use in industry (19 in-depth interviews with MDE practitioners) and reanalyzes that data through the speciﬁc lens of MDE tools in an attempt to identify and categorize the issues that users had with the tools they adopted. In addition, the article presents new data: 20 new interviews in two speciﬁc companies—and analyzes it through the same lens. A key contribution of the paper is a loose taxonomy of tool-related considerations, based on empirical industry data, which can be used to reﬂect on the tooling landscape as well as inform future research on MDE tools.",
        "keywords": [
            "Model-driven engineering",
            "Modeling tools",
            "Organizational change"
        ],
        "authors": [
            "Jon Whittle",
            "John Hutchinson",
            "Mark Rounceﬁeld",
            "Håkan Burden",
            "Rogardt Heldal"
        ],
        "file_path": "data/sosym-all/s10270-015-0487-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A cross-technology benchmark for incremental graph queries",
        "submission-date": "2020/12",
        "publication-date": "2021/12",
        "abstract": "To cope with the increased complexity of systems, models are used to capture what is considered the essence of a system. Such models are typically represented as a graph, which is queried to gain insight into the modelled system. Often, the results of these queries need to be adjusted according to updated requirements and are therefore a subject of maintenance activities. It is thus necessary to support writing model queries with adequate languages. However, in order to stay meaningful, the analysis results need to be refreshed as soon as the underlying models change. Therefore, a good execution speed is mandatory in order to cope with frequent model changes. In this paper, we propose a benchmark to assess model query technologies in the presence of model change sequences in the domain of social media. We present solutions to this benchmark in a variety of 11 different tools and compare them with respect to explicitness of incrementalization, asymptotic complexity and performance.",
        "keywords": [
            "Graph queries",
            "Graph analytics",
            "Model-driven engineering",
            "Performance benchmark",
            "Graph databases",
            "relational databases",
            "Incremental queries",
            "Incremental computing"
        ],
        "authors": [
            "Georg Hinkel",
            "Antonio Garcia-Dominguez",
            "René Schöne",
            "Artur Boronat",
            "Massimo Tisi",
            "Théo Le Calvar",
            "Frederic Jouault",
            "József Marton",
            "Tamás Nyíri",
            "János Benjamin Antal",
            "Márton Elekes",
            "Gábor Szárnyas"
        ],
        "file_path": "data/sosym-all/s10270-021-00927-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On Enterprise-Grade Tool Support for DEMO",
        "submission-date": "2021/03",
        "publication-date": "2021/07",
        "abstract": "The Design and Engineering Methodology for Organisations (DEMO) is a core method within the discipline of enterprise engineering. It enables the creation of so-called essential models of enterprises. Such models are enterprise models that aim to focus on the organisational essence of an enterprise by leaving out (as much as possible) details of the socio-technical implementation. The organisational essence is then expressed primarily in terms of the actor roles involved, and the business transactions between these roles. The DEMO method has a firm theoretical foundation. At the same time, there is an increasing uptake of DEMO in practice. This also results in a need for enterprise-grade tool support for the use of the method. In this paper, we report on a study concerning the selection, configuration, and extension, of an enterprise-grade tool platform to support the use of DEMO in practice. The configuration of the selected tool framework to support DEMO modelling, provided general insights regarding the development of enterprise-grade tool support for (model-driven) methods such as DEMO, while also providing feedback on the consistency and completeness of the DEMO specification language; the specification language that accompanies the DEMO method.",
        "keywords": [
            "Enterprise engineering",
            "DEMO",
            "Modelling tools"
        ],
        "authors": [
            "Mark A. T. Mulder",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-021-00911-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OMEGA: correct development of real time and embedded systems",
        "submission-date": "2007/11",
        "publication-date": "2008/01",
        "abstract": "Building embedded real time software systems of guaranteed quality, in a cost-effective manner, is an important technological challenge. Component-based design leads to a need for a new architecture-based software engineering practice in which complex systems are built by composing avai-lable components with known properties and evaluating the impact of local design choices on their global behaviour. This requires new tool support for their development. The aim of the Omega project1 was to contribute to this new development paradigm by providing a framework for model-based development of real-time and embedded sys-tems. It should enable a consistent use of models by advanced analysis tools on one hand and commercial code generators on the other. The project has obtained a number of interesting results concerning the following three main work directions.",
        "keywords": [
            "Support for object oriented",
            "Model-based",
            "Component-based system design",
            "UML",
            "Type-checking",
            "Model checking",
            "Theorem proving",
            "Real time",
            "Embedded systems"
        ],
        "authors": [
            "Susanne Graf"
        ],
        "file_path": "data/sosym-all/s10270-007-0077-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Erratum to: Introduction to the theme issue on variability modeling of software-intensive systems",
        "submission-date": "2015/11",
        "publication-date": "2015/11",
        "abstract": "Unfortunately there is a mistake in the author information for references [1, 5] in the text. The correct information is (as in the references itself): “Bonifácio, Borba, Ferraz, and Accioly (‘Empirical assessment of two approaches for specifying software prod-uct lines use case scenarios’ [5]) report the results of an empirical investigation of two different speciﬁcation approaches for software product line scenarios, . . .” and “In ‘The shape of feature code: an investigation of twenty c-preprocesor-based system’ [1] Queiroz, Passos, Valente, Hunsen, Apel, and Czarnecki investigate twenty projects using C preprocessor directives for variability management.”",
        "keywords": [],
        "authors": [
            "Andrzej Wa˛sowski",
            "Thorsten Weyer"
        ],
        "file_path": "data/sosym-all/s10270-015-0507-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reﬂection on the differences between modeling and programming",
        "submission-date": "2022/11",
        "publication-date": "2022/11",
        "abstract": "For a 2012 editorial [1], the relationships between modeling and programming languages were discussed. A decade later, it seems appropriate to revisit this issue because there is still not a widely accepted consensus on the main purpose of modeling languages and their models in software development. In this editorial, we highlight the most important aspects when identifying commonalities and differences between modeling and programming languages. We are fully aware that a detailed examination of all these relationships, properties, and their interdependencies is far beyond this short reﬂection.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-01057-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Context-aware modeling for knowledge-intensive medicinal product development processes",
        "submission-date": "2022/02",
        "publication-date": "2022/12",
        "abstract": "Due to their unique characteristics, knowledge-intensive processes (KiPs) are difﬁcult to capture with conventional modeling and management approaches. One such KiP is the advanced therapy medicinal product (ATMP) development process. ATMPs are highly innovative medicinal products that are based on biomedical technology. ATMP development processes need to comply with complex regulatory frameworks. Currently, biomedical scientists that develop ATMPs manage the regulatory aspects of the ATMP development processes in an ad hoc fashion, resulting in inefﬁciencies such as reworks or even withdrawal of ATMPs from the market. This paper presents an explorative case study in which we use Enterprise Modeling and Context-aware Business Processes to support ATMP scientists in managing the regulatory aspects of ATMP development processes more efﬁciently and effectively. In our explorative case study, we use enterprise models to describe the important concepts and views in ATMP development processes. By introducing context-awareness to the models, we support ATMP scientists in performing relevant tasks to address the regulatory requirements efﬁciently and effectively under different contexts. We introduce the novel concept of execution-dependent dynamic context to properly deﬁne the context in ATMP development processes. Additionally, this paper takes a broader perspective on the case study by discussing the relevance of the solutions derived for the case study for other KiPs. Thereby this paper aims to present an exemplary approach for context-aware modeling of KiPs. The practical contribution of this paper are the models realized in a real-life ATMP development project. The scientiﬁc contribution of this paper is providing an exemplary approach for supporting knowledge workers who perform ﬂexible, KiPs under dynamic contexts and introducing the notion of execution-dependent dynamic context.",
        "keywords": [
            "Context-awareness",
            "Enterprise modeling",
            "Business process management",
            "Conceptual modeling",
            "Knowledge-intensive process"
        ],
        "authors": [
            "Zeynep Ozturk Yurt",
            "Rik Eshuis",
            "Anna Wilbik",
            "Irene Vanderfeesten"
        ],
        "file_path": "data/sosym-all/s10270-022-01070-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scalable model views over heterogeneous modeling technologies and resources",
        "submission-date": "2019/07",
        "publication-date": "2020/04",
        "abstract": "When engineering complex systems, models are typically used to represent various systems aspects. These models are often heterogeneous in terms of modeling languages, provenance, number or scale. As a result, the information actually relevant to engineers is usually split into different kinds of interrelated models. To be useful in practice, these models need to be properly integrated to provide global views over the system. This has to be made possible even when those models are serialized or stored in different formats adapted to their respective nature and scalability needs. Model view approaches have been proposed to tackle this issue. They provide uniﬁcation mechanisms to combine and query various different models in a transparent way. These views usually target speciﬁc engineering tasks such as system design, monitoring and evolution. In an industrial context, there can be many large-scale use cases where model views can be beneﬁcial, in order to trace runtime and design-time data, for example. However, existing model view solutions are generally designed to work on top of one single modeling technology (even though model import/export capabilities are sometimes provided). Moreover, they mostly rely on in-memory constructs and low-level modeling APIs that have not been designed to scale in the context of large models stored in different kinds of data sources. This paper presents a general solution to efﬁciently support scalable model views over heterogeneous modeling resources possibly handled via different modeling technologies. To this intent, it describes our integration approach between a model view framework and various modeling technologies providing access to multiple types of modeling resources (e.g., in XML/XMI, CSV, databases). It also presents how queries on such model views can be executed efﬁciently by beneﬁting from the optimization of the different model technologies and underlying persistence backends. Our solution has been evaluated on a practical large-scale use case provided by the industry-driven European MegaM@Rt2 project that aims at implementing a runtime ↔design time feedback loop. The corresponding EMF-based tooling support, modeling artifacts and reproducible benchmarks are all available online.",
        "keywords": [
            "Modeling",
            "Views",
            "Heterogeneity",
            "Scalability",
            "Persistence",
            "Database",
            "Design time",
            "Runtime"
        ],
        "authors": [
            "Hugo Bruneliere",
            "Florent Marchand de Kerchove",
            "Gwendal Daniel",
            "Sina Madani",
            "Dimitris Kolovos",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-020-00794-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incremental execution of temporal graph queries over runtime models with history and its applications",
        "submission-date": "2021/03",
        "publication-date": "2021/12",
        "abstract": "Modern software systems are intricate and operate in highly dynamic environments for which few assumptions can be made at design-time. This setting has sparked an interest in solutions that use a runtime model which reﬂects the system state and operational context to monitor and adapt the system in reaction to changes during its runtime. Few solutions focus on the evolution of the model over time, i.e., its history, although history is required for monitoring temporal behaviors and may enable more informed decision-making. One reason is that handling the history of a runtime model poses an important technical challenge, as it requires tracing a part of the model over multiple model snapshots in a timely manner. Additionally, the runtime setting calls for memory-efﬁcient measures to store and check these snapshots. Following the common practice of representing a runtime model as a typed attributed graph, we introduce a language which supports the formulation of temporal graph queries, i.e., queries on the ordering and timing in which structural changes in the history of a runtime model occurred. We present a querying scheme for the execution of temporal graph queries over history-aware runtime models. Features such as temporal logic operators in queries, the incremental execution, the option to discard history that is no longer relevant to queries, and the in-memory storage of the model, distinguish our scheme from relevant solutions. By incorporating temporal operators, temporal graph queries can be used for runtime monitoring of temporal logic formulas. Building on this capability, we present an implementation of the scheme that is evaluated for runtime querying, monitoring, and adaptation scenarios from two application domains.",
        "keywords": [
            "Runtime models",
            "History-awareness",
            "Historic data",
            "Temporal graph queries",
            "Incremental pattern matching",
            "Runtime monitoring",
            "Self-adaptive systems"
        ],
        "authors": [
            "Lucas Sakizloglou",
            "Sona Ghahremani",
            "Matthias Barkowsky",
            "Holger Giese"
        ],
        "file_path": "data/sosym-all/s10270-021-00950-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Empirical evaluation of CMMN models: a collaborative process case study",
        "submission-date": "2018/11",
        "publication-date": "2020/06",
        "abstract": "Case Management Model and Notation (CMMN) was introduced by the Object Management Group as an alternative modeling language, targeting human-centric processes characterized by lack of structure and agility. However, although it is adequately supported by well-known process management tools, CMMN applicability as a modeling language is being questioned in practice. In this work, an empirical evaluation of CMMN models is presented, through a real-world case study where CMMN has been used for the analysis and implementation of a collaborative process by independent groups of process engineers. Their experience is being discussed, based on their modeling perspective. The produced models in the analysis and implementation phase are evaluated, using pre-existing metrics customized for CMMN. Based on the experience of engineers, CMMN applicability is evaluated, highlighting aspects in which its application might be limited, that should be addressed.",
        "keywords": [
            "Process modeling",
            "Collaborative processes",
            "Modeling perspectives",
            "Case Management Model and Notation",
            "Evaluation metrics",
            "Empirical evaluation"
        ],
        "authors": [
            "Ioannis Routis",
            "Mara Nikolaidou",
            "Dimosthenis Anagnostopoulos"
        ],
        "file_path": "data/sosym-all/s10270-020-00802-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "End-to-end model-transformation comprehension through ﬁne-grained traceability information",
        "submission-date": "2016/10",
        "publication-date": "2017/06",
        "abstract": "Abstract The construction and maintenance of model-to-\nmodel and model-to-text transformations pose numerous\nchallenges to novice and expert developers. A key chal-\nlenge involves tracing dependency relationships between\nartifacts of a transformation ecosystem. This is required\nto assess the impact of metamodel evolution, to determine\nmetamodel coverage, and to debug complex transformation\nexpressions. This paper presents an empirical study that\ninvestigates the performance of developers reﬂecting on the\nexecution semantics of model-to-model and model-to-text\ntransformations. We measured the accuracy and efﬁciency\nof 25 developers completing a variety of traceability-driven\ntasks in two model-based code generators. We compared the\nperformance of developers using ChainTracker, a traceabil-\nity analysis environment developed by our team, and that of\ndevelopers using Eclipse Modeling. We present statistically\nsigniﬁcant evidence that ChainTracker improves the perfor-\nmance of developers reﬂecting on the execution semantics of\ntransformation ecosystems. We discuss how developers sup-\nported by off-the-shelf development environments are unable\nto effectively identify dependency relationships in nontrivial\nmodel-transformation chains.",
        "keywords": [
            "Model-transformation comprehension",
            "Transformation comprehension",
            "Traceability analysis",
            "Software maintenance",
            "Development environments"
        ],
        "authors": [
            "Victor Guana",
            "Eleni Stroulia"
        ],
        "file_path": "data/sosym-all/s10270-017-0602-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "RBPMN: the value of roles for business process modeling",
        "submission-date": "2023/01",
        "publication-date": "2024/08",
        "abstract": "Business process modeling is essential for organizations to comprehend, analyze, and enhance their business operations. The\nbusiness process model and notation (BPMN) is a standard widely adopted for illustrating business processes. However, it falls\nshort when modeling roles, interactions, and responsibilities within complex modern processes that involve digital, human,\nand non-human entities, typically found in cyber-physical systems (CPS). In this paper, we introduce Role-based BPMN\n(RBPMN), a standard-compliant extension of BPMN 2.0 that distinctly depicts roles and their interactions within business\nprocesses. We underscore the value of RBPMN and a role-based context modeling approach through a modeling example in\nCPS that facilitates the representation of role-based variations in the process ﬂow, namely a production process in a smart\nfactory. Our ﬁndings suggest that RBPMN is a valuable BPMN extension that enhances the expressiveness, variability, and\ncomprehensiveness of business process models, especially in complex and context-sensitive processes.",
        "keywords": [
            "Business process modeling",
            "Role modeling",
            "Cyber-physical systems"
        ],
        "authors": [
            "Tarek Skouti\nRonny Seiger\nFrank J. Furrer\nSusanne Strahringer"
        ],
        "file_path": "data/sosym-all/s10270-024-01202-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reflections on the standardization of SysML 2",
        "submission-date": "2021/03",
        "publication-date": "2021/03",
        "abstract": "In 2018, we wrote an editorial entitled “Agile Model-Based System Development” [GR18c], in which we made several observations: 1. “In fact, modeling is at the heart of almost any engineering discipline. Thus, it is not surprising that our engineering colleagues have developed a detailed portfolio of modeling techniques to describe their systems in various perspectives, viewpoints, and abstractions.” 2. “However, the state-of-the-art in systems modeling has several challenges, where each modeling aspect and view is often assisted by an individual modeling and analysis tool. Data exchange between the tools is complicated, even though mostly automated, but suffers from concerns about robustness, completeness of the mappings between the models, as well as regular version upgrades of tools. A second problem is that these mappings between models are not easy to standardize, because different projects use the same modeling languages in different forms (semantics), which enforces configurable mappings or individually developed translations per project.” 3. “Traditional engineers use system models to design and understand the product through analysis and automation of engineering tasks. Therefore, models should be more than just documentation artifacts. Each model that captures a perspective of the project should either become a part of the product construction or should be used for automated validation and test quality management.” We also discussed that traditional systems development has not yet reached the maturity of agile software development for a number of reasons, including the far too long, complex, and dependent tool chain. Currently, the development of the updated standard SysML 2.0 is a larger effort, where the stakeholders are trying to accommodate a number of these problems:",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-021-00881-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving timing analysis effectiveness for scenario-based specifications by combining SAT and LP techniques",
        "submission-date": "2021/04",
        "publication-date": "2022/02",
        "abstract": "Open environmental software systems are often time-sensitive, as they need to respond to other entities within the systems and/or in the environments promptly. The timing requirements are therefore an essential part of the system correctness. Scenario-based specifications (SBS) such as message sequence charts and UML interaction models play an important role in specifying open environmental software systems since they intuitively model interactions between different entities. While modelling these systems, the timing requirements can be specified as timing constraints. In this paper, we study the problem of checking the timing consistency of SBS with timing constraints. Although this problem can be transformed into a reachability analysis problem, checking its reachability can still be time-consuming. Therefore, we propose a novel SAT and linear programming (LP) collaborative timing analysis approach named Tassat for the bounded timing analysis of SBS. Instead of using depth-ﬁrst traversal algorithms, Tassat encodes the structures of the SBS into propositional formulas and adopts SAT solvers to find candidate paths. The timing analysis of candidate paths is then transformed to LP problems, where the irreducible infeasible set of the infeasible paths can be utilized to filter out candidate paths for checking. In addition, we propose an enhanced version of the approach that extends the bounded analysis results to the entire models if the infeasible path segments do not contain intermediate loops. The enhanced algorithm can prove that the given SBS satisfy the required properties on any bound condition. The experimental results show that Tassat is effective and has better performance than existing tools in terms of both time consumption and memory footprint.",
        "keywords": [
            "Scenario-based specifications",
            "Message sequence charts",
            "Model checking",
            "Reachability analysis"
        ],
        "authors": [
            "Longlong Lu",
            "Minxue Pan",
            "Tian Zhang",
            "Xuandong Li"
        ],
        "file_path": "data/sosym-all/s10270-022-00980-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven system-level validation and veriﬁcation on the space software domain",
        "submission-date": "2020/11",
        "publication-date": "2021/11",
        "abstract": "The development process of on-board software applications can beneﬁt from model-driven engineering techniques. Model validation and model transformations can be applied to drive the activities of speciﬁcation, requirements deﬁnition, and system-level validation and veriﬁcation according to the space software engineering standards ECSS-E-ST-40 and ECSS-Q-ST-80. This paper presents a model-driven approach to completing these activities by avoiding inconsistencies between the documents that support them and providing the ability to automatically generate the system-level validation tests that are run on the Ground Support Equipment and the matrices required to complete the software veriﬁcation. A demonstrator of the approach has been built using as a proof of concept a subset of the functionality of the software of the control unit of the Energetic Particle Detector instrument on-board Solar Orbiter.",
        "keywords": [
            "MDE",
            "Validation",
            "Veriﬁcation",
            "Space",
            "Software",
            "ECSS"
        ],
        "authors": [
            "Aarón Montalvo",
            "Pablo Parra",
            "Óscar Rodríguez Polo",
            "Alberto Carrasco",
            "Antonio Da Silva",
            "Agustín Martínez",
            "Sebastián Sánchez"
        ],
        "file_path": "data/sosym-all/s10270-021-00940-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSyM reflections: the 2019 “state of the journal” report",
        "submission-date": "2020/01",
        "publication-date": "2020/01",
        "abstract": "We are delighted to bring you this first issue of SoSyM for 2020! Over the past year, SoSyM has achieved many new milestones. As reported in the next section, the SoSyM Impact Factor has increased by almost a full point, to its highest historical level and the number of downloads continues to rise. This suggests that the journal remains very healthy and that the software systems and modeling research community continues to thrive!",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-020-00778-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Train Benchmark: cross-technology performance evaluation of continuous model queries",
        "submission-date": "2015/10",
        "publication-date": "2017/01",
        "abstract": "In model-driven development of safety-critical systems (like automotive, avionics or railways), well-formedness of models is repeatedly validated in order to detect design ﬂaws as early as possible. In many industrial tools, validation rules are still often implemented by a large amount of imperative model traversal code which makes those rule implementations complicated and hard to maintain. Additionally, as models are rapidly increasing in size and complexity, efﬁcient execution of validation rules is challenging for the currently available tools. Checking well-formedness constraints can be captured by declarative queries over graph models, while model update operations can be speciﬁed as model transformations. This paper presents a benchmark for systematically assessing the scalability of validating and revalidating well-formedness constraints over large graph models. The benchmark deﬁnes well-formedness validation scenarios in the railway domain: a metamodel, an instance model generator and a set of well-formedness constraints captured by queries, fault injection and repair operations (imitating the work of systems engineers by model transformations). The benchmark focuses on the performance of query evaluation, i.e. its execution time and memory consumption, with a particular emphasis on reevaluation. We demonstrate that the benchmark can be adopted to various technologies and query engines, including modeling tools; relational, graph and semantic databases. The Train Benchmark is available as an open-source project with continuous builds from https://github.com/FTSRG/trainbenchmark.",
        "keywords": [
            "Well-formedness validation",
            "Query evaluation",
            "Performance benchmark",
            "Graph databases",
            "Semantic databases",
            "Relational databases"
        ],
        "authors": [
            "Gábor Szárnyas",
            "Benedek Izsó",
            "István Ráth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-016-0571-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Adaptive caching for operation-based versioning of models",
        "submission-date": "2024/01",
        "publication-date": "2025/01",
        "abstract": "In a collaborative multi-user model-driven engineering context, it becomes important to track who changed what model part how and why. Operation-based versioning addresses this need by persisting a meaningful edit history which enables a single user to navigate through a model’s evolution over time, to analyze arbitrary previous model versions, or to trace the impact of an operation. However, to load a distinct prior version, it must be restored by reapplying all previous operations, which is time-consuming and, thus, interrupts a user’s workﬂow. Caching with a ﬁxed distance between caches helps to overcome this problem to the cost of increasing memory requirements. Further, there is no caching approach supporting branches, merges, and possibly resolved conﬂicts. We propose two advanced caching strategies for operation-based versioning capable of the previously mentioned features: zonal and adaptive caching. Both strategies reduce the memory in use by not applying the same static distance between two caches across the whole edit history. Instead, the distance increases depending on a version’s age and its distance to a branch’s head. Both strategies aim to reduce the restoration time of arbitrary prior versions below a threshold to not interrupt a user’s ﬂow of thought. Zonal caching employs predeﬁned distances compatible with a broad range of model sizes. In contrast, adaptive caching derives the distances individually depending on the initial time to load the model on a user’s computer and the model’s size.We conducted controlled experiments with models of varying sizes and compared the time to restore model versions and the memory in use for no caching, caching with static distances, zonal, and adaptive strategies on different computers. The developed strategies decrease the time to restore a version remarkably while using less memory than static caching. Our results show that for all considered systems and models individual adaptive caching reduces memory usage even further compared to zone-based caching while still satisfying application responsiveness requirements.",
        "keywords": [
            "Operation-based versioning",
            "Modeling",
            "Caching",
            "Event-sourcing"
        ],
        "authors": [
            "Jakob Pietron\nHeiko Raab\nMatthias Tichy"
        ],
        "file_path": "data/sosym-all/s10270-024-01214-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special issue on MODELS 2009",
        "submission-date": "2011/04",
        "publication-date": "2011/04",
        "abstract": "The pioneering organizers of the ﬁrst “UML” ‘98 workshop in Mulhouse, France in the summer of 1998 could hardly have anticipated that, in little over a decade, their initiative would blossom into today’s highly successful MODELS conference series. MODELS is unquestionably the premier annual gathering of researchers and practitioners working on what is starting to emerge as a bona ﬁde new technical discipline: model-based engineering of systems and software—a discipline distinguished by the fact that it fundamentally relies on the development and exploitation of models and supporting technologies.",
        "keywords": [],
        "authors": [
            "Andy Schürr",
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-011-0200-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling of, for, and with digital twins",
        "submission-date": "2022/09",
        "publication-date": "2022/09",
        "abstract": "There exists a plethora of definitions for Digital Twins (DTs). Although there is a rough convergence toward a common definition, there is no consensus about what a digital twin actually comprises. We have found differences in the definition given in publications, such as issues of narrowness, by focusing on specific use cases, domains, or technologies in the definition.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-01046-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A technique for evaluating and improving the semantic transparency of modeling language notations",
        "submission-date": "2020/09",
        "publication-date": "2021/06",
        "abstract": "The notation of a modeling language is of paramount importance for its efﬁcient use and the correct comprehension of created models. A graphical notation, especially for domain-speciﬁc modeling languages, should therefore be aligned to the knowledge, beliefs, and expectations of the targeted model users. One quality attributed to notations is their semantic transparency, indicating the extent to which a notation intuitively suggests its meaning to untrained users. Method engineers should thus aim at semantic transparency for realizing intuitively understandable notations. However, notation design is often treated poorly—if at all—in method engineering methodologies. This paper proposes a technique that, based on iterative evaluation and improvement tasks, steers the notation toward semantic transparency. The approach can be efﬁciently applied to arbitrary modeling languages and allows easy integration into existing modeling language engineering methodologies. We show the feasibility of the technique by reporting on two cycles of Action Design Research including the evaluation and improvement of the semantic transparency of the Process-Goal Alignment modeling language notation. An empirical evaluation comparing the new notation against the initial one shows the effectiveness of the technique.",
        "keywords": [
            "Modeling language",
            "Notation",
            "Concrete syntax",
            "Semantic transparency",
            "Empirical evaluation"
        ],
        "authors": [
            "Dominik Bork",
            "Ben Roelens"
        ],
        "file_path": "data/sosym-all/s10270-021-00895-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal veriﬁcation of software source code through semi-automatic modeling",
        "submission-date": "2002/11",
        "publication-date": "2004/01",
        "abstract": "We describe the experience of modeling and formally verifying a software cache algorithm using the model checker RuleBase. Contrary to prevailing wisdom, we used a highly detailed model created directly from the C code itself, rather than a high-level abstract model.",
        "keywords": [
            "Software model checking",
            "Software veriﬁcation",
            "Program veriﬁcation",
            "Functional veriﬁcation"
        ],
        "authors": [
            "Cindy Eisner"
        ],
        "file_path": "data/sosym-all/s10270-003-0042-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A language-parametric test ampliﬁcation framework for executable domain-speciﬁc languages",
        "submission-date": "2023/05",
        "publication-date": "2025/05",
        "abstract": "Behavioral models are important assets that must be thoroughly veriﬁed early in the design process. This can be achieved with manually-written test cases that embed carefully hand-picked domain-speciﬁc input data. However, such test cases may not always reach the desired level of quality, such as high coverage or being able to localize faults efﬁciently. Test ampliﬁcation is an interesting emergent approach to improve a test suite by automatically generating new test cases out of existing manually-written ones. Yet, while ad-hoc test ampliﬁcation solutions have been proposed for a few programming languages, no solution currently exists for amplifying the test suites of behavioral models. In order to ﬁll this gap, we propose an automated and generic test ampliﬁcation approach for executable domain-speciﬁc languages (DSLs). Hence, given an executable DSL, a conforming behavioral model, and an existing test suite, our approach synthesizes new regression test cases in three steps: (i) generating new test inputs by applying a set of generic modiﬁers on the existing test inputs; (ii) running the model under test with new inputs and generating assertions from the execution traces; and (iii) selecting the new test cases that increase the initial test quality. We provide a textual DSL to control and conﬁgure the ampliﬁcation process, along with tool support for the whole approach atop the Eclipse GEMOC Studio. For assessment, we report on empirical evaluations over two different executable DSLs, which show improved test quality in terms of both coverage and mutation score.",
        "keywords": [
            "Test ampliﬁcation",
            "Regression testing",
            "Mutation testing",
            "Executable model",
            "Executable DSL"
        ],
        "authors": [
            "Faezeh Khorram",
            "Erwan Bousse",
            "Jean-Marie Mottu",
            "Gerson Sunyé",
            "Djamel Eddine Khelladi",
            "Pablo Gómez-Abajo",
            "Pablo C. Cañizares",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-025-01283-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Teaching model-driven engineering from a relational database perspective",
        "submission-date": "2014/09",
        "publication-date": "2015/08",
        "abstract": "We reinterpret MDE from the viewpoint of relational databases to provide an alternative way to understand, demonstrate, and teach MDE using concepts and technologies that should be familiar to undergraduates. We use (1) relational database schemas to express metamodels, (2) relational databases to express models, (3) Prolog to express constraints and M2M transformations, (4) Java tools to implement M2T and T2M transformations, and (5) Java to execute transformations. Application case studies and a user study illuminate the viability and beneﬁts of our approach.",
        "keywords": [
            "MDE",
            "Teaching",
            "Tools"
        ],
        "authors": [
            "Don Batory",
            "Maider Azanza"
        ],
        "file_path": "data/sosym-all/s10270-015-0488-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Compositional model analysis",
        "submission-date": "2020/03",
        "publication-date": "2020/03",
        "abstract": "Recently, one of the editors attended a Dagstuhl seminar about “Composing Model-Based Analysis Tools” organized by Francisco Durán, Robert Heinrich, Diego Pérez-Palacín, Carolyn L. Talcott, and Steffen Zschaler [1]. Never heard about Dagstuhl? Schloss Dagstuhl is a wonderful venue that offers invitation-only Computer Science seminars every week of the year (except Christmas). It regularly manages to bring together experienced scientists and young aspiring researchers with interest in a specific research theme to foster the discussion of exciting topics. Dagstuhl is located in a rural area of Germany, which helps to remove distractions from other concerns. Attendees are compelled to spend time together while focused on scientific and social discussions that are detailed, interesting, and fruitful. The local organizers and staff do everything to help participants focus on collaborative scientific activities. The food is exceptional, and the organization is always professional and attentive. An invitation to Dagstuhl is an honor that should be deeply considered when offered!\nSoSyM regularly benefits from Dagstuhl seminars through papers that originate from these discussions. The “Composing Model-Based Analysis Tools” seminar had approximately 50 attendees who explored various topics related to model analysis; in particular, how model analysis techniques can be modularized in such a way that they can be composed and built upon each other. There are various examples where simple analysis techniques (e.g., typing of expressions, completeness and reachability algorithms) are the foundation of analysis, with other desired characteristics (e.g., reliability, efficiency, and security metrics) built from the foundational concepts.\nThe participants at this seminar represented diverse backgrounds. They introduced many different ways models are used to describe views on systems and their (potentially hierarchically decomposed) components. One model typically describes a particular view on a part of the system (e.g., subsystem, component). Thus, many models are often used to describe a larger system. The multiple-models systems view can be contrasted with the Big-Blob-Gozilla-Model that covers everything, with limited potential for abstraction and thus reducing the benefits of modeling. A single large model also prevents reuse of any independently developed model and has some other drawbacks (e.g., less potential for collaborative design). The idea of compositional model analysis can benefit from much of the earlier research that intersected programming languages and software engineering to promote a notion of composition together with the idea of encapsulation (e.g., the works of Parnas, such as “On the criteria to be used in decomposing systems into modules” and “Modular structure of complex systems”).\nHowever, as models are being composed, it is an interesting question to ask whether the model composition structure and system composition structure are orthogonal, or exactly the same. Several examples have been discussed that support both variants (e.g., several models can be used to describe different aspects of the same system component, such as interface, internal data structure, behavioral constraints, or implementation; but, e.g., typically one architecture diagram describes many system components). The situation becomes more wicked as various viewpoints of the system are modeled using different modeling techniques, which enforces that not only models, but also the modeling languages need to be composed (e.g., usually quality analyses depend on very specific quality attributes evenly spread over many models, but defined within one sub-language; on the contrary, interaction analysis can be spread over many forms of interactions, imports, includes, and other forms of uses spear across many sub-languages).\nModeling is not a simple and easy task. Deep skills and expertise are needed to create models that are useful, well-structured, understandable, and correct. Models offer many benefits, from the sketching of new ideas to the full formal specification of a dynamic system. Many industry applica-",
        "keywords": [],
        "authors": [
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-020-00784-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on MODELS 2018",
        "submission-date": "2020/04",
        "publication-date": "2020/05",
        "abstract": "The MODELS conference series is the premier event for model-based software and systems engineering. This special section presents the five articles that resulted from an invitation to authors of the best papers at MODELS 2018 to submit revised and extended versions of their papers.",
        "keywords": [],
        "authors": [
            "Andrzej Wa˛sowski",
            "Richard F. Paige",
            "Øystein Haugen"
        ],
        "file_path": "data/sosym-all/s10270-020-00800-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Process mining using BPMN: relating event logs and process models",
        "submission-date": "2015/03",
        "publication-date": "2015/10",
        "abstract": "Process-aware information systems (PAIS) are systems relying on processes, which involve human and software resources to achieve concrete goals. There is a need to developapproachesformodeling,analysis,improvementand monitoring processes within PAIS. These approaches include process mining techniques used to discover process models from event logs, ﬁnd log and model deviations, and analyze performance characteristics of processes. The representational bias (a way to model processes) plays an important role in process mining. The BPMN 2.0 (Business Process Model and Notation) standard is widely used and allows to build conventional and understandable process models. In addition to the ﬂat control ﬂow perspective, subprocesses, data ﬂows, resources can be integrated within one BPMN diagram. This makes BPMN very attractive for both process miners and business users, since the control ﬂow perspec-tive can be integrated with data and resource perspectives discovered from event logs. In this paper, we describe and justify robust control ﬂow conversion algorithms, which provide the basis for more advanced BPMN-based discovery and conformance checking algorithms. Thus, on the basis of these conversion algorithms low-level models (such as Petri nets, causal nets and process trees) discovered from event logs using existing approaches can be represented in terms of BPMN. Moreover, we establish behavioral relations between Petri nets and BPMN models and use them to adopt existing conformance checking and performance analysis techniques in order to visualize conformance and performance infor-mation within a BPMN diagram. We believe that the results presented in this paper can be used for a wide variety of BPMN mining and conformance checking algorithms. We also provide metrics for the processes discovered before and after the conversion to BPMN structures. Cases for which conversion algorithms produce more compact or more complicated BPMN models in comparison with the initial models are identiﬁed.",
        "keywords": [
            "Process mining",
            "Process discovery",
            "Conformance checking",
            "BPMN (Business Process Model and Notation)",
            "Petri nets",
            "Bisimulation"
        ],
        "authors": [
            "Anna A. Kalenkova",
            "Wil M. P. van der Aalst",
            "Irina A. Lomazova",
            "Vladimir A. Rubin"
        ],
        "file_path": "data/sosym-all/s10270-015-0502-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling more software performance antipatterns in cyber-physical systems",
        "submission-date": "2022/11",
        "publication-date": "2023/12",
        "abstract": "The design of cyber-physical systems (CPS) is challenging due to the heterogeneity of software and hardware components that operate in uncertain environments (e.g., ﬂuctuating workloads), hence they are prone to performance issues. Software performance antipatterns could be a key means to tackle this challenge since they recognize design problems that may lead to unacceptable system performance. This manuscript focuses on modeling and analyzing a variegate set of software performance antipatterns with the goal of quantifying their performance impact on CPS. Starting from the speciﬁcation of eight software performance antipatterns, we build a baseline queuing network performance model that is properly extended to account for the corresponding bad practices. The approach is applied to a CPS consisting of a network of sensors and experimental results show that performance degradation can be traced back to software performance antipatterns. Sensitivity analysis investigates the peculiar characteristics of antipatterns, such as the frequency of checking the status of resources, that provides quantitative information to software designers to help them identify potential performance problems and their root causes. Quantifying the performance impact of antipatterns on CPS paves the way for future work enabling the automated refactoring of systems to remove these bad practices.",
        "keywords": [
            "Software modeling",
            "Software performance antipatterns",
            "Model-based performance analysis",
            "Cyber-physical systems"
        ],
        "authors": [
            "Riccardo Pinciroli",
            "Connie U. Smith",
            "Catia Trubiani"
        ],
        "file_path": "data/sosym-all/s10270-023-01137-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Concern coverage in base station development: an empirical investigation",
        "submission-date": "2010/02",
        "publication-date": "2011/01",
        "abstract": "Contemporary model driven development tools only partially support the abstractions occurring in complex embedded systems development. This article presents an interpretive case study in which architectural concerns important to seven engineers in a large product developing organization were compared to the views actually provided by the organization’s models. The paper’s main ﬁnding is an empirically grounded catalogue of architectural concerns for a large, complex embedded systems project, and an assessment of the degree to which the studied organization has managed to realize support for these concerns within economical and organizational constraints. In the studied case, 114 different architectural concerns were found to be important to the interviewed engineers. Of this sample, 75% were documented in models, structured text, or informal documentation, whereas 47% of all documented concerns were modeled. The paper’s conclusion is that current modeling languages and methods inadequately address the full set of concerns that are important to engineers in base station development.",
        "keywords": [
            "Model driven development (MDD)",
            "Software architecture",
            "Embedded systems",
            "Telecommunication systems"
        ],
        "authors": [
            "Lars Pareto",
            "Peter Eriksson",
            "Staffan Ehnebom"
        ],
        "file_path": "data/sosym-all/s10270-010-0188-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Supporting ﬁne-grained generative model-driven evolution",
        "submission-date": "2007/08",
        "publication-date": "2010/01",
        "abstract": "In the standard generative Model-driven Architecture (MDA), adapting the models of an existing system requires re-generation and restarting of that system. This is due to a strong separation between the modeling environment and the runtime environment. Certain current approaches remove this separation, allowing a system to be changed smoothly when the model changes. These approaches are, however, based on interpretation of modeling information rather than on generation, as in MDA. This paper describes an architecture that supports ﬁne-grained evolution combined with generative model-driven development. Fine-grained changes are applied in a generative model-driven way to a system that has itself been developed in this way. To achieve this, model changes must be propagated correctly toward impacted elements. The impact of a model change ﬂows along three dimensions: implementation, data (instances), and modeled dependencies. These three dimensions are explicitly represented in an integrated modeling-runtime environment to enable traceability. This implies a fundamental rethinking of MDA.",
        "keywords": [
            "Evolution",
            "Model-driven development",
            "Generative development",
            "Interpretive development"
        ],
        "authors": [
            "Theo Dirk Meijler",
            "Jan Pettersen Nytun",
            "Andreas Prinz",
            "Hans Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-009-0144-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An asynchronous communication model for distributed concurrent objects",
        "submission-date": "2005/02",
        "publication-date": "2006/08",
        "abstract": "Distributed systems are often modeled by objects that run concurrently, each with its own processor, and communicate by synchronous remote method calls. This may be satisfactory for tightly coupled systems, but in the distributed setting synchronous external calls lead to much waiting; at best resulting in inefficient use of processor capacity, at worst resulting in deadlock. Furthermore, it is difficult to combine active and passive behavior in concurrent objects. This paper proposes an object-oriented solution to these problems by means of asynchronous method calls and conditional processor release points. Although at the cost of additional internal nondeterminism in the objects, this approach seems attractive in asynchronous or unreliable environments. The concepts are integrated in a small object-oriented language with an operational semantics defined in rewriting logic, and illustrated by examples.",
        "keywords": [
            "Asynchronous method calls",
            "Concurrent objects",
            "Distributed systems",
            "Rewriting logic"
        ],
        "authors": [
            "Einar Broch Johnsen",
            "Olaf Owe"
        ],
        "file_path": "data/sosym-all/s10270-006-0011-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Conceptualization, measurement, and application of semantic transparency in visual notations\nA systematic literature review",
        "submission-date": "2020/09",
        "publication-date": "2021/05",
        "abstract": "Numerous visual notations are present in technical and business domains. Notations have to be cognitively effective to ease the planning, documentation, and communication of the domains’ concepts. Semantic transparency (ST) is one of the elementary principles that inﬂuence notations’ cognitive effectiveness. However, the principle is criticized for not being well deﬁned and challenges arise in the evaluations and applications of ST. Accordingly, this research’s objectives were to answer how the ST principle is deﬁned, operationalized, and evaluated in present notations as well as applied in the design of new notations in ICT and related areas. To meet these objectives, a systematic literature review was conducted with 94 studies passing the selection process criteria. The results reject one of the three aspects, which deﬁne semantic transparency, namely “ST is achieved with the use of icons.” Besides, taxonomies of related concepts and research methods, evaluation metrics, and other ﬁndings from this study can help to conduct veriﬁable ST-related experiments and applications, consequently improving the visual vocabularies of notations and effectiveness of the resulting diagrams.",
        "keywords": [
            "Semantic transparency",
            "Visual notations",
            "Cognitive effectiveness",
            "Systematic literature review"
        ],
        "authors": [
            "Saša Kuhar\nGregor Polanˇciˇc"
        ],
        "file_path": "data/sosym-all/s10270-021-00888-9.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Evaluating the effort of composing design models: a controlled experiment",
        "submission-date": "2013/06",
        "publication-date": "2014/05",
        "abstract": "Model composition plays a key role in many tasks in model-centric software development, e.g., evolving UML diagrams to add new features or reconciling models developed in parallel by different software development teams. However, based on our experience in previous empirical studies, one of the main impairments for the widespread adoption of composition techniques is the lack of empirical knowledge about their effects on developers’ effort. This problem applies to both existing categories of model composition techniques, i.e., speciﬁcation-based (e.g., Epsilon) and heuristic-based techniques (e.g., IBM RSA). This paper, therefore, reports on a controlled experiment that investigates the effort of (1) applying both categories of model composi-tion techniques and (2) detecting and resolving inconsistencies in the output composed models. We evaluate the techniques in 144 evolution scenarios, where 2,304 compositions of elements of UML class diagrams were produced. The main results suggest that (1) the employed heuristic-based techniques require less effort to produce the intended model than the chosen speciﬁcation-based technique, (2) there is no signiﬁcant difference in the correctness of the output composed models generated by these techniques, and (3) the use of manual heuristics for model composition outperforms their automated counterparts.",
        "keywords": [
            "Model composition effort",
            "Empirical studies",
            "Effort measurement"
        ],
        "authors": [
            "Kleinner Farias",
            "Alessandro Garcia",
            "Jon Whittle",
            "Christina von Flach Garcia Chavez",
            "Carlos Lucena"
        ],
        "file_path": "data/sosym-all/s10270-014-0408-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward live domain-speciﬁc languages\nFrom text differencing to adapting models at run time",
        "submission-date": "2016/06",
        "publication-date": "2017/08",
        "abstract": "Live programming is a style of development characterized by incremental change and immediate feedback. Instead of long edit-compile cycles, developers modify a running program by changing its source code, receiving immediate feedback as it instantly adapts in response. In this paper, we propose an approach to bridge the gap between running programs and textual domain-speciﬁc languages (DSLs). The ﬁrst step of our approach consists of applying a novel model differencing algorithm, tmdiff, to the textual DSL code. By leveraging ordinary text differencing and origin tracking, tmdiff produces deltas deﬁned in terms of the metamodel of a language. In the second step of our approach, the model deltas are applied at run time to update a running system, without having to restart it. Since the model deltas are derived from the static source code of the program, they are unaware of any run-time state maintained during model execution. We therefore propose a generic, dynamic patch architecture, rmpatch, which can be customized to cater for domain-speciﬁc state migration. We illustrate rmpatch in a case study of a live programming environment for a simple DSL implemented in Rascal for simultaneously deﬁning and executing state machines.",
        "keywords": [
            "Live programming",
            "Domain-speciﬁc languages",
            "Text differencing",
            "Model patching",
            "Adapting models",
            "Models at run time"
        ],
        "authors": [
            "Riemer van Rozen\nTijs van der Storm"
        ],
        "file_path": "data/sosym-all/s10270-017-0608-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "P-stable abstractions of hybrid systems",
        "submission-date": "2022/07",
        "publication-date": "2024/01",
        "abstract": "Stability is a fundamental requirement of dynamical systems. Most of the works concentrate on verifying stability for a given stability region. In this paper, we tackle the problem of synthesizing P-stable abstractions. Intuitively, the P-stable abstraction of a dynamical system characterizes the transitions between stability regions in response to external inputs. The stability regions are not given—rather, they are synthesized as their most precise representation with respect to a given set of predicates P. A P-stable abstraction is enriched by timing information derived from the duration of stabilization. We implement a synthesis algorithm in the framework of Abstract Interpretation that allows different degrees of approximation. We show the representational power of P-stable abstractions that provide a high-level account of the behavior of the system with respect to stability, and we experimentally evaluate the effectiveness of the algorithm in synthesizing P-stable abstractions for signiﬁcant systems.",
        "keywords": [
            "P-stable abstraction",
            "Hybrid systems",
            "Reverse engineering Abstract Interpretation",
            "Predicate abstraction",
            "Run-to-completion"
        ],
        "authors": [
            "Anna Becchi",
            "Alessandro Cimatti",
            "Enea Zaffanella"
        ],
        "file_path": "data/sosym-all/s10270-023-01145-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Engineering model transformations with transML",
        "submission-date": "2011/03",
        "publication-date": "2011/09",
        "abstract": "Model transformation is one of the pillars of model-driven engineering (MDE). The increasing complexity of systems and modelling languages has dramatically raised the complexity and size of model transformations as well. Even though many transformation languages and tools have been proposed in the last few years, most of them are directed to the implementation phase of transformation development. In this way, even though transformations should be built using sound engineering principles—just like any other kind of software—there is currently a lack of cohesive support for the other phases of the transformation development, like requirements, analysis, design and testing. In this paper, we propose a uniﬁed family of languages to cover the life cycle of transformation development enabling the engineering of transformations. Moreover, following an MDE approach, we provide tools to partially automate the progressive reﬁnement of models between the different phases and the generation of code for several transformation implementation languages.",
        "keywords": [
            "Model-driven engineering",
            "Model transformation",
            "Domain-speciﬁc languages"
        ],
        "authors": [
            "Esther Guerra",
            "Juan de Lara",
            "Dimitrios S. Kolovos",
            "Richard F. Paige",
            "Osmar Marchi dos Santos"
        ],
        "file_path": "data/sosym-all/s10270-011-0211-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "transML"
        }
    },
    {
        "title": "Speciﬁcation-driven predictive business process monitoring",
        "submission-date": "2018/11",
        "publication-date": "2019/11",
        "abstract": "Predictive analysis in business process monitoring aims at forecasting the future information of a running business process. The prediction is typically made based on the model extracted from historical process execution logs (event logs). In practice, different business domains might require different kinds of predictions. Hence, it is important to have a means for properly specifying the desired prediction tasks, and a mechanism to deal with these various prediction tasks. Although there have been many studies in this area, they mostly focus on a speciﬁc prediction task. This work introduces a language for specifying the desired prediction tasks, and this language allows us to express various kinds of prediction tasks. This work also presents a mechanism for automatically creating the corresponding prediction model based on the given speciﬁcation. Differently from previous studies, instead of focusing on a particular prediction task, we present an approach to deal with various prediction tasks based on the given speciﬁcation of the desired prediction tasks. We also provide an implementation of the approach which is used to conduct experiments using real-life event logs.",
        "keywords": [
            "Predictive business process monitoring",
            "Prediction task speciﬁcation language",
            "Automatic prediction model creation",
            "Machine learning-based prediction"
        ],
        "authors": [
            "Ario Santoso",
            "Michael Felderer"
        ],
        "file_path": "data/sosym-all/s10270-019-00761-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Circulise4LCE, a model-driven sustainability development framework for local circular economy engineering and simulation",
        "submission-date": "2025/01",
        "publication-date": "Not found",
        "abstract": "A local circular economy (LCE) is a closed-loop system operating at a community or regional scale where resources are efficiently recycled, repurposed, and managed to achieve sustainability. This paper introduces Circulise4LCE, a model-driven framework designed to support the development of LCEs through conceptual modeling. The framework consists of an i* LCE pattern, a circular BPMN workflow, a derived class diagram and a supporting simulation environment. It is designed to be adaptable, scalable, and replicable across various local contexts. The framework is applied to the Fanyatu case study (a non-profit organization supporting reforestation in the Congo Basin) allowing to show its ability to synchronize social, technological, and environmental stakeholders in a minimal system. The application leads to a simulation environment offering dynamic analysis of circularity and allowing for optimization and evaluation of local sustainability solutions. Results show how the framework helps address sustainability challenges by aligning resource management, community engagement, and technology integration.",
        "keywords": [
            "Local circular economy",
            "Circular economy",
            "Twin transition",
            "Digital twin",
            "iStar",
            "Sustainability",
            "Structure-in-5"
        ],
        "authors": [
            "Yves Wautelet",
            "Xavier Rouget"
        ],
        "file_path": "data/sosym-all/s10270-025-01314-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Petri net twist in explicit model checking",
        "submission-date": "2014/01",
        "publication-date": "2014/06",
        "abstract": "The invention of Petri nets was based on a critical analysis of then dominating automata models of systems. Explicit model checking explores the reachable states of a Petri net one by one. Essentially, it transforms a Petri net back to a transition system, that is, an automata-like model. At ﬁrst glance, this transformation appears to give up on all the speciﬁcs of Petri nets. Surveying the most dominant techniques of explicit state space veriﬁcation, we will, however, work out that even in explicit model checking, the deﬁning features of Petri nets are beneﬁcial and lead to more efﬁcient exploration routines. The ﬁndings in this paper are based on practical experience with a Petri net-based explicit model checking tool.",
        "keywords": [
            "Petri net",
            "Model checking",
            "Symmetry",
            "Sweep-line method",
            "Partial order reduction"
        ],
        "authors": [
            "Karsten Wolf"
        ],
        "file_path": "data/sosym-all/s10270-014-0422-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section of models 2019",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "The IEEE/ACM International Conference on Model Driven Engineering Languages and Systems (MODELS) serves as a venue for researchers and practitioners to share recent advances and practices in the area of model-based software and system engineering (MBE including MBSE). The 22nd edition of the conference, i.e., MODELS 2019, took place in Munich, Germany, with the overall aim to provide a platform for communicating advances in the foundations of MBE, and its innovative applications in engineering complex software systems, for sharing experiences of developing solutions particularly for handling properties such as uncertainty and robustness, by integrating machine learning and AI techniques when needed. A total of 114 papers were submitted, with 81 papers submitted to the Foundations Track (of which 18 were accepted) and 33 to the Practice and Innovation Track (of which 12 were accepted). Together, both tracks had an acceptance rate of 26%. Following the tradition of previous years, we selected and invited papers with the highest scores and reviews for this special section. These papers were extended according to the relevant SoSyM requirements and went through the SoSyM review process for the journal publication. Authors received anonymous feedback in two or three rounds of reviewing from three expert reviewers per paper. All in all, eleven submissions form the special section, as we discuss below.",
        "keywords": [],
        "authors": [
            "Tao Yue",
            "Silvia Abrahão",
            "Man Zhang"
        ],
        "file_path": "data/sosym-all/s10270-021-00939-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Utilizing multi-level concepts for multi-phase modeling Context-awareness and process-based constraints to enable model evolution",
        "submission-date": "2020/09",
        "publication-date": "2022/01",
        "abstract": "In model-based systems engineering projects, engineers from multiple domains collaborate by establishing a common system model. Multi-level modeling is a technique that can be used to model the development from abstract ideas to concrete implementations. However, current multi-level modeling approaches are not adequate for processes with multiple modeling phases that might have to be rearranged later. In this paper, we introduce multi-phase modeling that utilizes concepts of multi-level modeling by considering a description of the expected phase ordering per domain. Constraints aware of this context can express that certain elements are only valid in speciﬁc phases without having to determine a concrete phase ordering for a particular model. This enables using multi-phase modeling in ﬂexible workﬂows, adapting to changing requirements and the deﬁnition of access rules in domain notation. We show feasibility of this multi-phase modeling by applying it to multiple real-life systems engineering projects of the aerospace domain.",
        "keywords": [
            "Model-based systems engineering",
            "Multi-level modeling",
            "Domain-speciﬁc languages",
            "Systems engineering"
        ],
        "authors": [
            "Tobias Franz",
            "Christoph Seidl",
            "Philipp M. Fischer",
            "Andreas Gerndt"
        ],
        "file_path": "data/sosym-all/s10270-021-00963-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A classiﬁcation and rationalization of model-based software development",
        "submission-date": "2013/01",
        "publication-date": "2013/06",
        "abstract": "The use of model-based software development is increasingly popular due to recent advancements in modeling technology. Numerous approaches exist; this paper seeks to organize and characterize them. In particular, important terminological confusion, challenges, and recurring techniques of model-based software development are identiﬁed and rationalized. New perspectives are provided on some fundamental issues, such as the distinctions between model-driven development and architecture-centric development, code generation, and metamodeling. On the basis of this discussion, we opine that architecture-centric development and domain-speciﬁc model-driven development are the two most promising branches of model-based software development. Achieving a positive future will require, however, speciﬁc advances in software modeling, code generation, and model-code consistency management.",
        "keywords": [
            "Model-based software development",
            "Model-driven development",
            "Architecture-centric development"
        ],
        "authors": [
            "Yongjie Zheng",
            "Richard N. Taylor"
        ],
        "file_path": "data/sosym-all/s10270-013-0355-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A study on the impact of the level of participation in enterprise modeling",
        "submission-date": "2024/02",
        "publication-date": "2025/01",
        "abstract": "Participatory enterprise modeling (PEM) is presumed to have a positive impact on commitment, ownership feelings and further appraisals by domain experts with respect to the model. However, there has not been a lot of research into whether PEM actually has the desired effects. In this paper we report on an investigation of the effects of three settings with different levels of involving domain experts: an overall model was created (1) from four individual interviews, (2) from four individual models, or (3) in a joint meeting of domain and modeling consultants. The results show that the non-participatory interview setting led to less favorable appraisals, e.g., the possibility to participate and the value of the model were perceived as lower and the contribution of the modeling consultants was perceived as higher. Our ﬁndings should help practitioners in weighing possible beneﬁts of participatory enterprise modeling against the organizational effort and monetary cost it involves.",
        "keywords": [
            "Participatory enterprise modeling",
            "Experiment",
            "Participation",
            "Conceptual modeling"
        ],
        "authors": [
            "Anne Gutschmidt",
            "Charlotte Verbruggen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-025-01275-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Proﬁling users via their reviews: an extended systematic mapping study",
        "submission-date": "2019/09",
        "publication-date": "2020/03",
        "abstract": "Withtheextensivedevelopmentofbigdataandsocialnetworks,theuserproﬁleﬁeldhasreceivedmuchattention.Userproﬁling\nis essential for understanding the characteristics of various users, contributing to better understanding of their requirements\nin speciﬁc scenarios. User-generated contents which directly reﬂect people’s thoughts and intention are a valuable source\nfor proﬁling users, among which user reviews by nature are invaluable sources for acquiring user requirements and have\ndrawn increasing attention from both academia and industry. However, review-based user proﬁling (RBUP), as an emerging\nresearch direction, has not been systematically reviewed, hindering researchers from further investigation. In this work, we\ncarry out a systematic mapping study on review-based user proﬁling, with an emphasis on investigating the generic analysis\nprocess of RBUP and identifying potential research directions. Speciﬁcally, 51 out of 2478 papers were carefully selected\nfor investigation under a standardized and systematic procedure. By carrying out in-depth analysis over such papers, we\nhave identiﬁed a generic process that should be followed to perform review-based user proﬁling. In addition, we perform\nmulti-dimensional analysis on each step of the process in order to review current research progress and identify challenges\nand potential research directions. The results show that although traditional methods have been continuously improved,\nthey are not sufﬁcient to unleash the full potential of large-scale user reviews, especially the use of heterogeneous data for\nmulti-dimensional user proﬁling.",
        "keywords": [
            "User-generated reviews",
            "User proﬁling",
            "Systematic mapping study",
            "Software requirements"
        ],
        "authors": [
            "Xin Dong",
            "Tong Li",
            "Rui Song",
            "Zhiming Ding"
        ],
        "file_path": "data/sosym-all/s10270-020-00790-w.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Semantics of trace relations in requirements models for consistency checking and inferencing",
        "submission-date": "2009/01",
        "publication-date": "2009/12",
        "abstract": "Requirements traceability is the ability to relate requirements back to stakeholders and forward to corresponding design artifacts, code, and test cases. Although considerable research has been devoted to relating requirements in both forward and backward directions, less attention has been paid to relating requirements with other requirements. Relations between requirements influence a number of activities during software development such as consistency checking and change management. In most approaches and tools, there is a lack of precise definition of requirements relations. In this respect, deficient results may be produced. In this paper, we aim at formal definitions of the relation types in order to enable reasoning about requirements relations. We give a requirements metamodel with commonly used relation types. The semantics of the relations is provided with a formalization in first-order logic. We use the formalization for consistency checking of relations and for inferring new relations. A tool has been built to support both reasoning activities. We illustrate our approach in an example which shows that the formal semantics of relation types enables new relations to be inferred and contradicting relations in requirements documents to be determined. The application of requirements reasoning based on formal semantics resolves many of the deficiencies observed in other approaches. Our tool supports better understanding of dependencies between requirements.",
        "keywords": [
            "Requirements metamodel",
            "Requirements traceability",
            "Inferencing",
            "Consistency checking",
            "Reasoning"
        ],
        "authors": [
            "Arda Goknil",
            "Ivan Kurtev",
            "Klaas van den Berg",
            "Jan-Willem Veldhuis"
        ],
        "file_path": "data/sosym-all/s10270-009-0142-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A metamodel for modeling system features and their reﬁnement, constraint and interaction relationships",
        "submission-date": "2005/04",
        "publication-date": "2006/02",
        "abstract": "This paper presents a metamodel for modeling system features and relationships between features. The underlying idea of this metamodel is to employ features as ﬁrst-class entities in the problem space of software and to improve the customization of software by explicitly specifying both static and dynamic dependencies between system features. In this metamodel, features are organized as hierarchy structures by the reﬁnement relationships, static dependencies between features are speciﬁed by the constraint relationships, and dynamic dependencies between features are captured by the interaction relationships. A ﬁrst-order logic based method is proposed to formalize constraints and to verify constraints and customization. This paper also presents a framework for interaction classiﬁcation, and an informal mapping between interactions and constraints through constraint semantics.",
        "keywords": [
            "Feature model",
            "Relationships between features",
            "Reﬁnement",
            "Constraint",
            "Interaction",
            "Customization"
        ],
        "authors": [
            "Hong Mei",
            "Wei Zhang",
            "Haiyan Zhao"
        ],
        "file_path": "data/sosym-all/s10270-006-0004-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ParDSL: a domain-speciﬁc language framework for supporting deployment of parallel algorithms",
        "submission-date": "2017/10",
        "publication-date": "2018/12",
        "abstract": "An important challenge in parallel computing is the mapping of parallel algorithms to parallel computing platforms. This requires several activities such as the analysis of the parallel algorithm, the deﬁnition of the logical conﬁguration of the platform and the implementation and deployment of the algorithm to the computing platform. However, in current parallel computing approaches very often only conceptual and idiosyncratic models are used which fall short in supporting the communication and analysis of the design decisions. In this article, we present ParDSL, a domain-speciﬁc language framework for providing explicit models to support the activities for mapping parallel algorithms to parallel computing platforms. The language framework includes four coherent set of domain-speciﬁc languages each of which focuses on an activity of the mapping process. We use the domain-speciﬁc languages for modeling the design as well as for generating the required platform-speciﬁc models and the code of the selected parallel algorithm. In addition to the languages, a library is deﬁned to support systematic reuse. We discuss the overall architecture of the language framework, the separate DSLs, the corresponding model transformations and the toolset. The framework is illustrated for four different parallel computing algorithms.",
        "keywords": [
            "Model-driven software development",
            "Parallel programming",
            "High-performance computing",
            "Domain-speciﬁc language",
            "Architecture framework"
        ],
        "authors": [
            "Bedir Tekinerdogan",
            "Ethem Arkin"
        ],
        "file_path": "data/sosym-all/s10270-018-00705-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Personal programming and the object computer",
        "submission-date": "2019/10",
        "publication-date": "2019/12",
        "abstract": "My objective is to create an intuitive computer for laypeople who want to go beyond ready-made apps and create programs to control their electronic environment. I submit Loke, a new kind of computer that is a universe of objects and nothing but objects. I call it an object computer. Loke is implemented in Squeak, a variant of Smalltalk, and is an extensible, conceptual model for execution, inspection, and exploration. It was first used to demonstrate how Ellen, a novice, programs a smart alarm clock through a GUI adapted to her competence, needs, and preferences. Informal demonstrations indicated that laypeople immediately grasp the idea of communicating objects that represent real things in their environment. They also wanted to use it for their own purposes. They were creative in identifying personal opportunities for Loke and in sketching out their implementations. Interestingly, expert programmers who attended the demonstration did not see the point of Loke. I have completed the programming of Loke qua conceptual model. The model underpins its potential security and privacy and sustains its object and message models. The Loke qua programming environment is still in its infancy, and its inherent security and privacy properties are still not realized in practice. A future Loke device will be accessible from anywhere and embedded in its own hardware to achieve them. The Loke IDE rests on Data–Context–Interaction (DCI), a new program-ming paradigm that leads to readable code with a clear architecture. I submit Loke for the pleasure of personal programming.",
        "keywords": [
            "Personal programming",
            "Laypeople programming",
            "IoT",
            "Smart home",
            "Industry 5.0",
            "Loke",
            "Object computer",
            "BabyIDE",
            "MVC",
            "DCI",
            "Smalltalk",
            "Squeak"
        ],
        "authors": [
            "Trygve M. H. Reenskaug"
        ],
        "file_path": "data/sosym-all/s10270-019-00768-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Trustworthy agent-based simulation: the case for domain-speciﬁc modelling languages",
        "submission-date": "2022/12",
        "publication-date": "2023/02",
        "abstract": "Simulation is a key tool for researching complex system behaviour. Agent-based simulation has been applied across domains, such as biology, health, economics and urban sciences. However, engineering robust, efﬁcient, maintainable, and reliable agent-based simulations is challenging. We present a vision for engineering agent simulations comprising a family of domain-speciﬁc modelling languages (DSMLs) that integrates core software engineering, validation and simulation experimentation. We relate the vision to examples of principled simulation, to show how the DSMLs would improve robustness, efﬁciency, and maintainability of simulations. Focusing on how to demonstrate the ﬁtness for purpose of a simulator, the envisaged approach supports bi-directional transparency and traceability between the original domain understanding to the implementation, interpretation of results and evaluation of hypotheses.",
        "keywords": [],
        "authors": [
            "Steffen Zschaler",
            "Fiona A. C. Polack"
        ],
        "file_path": "data/sosym-all/s10270-023-01082-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Petri and how he saw the world",
        "submission-date": "2014/01",
        "publication-date": "2014/09",
        "abstract": "This article is a short summary and explanation of the scientiﬁc work of Carl Adam Petri. The very basics of net theory are sufﬁcient to understand it.",
        "keywords": [
            "Petri Nets",
            "Concurrency",
            "Asynchronous systems",
            "Discrete density",
            "Measuring theory"
        ],
        "authors": [
            "Einar Smith"
        ],
        "file_path": "data/sosym-all/s10270-014-0427-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Exploring how users engage with hybrid process artifacts based on declarative process models: a behavioral analysis based on eye-tracking and think-aloud",
        "submission-date": "2019/11",
        "publication-date": "2020/07",
        "abstract": "Process design artifacts have been increasingly used to guide the modeling of business processes. To support users in designing and understanding process models, different process artifacts have been combined in several ways leading to the emergence of the so-called “hybrid process artifacts”. While many hybrid artifacts have been proposed in the literature, little is known about how they can actually support users in practice. To address this gap, this work investigates the way users engage with hybrid process artifacts during comprehension tasks. In particular, we focus on a hybrid representation of DCR Graphs (DCR-HR) combining a process model, textual annotations and an interactive simulation. Following a qualitative approach, we conduct a multi-granular analysis exploiting process mining, eye-tracking techniques, and verbal data analysis to scrutinize the reading patterns and the strategies adopted by users when being confronted with DCR-HR. The ﬁndings of the coarse-grained analysis provide important insights about the behavior of domain experts and IT specialists and show how user’s background and task type change the use of hybrid process artifacts. As for the ﬁne-grained analysis, user’s behavior was classiﬁed into goal-directed and exploratory and different strategies of using the interactive simulation were identiﬁed. In addition, a progressive switch from an exploratory behavior to a goal-directed behavior was observed. These insights pave the way for an improved development of hybrid process artifacts and delineate several directions for future work.",
        "keywords": [
            "Process models",
            "Hybrid process artifacts",
            "DCR graphs",
            "Eye-tracking",
            "Think-aloud",
            "Behavioral analysis"
        ],
        "authors": [
            "Amine Abbad Andaloussi",
            "Francesca Zerbato",
            "Andrea Burattin",
            "Tijs Slaats",
            "Thomas T. Hildebrandt",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-020-00811-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guidelines to derive an e3value business model from a BPMN process model: an experiment on real-world scenarios",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Process models, e.g., BPMN models, may represent how companies in an ecosystem interact with each other. However, the business model of the same ecosystem, e.g., expressed by an e3value model, is often left implicit. This hinders the proper analysis of the ecosystem at the business level, and more speciﬁcally ﬁnancial assessment, for which process models are less appropriate. Therefore, the question is if we can somehow derive e3value models from BPMN models. This would not only allow for proper business model analysis but would also facilitate business model mining, similar to the success of process mining. However, although an e3value model and BPMN model represent the same ecosystem, their perspectives differ signiﬁcantly. Therefore an automated derivation of an e3value model from a BPMN seems not to be feasible, but we can assist the e3value model designer with practical guidelines. We explore and test our guidelines in two real-world settings, we then analyze and evaluate its application to better understand their limitations and how to improve them.",
        "keywords": [
            "Ecosystems",
            "Business model",
            "Process model",
            "e3value",
            "BPMN"
        ],
        "authors": [
            "Isaac da Silva Torres",
            "Marcelo Fantinato",
            "Gabriela Musse Branco",
            "Jaap Gordijn"
        ],
        "file_path": "data/sosym-all/s10270-022-01074-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Uniﬁed veriﬁcation and monitoring of executable UML speciﬁcations\nA transformation-free approach",
        "submission-date": "2020/05",
        "publication-date": "2021/11",
        "abstract": "The increasing complexity of embedded systems renders software veriﬁcation more complex, requiring monitoring and\nformal techniques, like model-checking. However, to use such techniques, system engineers usually need formal expertise to\nexpress the software requirements in a formal language. To facilitate the use of model-checking tools by system engineers,\nour approach uses a UML model interpreter through which the software requirements can directly be expressed in UML as\nwell. Formal requirements are encoded as UML state machines with the transition guards written in a speciﬁc observation\nlanguage, which expresses predicates on the execution of the system model. Each such executable UML speciﬁcation can\nmodel either a Büchi automaton or an observer automaton, and is synchronously composed with the system, to follow its\nexecution during model-checking. Formal veriﬁcation can continue at runtime for all deterministic observer automata used\nduring ofﬂine veriﬁcation by deploying them on real embedded systems. Our approach has been evaluated on multiple case\nstudies and is illustrated, in this paper, through the user interface model of a cruise-control system. The automata-based\nveriﬁcation results are in line with the veriﬁcation of the equivalent LTL properties. The runtime overhead during monitoring\nis proportional to the number of monitors.",
        "keywords": [
            "Model-checking",
            "Monitoring",
            "Model interpretation",
            "Embedded systems",
            "Observation Language",
            "Synchronous\nComposition"
        ],
        "authors": [
            "Valentin Besnard\nCiprian Teodorov\nFrédéric Jouault\nMatthias Brun\nPhilippe Dhaussy"
        ],
        "file_path": "data/sosym-all/s10270-021-00923-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards the efﬁcient development of model transformations using model weaving and matching transformations",
        "submission-date": "2007/07",
        "publication-date": "2008/07",
        "abstract": "Model transformations can be used in many\ndifferent application scenarios, for instance, to provide inter-\noperability between models of different size and complexity.\nAs a consequence, they are becoming more and more\ncomplex. However, model transformations are typically\ndeveloped manually. Several code patterns are implemen-\nted repetitively, thus increasing the probability of program-\nming errors and reducing code reusability. There is not yet a\ncomplete solution that automates the development of model\ntransformations. In this paper, we present a novel approach\nthat uses matching transformations and weaving models to\nsemi-automate the development of transformations. Weaving\nmodels are models that contain different kinds of relation-\nships between model elements. These relationships capture\ndifferent transformation patterns. Matching transformations\nare a special kind of transformations that implement methods\nthat create weaving models. We present a practical\nsolution that enables the creation and the customization of\ndifferent creation methods in an efﬁcient way. We combine\ndifferent methods, and present a metamodel-based method\nthat exploits metamodel data to automatically produce wea-\nving models. The weaving models are derived into model\nintegration transformations. To validate our approach, we\npresent an experiment using metamodels with distinct size\nand complexity, which show the feasibility and scalability of\nour solution.",
        "keywords": [
            "Model engineering",
            "Matching transforma-\ntions",
            "Model weaving"
        ],
        "authors": [
            "Marcos Didonet Del Fabro",
            "Patrick Valduriez"
        ],
        "file_path": "data/sosym-all/s10270-008-0094-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Continuous situation-speciﬁc development of business models: knowledge provision, method composition, and method enactment",
        "submission-date": "2021/10",
        "publication-date": "2022/07",
        "abstract": "The development of new business models is essential for startups to become successful, as well as for established companies to explore new business opportunities. However, developing such business models is a continuous challenging activity where different tasks need to be performed, and business decisions need to be made. Both have to ﬁt the constantly changeable situation in which the business model is developed to reduce the risk of developing ineffective business models with low market penetration. Therefore, a method for developing situation-speciﬁc business models is needed. As a solution, we reﬁne the concept of situational method engineering (SME) to business model development. SME, in turn, provides means to construct situation-speciﬁc development methods out of fragments from a method repository. We develop a concept for the continuous situation-speciﬁc development of business models based on design science. The approach uses the roles of a domain expert, a method engineer, and a business developer together with a repository with method fragments for developing business models and a repository with modeling artifacts for supporting the development. Both repositories are ﬁlled by utilizing the experience of domain experts. Out of these repositories, situation-speciﬁc development methods for developing business models can be continuously composed based on the changeable situation by the method engineer and enacted by the business developer. We implement it as an open-source tool and evaluate its applicability in an industrial case study of developing a business model for a local event platform. Our results show that situation awareness supports the continuous development of business models.",
        "keywords": [
            "Business model development",
            "Situational method engineering",
            "Situation-speciﬁc",
            "Business model canvas",
            "Continuous development"
        ],
        "authors": [
            "Sebastian Gottschalk",
            "Enes Yigitbas",
            "Alexander Nowosad",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-022-01018-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending the UML use case metamodel with behavioral information to facilitate model analysis and interchange",
        "submission-date": "2011/12",
        "publication-date": "2013/03",
        "abstract": "Use case diagrams are primary artifacts used for modeling functional requirements. Use case diagrams are part of the Uniﬁed Modeling Language (UML) suite of models that has become a de facto standard for modeling object oriented languages. The use case diagram is considered the most controversial diagram in UML. Practitioners claim that the use case diagram cannot be used as a valuable artifact for requirement analysis. The main reason behind this concern is the lack of behavioral description of a use case depicted within the model. Quite a few extensions to the use case metamodel have been proposed in literature to incorporate behavioral aspect of a use case within the metamodel. All these extensions omit a few important features like generalization and most of them can only be used for model representation and cannot be used for model analysis and evaluation. In this paper, we propose an extension to the UML use case metamodel with use case behavior speciﬁcation elements. The main objective of the proposed extension is to provide a complete metamodel for use case diagrams which includes representation for all its elements and relationships in a conﬂict-free manner and one that includes information for model analysis, evaluation, and interchange among modeling tools. In order to include all valuable information related to a use case, a number of use case representation templates were considered for the proposed extension. Simultaneously, to enable the use case models generated based on the proposed metamodel to be used for analysis, pertinent information related to model usage in analysis such as effort estimation, use case scheduling, and use case metrics evaluation were considered from published studies, tools, and paradigms and included within the proposed metamodel.",
        "keywords": [
            "UML",
            "Use case diagram",
            "Metamodel",
            "Behavior speciﬁcation"
        ],
        "authors": [
            "Mohammed Misbhauddin",
            "Mohammad Alshayeb"
        ],
        "file_path": "data/sosym-all/s10270-013-0333-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A fractal enterprise model and its application for business development",
        "submission-date": "2014/10",
        "publication-date": "2016/08",
        "abstract": "This paper suggests a new type of enterprise models called fractal enterprise models (FEM), with accompanying methodological support for their design. FEM shows interconnections between the business processes in an enterprise by connecting them to the assets they use and manage. Assets considered in the model could be tangible (buildings, heavy machinery, etc.) and intangible (employees, business process definitions, etc.). A FEM model is built by using two types of patterns called archetypes: a process-assets archetype that connects a process with assets used in it, and an asset-processes archetype that connects an asset with processes aimed to manage this asset (e.g., hiring people, or servicing machinery). Alternating these patterns creates a fractal structure that makes relationships between various parts of the enterprise explicit. FEM can be used for different purposes, including finding a majority of the processes in an enterprise and planning business change or radical transformation. Besides discussing FEM and areas of its usage, the paper presents results from a completed project in order to test the practical usefulness of FEM and its related methodological support.",
        "keywords": [
            "Business process",
            "Enterprise modeling",
            "Fractal enterprise",
            "Asset"
        ],
        "authors": [
            "Ilia Bider",
            "Erik Perjons",
            "Mturi Elias",
            "Paul Johannesson"
        ],
        "file_path": "data/sosym-all/s10270-016-0554-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design rationale capture for process improvement in the globalised enterprise: an industrial study",
        "submission-date": "2011/03",
        "publication-date": "2011/12",
        "abstract": "Design rationale ﬁlls in the gaps between the original requirements of a system and the ﬁnished product encompassing decisions, constraints and other information that inﬂuenced the outcome. Existing research in Software Engineering corroborates the importance of design rationale to capture knowledge assets, particularly in the context of the global enterprise, with its increased reliance on IT systems, and risk of knowledge loss through staff movement and attri- tion. Despite this, the practice of design rationale capture is not as extensive as could be expected due to reasons which include time and budget constraints, the lack of standards and tools, and some uncertainty as to its actual added value. In this paper, we address the viability and beneﬁts of capturing design rationale as a by-product of design in the context of a real-world global organisational setting. This was achieved through a study in which an emerging design approach—Problem Oriented Engineering—was applied in the context of a global ﬁnancial institution to address a critical IT prob- lem as part of its software supplier’s client resolution process. The study provides some positive evidence that the approach-guided knowledge capture of key design rationale elements and that it combined well with existing practices within the organisation and even led to improvement to one of their key processes.",
        "keywords": [
            "Design rationale",
            "Process improvement",
            "Problem oriented engineering",
            "Assurance-driven design"
        ],
        "authors": [
            "A. Nkwocha",
            "J. G. Hall",
            "L. Rapanotti"
        ],
        "file_path": "data/sosym-all/s10270-011-0223-y.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Extending enterprise architecture modeling languages for domain specificity and collaboration: application to telecommunication service design",
        "submission-date": "2011/11",
        "publication-date": "2012/11",
        "abstract": "The competitive market forces organizations to be agile and flexible so as to react robustly to complex events. Modeling helps managing this complexity. However, in order to model an enterprise, many stakeholders, with different expertise, must work together and take decisions. These decisions and their rationale are not always captured explicitly, in a standard, formal manner. The main problem is to persuade stakeholders to capture them. This article synthesizes an approach for capturing and using the rationale behind enterprise modeling decisions. The approach is implemented through a domain-specific modeling language, defined as an extension of a standard enterprise architecture modeling language. It promotes coordination, enables presenting different stakeholders’ points of view, facilities participation and collaboration in modeling activities—activities focused here on enterprise architecture viewpoints. To present its benefits, such as rapid prototyping, the approach is applied to large organizations in the context of telecommunication service design. It is exemplified on modeling and capturing decisions on a conference service.",
        "keywords": [
            "Enterprise architecture framework",
            "Enterprise architecture modeling language",
            "Domain-specific modeling language",
            "Language extension",
            "Meta-model",
            "Design rationale",
            "Telecommunication service"
        ],
        "authors": [
            "Vanea Chiprianov",
            "Yvon Kermarrec",
            "Siegfried Rouvrais",
            "Jacques Simonin"
        ],
        "file_path": "data/sosym-all/s10270-012-0298-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Implementing QVT-R via semantic interpretation in UML-RSDS",
        "submission-date": "2020/01",
        "publication-date": "2020/09",
        "abstract": "The QVT-Relations (QVT-R) model transformation language is an OMG standard notation for model transformation spec-\niﬁcation. It is highly declarative and supports (in principle) bidirectional (bx) transformation speciﬁcation. However, there\nare many unclear or unsatisfactory aspects to its semantics, which is not precisely deﬁned in the standard. UML-RSDS is an\nexecutable subset of UML and OCL. It has a precise mathematical semantics and criteria for ensuring correctness of applica-\ntions (including model transformations) by construction. There is extensive tool support for veriﬁcation and for production\nof 3GL code in multiple languages (Java, C#, C++, C, Swift and Python). In this paper, we deﬁne a translation from QVT-R\ninto UML-RSDS, which provides a logically oriented semantics for QVT-R, aligned with the RelToCore mapping semantics\nin the QVT standard. The translation includes variation points to enable specialised semantics to be selected in particular\ntransformation cases. The translation provides a basis for veriﬁcation and static analysis of QVT-R speciﬁcations and also\nenables the production of efﬁcient code implementations of QVT-R speciﬁcations. We evaluate the approach by applying it\nto solve benchmark examples of bx.",
        "keywords": [
            "Model transformations",
            "QVT-Relations",
            "UML-RSDS",
            "Model transformation semantics",
            "Model transformation tools"
        ],
        "authors": [
            "K. Lano",
            "S. Kolahdouz-Rahimi"
        ],
        "file_path": "data/sosym-all/s10270-020-00824-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-Relations (QVT-R)"
        }
    },
    {
        "title": "Analysing factors impacting BPMS performance: a case of a challenged technology adoption",
        "submission-date": "2020/10",
        "publication-date": "2021/09",
        "abstract": "Business Process Management Suites (BPMSs) have been adopted in organisations to model, improve and automate busi-ness processes as they aim to increase the quality, efficiency and agility of their business processes. Yet, many organisations struggle to achieve the benefits they expected from a BPMS. This interpretive case study in a large South African financial services organisation explains factors found to negatively impact successful BPMS adoption. The paper describes how an IT team struggled to increase process agility with a BPMS in a large legacy application landscape. The dominant factors caus-ing the struggle were the difficulty of integrating with other applications and a lack of governance around BPM. Interesting findings on the difficulties in resourcing BPM IT teams are presented. The impact of BPM strategy, culture and governance on BPM methods, resourcing, data and technology is explained. The BPM literature lacks empirical qualitative case studies and theoretical models. This paper aimed to contribute to both needs. The theoretical contribution of this paper is two mod-els. The first inductively derived explanatory contextual model should be useful for practitioners wanting to adopt a BPMS. Using this study’s findings and models from the literature, a second, more generic explanatory model of information system performance is derived for a BPMS.",
        "keywords": [
            "Business Process Management",
            "Business Process Management Suites",
            "BPM adoption",
            "Information systems performance"
        ],
        "authors": [
            "Lisa F. Seymour",
            "Ashley Koopman"
        ],
        "file_path": "data/sosym-all/s10270-021-00922-w.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Adherence preserving reﬁnement of trace-set properties in STAIRS: exempliﬁed for information ﬂow properties and policies",
        "submission-date": "2008/07",
        "publication-date": "2008/09",
        "abstract": "STAIRS is a formal approach to system development with UML 2.1 sequence diagrams that supports an incremental and modular development process. STAIRS is underpinned by denotational and operational semantics that have been proved to be equivalent. STAIRS is more expressive than most approaches with a formal notion of reﬁnement. STAIRS supports a stepwise reﬁnement process under which trace properties as well as trace-set properties are preserved. This paper demonstrates the potential of STAIRS in this respect, in particular that reﬁnement in STAIRS preserves adherence to information ﬂow properties as well as policies.",
        "keywords": [],
        "authors": [
            "Fredrik Seehusen",
            "Bjørnar Solhaug",
            "Ketil Stølen"
        ],
        "file_path": "data/sosym-all/s10270-008-0102-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Efﬁcient model similarity estimation with robust hashing",
        "submission-date": "2020/04",
        "publication-date": "2021/08",
        "abstract": "As model-driven engineering (MDE) is increasingly adopted in complex industrial scenarios, modeling artefacts become a key and strategic asset for companies. As such, any MDE ecosystem must provide mechanisms to protect and exploit them. Current approaches depend on the calculation of the relative similarity among pairs of models. Unfortunately, model similarity calculation mechanisms are computationally expensive which prevents their use in large repositories or very large models. In this sense, this paper explores the adaptation of the robust hashing technique to the MDE domain as an efﬁcient estimation method for model similarity. Indeed, robust hashing algorithms (i.e., hashing algorithms that generate similar outputs from similar input data) have proved useful as a key building block in intellectual property protection, authenticity assessment and fast comparison and retrieval solutions for different application domains. We present a detailed method for the generation of robust hashes for different types of models. Our approach is based on the translation to the MDE domain of diverse techniques such as summary extraction, minhash generation and locality-sensitive hash function families, originally developed for the comparison and classiﬁcation of large datasets. We validate our approach with a prototype implementation and show that: (1) our approach can deal with any graph-based model representation; (2) a strong correlation exists between the similarity calculated directly on the robust hashes and a distance metric calculated over the original models; and (3) our approach scales well on large models and greatly reduces the time required to ﬁnd similar models in large repositories.",
        "keywords": [
            "Model-driven engineering",
            "Locality-sensitive hashing",
            "Near-similar search",
            "Robust hashing Comparison"
        ],
        "authors": [
            "Salvador Martínez\nSébastien Gérard\nJordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-021-00915-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Plug-and-play composition of features and feature interactions with statechart diagrams",
        "submission-date": "2003/11",
        "publication-date": "2003/11",
        "abstract": "This paper presents a new approach for modular design of highly-entangled software components by statechart diagrams. We structure the components into features, which represent reusable, self-contained services. These are modeled individually by statechart diagrams. For composition of components from features, we need to consider the interactions between the features. These feature interactions, well known in the telecommunications area, typically describe special cases or cooperations which only occur when two features are combined. We describe these interactions graphically by reﬁnement relations between statecharts. The main novelty is that full component descriptions are created in a plug-and-play fashion by combining the statecharts for the required features and interactions. Furthermore, we develop different classes of statecharts and show the interactions on a case-by-case basis. For composition, we use semantic refinement concepts for statecharts which preserve the original behavior.",
        "keywords": [
            "Graphic modeling techniques",
            "Plug-and-play composition",
            "Feature interaction",
            "Statechart diagrams",
            "Semantic refinement",
            "UML"
        ],
        "authors": [
            "Christian Prehofer"
        ],
        "file_path": "data/sosym-all/s10270-003-0040-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Cyber security threat modeling based on the MITRE Enterprise ATT&CK Matrix",
        "submission-date": "2020/07",
        "publication-date": "2021/06",
        "abstract": "Enterprise systems are growing in complexity, and the adoption of cloud and mobile services has greatly increased the attack surface. To proactively address these security issues in enterprise systems, this paper proposes a threat modeling language for enterprise security based on the MITRE Enterprise ATT&CK Matrix. It is designed using the Meta Attack Language framework and focuses on describing system assets, attack steps, defenses, and asset associations. The attack steps in the language represent adversary techniques as listed and described by MITRE. This entity-relationship model describes enterprise ITsystemsasawhole;byusingavailabletools,theproposedlanguageenablesattacksimulationsonitssystemmodelinstances. These simulations can be used to investigate security settings and architectural changes that might be implemented to secure the system more effectively. Our proposed language is tested with a number of unit and integration tests. This is visualized in the paper with two real cyber attacks modeled and simulated.",
        "keywords": [
            "Threat modeling",
            "Domain-speciﬁc language",
            "Attack simulations",
            "Enterprise systems"
        ],
        "authors": [
            "Wenjun Xiong",
            "Emeline Legrand",
            "Oscar Åberg",
            "Robert Lagerström"
        ],
        "file_path": "data/sosym-all/s10270-021-00898-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "First Issue of the International Journal on Software and Systems Modeling",
        "submission-date": "2002/09",
        "publication-date": "2002/09",
        "abstract": "This paper introduces the first issue of the Software and Systems Modeling (SoSyM) journal. It outlines the journal's aims, scope, and objectives, focusing on theoretical and practical aspects of software and system modeling languages, methods, and techniques. The journal aims to publish high-quality research that benefits both practitioners and researchers in the field.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-002-0006-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On leveraging the fruits of research efforts in the arena of business process modeling formalisms: a map-driven approach for decision making",
        "submission-date": "2017/10",
        "publication-date": "2018/08",
        "abstract": "The importance of business process (BP) modeling to Business Process Management can be justiﬁed by the serious problems, which may arise in the latter, if the former is not conducted correctly. This can take place, for instance, in the case of an inappropriate choice of a BP modeling formalism. Many frameworks have therefore been proposed in order to guide the selection of the most suitable formalism. Nevertheless, these research efforts still scattered throughout a wide range of publications, unlinked and often with a narrow scope. Hence, there is a need for both a combination of these research labors into a single reference source and an approach to support such combination. This paper is the ﬁrst contribution toward responding to this need. It proposes a methodological roadmap as a basis for developing a selection support system. The results presented in the current paper stand for a ﬁrst step toward a reference framework for helping researchers and practitioners in choosing the most appropriate BP modeling formalism given a set of contextual variables. The proposed methodological roadmap is speciﬁed using a modeling formalism (MAP) which considers the “decision” as a ﬁrst-class citizen.",
        "keywords": [
            "Business process modeling formalism",
            "Map-driven approach",
            "Methodological guidelines",
            "Systematic literature review",
            "Context model",
            "Roadmap"
        ],
        "authors": [
            "Afef Awadid",
            "Selmin Nurcan",
            "Sonia Ayachi Ghannouchi"
        ],
        "file_path": "data/sosym-all/s10270-018-0689-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "What can we learn from enterprise architecture models? An experiment comparing models and documents for capability development",
        "submission-date": "2015/08",
        "publication-date": "2016/05",
        "abstract": "Enterprise architecture (EA) has been established as a discipline to cope with the complex interactions of business operations and technology. Models, i.e., formal descriptionsintermsofdiagramsandviews,areattheheartof the approach. Though it is widely thought that such architecture models can contribute to improved understanding and decision making, this proposition has not rigorously been tested. This article describes an experiment conducted with a real EA model and corresponding real traditional documents, investigating whether the model or the documents lead to bet- ter and faster understanding. Understanding is interesting to study, as it is a prerequisite to other EA uses. The subjects (N = 98) were ofﬁcer cadets, and the experiment was carried out using a comprehensive description of military Close Air Support capability either (1) in the form of a MODAF model or (2) in the form of traditional documents. Based on the results, the model seems to lead to better, though not faster, understanding.",
        "keywords": [
            "Enterprise architecture",
            "MODAF",
            "Model-based capability development",
            "Experiment",
            "Models versus documents"
        ],
        "authors": [
            "Ulrik Franke",
            "Mika Cohen",
            "Johan Sigholm"
        ],
        "file_path": "data/sosym-all/s10270-016-0535-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to the theme section on model-based design of cyber-physical systems",
        "submission-date": "2018/02",
        "publication-date": "2018/03",
        "abstract": "Cyber-physical systems (CPS) are engineered systems where functionalities are emerging from the networked interaction of physical and computational processes. CPS are a pervasive technology advancement, which is impacting today all industrial sectors and almost all aspects of society. While CPS design as a new research area started over a decade ago, the recent emergence of new industrial platforms for creating CPS such as the Internet of things (IoT), industrial Internet (II)andfogcomputingintheUSAandIndustrie4.0inEurope greatly accelerated the productization and deployment of the technology and created a “gold rush” toward new markets. Since model-based design plays pivotal role in all areas of engineered systems, it is an important issue to examine what are the new challenges for model-based design in CPS?",
        "keywords": [],
        "authors": [
            "Manfred Broy",
            "Heinrich Daembkes",
            "Janos Sztipanovits"
        ],
        "file_path": "data/sosym-all/s10270-018-0670-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling data protection and privacy: application and experience with GDPR",
        "submission-date": "2020/07",
        "publication-date": "2021/11",
        "abstract": "In Europe and indeed worldwide, the General Data Protection Regulation (GDPR) provides protection to individuals regarding their personal data in the face of new technological developments. GDPR is widely viewed as the benchmark for data protection and privacy regulations that harmonizes data privacy laws across Europe. Although the GDPR is highly beneﬁcial to individuals, it presents signiﬁcant challenges for organizations monitoring or storing personal information. Since there is currently no automated solution with broad industrial applicability, organizations have no choice but to carry out expensive manual audits to ensure GDPR compliance. In this paper, we present a complete GDPR UML model as a ﬁrst step toward designing automated methods for checking GDPR compliance. Given that the practical application of the GDPR is inﬂuenced by national laws of the EU Member States, we suggest a two-tiered description of the GDPR, generic and specialized. In this paper, we provide (1) the GDPR conceptual model we developed with complete traceability from its classes to the GDPR, (2) a glossary to help understand the model, (3) the plain-English description of 35 compliance rules derived from GDPR along with their encoding in OCL and (4) the set of 20 variations points derived from GDPR to specialize the generic model. We further present the challenges we faced in our modeling endeavor, the lessons we learned from it and future directions for research.",
        "keywords": [
            "General Data Protection Regulation (GDPR)",
            "Conceptual modeling",
            "Model variability",
            "Regulatory compliance",
            "Uniﬁed Modeling Language (UML)"
        ],
        "authors": [
            "Damiano Torre",
            "Mauricio Alferez",
            "Ghanem Soltana",
            "Mehrdad Sabetzadeh",
            "Lionel Briand"
        ],
        "file_path": "data/sosym-all/s10270-021-00935-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling to improve quality or efﬁciency? An automotive domain perspective",
        "submission-date": "2012/05",
        "publication-date": "2012/05",
        "abstract": "What added-value does modeling bring to the development process? An often stated value is the productivity improvement gained by using models to automatically generate code. Model-to-code generators provide automated support for bridging abstraction gaps, and thus relieve developers from routine, often tedious, decision-making and implementation activities. Developers using model-driven software development techniques should be able to deliver working software at a faster rate, and thus reduce time-to-market. Model-to-code generation is particularly valuable when it supports synchronized evolution of generated code and models, as opposed to “one-shot generation” in which generated code that has been modiﬁed cannot be re-generated from models because the models do not reﬂect the changes made to the generated code. Maintaining ﬁdelity between abstract models and the more detailed code as they evolve can lead to faster delivery of software extended with new or modiﬁed features. However, in the automotive and similar embedded software industries, process efﬁciency may be less of an issue. Consider the case of a particular European automotive company that had a total turnover of 60 Billion Euros and earned 6 Billion Euros. Car development, production, sales, after-sales,andothercostsforthiscompanythustotaledroughly54 Billion Euros. The company employs approximately 1,000 software developers and approximately another 1,200 software developers from component suppliers perform work for the company. If an engineer’s average salary is 120,000 Euros (this is the average salary reported in a recent study), then software development manpower cost for this company is approximately 264 Million Euros. This is 0.45 % of the overall costs. While 264 Million sounds like a lot, it is relatively insignificant compared to other costs. For example, a software error that results in a recall of over one million cars can be very costly. Assuming a cost of 400 Euros per recall, the result would be over 400 Million Euros in cost. In such a scenario the cost of a recall easily exceeds the total software development manpower costs. So what really counts in these cases? The answer should not be surprising: quality, quality, and quality. Time-to-market is second significant concern for these organizations. Efﬁciency probably only counts, when the availability of experienced developers to hire is low. Quality concerns can very nicely be addressed by model-based development methods. Modeling techniques can be used to automatically synthesize and compose sub-models that capture different features of a distributed system. Composed models can be used to generate high quality code that works on appropriate middleware, but was developed independent of any technology stack and can therefore be reused rather unchanged. The big advantage of reuse is not the efﬁ-ciency increase, but the reuse of quality that was already proven. We therefore need appropriate tooling infrastructure for developers in the automotive and similar domains for qual-ity and not that much for efﬁciency reasons. But we also need industries that have the courage to accept that the development process for software is significantly different from the traditional engineering process. Only then soft-ware will keep up with the desired quality and be there in time. Traditional engineering processes today significantly lack to address software speciﬁc characteristics, but appropriate combinations of software-aware development with",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-012-0251-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Logic formulas in models",
        "submission-date": "2017/06",
        "publication-date": "2017/06",
        "abstract": "Many modeling languages, especially graphical ones, concentrate on the ease of expression in specifying certain aspects of a system that need to be developed. However, this often leads to a reduced ability to express complex relations between particular elements in the model. This is certainly obvious when using UML’s class diagrams, but also occurs in speciﬁcation-oriented versions of state machines, activity diagrams, business process models, and variants of architectural description languages. The restricted ability to describe additional constraints usually leads to the demand for an expressive logic that is used on top of the underlying modeling language. The Object Constraint Language (OCL), as part of the UML, is such an example.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-017-0605-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An ontology-based framework for domain-speciﬁc modeling",
        "submission-date": "2010/11",
        "publication-date": "2012/04",
        "abstract": "Domain-speciﬁc languages (DSLs) provide abstractions and notations for better understanding and easier modeling of applications in a special domain. Current short-comings of DSLs include learning curve and formal semantics. This paper reports on a framework that allows the use of ontology technologies to describe and reason on DSLs. The formal semantics of OWL together with reasoning services allows for addressing constraint definition, progressive eval-uation, suggestions, and debugging. The approach integrates existing metamodels and concrete syntaxes in a new techni-cal space. A scenario in which domain models for network devices are created illustrates the framework.",
        "keywords": [
            "Domain-speciﬁc modeling",
            "Ontology technologies"
        ],
        "authors": [
            "Tobias Walter",
            "Fernando Silva Parreiras",
            "Steffen Staab"
        ],
        "file_path": "data/sosym-all/s10270-012-0249-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automaton-based comparison of Declare process models",
        "submission-date": "2022/03",
        "publication-date": "2022/12",
        "abstract": "The Declare process modeling language has been established within the research community for modeling so-called ﬂexible processes. Declare follows the declarative modeling paradigm and therefore guarantees ﬂexible process execution. For several reasons, declarative process models turned out to be hard to read and comprehend. Thus, it is also hard to decide whether two process models are equal with respect to their semantic meaning, whether one model is completely contained in another one or how far two models overlap. In this paper, we follow an automaton-based approach by transforming Declare process models into ﬁnite state automatons and applying automata theory for solving this issue.",
        "keywords": [
            "Business process modeling",
            "Declare",
            "Model comparison",
            "Declarative process management",
            "Automata theory"
        ],
        "authors": [
            "Nicolai Schützenmeier",
            "Martin Käppel",
            "Lars Ackermann",
            "Stefan Jablonski",
            "Sebastian Petter"
        ],
        "file_path": "data/sosym-all/s10270-022-01069-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lossless compaction of model execution traces",
        "submission-date": "2018/10",
        "publication-date": "2019/06",
        "abstract": "Dynamic veriﬁcation and validation (V&V) techniques are used to verify and validate the behavior of software systems early\nin the development process. In the context of model-driven engineering, such behaviors are usually deﬁned using executable\ndomain-speciﬁc modeling languages (xDSML). Many V&V techniques rely on execution traces to represent and analyze the\nbehavior of executable models. Traces, however, tend to be overwhelmingly large, hindering effective and efﬁcient analysis\nof their content. While there exist several trace metamodels to represent execution traces, most of them suffer from scalability\nproblems. In this paper, we present a generic compact trace representation format called generic compact trace metamodel\n(CTM) that enables the construction and manipulation of compact execution traces of executable models. CTM is generic\nin the sense that it supports a wide range of xDSMLs. We evaluate CTM on traces obtained from real-world fUML models.\nCompared to existing trace metamodels, the results show a signiﬁcant reduction in memory and disk consumption. Moreover,\nCTM offers a common structure with the aim to facilitate interoperability between existing trace analysis tools.",
        "keywords": [
            "Execution trace",
            "Compaction",
            "Model execution",
            "Trace metamodel"
        ],
        "authors": [
            "Fazilat Hojaji",
            "Bahman Zamani",
            "Abdelwahab Hamou-Lhadj",
            "Tanja Mayerhofer",
            "Erwan Bousse"
        ],
        "file_path": "data/sosym-all/s10270-019-00737-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An algorithm for generating model-sensitive search plans for pattern matching on EMF models",
        "submission-date": "2012/10",
        "publication-date": "2013/09",
        "abstract": "In this paper, we propose a new model-sensitive search plan generation algorithm to speed up the process of graph pattern matching. This dynamic-programming-based algorithm, which is able to handle general n-ary constraints in an integrated manner, collects statistical data from the underlying EMF model and uses this information for optimization purposes. Additionally, the search plan generation algorithm itself and its runtime effects on the pattern matching engine have been evaluated by complexity analysis techniques and by quantitative performance measurements, respectively.",
        "keywords": [
            "Graph pattern matching",
            "Search plan generation algorithm",
            "Model-sensitive search plan"
        ],
        "authors": [
            "Gergely Varró",
            "Frederik Deckwerth",
            "Martin Wieber",
            "Andy Schürr"
        ],
        "file_path": "data/sosym-all/s10270-013-0372-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward a well-founded theory for multi-level conceptual modeling",
        "submission-date": "2015/02",
        "publication-date": "2016/06",
        "abstract": "Multi-level conceptual modeling addresses the representation of subject domains dealing explicitly with multiple classiﬁcation levels. Despite the recent advances in multi-level modeling techniques, we believe that the literature in multi-level conceptual modeling would beneﬁt from a theory that: (1) formally characterizes the nature of classiﬁcation levels and (2) precisely deﬁnes the structural relations that may occur between elements of different classiﬁcation levels. This work aims to ﬁll this gap by proposing an axiomatic theory that can be considered a reference top-level ontology for types in multi-level conceptual modeling. The theory provides the modeler with basic concepts and patterns to articulate domains that require multiple levels of classification as well as to inform the development of well-founded languages for multi-level conceptual modeling. The whole theory is founded on a basic instantiation relation and characterizes the concepts of individuals and types, with types organized in levels related by instantiation. Further, it includes intra-level structural relations that are used to deﬁne expressive multi-level models and cross-level relations that allow us to account for and incorporate the different notions of power type in the literature.",
        "keywords": [
            "Multi-level modeling",
            "Conceptual modeling",
            "Power types",
            "Clabjects",
            "Ontology"
        ],
        "authors": [
            "Victorio A. Carvalho",
            "João Paulo A. Almeida"
        ],
        "file_path": "data/sosym-all/s10270-016-0538-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Composing domain-speciﬁc physical models with general-purpose software modules in embedded control software",
        "submission-date": "2010/12",
        "publication-date": "2012/10",
        "abstract": "A considerable portion of software systems today are adopted in the embedded control domain. Embedded control software deals with controlling a physical sys-tem, and as such models of physical characteristics become part of the embedded control software. In current practices, usually general-purpose languages (GPL), such as C/C++ are used for embedded systems development. Although a GPL is suitable for expressing general-purpose computa-tion, it falls short in expressing the models of physical characteristics as desired. This reduces not only the read-ability of the code but also hampers reuse due to the lack of dedicated abstractions and composition operators. More-over, domain-speciﬁc static and dynamic checks may not be applied effectively. There exist domain-speciﬁc modeling languages (DSML) and tools to specify models of physical characteristics. Although they are commonly used for simu-lation and documentation of physical systems, they are often not used to implement embedded control software. This is due to the fact that these DSMLs are not suitable to express the general-purpose computation and they cannot be easily composed with other software modules that are implemented in GPL. This paper presents a novel approach to combine a DSML to model physical characteristics and a GPL to imple-ment general-purpose computation. The composition ﬁlters model is used to compose models speciﬁed in the DSML with modules speciﬁed in the GPL at the abstraction level of both languages. As such, this approach combines the bene-ﬁts of using a DSML to model physical characteristics with the freedom of a GPL to implement general-purpose compu-tation. The approach is illustrated using two industrial case studies from the printing systems domain.",
        "keywords": [
            "Domain speciﬁc languages",
            "Embedded systems",
            "Software composition",
            "Composition ﬁlters",
            "Aspect-oriented programming"
        ],
        "authors": [
            "Arjan de Roo",
            "Hasan Sözer",
            "Mehmet Ak¸sit"
        ],
        "file_path": "data/sosym-all/s10270-012-0283-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating repairs for inconsistent models",
        "submission-date": "2021/04",
        "publication-date": "2022/04",
        "abstract": "There are many repair alternatives for resolving model inconsistencies, each involving one or more model changes. Enumer-\ning them all could overwhelm the developer because the number of possible repairs can grow exponentially. To address\nthis problem, this paper focuses on the immediate cause of an inconsistency. By focusing on the cause, we can generate a\nrepair tree with a subset of repair actions focusing on ﬁxing this cause. This strategy identiﬁes model elements that must be\nrepaired, as opposed to additional model elements that may or may not have to be repaired later. Furthermore, our approach\ncan provide an ownership-based ﬁlter for ﬁltering repairs that modify model elements not owned by a developer. This ﬁltering\ncan further reduce the repair possibilities, aiding the developer when choosing repairs to be performed. We evaluated our\napproach on 24 UML models and four Java systems, using 17 UML consistency rules and 14 Java consistency rules. The\nevaluation data contained 39,683 inconsistencies, showing our approach’s usability as the repair trees sizes ranged from ﬁve\nto nine on average per model. Also, these repair trees were generated in 0.3 seconds on average, showing our approach’s\nscalability. Based on the results, we discuss the correctness and minimalism with regard to the cause of the inconsistency.\nLastly, we evaluated the ﬁltering mechanism, showing that it is possible to further reduce the number of repairs generated by\nfocusing on ownership.",
        "keywords": [
            "Model-driven engineering",
            "Inconsistency repair",
            "Consistency checking",
            "Repair generation"
        ],
        "authors": [
            "Luciano Marchezan",
            "Roland Kretschmer",
            "Wesley K. G. Assunção",
            "Alexander Reder",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-022-00996-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "AI-driven streamlined modeling: experiences and lessons learned from multiple domains",
        "submission-date": "2021/03",
        "publication-date": "2022/02",
        "abstract": "Model-driven technologies (MD*), considered beneﬁcial through abstraction and automation, have not enjoyed widespread adoption in the industry. In keeping with the recent trends, using AI techniques might help the beneﬁts of MD* outweigh their costs. Although the modeling community has started using AI techniques, it is, in our opinion, quite limited and requires a change in perspective. We provide such a perspective through ﬁve industrial case studies where we use AI techniques in different modeling activities. We discuss our experiences and lessons learned, in some cases evolving purely modeling solutions with AI techniques, and in others considering the AI aids from the beginning. We believe that these case studies can help the researchers and practitioners make sense of various artifacts and data available to them and use applicable AI techniques to enhance suitable modeling activities.",
        "keywords": [
            "AI-driven",
            "Domain modeling",
            "Natural language processing",
            "Information extraction",
            "Knowledge graphs"
        ],
        "authors": [
            "Sagar Sunkle",
            "Krati Saxena",
            "Ashwini Patil",
            "Vinay Kulkarni"
        ],
        "file_path": "data/sosym-all/s10270-022-00982-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the theme section on multi-level modeling",
        "submission-date": "2022/02",
        "publication-date": "2022/03",
        "abstract": "Multi-level modeling (MLM) [5] represents a signiﬁcant extension to the traditional two-level object-oriented paradigm with the potential to improve upon the utility, reliability,andcomplexityofmodels.Differentfromconventional approaches, MLM approaches allow for an arbitrary number of classiﬁcation levels and introduce other concepts that foster expressiveness, reuse, and adaptability. A key aspect of the MLM paradigm is the use of entities (so-called clabjects) that are simultaneously types and instances [6], a feature which has consequences for conceptual modeling, for language engineering, and for the model-based development of software-intensive systems. MLM facilitates also deep instantiation [7], which, in contrast to shallow instantia-tion, allows model elements at a level to not only specify a scheme for elements at the next lower level but also to specify schemes for elements located at levels further down in the hierarchy. Different MLM approaches use different techniques to control and maintain this kind of instantiation. In Potency-based approaches [6,8], for instance, a natural number (potency) is assigned to each model element indicat-ing how many levels down in the hierarchy that element can be instantiated. Different variants of potency have been proposed to satisfy practical requirements, such as leap potency (facilitating jumps over levels) and depth (enforcing the last level at which an element may be instantiated). While MLM collects a group of modeling features under one umbrella term, it has also fertilized interesting discus-sions regarding some of its basic modeling constructs, the principles behind levels, and the mechanisms used for con-trolling deep instantiation. The MULTI workshop series is providing a forum for the MLM community to address the foundations of MLM approaches and support future developments through better modeling languages, tools, methods, and guidelines. The workshop calls have encouraged the presentation of case studies and tool demonstrations as well as novel concepts, implementation approaches, formalisms, controversial positions, and requirements for evaluation criteria. In the workshops, successful applications of MLM have been reported in domains such as software engineering, process modeling, enterprise modeling, industrial automation engineering, smart cities, and building information mod-eling. Furthermore, multiple common challenges, e.g., the bicycle challenge [2] and the process challenge [4], have been proposed as case studies in order to compare MLM approaches. Challenge participants were asked to employ an MLM approach to represent a domain that was initially described in natural language. These initiatives have brought together researchers and practitioners in the area of MLM, speciﬁcally interested in developing conceptual modeling, domain-speciﬁc lan-guages, database systems, and ontologies, to discuss syn-ergies, common problems, and solutions of the different engineering disciplines, tool building concerns and tech-niques, and importantly, the vision for the future of MLM. The Model-Driven Engineering (MDE) community has long embraced the need for MLM. Since the ﬁrst MULTI workshop on MLM in 2014, a series of 8 workshop edi-tions were co-located with the ﬂagship conference in MDE: the International Conference on Model Driven Engineering Languages and Systems (MODELS). Many articles relying on MLM principles have been published in the Software and Systems Modeling (SoSyM) journal, and in a special issue of the EMISA journal. Furthermore, a Dagstuhl Seminar [3] in 2017 was organized and an increasing number of tools and languages have been proposed over the last years [1] including DPF workbench, GModel, Melanee, MetaDepth, MultEcore, Nivel, OMME, ML2, and XModeller.",
        "keywords": [],
        "authors": [
            "Adrian Rutle",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-022-00987-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Empirically evaluating modeling language ontologies: the Peira framework",
        "submission-date": "2023/06",
        "publication-date": "2024/04",
        "abstract": "Conceptual modeling plays a central role in planning, designing, developing and maintaining software-intensive systems. One of the goals of conceptual modeling is to enable clear communication among stakeholders involved in said activities. To achieve effective communication, conceptual models must be understood by different people in the same way. To support such shared understanding, conceptual modeling languages are deﬁned, which introduce rules and constraints on how individual models can be built and how they are to be understood. A key component of a modeling language is an ontology, i.e., a set of concepts that modelers must use to describe world phenomena. Once the concepts are chosen, a visual and/or textual vocabulary is adopted for representing the concepts. However, the choices both of the concepts and of the vocabulary used to represent them may affect the quality of the language under consideration: some choices may promote shared understanding better than other choices. To allow evaluation and comparison of alternative choices, we present Peira, a framework for empirically measuring the domain and comprehensibility appropriateness of conceptual modeling language ontologies. Given a language ontology to be evaluated, the framework is based on observing how prospective language users classify domain content under the concepts put forth by said ontology. A set of metrics is then used to analyze the observations and identify and characterize possible issues that the choice of concepts or the way they are represented may have. The metrics are abstract in that they can be operationalized into concrete implementations tailored to speciﬁc data collection instruments or study objectives. We evaluate the framework by applying it to compare an existing language against an artiﬁcial one that is manufactured to exhibit speciﬁc issues. We then test if the metrics indeed detect these issues. We ﬁnd that the framework does offer the expected indications, but that it also requires good understanding of the metrics prior to committing to interpretations of the observations.",
        "keywords": [
            "Conceptual modeling",
            "Modeling language quality",
            "Empirical methods",
            "Peira"
        ],
        "authors": [
            "Sotirios Liaskos",
            "Saba Zarbaf",
            "John Mylopoulos",
            "Shakil M. Khan"
        ],
        "file_path": "data/sosym-all/s10270-023-01147-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A behavioral analysis and veriﬁcation approach to pattern-based design composition",
        "submission-date": "2002/09",
        "publication-date": "2004/04",
        "abstract": "Integrating software components to produce large-scale software systems is an eﬀective way to reuse experience and reduce cost. However, unexpected interactions among components when integrated into software systems are often the cause of failures. Discovering these composition errors early in the development process could lower the cost and eﬀort in ﬁxing them. This paper introduces a rigorous analysis approach to software design composition based on automated veriﬁcation techniques. We show how to represent, instantiate and integrate design components, and how to ﬁnd design composition errors using model checking techniques. We illustrate our approach with a Web-based hypermedia case study.",
        "keywords": [
            "Design patterns",
            "Software design",
            "Software components",
            "Software speciﬁcation",
            "Veriﬁcation",
            "Model checking",
            "Hypermedia systems"
        ],
        "authors": [
            "Jing Dong",
            "Paulo S.C. Alencar",
            "Donald D. Cowan"
        ],
        "file_path": "data/sosym-all/s10270-004-0056-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "FloWare: a model-driven approach fostering reuse and customisation in IoT applications modelling and development",
        "submission-date": "2021/10",
        "publication-date": "2022/08",
        "abstract": "The relevance of IoT-based solutions in everyday life is continuously increasing. The capability to sense the world, activate\ncomputation based on data gathered by sensors, and possibly produce reactions on the world itself results in an almost never-\nending identiﬁcation of novel IoT solutions and application scenarios. Nonetheless, IoT’s intrinsic nature, which includes a\nhigh degree of variability in used devices, data formats, resources, and communication protocols, complicates the design,\ndevelopment, reuse and customisation of IoT-based software systems. In addition, customers require personalised solutions\nstrongly based on their speciﬁc requirements. Reducing the complexity of building customised solutions and increasing the\nreusability of developed artefacts are among the topmost challenges for enterprises and IoT application developers. Upon\nthese challenges, we propose a model-driven approach organising the modelling and development of IoT applications in\ndifferent steps, handling the complexity in representing the IoT domain variability, and empowering the reusability of design\ndecisions and artefacts to simplify the derivation of customised IoT applications. Our proposal is named FloWare. It follows\nthe typical path of an MDE solution, providing modelling support through feature models to fully represent and handle the\npossible variability of devices in a speciﬁc IoT application domain. Once a speciﬁc conﬁguration has been selected, this\nwill be complemented with speciﬁc information about the deployment context to automatically derive fragments of the IoT\napplications, that will be successively combined by the developer within a low-code development environment. The approach\nis fully supported by a toolchain that has been released for public use.",
        "keywords": [
            "IoT application development",
            "Model-driven",
            "Variability modelling",
            "Low-code",
            "Customised applications",
            "Design artefact reusability"
        ],
        "authors": [
            "Flavio Corradini",
            "Arianna Fedeli",
            "Fabrizio Fornari",
            "Andrea Polini",
            "Barbara Re"
        ],
        "file_path": "data/sosym-all/s10270-022-01026-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Web services-based tool-integration in the ETI platform",
        "submission-date": "2003/12",
        "publication-date": "2004/11",
        "abstract": "In this paper we present dETI, the next gen-eration of the Electronic Tool Integration (ETI) plat-form, an open platform for the interactive experimen-tation with and coordination of heterogeneous software tools via the internet. Our redesign, which is based on the experience gained while running the ETI platform since 1997, focusses on the tool integration process, which clearly marked the bottleneck for the wide acceptance of the ETI platform on the side of an important group of users: the tool providers. The new integration approach makes use of standard Web Services technology, which perfectly ﬁts in the overall ETI architecture.\nOur approach realizes a clear separation of concerns, which overcomes all the previously observed obstacles by (i) decoupling the integration tasks of the tool providers and the ETI team, (ii) pulling the ETI team out of the upgrading and maintenance loop and (iii) handing the upgrading and access control over to the tool providers. This guarantees the scalability in the number of tools available within ETI, and addresses the ﬂexibility con-cerns of the tool providers.",
        "keywords": [
            "Distributed coordination",
            "ETI",
            "Tool-experimentation platform",
            "Tool-integration",
            "Web services"
        ],
        "authors": [
            "Tiziana Margaria"
        ],
        "file_path": "data/sosym-all/s10270-004-0072-z.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A generic approach to detect design patterns in model transformations using a string-matching algorithm",
        "submission-date": "2020/06",
        "publication-date": "2021/11",
        "abstract": "Maintaining software artifacts is a complex and time-consuming task. Like any other program, model transformations are subject to maintenance. In a maintenance process, much effort is dedicated to the comprehension of programs. To this end, several techniques are used, such as feature location and design pattern detection. In the particular case of model transformations, detecting design patterns contributes to a better comprehension as they carry valuable information on the transformation structure. In this paper, we propose a generic approach to detect, semi-automatically, design patterns and their variations in model transformations. Our approach encodes both design patterns and transformations as strings and use a string-matching algorithm for the detection. The approach is able to detect complete and partial implementations of design patterns in transformations, which is useful to refactoring and improving model transformations.",
        "keywords": [
            "Design pattern",
            "Model transformation",
            "Pattern detection",
            "String matching",
            "Bit-vector",
            "Model-driven engineering"
        ],
        "authors": [
            "Chihab eddine Mokaddem",
            "Houari Sahraoui",
            "Eugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-021-00936-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The OsMoSys approach to multi-formalism modeling of systems",
        "submission-date": "2003/11",
        "publication-date": "2003/11",
        "abstract": "Analysis and simulation of complex systems are facilitated by the availability of appropriate modeling formalisms and tools. In many cases, no single analysis and modeling method can successfully cope with all aspects of a complex system: a multi-formalism multi-solution approach is very appealing, since it oﬀers the possibility of applying the most suitable formalisms and solution techniques to model and analyze diﬀerent components or aspects of a system. Another important feature that a successfull modeling approach should include is the possibility of reusing (sub)models: by composing parameterized submodels and then instantiating the parameters, complete models of diﬀerent scenarios can be obtained and analyzed.\nThis paper introduces an innovative approach to multi-formalism modeling of systems that is part of the OsMoSys (Object-based multi-formaliSm MOdeling of SYStems) framework. OsMoSys uses the proposed modeling approach to build multi-formalism models, and workﬂow management to achieve multi-solution. Our modeling approach is based on meta-modeling, allowing to easily deﬁne and integrate diﬀerent formalisms, and on some concepts from object orientation. Its main objectives are the interoperability of diﬀerent formalisms and the deﬁnition of mechanisms to guarantee the ﬂexibility and the scalability of the modeling framework.",
        "keywords": [
            "Multi-formalism modeling",
            "Meta-languages",
            "Object orientation",
            "Compositionality"
        ],
        "authors": [
            "V. Vittorini",
            "M. Iacono",
            "N. Mazzocca",
            "G. Franceschinis"
        ],
        "file_path": "data/sosym-all/s10270-003-0039-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enforcing ﬁne-grained access control for secure collaborative modelling using bidirectional transformations",
        "submission-date": "2017/03",
        "publication-date": "2017/11",
        "abstract": "Large-scale model-driven system engineering projects are carried out collaboratively. Engineering artefacts stored in model repositories are developed in either ofﬂine (checkout–modify–commit) or online (GoogleDoc-style) scenarios. Complex systems frequently integrate models and components developed by different teams, vendors and suppliers. Thus, conﬁdentiality and integrity of design artefacts need to be protected in accordance with access control policies. We propose a secure collaborative modelling approach where ﬁne-grained access control for models is strictly enforced by bidirectional model transformations. Collaborators obtain ﬁltered local copies of the model containing only those model elements which they are allowed to read; write access control policies are checked on the server upon submitting model changes. We present a formal collaboration schema which provenly guarantees certain correctness constraints, and its adaption to online scenarios with on-the-ﬂy change propagation and the integration into existing version control systems to support ofﬂine scenarios. The approach is illustrated, and its scalability is evaluated using a case study of the MONDO EU project.",
        "keywords": [
            "Collaborative modelling",
            "Secured views",
            "Access control",
            "Online collaboration",
            "Ofﬂine collaboration",
            "Bidirectional model transformation"
        ],
        "authors": [
            "Csaba Debreceni",
            "Gábor Bergmann",
            "István Ráth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-017-0631-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Going beyond templates: composition and evolution in nested OSTRICH",
        "submission-date": "2023/05",
        "publication-date": "2024/05",
        "abstract": "Low-code frameworks strive to simplify and speed up application development. An essential mechanism to achieve these goals is to have native support for the safe reuse and usage of parameterized coarse-grain components, providing developers with strong guardrails and a rich software-building experience. OSTRICH—a rich template language for the OutSystems platform—was designed to simplify the use and creation of such components. Thus, the application developer can quickly reuse and assemble sophisticated and thoroughly tested application blocks. However, without a built-in composition and evolution mechanism, OSTRICH templates are still hard to create and maintain. This sometimes requires the repetition of code across different templates and creates a conflict between the customizations of the instantiated application models, and the update and reapplication of a template definition. This paper presents a principled mechanism for using abstraction in the creation of templates and simultaneously supporting the evolution of OSTRICH templates in applications after use. First, we introduce a template composition mechanism, its typing discipline, and its instantiation algorithm for model-driven low-code development environments. We start by extending OSTRICH to support nested templates and allow the instantiation (hatching) of templates in the definition of other templates. Nesting promotes a significant increase in code reuse potential, leading to a safer evolution of applications. We then introduce the support for customizable template instances, which allows one to evolve templates’ code and then update a template instance without losing customizations performed in the generated code. The present definition seamlessly extends the existing OutSystems metamodel with template constructs expressed by model annotations that maintain backward compatibility with the existing language toolchain. We present the metamodel, a set of annotations to support the extensions, and the corresponding validation and instantiation algorithms. In particular, we introduce a type-based validation procedure for abstractions that ensures that using templates always produces valid models. This work also extends prior developments on Nested OSTRICH with the support for safe customizations of instantiated code. We validate Nested OSTRICH using the OSTRICH benchmark by identifying the degree of reusability that can be reached in the existing sample of real templates and template uses. Our prototype is an extension of the OutSystems IDE that allows the annotation of models and their use to produce new models. We also analyze which existing OutSystems sample screen templates can be improved by using and sharing nested templates.",
        "keywords": [
            "Metaprogramming",
            "Low-code models",
            "Metamodel",
            "Templating",
            "Typechecking templates",
            "Model reuse"
        ],
        "authors": [
            "João Costa Seco",
            "Hugo Lourenço",
            "Joana Parreira",
            "Carla Ferreira"
        ],
        "file_path": "data/sosym-all/s10270-024-01178-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "OSTRICH"
        }
    },
    {
        "title": "Automated synthesis of local time requirement for service composition",
        "submission-date": "2018/02",
        "publication-date": "2020/03",
        "abstract": "Service composition aims at achieving a business goal by composing existing service-based applications or components. The response time of a service is crucial, especially in time-critical business environments, which is often stated as a clause in service-level agreements between service providers and service users. To meet the guaranteed response time requirement of a composite service, it is important to select a feasible set of component services such that their response time will collectively satisfy the response time requirement of the composite service. In this work, we use the BPEL modeling language that aims at specifying Web services. We extend it with timing parameters and equip it with a formal semantics. Then, we propose a fully automated approach to synthesize the response time requirement of component services modeled using BPEL, in the form of a constraint on the local response times. The synthesized requirement will guarantee the satisfaction of the global response time requirement, statically or dynamically. We implemented our work into a tool, Selamat and performed several experiments to evaluate the validity of our approach.",
        "keywords": [
            "Web service composition",
            "Parameter synthesis",
            "Modeling Web services",
            "Formal semantics",
            "BPEL",
            "Parametric model checking"
        ],
        "authors": [
            "Étienne André",
            "Tian Huat Tan",
            "Manman Chen",
            "Shuang Liu",
            "Jun Sun",
            "Yang Liu",
            "Jin Song Dong"
        ],
        "file_path": "data/sosym-all/s10270-020-00787-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Validation and veriﬁcation in domain-speciﬁc modeling method engineering: an integrated life-cycle view",
        "submission-date": "2022/03",
        "publication-date": "2022/10",
        "abstract": "Enterprise models have the potential to constitute a valuable asset for organizations, e.g., in terms of enabling a variety of\nanalyses or by fostering cross-organizational communication. Therefore, while designing an enterprise modeling method one\nneeds to ensure that created enterprise models are of good quality in terms of: (1) syntactic validity, which entails that a\nmodel adheres to syntactic rules encoded in the underlying modeling language, (2) semantic validity, i.e., that the model\nshould make sense in its context of use, and (3) pragmatic validity, i.e., that the model should effectively and efﬁciently serve\nthe intended purpose. To ensure these three validity types, veriﬁcation and validation (V&V) techniques need to be exploited\nwhile designing the enterprise modeling method, e.g., to check created enterprise models against syntactic rules, or to ensure\nintra- and inter-model consistency. This paper targets the systematic embedding of V&V techniques into the engineering\nof (enterprise) domain-speciﬁc modeling methods (DSMMs). Speciﬁcally, after identifying and analyzing existing DSMM\nengineering approaches, we synthesize their elements (such as typical phases and steps) and enrich them with V&V techniques.\nThis paper is an extension of our previous work and additionally contributes (1) a systematic analysis of a wider set of existing\napproaches to DSMM engineering, (2) an extended background that covers information on models, modeling languages and\nmodeling methods, (3) additional details regarding selected validation and veriﬁcation techniques for each phase, and ﬁnally\n(4) a road-map encompassing desiderata for further advances in V&V in domain-speciﬁc modeling method engineering, from\nthe perspectives of practice, research and education.",
        "keywords": [
            "Domain-speciﬁc modeling method engineering",
            "Validation and veriﬁcation",
            "Enterprise modeling"
        ],
        "authors": [
            "Qin Ma",
            "Monika Kaczmarek-Heß",
            "Sybren de Kinderen"
        ],
        "file_path": "data/sosym-all/s10270-022-01056-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Protocol modelling: A modelling approach that supports reusable behavioural abstractions",
        "submission-date": "2005/02",
        "publication-date": "2005/09",
        "abstract": "We describe a behavioural modelling approach based on the concept of a “Protocol Machine”, a machine whose behaviour is governed by rules that determine whether it accepts or refuses events that are presented to it. We show how these machines can be composed in the manner of mixins to model object behaviour and show how the approach provides a basis for deﬁning reusable ﬁne-grained behavioural abstractions. We suggest that this approach provides better encapsulation of object behaviour than traditional object modelling techniques when modelling transactional business systems. We relate the approach to work going on in model driven approaches, speciﬁcally the Model Driven Architecture initiative sponsored by the Object Management Group.",
        "keywords": [
            "Behavioural modelling",
            "Reuse",
            "Protocols",
            "State machines",
            "Mixins",
            "Executable modelling"
        ],
        "authors": [
            "Ashley McNeile",
            "Nicholas Simons"
        ],
        "file_path": "data/sosym-all/s10270-005-0100-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven optimal resource scaling in cloud",
        "submission-date": "2015/07",
        "publication-date": "2017/02",
        "abstract": "Cloud computing offers the ﬂexibility to dynamically size the infrastructure in response to changes in workload demand. While both horizontal scaling and vertical scaling of infrastructure are supported by major cloud providers, these scaling options differ signiﬁcantly in terms of their cost, provisioning time, and their impact on workload performance. Importantly, the efﬁcacy of horizontal and vertical scaling critically depends on the workload characteristics, such as the workload’s parallelizability and its core scalability. In today’s cloud systems, the scaling decision is left to the users, requiring them to fully understand the trade-offs associated with the different scaling options. In thispaper,wepresentoursolutionforoptimizingtheresource scaling of cloud deployments via implementation in Open-Stack. The key component of our solution is the modeling engine that characterizes the workload and then quantitatively evaluates different scaling options for that workload. Our modeling engine leverages Amdahl’s Law to model service timescaling in scale-up environments and queueing-theoretic concepts to model performance scaling in scale-out environments.WefurtheremployKalmanﬁlteringtoaccount for inaccuracies in the model-based methodology and to dynamically track changes in the workload and cloud environment.",
        "keywords": [
            "Autoscaling",
            "Modeling",
            "Scale-up",
            "Scale-out",
            "Cost",
            "Optimal",
            "Experimentation",
            "Implementation"
        ],
        "authors": [
            "Anshul Gandhi",
            "Parijat Dube",
            "Alexei Karve",
            "Andrzej Kochut",
            "Li Zhang"
        ],
        "file_path": "data/sosym-all/s10270-017-0584-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Contrasting dedicated model transformation languages versus general purpose languages: a historical perspective on ATL versus Java",
        "submission-date": "2021/06",
        "publication-date": "2021/11",
        "abstract": "Model transformations are among the key concepts of model-driven engineering (MDE), and dedicated model transformation languages (MTLs) emerged with the popularity of the MDE pssaradigm about 15 to 20 years ago. MTLs claim to increase the ease of development of model transformations by abstracting from recurring transformation aspects and hiding complex semantics behind a simple and intuitive syntax. Nonetheless, MTLs are rarely adopted in practice, there is still no empirical evidence for the claim of easier development, and the argument of abstraction deserves a fresh look in the light of modern general purpose languages (GPLs) which have undergone a signiﬁcant evolution in the last two decades. In this paper, we report about a study in which we compare the complexity and size of model transformations written in three different languages, namely (i) the Atlas Transformation Language (ATL), (ii) Java SE5 (2004–2009), and (iii) Java SE14 (2020); the Java transformations are derived from an ATL speciﬁcation using a translation schema we developed for our study. In a nutshell, we found that some of the new features in Java SE14 compared to Java SE5 help to signiﬁcantly reduce the complexity of transformations written in Java by as much as 45%. At the same time, however, the relative amount of complexity that stems from aspects that ATL can hide from the developer, which is about 40% of the total complexity, stays about the same. Furthermore we discovered that while transformation code in Java SE14 requires up to 25% less lines of code, the number of words written in both versions stays about the same. And while the written number of words stays about the same their distribution throughout the code changes signiﬁcantly. Based on these results, we discuss the concrete advancements in newer Java versions. We also discuss to which extent new language advancements justify writing transformations in a general purpose language rather than a dedicated transformation language. We further indicate potential avenues for future research on the comparison of MTLs and GPLs in a model transformation context.",
        "keywords": [
            "ATL",
            "Java",
            "Model transformations",
            "Model transformation language",
            "General purpose language",
            "Comparison",
            "MTL versus GPL",
            "Historical perspective",
            "Complexity measure",
            "Size measure"
        ],
        "authors": [
            "Stefan Höppner",
            "Timo Kehrer",
            "Matthias Tichy"
        ],
        "file_path": "data/sosym-all/s10270-021-00937-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Evaluating user acceptance of knowledge-intensive business process modeling languages",
        "submission-date": "2022/02",
        "publication-date": "2023/08",
        "abstract": "CaseManagementhasbeenevolvingtosupportknowledge-intensivebusinessprocessmanagement,resultingindifferentmod-eling languages, e.g., Declare, Dynamic Condition Response (DCR), and Case Management Model and Notation (CMMN). A language will die if users do not accept and use it in practice—similar to extinct human languages. Thus, evaluating how users perceive languages is important to improve them. Although some studies have investigated how the process designers perceived Declare and DCR, there is a lack of research on how they perceive CMMN—especially in comparison with other languages. Therefore, this paper investigates and compares how process designers perceive these languages based on the Technology Acceptance Model. The paper includes two studies conducted in 2020 and 2022, both performed by educating participants through a course, with feedback on their assignments, to reduce biases. The perceptions are collected through questionnaires before and after feedback on the ﬁnal practice. Results show that the perceptions change is insigniﬁcant after feedback due to the participants being well-trained. The reliability of responses was tested using Cronbach’s alpha. The results of the ﬁrst study show that both DCR and CMMN were perceived as having acceptable usefulness and ease of use, but CMMN was perceived as signiﬁcantly better than DCR in terms of ease of use. The results of the second study show that only DCR was perceived signiﬁcantly better than Declare in terms of usefulness. The participants’ feedback shows potential areas for improvement in languages and tool support to enhance perceived usefulness and ease of use.",
        "keywords": [
            "Business process modeling",
            "Knowledge-intensive",
            "Case management",
            "CMMN",
            "DCR",
            "Declare"
        ],
        "authors": [
            "Amin Jalali"
        ],
        "file_path": "data/sosym-all/s10270-023-01120-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "TURTLE-P: a UML proﬁle for the formal validation of critical and distributed systems",
        "submission-date": "2004/01",
        "publication-date": "2006/07",
        "abstract": "The timed UML and RT-LOTOS environ-\nment, or TURTLE for short, extends UML class and\nactivity diagrams with composition and temporal oper-\nators. TURTLE is a real-time UML proﬁle with a for-\nmal semantics expressed in RT-LOTOS. Further, it is\nsupported by a formal validation toolkit. This paper\nintroduces TURTLE-P, an extended proﬁle no longer\nrestricted to the abstract modeling of distributed sys-\ntems. Indeed, TURTLE-P addresses the concrete\ndescriptions of communication architectures, including\nquality of service parameters (delay, jitter, etc.). This\nnew proﬁle enables co-design of hardware and soft-\nware components with extended UML component and\ndeployment diagrams. Properties of these diagrams can\nbe evaluated and/or validated thanks to the formal\nsemantics given in RT-LOTOS. The application of TUR-\nTLE-P is illustrated with a telecommunication satellite\nsystem.",
        "keywords": [],
        "authors": [
            "Ludovic Apvrille",
            "Pierre de Saqui-Sannes",
            "Ferhat Khendek"
        ],
        "file_path": "data/sosym-all/s10270-006-0029-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Visual modeling of RESTful conversations with RESTalk",
        "submission-date": "2016/01",
        "publication-date": "2016/05",
        "abstract": "Abstract The cost savings introduced by Web services through code reuse and integration opportunities have motivated many businesses to develop Web APIs, with ever increasing numbers opting for the REST architectural style. RESTful Web APIs are decomposed in multiple resources, which the client can manipulate through HTTP interactions with well-deﬁned semantics. Getting the resource in the desired state might require multiple client–server interactions, what we deﬁne as a RESTful conversation. RESTful conversations are dynamically guided by hypermedia controls, such as links. Thus, when deciding whether and how to use a given RESTful service, the client might not be aware of all the interactions which are necessary to achieve its goal. This is because existing documentation of RESTful APIs describes the static structure of the interface, exposing low-level HTTP details, while little attention has been paid to conceptual, high-level, modeling of the dynamics of REST-ful conversations. Low-level HTTP details can be abstracted from during the design phase of the API, or when deciding which API to use. We argue that, in these situations, visual models of the required client–server interactions might increase developers’ efﬁciency and facilitate their understanding. Thus, to capture all possible interaction sequences in a given RESTful conversation, we propose RESTalk, an extension to the BPMN Choreography diagrams to render them more concise and yet sufficiently expressive in the specific REST domain. We also report on the results obtained from an exploratory survey we have conducted to assess the maturity of the field for a domain-specific language and to obtain feedback for future improvements of RESTalk.",
        "keywords": [
            "RESTful Web services",
            "Conversations",
            "BPMN Choreography",
            "Modeling notation extension",
            "Exploratory study",
            "Domain-specific language",
            "Questionnaire",
            "RESTalk"
        ],
        "authors": [
            "Ana Ivanchikj",
            "Cesare Pautasso",
            "Silvia Schreier"
        ],
        "file_path": "data/sosym-all/s10270-016-0532-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dash: declarative behavioural modelling in Alloy with control state hierarchy",
        "submission-date": "2021/06",
        "publication-date": "2022/08",
        "abstract": "We present Dash, an extension to the Alloy language to model dynamic behaviour using the labelled control state hierarchy of Statecharts. From Statecharts, Dash borrows the concepts to specify hierarchy, concurrency, and communication for describing behaviour in a compositional manner. From Alloy, Dash uses the expressiveness of relational logic and set theory to abstractly and declaratively describe structures, data, and operations. We justify our semantic design decisions for Dash, which carefully mix the usual semantic understanding of control state hierarchy with the declarative perspective. We describe and implement the semantics of a Dash model by translating it to Alloy, taking advantage of Alloy language features. We evaluate our Dash translation and perform model checking analysis, enabled by our translation, in the Alloy Analyzer using several case studies. Dash provides modellers with a language that seamlessly combines the semantics of control-modelling paradigms with Alloy’s existing strengths in modelling data and operations abstractly.",
        "keywords": [
            "Declarative modelling",
            "Transition systems",
            "Alloy",
            "Statecharts"
        ],
        "authors": [
            "Jose Serna",
            "Nancy A. Day",
            "Shahram Esmaeilsabzali"
        ],
        "file_path": "data/sosym-all/s10270-022-01012-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model interoperability in building information modelling",
        "submission-date": "2009/11",
        "publication-date": "2010/10",
        "abstract": "The exchange of design models in the design and construction industry is evolving away from 2-dimensional computer-aided design (CAD) and paper towards semantically-rich 3-dimensional digital models. This approach, known as Building Information Modelling (BIM), is anticipated to become the primary means of information exchange between the various parties involved in construction projects. From a technical perspective, the domain represents an interesting study in model-based interoperability, since the models are large and complex, and the industry is one in which collaboration is a vital part of business. In this paper, we present our experiences with issues of model-based interoperability in exchanging building information models between various tools, and in implementing tools which consume BIM models, particularly using the industry standard IFC data modelling format. We report on the successes and challenges in these endeavours, as the industry endeavours to move further towards fully digitised information exchange.",
        "keywords": [
            "Building Information Modelling",
            "Interoperability"
        ],
        "authors": [
            "Jim Steel",
            "Robin Drogemuller",
            "Bianca Toth"
        ],
        "file_path": "data/sosym-all/s10270-010-0178-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MemoRec: a recommender system for assisting modelers in specifying metamodels",
        "submission-date": "2020/11",
        "publication-date": "2022/03",
        "abstract": "Model-driven engineering has been widely applied in software development, aiming to facilitate the coordination among various stakeholders. Such a methodology allows for a more efﬁcient and effective development process. Nevertheless, modeling is a strenuous activity that requires proper knowledge of components, attributes, and logic to reach the level of abstraction required by the application domain. In particular, metamodels play an important role in several paradigms, and specifying wrong entities or attributes in metamodels can negatively impact on the quality of the produced artifacts as well as other elements of the whole process. During the metamodeling phase, modelers can beneﬁt from assistance to avoid mistakes, e.g., getting recommendations like metaclasses and structural features relevant to the metamodel being deﬁned. However, suitable machinery is needed to mine data from repositories of existing modeling artifacts and compute recommendations. In this work, we propose MemoRec, a novel approach that makes use of a collaborative ﬁltering strategy to recommend valuable entities related to the metamodel under construction. Our approach can provide suggestions related to both metaclasses and structured features that should be added in the metamodel under deﬁnition. We assess the quality of the work with respect to different metrics, i.e., success rate, precision, and recall. The results demonstrate that MemoRec is capable of suggesting relevant items given a partial metamodel and supporting modelers in their task.",
        "keywords": [
            "Model-driven engineering",
            "Recommender systems",
            "Collaborative filtering techniques"
        ],
        "authors": [
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Claudio Di Sipio",
            "Phuong T. Nguyen",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-022-00994-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The quest for runware: on compositional, executable and intuitive models",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "We believe that future models of complex soft-ware and systems will combine the crucial traits of intuitive-ness, compositionality, and executability. The importance of each of these to modeling is already well recognized, but our vision suggests a far more powerful synergy between them. First, models will be aligned with cognitive processes used by humans to think about system behavior and will be under-stood, and perhaps creatable, by almost anyone. Second, one will be able to build models incrementally, adding to, reﬁn-ing or sculpting away already-speciﬁed behaviors without changing most existing parts of the model. Third, there will be powerful ways to execute such intuitive and compositional models, in whole or in part, at any stage of the development. The presence of these three traits in a single artifact will blur the boundaries between natural-language requirements, for-mal models, and actual software, bringing in its wake a major advance in the way systems are built, and in their cost and quality. We propose the term runware1 to refer to this kind of higher level artifact.",
        "keywords": [
            "Executable speciﬁcations",
            "Model-driven engineering",
            "Behavioral programming",
            "Computational methods"
        ],
        "authors": [
            "David Harel",
            "Assaf Marron"
        ],
        "file_path": "data/sosym-all/s10270-012-0258-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "T-Core: a framework for custom-built model transformation engines",
        "submission-date": "2012/11",
        "publication-date": "2013/08",
        "abstract": "A large number of model transformation lan-guages and tools have emerged since the early 2000s. A transformation engineer is thus left with too many choices for the language he use to perform a speciﬁc transformation task. Furthermore, it is currently not possible to combine or reuse transformations implemented in different languages. We therefore propose T-Core, a framework where primitive transformation constructs can be combined to deﬁne and encapsulate reusable model transformation idioms. In this context, the transformation engineer is free to use existing transformation building blocks from an extensible library or deﬁne his own transformation units. The proposed primi-tive transformation operators are the result of deconstructing different existing transformation languages. Reconstructing these languages offers a common basis to compare their expressiveness, provides a framework for inter-operating them, and allows the transformation engineer to design trans-formations with the most appropriate constructs for the task at hand.",
        "keywords": [
            "Model transformation",
            "Domain-speciﬁc model transformation",
            "Transformation library",
            "Reengineering"
        ],
        "authors": [
            "Eugene Syriani",
            "Hans Vangheluwe",
            "Brian LaShomb"
        ],
        "file_path": "data/sosym-all/s10270-013-0370-4.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Privacy-enhanced BPMN: enabling data privacy analysis in business process models",
        "submission-date": "2018/02",
        "publication-date": "2019/01",
        "abstract": "Privacy-enhancing technologies play an important role in preventing the disclosure of private data as information is transmitted and processed. Although business process model and notation (BPMN) is well suited for expressing stakeholder collaboration and business processes support by technical solutions, little is done to depict and analyze the ﬂow of private information and its technical safeguards as it is disclosed to process participants. This gap motivates the development of privacy-enhanced BPMN (PE-BPMN)—a BPMN language for capturing PET-related activities in order to study the ﬂow of private information and ease the communication of privacy concerns and requirements among stakeholders. We demonstrate its feasibility in a mobile app scenario and present techniques to analyze information disclosures identiﬁed by models enriched with PE-BPMN.",
        "keywords": [
            "Privacy",
            "Business process model and notation (BPMN)",
            "Privacy-enhancing technology (PET)",
            "Information disclosure"
        ],
        "authors": [
            "Pille Pullonen",
            "Jake Tom",
            "Raimundas Matuleviˇcius",
            "Aivo Toots"
        ],
        "file_path": "data/sosym-all/s10270-019-00718-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven traceability framework for software product lines",
        "submission-date": "2009/01",
        "publication-date": "2009/06",
        "abstract": "Software product line (SPL) engineering is a\nrecent approach to software development where a set of soft-\nwareproducts arederivedfor awell deﬁnedtarget application\ndomain, from a common set of core assets using analogous\nmeans of production (for instance, through Model Driven\nEngineering). Therefore, such family of products are built\nfrom reuse, instead of developed individually from scratch.\nSPL promise to lower the costs of development, increase the\nquality of software, give clients more ﬂexibility and reduce\ntime to market. These beneﬁts come with a set of new prob-\nlems and turn some older problems possibly more complex.\nOne of these problems is traceability management. In the\nEuropean AMPLE project we are creating a common trace-\nability framework across the various activities of the SPL\ndevelopment. We identiﬁed four orthogonal traceability\ndimensions in SPL development, one of which is an exten-\nsion of what is often considered as “traceability of variabil-\nity”. This constitutes one of the two contributions of this\npaper. The second contribution is the speciﬁcation of a meta-\nmodel for a repository of traceability links in the context\nof SPL and the implementation of a respective traceabil-\nity framework. This framework enables fundamental trace-\nability management operations, such as trace import and\nCommunicated by Prof. Richard Paige.\nexport, modiﬁcation, query and visualization. The power of\nour framework is highlighted with an example scenario.",
        "keywords": [
            "Traceability",
            "Software product line",
            "Model driven engineering"
        ],
        "authors": [
            "Nicolas Anquetil",
            "Uirá Kulesza",
            "Ralf Mitschke",
            "Ana Moreira",
            "Jean-Claude Royer",
            "Andreas Rummler",
            "André Sousa"
        ],
        "file_path": "data/sosym-all/s10270-009-0120-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Theme section on model-driven requirements engineering",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "To date, the Model-Driven Requirements Engineering (MoDRE) workshop series has produced 12 workshops from 2011 to 2022, all of which were co-located with the IEEE International Requirements Engineering (RE) Conference. The MoDRE workshop series provides a forum to discuss the challenges of Model-Driven Development (MDD) for RE. Building on the interest of MDD for design and implementation, RE may benefit from MDD techniques when properly balancing flexibility for capturing varied user needs with formal rigidity required for model analysis and transformations as well as high-level abstraction with information richness. MoDRE seeks to explore those areas of RE that have not yet been formalized sufficiently to be incorporated into an MDD environment as well as how RE models can benefit from emerging topics in the model-driven community. Over the years, this included topics such as models@runtime, reuse, flexible and collaborative modeling, modeling for DevOps and iterative development, sustainability, human values and ethics, equality and fairness, adaptive systems, and artificial intelligence-enabled modeling and applications.",
        "keywords": [],
        "authors": [
            "Ana Moreira",
            "Gunter Mussbacher",
            "João Araújo",
            "Pablo Sánchez"
        ],
        "file_path": "data/sosym-all/s10270-022-01055-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Code generation for a family of executable modelling notations",
        "submission-date": "2009/06",
        "publication-date": "2010/10",
        "abstract": "We are investigating semantically configurable model-driven engineering (MDE). The goal of this work is a modelling environment that supports flexible, configurable modelling notations, in which specifiers can configure the semantics of notations to suit their needs and yet still have access to the types of analysis tools and code generators normally associated with MDE. In this paper, we describe semantically configurable code generation for a family of behavioural modelling notations. The family includes variants of statecharts, process algebras, Petri Nets, and SDL88. The semantics of this family is defined using template semantics, which is a parameterized structured operational semantics in which parameters represent semantic variation points. A specific notation is derived by instantiating the family’s template semantics with parameter values that specify semantic choices. We have developed a code-generator generator (CGG) that creates a suitable Java code generator for a subset of derivable modelling notations. Our prototype CGG supports 26 semantics parameters, 89 parameter values, and 7 composition operators. As a result, we are able to produce code generators for a sizable family of modelling notations, though at present the performance of our generated code is about an order of magnitude slower than that produced by commercial-grade generators.",
        "keywords": [
            "Model-driven engineering",
            "Code generation"
        ],
        "authors": [
            "Adam Prout",
            "Joanne M. Atlee",
            "Nancy A. Day",
            "Pourya Shaker"
        ],
        "file_path": "data/sosym-all/s10270-010-0176-6.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "The importance of ﬂow in software development",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "From social and psychological theories and studies [1], we know that there exists a mental state called “ﬂow” that allows individuals to concentrate deeply on a speciﬁc task without noticing the surrounding environment or the time, while remaining fully aware of the current work that they are doing.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-017-0621-x.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A structured operational semantics for UML-statecharts",
        "submission-date": "2002/02",
        "publication-date": "2002/12",
        "abstract": "The Uniﬁed Modeling Language (UML) has gained wide acceptance in very short time because of its variety of well-known and intuitive graphical notations. However, this comes at the price of an unprecise and incomplete semantics deﬁnition. This insuﬃciency concerns single UML diagram notations on their own as well as their integration. In this paper, we focus on the notation of UML-statecharts. Starting with a precise textual syntax deﬁnition, we develop a precise structured operational semantics (SOS) for UML-statecharts. Besides the support of interlevel transitions and in contrast to related work, our semantics deﬁnition supports characteristic UML-statechart features like the history mechanism as well as entry and exit actions.",
        "keywords": [
            "Statecharts",
            "UML",
            "UML-statecharts",
            "Formal semantics",
            "Structured operational semantics (SOS)",
            "Labeled transition systems"
        ],
        "authors": [
            "Michael von der Beeck"
        ],
        "file_path": "data/sosym-all/s10270-002-0012-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A methodology for the selection of requirements engineering techniques",
        "submission-date": "2004/11",
        "publication-date": "2007/06",
        "abstract": "The complexity of software projects as well as the multidisciplinary nature of requirements engineering (RE) requires developers to carefully select RE techniques andpracticesduringsoftwaredevelopment.Nevertheless,the selection of RE techniques is usually based on personal preference or existing company practice rather than on characteristics of the project at hand. Furthermore, there is a lack of guidance on which techniques are suitable for a certain project context. So far, only a limited amount of research has been done regarding the selection of RE techniques based on the attributes of the project under development. The few approaches that currently exist for the selection of RE techniques provide only little guidance for the actual selection process. We believe that the evaluation of RE techniques in the context of an application domain and a specific project is of great importance. This paper describes a Methodology for Requirements Engineering Techniques Selection (MRETS) as an approach that helps requirements engineers select suitable RE techniques for the project at hand. The MRETS has three aspects: Firstly, it aids requirements engineers in establishing a link between the attributes of the project and the attributes of RE techniques. Secondly, based on the evaluation schema proposed in our research, MRETS provides an opportunity to analyze RE techniques in detail using clustering. Thirdly, the objective function used in our approach provides an effective decision support mechanism for the selection of RE techniques. This paper makes contributions to RE techniques analysis, the application of RE techniques in practice, RE research, and software engineering in general. The application of the proposed methodology to an industrial project provides preliminary information on the effectiveness of MRETS for the selection of RE techniques.",
        "keywords": [
            "Requirements engineering",
            "Technique evaluation",
            "Decision support",
            "Clustering"
        ],
        "authors": [
            "Li Jiang",
            "Armin Eberlein",
            "Behrouz H. Far",
            "Majid Mousavi"
        ],
        "file_path": "data/sosym-all/s10270-007-0055-y.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A modeling-based approach for dependability analysis of a constellation of satellites",
        "submission-date": "2024/01",
        "publication-date": "2024/07",
        "abstract": "Satellite constellations play critical roles across various sectors, encompassing communication, Earth observation and space exploration. Ensuring the dependable operation of these constellations is of utmost importance. This paper introduces a dependability modeling approach using stochastic Petri nets to analyze satellite constellations. The primary focus is on improving operational efﬁciency through the assessment of availability, reliability and maintainability. The approach helps satellite designers make informed decisions when selecting constellation conﬁgurations by assessing various dependability metrics. Using a global navigation satellite system as a case study, we conduct extensive numerical experiments to evaluate the feasibility of our approach. The results demonstrate quantitatively the signiﬁcant impact of redundant components on both reliability and availability. They also illustrate how utilizing satellites in repair and operational orbits can inﬂuence these metrics and highlight the direct correlation between reliability and maintainability.",
        "keywords": [
            "Dependability",
            "Constellation dependability approach",
            "Satellite constellations",
            "Stochastic Petri net"
        ],
        "authors": [
            "Daniel Farias",
            "Bruno Nogueira",
            "Ivaldir Farias Júnior",
            "Ermeson Andrade"
        ],
        "file_path": "data/sosym-all/s10270-024-01197-7.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Model-based cloud resource management with TOSCA and OCCI",
        "submission-date": "2019/12",
        "publication-date": "2021/02",
        "abstract": "With the advent of cloud computing, different cloud providers with heterogeneous cloud services (compute, storage, network, applications, etc.) and their related Application Programming Interfaces (APIs) have emerged. This heterogeneity complicates the implementation of an interoperable cloud system. Several standards have been proposed to address this challenge and provide a uniﬁed interface to cloud resources. The Open Cloud Computing Interface (OCCI) thereby focuses on the standardization of a common API for Infrastructure-as-a-Service (IaaS) providers, while the Topology and Orchestration Speciﬁcation for Cloud Applications (TOSCA) focuses on the standardization of a template language to enable the proper deﬁnition of the topology of cloud applications and their orchestrations on top of a cloud system. TOSCA thereby does not deﬁne how the application topologies are created on the cloud. Therefore, we analyze the conceptual similarities between the two approaches and we study how we can integrate them to obtain a complete standard-based approach to manage both Cloud Infrastructure and Cloud application layers. We propose an automated extensive mapping between the concepts of the two standards, and we provide TOSCA Studio, a model-driven tool chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically design cloud applications as well as to deploy and manage them at runtime using a fully model-driven cloud orchestrator based on the two standards. Our contribution is validated by successfully transforming and deploying three cloud applications: WordPress, Node Cellar and Multi-Tier.",
        "keywords": [
            "Cloud computing",
            "Standards",
            "OCCI",
            "TOSCA",
            "Model-driven engineering",
            "Metamodels",
            "Cloud orchestrator",
            "Models@run.time"
        ],
        "authors": [
            "Stéphanie Challita",
            "Fabian Korte",
            "Johannes Erbel",
            "Faiez Zalila",
            "Jens Grabowski",
            "Philippe Merle"
        ],
        "file_path": "data/sosym-all/s10270-021-00869-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the theme issue on traceability in model-driven engineering",
        "submission-date": "2010/02",
        "publication-date": "2010/02",
        "abstract": "This issue of Software and Systems Modeling, and part of the issue that follows, are dedicated to the theme of traceability in model-driven engineering (MDE). Traceability is a fundamental concern in MDE processes, where models are related via application of different model management operations, such as model-to-model transformations, model-to-text transformations, model merging, model comparison, and many others. MDE emphasises on the application of automated model management operations, and substantial traceability information can be produced as a side-effect of applying these operations. In addition, in realistic MDE processes, traceability information can be produced by hand, through engineers manually relate MDE artefacts, or relate MDE artefacts with other artefacts (such as requirements documents). Overall, there are many challenges to traceability in MDE, ranging from managing large traceability models, to synchronizing models, and to keep traceability information consistent when models are being modiﬁed automatically and manually. This theme issue presents state-of-the-art research on these and other challenges.",
        "keywords": [],
        "authors": [
            "Richard F. Paige",
            "Goran K. Olsen",
            "Jon Oldevik",
            "Tor Neple"
        ],
        "file_path": "data/sosym-all/s10270-010-0153-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "TacoFlow: optimizing SAT program veriﬁcation using dataﬂow analysis",
        "submission-date": "2012/04",
        "publication-date": "2014/02",
        "abstract": "In previous work, we presented TACO, a tool for efﬁcient bounded veriﬁcation. TACO translates programs annotated with contracts to a SAT problem which is then solved resorting to off-the-shelf SAT-solvers. TACO may deem propositional variables used in the description of a program initial states as being unnecessary. Since the worst-case complexity of SAT (a known NP problem) depends on the number of variables, most times this allows us to obtain signiﬁcant speed ups. In this article, we present TacoFlow, an improvement over TACO that uses dataﬂow analysis in order to also discard propositional variables that describe intermediate program states. We present an extensive empirical evaluation that considers the effect of removing those variables at different levels of abstraction, and a discussion on the beneﬁts of the proposed approach.",
        "keywords": [
            "SAT-based veriﬁcation",
            "Dataﬂow analysis",
            "Java-like programs veriﬁcation"
        ],
        "authors": [
            "Bruno Cuervo Parrino",
            "Juan Pablo Galeotti",
            "Diego Garbervetsky",
            "Marcelo F. Frias"
        ],
        "file_path": "data/sosym-all/s10270-014-0401-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Problem frame semantics for software development",
        "submission-date": "2003/11",
        "publication-date": "2004/07",
        "abstract": "This paper presents a framework for understanding Problem Frames that locates them within the Requirements Engineering model of Zave and Jackson, and its subsequent formalization in the Reference Model of Gunter et al. It distinguishes between problem frames, context diagrams and problem diagrams, and allows us to formally define the relationship between them as assumed in the Problem Frames framework.\nThe semantics of a problem diagram is given in terms of ‘challenges’, a notion that we also introduce. The notion of a challenge is interesting in its own right for two reasons: its proof theoretic derivation leads us to consider a challenge calculus that might underpin the Problem Frame operations of decomposition and recomposition; and it promises to extend the notion of formal refinement from software development to requirements engineering.\nIn addition, the semantics supports a textual representation of the diagrams in which Problem Frames capture problems and their relationship to solutions. This could open the way for graphical Problem Frames tools.",
        "keywords": [
            "Requirements engineering",
            "Problem Frames",
            "Semantics",
            "Reference model",
            "Framework"
        ],
        "authors": [
            "Jon G. Hall",
            "Lucia Rapanotti",
            "Michael Jackson"
        ],
        "file_path": "data/sosym-all/s10270-004-0062-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section of business process modeling, development and support (BPMDS) 2019: transformative BPMDS",
        "submission-date": "2021/09",
        "publication-date": "2021/10",
        "abstract": "The business process modeling, development and support (BPMDS) working conference series is a meeting place for researchers and practitioners in the areas of business development and business applications development. By incorporating these views, BPMDS offers a unique community venue that integrates different streams of research on business processes and business information systems, and takes in a broad view on the whole range of BPMDS research and interrelationships among different perspectives. This makes it attractive for authors to publish cutting-edge research results at BPMDS. In this special section, a selection of the most influential contributions to the 2019 edition of the working conference are collected.",
        "keywords": [],
        "authors": [
            "Jens Gulden"
        ],
        "file_path": "data/sosym-all/s10270-021-00933-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Temporal property patterns for model-based testing from UML/OCL",
        "submission-date": "2016/09",
        "publication-date": "2017/11",
        "abstract": "This article describes a new property- and model-based testing approach using UML/OCL models, driven by temporal property patterns and a tool for assisting the temporal properties formalization. The patterns are expressed in the TOCL language, an adaptation of Dwyer’s property patterns to OCL. The patterns are used to formalize temporal requirements without having to learn a complex temporal logics such as LTL or CTL. From these properties, automata are automatically computed. These can be used for two purposes. First, it is possible to evaluate the quality of a test suite by measuring the coverage of a property using its associated automaton. Second, the automaton can be used to drive the test generation in order to produce complementary test cases. To this end, we deﬁned dedicated coverage criteria, targeting speciﬁc events of the property, and aiming either at illustrating the expected behaviour of the system, or checking its robustness w.r.t. the property. However, it was observed that the semantics of the property language may be more subtle that it seems. To facilitate the adoption of the language by industrials, we have proposed a tool-supported assistant for property design, aiming to help the validation engineer choosing which constructs faithfully correspond to his intention. This approach has been experimented on several case studies with industrial partners. It has shown its interest for software validation, providing useful information thanks to adequate traceability features.",
        "keywords": [
            "Behavioural model",
            "Property patterns",
            "Coverage measure",
            "Test generation",
            "Property design"
        ],
        "authors": [
            "Frédéric Dadeau",
            "Elizabeta Fourneret",
            "Abir Bouchelaghem"
        ],
        "file_path": "data/sosym-all/s10270-017-0635-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "TOCL"
        }
    },
    {
        "title": "Toward an analytical method for SLA validation",
        "submission-date": "2015/06",
        "publication-date": "2017/06",
        "abstract": "Quantitative properties of modern software systems are often defined as a part of a service-level agreement (SLA) that fixes the maximal load to be submitted to a system and guarantees bounds for the response time or delay. The evaluation of software architectures in order to validate SLAs is a challenging task since the systems tend to be complex, highly dynamic and to some extent unpredictable. Thus, there is a need for fast and abstract techniques to evaluate the performance of modern software architectures based on the information available in the SLAs. The paper presents an efficient approach to compute bounds on the delay of composed systems based on available bounds for the load and the response times of components. The technique can be used by a user of a software architecture to validate SLAs of composed services based on SLAs of the components. It can also be used by a provider of a software architecture to validate whether additional users can be accepted or to compute required service capacities to fulfill an SLA.",
        "keywords": [
            "Service-level agreements",
            "Performance analysis",
            "Analytical techniques",
            "Quantitative validation",
            "Capacity planning"
        ],
        "authors": [
            "Peter Buchholz",
            "Sebastian Vastag"
        ],
        "file_path": "data/sosym-all/s10270-017-0604-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain-speciﬁc discrete event modelling and simulation using graph transformation",
        "submission-date": "2011/04",
        "publication-date": "2012/03",
        "abstract": "Graph transformation is being increasingly used to express the semantics of domain-speciﬁc visual languages since its graphical nature makes rules intuitive. However, many application domains require an explicit handling of time to accurately represent the behaviour of a real system and to obtain useful simulation metrics to measure through- puts, utilization times and average delays. Inspired by the vast knowledge and experience accumulated by the discrete event simulation community, we propose a novel way of add- ing explicit time to graph transformation rules. In particu- lar, we take the event scheduling discrete simulation world view and provide rules with the ability to schedule the occur- rence of other rules in the future. Hence, our work combines standard, efﬁcient techniques for discrete event simulation (based on the handling of a future event set) and the intu- itive, visual nature of graph transformation. Moreover, we show how our formalism can be used to give semantics to other timed approaches and provide an implementation on top of the rewriting logic system Maude.",
        "keywords": [
            "Graph transformation",
            "Discrete event simulation",
            "Domain-speciﬁc modelling"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra",
            "Artur Boronat",
            "Reiko Heckel",
            "Paolo Torrini"
        ],
        "file_path": "data/sosym-all/s10270-012-0242-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bridging the gap between IEEE 1471, an architecture description language, and UML",
        "submission-date": "2002/02",
        "publication-date": "2002/12",
        "abstract": "A lot of attention has been paid to soft-ware architecture issues in academia, industrial research and standardization organizations working in the soft-ware area. The software architecture research community has focused on the creation and improvement of special-purpose languages: architecture description languages (ADLs). However, ADLs lack adequate support for separating various kinds of stakeholders’ concerns along diﬀerent viewpoints. But also, they do not address the diﬀerence between the architecture of a software system and its representations. In contrast, ANSI/IEEE-Std-1471 makes a clear distinction between the architecture and the architectural description of a software system. In this paper, we propose ConcernBASE, a UML-based approach to software architecture, which instanti-ates the conceptual framework deﬁned in ANSI/IEEE-Std-1471 and complements the abstractions and mech-anisms found in current ADLs. ConcernBASE provides a viewpoint for structural descriptions of software archi-tectures that supports key concepts of ADLs and deﬁnes a UML proﬁle as a viewpoint language. We validate this proﬁle through a mapping between a representative ADL, called Structural ADL (SADL), and the new proﬁle and by providing a UML-based tool prototype, called Con-cernBASE Modeler, which integrates with SADL tools.",
        "keywords": [
            "Software architecture",
            "Architecture de-scription",
            "UML",
            "ADL",
            "ANSI/IEEE-Std-1471",
            "SADL",
            "Advanced separation of concerns",
            "MDSOC",
            "Views",
            "Viewpoints",
            "Concern space"
        ],
        "authors": [
            "Mohamed M. Kand´e",
            "Valentin Crettaz",
            "Alfred Strohmeier",
            "Shane Sendall"
        ],
        "file_path": "data/sosym-all/s10270-002-0010-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assessing the usefulness of a visual programming IDE for large-scale automation software",
        "submission-date": "2022/04",
        "publication-date": "2023/02",
        "abstract": "Industrial control applications are usually designed by domain experts instead of software engineers. These experts frequently\nuse visual programming languages based on standards such as IEC 61131-3 and IEC 61499. The standards apply model-\nbased engineering concepts to abstract from hardware and low-level communication. Developing industrial control software\nis challenging due to the fact that control systems are usually unique and need to be maintained for many years. The\narising challenges, together with the growing complexity of control software, require very usable model-based development\nenvironmentsforvisualprogramminglanguages.However,sofaronlylittleempiricalresearchexistsonthepracticalusefulness\nof such environments, i.e., their usability and utility. In this paper, we discuss common control software maintenance tasks\nand tool capabilities based on existing research and show the realization of these capabilities in the 4diac IDE. We performed\na walkthrough of the demonstrated capabilities using the cognitive dimensions of notations framework from the ﬁeld of\nhuman–computer interaction. We then improved the tool and conducted a user study involving ten industrial automation\nengineers, who used the 4diac IDE in a realistic control software maintenance scenario. Based on lessons learnt from this\nstudy, we adapted the 4diac IDE to better handle large graphical models. We evaluated these changes in a reassessment\nstudy with automation engineers from seven industrial enterprises. We derive general implications with respect to large-scale\napplications for developers of IDEs that we deem applicable in the context of (visual) model-based engineering tools.",
        "keywords": [
            "Usefulness study",
            "Open source software",
            "IEC 61499",
            "Modeling tools",
            "Model-driven engineering"
        ],
        "authors": [
            "Bianca Wiesmayr",
            "Alois Zoitl",
            "Rick Rabiser"
        ],
        "file_path": "data/sosym-all/s10270-023-01084-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Large language models as an “operating” system for software and systems modeling",
        "submission-date": "2023/09",
        "publication-date": "2023/09",
        "abstract": "Large Language Models (LLM), such as the variants of Chat-GPT or BERT, are currently under intensive development and enhancement, but it has become clear that they will contribute a signiﬁcant change to the way text and images are created in the future. It is therefore not surprising that the challenges, risks and opportunities of generative AI are discussed intensively in various scientiﬁc communities as well as in industry, government, education, and society, in general. There are of course technological aspects to be understood, but the impact on society and the industrial world has been significantly changed by the advantages and consequences of an expanded focus of generative AI, as supported by LLMs. Although there is much to learn from the emergence of this technology, it is becoming more evident how the usage of LLMs may impact the way we develop software and softwareintensivesystems.WeassumethatmanySoSyMreaders have made their own experiments to understand the capabilities and limitations of the currently available LLMs. Similar to any other complex system, it is challenging to understand analytically what the system does and why it produced a specific output. In particular, the traditional computer science formal and precise analytical methods often do not apply to the use of LLMs. Thus, we may need to rely on experimentation to gain initial insights into the capabilities and limitations of the potential for using LLMs in our own work. For a deeper validation of these insights and the hypotheses that will be built in the near future, it may make sense to (among other techniques) apply psychological methods to understand AI systems. Psychology has understood how to analyze brain capabilities and how to deal with uncertainty based on statistical evidence. Such statistical evidence may not be enough when a general AI system is used in safety critical applications (e.g., autonomous driving), but it may be helpful when trying to understand the usefulness of the conversation with such an AI, and how the AI is generally thinking. It may also be helpful to complement the analytical approaches to cope with uncertainty in the modeling activities. In this editorial, we pose another hypothesis on the future use of LLMs. The hypothesis is: 1. Variations of LLMs will emerge, with some very specialized to a distinct task or a specific domain, which focus on a particular underlying knowledge that is constrained by technical or societal concerns. 2. However, it will not be the case that there will be a unique LLM developed and trained for each purpose. Instead, there will be individual post-training of an underlying LLM that allows configuration of an AI model in such a way that it can be adapted easily to the individual functions that need assistance. 3. There will only be a few fully trained core LLMs and they will be provided in an open form. They will act like a kind of “operating system” for AI-systems where “application”-specific extensions are created.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01126-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based lifecycle management of software-intensive systems, applications, and services",
        "submission-date": "2013/06",
        "publication-date": "2013/06",
        "abstract": "The lifecycle of a successful system is the time period that covers all activities associated with developing, conﬁguring, deploying, operating, and retiring the system. Variations in system lifecycles can be expected, for example, differences may arise as a result of the inclusion of physical parts in the system and the number of installations. In addition, software retirement activities may extend over a long period of time, for example, in cases where access to data provided by a system may be required long after the system is terminated. Lifecycle management has a lot to do with managing the available information about a system. A signiﬁcant amount of this information can typically be found in the models produced during various development. Software models can thus play a vital role in system lifecycle management. For exam- ple, requirement models can be used to support management of requirements, feature models can be used to manage sys- tem and user speciﬁc variabilities as well as commonalities, and architecture and design models can provide information that support management of deployment and validation activ- ities. The potential role that models can play in lifecycle man- agement raises the following questions: “To what extent do the models produced during software development help (or hinder) lifecycle management?” “Should the software mod- eling activity be integrated with the lifecycle management of systems, and, if yes, how can this be done?” “What tools are needed to better leverage the use of models in lifecycle man- agement?” “Does a model also have a lifecycle that needs to be managed?”",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-013-0362-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to theme section on requirements formalisation",
        "submission-date": "2024/10",
        "publication-date": "2024/11",
        "abstract": "Requirementsformalisation(RF)isaprocessinrequirements engineering which derives a precise requirements speciﬁcation, expressed, for example, in UML/OCL, from informal or semi-formal requirements documentation expressed using natural language, sketches, etc.\nRequirements formalisation can be a highly resource-intensive process if performed manually, so there has been much research interest in the automation of requirements formalisation. RF automation has signiﬁcant potential as a means of reducing software development costs, accelerating development processes and increasing the rigour of requirements engineering processes. Many approaches have been proposed for automated or semi-automated formalisation of software requirements, typically involving some form of nat-ural language processing (NLP) or machine learning (ML): [1, 2]. Heuristic rule-based approaches have been widely used for RF, based on NLP analysis of textual requirements, and rules that map identiﬁed linguistic constructs to mod-elling elements. For example, a recognised named entity in the requirements documents, that represents a category of system user, can be mapped to a UML use-case actor. Recent work in the ﬁeld has explored the use of large language mod-els (LLMs) such as GPT-4 to perform RF: [3–5].",
        "keywords": [],
        "authors": [
            "Kevin Lano",
            "Shekoufeh Rahimi",
            "Sobhan Tehrani",
            "Lola Burgueño",
            "Mohammad Aminu Umar"
        ],
        "file_path": "data/sosym-all/s10270-024-01241-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A benchmark of incremental model transformation tools based on an industrial case study with AADL",
        "submission-date": "2020/09",
        "publication-date": "2022/03",
        "abstract": "Incremental model transformation (IMT) tools have been proposed to improve performances of model transformations by updating only the parts of a model that need to be changed when another model on which it depends has been changed. Yet, the question is how these tools are suitable for modeling large and complex systems with rich modeling languages as used in industry. In this paper, we report the results of a benchmark of the most mature IMT tools. Particularly, we benchmark MoTE, eMoﬂon, VIATRA and YAMTL to evaluate their usability, maintainability and runtime performances. Our benchmark is based on a model transformation of an industrial case study using the standard architecture description language AADL. We propose a reusable evaluation framework, available for tool developers and users. Besides the capability to process large models, our benchmark also assesses IMT tool performances according to the different kinds of complex structures that typically exist in models of rich languages, as well as the complexity of the transformation speciﬁcations. Our results show the promising potential of some tools to specify sophisticated speciﬁcations and transform large models with good performance, but their use still requires the help of their developers, especially to solve serious problems with their runtime performance.",
        "keywords": [
            "Incremental model transformations",
            "Benchmark",
            "AADL",
            "MoTE",
            "eMoﬂon",
            "VIATRA",
            "YAMTL"
        ],
        "authors": [
            "Hana Mkaouar",
            "Dominique Blouin",
            "Etienne Borde"
        ],
        "file_path": "data/sosym-all/s10270-022-00989-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "TEC-MAP: a taxonomy of evaluation criteria and its application to the multi-modelling of data and processes",
        "submission-date": "2023/11",
        "publication-date": "2024/08",
        "abstract": "The domain of Enterprise Information Systems Engineering uses many different conceptual modelling languages and methods to specify the requirements of a system under development. The complexity of the systems under development may require addressing different perspectives with different models, such as the data and process perspectives. The modeller will thus have to choose the appropriate (set of) modelling languages according to their speciﬁc modelling goal. Given that the different aspects relate to a single system, ideally, the models that capture the different perspectives should be aligned and consistent to ensure their integration. Each candidate (set of) modelling languages comes with advantages and disadvantages. To make an informed choice in this matter, the modeller should select a number of criteria relevant to their problem domain and compare candidate modelling languages based on these criteria. A comprehensive evaluation framework for integrated modelling approaches, that considers more general aspects such as understandability, ease of use, model quality, etc. besides the ability to model the desired aspects, does not yet exist and is therefore the focus of this paper. In recent years, several combinations of modelling languages have been investigated. Amongst these combinations, data + process modelling has attracted a lot of interest, and, interestingly, evaluation frameworks for this combination have been proposed as well. Therefore, this paper will primarily focus on the integrated multi-modelling of data and processes, including the process-related viewpoints of users and authorisations. The contribution of this paper is two-fold: on a theoretical level, the paper provides an overview of existing evaluation frameworks in the literature, builds a more complete set of evaluation criteria and proposes a uniﬁed taxonomy for the classiﬁcation of these evaluation criteria (TEC-MAP); on a practical level, the paper provides guidance and support to the modeller for selecting the appropriate evaluation criteria for their problem domain and presents three examples of the application of TEC-MAP.",
        "keywords": [
            "Data-centric process modelling",
            "Taxonomy",
            "Evaluation framework"
        ],
        "authors": [
            "Charlotte Verbruggen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-024-01198-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MORGAN: a modeling recommender system based on graph kernel",
        "submission-date": "2022/04",
        "publication-date": "2023/04",
        "abstract": "Model-driven engineering (MDE) is an effective means of synchronizing among stakeholders, thereby being a crucial part of\nthe software development life cycle. In recent years, MDE has been on the rise, triggering the need for automatic modeling\nassistants to support metamodelers during their daily activities. Among others, it is crucial to enable model designers to\nchoose suitable components while working on new (meta)models. In our previous work, we proposed MORGAN, a graph\nkernel-based recommender system to assist developers in completing models and metamodels. To provide input for the rec-\nommendation engine, we convert training data into a graph-based format, making use of various natural language processing\n(NLP) techniques. The extracted graphs are then fed as input for a recommendation engine based on graph kernel simi-\nlarity, which performs predictions to provide modelers with relevant recommendations to complete the partially speciﬁed\n(meta)models. In this paper, we extend the proposed tool in different dimensions, resulting in a more advanced recommender\nsystem. Firstly, we equip it with the ability to support recommendations for JSON schema that provides a model representation\nof data handling operations. Secondly, we introduce additional preprocessing steps and a kernel similarity function based on\nitem frequency, aiming to enhance the capabilities, providing more precise recommendations. Thirdly, we study the proposed\nenhancements, conducting a well-structured evaluation by considering three real-world datasets. Although the increasing size\nof the training data negatively affects the computation time, the experimental results demonstrate that the newly introduced\nmechanisms allow MORGAN to improve its recommendations compared to its preceding version.",
        "keywords": [
            "Model-driven engineering",
            "Recommender systems",
            "Graph kernels"
        ],
        "authors": [
            "Claudio Di Sipio",
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Phuong T. Nguyen"
        ],
        "file_path": "data/sosym-all/s10270-023-01102-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based veriﬁcation of data protection mechanisms in collaborative business processes",
        "submission-date": "2022/05",
        "publication-date": "2025/01",
        "abstract": "In scenarios where multiple autonomous systems collaborate to execute a business process, it is often necessary for them to exchange conﬁdential or private data. In this setting, mechanisms need to be put in place to ensure that each participant accesses data in a way that respects conﬁdentiality or privacy constraints. The PE-BPMN notation is an extension of the Business Process Model and Notation (BPMN), speciﬁcally designed to model collections of autonomous systems that execute collaborative processes under the safeguard of privacy-enhancing technologies. Given a PE-BPMN speciﬁcation, we address the problem of verifying that the privacy-enhancing technologies captured in the speciﬁcation are used correctly, and no unexpected data disclosure may happen during process execution. To this end, we formalize the semantics of PE-BPMN collaboration speciﬁcations via a translation into process algebraic models. This makes it possible to check the correct usage of different kinds of privacy-enhancing technologies—e.g. secret sharing, encryption and multi-party computation—via model checking techniques. The approach has been implemented on top of the mCRL2 toolset and integrated into the Pleak toolset for privacy-enhanced business process analysis. The proposal has been evaluated using both real and synthetic scenarios.",
        "keywords": [
            "Collaborative distributed system",
            "Business process",
            "Privacy-enhancing technology",
            "Model checking",
            "MCRL2"
        ],
        "authors": [
            "Sara Belluccini",
            "Rocco De Nicola",
            "Marlon Dumas",
            "Pille Pullonen-Raudvere",
            "Barbara Re",
            "Francesco Tiezzi"
        ],
        "file_path": "data/sosym-all/s10270-024-01217-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "iDOCEM: deﬁning a common terminology for object-centric event logging and data-centric process modelling",
        "submission-date": "2023/10",
        "publication-date": "2024/07",
        "abstract": "In the business process lifecycle, models can be approached from two perspectives: on the one hand, models are used to create systems in the design phase, and on the other hand, systems in use produce (event) logs that are used to discover the models representing the structure of the systems. These discovered models can be the starting point of a new cycle of analysis, redesign, implementation, etc. Therefore, proper logging of implemented processes in line with system design is a critical element for process discovery. Recently, the consideration of the integration of data and process aspects has seen a surge in interest in both the model-for-design domain as in the automated-model-discovery domain. However, it seems that these domains use different conceptualizations of data/object-aware systems. A deﬁnition of how the captured event logs are related to the structure of the global system they are extracted from or are trying to discover is still missing. Especially the concept of an event needs to be aligned, as this is the main concept that the domains have in common. This paper investigates the concepts and terminology used in the different phases of the business process lifecycle: the design phase, the implementation phase (including the implementation of logging) and the discovery phase. The paper contains an extensive running example that is used to illustrate ﬁve misalignment issues. The main contribution of this paper is a meta-model that presents a uniﬁed terminology for modelling both domains and is demonstrated using the running example. The paper also shows how the concepts of iDOCEM relate to the concepts of a conceptual modelling approach and several event logging formats. iDOCEM is validated with the implementation of a log generator for the running case, demonstrating the feasibility of generating DOCEL-compliant logs from an application.",
        "keywords": [
            "Conceptual modelling",
            "Object-centric process logging",
            "Artefact-centric process modelling",
            "Data-aware process modelling",
            "Object-centric process discovery"
        ],
        "authors": [
            "Charlotte Verbruggen",
            "Alexandre Goossens",
            "Johannes De Smedt",
            "Jan Vanthienen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-024-01191-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Adapting transformations to metamodel changes via external transformation composition",
        "submission-date": "2011/03",
        "publication-date": "2013/02",
        "abstract": "Evolution is inherent to software systems because of the rapid improvement of technologies and business logic. As a software development paradigm, model driven engineering (MDE) is also affected by this problem. More concretely, being metamodels the cornerstone of MDE, their evolution impacts the rest of software artefacts involved in a development process, i.e., models and transformations. The inﬂuence over models has been tackled and partially solved in previous works. This paper focuses on the impact over transformations. We propose an approach to adapt transformations by means of external transformation composition. That is, we chain impacted transformations to particular adaptation transformations which deal with either refactoring/destruction changes or construction changes. Our approach semi-automatically generates such transformations by using the AtlanMod matching language, a DSL to deﬁne model matching strategies. To provide with a proof of concept for our proposal, we adapt transformations written in terms of object-relational database metamodels when such metamodels evolve in time.",
        "keywords": [
            "Model-driven engineering",
            "Metamodel evolution",
            "Transformation adaptation"
        ],
        "authors": [
            "Kelly Garcés",
            "Juan M. Vara",
            "Frédéric Jouault",
            "Esperanza Marcos"
        ],
        "file_path": "data/sosym-all/s10270-012-0297-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "AtlanMod matching language"
        }
    },
    {
        "title": "Design veriﬁcation in model-based µ-controller development using an abstract component",
        "submission-date": "2009/01",
        "publication-date": "2010/02",
        "abstract": "Component-based software development is a promising approach for controlling the complexity and quality of software systems. Nevertheless, recent advances in quality control techniques do not seem to keep up with the growing complexity of embedded software; embedded systemsoftenconsistofdozenstohundredsofsoftware/hardware components that exhibit complex interaction behav-ior. Unanticipated quality defects in a component can be a major source of system failure. To address this issue, this paper suggests a design veriﬁcation approach integrated into the model-driven, component-based development methodology Marmot. The notion of abstract components—the basic building blocks of Marmot—helps to lift the level of abstraction, facilitates high-level reuse, and reduces veriﬁcation complexity by localizing veriﬁcation problems between abstract components before reﬁnement and after reﬁnement. This enables the identiﬁcation of unanticipated design errors in the early stages of development. This work introduces Communicated by Dr. Perry Alexander. This paper is an extended version of Choi [11] and Choi and Bunse [13]. This work has been supported by the Korea Research Foundation Grant funded by the Korean Government (KRF-2008-331-D00525) and the Engineering Research Center of Excellence Program of the Korean Ministry of Education, Science and Technology (MEST)/Korea Science and Engineering Foundation (KOSEF), grant number R11-2008-007-03002-0. the Marmot methodology, presents a design veriﬁcation approach in Marmot, and demonstrates its application on the development of a μ-controller-based abstraction of a car mirror control system. An application on TinyOS shows that the approach helps to reuse models as well as their veriﬁca-tion results in the development process.",
        "keywords": [
            "Abstract component",
            "Model-driven development",
            "Design veriﬁcation",
            "Embedded systems"
        ],
        "authors": [
            "Yunja Choi",
            "Christian Bunse"
        ],
        "file_path": "data/sosym-all/s10270-010-0147-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Refactoring goal-oriented models: a linguistic improvement using large language models",
        "submission-date": "2024/03",
        "publication-date": "Not found",
        "abstract": "Goal-oriented requirements engineering (GORE) facilitates effective communication and collaboration between stakeholders. Using goal models, GORE provides a structured approach to elicit, analyze, and manage requirements from the perspective of stakeholders’ goals and intentions. However, goal models are prone to several poor practices, called bad smells, which can obstruct effective communication between stakeholders. As a result, there might be misinterpretations and inconsistencies in the requirements. Goal models are particularly prone to linguistic bad smells, encompassing unclear or ambiguous goal statements, conﬂicting or contradictory requirements, and occurrences of misspellings. It is therefore imperative that linguistic bad smells are identiﬁed and addressed in goal models to ensure their quality and accuracy. In this paper, we build upon our previous research by enhancing the catalog of 17 goal-oriented requirements language (GRL) linguistic bad smells. We reﬁne the detection techniques using a combination of NLP-based and LLM-based techniques. These enhancements signiﬁcantly improved the tool’s detection capabilities compared to our previous work. Furthermore, we offer automated refactoring solutions for 9 of these bad smells through GPT prompts. The remaining four identiﬁed bad smells are left to the user’s discretion for refactoring, due to their subjective nature. The detection and refactoring processes are implemented in a tool, tailored to the Textual GRL (TGRL) language. We evaluated the bad smells refactoring approach and tool by administering a questionnaire to 13 participants, who assessed the correctness of the refactoring of 71 linguistic bad smells found in four (4) TGRL models. Participants perceived the refactored sentences as highly correct across the different types of linguistic bad smells.",
        "keywords": [
            "Textual goal-oriented requirement language (TGRL)",
            "Linguistic bad smells",
            "NLP",
            "Refactoring",
            "GPT",
            "LLM"
        ],
        "authors": [
            "Nouf Alturayeif",
            "Jameleddine Hassine"
        ],
        "file_path": "data/sosym-all/s10270-024-01254-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Identifying and ﬁxing ambiguities in, and semantically accurate formalisation of, behavioural requirements",
        "submission-date": "2023/08",
        "publication-date": "2024/03",
        "abstract": "To correctly formalise requirements expressed in natural language, ambiguities must ﬁrst be identiﬁed and then ﬁxed. This paper focuses on behavioural requirements (i.e. requirements related to dynamic aspects and phenomena). Its ﬁrst objective is to show, based on a practical, public case study, that the disambiguation process cannot be fully automated: even though natural language processing (NLP) tools and machine learning might help in the identiﬁcation of ambiguities, ﬁxing them often requires a deep, application-speciﬁc understanding of the reasons of being of the system of interest, of the characteristics of its environment, of which trade-offs between conﬂicting objectives are acceptable, and of what is achievable and what is not; it may also require arduous negotiations between stakeholders. Such an understanding and consensus-making ability is not in the reach of current tools and technologies, and will likely remain so for a long while. Beyond ambiguity, requirements are often marred by various other types of defects that could lead to wholly unacceptable consequences. In particular, operational experience shows that requirements inadequacy (whereby, in some of the situations the system could face, what is required is woefully inappropriate or what is necessary is left unspeciﬁed) is a signiﬁcant cause for systems failing to meet expectations. The second objective of this paper is to propose a semantically accurate behavioural requirements formalisation format enabling tool-supported requirements veriﬁcation, notably with simulation. Such support is necessary for the engineering of large and complex cyber-physical and socio-technical systems to ensure, ﬁrst, that the speciﬁed requirements indeed reﬂect the true intentions of their authors and second, that they are adequate for all the situations the system could face. To that end, the paper presents an overview of the BASAALT (Behaviour Analysis and Simulation All Along systems Life Time) systems engineering method, and of FORM-L (FOrmal Requirements Modelling Language), its supporting language, which aims at representing as accurately and completely as possible the semantics expressed in the original, natural language behavioural requirements, and is markedly different from languages intended for software code generation. The paper shows that generally, semantically accurate formalisation is not a simple paraphrasing of the original natural language requirements: additional elements are often needed to fully and explicitly reﬂect all that is implied in natural language. To provide such complements for the case study presented in the paper, we had to follow different formalisation patterns, i.e. sequences of formalisation steps. For this paper, to avoid being skewed by what a particular automatic tool can and cannot do, BASAALT and FORM-L were applied manually. Still, the lessons learned could be used to specify and develop NLP tools that could assist the disambiguation and formalisation processes. However, more studies are needed to determine whether an exhaustive set of formalisation patterns can be identiﬁed to fully automate the formalisation process.",
        "keywords": [
            "Requirements",
            "Formalisation",
            "k3 case study",
            "Models",
            "Disambiguation"
        ],
        "authors": [
            "Thuy Nguyen",
            "Imen Sayar",
            "Sophie Ebersold",
            "Jean-Michel Bruel"
        ],
        "file_path": "data/sosym-all/s10270-023-01142-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Testing Web applications by modeling with FSMs",
        "submission-date": "2005/01",
        "publication-date": "2005/01",
        "abstract": "Researchers and practitioners are still trying\nto ﬁnd eﬀective ways to model and test Web applications.\nThis paper proposes a system-level testing technique that\ncombines test generation based on ﬁnite state machines\nwith constraints. We use a hierarchical approach to model\npotentially large Web applications. The approach builds\nhierarchies of Finite State Machines (FSMs) that model\nsubsystems of the Web applications, and then generates\ntest requirements as subsequences of states in the FSMs.\nThese subsequences are then combined and reﬁned to\nform complete executable tests. The constraints are used\nto select a reduced set of inputs with the goal of reduc-\ning the state space explosion otherwise inherent in using\nFSMs. The paper illustrates the technique with a running\nexample of a Web-based course student information sys-\ntem and introduces a prototype implementation to sup-\nport the technique.",
        "keywords": [
            "Testing of Web applications",
            "System testing",
            "Finite state machines"
        ],
        "authors": [
            "Anneliese A. Andrews",
            "JeﬀOﬀutt",
            "Roger T. Alexander"
        ],
        "file_path": "data/sosym-all/s10270-004-0077-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Petri nets for the control of discrete event systems",
        "submission-date": "2014/01",
        "publication-date": "2014/07",
        "abstract": "The interest in Petri nets has grown within the automatic control community in parallel with the development of the theory of discrete event systems. In this article, our goal is to give a ﬂavor of the features that make Petri nets a good model for discrete event systems and to point out the main areas where Petri nets have offered the most signiﬁcant contributions.",
        "keywords": [
            "Petri nets",
            "Discrete event systems",
            "Supervisory control",
            "Monitor places",
            "State estimation"
        ],
        "authors": [
            "Alessandro Giua",
            "Carla Seatzu"
        ],
        "file_path": "data/sosym-all/s10270-014-0425-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Detecting cross-case associations in an event log: toward a pattern-based detection",
        "submission-date": "2022/02",
        "publication-date": "2023/04",
        "abstract": "Business process management, design, and analysis is mostly centered around a process model, which depicts the behavior of a process case (instance). As a result, behavior that associates several cases together has received less attention. Yet, it is important to understand and track associations among cases, as they bear substantial consequences for compliance with regulations, root cause analysis of performance issues, exception handling, and prediction. This paper presents a framework of cross-case association patterns, categorized as intended association patterns and contextual association patterns. It further conceptualizes two example patterns—one for each category, and proposes techniques for detecting these patterns in an event log. The “split-case” workaround is an example of a pattern in the intended association category, and its proposed detection method exempliﬁes how patterns in this category can be approached. The patterns of a shared entity and a shared resource are contextual association patterns, which we propose to detect by means of hidden concept drifts. Evaluation of the two detection approaches is reported, using simulated logs for assessing their internal validity as well as real-life ones for exploring their external validity.",
        "keywords": [
            "Process mining",
            "Cross-case patterns",
            "Split-case workaround"
        ],
        "authors": [
            "Yael Dubinsky",
            "Pnina Soffer",
            "Irit Hadar"
        ],
        "file_path": "data/sosym-all/s10270-023-01100-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalizing and appling compliance patterns for business process compliance",
        "submission-date": "2013/05",
        "publication-date": "2014/02",
        "abstract": "Today’s enterprises demand a high degree of compliance of business processes to meet diverse regulations and legislations. Several industrial studies have shown that compliance management is a daunting task, and organizations are still struggling and spending billions of dollars annually to ensure and prove their compliance. In this paper, we introduce a comprehensive compliance management framework with a main focus on design-time compliance management as a ﬁrst step towards a preventive lifetime compliance support. The framework enables the automation of compliance-related activities that are amenable to automation, and therefore can signiﬁcantly reduce the expenditures spent on compliance. It can help experts to carry out their work more efﬁciently, cut the time spent on tedious manual activities, and reduce potential human errors. An evident candidate compliance activity for automation is the compliance checking, which can be achieved by utilizing formal reasoning and veriﬁcation techniques. However, formal languages are well known of their complexity as only versed users in mathematical theories and formal logics are able to use and understand them. However, this is generally not the case with businessandcompliancepractitioners.Therefore,intheheart of the compliance management framework, we introduce the Compliance Request Language (CRL), which is formally grounded on temporal logic and enables the abstract pattern-based speciﬁcation of compliance requirements. CRL constitutes a series of compliance patterns that spans three structural facets of business processes; control ﬂow, employed resources and temporal perspectives. Furthermore, CRL supports the speciﬁcation of compensations and non-monotonic requirements, which permit the relaxation of some compliance requirements to handle exceptional situations. An integrated tool suite has been developed as an instantiation artefact, and the validation of the approach is undertaken in several directions, which includes internal validity, controlled experiments, and functional testing.",
        "keywords": [
            "Business process compliance",
            "Compliance patterns",
            "Formal speciﬁcation",
            "Regulatory compliance",
            "Compliance management tool support",
            "Design-time compliance management"
        ],
        "authors": [
            "Amal Elgammal",
            "Oktay Turetken",
            "Willem-Jan van den Heuvel",
            "Mike Papazoglou"
        ],
        "file_path": "data/sosym-all/s10270-014-0395-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A metrics suite for UML model stability",
        "submission-date": "2016/06",
        "publication-date": "2016/12",
        "abstract": "Software metrics have become an essential part of software development because of their importance in estimating cost, effort, and time during the development phase. Manymetricshavebeenproposedtoassessdifferentsoftware quality attributes, including stability. A number of software stability metrics have been proposed at the class, architecture, and system levels. However, these metrics typically target the source code. This paper proposes a software stability met-rics suite at the model level for three UML diagrams: class, use case, and sequence. These three diagrams represent the most common diagrams in the three UML views: structural, functional, and behavioral. We introduce a client–master assessment approach to avoid measurement duplication. We also theoretically and empirically validate the proposed met-rics suite. We also provide examples to demonstrate the use of the proposed metrics and their application as indicators of software stability.",
        "keywords": [
            "Model stability",
            "Software metrics",
            "Metrics suite"
        ],
        "authors": [
            "Amjad AbuHassan",
            "Mohammad Alshayeb"
        ],
        "file_path": "data/sosym-all/s10270-016-0573-6.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A reconﬁguration pattern for distributed embedded systems",
        "submission-date": "2007/03",
        "publication-date": "2007/11",
        "abstract": "A reconﬁguration pattern for UML-based projects of embedded (real-time) systems is deﬁned. It enables to set up hardware/software conﬁgurations, and to specify conditions and methods for dynamic reconﬁguration. The reconﬁguration pattern was inspired by the reconﬁguration management solution of the Speciﬁcation PEARL methodology, which is based on the standard for Multiprocessor PEARL whose original idea it was to extend the language to enable the programming of distributed real-time applications in PEARL. In Speciﬁcation PEARL, the possibility for abstract descriptions of hardware and software architectures and for deﬁning mappings from software to hardware components has been enhanced in correspondence with the standard. Here, a UML pattern for reconﬁguration management in distributed embedded applications based on concepts from Speciﬁcation PEARL is presented. Its behavioural, structural and functional aspects are outlined. It addresses stereotype entities from the Speciﬁcation PEARL language, which were joined in a UML proﬁle, and outlines the related reconﬁguration management mechanisms, which were carried over to the mentioned UML pattern. The proposed reconﬁguration pattern is to facilitate the development of distributed embedded application in UML with consistent and temporally predictable reconﬁguration support. It should also support and enhance the applications’ ﬂexibility and portability.",
        "keywords": [
            "Real-time",
            "Distributed",
            "Embedded systems",
            "Dynamic reconﬁguration",
            "UML proﬁles and patterns",
            "UML-RT",
            "Speciﬁcation PEARL"
        ],
        "authors": [
            "Roman Gumzej",
            "Matjaž Colnariˇc",
            "Wolfgang A. Halang"
        ],
        "file_path": "data/sosym-all/s10270-007-0075-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using recommender systems to improve proactive modeling",
        "submission-date": "2019/11",
        "publication-date": "2021/01",
        "abstract": "This article investigates using recommender systems within graphical domain-speciﬁc modeling languages (DSMLs). The\nobjective of using recommender systems within a graphical DSML is to overcome a shortcoming of proactive modeling\nwhere the modeler must inform the model intelligence engine how to progress when it cannot automatically determine\nthe next modeling action to execute (e.g., add, delete, or edit). To evaluate our objective, we implemented a recommender\nsystem into the Proactive Modeling Engine, which is an add-on for the Generic Modeling Environment. We then conducted\nexperiments to subjectively and objectively evaluate enhancements to the Proactive Modeling Engine. The results of our\nexperiments show that extending proactive modeling with a recommender system results in an average reciprocal hit-rank of\n0.871. Likewise, the enhancements yield a System Usability Scale rating of 77. Finally, user feedback shows that integrating\nrecommender systems into DSMLs increases usability and learnability.",
        "keywords": [
            "Domain-speciﬁc modeling languages",
            "Proactive modeling",
            "Recommender systems"
        ],
        "authors": [
            "Arvind Nair",
            "Xia Ning",
            "James H. Hill"
        ],
        "file_path": "data/sosym-all/s10270-020-00841-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Blended modeling in commercial and open-source model-driven software engineering tools: A systematic study",
        "submission-date": "2021/09",
        "publication-date": "2022/06",
        "abstract": "Blended modeling aims to improve the user experience of modeling activities by prioritizing the seamless interaction with models through multiple notations over the consistency of the models. Inconsistency tolerance, thus, becomes an important aspect in such settings. To understand the potential of current commercial and open-source modeling tools to support blended modeling, we have designed and carried out a systematic study. We identify challenges and opportunities in the tooling aspect of blended modeling. Specifically, we investigate the user-facing and implementation-related characteristics of existing modeling tools that already support multiple types of notations and map their support for other blended aspects, such as inconsistency tolerance, and elevated user experience. For the sake of completeness, we have conducted a multivocal study, encompassing an academic review, and grey literature review. We have reviewed nearly 5000 academic papers and nearly 1500 entries of grey literature. We have identified 133 candidate tools, and eventually selected 26 of them to represent the current spectrum of modeling tools.",
        "keywords": [
            "Model-driven development",
            "Inconsistency tolerance",
            "Multi-view modeling",
            "Modeling tools",
            "Survey"
        ],
        "authors": [
            "Istvan David",
            "Malvina Latifaj",
            "Jakob Pietron",
            "Weixing Zhang",
            "Federico Ciccozzi",
            "Ivano Malavolta",
            "Alexander Raschke",
            "Jan-Philipp Steghöfer",
            "Regina Hebig"
        ],
        "file_path": "data/sosym-all/s10270-022-01010-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Quantifying Privacy Risk with Gaussian Mixtures",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "Data anonymization methods gain legal importance as data collection and analysis are expanding dramatically in data man-agement and statistical research. Yet applying anonymization, or understanding how well a given analytics program hides sensitive information, is non-trivial. Privug is a method to quantify privacy risks of data analytics programs by analyzing their source code. The method uses probability distributions to model attacker knowledge and Bayesian inference to update said knowledge based on observable outputs. Currently, Privug is equipped with approximate Bayesian inference methods (such as Markov Chain Monte Carlo), and an exact Bayesian inference method based on multivariate Gaussian distributions. This paper introduces a privacy risk analysis engine based on Gaussian mixture models that combines exact and approximate inference. It extends the multivariate Gaussian engine by supporting exact inference in programs with continuous and discrete distributions as well as if-statements. Furthermore, the engine allows for approximating attacker knowledge that is not normally distributed. We evaluate the method by analyzing privacy risks in programs to release public statistics, differential privacy mechanisms, randomized response and attribute generalization. Finally, we show that our engine can be used to analyze programs involving thousands of sensitive records.",
        "keywords": [
            "Privacy risk analysis",
            "Bayesian inference",
            "Probabilistic programming",
            "Data analytics programs"
        ],
        "authors": [
            "Rasmus C. Rønneberg",
            "Francesca Randone",
            "Raúl Pardo",
            "Andrzej Wa˛sowski"
        ],
        "file_path": "data/sosym-all/s10270-025-01298-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A method for describing the syntax and semantics of UML statecharts",
        "submission-date": "2003/01",
        "publication-date": "2004/04",
        "abstract": "In this article we present a method for de-scribing the language of UML statecharts. Statecharts are syntactically deﬁned as attributed graphs, with well-formedness rules speciﬁed by a set of ﬁrst-order predicates over the abstract syntax of the graphs. The dynamic semantics of statecharts is deﬁned by Abstract State Machines parameterized with syntactically-correct attributed graphs. The presented approach covers many important constructs of UML statecharts, including internal, completion, interlevel and compound transitions as well as history pseudostates. It also contains strategies to handle state entry/exit actions, state activities, synch states and choice pseudostates.",
        "keywords": [
            "Visual languages",
            "UML",
            "Statecharts",
            "UML statecharts",
            "Syntax deﬁnition",
            "Formal operational semantics",
            "Abstract State Machines"
        ],
        "authors": [
            "Yan Jin",
            "Robert Esser",
            "J¨orn W. Janneck"
        ],
        "file_path": "data/sosym-all/s10270-003-0046-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "3LConOnt: a three-level ontology for context modelling in context-aware computing",
        "submission-date": "2016/09",
        "publication-date": "2017/08",
        "abstract": "Context-aware computing is the ability of Services and applications to adapt and react to context changes. Context modelling is a core feature of context-aware computing. Although a lot of research has been made in the ﬁeld of context modelling, most of the context-aware computing proposals prefer to design their own customized context model instead of reusing an existing one. The main reason for this behaviour is that current context models present some problems concerning reusability, extensibility and adaptation. To contribute solving these issues, in this paper we present 3LConOnt, a three-level context ontology that can be easily reused, extended and adapted for speciﬁc or generic purposes. The proposed context model consolidates the context knowledge already available from a modular perspective yielding a clear schema of knowledge reutilization. To do so, we gathered context knowledge pieces from different ontologies to be integrated into standardized and well-deﬁned levels of abstraction and modules. The proposal has been validated considering: (1) reusability, extensibility and adaptation by instantiating different smart scenarios; (2) consistency and reasoning by triggering queries to the proposed model based on some competence questions; and (3) reusability in existing ontologies by importing the needed module or level of the model. Additionally, we also illustrate its usability in context-aware Services by modelling a context-aware framework architecture for supporting the whole context life cycle: acquisition, modelling, reasoning and distribution.",
        "keywords": [
            "Context-aware computing",
            "Service-oriented computing",
            "Context life cycle",
            "Context modelling",
            "Context reasoning",
            "Context ontology"
        ],
        "authors": [
            "Oscar Cabrera",
            "Xavier Franch",
            "Jordi Marco"
        ],
        "file_path": "data/sosym-all/s10270-017-0611-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From software extensions to product lines of dataflow programs",
        "submission-date": "2014/11",
        "publication-date": "2015/09",
        "abstract": "Dataflow programs are widely used. Each program is a directed graph where nodes are computations and edges indicate the flow of data. In prior work, we reverse-engineered legacy dataflow programs by deriving their optimized implementations from a simple specification graph using graph transformations called refinements and optimizations. In MDE speak, our derivations were PIM-to-PSM mappings. In this paper, we show how extensions complement refinements, optimizations, and PIM-to-PSM derivations to make the process of reverse engineering complex legacy dataflow programs tractable. We explain how optional functionality in transformations can be encoded, thereby enabling us to encode product lines of transformations as well as product lines of dataflow programs. We describe the implementation of extensions in the ReFlO tool and present two non-trivial case studies as evidence of our work’s generality.",
        "keywords": [
            "MDE",
            "PIM",
            "PSM",
            "Model transformations",
            "Software extensions",
            "Dataflow programs",
            "Software product lines"
        ],
        "authors": [
            "Rui C. Gonçalves",
            "Don Batory",
            "João L. Sobral",
            "Taylor L. Riché"
        ],
        "file_path": "data/sosym-all/s10270-015-0495-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A powertype-based metamodelling framework",
        "submission-date": "2004/11",
        "publication-date": "2005/11",
        "abstract": "Software development methodologies may be de-scribed in the context of an underpinning metamodel, but theprecise mechanisms that permit them to be deﬁned in terms oftheir metamodels are usually difﬁcult to explain and do notcover all needs. For example, it is difﬁcult to devise a waythat allows the deﬁnition of properties of the elements thatcompose the methodology and, at the same time, of the entities(such as work products) created when the method-ology is applied. This article introduces a new approach toconstructing metamodels and deriving methodologies fromthem based on the concept of powertype. It combines keyadvantages of other metamodelling approaches and allows theseamless integration of process, modelling and docu-mentational aspects of methodologies. With this approach,both methodology components and project entities can bedescribed directly by the same metamodel.",
        "keywords": [
            "Metamodelling",
            "Powertype",
            "Software development methodologies"
        ],
        "authors": [
            "Cesar Gonzalez-Perez",
            "Brian Henderson-Sellers"
        ],
        "file_path": "data/sosym-all/s10270-005-0099-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Qualifying input test data for model transformations",
        "submission-date": "2007/05",
        "publication-date": "2007/11",
        "abstract": "Model transformation is a core mechanism for model-driven engineering (MDE). Writing complex model transformations is error-prone, and efﬁcient testing techniques are required as for any complex program development. Testing a model transformation is typically performed by checking the results of the transformation applied to a set of input models. While it is fairly easy to provide some input models, it is difﬁcult to qualify the relevance of these models for testing. In this paper, we propose a set of rules and a framework to assess the quality of given input models for testing a given transformation. Furthermore, the framework identiﬁes missing model elements in input models and assists the user in improving these models.",
        "keywords": [
            "Software testing",
            "Model transformation",
            "Test criteria",
            "Test qualiﬁcation",
            "Metamodelling",
            "Model-based testing"
        ],
        "authors": [
            "Franck Fleurey",
            "Benoit Baudry",
            "Pierre-Alain Muller",
            "Yves Le Traon"
        ],
        "file_path": "data/sosym-all/s10270-007-0074-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Goal-oriented modeling and veriﬁcation of feature-oriented product lines",
        "submission-date": "2012/08",
        "publication-date": "2014/02",
        "abstract": "Goal models represent requirements and intentions of a software system. They play an important role in the development life cycle of software product lines (SPLs). In the domain engineering phase, goal models guide the development of variability in SPLs by providing the rationale for the variability, while they are used for the conﬁguration of SPLs in the application engineering phase. However, variability in SPLs, which is represented by feature models, usually has design and implementation-induced constraints. When those constraints are not aligned with variability in goal models, the conﬁguration with goal models becomes error prone. To remedy this problem, we propose a description logic (DL)-based approach to represent both models and their relations in a common DL knowledge base. Moreover, we apply reasoning to detect inconsistencies in the variability of goal and feature models. A formal proof is provided to demonstrate the correctness of the reasoning approach. An empirical evaluation shows computational tractability of the inconsistency detection.",
        "keywords": [
            "Software engineering",
            "Feature oriented software families",
            "Goal-oriented requirements engineering",
            "Description Logic",
            "Feature Models",
            "Veriﬁcation"
        ],
        "authors": [
            "Mohsen Asadi",
            "Gerd Gröner",
            "Bardia Mohabbati",
            "Dragan Gaševi´c"
        ],
        "file_path": "data/sosym-all/s10270-014-0402-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSyM at 7 years",
        "submission-date": "2008/11",
        "publication-date": "2008/11",
        "abstract": "Over its 7 years in circulation, SoSyM, the International Journal on Software and Systems Modeling, has received a reputation for publishing quality research and experience papers on building and using models in the development of software-based systems. During that time, we have witnessed a growing body of experience related to the use of modeling techniques in practice. The insights gained have led to the development of better solutions, but, more importantly, they have led to (1) a deeper understanding of some of the more critical problems that practitioners face when applying current modeling technologies, and (2) a broader vision of model-driven engineering (MDE) that encompasses not only development, but also software operation. Papers published in SoSyM, and in a number of related conferences and workshops that have contributed to special SoSyM issues, have made significant contributions to this growing body of experience and research on modeling software-based systems.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-008-0107-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Development of logical and technical architectures for automotive systems",
        "submission-date": "2005/02",
        "publication-date": "2006/07",
        "abstract": "This paper presents a modeling approach for the development of software for electronic control units in the automotive domain. The approach supports the development of two related architecture models in the overall development process: the logical architecture provides a graphical, quite abstract representation of a typically large set of automotive functions. On this abstraction level no design decisions are taken. The technical architecture provides a software and a hardware representation in separated views: the software architecture describes the software realization of functions as software components, whereas the hardware architecture models hardware entities, on which the software components are deployed. Logical as well as technical architectures only model structural information, but no behavioural information. A tight integration of both architecture levels — on the conceptual and on the tool level — with related development phases such as requirements engineering, behaviour modeling, code generation as well as version and conﬁguration management resulting in a seamless overall development process is presented. This architecture modeling approach has been developed within a safety-relevant project at BMW Group. Positive as well as negative experiences with the application of this approach are described.",
        "keywords": [],
        "authors": [
            "Michael von der Beeck"
        ],
        "file_path": "data/sosym-all/s10270-006-0022-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Analysing the cognitive effectiveness of the WebML visual notation",
        "submission-date": "2013/07",
        "publication-date": "2015/01",
        "abstract": "WebML is a domain-speciﬁc language used to design complex data-intensive Web applications at a conceptual level. As WebML was devised to support design tasks, the need to deﬁne a visual notation for the language was identiﬁed from the very beginning. Each WebML element is consequently associated with a separate graphical symbol which was mainly deﬁned with the idea of providing simple and expressive modelling artefacts rather than by adopting a rigorous scientiﬁc approach. As a result, the graphical models deﬁned with WebML may sometimes prevent proper communication from taking place between the various stakeholders. In fact, this is a common issue for most of the existing model-based proposals that have emerged during the last few years under the umbrella of model-driven engineering. In order to illustrate this issue and foster in using a scientiﬁc basis to design, evaluate, improve and compare visual notations, this paper analyses WebML according to a set of solid principles, based on the theoretical and empirical evidence concerning the cognitive effectiveness of visual notations. As a result, we have identiﬁed a set of possible improvements, some of which have been veriﬁed by an empirical study. Furthermore, a number of ﬁndings, experiences and lessons learnt on the assessment of visual notations are presented.",
        "keywords": [
            "Web modelling language (WebML)",
            "Visual notation",
            "Cognitive effectiveness",
            "Visual communication",
            "Visual syntax",
            "Concrete syntax"
        ],
        "authors": [
            "David Granada",
            "Juan Manuel Vara",
            "Marco Brambilla",
            "Verónica Bollati",
            "Esperanza Marcos"
        ],
        "file_path": "data/sosym-all/s10270-014-0447-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSyM significantly reduces its backlog",
        "submission-date": "2019/02",
        "publication-date": "2019/02",
        "abstract": "SoSyM has been growing in two important areas: an increase in annual impact factor (IF) and number of submissions. The large uptick in the number of submissions is a very healthy sign for a journal, especially when coupled with a corresponding increase in IF. However, an increase in submissions also has an interesting side effect that can hamper the ability to process accepted articles in a timely manner. Editors and publishers are challenged with estimating the future set of incoming submitted papers. Also, the perception of a journal decreases when the number of papers published per issue is observed as decreasing or near empty. Publishers must carefully balance the number of issues released per year and the overall number of pages printed to match the projected incoming submissions.",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-019-00726-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "XTraQue: traceability for product line systems",
        "submission-date": "2006/09",
        "publication-date": "2007/09",
        "abstract": "Product line engineering has been increasingly\nused to support the development and deployment of soft-\nware systems that share a common set of features and are\ndeveloped based on the reuse of core assets. The large num-\nber and heterogeneity of documents generated during the\ndevelopment of product line systems may cause difﬁculties\nto identify common and variable aspects among applications,\nand to reuse core assets that are available under the product\nline. In this paper, we present a traceability approach for\nproduct line systems. Traceability has been recognised as an\nimportant task in software system development. Traceabil-\nity relations can improve the quality of the product being\ndeveloped and reduce development time and cost. We pres-\nent a rule-based approach to support automatic generation of\ntraceability relations between feature-based object-oriented\ndocuments. We deﬁne a traceability reference model with\nnine different types of traceability relations for eight types of\ndocuments. The traceability rules used in our work are clas-\nsiﬁed into two groups namely (a) direct rules, which support\nthe creation of traceability relations that do not depend on\nthe existence of other relations, and (b) indirect rules, which\nrequire the existence of previously generated relations. The\ndocuments are represented in XML and the rules are repre-\nsented in an extension of XQuery. A prototype tool called\nXTraQue has been implemented. This tool, together with a\nmobile phone product line case study, has been used to dem-\nonstrate and evaluate our work in various experiments. The\nresults of these experiments are encouraging and comparable\nwith other approaches that support automatic generation of\ntraceability relations.",
        "keywords": [
            "Software traceability",
            "Product line",
            "Traceability relations",
            "Traceability rules",
            "Feature-based\nobject-oriented documents"
        ],
        "authors": [
            "Waraporn Jirapanthong",
            "Andrea Zisman"
        ],
        "file_path": "data/sosym-all/s10270-007-0066-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Metamodeling live sequence charts for code generation",
        "submission-date": "2007/11",
        "publication-date": "2009/02",
        "abstract": "This article presents a metamodeling study for Live Sequence Charts (LSCs) and Message Sequence Charts (MSCs) with an emphasis on code generation. The article discusses specifically the following points: the approach to building a metamodel for MSCs and LSCs, a metamodel extension from MSC to LSC, support for model-based code generation,andﬁnallyactionmodelanddomain-speciﬁcdata model integration. The metamodel is formulated in meta-GME, the metamodel language for the Generic Modeling Environment.",
        "keywords": [
            "Metamodeling",
            "Code generation",
            "Message sequence charts",
            "Live sequence charts"
        ],
        "authors": [
            "Okan Topçu",
            "Mehmet Adak",
            "Halit O˘guztüzün"
        ],
        "file_path": "data/sosym-all/s10270-009-0113-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward an execution system for self-healing workﬂows in cyber-physical systems",
        "submission-date": "2015/10",
        "publication-date": "2016/08",
        "abstract": "Cyber-physical systems (CPS) represent a new class of information system that also takes real-world data and effects into account. Software-controlled sensors, actuators and smart objects enable a close coupling of the cyber and physical worlds. Introducing processes into CPS to automate repetitive tasks promises advantages regarding resource utilization and ﬂexibility of control systems for smart spaces. However, process execution systems face new challenges when being adapted for process execution in CPS: the automated processing of sensor events and data, the dynamic invocation of services, the integration of human interaction, and the synchronization of the cyber and physical worlds. Current workﬂow engines fulﬁll these requirements only to a certain degree. In this work, we present PROtEUS—an integrated system for process execution in CPS. PROtEUS integrates components for event processing, data routing, dynamic service selection and human interaction on the modeling and execution level. It is the basis for executing self-healing model-based workﬂowsin CPS. We demonstrate the applicability of PROtEUS within two case studies from the Smart Home domain and discuss its feasibility for introducing workﬂows into cyber-physical systems.",
        "keywords": [
            "Process execution",
            "Cyber-physical systems",
            "Workﬂow system",
            "Internet of things",
            "System architecture",
            "Middleware",
            "Event processing"
        ],
        "authors": [
            "Ronny Seiger",
            "Steffen Huber",
            "Thomas Schlegel"
        ],
        "file_path": "data/sosym-all/s10270-016-0551-z.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Requirements speciﬁcation using templates: a model-driven approach",
        "submission-date": "2024/05",
        "publication-date": "2025/01",
        "abstract": "Requirements speciﬁcation and veriﬁcation are crucial processes of software development. These processes are particularly costly for safety critical systems due to the high number of requirements and their complexity. For such systems, it is important to use natural language for the speciﬁcation, as requirements need to be readable by non-technical stakeholders and certiﬁcation agents. To mitigate the inherent ambiguity caused by the use of natural language, controlled natural languages (CNL) are introduced as a means to constrain the speciﬁcation while maintaining readability. In this paper, we leverage model-driven engineering (MDE) to propose RESPECT, REquirements SPECiﬁcation using Templates, a CNL-based approach for requirements speciﬁcation and veriﬁcation. The fundamental idea of RESPECT is to use MDE techniques to: 1) model requirements’ templates and thus ease their creation, implementation and evolution and 2) link the template models to existing domain models to support, to some extent, requirements veriﬁcation and auto-ﬁlling. We provide a systematic process for the creation of customizable and reusable templates, which, to the best of our knowledge, represents a novel contribution. The application of this systematic process to a subset of the ARINC-653 standard from the avionics domain, resulted in seven templates that cover various types of requirements. We developed a tool, called MD-RSuT, that supports the speciﬁcation of requirements using the seven templates created for ARINC-653, and the automated veriﬁcation and auto-ﬁlling of requirements using an ARINC domain model. We evaluated the applicability of the approach across domains, and its effectiveness in improving requirements quality in terms of necessity, unambiguity, completeness, singularity, and veriﬁability. To do so, we applied the approach on three case studies coming from different domains, namely avionics, automotive, and general purpose software. This evaluation encompasses over a thousand requirements. We also performed a quantitative and qualitative analysis of the results. The results show that RESPECT is applicable across domains, and it yields requirements with higher quality.",
        "keywords": [
            "Model-driven engineering",
            "Requirements speciﬁcation",
            "Requirements veriﬁcation",
            "Controlled natural language",
            "Requirement templates",
            "Domain models",
            "Safety critical systems"
        ],
        "authors": [
            "Ikram Darif",
            "Ghizlane El Boussaidi",
            "Sègla Kpodjedo"
        ],
        "file_path": "data/sosym-all/s10270-025-01265-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Partitioning of perfect synchroneous reactive speciﬁcations to distributed processors using µ-Charts",
        "submission-date": "2004/10",
        "publication-date": "2005/11",
        "abstract": "In this contribution, it is shown that perfect syn-chroneous speciﬁcations can be partitioned to and imple-mented on a distributed processor network. To this end, we introduce a lean visual formalism, called µ-Charts, that is similar to the speciﬁcation language Statecharts. This for-malism consists of fewer syntactic constructs than State-charts. Further syntax like hierarchical decomposition can be derived by means of syntactic abbreviation. µ-Charts’ semantics is based on the assumption of perfect synchrony. This paper is one of several contributions in this context; it gives a formal background and concentrates on the ques-tion how to use perfect synchroneous, state-based descrip-tion techniques as a basis for distributed implementations. The main contribution presented in this article is that the (formal and compositional) semantics of a perfect synchro-neous speciﬁcation is preserved when it is partitioned and implemented on distributed processors. We prove a theorem which guarantees that the communication ﬂow between dis-tributed parts of a perfect synchroneous speciﬁcation stabi-lizes in a ﬁxed point, i.e. terminates, independently of the processor speeds.",
        "keywords": [
            "Reactive systems",
            "Distributed systems",
            "Statecharts",
            "µ-charts",
            "Perfect synchrony",
            "Partitioning",
            "Fixed point semantics"
        ],
        "authors": [
            "Peter Scholz"
        ],
        "file_path": "data/sosym-all/s10270-005-0094-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic generation of UML proﬁle graphical editors for Papyrus",
        "submission-date": "2019/03",
        "publication-date": "2020/08",
        "abstract": "UML proﬁles offer an intuitive way for developers to build domain-speciﬁc modelling languages by reusing and extending\nUML concepts. Eclipse Papyrus is a powerful open-source UML modelling tool which supports UML proﬁling. However, with\npower comes complexity, implementing non-trivial UML proﬁles and their supporting editors in Papyrus typically requires\nthe developers to handcraft and maintain a number of interconnected models through a loosely guided, labour-intensive and\nerror-prone process. We demonstrate how metamodel annotations and model transformation techniques can help manage\nthe complexity of Papyrus in the creation of UML proﬁles and their supporting editors. We present Jorvik, an open-source\ntool that implements the proposed approach. We illustrate its functionality with examples, and we evaluate our approach by\ncomparing it against manual UML proﬁle speciﬁcation and editor implementation using a non-trivial enterprise modelling\nlanguage (Archimate) as a case study. We also perform a user study in which developers are asked to produce identical editors\nusing both Papyrus and Jorvik demonstrating the substantial productivity and maintainability beneﬁts that Jorvik delivers.",
        "keywords": [
            "Model-driven engineering",
            "UML proﬁling",
            "Papyrus"
        ],
        "authors": [
            "Ran Wei",
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Simos Gerasimou",
            "Dimitrios S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-020-00813-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling guidance in software engineering: a systematic literature review",
        "submission-date": "2022/06",
        "publication-date": "2023/07",
        "abstract": "Despite potential beneﬁts in Software Engineering, adoption of software modelling in industry is low. Technical issues such as tool support have gained signiﬁcant research before, but individual guidance and training have received little attention. As a ﬁrst step towards providing the necessary guidance in modelling, we conduct a systematic literature review to explore the current state of the art. We searched academic literature for guidance on model creation and selected 35 papers for full-text screening through three rounds of selection. We ﬁnd research on model creation guidance to be fragmented, with inconsistent usage of terminology, and a lack of empirical validation or supporting evidence. We outline the different dimensions commonly used to provide guidance on software and system model creation. Additionally, we provide deﬁnitions of the three terms modelling method, style, and guideline as current literature lacks a well-deﬁned distinction between them. These deﬁnitions can help distinguishing between important concepts and provide precise modelling guidance.",
        "keywords": [
            "Modelling styles",
            "Modelling training",
            "Modelling guidance",
            "Modelling method",
            "Systematic literature review"
        ],
        "authors": [
            "Shalini Chakraborty",
            "Grischa Liebel"
        ],
        "file_path": "data/sosym-all/s10270-023-01117-1.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Understanding the need for assistance in software modeling: interviews with experts",
        "submission-date": "2022/03",
        "publication-date": "2023/05",
        "abstract": "Software modeling has shown for many years that it brings many advantages at the cost of various efforts and constraints. A large corpus of literature has indeed grown up over the years, pointing out the problems related to the modeling abstraction process, the usability of tools, or the practical difﬁculty of using modeling languages. While these works identify problems, few of them focus on proposing directions to explore in order to ﬁx them. To move toward a smoother and less constraining modeling experience and then increase the added value of modeling approaches, it is necessary to identify new paths to improve current tooling. In this paper, we explore one speciﬁc path by investigating how new software assistance features could support users performing modeling tasks that they perceive as complex. We used UML knowledge as a criterion for the selection of participants and built a questionnaire general to software modeling. We followed a user-centered research approach and collected the feedback from practitioners who use the modeling languages and the modeling tools on a regular basis in an industrial context. This article reports on a set of individual interview sessions with 16 modeling experts about how they perform modeling and how they imagine assistance in the context of their work. From the analysis of this qualitative study, we draw twelve observations on how to design software assistants for software modeling. These observations highlight research directions for both tool vendors and academics to explore, to identify and design new solutions to the friction points of the software modeling experience.",
        "keywords": [
            "Software modeling",
            "Practitioners",
            "Software assistant",
            "Interviews"
        ],
        "authors": [
            "Maxime Savary-Leblanc",
            "Xavier Le Pallec",
            "Sébastien Gérard"
        ],
        "file_path": "data/sosym-all/s10270-023-01104-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Certifying delta-oriented programs",
        "submission-date": "2017/04",
        "publication-date": "2018/12",
        "abstract": "A major design concern in modern software development frameworks is to ensure that mechanisms for updating code running on remote devices comply with given safety speciﬁcations. This paper presents a delta-oriented approach for implementing product lines where software reuse is achieved at the three levels of state-diagram modeling, C/C++ source code and binary code. A safety speciﬁcation is expressed on the properties of reusable software libraries that can be dynamically loaded at run time after an over-the-air update. The compilation of delta-engineered code is certiﬁed using the framework of proof-carrying code in order to guarantee safety of software updates on remote devices. An empirical evaluation of the computational cost associated with formal safety checks is done by means of experimentation.",
        "keywords": [
            "Model-driven development",
            "Delta-oriented programming",
            "Safety properties",
            "Proof-carrying code",
            "Runtime systems"
        ],
        "authors": [
            "Vítor Rodrigues",
            "Simone Donetti",
            "Ferruccio Damiani"
        ],
        "file_path": "data/sosym-all/s10270-018-00704-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic generation of atomic multiplicity-preserving search operators for search-based model engineering",
        "submission-date": "2020/05",
        "publication-date": "2021/08",
        "abstract": "Recently, there has been increased interest in combining model-driven engineering and search-based software engineering. Such approaches use meta-heuristic search guided by search operators (model mutators and sometimes breeders) implemented as model transformations. The design of these operators can substantially impact the effectiveness and efﬁciency of the meta-heuristic search. Currently, designing search operators is left to the person specifying the optimisation problem. However, developing consistent and efﬁcient search-operator rules requires not only domain expertise but also in-depth knowledge about optimisation, which makes the use of model-based meta-heuristic search challenging and expensive. In this paper, we propose a generalised approach to automatically generate atomic multiplicity-preserving search operators for a given optimisation problem. This reduces the effort required to specify an optimisation problem and shields optimisation users from the complexity of implementing efﬁcient meta-heuristic search mutation operators. We evaluate our approach with a set of case studies and show that the automatically generated rules are comparable to, and in some cases better than, manually created rules at guiding evolutionary search towards near-optimal solutions.",
        "keywords": [
            "Model-driven optimisation",
            "Search-based software engineering",
            "Multi-objective optimisation"
        ],
        "authors": [
            "Alexandru Burdusel",
            "Steffen Zschaler",
            "Stefan John"
        ],
        "file_path": "data/sosym-all/s10270-021-00914-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "What makes a good process model? Lessons learned from process mining",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "There seems to be a never ending stream of\nnew process modeling notations. Some of these notations\nare foundational and have been around for decades (e.g.,\nPetri nets). Other notations are vendor speciﬁc, incremen-\ntal, or are only popular for a short while. Discussions on\nthe various competing notations concealed the more impor-\ntant question “What makes a good process model?”. Fortu-\nnately, large scale experiences with process mining allow us\nto address this question. Process mining techniques can be\nused to extract knowledge from event data, discover models,\nalign logs and models, measure conformance, diagnose bot-\ntlenecks, and predict future events. Today’s processes leave\nmany trails in data bases, audit trails, message logs, transac-\ntion logs, etc. Therefore, it makes sense to relate these event\ndata to process models independent of their particular nota-\ntion. Process models discovered based on the actual behavior\ntend to be very different from the process models made by\nhumans. Moreover, conformance checking techniques often\nreveal important deviations between models and reality. The\nlessons that can be learned from process mining shed a new\nlight on process model quality. This paper discusses the role\nof process models and lists seven problems related to process\nmodeling. Based on our experiences in over 100 process min-\ning projects, we discuss these problems. Moreover, we show\nthat these problems can be addressed by exposing process\nmodels and modelers to event data.",
        "keywords": [
            "Process mining",
            "Process modeling",
            "Process model quality"
        ],
        "authors": [
            "W. M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-012-0265-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Computation orchestration",
        "submission-date": "2005/02",
        "publication-date": "2006/05",
        "abstract": "The widespread deployment of networked\napplications and adoption of the internet has fostered\n an environment in which many distributed services are\n available. There is great demand to automate business\n processes and workﬂows among organizations and indi-\n viduals. Solutions to such problems require orchestra-\n tion of concurrent and distributed services in the face of\n arbitrary delays and failures of components and com-\n munication. We propose a novel approach, called Orc\n for orchestration, that supports a structured model of\n concurrent and distributed programming. This model\n assumes that basic services, like sequential computa-\n tion and data manipulation, are implemented by prim-\n itive sites. Orc provides constructs to orchestrate the\n concurrent invocation of sites to achieve a goal – while\n managing time-outs, priorities, and failure of sites or\n communication.",
        "keywords": [
            "Wide-area computing",
            "Web services",
            "Computation orchestration",
            "Distributed computing",
            "Process algebra",
            "Thread-based programming"
        ],
        "authors": [
            "Jayadev Misra",
            "William R. Cook"
        ],
        "file_path": "data/sosym-all/s10270-006-0012-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal speciﬁcation of non-functional properties of component-based software systems",
        "submission-date": "2007/09",
        "publication-date": "2009/02",
        "abstract": "Component-based software engineering (CBSE) is viewed as an opportunity to deal with the increasing complexity of modern-day software. Along with CBSE comes the notion of component markets, where more or less generic pieces of software are traded, to be combined into applications by third-party application developers. For such a component market to work successfully, all relevant properties of components must be precisely and formally described. This is especially true for non-functional properties, such as performance, memory foot print, or security. While the speciﬁcation offunctionalpropertiesiswellunderstood,non-functional properties areonlybeginningtobecomearesearch focus. This paper discusses semantic concepts for the speciﬁcation of non-functional properties, taking into account the speciﬁc needs of a component market. Based on these semantic concepts, we present a new speciﬁcation language QML/CS that can be used to model non-functional prod-uct properties of components and component-based software systems.",
        "keywords": [
            "Non-functional properties",
            "Formal speciﬁcation",
            "Component-based software engineering",
            "QML/CS"
        ],
        "authors": [
            "Steffen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-009-0115-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Inferring physical units in formal models",
        "submission-date": "2014/02",
        "publication-date": "2015/03",
        "abstract": "Most state-based formal methods, like B, Event-B or Z, provide support for static typing. However, these methods and the associated tools lack support for annotating variables with (physical) units of measurement. There is thus no obvious way to reason about correct or incorrect usage of such units. We present a technique that analyzes the usage of physical units throughout B and Event-B machines infers missing units and notiﬁes the user of incorrectly handled units. The technique combines abstract interpretation with classical animation, constraint solving and model checking andhasbeenintegratedintotheProBvalidationtool,bothfor classicalBandforEvent-B.Itprovidessource-levelfeedback about errors detected in the models. We also describe how to extend our approach to TLA+, an untyped formal language. Weprovideanin-depthempirical evaluationanddemonstrate that our technique scales up to real-life industrial models.",
        "keywords": [
            "B-method",
            "Event-B",
            "Physical units",
            "Model checking",
            "Abstract interpretation"
        ],
        "authors": [
            "Sebastian Krings",
            "Michael Leuschel"
        ],
        "file_path": "data/sosym-all/s10270-015-0458-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Template-based model generation",
        "submission-date": "2016/03",
        "publication-date": "2017/11",
        "abstract": "Given their vital roles in model-based software engineering, the performance of model-related operations (MOs, such as model transformations) must be systematically tested. However, how to produce a set of large input models that conform to structure-related constraints presents a major challenge to such test. This paper proposes a template-based approach to efﬁcient model generation. Firstly, a DSL is provided to describe templates that specify how to generate a valid model that conforms to structure-related constraints. Secondly, a folding semantic is deﬁned to converttemplatesintoawrappermetamodel.Thirdly,awrap-per model is generated using the existing model generators (e.g., a random model generator) according to the wrapper metamodel. Fourthly, an unfolding semantics is speciﬁed to translate the wrapper model into the desired test input. This paper also presents ﬁve case studies to evaluate the proposed approach, and the results demonstrate that such approach can generate large models based on structure-related constraints and facilitate the performance testing of MOs.",
        "keywords": [
            "Model generation",
            "Templates",
            "Performance testing",
            "Model-oriented operations",
            "Model-based engineering"
        ],
        "authors": [
            "Xiao He",
            "Tian Zhang",
            "Minxue Pan",
            "Zhiyi Ma",
            "Chang-Jun Hu"
        ],
        "file_path": "data/sosym-all/s10270-017-0634-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the modeling and generation of service-oriented tool chains",
        "submission-date": "2011/10",
        "publication-date": "2012/09",
        "abstract": "Tool chains have grown from ad-hoc solutions to complex software systems, which often have a service-oriented architecture. With service-oriented tool integration, development tools are made available as services, which can be orchestrated to form tool chains. Due to the increasing sophistication and size of tool chains, there is a need for a systematic development approach for service-oriented tool chains. We propose a domain-speciﬁc modeling language (DSML) that allows us to describe the tool chain on an appropriate level of abstraction. We present how this language supports three activities when developing service-oriented tool chains: communication, design and realization. A generative approach supports the realization of the tool chain using the service component architecture. We present experiences from an industrial case study, which applies the DSML to support the creation of a service-oriented tool chain. We evaluate the approach both qualitatively and quantitatively by comparing it with a traditional development approach.",
        "keywords": [
            "Domain speciﬁc modeling language",
            "Generative approach",
            "Service-oriented architecture",
            "Tool integration"
        ],
        "authors": [
            "Matthias Biehl",
            "Jad El-Khoury",
            "Frédéric Loiret",
            "Martin Törngren"
        ],
        "file_path": "data/sosym-all/s10270-012-0275-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Addressing the evolution of automated user behaviour patterns by runtime model interpretation",
        "submission-date": "2012/04",
        "publication-date": "2013/09",
        "abstract": "The use of high-level abstraction models can facilitate and improve not only system development but also runtime system evolution. This is the idea of this work, in which behavioural models created at design time are also used at runtime to evolve system behaviour. These behavioural models describe the routine tasks that users want to be automated by the system. However, users’ needs may change after system deployment, and the routine tasks automated by the system must evolve to adapt to these changes. To facilitate this evolution, the automation of the speciﬁed routine tasks is achieved by directly interpreting the models at runtime. This turns models into the primary means to understand and interact with the system behaviour associated with the routine tasks as well as to execute and modify it. Thus, we provide tools to allow the adaptation of this behaviour by modifying the models at runtime. This means that the system behaviour evolution is performed by using high-level abstractions and avoiding the costs and risks associated with shutting down and restarting the system.",
        "keywords": [
            "System behaviour evolution",
            "Routine task automation",
            "Models at runtime",
            "Runtime interpretation of models"
        ],
        "authors": [
            "Estefanía Serral",
            "Pedro Valderas",
            "Vicente Pelechano"
        ],
        "file_path": "data/sosym-all/s10270-013-0371-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Non-functional properties in the model-driven development of service-oriented systems",
        "submission-date": "2009/01",
        "publication-date": "2010/03",
        "abstract": "Systems based on the service-oriented architecture (SOA) principles have become an important cornerstone of the development of enterprise-scale software applications. They are characterized by separating functions into distinct software units, called services, which can be published, requested and dynamically combined in the production of business applications. Service-oriented systems (SOSs) promise high ﬂexibility, improved maintainability, and simple re-use of functionality. Achieving these properties requires an understanding not only of the individual arti-facts of the system but also their integration. In this context, non-functional aspects play an important role and should be analyzed and modeled as early as possible in the development cycle. In this paper, we discuss modeling of non-functional aspects of service-oriented systems, and the use of these models for analysis and deployment. Our contribution in this paper is threefold. First, we show how services and service compositions may be modeled in UML by using a proﬁle for SOA (UML4SOA) and how non-functional properties of service-oriented systems can be represented using the non-functional extension of UML4SOA (UML4SOA-NFP) and the MARTE proﬁle. This enables modeling of performance, security and reliable messaging. Second, we discuss formal analysis of models which respect this design, in particular we consider performance estimates and reliability analysis using the stochastically timed process algebra PEPA as the underlying analytical engine. Last but not least, our models are the source for the application of deployment mechanisms which comprise model-to-model and model-to-text transformations implemented in the framework VIATRA. All techniques presented in this work are illustrated by a running example from an eUniversity case study.",
        "keywords": [
            "Non-functional properties",
            "Service-oriented software",
            "SOA",
            "Modeling",
            "Model-driven engineering"
        ],
        "authors": [
            "Stephen Gilmore",
            "László Gönczy",
            "Nora Koch",
            "Philip Mayer",
            "Mirco Tribastone",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-010-0155-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Process mining: a two-step approach to balance between underﬁtting and overﬁtting",
        "submission-date": "2008/04",
        "publication-date": "2010/09",
        "abstract": "Process mining includes the automated discovery of processes from event logs. Based on observed events (e.g., activities being executed or messages being exchanged) a process model is constructed. One of the essential problems in process mining is that one cannot assume to have seen all possible behavior. At best, one has seen a representative sub-set. Therefore, classical synthesis techniques are not suitable as they aim at ﬁnding a model that is able to exactly reproduce the log. Existing process mining techniques try to avoid such “overﬁtting” by generalizing the model to allow for more behavior. This generalization is often driven by the representation language and very crude assumptions about complete-ness. As a result, parts of the model are “overﬁtting” (allow only for what has actually been observed) while other parts maybe“underﬁtting”(allowformuchmorebehaviorwithout strong support for it). None of the existing techniques enables the user to control the balance between “overﬁtting” and “underﬁtting”. To address this, we propose a two-step approach. First, using a conﬁgurable approach, a transition system is constructed. Then, using the “theory of regions”, the model is synthesized. The approach has been imple-mented in the context of ProM and overcomes many of the limitations of traditional approaches.",
        "keywords": [],
        "authors": [
            "W. M. P. van der Aalst",
            "V. Rubin",
            "H. M. W. Verbeek",
            "B. F. van Dongen",
            "E. Kindler",
            "C. W. Günther"
        ],
        "file_path": "data/sosym-all/s10270-008-0106-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Three decades of the OO-Method: fostering conceptual-model software engineering",
        "submission-date": "2025",
        "publication-date": "2025",
        "abstract": "The consolidation of the object-oriented (OO) programming paradigm in the early 1990s sparked a growing need for methodological support, leading to significant advancements in software engineering. Conceptual modeling took advantage of this OO-based wave to provide modeling techniques that were designed to incorporate that expressiveness. This period emphasized the precise conceptualization of complex problem domains as a foundational step toward developing robust technological solutions and formalizing practices that had been evolving since the 1980s. The OO-Method marked a significant milestone by introducing a model-driven approach to generate functional software directly from models defining the system’s structure, behavior, logic, and presentation. This innovation led to the development of a supporting tool capable of automating software generation, which has been continuously updated since the 2000s to align with evolving technologies. Additionally, the integration of requirements engineering approaches expanded the method’s scope, enhancing its ability to capture and formalize system needs with semantic consistency. This work reviews the evolution of the OO-Method, its derived and integrated initiatives, and the scientific studies that have historically validated its contributions. Adopting a historical perspective, the review explores the present and future of model-driven software engineering, from code generation based on software models to the strategic alignment of agile and current challenges for leveraging generative artificial intelligence techniques.",
        "keywords": [
            "Model-driven development",
            "Conceptual modeling",
            "Strategy modeling",
            "Model compiler"
        ],
        "authors": [
            "Oscar Pastor",
            "Rene Noel",
            "Jose Ignacio Panach"
        ],
        "file_path": "data/sosym-all/s10270-025-01299-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multidimensional context modeling applied to non-functional analysis of software",
        "submission-date": "2016/10",
        "publication-date": "2017/12",
        "abstract": "Context awareness is a ﬁrst-class attribute of today software systems. Indeed, many applications need to be aware of their context in order to adapt their structure and behavior for offering the best quality of service even in case the software and hardware resources are limited. Modeling the context, its evolution, and its inﬂuence on the services provided by (possibly resource constrained) applications are becoming primary activities throughout the whole software life cycle, although it is still difﬁcult to capture the multidimensional nature of context. We propose a framework for modeling and reasoning on the context and its evolution along multiple dimensions. Our approach enables (1) the representation of dependencies among heterogeneous context attributes through a formally deﬁned semantics for attribute composition and (2) the stochastic analysis of context evolution. As a result, context can be part of a model-based software development process, and multidimensional context analysis can be used for different purposes, such as non-functional analysis. We demonstrate how certain types of analysis, not feasible with context-agnostic approaches, are enabled in our framework by explicitly representing the interplay between context evolution and non-functional attributes. Such analyses allow the identiﬁcation of critical aspects or design errors that may not emerge without jointly taking into account multiple context attributes. The framework is shown at work on a case study in the eHealth domain.",
        "keywords": [
            "Context modeling",
            "Context evolution",
            "Reliability",
            "Performance",
            "Transient and steady-state analysis"
        ],
        "authors": [
            "Luca Berardinelli",
            "Marco Bernardo",
            "Vittorio Cortellessa",
            "Antinisca Di Marco"
        ],
        "file_path": "data/sosym-all/s10270-017-0645-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The hidden models of model checking",
        "submission-date": "2011/12",
        "publication-date": "2012/08",
        "abstract": "In the past, applying formal analysis, such as model checking, to industrial problems required a team of formal methods experts and a great deal of effort. Model checking has become popular, because model checkers have evolved to allow domain-experts, who lack model check-ing expertise, to analyze their systems. What made this shift possible and what roles did models play in this? That is the main question we consider here. We survey approaches that transform domain-speciﬁc input models into alternative forms that are invisible to the user and which are amenable to model checking using existing techniques—we refer to these as hidden models. We observe that keeping these mod-els hidden from the user is in fact paramount to the success of the domain-speciﬁc model checker. We illustrate the value of hidden models by surveying successful examples of their use in different areas of model checking (hardware and soft-ware) and how a lack of suitable models hamper a new area (biological systems).",
        "keywords": [
            "Model checking",
            "Models temporal logic",
            "Biological systems"
        ],
        "authors": [
            "Willem Visser",
            "Matthew B. Dwyer",
            "Michael Whalen"
        ],
        "file_path": "data/sosym-all/s10270-012-0281-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting aspect orientation in business process management\nFrom process modelling to process enactment",
        "submission-date": "2015/01",
        "publication-date": "2015/09",
        "abstract": "Coping with complexity is an important issue in both research and industry. One strategy to deal with complexity is separation of concerns, which can be addressed using aspect-oriented paradigm. Despite being well researched in programming, this paradigm is still in a preliminary stage in the area of business process management (BPM). While some efforts have been made to introduce aspect orientation in business process modelling, there is no holistic approach with a formal underlying foundation to support aspect-oriented business process design and enactment, and this gap restricts aspect-oriented paradigm from being practically deployed in the area of BPM. Therefore, this paper proposes a sound systematic approach which builds on a formal syntax for modelling aspect-oriented business processes and a Petri Net-based operational semantics for enacting these processes. The approach enables the implementation of software system artefacts as a proof of concept to support design and enactment of aspect-oriented business processes in practice. The approach is demonstrated using a banking case study, where processes are modelled using a concrete notation that conforms to the proposed formal syntax and then executed in a state-of-the-art BPM system where the implemented artefacts are deployed.",
        "keywords": [
            "Business process management",
            "Aspect-oriented decomposition",
            "Process modelling",
            "Process enactment",
            "Weaving",
            "Cross-cutting concerns"
        ],
        "authors": [
            "Amin Jalali",
            "Chun Ouyang",
            "Petia Wohed",
            "Paul Johannesson"
        ],
        "file_path": "data/sosym-all/s10270-015-0496-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial",
        "submission-date": "2003/02",
        "publication-date": "2003/02",
        "abstract": "This editorial welcomes readers to the first 2003 issue of the Software and System Modeling (SoSyM) journal. It discusses the successful launch of the journal in 2002 and positive feedback received. It outlines the journal's aims to be a premier source of high-quality papers on modeling IT-based systems and highlights the content of this issue, including revised papers from the Modellierung 2002 workshop and a paper on object-relational database design using UML.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-003-0021-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Model driven architecture: Principles and practice",
        "submission-date": "2004/08",
        "publication-date": "2004/08",
        "abstract": "Model Driven Architecture (MDA1) is an ap-\nproach to application modeling and generation that has\nreceived a lot of attention in recent months. Champi-\noned by the Object Management Group (OMG), many\norganizations are now looking at the ideas of MDA as\na way to organize and manage their application solutions,\ntool vendors are explicitly referring to their capabilities\nin terms of “MDA compliance”, and the MDA lexicon\nof platform-speciﬁc and platform-independent models is\nnow widely referenced in the industry.\nIn spite of this interest and market support, there is\nlittle clear guidance on what MDA means, where we are\nin its evolution, what is possible with today’s technology,\nand how to take advantage of it in practice. This paper ad-\ndresses that need by providing an analysis of how modeling\nis used in industry today, the relevance of MDA to today’s\nsystems, a classiﬁcation of MDA tooling support, and ex-\namples of its use. The paper concludes with a set of recom-\nmendations for how MDA can be successful in practice.",
        "keywords": [
            "Software architecture",
            "Software design",
            "Uniﬁed Modeling Language (UML)"
        ],
        "authors": [
            "Alan W. Brown"
        ],
        "file_path": "data/sosym-all/s10270-004-0061-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tool support for reﬁnement of non-functional speciﬁcations",
        "submission-date": "2005/02",
        "publication-date": "2006/07",
        "abstract": "Model driven architecture (MDA) views\napplication development as a continuous transforma-\ntion of models of the target system. We propose a\nmethodology which extends this view to non-functional\nproperties. In previous publications we have shown how\nwe can use so-called context models to make the speciﬁ-\ncation of non-functional measurements independent of\ntheir application in concrete system speciﬁcations. We\nhave also shown how this allows us to distinguish two\nroles in the development process: the measurement de-\nsigner and the application designer.\nIn this paper we use the notion of context models\nto allow the measurement designer to provide mea-\nsurement deﬁnitions at different levels of abstraction.\nA measurement in our terminology is a non-functional\ndimension that can be constrained to describe a non-\nfunctional\nproperty.\nRequiring\nthe\nmeasurement\ndesigner to deﬁne transformations between context\nmodels, and applying them to measurement deﬁnitions,\nenables us to provide tool support for reﬁnement of non-\nfunctional constraints to the application designer. The\npaper presents the concepts for such tool support as well\nas a prototype implementation.",
        "keywords": [
            "Non-functional properties",
            "Model\ntransformation",
            "Reﬁnement",
            "CASE tool support"
        ],
        "authors": [
            "Simone Röttger",
            "Steffen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-006-0024-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models in simulation",
        "submission-date": "2016/07",
        "publication-date": "2016/07",
        "abstract": "In the presence of increasing complexity, simulation has become a vital tool for obtaining additional information about a subset of the world, such as the interactions of all entities, processes, life forms, and other key components that aid in understanding a particular context. A key advantage of simulation is the Principle of Substitution, which is perhaps best summarized by a quote from Marvin Minsky: We use the term “model” in the following sense: To an observer B, an object A∗is a model of an object A to the extent that B can use A∗to answer questions that interest him about A. It is understood that B’s use of a model entails the use of encodings for input and output, both for A and A∗. If A is the world, questions for A are experiments. A∗is a good model of A, in B’s view, to the extent that A∗’s answers agree with those of A’s, on the whole, with respect to the questions important to B. (Minsky 1965) The following deﬁnition offers several insights into the ben-eﬁts of simulation and the importance of a model toward understanding parts of a real system: Simulation is the imitation of the operation of a real-world process or system over time. The act of simulating something ﬁrst requires that a model be developed; this model represents the key character-istics or behaviors/functions of the selected physical or abstract system or process. The model represents B the system itself, whereas the simulation represents the operation of the system over time. Simulation is used in many contexts, such as simulation of technology for performance optimization, safety engineering, testing, training, education, and video games. Often, computer experiments are used to study simulation models. Sim-ulation is also used with scientiﬁc modeling of natural systems or human systems to gain insight into their functioning. (Wikipedia 2016)",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-016-0544-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Heuristic search for equivalence checking",
        "submission-date": "2013/07",
        "publication-date": "2014/05",
        "abstract": "Equivalence checking plays a crucial role in formal veriﬁcation since it is a natural relation for expressing the matching of a system implementation against its speciﬁcation. In this paper, we present an efﬁcient procedure, based on heuristic search, for checking well-known bisimulation equivalences for concurrent systems speciﬁed through process algebras. The method tries to improve, with respect to other solutions, both the memory occupation and the time required for proving the equivalence of systems. A prototype has been developed to evaluate the approach on several examples of concurrent system speciﬁcations.",
        "keywords": [
            "Heuristic search algorithms",
            "Bisimulation",
            "Concurrent systems",
            "Model checking"
        ],
        "authors": [
            "Nicoletta De Francesco",
            "Giuseppe Lettieri",
            "Antonella Santone",
            "Gigliola Vaglini"
        ],
        "file_path": "data/sosym-all/s10270-014-0416-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Cyber-physical systems challenges: a needs analysis for collaborating embedded software systems",
        "submission-date": "2015/01",
        "publication-date": "2016/01",
        "abstract": "Embedding computing power in a physical environment has provided the functional flexibility and performance necessary in modern products such as automobiles, aircraft, smartphones, and more. As product features came to increasingly rely on software, a network infrastructure helped factor out common hardware and offered sharing functionality for further innovation. A logical consequence was the need for system integration. Even in the case of a single original end manufacturer who is responsible for the final product, system integration is quite a challenge. More recently, there have been systems coming online that must perform system integration even after deployment—that is, during operation. This has given rise to the cyber-physical systems (CPS) paradigm. In this paper, select key enablers for a new type of system integration are discussed. The needs andchallengesfordesigningandoperatingCPSareidentiﬁed along with corresponding technologies to address the chal- lenges and their potential impact. The intent is to contribute to a model-based research agenda in terms of design methods, implementation technologies, and organization challenges necessary to bring the next-generation systems online.",
        "keywords": [
            "Cyber-physical systems",
            "Computation",
            "Embedded systems",
            "Challenges",
            "Internet of Things",
            "Modeling and simulation"
        ],
        "authors": [
            "Pieter J. Mosterman",
            "Justyna Zander"
        ],
        "file_path": "data/sosym-all/s10270-015-0469-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assessing model quality",
        "submission-date": "2004/08",
        "publication-date": "2004/08",
        "abstract": "Students in software engineering courses that cover modeling often ask some variant of the following question: “How do I know that my model is a good model?”. It is not easy to provide a satisfactory response to this question. Good instructors provide students with some criteria and guidelines in the form of patterns (e.g., Craig Larman’s GRASP patterns), rules of thumb (e.g., “minimize coupling, maximize cohesion”, “keep inheritance depth shallow”), and exemplar models to better understand good modeling practices. While these help, the reality is that students ultimately rely on feedback from their instructors to determine the quality of their models. The instructors play the role of expert modelers and the students are their apprentices. The state of the practice in assessing model quality in the classroom and in industry seems to indicate that modeling is still in the craftsmanship phase.\nResearch on rigorous assessment of model quality has given us a glimpse of how we can progress to the next phase in which models are engineered. A number of researchers are working on developing rigorous static analysis techniques that are based on well-deﬁned models of behavior. Articles on model-checking of modeled behavior published in SoSyM are a good reﬂection of the work in this area. Another promising area of research is systematic model testing (i.e., systematic dynamic analysis of modeled behavior). Systematic dynamic analysis of code (i.e., code testing) involves executing programs on a selected set of test inputs that satisfy some test criteria. These ideas can be extended to the modeling phases when models with operational semantics are used. Most educators in the modeling community have heard students gripe about their inability to animate or execute the models they have created in order to explore the behavior they have modeled. Model testing is concerned with providing modelers with this ability. Systematic model testing techniques provide opportunities for automating the testing process and for reusing tests. Systematic regression testing techniques in particular can enable more rigorous model evolution. The notion of model testing is not new. For example, SDL (Speciﬁcation and Description Language) tools of provide facilities for exercising the state-machine based SDL models using an input set of test events. Work on executable variants of the UML also aims to provide modelers with feedback on the adequacy of their models. More recently a small, but growing, number of researchers have begun looking at developing systematic model testing techniques. This is an important area of research and helps pave the way towards more effective use of models during software development. There are a number of lessons from the systematic code testing community that can be applied, but the peculiarities of modeling languages also requires the development of new and innovative approaches. In particular, innovative work on deﬁning eﬀective test criteria that are based on coverage of model elements and on the generation of model-level test cases that provide desired levels of coverage is needed. It is also useful to look at how other engineering disciplines determine the quality of their models. Engineers in other disciplines typically explore answers to the following questions when determining the adequacy of their models: Is the model a good predictor of how the physical artifact will behave? What are the (simplifying) assumptions underlying the model and what impact will they have on actual behavior? The answer to the first question is often based on evidence gathered from past applications of the model. Evidence of model fidelity is built up by comparing the actual behavior of systems built using the models with the behavior predicted by the models. Each time engineers build a system the experience gained either reinforces their confidence in the predictive power of the models used or the experience is used to improve the predictive power of models.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-004-0068-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On building location aware applications using an open platform based on the NEXUS Augmented World Model",
        "submission-date": "2002/09",
        "publication-date": "2004/04",
        "abstract": "How should the World Wide Web look like if it were for location-based information? And how would mobile, spatially aware applications deal with such a platform? In this paper we present the neXus Augmented World Model, an object oriented data model which plays a major role in an open framework for both providers of location-based information and new kinds of applications: the neXus platform. We illustrate the usability of the model with several sample applications and show the extensibility of this framework. At last we present a step-wise approach for building spatially aware applications in this environment.",
        "keywords": [
            "Location-awawe",
            "Infrastructure",
            "Augmented world model",
            "Nexus",
            "Open platform"
        ],
        "authors": [
            "Daniela Nicklas",
            "Bernhard Mitschang"
        ],
        "file_path": "data/sosym-all/s10270-004-0055-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Instant and global consistency checking during collaborative engineering",
        "submission-date": "2020/12",
        "publication-date": "2022/04",
        "abstract": "Engineering projects involve a variety of artifacts such as requirements, design, or source code. These artifacts, many of which tend to be interdependent, are often manipulated concurrently. To keep artifacts consistent, engineers must continuously consider their work in relation to the work of multiple other engineers. Traditional consistency checking approaches reason efficiently over artifact changes and their consistency implications. However, they do so solely within the boundaries of specific tools and their specific artifacts (e.g., consistency checking between different UML models). This makes it difficult to examine the consistency between different types of artifacts (e.g., consistency checking between UML models and the source code). Global consistency checking can help addressing this problem. However, it usually requires a disruptive and time-consuming merging process for artifacts. This article presents a novel, cloud-based approach to global consistency checking in a multi-developer/-tool engineering environment. It allows for global consistency checking across all artifacts that engineers work on concurrently. Moreover, it reasons over artifact changes immediately after the change happened, while keeping the (memory/CPU) cost of consistency checking minimal. The feasibility and scalability of our approach were demonstrated by a prototype implementation and through an empirical validation.",
        "keywords": [
            "Consistency checking",
            "Multi-developer environment",
            "Model-driven engineering"
        ],
        "authors": [
            "Michael Alexander Tröls",
            "Luciano Marchezan",
            "Atif Mashkoor",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-022-00984-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven framework for developing multi-agent systems in emergency response environments",
        "submission-date": "2016/12",
        "publication-date": "2017/10",
        "abstract": "In emergency response environments, variant entities with speciﬁc behaviors and interaction between them form a complex system that can be well modeled by multi-agent systems. To build such complex systems, instead of writing the code from scratch, one can follow the model-driven development approach, which aims to generate software from design models automatically. To achieve this goal, two important prerequisites are: a domain-speciﬁc modeling language for designing an emergency response environment model, and transformation programs for automatic code generation from a model. In addition, for modeling with the language, a modeling tool is required, and for executing the generated code there is a need to a platform. In this paper, a model-driven framework for developing multi-agent systems in emergency response environments is provided which includes several items. A domain-speciﬁc modeling language as well as a modeling tool is developed for this domain. The language and the tool are called ERE-ML and ERE-ML Tool, respectively. Using the ERE-ML Tool, a designer can model an emergency response situation and then validate the model against the predeﬁned constraints. Furthermore, several model to code transformations are deﬁned for automatic multi-agent system code generation from an emergency response environment model. For executing the generated code, an extension of JAMDER platform is also provided. To evaluate our framework, several case studies including the Victorian bushﬁre disaster are modeled to show theabilityoftheframeworkinmodelingreal-worldsituations and automatic transformation of the model into the code.",
        "keywords": [
            "Domain-speciﬁc modeling language",
            "Emergency response environment",
            "Multi-agent system",
            "Model-driven development",
            "ERE-ML",
            "Model to code transformation"
        ],
        "authors": [
            "Samaneh HoseinDoost",
            "Tahereh Adamzadeh",
            "Bahman Zamani",
            "Afsaneh Fatemi"
        ],
        "file_path": "data/sosym-all/s10270-017-0627-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CooPS – Towards a method for coordinating personalized services",
        "submission-date": "2004/04",
        "publication-date": "2006/02",
        "abstract": "This paper presents CooPS, which is a method for Coordinating Personalized Services. These services are primarily offered to mobile users. The concept of services is the object of intense investigations from both academia and industry. However, very little has been accomplished so far regarding ﬁrst, personalizing services for the beneﬁt of mobile users, and second, providing the appropriate methodological support for those (i.e., designers) who will be specifying the operations of personalization. Various obstacles still exist such as lack of techniques for modeling and specifying the integration of personalization into services, and existing approaches for service composition typically facilitate orchestration only, while neglecting contexts of users and services. CooPS consists of several steps ranging from service deﬁnition and personalization to service deployment. Each step has some representation techniques, which aim at facilitating the speciﬁcation and validation of the operations of coordinating personalized services.",
        "keywords": [
            "Service",
            "Coordination",
            "Personalization",
            "Method"
        ],
        "authors": [
            "Zakaria Maamar",
            "Djamal Benslimane",
            "Michael Mrissa",
            "Chirine Ghedira"
        ],
        "file_path": "data/sosym-all/s10270-006-0006-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An operational guide to monitorability with applications to regular properties",
        "submission-date": "2020/02",
        "publication-date": "2021/02",
        "abstract": "Monitorability underpins the technique of runtime veriﬁcation because it delineates what properties can be veriﬁed at runtime. Although many monitorability deﬁnitions exist, few are deﬁned explicitly in terms of the operational guarantees provided by monitors, i.e. the computational entities carrying out the veriﬁcation. We view monitorability as a spectrum, where the fewer guarantees that are required of monitors, the more properties become monitorable. Accordingly, we present a monitorability hierarchy based on this trade-off. For regular speciﬁcations, we give syntactic characterisations in Hennessy–Milner logic with recursion for its levels. Finally, we map existing monitorability deﬁnitions into our hierarchy. Hence, our work gives a uniﬁed framework that makes the operational assumptions and guarantees of each deﬁnition explicit. This provides a rigorous foundation that can inform design choices and correctness claims for runtime veriﬁcation tools.",
        "keywords": [
            "Runtime Veriﬁcation",
            "Monitors",
            "Monitorability",
            "Logical Fragments"
        ],
        "authors": [
            "Luca Aceto",
            "Antonis Achilleos",
            "Adrian Francalanza",
            "Anna Ingólfsdóttir",
            "Karoliina Lehtinen"
        ],
        "file_path": "data/sosym-all/s10270-020-00860-z.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Exploring the interaction of design variability and stochastic operational uncertainties in software-intensive systems through the lens of modeling",
        "submission-date": "2024/02",
        "publication-date": "2024/11",
        "abstract": "In software-intensive systems, navigating the complexities that emerge from the interaction of design variability and stochastic operational uncertainties presents a daunting challenge. This paper delves into the dynamics between these two dimensions of uncertainty, offering novel insights about how modeling can contribute to the analysis of their combined impact upon system properties. By elevating the abstraction level at which probabilistic models are conceptualized, our approach enables an integrated analysis framework that considers both structural and quantitative dimensions of design spaces. Through the introduction of novel language constructs, our methodology facilitates the direct referencing of structural relationships within probabilistic behavioral speciﬁcations. Furthermore, the adoption of novel quantiﬁers in probabilistic temporal logic enables evaluating complex properties across diverse design variants, thereby streamlining the assessment of guarantees within the solution space. We demonstrate the feasibility of this approach on four case studies, showcasing its potential to offer comprehensive insights into the trade-offs and decision-making processes inherent in managing different types of structural design variability and operational uncertainties in software-intensive systems.",
        "keywords": [
            "Design variability",
            "Operational uncertainty",
            "Uncertainty interaction",
            "Quantitative veriﬁcation"
        ],
        "authors": [
            "Javier Cámara"
        ],
        "file_path": "data/sosym-all/s10270-024-01226-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Separation of non-orthogonal concerns in software architecture and design",
        "submission-date": "2004/01",
        "publication-date": "2006/01",
        "abstract": "Abstract Separation of concerns represents an important\nprinciple for managing complexity in the design and ar-\nchitecture of large component-based software systems. The\nfundamental approach is to develop local solutions for indi-\nvidual concerns ﬁrst, and combine them later into an overall\nsolution for the complete system. However, comprehensive\nsupport for the integration of interdependent, possibly con-\nﬂicting concerns related to synchronization behavior is still\nmissing. In our work, we propose a sound solution for this\ncomplex type of composition, employing well-known UML\ndescription techniques as well as a rigorous formal model of\ncomponent synchronization behavior. Based on this founda-\ntion, we describe a constructive synthesis algorithm which\nreliably detects conﬂicting concerns or generates a maxi-\nmal synchronization behavior for software components with\nmultiple interactions. An optimized implementation of the\nalgorithm has been integrated into a CASE tool to illustrate\nfeasibility and scalability of the presented technique to the\nexample of a moderately large case study.",
        "keywords": [
            "Separation of concern",
            "Software architecture",
            "Consistency",
            "Behavior synthesis",
            "Design by contract"
        ],
        "authors": [
            "Holger Giese",
            "Alexander Vilbig"
        ],
        "file_path": "data/sosym-all/s10270-005-0103-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synthesizing veriﬁed components for cyber assured systems engineering",
        "submission-date": "2022/03",
        "publication-date": "2023/03",
        "abstract": "Safety-critical systems such as avionics need to be engineered to be cyber resilient meaning that systems are able to detect and recover from attacks or safely shutdown. As there are few development tools for cyber resiliency, designers rely on guidelines and checklists, sometimes missing vulnerabilities until late in the process where remediation is expensive. Our solution is a model-based approach with cyber resilience-improving transforms that insert high-assurance components such as ﬁlters to block malicious data or monitors to detect and alarm anomalous behavior. Novel is our use of model checking and a veriﬁed compiler to specify, verify, and synthesize these components. We deﬁne code contracts as formal speciﬁcations that designers write for high-assurance components, and test contracts as tests to validate their behavior. A model checker proves whether or not code contracts satisfy test contracts in an iterative development cycle. The same model checker also proves whether or not a system with the inserted components, assuming they adhere to their code contracts, provides the desired cyber resiliency for the system. We deﬁne an algorithm to synthesize implementations for code contracts in a semantics-preserving way that is backed by a veriﬁed compiler. The entire workﬂow is implemented as part of the open source BriefCASE toolkit. We report on our experience using BriefCASE with a case study on a UAV system that is transformed to be cyber resilient to communication and supply chain cyber attacks. Our case study demonstrates that writing code contracts and then synthesizing correct implementations from them are feasible in real-world systems engineering for cyber resilience.",
        "keywords": [
            "AADL",
            "OSATE",
            "Assume-guarantee reasoning",
            "Synthesis",
            "Cyber hardening",
            "Formal veriﬁcation",
            "Model-based systems engineering"
        ],
        "authors": [
            "Eric Mercer",
            "Konrad Slind",
            "Isaac Amundson",
            "Darren Cofer",
            "Junaid Babar",
            "David Hardin"
        ],
        "file_path": "data/sosym-all/s10270-023-01096-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bootstrapping MDE development from ROS manual code: Part 2—Model generation and leveraging models at runtime",
        "submission-date": "2020/03",
        "publication-date": "2021/04",
        "abstract": "Model-driven engineering (MDE) addresses central aspects of robotics software development. MDE could enable domain experts to leverage the expressiveness of models, while implementation details on different hardware platforms would be handled by automatic code generation. Today, despite strong MDE efforts in the robotics research community, most evidence points to manual code development being the norm. A possible reason for MDE not being accepted by robot software developers could be the wide range of applications and target platforms, which make all-encompassing MDE IDEs hard to develop and maintain. Therefore, we chose to leverage a large corpus of open-source software widely adopted by the robotics community to extract common structures and gain insight on how and where MDE can support the developers to work more efﬁciently. We pursue modeling as a complement, rather than imposing MDE as separate solution. Our previous work introduced metamodels to describe components, their interactions, and their resulting composition. In this paper, we present two methods based on metamodels for automated generation of models from manually written artifacts: (1) through static code analysis and (2) by monitoring the execution of a running system. For both methods, we present tools that leverage the potentials of our contributions, with a special focus on their application at runtime to observe and diagnose a real system during its execution. A comprehensive example is provided as a walk-through for robotics software practitioners.",
        "keywords": [
            "ROS",
            "Models",
            "MDE",
            "Robotics"
        ],
        "authors": [
            "Nadia Hammoudeh García",
            "Harshavardhan Deshpande",
            "André Santos",
            "Björn Kahl",
            "Mirko Bordignon"
        ],
        "file_path": "data/sosym-all/s10270-021-00873-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A formal component model for UML based on CSP aiming at compositional veriﬁcation",
        "submission-date": "2022/03",
        "publication-date": "2023/10",
        "abstract": "Model-based engineering emerged as an approach to tackle the complexity of current system development. In particular, compositional strategies assume that systems can be built from reusable and loosely coupled units. However, it is still a challenge to ensure that desired properties hold for component integration. We present a component-based model for UML, including a metamodel, well-formedness conditions and formal semantics via translation into BRIC; the presentation of the semantics is given by a set of rules that cover all the metamodel elements and map them to their respective BRIC denotations. We use our previous work on BRIC as an underlying (and totally hidden) component development framework so that our approach beneﬁts from all the formal infrastructure developed for BRIC using CSP. Component composition, speciﬁed via UML structural diagrams, ensures adherence to classical concurrent properties: our focus is on the preservation of deadlock freedom. Automated support is developed as a plug-in to the Astah modelling tool. Veriﬁcation is carried out using FDR (a model checker for CSP); we address scalability using compositional reasoning (inherent to the approach) and behavioural patterns. The formal reasoning is transparent to the user: a distinguishing feature of our approach is its support for traceability. For instance, when FDR uncovers a deadlock, a sequence diagram is constructed from the deadlock trace and presented to the user at the modelling level. The overall approach is illustrated with a running example and two additional case studies.",
        "keywords": [
            "CSP",
            "Component",
            "Compositional veriﬁcation",
            "UML",
            "Deadlock analysis"
        ],
        "authors": [
            "Flávia Falcão",
            "Lucas Lima",
            "Augusto Sampaio",
            "Pedro Antonino"
        ],
        "file_path": "data/sosym-all/s10270-023-01127-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dynamic constraint satisfaction problems over models",
        "submission-date": "2010/07",
        "publication-date": "2011/01",
        "abstract": "In early phases of designing complex systems, models are not sufficiently detailed to serve as an input for automated synthesis tools. Instead, a design space is constituted by multiple models representing different valid design candidates. Design space exploration aims at searching through these candidates defined in the design space to find solutions that satisfy the structural and numeric design constraints and provide a balanced choice with respect to various quality metrics. Design space exploration in an model-driven engineering (MDE) context is frequently tackled as specific sort of constraint satisfaction problem (CSP). In CSP, declarative constraints capture restrictions over variables with finite domains where both the number of variables and their domains are required to be a priori finite. However, the existing formulation of constraint satisfaction problems can be too restrictive to capture design space exploration in many MDE applications with complex structural constraints expressed over the underlying models. In this paper, we interpret flexible and dynamic constraint satisfaction problems directly in the context of models. These extensions allow the relaxation of constraints during a solving process and address problems that are subject to change and require incremental re-evaluation. Furthermore, we present our prototype constraint solver for the domain of graph models built upon the Viatra2 model transformation framework and provide an evaluation of its performance with comparison to related tools.",
        "keywords": [
            "Constraint satisfaction programming",
            "Graph transformation",
            "Dynamic constraint satisfaction programming",
            "Flexible constraint satisfaction problem"
        ],
        "authors": [
            "Ákos Horváth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-010-0185-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Exchanging information in cooperative software validation",
        "submission-date": "2023/02",
        "publication-date": "2024/03",
        "abstract": "Cooperative software validation aims at having veriﬁcation and/or testing tools cooperate on the task of correctness checking. Cooperation involves the exchange of information about currently achieved results in the form of (veriﬁcation) artifacts. These artifacts are typically specialized to the type of analysis performed by the tool, e.g., bounded model checking, abstract interpretation or symbolic execution, and hence require the deﬁnition of a new artifact for every new cooperation to be built. In this article, we introduce a uniﬁed artifact (called Generalized Information Exchange Automaton, short GIA) supporting the cooperation of over-approximating with under-approximating analyses. It provides information gathered by an analysis to its partner in a cooperation, independent of the type of analysis and usage context within software validation. We provide a formal deﬁnition of this artifact in the form of an automaton together with two operators on GIAs. The ﬁrst operation reduces a program by excluding these parts, where the information that they are already processed is encoded in the GIA. The second operation combines partial results from two GIAs into a single on. We show that computed analysis results are never lost when connecting tools via these operations. To experimentally demonstrate the feasibility, we have implemented two such cooperation: one for veriﬁcation and one for testing. The obtained results show the feasibility of our novel artifact in different contexts of cooperative software validation, in particular how the new artifact is able to overcome some drawbacks of existing artifacts.",
        "keywords": [
            "Cooperative software veriﬁcation",
            "Veriﬁcation artifact",
            "Test case generation",
            "Component-based CEGAR"
        ],
        "authors": [
            "Jan Haltermann",
            "Heike Wehrheim"
        ],
        "file_path": "data/sosym-all/s10270-024-01155-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Tracing security requirements in industrial control systems using graph databases",
        "submission-date": "2021/06",
        "publication-date": "2022/07",
        "abstract": "We must explicitly capture relationships and hierarchies between the multitude of system and security standards requirements. Current security requirements speciﬁcation methods do not capture such structure effectively, making requirements management and traceability harder, consequently increasing costs and time to market for developing certified ICS. We propose a novel requirements repository model for ICS that uses labelled property graphs to structure and store system-specific and standards-based requirements using well-defined relationship types. Furthermore, we integrate the proposed requirements repository with design-time ICS tools to establish requirements traceability. A wind turbine case study illustrates the overall workflow in our framework. We demonstrate that a robust requirements traceability matrix is a natural consequence of using labelled property graphs. We also introduce a compatible requirements change management procedure that aids in adapting to changes in development and certification schemes.",
        "keywords": [
            "Requirements engineering",
            "Security",
            "Requirements repository",
            "Security standards",
            "Industrial control systems",
            "Traceability",
            "Graph databases",
            "Labelled property graphs"
        ],
        "authors": [
            "Awais Tanveer",
            "Chandan Sharma",
            "Roopak Sinha",
            "Matthew M. Y. Kuo"
        ],
        "file_path": "data/sosym-all/s10270-022-01019-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting timing analysis of vehicular embedded systems through the refinement of timing constraints",
        "submission-date": "2016/03",
        "publication-date": "2017/01",
        "abstract": "The collective use of several models and tools at various abstraction levels and phases during the development of vehicular distributed embedded systems poses many challenges. Within this context, this paper targets the challenges that are concerned with the unambiguous refinement of timing requirements, constraints and other timing information among various abstraction levels. Such information is required by the end-to-end timing analysis engines to provide pre-run-time verification about the predictability of these systems. The paper proposes an approach to represent and refine such information among various abstraction levels. As a proof of concept, the approach provides a representation of the timing information at the higher levels using the models that are developed with EAST-ADL and Timing Augmented Description Language. The approach then refines the timing information for the lower abstraction levels. The approach exploits the Rubus Component Model at the lower level to represent the timing information that cannot be clearly specified at the higher levels, such as trigger paths in distributed chains. A vehicular-application case study is conducted to show the applicability of the proposed approach.",
        "keywords": [
            "Distributed embedded systems",
            "Component-based development",
            "Timing model",
            "Component model",
            "End-to-end timing analysis"
        ],
        "authors": [
            "Saad Mubeen",
            "Thomas Nolte",
            "Mikael Sjödin",
            "John Lundbäck",
            "Kurt-Lennart Lundbäck"
        ],
        "file_path": "data/sosym-all/s10270-017-0579-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deriving performance-relevant infrastructure properties through model-based experiments with Ginpex",
        "submission-date": "2012/01",
        "publication-date": "2013/03",
        "abstract": "To predict the performance of an application, it is crucial to consider the performance of the underlying infrastructure. Thus, to yield accurate prediction results, performance-relevant properties and behaviour of the infrastructure have to be integrated into performance models. However, capturing these properties is a cumbersome and error-prone task, as it requires carefully engineered measurements and experiments. Existing approaches for creating infrastructure performance models require manual coding of these experiments, or ignore the detailed properties in the models. The contribution of this paper is the Goal-oriented INfrastructure Performance EXperi-ments (Ginpex) approach, which introduces goal-oriented and model-based speciﬁcation and generation of executable performance experiments for automatically detecting and quantifying performance-relevant infrastructure properties. Ginpex provides a metamodel for experiment speciﬁcation and comes with predeﬁned experiment templates that provide automated experiment execution on the target platform and also automate the evaluation of the experiment results. We evaluate Ginpex using three case studies, where experiments are executed to quantify various infrastructure properties.",
        "keywords": [
            "Metamodelling",
            "Experiments",
            "Measurements",
            "Infrastructure",
            "Deriving infrastructure properties",
            "Performance prediction"
        ],
        "authors": [
            "Michael Hauck",
            "Michael Kuperberg",
            "Nikolaus Huber",
            "Ralf Reussner"
        ],
        "file_path": "data/sosym-all/s10270-013-0335-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How to write a successful SoSyM submission",
        "submission-date": "2016/09",
        "publication-date": "2016/09",
        "abstract": "Authors of research papers often want to gain insight on how to improve the chances of their research being published. There have been several “how to” guides created for this purpose. Within its first five years, OOPSLA’s popularity grew to the point that there was a panel focused on the topic of how to get a paper accepted [1]. Mary Shaw’s tutorial/paper on how to write a good software engineering contribution was directed to an ICSE audience, but has many timeless suggestions that are still relevant in many general contexts [2]. This issue’s editorial discusses the SoSyM review process and why a paper might be rejected at the various stages of review.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-016-0558-5.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Special section of BPMDS’2021 business process improvement",
        "submission-date": "2023/10",
        "publication-date": "2023/12",
        "abstract": "The Business Process Modeling, Development and Support (BPMDS) working conference series, held in conjunction with CAiSE conferences, serve as a meeting place for researchers and practitioners in Business Process Modeling, Development, and Support. Business process analysis, design, and support, addressed by the BPMDS series, have been recognized as a central issue in information systems (IS) engineering. In 2011, BPMDS became a two-day working conference held in conjunction with CAiSE (Conference on Advanced Information Systems Engineering). The goals, format, and history of BPMDS can be found on the website http://www.bpmds.org/. ",
        "keywords": [],
        "authors": [
            "Adriano Augusto",
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-023-01139-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Exploiting model driven technology: a tale of two startups",
        "submission-date": "2012/01",
        "publication-date": "2012/08",
        "abstract": "This article describes the experiences of two independent start-up companies that were created in the white-heat of the early days of model-based engineering. Each company aimed to revolutionise software development by raising the level of abstraction through modelling. The article describes the context, technical innovations, business experiences, demise and lessons learned by each company.",
        "keywords": [
            "Startup",
            "Tool company",
            "Model driven development"
        ],
        "authors": [
            "Tony Clark",
            "Pierre-Alain Muller"
        ],
        "file_path": "data/sosym-all/s10270-012-0260-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Testing timed systems modeled by Stream X-machines",
        "submission-date": "2009/03",
        "publication-date": "2009/08",
        "abstract": "Stream X-machines have been used to specify real systems where complex data structures. They are a variety of extended ﬁnite state machine where a shared memory is used to represent communications between the components of systems. In this paper we introduce an extension of the Stream X-machines formalism in order to specify systems that present temporal requirements. We add time in two different ways. First, we consider that (output) actions take time to be performed. Second, our formalism allows to specify timeouts. Timeouts represent the time a system can wait for the environment to react without changing its internal state. Since timeous affect the set of available actions of the system, a relation focusing on the functional behavior of systems, that is, the actions that they can perform, must explicitly take into account the possible timeouts. In this paper we also propose a formal testing methodology allowing to systematically test a system with respect to a speciﬁcation. Finally, we introduce a test derivation algorithm. Given a speciﬁcation, the derived test suite is sound and complete, that is, a system under test successfully passes the test suite if and only if this system conforms to the speciﬁcation.",
        "keywords": [
            "Formal testing",
            "Timed systems",
            "Stream X-machines"
        ],
        "authors": [
            "Mercedes G. Merayo",
            "Manuel Núñez",
            "Robert M. Hierons"
        ],
        "file_path": "data/sosym-all/s10270-009-0126-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal veriﬁcation and validation of embedded systems: the UML-based MADES approach",
        "submission-date": "2012/11",
        "publication-date": "2013/06",
        "abstract": "Formal veriﬁcation and validation activities from the early development phases can foster system consistency, correctness, and integrity, but they are often hard to carry out as most designers do not have the necessary background. To address this difﬁculty, a possible approach is to allow engineers to continue using familiar notations and tools, while veriﬁcation and validation are performed on demand, automatically, and transparently. In this paper we describe how the problem of making formal veriﬁcation and valida-tion tasks more designer-friendly is tackled by the MADES approach. Our solution is based on a tool chain that is built atop mature, popular, and widespread technologies. The paper focuses on the veriﬁcation and closed-loop simulation (validation) aspects of the approach and shows how it can be applied to signiﬁcant embedded software systems.",
        "keywords": [
            "Model-driven development",
            "Veriﬁcation",
            "Closed-loop simulation",
            "MARTE",
            "Embedded systems"
        ],
        "authors": [
            "Luciano Baresi",
            "Gundula Blohm",
            "Dimitrios S. Kolovos",
            "Nicholas Matragkas",
            "Alfredo Motta",
            "Richard F. Paige",
            "Alek Radjenovic",
            "Matteo Rossi"
        ],
        "file_path": "data/sosym-all/s10270-013-0330-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Does model driven engineering tame complexity?",
        "submission-date": "2007/01",
        "publication-date": "2007/01",
        "abstract": "Advocates of model-driven (software) engineering (MDE) tout the need to raise the level of abstraction at which software is conceived, implemented, and evolved to better manage the inherent complexity of modern software-based systems. Examples of MDE-related languages, technologies, and techniques that have been proposed as “tamers” of inherent complexity are languages supporting multi-view modeling of systems (e.g., the UML), metamodeling approaches to specifying model transformations, metamodeling environments for creating and using domain speciﬁc languages, megamodeling environments for manipulating and managing models, and aspect-oriented modeling environments supporting multi-dimensional separation of concerns and composition. Experiences suggest that some forms of MDE technologies may introduce signiﬁcant accidental complexities that can detract from their use as managers of inherent software complexity.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-006-0041-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Recommending metamodel concepts during modeling activities with pre-trained language models",
        "submission-date": "2021/04",
        "publication-date": "2022/02",
        "abstract": "The design of conceptually sound metamodels that embody proper semantics in relation to the application domain is particularly tedious in model-driven engineering. As metamodels deﬁne complex relationships between domain concepts, it is crucial for a modeler to deﬁne these concepts thoroughly while being consistent with respect to the application domain. We propose an approach to assist a modeler in the design of metamodel by recommending relevant domain concepts in several modeling scenarios. Our approach does not require knowledge from the domain or to hand-design completion rules. Instead, we design a fully data-driven approach using a deep learning model that is able to abstract domain concepts by learning from both structural and lexical metamodel properties in a corpus of thousands of independent metamodels. We evaluate our approach on a test set containing 166 metamodels, unseen during the model training, with more than 5000 test samples. Our preliminary results show that the trained model is able to provide accurate top 5 lists of relevant recommendations for concept renaming scenarios. Although promising, the results are less compelling for the scenario of the iterative construction of the metamodel, in part because of the conservative strategy we use to evaluate the recommendations.",
        "keywords": [
            "Intelligent modeling assistants",
            "Domain concepts",
            "Recommender systems",
            "Pre-trained language models"
        ],
        "authors": [
            "Martin Weyssow",
            "Houari Sahraoui",
            "Eugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-022-00975-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Behavioral speciﬁcation of reactive systems using stream-based I/O tables",
        "submission-date": "2010/06",
        "publication-date": "2011/05",
        "abstract": "Acoreprobleminformalmethodsisthetransition from informal requirements to formal speciﬁcations. Especially when specifying the behavior of reactive systems, many formalisms require the user to either understand a complex mathematical theory and notation or to derive details not given in the requirements, such as the state space of the problem. For many approaches also a consistent set of requirements is needed, which enforces to resolve requirements conﬂicts prior to formalization. This paper describes a speciﬁcation technique, where not states but signal patterns are the main elements. The notation is based on tables of regular expressions and supports a piece-wise formalization of potentially inconsistent requirements. Many properties, such as input completeness and consistency, can be checked automatically for these speciﬁcations. The detection and resolution of conﬂicts can be performed within our framework after formalization. Besides the formal foundation of our approach, this paper presents prototypical tool support and results from an industrial case study.",
        "keywords": [
            "Tabular speciﬁcation",
            "Consistency",
            "Streams"
        ],
        "authors": [
            "Judith Thyssen",
            "Benjamin Hummel"
        ],
        "file_path": "data/sosym-all/s10270-011-0204-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Verifying workﬂow processes: a transformation-based approach",
        "submission-date": "2008/10",
        "publication-date": "2010/02",
        "abstract": "Workﬂow modeling is a challenging activity and designers are likely to introduce errors, especially in complex industrial processes. Effective process veriﬁcation is essential at design time because the cost of ﬁxing errors during runtime is substantially higher. However, most user-oriented workﬂow modeling languages lack formal semantics that hinders such veriﬁcation. In this paper, we propose a generic approach based on the model transformation to verify workﬂow processes. The model transformation includes two steps: ﬁrst, it formalizes the desirable semantics of each modeling element; secondly, it translates a workﬂow process with clear semantics to an equivalent Petri net. Thus, we can verify the original workﬂow process using existing Petri net theory and analysis tools. As a comprehensive case study, verifying workﬂow processes in an industrial modeling language (TiPLM) is presented. Experimental evaluations on verifying real-world business processes validate our approach.",
        "keywords": [
            "Workﬂow",
            "Model transformation",
            "Process veriﬁcation",
            "Workﬂow veriﬁcation",
            "Petri net"
        ],
        "authors": [
            "Haiping Zha",
            "Wil M. P. van der Aalst",
            "Jianmin Wang",
            "Lijie Wen",
            "Jiaguang Sun"
        ],
        "file_path": "data/sosym-all/s10270-010-0149-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model checking LTL properties over ANSI-C programs with bounded traces",
        "submission-date": "2012/04",
        "publication-date": "2013/07",
        "abstract": "Context-bounded model checking has been used successfully to verify safety properties in multi-threaded systems automatically, even if they are implemented in low-level programming languages such as C. In this paper, we describe and experiment with an approach to extend context-bounded software model checking to safety and liveness properties expressed in linear-time temporal logic (LTL). Our approach checks the actual C program, rather than an extracted abstract model. It converts the LTL formulas into Büchi automata for the corresponding never claims and then further into C monitor threads that are interleaved with the execution of the program under analysis. This combined system is then checked using the ESBMC model checker. We use an extended, four-valued LTL semantics to handle the finite traces that bounded model checking explores; we thus check the combined system several times with different acceptance criteria to derive the correct truth value. In order to mitigate the state space explosion, we use a dedicated scheduler that selects the monitor thread only after updates to global variables occurring in the LTL formula. We demonstrate our approach on the analysis of the sequential firmware of a medical device and a small multi-threaded control application.",
        "keywords": [
            "Model checking",
            "Linear temporal logic",
            "Software verification"
        ],
        "authors": [
            "Jeremy Morse",
            "Lucas Cordeiro",
            "Denis Nicole",
            "Bernd Fischer"
        ],
        "file_path": "data/sosym-all/s10270-013-0366-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Environment modeling and simulation for automated testing of soft real-time embedded software",
        "submission-date": "2012/03",
        "publication-date": "2013/04",
        "abstract": "Given the challenges of testing at the system level, only a fully automated approach can really scale up to industrial real-time embedded systems (RTES). Our goal is to provide a practical approach to the model-based testing of RTES by allowing system testers, who are often not familiar with the system’s design but are application domain experts, to model the system environment in such a way as to enable its black-box test automation. Environment models can support the automation of three tasks: the code generation of an environment simulator to enable testing on the development platform or without involving actual hardware, the selection of test cases, and the evaluation of their expected results (oracles). From a practical standpoint—and such considerations are crucial for industrial adoption—environment modeling should be based on modeling standards (1) that are at an adequate level of abstraction, (2) that software engineers are familiar with, and (3) that are well supported by commercial or open source tools. In this paper, we propose a precise environment modeling methodology fitting these requirements and discuss how these models can be used to generate environment simulators. The environment models are expressed using UML/MARTE and OCL, which are international standards for real-time systems and constraint modeling. The presented techniques are evaluated on a set of three artificial problems and on two industrial RTES.",
        "keywords": [
            "Environment modeling",
            "Environment simulation",
            "Automated testing",
            "Model-based testing",
            "Real-time embedded systems",
            "Search based software engineering"
        ],
        "authors": [
            "Muhammad Zohaib Iqbal",
            "Andrea Arcuri",
            "Lionel Briand"
        ],
        "file_path": "data/sosym-all/s10270-013-0328-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MoDMaCAO: a model-driven framework for the design, validation and configuration management of cloud applications based on OCCI",
        "submission-date": "2021/02",
        "publication-date": "2022/09",
        "abstract": "To tackle the cloud-provider lock-in, the open grid forum is developing the open cloud computing interface (OCCI), a standardized interface for managing any kind of cloud resources. Besides the OCCI Core model, which defines the basic modeling elements for cloud resources, further standardized extensions exist that reflect the requirements of different cloud service levels, such as infrastructure and platform elements. However, so far the OCCI platform extension is very coarse-grained and lacks supporting use cases and implementations. Especially, it does not define how the components of the application itself can be managed. In this paper, we discuss the features of MoDMaCAO, a model-driven framework that extends the OCCI platform extension. The users of the framework are able to design and validate cloud application topologies and subsequently deploy them on OCCI compliant clouds by using configuration management tools.",
        "keywords": [
            "Cloud computing",
            "Open cloud computing interface",
            "OCCI",
            "Models@run.time"
        ],
        "authors": [
            "Faiez Zalila\nFabian Korte\nJohannes Erbel\nStéphanie Challita\nJens Grabowski\nPhilippe Merle"
        ],
        "file_path": "data/sosym-all/s10270-022-01024-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MEMORIAL",
        "submission-date": "2023/02",
        "publication-date": "2023/03",
        "abstract": "This paper is a memorial to Heinrich Hussmann, detailing his contributions to software engineering, formal methods, and multimedia technologies. It highlights his research on algebraic specifications, the RAP tool, the SPECTRUM language, and his work on integrating relational databases with object-oriented applications.",
        "keywords": [],
        "authors": [
            "Manfred Broy",
            "Albrecht Schmidt",
            "Martin Wirsing"
        ],
        "file_path": "data/sosym-all/s10270-023-01099-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CHECKSUM: tracking changes and measuring contributions in cooperative systems modeling",
        "submission-date": "2020/02",
        "publication-date": "2021/01",
        "abstract": "Models are often used to represent various types of systems. This is especially true for software systems, where cooperat-\ning teams create models using a modeling language (e.g., UML). In cooperative modeling scenarios, it is useful to identify \ncontributions and changes performed by individuals and teams. This paper presents a technique called CHECKSUM, which \nmonitors the cooperative work done on models and maintains an immutable changelog. CHECKSUM uses its changelog to \nmeasure contributions based on points, time, and quality, and to enable the auditing of a model’s change-history. This paper \nalso presents GEneric Meta-Model (GEMM). The latter unifies the underlying representation of different types of models \nthat follow varying visualization patterns including box and line, container, and interleaving. GEMM enables CHECKSUM \nto support an extensible variety of model types. We developed a prototype tool that realizes CHECKSUM’s concepts and \nintegrates it into two existing modeling tools. We conducted two studies to evaluate CHECKSUM from two perspectives: \ntechnical and user. The studies yielded positive results concerning various qualities including integrability into existing tools, \neffectiveness, efficiency, usability, and usefulness.",
        "keywords": [
            "Models",
            "Diagrams",
            "Changes",
            "Contributions",
            "Cooperative work",
            "Design tools and techniques"
        ],
        "authors": [
            "Pierre A. Akiki\nHoda W. Maalouf"
        ],
        "file_path": "data/sosym-all/s10270-020-00840-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MUREQ: a multilayer framework for analyzing and operationalizing visualization requirements",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "Understanding and interpreting vast amounts of information is pivotal in the contemporary data-rich age. Data visualization has emerged as a signiﬁcant measure of comprehending these data. Similarly, an appropriate visualization can also enhance software modeling by providing straightforward and interactive representations. However, current data visualization methods predominantly require users to have data visualization-related expertise, which is usually challenging to obtain in reality. It is essential to bridge the gap between visualization requirements and visualization solutions for non-expert users, assisting them in automatically operationalizing their visualization requirements. This paper proposes a MUltilayer framework for analyzing andoperationalizingvisualizationREQuirementsthatautomaticallyderivesappropriatevisualizationsolutionsbasedonusers’ requirements. Speciﬁcally, we systematically investigate the connections among visualization requirements, visual variable characteristics, visual variable attributes, and visualization solutions, based on which we establish a conceptual framework that characterizes the relationships among different layers. Our proposal contributes to not only automatically operationalizing visualization requirements but also providing meaningful explanations for the derived visualization solutions. To promote our proposal and pragmatically beneﬁt real users, we have developed and deployed a prototype tool based on the proposed framework, which is publicly available at https://reqdv.vmasks.fun. To evaluate our proposed framework, we conducted an initial controlled experiment with 44 participants to test the performance of the evolved mappings within our framework. Based on the expert’s feedback, we reﬁned the mappings and incorporated a ranking system for visualization solutions tailored to speciﬁc requirements. To assess the current method, a subsequent experiment with another group of 44 participants and a focused case study involving two new participants were carried out. The results demonstrate that users perceive that the current method accelerates task completion, especially for complex tasks, by efﬁciently narrowing down options and prioritizing them. This approach is particularly advantageous for users with limited data visualization experience. Besides, the multilayer framework can be used to inspire the visualization of models in the software modeling community.",
        "keywords": [
            "Visualization requirements operationalization",
            "Multilayer analysis",
            "Empirical evaluation",
            "Prototype tool"
        ],
        "authors": [
            "Tong Li",
            "Yiting Wang",
            "Xiang Wei",
            "Xueying Zhang",
            "Yu Liu"
        ],
        "file_path": "data/sosym-all/s10270-024-01204-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling foundations for executable model-based testing of self-healing cyber-physical systems",
        "submission-date": "2018/01",
        "publication-date": "2018/11",
        "abstract": "Self-healing cyber-physical systems (SH-CPSs) detect and recover from faults by themselves at runtime. Testing such systems is challenging due to the complex implementation of self-healing behaviors and their interaction with the physical environment, both of which are uncertain. To this end, we propose an executable model-based approach to test self-healing behaviors under environmental uncertainties. The approach consists of a Modeling Framework of SH-CPSs (MoSH) and an accompanying Test Model Executor (TM-Executor). MoSH provides a set of modeling constructs and a methodology to specify executable test models, which capture expected system behaviors and environmental uncertainties. TM-Executor executes the test models together with the systems under test, to dynamically test their self-healing behaviors under uncertainties. We demonstrated the successful application of MoSH to specify 11 self-healing behaviors and 17 uncertainties for three SH-CPSs. The time spent by TM-Executor to perform testing activities was in the order of milliseconds, though the time spent was strongly correlated with the complexity of test models.",
        "keywords": [
            "Cyber-physical systems",
            "Self-healing",
            "Uncertainty",
            "Model execution",
            "Model-based testing"
        ],
        "authors": [
            "Tao Ma",
            "Shaukat Ali",
            "Tao Yue"
        ],
        "file_path": "data/sosym-all/s10270-018-00703-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalised EMFTVM bytecode language for sound veriﬁcation of model transformations",
        "submission-date": "2015/12",
        "publication-date": "2016/08",
        "abstract": "Abstract Model-driven engineering is an effective approach for addressing the full life cycle of software devel-opment. Model transformation is widely acknowledged as one of its central ingredients. With the increasing complexity of model transformations, it is urgent to develop veriﬁcation tools that prevent incorrect transformations from generating faulty models. However, the development of sound veriﬁca-tion tools is a non-trivial task, due to unimplementable or erroneous execution semantics encoded for the target model transformation language. In this work, we develop a for-malisation for the EMFTVM bytecode language by using the Boogie intermediate veriﬁcation language. It ensures the model transformation language has an implementable exe-cution semantics by reliably prototyping the implementation of the model transformation language. It also ensures the absence of erroneous execution semantics encoded for the target model transformation language by using a translation validation approach.",
        "keywords": [
            "MDE",
            "EMFTVM",
            "Boogie",
            "Model transformation veriﬁcation",
            "Intermediate veriﬁcation language"
        ],
        "authors": [
            "Zheng Cheng",
            "Rosemary Monahan",
            "James F. Power"
        ],
        "file_path": "data/sosym-all/s10270-016-0553-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "EMFTVM"
        }
    },
    {
        "title": "Design science in action: developing a modeling technique for eliciting requirements on business process management (BPM) tools",
        "submission-date": "2012/10",
        "publication-date": "2014/05",
        "abstract": "Selecting a suitable business process management (BPM) tool to build a business process support system for a particular business process is difﬁcult. There are a number of BPM tools on the market that are available as systems to install locally and as services in the cloud. These tools are based on different BPM paradigms (e.g., workﬂow or case management) and provide different capabilities (e.g., enforcement of the control ﬂow, shared spaces, or a collaborative environment). This makes it difﬁcult for an organization to select a tool that would ﬁt the business processes at hand. The paper suggests a solution for this problem. The core of the solution is a modeling technique for business processes for eliciting their requirements for a suitable BPM tool. It produces a high-level, business process model, called a “step-relationship” model that depicts the essential characteristics of a process in a paradigm-independent way. The solution presented in this paper has been developed based on the paradigm of design science research, and the paper discusses the research project from the design science perspective. The solution has been applied in two case studies in order to demonstrate its feasibility.",
        "keywords": [
            "Business process modeling",
            "Workﬂow",
            "Case management",
            "Shared space",
            "Design science"
        ],
        "authors": [
            "Ilia Bider",
            "Erik Perjons"
        ],
        "file_path": "data/sosym-all/s10270-014-0412-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction: Modeling competences in enterprise architecture: from knowledge, skills, and attitudes to organizational capabilities",
        "submission-date": "2024/05",
        "publication-date": "2024/05",
        "abstract": "In the published article Figs. 14, 15 and 16 were published wrong. During the proof correction stage typesetters did not update the correct ﬁgures provided by the author. The correct ﬁgures are given below: We apologize for the error. The correct ﬁgures are updated in the original publication.",
        "keywords": [],
        "authors": [
            "Rodrigo F. Calhau",
            "João Paulo A. Almeida",
            "Satyanarayana Kokkula",
            "Giancarlo Guizzardi"
        ],
        "file_path": "data/sosym-all/s10270-024-01182-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSyM Special Section on Software Engineering and Formal Methods",
        "submission-date": "2004/09",
        "publication-date": "2006/06",
        "abstract": "This section of “Software & Systems Modeling” contains three papers presenting current trends on the use of formal methods and software engineering for the development of complex distributed applications. These articles are based on presentations at SEFM 2004, the Second IEEE International Conference on Software Engineering and Formal Methods, that took place during 28–30 September 2004 in Beijing, China. The best papers of the conference were invited to prepare revised and extended versions for publication in this special section.",
        "keywords": [],
        "authors": [
            "J. Cuellar",
            "Z. Liu"
        ],
        "file_path": "data/sosym-all/s10270-006-0010-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Graph-based traceability: a comprehensive approach",
        "submission-date": "2009/01",
        "publication-date": "2009/11",
        "abstract": "In recent years, traceability has been globally accepted as being a key success factor of software development projects. However, the multitude of different, poorly integratedtaxonomies,approachesandtechnologiesimpedes the application of traceability techniques in practice. This paper presents a comprehensive view on traceability, pertaining to the whole software development process. Based on the state of the art, the ﬁeld is structured according to six speciﬁc activities related to traceability as follows: deﬁnition, recording, identiﬁcation, maintenance, retrieval, and utilization. Using graph technology, a comprehensive and seamless approach for supporting these activities is derived, combining them in one single conceptual framework. This approach supports the deﬁnition of metamodels for traceability information, recording of traceability information in graph-based repositories, identiﬁcation and maintenance of traceability relationships using transformations, as well as retrieval and utilization of traceability information using a graph query language. The approach presented here is applied in the context of the ReDSeeDS project (Requirements Driven Software Development System) that aims at requirements-based software reuse. ReDSeeDS makes use of traceability information to determine potentially reusable architectures, design, or code artifacts based on a given set of reusable requirements. The project provides case studies from different domains for the validation of the approach.",
        "keywords": [
            "Traceability",
            "Graph technology",
            "Model transformations",
            "Software engineering"
        ],
        "authors": [
            "Hannes Schwarz",
            "Jürgen Ebert",
            "Andreas Winter"
        ],
        "file_path": "data/sosym-all/s10270-009-0141-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on PoEM’2021",
        "submission-date": "2023/01",
        "publication-date": "2023/02",
        "abstract": "This guest editorial presents the special section of 14th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modeling (PoEM 2021). The best papers of PoEM 2021 were invited to be revised and signiﬁcantly expanded. Eight papers were ﬁnally accepted for publication in the special section. These papers are an excellent representation of the state of the art on Enterprise Modeling, showing as well the importance of applying the research contributions into practice.",
        "keywords": [
            "Enterprise modeling",
            "Conceptual modeling",
            "Modeling methods"
        ],
        "authors": [
            "Estefanía Serral",
            "Janis Stirna",
            "Jolita Ralyté",
            "Janis Grabis"
        ],
        "file_path": "data/sosym-all/s10270-023-01089-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the comprehension of workﬂows modeled with a precise style: results from a family of controlled experiments",
        "submission-date": "2013/03",
        "publication-date": "2013/11",
        "abstract": "In this paper, we present the results from a family of experiments conducted to assess whether the level of formality/precision in workﬂow modeling, based on UML activity diagrams, inﬂuences two aspects of construct comprehensibility: correctness of understanding and task completion time. In particular, we have considered two styles for workﬂow modeling with different levels of formality: a precise style (with speciﬁc rules and imposed constraints) and an ultra-light style (no rules, no imposed constraints). Experiments were conducted with 111 participants (Bachelor and Master students). In each experiment, participants accomplished comprehension tasks on two workﬂows, modeled either with the precise style or with a lighter variant. The main results from our data analysis can be summarized as follows: (i) all participants achieved a signiﬁcantly better comprehension of workﬂows written in the precise style, (ii) the style had no signiﬁcant impact on task completion time, (iii) more experienced participants beneﬁted more, with respect to less experienced ones, from the precise style, as for their correctness of understanding, and (iv) all participants found the precise style useful in comprehending workﬂows.",
        "keywords": [
            "Family of experiments",
            "Precise and Ultra-light styles",
            "UML activity diagrams",
            "Workﬂow modeling"
        ],
        "authors": [
            "Gianna Reggio",
            "Filippo Ricca",
            "Giuseppe Scanniello",
            "Francesco Di Cerbo",
            "Gabriella Dodero"
        ],
        "file_path": "data/sosym-all/s10270-013-0386-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Theme section on performance modelling and engineering of software and systems",
        "submission-date": "2017/08",
        "publication-date": "2017/09",
        "abstract": "The modelling of the performance of today’s computer systems is getting more and more challenging. While most models traditionally focused on utilization and response time of a single system, technologies such as cloud and containerization require new approaches to efficiently handle the complexity of current installations. The meaning of the word performance is constantly changing—currently covers additional non-functional aspects including security and reliability. In a world full of software systems composed of huge number of microservices running on cloud infrastructures, a single service is less important than in the traditional system architectures. In contrast to the classical systems, the entire network of containerized services needs to be efficiently operated, monitored and orchestrated. Containers are automatically deployed, scaled and operated by orchestration middleware, IaaS is re-arranging their resources dynamically, and several independent services might compete for the same resources. To optimize efficiency users, developers and operators need new approaches to model systems and their behaviour, allowing not only to evaluate the systems retrospectively, but also to take the right actions proactively in near real time.",
        "keywords": [],
        "authors": [
            "Catalina M. Lladó",
            "Kai Sachs"
        ],
        "file_path": "data/sosym-all/s10270-017-0624-7.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Modeling robustness behavior using aspect-oriented modeling to support robustness testing of industrial systems",
        "submission-date": "2010/12",
        "publication-date": "2011/06",
        "abstract": "Model-based robustness testing requires precise and complete behavioral, robustness modeling. For example, state machines can be used to model software behavior when hardware (e.g., sensors) breaks down and be fed to a tool to automate test case generation. But robustness behav-ior is a crosscutting behavior and, if modeled directly, often results in large, complex state machines. These in practice tend to be error prone and difﬁcult to read and understand. As a result, modeling robustness behavior in this way is not scalable for complex industrial systems. To overcome these problems, aspect-oriented modeling (AOM) can be employed to model robustness behavior as aspects in the form of state machines specifically designed to model robustness behavior. In this paper, we present a RobUstness Model-ing Methodology (RUMM) that allows modeling robustness behavior as aspects. Our goal is to have a complete and practical methodology that covers all features of state machines and aspect concepts necessary for model-based robustness testing. At the core of RUMM is a UML proﬁle (AspectSM) that allows modeling UML state machine aspects as UML state machines (aspect state machines). Such an approach, relying on a standard and using the target notation as the basis to model the aspects themselves, is expected to make Communicated by Dr. Jean-Michel Bruel.",
        "keywords": [
            "Aspect-oriented modeling",
            "UML state machines",
            "Robustness",
            "UML proﬁle",
            "Crosscutting behavior",
            "Robustness testing"
        ],
        "authors": [
            "Shaukat Ali",
            "Lionel C. Briand",
            "Hadi Hemmati"
        ],
        "file_path": "data/sosym-all/s10270-011-0206-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to the theme section on model-based engineering of smart systems",
        "submission-date": "2019/09",
        "publication-date": "2020/01",
        "abstract": "The term ‘smart’ is widely applied to products and systems that are enabled by, and depend upon, computing and communication technology to analyse and respond to changing conditions in their environment. The papers in this thematic section address some of these important and intriguing challenges.",
        "keywords": [],
        "authors": [
            "John Fitzgerald",
            "Fuyuki Ishikawa",
            "Peter Gorm Larsen"
        ],
        "file_path": "data/sosym-all/s10270-019-00758-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "FLAME: a formal framework for the automated analysis of software product lines validated by automated speciﬁcation testing",
        "submission-date": "2013/11",
        "publication-date": "2015/12",
        "abstract": "In a literature review on the last 20 years of automated analysis of feature models, the formalization of analysis operations was identiﬁed as the most relevant challenge in the ﬁeld. This formalization could provide very valuable assets for tool developers such as a precise deﬁnition of the analysis operations and, what is more, a reference implementation, i.e., a trustworthy, not necessarily efﬁcient implementation to compare different tools outputs. In this article, we present the FLAME framework as the result of facing this challenge. FLAME is a formal framework that can be used to formally specify not only feature models, but other variability modeling languages (VMLs) as well. This reusability is achieved by its two-layered architecture. The abstract foundation layer is the bottom layer in which all VML-independent analysis operations and concepts are speciﬁed. On top of the foundation layer, a family of characteristic model layers—one for each VML to be formally speciﬁed—can be developed by redeﬁning some abstract types and relations. The veriﬁcation and validation of FLAME has followed a process in which formal veriﬁcation has been performed traditionally by manual theorem proving, but validation has been performed by integrating our experience on metamorphic testing of variability analysis tools, something that has shown to be much more effective than manually designed test cases. To follow this automated, test-based validation approach, the speciﬁcation of FLAME, written in Z, was translated into Prolog and 20,000 random tests were automatically generated and executed. Tests results helped to discover some inconsistencies not only in the formal speciﬁcation, but also in the previous informal deﬁnitions of the analysis operations and in current analysis tools. After this process, the Prolog implementation of FLAME is being used as a reference implementation for some tool developers, some analysis operations have been formally speciﬁed for the ﬁrst time with more generic semantics, and more VMLs are being formally speciﬁed using FLAME.",
        "keywords": [
            "Formal speciﬁcation",
            "Speciﬁcation testing",
            "Software product lines",
            "Feature models"
        ],
        "authors": [
            "Amador Durán",
            "David Benavides",
            "Sergio Segura",
            "Pablo Trinidad",
            "Antonio Ruiz-Cortés"
        ],
        "file_path": "data/sosym-all/s10270-015-0503-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Handling index-out-of-bounds in safety-critical embedded C code using model-based development",
        "submission-date": "2017/03",
        "publication-date": "2018/10",
        "abstract": "Embedded C code for safety critical systems faces some substantial challenges: like every other embedded SW code it must be efﬁcient in terms of code size, data size and execution time, but it must also behave safely under all circumstances, without a user or operator who could handle the errors. One kind of problem is array accesses where the index is outside the speciﬁed value range. The C language does not specify the behaviour in such cases, which clearly violates the requirements for safe code. In this paper, the approach of the model-based development tool “ASCET” is explained, and the experiences of three case studies that describe the adoption of index protection by the users are presented.",
        "keywords": [
            "Domain-speciﬁc languages",
            "Functional safety",
            "Software adaptation",
            "Embedded software",
            "Automotive engineering"
        ],
        "authors": [
            "Gunter Blache"
        ],
        "file_path": "data/sosym-all/s10270-018-0697-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ModelSet: a dataset for machine learning in model-driven engineering",
        "submission-date": "2021/03",
        "publication-date": "2021/10",
        "abstract": "The application of machine learning (ML) algorithms to address problems related to model-driven engineering (MDE) is currently hindered by the lack of curated datasets of software models. There are several reasons for this, including the lack of large collections of good quality models, the difﬁculty to label models due to the required domain expertise, and the relative immaturity of the application of ML to MDE. In this work, we present ModelSet, a labelled dataset of software models intended to enable the application of ML to address software modelling problems. To create it we have devised a method designed to facilitate the exploration and labelling of model datasets by interactively grouping similar models using off-the-shelf technologies like a search engine. We have built an Eclipse plug-in to support the labelling process, which we have used to label 5,466 Ecore meta-models and 5,120 UML models with its category as the main label plus additional secondary labels of interest. We have evaluated the ability of our labelling method to create meaningful groups of models in order to speed up the process, improving the effectiveness of classical clustering methods. We showcase the usefulness of the dataset by applying it in a real scenario: enhancing the MAR search engine. We use ModelSet to train models able to infer useful metadata to navigate search results. The dataset and the tooling are available at https://ﬁgshare.com/s/5a6c02fa8ed20782935c and a live version at http://modelset.github.io.",
        "keywords": [
            "Dataset",
            "Machine learning",
            "Model-driven engineering"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Javier Luis Cánovas Izquierdo",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-021-00929-3.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Automated formal veriﬁcation of visual modeling languages by model checking",
        "submission-date": "2003/02",
        "publication-date": "2004/04",
        "abstract": "Graph transformation has recently become more and more popular as a general, rule-based visual speciﬁcation paradigm to formally capture (a) requirements or behavior of user models (on the model-level), and (b) the operational semantics of modeling languages (on the meta-level) as demonstrated by benchmark applications around the Uniﬁed Modeling Language (UML). The current paper focuses on the model checking-based automated formal veriﬁcation of graph transformation systems used either on the model-level or meta-level. We present a general translation that inputs (i) a meta-model of an arbitrary visual modeling language, (ii) a set of graph transformation rules that deﬁnes a formal operational semantics for the language, and (iii) an arbitrary well-formed model instance of the language and generates a transitions system (TS) that serve as the underlying mathematical speciﬁcation formalism of various model checker tools. The main theoretical beneﬁt of our approach is an optimization technique that projects only the dynamic parts of the graph transformation system into the target transition system, which results in a drastical reduction in the state space. The main practical beneﬁt is the use of existing back-end model checker tools, which directly provides formal veriﬁcation facilities (without additional eﬀorts required to implement an analysis tool) for many practical applications captured in a very high-level visual notation. The practical feasibility of the approach is demonstrated by modeling and analyzing the well-known veriﬁcation benchmark of dining philosophers both on the model and meta-level.",
        "keywords": [
            "Graph transformation",
            "Metamodeling",
            "Formal veriﬁcation",
            "Model checking",
            "Model transform-ation"
        ],
        "authors": [
            "D´aniel Varr´o"
        ],
        "file_path": "data/sosym-all/s10270-003-0050-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A collection operator for graph transformation",
        "submission-date": "2009/12",
        "publication-date": "2011/02",
        "abstract": "Algebraic graph transformation has a well-\nestablished theory and associated tools that can be used to\nperform model transformations. However, the lack of a con-\nstructtomatchandtransformcollectionsofsimilarsubgraphs\nmakes graph transformation complex or even impractical to\nuse in a number of transformation cases. This is addressed\nin this paper, by deﬁning a collection operator which is\npowerful, yet simple to model and understand. A rule can\ncontain multiple collection operators, each with lower and\nupper bound cardinalities, and the collection operators can be\nnested. An associated matching process dynamically builds a\ncollection free rule that enables us to reuse the existing graph\ntransformation apparatus. We present model transformation\nexamples from different modeling domains to illustrate the\nbeneﬁt of the approach.",
        "keywords": [
            "Graph transformation",
            "Model transformation",
            "Matching"
        ],
        "authors": [
            "Roy Grønmo",
            "Stein Krogdahl",
            "Birger Møller-Pedersen"
        ],
        "file_path": "data/sosym-all/s10270-011-0190-3.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for the special section on MODELS 2020",
        "submission-date": "2022/08",
        "publication-date": "2022/09",
        "abstract": "The MODELS conference series is the premier venue for model-based software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. This special section presents the nine articles that resulted from an invitation to authors of the best papers at MODELS 2020, followed by a full SoSyM review cycle.",
        "keywords": [],
        "authors": [
            "Silvia Abrahão",
            "Juan de Lara",
            "Houari Sahraoui",
            "Eugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-022-01044-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Example-driven modeling: on effects of using examples on structural model comprehension, what makes them useful, and how to create them",
        "submission-date": "2017/01",
        "publication-date": "2018/01",
        "abstract": "We present a controlled experiment for the empirical evaluation of example-driven modeling (EDM), an approach that systematically uses examples for model comprehension and domain knowledge transfer. We conducted the experiment with 26 graduate (Masters and Ph.D. level) and undergraduate (Bachelor level) students from electrical and computer engineering, computer science, and software engineering programs at the University of Waterloo. The experiment involves a domain model, with UML class diagrams representing the domain abstractions and UML object diagrams representing examples of using these abstractions. The goal is to provide empirical evidence of the effects of suitable examples on model comprehension, compared to having model abstractions only, by having the participants perform model comprehension tasks. Our results show that EDM is superior to having model abstractions only, with an improvement of 39% for diagram completeness, 33% for questions completeness, 71% for efﬁciency, and a reduction in the number of mistakes by 80%. We provide qualitative results showing that participants receiving model abstractions augmented with examples experienced lower perceived difﬁculty in performing the comprehension tasks, higher perceived conﬁdence in their tasks’ solutions, and asked 90% fewer clarifying domain questions. We also present participants’ feedback regarding the usefulness of the provided examples, their number and types, as well as the use of partial examples. We present a taxonomy of the different types of examples, explain their signiﬁcance, and propose guidelines for manual and automatic creation of useful examples.",
        "keywords": [
            "Software engineering",
            "Structural modeling",
            "Empirical study",
            "Example-driven modeling"
        ],
        "authors": [
            "Dina Zayan",
            "Atrisha Sarkar",
            "Michał Antkiewicz",
            "Rita Suzana Pitangueira Maciel",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-017-0652-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Fairness, assumptions, and guarantees for extended bounded response LTL+P synthesis",
        "submission-date": "2022/06",
        "publication-date": "2023/08",
        "abstract": "Realizability and reactive synthesis from temporal logics are fundamental problems in formal veriﬁcation. The complexity\nof these problems for linear temporal logic with past (LTL+P) led to the identiﬁcation of fragments with lower complexities\nand simpler algorithms. Recently, the logic of extended bounded response LTL+P(LTLEBR+Pfor short) has been introduced.\nIt allows one to express safety languages deﬁnable in LTL+Pand it is provided with an efﬁcient, fully symbolic algorithm for\nreactive synthesis. This paper features four related contributions. First, we introduce GR-EBR, an extension of LTLEBR+Pwith\nfairness conditions, assumptions, and guarantees that, on the one hand, allows one to express properties beyond the safety\nfragment and, on the other, it retains the efﬁciency of LTLEBR+Pin practice. Second, we the expressiveness of GR-EBRstarting\nfrom the expressiveness of its fragments. In particular, we prove that: (1) LTLEBR+Pis expressively complete with respect to\nthe safety fragment of LTL+P, (2) the removal of past operators from LTLEBR+Presults into a loss of expressive power, and\n(3) GR-EBRis expressively equivalent to the logic GR(1)of Bloem et al. Third, we provide a fully symbolic algorithm for\nthe realizability problem from GR-EBRspeciﬁcations, that reduces it to a number of safety subproblems. Fourth, to ensure\nsoundness and completeness of the algorithm, we propose and exploit a general framework for safety reductions in the context\nof realizability of (fragments of) LTL+P. The experimental evaluation shows promising results.",
        "keywords": [
            "Reactive synthesis",
            "Temporal logics",
            "Safety reductions",
            "Expressiveness"
        ],
        "authors": [
            "Alessandro Cimatti\nLuca Geatti\nNicola Gigante\nAngelo Montanari\nStefano Tonetta"
        ],
        "file_path": "data/sosym-all/s10270-023-01122-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Detecting feature interaction in CPL",
        "submission-date": "2003/02",
        "publication-date": "2005/11",
        "abstract": "This article addresses the problem of detecting feature interactions in the area of telephony systems design. The proposed approach consists of two phases: filtering and testing. The filtering phase detects possible interactions by identifying incoherencies in a logic specification of the main elements of the features, consisting of preconditions, triggers, results and constraints. If incoherencies are identified, then an interaction is suspected, test cases corresponding to the suspected interaction are generated and testing is applied to see if the interaction actually exists. Two case studies, carried out on established benchmarks, show that this approach gives good results in practice.",
        "keywords": [
            "Telephony software",
            "Feature interaction",
            "Detection method",
            "Formal techniques"
        ],
        "authors": [
            "Nicolas Gorse",
            "Luigi Logrippo",
            "Jacques Sincennes"
        ],
        "file_path": "data/sosym-all/s10270-005-0101-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Networcat: applying analysis techniques of shared memory software on message-passing distributed systems",
        "submission-date": "2024/02",
        "publication-date": "2025/02",
        "abstract": "Communicationmodelsareakeyaspectinthedesignandimplementationofdistributedsystemarchitectures.Applicationlogic\nmust consider the guarantees of these models, which fundamentally inﬂuence its correctness. Modern multi-core processor\narchitectures face a similar problem when it comes to accessing shared memory: the guarantees of an architecture have a\nfundamental impact on the observable behavior of software. The formalization of these guarantees in a declarative way has led\nto powerful tools and algorithms to deﬁne reusable constraints on patterns of memory access events and their relationships,\nenabling the efﬁcient description and automatic formal analysis of software properties with respect to a speciﬁc architecture.\nThe Cat memory modeling language provides a standard means of specifying these constraints. Despite the parallels, the\naxiomatic modeling and analysis of communication models in distributed systems remain a relatively unexplored area. In\nthis paper, we address this gap and demonstrate how communication models can be mapped to the Cat language. We create\na standard library of reusable patterns and demonstrate our approach, called NetworCat, on the simple examples of UDP\nand TCP, and we also present its applicability to the vastly conﬁgurable OMG-DDS service. This adaptation-based approach\nenables the use of ever-improving veriﬁcation tools built for shared memory concurrency on distributed systems. We believe\nthis not only beneﬁts distributed system analyses by broadening the toolset for veriﬁcation but also positively impacts the\nﬁeld of memory-model-aware veriﬁcation by widening its audience to another domain.",
        "keywords": [
            "Distributed systems",
            "Systems modeling",
            "Formal veriﬁcation",
            "Cat",
            "OMG-DDS"
        ],
        "authors": [
            "Levente Bajczi\nVince Molnár"
        ],
        "file_path": "data/sosym-all/s10270-024-01258-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A reference framework for process-oriented software development organizations",
        "submission-date": "2003/12",
        "publication-date": "2004/07",
        "abstract": "In this paper, a proposal of a generic framework for process-oriented software development organizations is presented. Additionally, the respective way of managing the process model, and the instantiation of their processes with the Rational Uniﬁed Process (RUP) disciplines, whenever they are available, or with other kind of processes is suggested. The proposals made here were consolidated with experiences from real projects and we report the main results from one of those projects.",
        "keywords": [
            "RUP",
            "Process-oriented organizations",
            "Software development process",
            "Business modelling"
        ],
        "authors": [
            "João M. Fernandes",
            "Francisco J. Duarte"
        ],
        "file_path": "data/sosym-all/s10270-004-0063-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Simulation of system architectures using optimization and machine learning: the state of the art and research opportunities",
        "submission-date": "2024/01",
        "publication-date": "2025/02",
        "abstract": "Most software-intensive systems present large and complex architectures, which should satisfy different quality attributes, such as performance, reliability, and security. Some of these attributes could only be measured at runtime, which is undesired, particularly for critical systems whose attributes should still be evaluated at design time to avoid failures at runtime and losses, including human lives. Simulation has been considered a powerful solution to predict and evaluate different architectural arrangements at design time and, combined with optimization and machine learning, and it can ﬁnd suitable or even optimal architectures. However, there is a lack of an overview of such combinations and how they can work better. This work presents the state of the art of simulation using optimization and/or machine learning techniques. For this, we examined the literature of 1,342 studies retrieved from three publications databases and systematically selected 87 studies and scrutinized them. There is a variety of combinations of simulation with different optimization and/or machine learning techniques, each requiring speciﬁc simulation models and simulators. At the same time, studies are still isolated, lacking maturity in the area and remaining important future work to discover the beneﬁts of such combinations.",
        "keywords": [
            "Optimization",
            "Machine learning",
            "Simulation",
            "System architecture"
        ],
        "authors": [
            "Wallace Manzano",
            "Valdemar Vicente Graciano Neto",
            "Thiago Bianchi",
            "Mohamad Kassab",
            "Elisa Yumi Nakagawa"
        ],
        "file_path": "data/sosym-all/s10270-025-01280-7.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A participative end-user method for multi-perspective business process elicitation and improvement",
        "submission-date": "2014/10",
        "publication-date": "2015/08",
        "abstract": "A business process can be characterized by multiple perspectives (intentional, organizational, operational, functional, interactional, informational, etc). Business process modeling must allow different stakeholders to analyze and represent process models according to these different perspectives. This representation is traditionally built using classical data acquisition methods together with a process representation language such as BPMN or UML. These techniques and specialized languages can easily become hard, complex and time consuming. In this paper, we propose ISEA, a participative end-user modeling approach that allows the stakeholders in a business process to collaborate together in a simple way to communicate and improve the business process elicitation in an accurate and understandable manner. Our approach covers the organizational perspective of business processes, exploits the information compiled during the elicitation of the organizational perspective and touches lightly an interactional perspective allowing users to create customized interface sketches to test the user interface navigability and the coherence within the processes. Thus, ISEA can be seen as a participative end-user modeling approach for business process elicitation and improvement.",
        "keywords": [
            "Business process management",
            "Requirements engineering",
            "Domain modeling",
            "User interfaces modeling",
            "Participative approach"
        ],
        "authors": [
            "Agnès Front",
            "Dominique Rieu",
            "Marco Santorum",
            "Fatemeh Movahedian"
        ],
        "file_path": "data/sosym-all/s10270-015-0489-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling big smart data",
        "submission-date": "2014/04",
        "publication-date": "2014/04",
        "abstract": "Large volumes of data are generated continuously by billions of human data producers, sensors, surveillance systems, communication devices and networks (e.g., the Internet). Proper analysis of this data can lead to new scientific insights, new products and services, more creative outputs (e.g., new recipes, music scores, fashion styles), improved performanceofbusinessandcivicorganizations,andtobetter informed government and non-government organizations. In other words, deriving information from these large volumes of data can lead to, among other things, smarter individuals capable of making scientific breakthroughs, producing innovative products, and making effective decisions.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-014-0409-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incorporating measurement uncertainty into OCL/UML primitive datatypes",
        "submission-date": "2018/12",
        "publication-date": "2019/07",
        "abstract": "The correct representation of the relevant properties of a system is an essential requirement for the effective use and wide adoption of model-based practices in industry. Uncertainty is one of the inherent properties of any measurement or estimation that is obtained in any physical setting; as such, it must be considered when modeling software systems deal with real data. Although a few modeling languages enable the representation of measurement uncertainty, these aspects are not normally incorporated into their type systems. Therefore, operating with uncertain values and propagating their uncertainty become cumbersome processes, which hinder their realization in real environments. This paper proposes an extension of OCL/UML primitive datatypes that enables the representation of the uncertainty that comes from physical measurements or user estimates into the models, together with an algebra of operations that are deﬁned for the values of these types.",
        "keywords": [
            "Measurement uncertainty",
            "OCL",
            "UML",
            "Primitive datatypes"
        ],
        "authors": [
            "Manuel F. Bertoa",
            "Loli Burgueño",
            "Nathalie Moreno",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-019-00741-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Suggesting model transformation repairs for rule-based languages using a contract-based testing approach",
        "submission-date": "2020/06",
        "publication-date": "2021/05",
        "abstract": "Model transformations play an essential role in most model-driven software projects. As the size and complexity of model transformations increase, their reuse, evolution and maintenance become a challenge. This work further details the Model Transformation TEst Speciﬁcation (MoTES) approach, which leverages contract-based model testing techniques to assist engineers in model transformation evolution and repairing. The main novelty of our approach is to use contract-based model transformation testing as a foundation to derive suggestions of concrete adaptation actions. MoTES uses contracts to specify the expected behaviour of the model transformation under test. These contracts are transformed into model transformations which act as oracles on input–output model pairs, previously generated by executing the transformation under test on provided input models. By further processing, the oracles’ output model, precision and recall metrics are calculated for every output pattern (testing results). These metrics are then categorised to increase the user’s ability to interpret and act on them. The MoTES approach deﬁnes 8 cases for precision and recall values classiﬁcation (test result cases). As traceability information is retained from transformation rules to each output pattern, it is possible to classify each transformation rule involved according to its impact on the metrics, e.g. the number of true positives generated. The MoTES approach deﬁnes 37 cases for these classiﬁcations, with each one linked to a particular (abstract) action suggested on a rule, such as relaxation of the rules. A comprehensive evaluation of this approach is also presented, consisting of three case studies. Two previous case studies performed over two model transformations (UML2ER and E2M) are replicated to contrast MoTES with an existing model transformation fault localisation approach. An additional case study presents how MoTES helps with the evolution of an existing model transformation in the context of a reverse engineering project. Main evaluation results show that our approach can not only detect the errors introduced in the transformations but also localise the faulty rule and suggest the proper repair actions, which signiﬁcantly reduce testers’ effort. From a quantitative perspective, in the third case study, MoTES was able to indicate one faulty rule from 19 possibilities for each result case and suggest one or two repair actions from 6 possibilities for each faulty rule.",
        "keywords": [
            "Model Transformation",
            "Evolution",
            "Testing",
            "Repairing",
            "Testing Oracle",
            "Adaptations",
            "Veriﬁcation",
            "Fault Localisation"
        ],
        "authors": [
            "Roberto Rodriguez-Echeverria",
            "Fernando Macías",
            "Adrian Rutle",
            "José M. Conejero"
        ],
        "file_path": "data/sosym-all/s10270-021-00891-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section of BPMDS’2012: artefacts and processes for business process modeling and management",
        "submission-date": "2012/06",
        "publication-date": "2014/06",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a two-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of Information Systems theory. The goals, format and history of BPMDS can be found on the web site http://www.bpmds.org/. According to Seligman et al. [1], an information systems engineering method has four pillars: a way of thinking, a way of modeling, a way of working and a way of supporting. The framework presented in [2] was inspired from this work and aims to evaluate the capacity of business process engineering methods to design ﬂexible business process models. According to this framework (i) the way of thinking verbalizes the assumptions and viewpoints of the method on the kinds of problem domains, solutions and modelers; (ii) the way of modeling provides information on the modeling concepts, on their properties and on their relationships; gives a formalism and notation to express business process models; (iii) the way of working structures the way in which business process models are designed; deﬁnes the possible tasks to be performed as part of the design and development S. Nurcan (B) University Paris 1 Panthéon Sorbonne, Paris, France e-mail: nurcan@univ-paris1.fr R. Schmidt Munich University of Applied Sciences, Munich, Germany e-mail: Rainer.Schmidt@hm.edu process; provides heuristics on how these tasks should be performed; (iv) the way of supporting refers to the tools that support the design and development of business process models and offers a repository to store and to exploit them. We observe in the literature and also in BPMDS working conferences series that research developing way of work-ings and methodological guidelines for designing appropri-ate and valuable business process models is spreading more and more. This special section presents ﬁve of them.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-014-0419-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models in software engineering – an introduction",
        "submission-date": "2002/10",
        "publication-date": "2003/02",
        "abstract": "Modelling is a concept fundamental for software engineering. In this paper, the word is defined and discussed from various perspectives. The most important types of models are presented, and examples are given. Models are very useful, but sometimes also dangerous, in particular to those who use them unconsciously. Such problems are shown. Finally, the role of models in software engineering research is discussed.",
        "keywords": [
            "Models",
            "Software engineering",
            "Metaphors",
            "SESAM"
        ],
        "authors": [
            "Jochen Ludewig"
        ],
        "file_path": "data/sosym-all/s10270-003-0020-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Behavioral interfaces for executable DSLs",
        "submission-date": "2019/03",
        "publication-date": "2020/04",
        "abstract": "Executable domain-speciﬁc languages (DSLs) enable the execution of behavioral models. While an execution is mostly driven by the model content (e.g., control structures), many use cases require interacting with the running model, such as simulating scenarios in an automated or interactive way, or coupling the model with other models of the system or environment. The management of these interactions is usually hardcoded into the semantics of the DSL, which prevents its reuse for other DSLs and the provision of generic interaction-centric tools (e.g., event injector). In this paper, we propose a metalanguage for complementing the deﬁnition of executable DSLs with explicit behavioral interfaces to enable external tools to interact with executed models in a uniﬁed way. We implemented the proposed metalanguage in the GEMOC Studio and show how behavioral interfaces enable the realization of tools that are generic and thus usable for different executable DSLs.",
        "keywords": [
            "Language engineering",
            "Domain-speciﬁc language",
            "Model execution"
        ],
        "authors": [
            "Dorian Leroy",
            "Erwan Bousse",
            "Manuel Wimmer",
            "Tanja Mayerhofer",
            "Benoit Combemale",
            "Wieland Schwinger"
        ],
        "file_path": "data/sosym-all/s10270-020-00798-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying UML/MARTE on industrial projects: challenges, experiences, and guidelines",
        "submission-date": "2013/06",
        "publication-date": "2014/03",
        "abstract": "ModelingandAnalysisofReal-TimeandEmbed- ded Systems (MARTE) is a Uniﬁed Modeling Language (UML) proﬁle, which has been developed to model concepts speciﬁc to Real-Time and Embedded Systems (RTES). In the last 5years, we have applied UML/MARTE to three distinct industrial problems in three industry sectors: architecture modeling and conﬁguration of large-scale and highly conﬁgurableintegratedcontrolsystems,model-basedrobust- nesstestingofcommunication-intensivesystems,andmodel- based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experience of solving these problems by applying UML/MARTE on four industrial case studies. We highlight the challenges we faced with respect to the industrial adoption of MARTE. Based on our combined experience, we derive a framework to guide practitioners for future applications of UML/MARTE in an industrial context. The framework provides a set of detailed guidelines that help reduce the gap between the modeling notations and real-world industrial application needs.",
        "keywords": [
            "UML",
            "MARTE",
            "Real-Time Embedded Systems",
            "Architecture Modeling",
            "Model-based Testing",
            "Industrial Case Studies"
        ],
        "authors": [
            "Muhammad Zohaib Iqbal",
            "Shaukat Ali",
            "Tao Yue",
            "Lionel Briand"
        ],
        "file_path": "data/sosym-all/s10270-014-0405-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An exception handling framework for case management",
        "submission-date": "2020/11",
        "publication-date": "2022/04",
        "abstract": "In order to achieve their business goals, organizations heavily rely on the operational excellence of their business processes. In traditional scenarios, business processes are usually well-structured, clearly specifying when and how certain tasks have to be executed. Flexible and knowledge-intensive processes are gathering momentum, where a knowledge worker drives the execution of a process case and determines the exact process path at runtime. In the case of an exception, the knowledge worker decides on an appropriate handling. While there is initial work on exception handling in well-structured business processes, exceptions in case management have not been sufﬁciently researched. This paper proposes an exception handling framework for stage-oriented case management languages, namely Guard Stage Milestone Model, Case Management Model and Notation, and Fragment-based Case Management. The effectiveness of the framework is evaluated with two real-world use cases showing that it covers all relevant exceptions and proposed handling strategies.",
        "keywords": [
            "Exception handling",
            "Knowledge-intensive processes",
            "Flexible processes",
            "Case management"
        ],
        "authors": [
            "Kerstin Andree",
            "Sven Ihde",
            "Mathias Weske",
            "Luise Pufahl"
        ],
        "file_path": "data/sosym-all/s10270-022-00993-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Discovering architecture-aware and sound process models of multi-agent systems: a compositional approach",
        "submission-date": "2021/05",
        "publication-date": "2022/05",
        "abstract": "A process model discovered from an event log of a multi-agent system often does not fully cover certain viewpoints of its architecture. We consider those concerned with the structure of a model explicitly reﬂecting agent behavior and interactions. Thedirectdiscoveryfromaneventlogofamulti-agentsystemmayresultinanunclearmodelstructureandover-generalizations of agent behavior. We suggest applying a compositional approach that yields architecture-aware process models of multi-agent systems. An event log of a multi-agent system is ﬁltered by the behavior of individual agents. Then, a multi-agent system model is a composition of agent models discovered from ﬁltered logs. We use an intermediate model, called an interface pattern, specifying agent interactions and representing the architecture of a multi-agent system. We design a collection of speciﬁc interface patterns modeling typical agent interactions. An interface pattern provides an abstract speciﬁcation of interactions and has a part corresponding to the behavior of each agent. We use structural transformations to map agent models discovered from ﬁltered logs on the respective parts in an interface pattern. If such a mapping exists, we guarantee that a composition of agent models preserves their soundness. We conduct a series of experiments to evaluate the compositional approach. Experimental results conﬁrm the improvement in the structure of process models discovered using the compositional approach compared to those discovered directly from event logs.",
        "keywords": [
            "Multi-agent systems",
            "Event logs",
            "Process mining",
            "Process discovery",
            "Petri nets",
            "Composition",
            "Transformations",
            "Interface patterns"
        ],
        "authors": [
            "Roman Nesterov",
            "Luca Bernardinello",
            "Irina Lomazova",
            "Lucia Pomello"
        ],
        "file_path": "data/sosym-all/s10270-022-01008-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Uncertainty representation in software models: a survey",
        "submission-date": "2020/06",
        "publication-date": "2021/01",
        "abstract": "This paper provides a comprehensive overview and analysis of research work on how uncertainty is currently represented in software models. The survey presents the deﬁnitions and current research status of different proposals for addressing uncertainty modeling and introduces a classiﬁcation framework that allows to compare and classify existing proposals, analyze their current status and identify new trends. In addition, we discuss possible future research directions, opportunities and challenges.",
        "keywords": [
            "Software models",
            "Uncertainty",
            "Modeling languages",
            "UML",
            "Systematic literature review"
        ],
        "authors": [
            "Javier Troya",
            "Nathalie Moreno",
            "Manuel F. Bertoa",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-020-00842-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Requirements document relations\nA reuse perspective on traceability through standards",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Relations between requirements are part of nearly every requirements engineering approach. Yet, relations of views, such as requirements documents, are scarcely considered. This is remarkable as requirements documents and their structure are a key factor in requirements reuse, which is still challenging. Explicit formalized relations between documents can help to ensure consistency, improve completeness, and facilitate review activities in general. For example, this is relevant in space engineering, where many challenges related to complex document dependencies occur: 1. Several contractors contribute to a project. 2. Requirements from standards have to be applied in several projects. 3. Requirements from previous phases have to be reused. We exploit the concept of “layered traceability”, explicitly considering documents as views on sets of individual requirements and speciﬁc traceability relations on and between all of these representation layers. Different types of relations and their dependencies are investigated with a special focus on requirement reuse through standards and formalized in an Object-Role Modelling (ORM) conceptual model. Automated analyses of requirement graphs based on this model are able to reveal document inconsistencies. We show examples of such queries in Neo4J/Cypher for the EagleEye case study. This work aims to be a step toward a better support to handle highly complex requirement document dependencies in large projects with a special focus on requirements reuse and to enable automated quality checks on dependent documents to facilitate requirements reviews.",
        "keywords": [
            "Requirement document relations",
            "Requirement reuse",
            "Standards",
            "Space engineering requirements",
            "ECSS",
            "Traceability"
        ],
        "authors": [
            "Katharina Großer",
            "Volker Riediger",
            "Jan Jürjens"
        ],
        "file_path": "data/sosym-all/s10270-021-00958-y.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Aspectual templates in UML\nEnhancing the semantics of UML templates in OCL",
        "submission-date": "2013/10",
        "publication-date": "2015/04",
        "abstract": "UML Templates allow to capture reusable\nmodels through parameterization. The construct is general\nenough to be used in many ways, ranging from the rep-\nresentation of generic components (such as Java generics\nor C++ templates) to aspectual usage, including pattern-,\naspect- and view-oriented modeling. We concentrate on this\nlast usage and so-called aspectual templates which require\nthat parameters must form a model of systems in which\nto inject new functionalities. Starting from this strict con-\nstraint, we derive an in-depth semantic enhancement of the\nstandard. It is formalized as a fully UML-compliant inter-\npretation in OCL of the template construct and its binding\nmachanism. In particular, this aspectual interpretation must\nbe ensured in case of partial binding (not all parameters are\nvalued). Partial binding of UML is a powerful technique\nwhich allows to obtain richer templates from the compo-\nsition of other ones. As a major result, the present semantic\nenhancement is consistent with this capacity so that partial\nbinding of aspectual templates produces aspectual templates.\nFinally, at an operational level, an algorithm for aspectual\ntemplate (partial) bindingoperation is formulated and conse-\nquent reusable technology made available in EMF (Eclipse\nModeling Framework) is presented.",
        "keywords": [
            "Model templates",
            "UML",
            "OCL",
            "Metamodeling",
            "Aspects",
            "Patterns",
            "Template composition"
        ],
        "authors": [
            "Gilles Vanwormhoudt",
            "Olivier Caron",
            "Bernard Carré"
        ],
        "file_path": "data/sosym-all/s10270-015-0463-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MDWA: a model-driven Web augmentation approach—coping with client- and server-side support",
        "submission-date": "2018/08",
        "publication-date": "2020/02",
        "abstract": "Web augmentation is a set of techniques allowing users to deﬁne and execute software which is dependent on the presentation layer of a concrete Web page. Through the use of specialized Web augmentation artifacts, the end users may satisfy several kinds of requirements that were not considered by the analysts, developers and stakeholders that built the application. Although some augmentation approaches are contemplating a server-side counterpart (to support aspects such as collaboration or cross-browser session management), the augmentation artifacts are usually purely client-side. The server-side support increases the capabilities of the augmentations, since it may allow sharing information among users and devices. So far, this support is often deﬁned and developed in an ad hoc way. Although it is clear that server-side support brings new possibilities, it is also true that developing and deploying server-side Web applications is a challenging task that end users hardly may handle. This work presents a novel approach for designing Web augmentation applications based on client-side and server-side components. We propose a model-driven approach that raises the abstraction level of both, client- and server-side developments. We provide a set of tools for designing the composition of the core application with new features on the back-end and the augmentation of pages in the front-end. The usability and the value of the produced augmentations have been evaluated through two experiments involving 30 people in total.",
        "keywords": [
            "Model-driven Web engineering",
            "Augmentation",
            "Web development",
            "Separation of concern"
        ],
        "authors": [
            "Matias Urbieta",
            "Sergio Firmenich",
            "Gabriela Bosetti",
            "Pedro Maglione",
            "Gustavo Rossi",
            "Miguel Angel Olivero"
        ],
        "file_path": "data/sosym-all/s10270-020-00779-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Constraint-based test generation for automotive operating systems",
        "submission-date": "2014/02",
        "publication-date": "2015/01",
        "abstract": "This work suggests a method for systematically constructing a software-level environment model for safety checking automotive operating systems by introducing a constraint speciﬁcation language, OSEK_CSL. OSEK_CSL is designed to specify the usage constraints of automotive operating systems using a pre-deﬁned set of constraint types identiﬁed from the international standard OSEK/VDX. Each constraint speciﬁed in OSEK_CSL is interpreted as either a regular language or a context-free language that can be checked by a ﬁnite automaton or a pushdown automaton. The set of usage constraints is used to systematically classify the universal usage model of OSEK-/VDX-based operating systems and to generate test sequences with varying degrees of constraint satisfaction using LTL model checking. With pre-deﬁned constraint patterns and the full support of automation, test engineers can choose the degree of constraint satisfaction and generate test cases using combinatorial intersections of selected constraints that cover all corner cases classiﬁed by constraints. A series of experiments on an open-source automotive operating system show that our approach finds safety issuesmoreeffectivelythanconventionalspeciﬁcation-based testing, scenario-based testing, and conformance testing.",
        "keywords": [
            "Veriﬁcation",
            "Constraint speciﬁcation",
            "Operating system",
            "Automotive software",
            "Test generation"
        ],
        "authors": [
            "Yunja Choi",
            "Taejoon Byun"
        ],
        "file_path": "data/sosym-all/s10270-014-0449-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The MDENET education platform: zero-install directed activities for learning MDE",
        "submission-date": "2024/06",
        "publication-date": "2025/04",
        "abstract": "Setting up and conﬁguring model-driven engineering (MDE) tools is not straightforward because the MDE tooling landscape is highly fragmented and because many MDE tools are research prototypes with limited documentation. This creates signiﬁcant accidental complexity for learners of MDE, who have to overcome installation and conﬁguration hurdles before they can even begin to focus on the core MDE concepts they should be learning. This is further complicated by the complexity of modern MDE tools, which can overwhelm new learners, making it difﬁcult for them to work out what they should do next to achieve a given goal. To address these challenges, we have developed a web-based playground platform that enables learners to engage with MDE learning activities without the need to install anything. The playground metaphor allows teachers to expose only those functionalities directly required for the completion of a particular learning activity. We present the general architecture of the platform, our approach to the declarative integration of new MDE tools, and the way in which teachers can ﬂexibly and declaratively deﬁne new MDE learning activities. We have used our platform in a range of different contexts, from live tutorials and 10-week university courses, to developing documentation webpages for MDE tools. We describe examples of such uses, showcasing the ﬂexible conﬁgurability of the platform for different types of activities and contexts.",
        "keywords": [
            "MDE",
            "Education",
            "Online",
            "No installation",
            "Playground"
        ],
        "authors": [
            "Steffen Zschaler",
            "Will Barnett",
            "Artur Boronat",
            "Antonio Garcia-Dominguez",
            "Dimitris Kolovos"
        ],
        "file_path": "data/sosym-all/s10270-025-01292-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using internal domain-speciﬁc languages to inherit tool support and modularity for model transformations",
        "submission-date": "2016/03",
        "publication-date": "2017/01",
        "abstract": "Model-driven engineering (MDE) has proved to be a useful approach to cope with today’s ever-growing complexity in the development of software systems; nevertheless, it is not widely applied in industry. As suggested by multiple studies, tool support is a major factor for this lack of adoption. Inparticular,thedevelopmentofmodeltransformationslacks good tool support. Additionally, modularization techniques are inevitable for the development of larger model transfor- mations to keep them maintainable. Existing tools for MDE, in particular model transformation approaches, are often developed by small teams and cannot keep up with advanced tool support for mainstream general-purpose programming languages, such as IntelliJ or Visual Studio. Internal DSLs are a promising solution to these problems. In this paper, we investigate the impact of design decisions of an internal DSL to the reuse of tool support and modularization concepts from the host language. We validate our ﬁndings in terms of understandability, applicability, tool support, and extensibil- ity using three case studies from academia, a model-driven engineering platform, and the industrial automation domain where we apply an implementation of an internal model transformation language on the .NET platform. The results conﬁrm the value of inherited modularity and tool support while conciseness and understandability are still competi- tive.",
        "keywords": [
            "Model-driven engineering",
            "Model transformation",
            "Domain-speciﬁc language",
            "Tool support",
            "Extensibility"
        ],
        "authors": [
            "Georg Hinkel",
            "Thomas Goldschmidt",
            "Erik Burger",
            "Ralf Reussner"
        ],
        "file_path": "data/sosym-all/s10270-017-0578-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Mapping feature models onto domain models: ensuring consistency of configured domain models",
        "submission-date": "2011/06",
        "publication-date": "2012/12",
        "abstract": "We present an approach to model-driven software product line engineering which is based on feature models and domain models. A feature model describes both common and varying properties of the instances of a software product line. The domain model is composed of a structural model (package and class diagrams) and a behavioral model (story diagrams). Features are mapped onto the domain model by annotating elements of the domain model with features. An element of a domain model is specific to the features included in its feature annotation. An instance of the product line is defined by a set of selected features (a feature configuration). A configuration of the domain model is built by excluding all elements whose feature set is not included in the feature configuration. To ensure consistency of the configured domain model, we define constraints on the annotations of inter-dependent domain model elements. These constraints guarantee that a model element may be selected only when the model elements are also included on which it depends. Violations of dependency constraints may be removed automatically with the help of an error repair tool which propagates features to dependent model elements.",
        "keywords": [
            "Model-driven software product line engineering",
            "Feature models",
            "Domain models",
            "Feature mappings",
            "Dependency constraints"
        ],
        "authors": [
            "Thomas Buchmann",
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-012-0305-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From types to type requirements: genericity for model-driven engineering",
        "submission-date": "2011/03",
        "publication-date": "2011/11",
        "abstract": "Model-driven engineering (MDE) is a software engineering paradigm that proposes an active use of models during the development process. This paradigm is inherently type-centric, in the sense that models and their manipulation are deﬁned over the types of speciﬁc meta-models. This fact hinders the reuse of existing MDE artefacts with other meta-models in new contexts, even if all these meta-models share common characteristics. To increase the reuse opportunities of MDE artefacts, we propose a paradigm shift from type-centric to requirement-centric speciﬁcations by bringing genericity into models, meta-models and model management operations. For this purpose, we introduce so-called concepts gathering structural and behavioural requirements for models and meta-models. In this way, model management operations are deﬁned over concepts, enabling the application of the operations to any meta-model satisfying the requirements imposed by the concept. Model templates rely on concepts to deﬁne suitable interfaces, hence enabling the definition of reusable model components. Finally, similar to mixinlayers,templatescanbedeﬁnedatthemeta-modellevel as well, to deﬁne languages in a modular way, as well as layers of functionality to be plugged-in into other meta-models. These ideas have been implemented in MetaDepth, a multi-level meta-modelling tool that integrates action languages from the Epsilon family for model management and code generation.",
        "keywords": [
            "Model-driven engineering",
            "Language engineering",
            "Meta-modelling",
            "Genericity",
            "Reutilization"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra"
        ],
        "file_path": "data/sosym-all/s10270-011-0221-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon (ETL)"
        }
    },
    {
        "title": "The Unit-B method: reﬁnement guided by progress concerns",
        "submission-date": "2013/11",
        "publication-date": "2015/03",
        "abstract": "We present Unit-B, a formal method inspired by Event-B and UNITY. Unit-B aims at the stepwise design of software systems, satisfying safety and liveness properties. The method features the novel notion of coarse and ﬁne schedules, a generalisation of weak and strong fairness for specifying events’ scheduling assumptions. Based on events schedules, we propose proof rules to reason about progress properties and a reﬁnement order preserving both liveness and safety properties. We illustrate our approach by an example to show that systems development can be driven by not only safety but also liveness requirements.",
        "keywords": [
            "Progress properties",
            "Reﬁnement",
            "Fairness",
            "Scheduling",
            "Unit-B",
            "Proof-based formal methods",
            "Veriﬁcation of cyber-physical systems"
        ],
        "authors": [
            "Simon Hudon",
            "Thai Son Hoang",
            "Jonathan S. Ostroff"
        ],
        "file_path": "data/sosym-all/s10270-015-0456-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CalcGraph: taming the high costs of deep learning using models",
        "submission-date": "2020/07",
        "publication-date": "2022/10",
        "abstract": "Models based on differential programming, like deep neural networks, are well established in research and able to outperform\nmanually coded counterparts in many applications. Today, there is a rising interest to introduce this ﬂexible modeling to\nsolve real-world problems. A major challenge when moving from research to application is the strict constraints on computa-\ntional resources (memory and time). It is difﬁcult to determine and contain the resource requirements of differential models,\nespecially during the early training and hyperparameter exploration stages. In this article, we address this challenge by intro-\nducing CalcGraph, a model abstraction of differentiable programming layers. CalcGraph allows to model the computational\nresources that should be used and then CalcGraph’s model interpreter can automatically schedule the execution respecting\nthe speciﬁcations made. We propose a novel way to efﬁciently switch models from storage to preallocated memory zones\nand vice versa to maximize the number of model executions given the available resources. We demonstrate the efﬁciency of\nour approach by showing that it consumes less resources than state-of-the-art frameworks like TensorFlow and PyTorch for\nsingle-model and multi-model execution.",
        "keywords": [
            "Differentiable programming",
            "Computational graph model",
            "Edge AI"
        ],
        "authors": [
            "Joe Lorentz",
            "Thomas Hartmann",
            "Assaad Moawad",
            "Francois Fouquet",
            "Djamila Aouada",
            "Yves Le Traon"
        ],
        "file_path": "data/sosym-all/s10270-022-01052-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scope in model transformations",
        "submission-date": "2015/10",
        "publication-date": "2016/08",
        "abstract": "Abstract A notion of hierarchical scope is commonplace in\nmany programmatic systems. In the context of model, and in\nparticular graph transformation, the use of scope can present\ntwo advantages: ﬁrst, more natural expression of transforma-\ntion application locality, and second, reduction in the number\nof match candidates, promising performance improvements.\nPrevious work on scope, however, has focused on applying\nit to rule hierarchies, which reduces the number of matches\nperformed, but not necessarily the cost of ﬁnding a single\nmatch. In this paper we deﬁne and explore a hierarchical\nscope formalism applied to the input graph, with associ-\nated modiﬁcations to the transformation rule deﬁnition. We\nthen experimentally evaluate the beneﬁts and challenges\nof our scoped model transformations in the state-of-the-\nart graph rewriting tool GrGen and our research-oriented,\nmeta-modeling and rule-based model transformation tool\nAToMPM. We use a non-trivial “ﬁre spreading” simulation\nCommunicated by Mr. Juan de Lara.",
        "keywords": [
            "Scope",
            "Graph scoping",
            "Graph grammar",
            "Rule-based model transformations",
            "Search plans",
            "Efﬁcient\npattern matching"
        ],
        "authors": [
            "M¯aris Jukšs",
            "Clark Verbrugge",
            "Maged Elaasar",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-016-0555-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Code generation for classical-quantum software systems modeled in UML",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Quantum computing is gaining an increasing interest since it can solve certain problems exponentially faster than classical computing. Thus, many organizations are researching and launching investments for integrating quantum software into their existing systems. Software modernization (as based on Model-Driven Engineering) has been proposed to migrate from/to the so-called hybrid software systems, which integrate classical and quantum software. In that process, both, reverse engineering and restructuring phases, have already been investigated. However, forward engineering phase for generating hybrid source code from high-level design models has not yet been addressed. Thus, this research proposes a quantum code generation technique from extended UML design models. It consists of a set of Model-to-Text transformations (deﬁned through Epsilon Generation Language) to generate both Python and Qiskit code, which, respectively, integrate classical and quantum code. The transformation has been validated through a multi-case study with 7 hybrid software systems modeled in UML, which demonstrated that the transformation is effective and efﬁcient. The implication of this work is that the software modernization process for hybrid software systems can be completed by tackling forward engineering phase, and that Model-Driven Engineering can therefore globally facilitate industry adoption of quantum software.",
        "keywords": [
            "Quantum software",
            "Code generation",
            "MDE",
            "UML",
            "EGL"
        ],
        "authors": [
            "Luis Jiménez-Navajas",
            "Ricardo Pérez-Castillo",
            "Mario Piattini"
        ],
        "file_path": "data/sosym-all/s10270-024-01259-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon Generation Language"
        }
    },
    {
        "title": "Turning event logs into process movies: animating what has really happened",
        "submission-date": "2013/12",
        "publication-date": "2014/09",
        "abstract": "Today’s information systems log vast amounts of data. These collections of data (implicitly) describe events (e.g. placing an order or taking a blood test) and, hence, provide information on the actual execution of business processes. The analysis of such data provides an excellent starting point for business process improvement. This is the realm of process mining, an area which has provided a repertoire of many analysis techniques. Despite the impressive capabilities of existing process mining algorithms, dealing with the abundance of data recorded by contemporary systems and devices remains a challenge. Of particular importance is the capability to guide the meaningful interpretation of “oceans of data” by process analysts. To this end, insights fromtheﬁeldofvisualanalyticscanbeleveraged.Thisarticle proposes an approach where process states are reconstructed from event logs and visualised in succession, leading to an animated history of a process. This approach is customisable in how a process state, partially deﬁned through a collec-tion of activity instances, is visualised: one can select a map and specify a projection of events on this map based on the properties of the events. This paper describes a comprehensive implementation of the proposal. It was realised using the open-source process mining framework ProM. Moreover, this paper also reports on an evaluation of the approach conducted with Suncorp, one of Australia’s largest insurance companies.",
        "keywords": [
            "Process mining",
            "Visual analytics",
            "Event-log animation",
            "Process visualisation"
        ],
        "authors": [
            "Massimiliano de Leoni",
            "Suriadi Suriadi",
            "Arthur H. M. ter Hofstede",
            "Wil M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-014-0432-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An analysis of capability meta‑models for expressing dynamic business transformation",
        "submission-date": "2019/10",
        "publication-date": "2020/12",
        "abstract": "Environmental dynamism is gaining ground as a driving force for enterprise transformation. To address the changes, the capabilities of digital enterprises need to adapt. Capability modeling can facilitate this process of transformation. However, a plethora of approaches for capability modeling exist. This study explores how concepts relevant to change have been implemented in the meta-models of these approaches, aiming to visualize relationships among change-related concepts, and identify ways to improve capability modeling toward a more efficient depiction of capability change. The concepts are visualized in concept maps, and a framework is developed to assist the classification of concepts relevant to change functions. Similarities and differences among the existing models are discussed, leading to suggestions toward improvements of capability modeling for capability adaptation.",
        "keywords": [
            "Capability",
            "Enterprise modeling",
            "Change",
            "Adaptability",
            "Transformation"
        ],
        "authors": [
            "Georgios Koutsopoulos",
            "Martin Henkel",
            "Janis Stirna"
        ],
        "file_path": "data/sosym-all/s10270-020-00843-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating the analysis of multiple non-functional properties in model-driven engineering",
        "submission-date": "2021/06",
        "publication-date": "2021/11",
        "abstract": "This paper discusses the progress made so far and future challenges in integrating the analysis of multiple Non-Functional Properties (NFP) (such as performance, schedulability, reliability, availability, scalability, security, safety, and maintainability) intotheModel-DrivenEngineering(MDE)process.Thegoalistoguidethedesignchoicesfromanearlystageandtoensurethat the system under construction will meet its non-functional requirements. The evaluation of the NFPs considered in this paper uses various kinds of NFP analysis models (also known as quality models) based on existent formalisms and tools developed over the years. Examples are queueing networks, stochastic Petri nets, stochastic process algebras, Markov chains, fault trees, probabilistic time automata, etc. In the MDE context, these models are automatically derived by model transformations from the software models built for development. Developing software systems that exhibit a good trade-off between multiple NFPs is difﬁcult because the design of the software under construction and its underlying platforms have a large number of degrees of freedom spanning a very large discontinuous design space, which cannot be exhaustively explored. Another challenge in balancing the NFPs of a system under construction is due to the fact that some NFPs are conﬂicting—when one gets better the other gets worse—so an appropriate software process is needed to evaluate and balance all the non-functional requirements. The integration approach discussed in this paper is based on an ecosystem of inter-related heterogeneous modeling artifacts intended to support the following features: feedback of analysis results, consistent co-evolution of the software and analysis models, cross-model traceability, incremental propagation of changes across models, (semi)automated software process steps, and metaheuristics for reducing the design space size to be explored.",
        "keywords": [
            "Non-functional properties",
            "Model-driven engineering",
            "Analysis integration"
        ],
        "authors": [
            "Dorina C. Petriu"
        ],
        "file_path": "data/sosym-all/s10270-021-00953-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A graph-theoretic method for the inductive development of reference process models",
        "submission-date": "2014/10",
        "publication-date": "2015/09",
        "abstract": "Business process management is one of the most widely discussed topics in information systems research. As process models advance in both complexity and maturity, reference models, serving as reusable blueprints for the development of individual models, gain more and more importance. Only a few business domains have access to commonly accepted reference models, so there is a widespread need for the development of new ones. This article describes a new inductive approach for the development of reference models, based on existing individual models from the respective domain. It employs a graph-based paradigm, exploiting the underlying graph structures of process models by identifying frequent common subgraphs of the individual models, analyzing their order relations, and merging them into a new model. This newly developed approach is outlined and evaluated in this contribution. It is applied in three different case studies and compared to other approaches to the inductive development of reference models in order to highlight its characteristics as well as assets and drawbacks.",
        "keywords": [
            "Reference modeling",
            "Frequent subgraphs",
            "Order matrices",
            "Inductive development of reference models"
        ],
        "authors": [
            "Jana-Rebecca Rehse",
            "Peter Fettke",
            "Peter Loos"
        ],
        "file_path": "data/sosym-all/s10270-015-0490-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deontic BPMN: a powerful extension of BPMN with a trusted model transformation",
        "submission-date": "2012/04",
        "publication-date": "2013/03",
        "abstract": "The Business Process Model and Notation (BPMN) is a widely-used standard for process modelling. A drawback of BPMN, however, is that modality is implicitly expressed through the structure of the process ﬂow but not directly within the corresponding activity. Thus, an extension of BPMN with deontic logic has been proposed in previous work, called Deontic BPMN. Deontic BPMN reduces the structural complexity of the process ﬂow and increases the readability by explicitly highlighting obligatory and permissibleactivities.Inaddition,analgebraicgraphtransformation from a subset of BPMN to Deontic BPMN, called Deontic BpmnGTS, has been deﬁned. The goal of the current research is to show that DeonticBpmnGTS is terminating and confluent, resulting in a globally deterministic transformation. Moreover, the semantic equivalence of BPMN models and the respective Deontic BPMN models is proven based on Abstract State Machines (ASMs). Thus, DeonticBpmnGTS can be called a trusted model transformation.",
        "keywords": [
            "Business process modelling",
            "BPMN",
            "Deontic logic",
            "Graph transformation",
            "Deterministic transformation",
            "Semantic analysis"
        ],
        "authors": [
            "Christine Natschläger",
            "Felix Kossak",
            "Klaus-Dieter Schewe"
        ],
        "file_path": "data/sosym-all/s10270-013-0329-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven engineering city spaces via bidirectional model transformations",
        "submission-date": "2020/03",
        "publication-date": "2021/02",
        "abstract": "Engineering cyber-physical systems inhabiting contemporary urban spatial environments demands software engineering facilities to support design and operation. Tools and approaches in civil engineering and architectural informatics produce artifacts that are geometrical or geographical representations describing physical spaces. The models we consider conform to the CityGML standard; although relying on international standards and accessible in machine-readable formats, such physical space descriptions often lack semantic information that can be used to support analyses. In our context, analysis as commonly understood in software engineering refers to reasoning on properties of an abstracted model—in this case a city design. We support model-based development, firstly by providing a way to derive analyzable models from CityGML descriptions, and secondly, we ensure that changes performed are propagated correctly. Essentially, a digital twin of a city is kept synchronized, in both directions, with the information from the actual city. Specifically, our formal programming technique and accompanying technical framework assure that relevant information added, or changes applied to the domain (resp. analyzable) model are reflected back in the analyzable (resp. domain) model automatically and coherently. The technique developed is rooted in the theory of bidirectional transformations, which guarantees that synchronization between models is consistent and well behaved. Produced models can bootstrap graph-theoretic, spatial or dynamic analyses. We demonstrate that bidirectional transformations can be achieved in practice on real city models.",
        "keywords": [
            "Bidirectional model transformations",
            "Model-driven engineering",
            "CityGML",
            "Digital twins"
        ],
        "authors": [
            "Ennio Visconti",
            "Christos Tsigkanos",
            "Zhenjiang Hu",
            "Carlo Ghezzi"
        ],
        "file_path": "data/sosym-all/s10270-020-00851-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A generic model decomposition technique and its application to the Eclipse modeling framework",
        "submission-date": "2012/08",
        "publication-date": "2013/06",
        "abstract": "Model-driven software development aims at easing the process of software development by using models as primary artifacts. Although less complex than the real systems, they are based on models tend to be complex nevertheless, thus making the task of handling them non-trivial in many cases. In this paper, we propose a generic model decomposition technique to facilitate model management by decomposing complex models into smaller sub-models that conform to the same metamodel as the original model. The technique is based upon a formal foundation that consists of a formal capturing of the concepts of models, metamodels, and model conformance; a formal constraint language based on EssentialOCL; and a set of formally proved properties of the technique. We organize the decomposed sub-models in a mathematical structure as a lattice, and design a linear-time algorithm for constructing this decomposition. The generic model decomposition technique is applied to the Eclipse modeling framework, and the result is used to build a solution to a specific model comprehension problem of Ecore models based upon model pruning. We report two case studies of the model comprehension method: one in BPMN and the other in fUML.",
        "keywords": [
            "MDE",
            "EMF",
            "Model decomposition",
            "Model comprehension",
            "Linear-time algorithm",
            "Sub-model lattice",
            "OCL",
            "EssentialOCL",
            "BPMN",
            "fUML"
        ],
        "authors": [
            "Qin Ma",
            "Pierre Kelsen",
            "Christian Glodt"
        ],
        "file_path": "data/sosym-all/s10270-013-0348-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Survey and classiﬁcation of model transformation tools",
        "submission-date": "2017/05",
        "publication-date": "2018/03",
        "abstract": "Model transformation lies at the very core of model-driven engineering, and a large number of model transformation languages and tools have been proposed over the last few years. These tools can be used to develop, transform, merge, exchange, compare, and verify models and metamodels. In this paper, we present a comprehensive catalog of existing metamodel-based transformation tools and compare them using a qualitative framework. We begin by organizing the 60 tools we identiﬁed into a general classiﬁcation based on the transformation approach used. We then compare these tools using a number of particular facets, where each facet belongs to one of six different categories and may contain several attributes. The results of the study are discussed in detail and made publicly available in a companion website with a capability to search for tools using the speciﬁed facets as search criteria. Our study provides a thorough picture of the state-of-the-art in model transformation techniques and tools. Our results are potentially beneﬁcial to many stakeholders in the modeling community, including practitioners, researchers, and transformation tool developers.",
        "keywords": [
            "Model-driven development",
            "Model transformation tools",
            "Metamodel",
            "Classiﬁcation",
            "Survey"
        ],
        "authors": [
            "Naﬁseh Kahani",
            "Mojtaba Bagherzadeh",
            "James R. Cordy",
            "Juergen Dingel",
            "Daniel Varró"
        ],
        "file_path": "data/sosym-all/s10270-018-0665-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Experimental evaluation of a novel equivalence class partition testing strategy",
        "submission-date": "2016/07",
        "publication-date": "2017/03",
        "abstract": "In this paper, a complete model-based equivalence class testing strategy recently developed by the authors is experimentally evaluated. This black-box strategy applies to deterministic systems with inﬁnite input domains and ﬁnite internal state and output domains. It is complete with respect to a given fault model. This means that conforming behaviours will never be rejected, and all non-conforming behaviours inside a given fault domain will be uncovered. We investigate the question how this strategy performs for systems under test whose behaviours lie outside the fault domain. Furthermore, a strategy extension is presented, that is based on randomised data selection from input equivalence classes. While this extension is still complete with respect to the given fault domain, it also promises a higher test strength when applied against members outside this domain. This is conﬁrmed by an experimental evaluation that compares mutation coverage achieved by the original and the extended strategy with the coverage obtained by random testing. For mutation generation, not only typical software errors, but also critical HW/SW integration errors are considered. The latter can be caused by mismatches between hardware and software design, even in the presence of totally correct software.",
        "keywords": [
            "Model-based testing",
            "Equivalence class partition testing",
            "Adaptive random testing",
            "SysML",
            "SystemC",
            "State transition systems"
        ],
        "authors": [
            "Felix Hübner",
            "Wen-ling Huang",
            "Jan Peleska"
        ],
        "file_path": "data/sosym-all/s10270-017-0595-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Trade-off analysis for SysML models using decision points and CSPs",
        "submission-date": "2018/02",
        "publication-date": "2019/01",
        "abstract": "The expected beneﬁts of model-based systems engineering (MBSE) include assistance to the system designer in ﬁnding the set of optimal architectures and making trade-off analysis. Design objectives such as cost, performance and reliability are often conﬂicting. The SysML-based methods OOSEM and the ARCADIA method focus on the design and analysis of one alternative of the system. They freeze the topology and the execution platform before optimization starts. Further, their limitation quickly appears when a large number of alternatives were evaluated. The paper avoids these problems and improves trade-off analysis in a MBSE approach by combining the SysML modeling language and so-called “decision points.” An enhanced SysML model with decision points shows up alternatives for component redundancy and instance selection and allocation. The same SysML model is extended with constraints and objective functions using an optimization context and parametric diagrams. Then, a representation of a constraint satisfaction multi-criteria objective problem is generated and solved with a combination of solvers. A demonstrator implements the proposed approach into an Eclipse plug-in; it uses the Papyrus and CSP solvers, both are open-source tools. A case study illustrates the methodology: a mission controller for an Unmanned Aerial Vehicle that includes a stereoscopic camera sensor module.",
        "keywords": [
            "MBSE",
            "Optimization",
            "SysML",
            "CSP",
            "Papyrus",
            "System engineering",
            "Optimal architecture design",
            "Decision points"
        ],
        "authors": [
            "Patrick Leserf",
            "Pierre de Saqui-Sannes",
            "Jérôme Hugues"
        ],
        "file_path": "data/sosym-all/s10270-019-00717-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An Analysis of the Three Types of AASs and their Feasibility for Digital Twin Engineering",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Engineering digital twins is a software and systems engineering challenge for which no systematic approach exists. The\nAsset Administration Shell is becoming a popular foundation for digital twins in Industry 4.0 and it comes in different types\nthat support the engineering of different kinds and parts of digital twins. We investigate how it supports realizing common\nrequirements for digital twins. To this end, we investigate how each of the three Asset Administration Shell types can\ncontribute to the systematic engineering of speciﬁc components of digital twins. Therefore, we analyzed popular deﬁnitions\nand conceptual models of digital twins and extracted requirements that at least two of them share. We compare the resulting\nrequirements with Asset Administration Shells of different types and conclude with open challenges in the implementation\nof digital twins with this technology. This supports practitioners and researchers in identifying the most suitable type of Asset\nAdministration Shell for their speciﬁc digital twin engineering needs and identiﬁes gaps worthy of future research toward a\nsystematic engineering of digital twins.",
        "keywords": [
            "Asset administration shell",
            "Digital twin",
            "Requirements",
            "Manufacturing"
        ],
        "authors": [
            "Jingxi Zhang",
            "Carsten Ellwein",
            "Malte Heithoff",
            "Judith Michael",
            "Andreas Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-024-01255-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Hazard-driven realization views for Component Fault Trees",
        "submission-date": "2019/06",
        "publication-date": "2020/03",
        "abstract": "Traditionally, the preferred means of documentation used by safety engineers have been sheets- and text-based solutions. However, in the last decades, the introduction of model-driven engineering in conjunction with Component-Based Design has been influencing the way safety engineers perform their tasks; especially in the area of fault analysis, model-driven approaches have been developed aimed at coupling fault trees with architecture models. Doing this fosters communication between engineers, may reduce design effort, and makes artifacts easier to maintain and reuse. In this paper, we want to move forward in this direction and take another step in the modeling of Component Fault Trees in combination with the modeling of the architecture design. We propose a hazard-centric approach for the definition of multiple realization views for fault analysis using Component Fault Trees. The approach is composed of a modeling method and a tool solution. We illustrate our approach with a real-life example from the automotive industry.",
        "keywords": [
            "Model-driven engineering",
            "Component-based",
            "Hazard-centric",
            "Component Fault Trees",
            "Realization view"
        ],
        "authors": [
            "David Santiago Velasco Moncada"
        ],
        "file_path": "data/sosym-all/s10270-020-00792-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Standards harmonization: theory and practice",
        "submission-date": "2011/06",
        "publication-date": "2011/08",
        "abstract": "As software engineering (and other) standards are\ndeveloped over a period of years or decades, the suite of\nstandards thus developed often begins to lose any cohesion\nthat it originally possessed. This has led to discussions in\nthe standards communities of possible collaborative devel-\nopment, interoperability and harmonization of their existing\nstandards. Here, I assess how such harmonization efforts may\nbe aided by recent research results to create better quality\nstandards to replace the status quo.",
        "keywords": [
            "Standards",
            "Modelling",
            "Metamodels",
            "Software development",
            "Quality"
        ],
        "authors": [
            "B. Henderson-Sellers"
        ],
        "file_path": "data/sosym-all/s10270-011-0213-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards a model-driven engineering approach for the assessment\nof non-functional properties using multi-formalism",
        "submission-date": "2016/02",
        "publication-date": "2018/02",
        "abstract": "Model-driven techniques can be used to automatically produce formal models from different views of a system realised by\nusing several modelling languages and notations. Speciﬁcations are transformed into formal models so facilitating the analysis\nof complex system for design, validation or veriﬁcation purposes. However, no single formalism suits for representing all\nsystem’s views. In particular, the assessment of non-functional properties often requires integrated modelling approaches. The\nultimate goal of the research work described in this paper is to develop a comprehensive, theoretical and practical framework\nable to support the development and the integration of new or existing model-driven approaches for the automatic generation\nof multi-formalism models. This paper deﬁnes the core theoretical ideas on which the framework is based and demonstrates\ntheir concrete applicability to the development of a multi-formalism approach for performability assessment.",
        "keywords": [
            "Multi-formalism",
            "UML proﬁle",
            "Performability",
            "Model-driven engineering",
            "Generalised Stochastic Petri Nets",
            "Repairable fault trees"
        ],
        "authors": [
            "Simona Bernardi",
            "Stefano Marrone",
            "José Merseguer",
            "Roberto Nardone",
            "Valeria Vittorini"
        ],
        "file_path": "data/sosym-all/s10270-018-0663-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Where does model-driven engineering help? Experiences from three industrial cases",
        "submission-date": "2010/09",
        "publication-date": "2011/10",
        "abstract": "There have been few experience reports from industry on how Model-Driven Engineering (MDE) is applied and what the beneﬁts are. This paper summarizes the experiences of three large industrial participants in a European research project with the objective of developing techniques and tools for applying MDE on the development of large and complex software systems. The participants had varying degrees of previous experience with MDE. They found MDE to be particularly useful for providing abstractions of complex systems at multiple levels or from different viewpoints, for the development of domain-speciﬁc models that facilitate communication with non-technical experts, for the purposes of simulation and testing, and for the consumptionofmodelsforanalysis,suchasperformance-related decision support and system design improvements. From the industrial perspective, a methodology is considered to be use-ful and cost-efﬁcient if it is possible to reuse solutions in multiple projects or products. However, developing reusable solutions required extra effort and sometimes had a negative impact on the performance of tools. While the companies identiﬁed several beneﬁts of MDE, merging different tools with one another in a seamless development environment required several transformations, which increased the required implementation effort and complexity. Additionally, user-friendliness of tools and the provision of features for managing models of complex systems were identiﬁed as crucial for a wider industrial adoption of MDE.",
        "keywords": [
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Simulation",
            "Experience report",
            "Eclipse",
            "Complex systems"
        ],
        "authors": [
            "Parastoo Mohagheghi",
            "Wasif Gilani",
            "Alin Stefanescu",
            "Miguel A. Fernandez",
            "Bjørn Nordmoen",
            "Mathias Fritzsche"
        ],
        "file_path": "data/sosym-all/s10270-011-0219-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Unique identiﬁcation of elements in evolving software models",
        "submission-date": "2011/03",
        "publication-date": "2013/02",
        "abstract": "Evolving models are often managed in ﬁle-based software conﬁguration management systems. This causes the identiﬁcationproblem:ifthemodelelementsarenotassigned with globally unique identiﬁers, we cannot identify them over time. However, if such identiﬁers would be given, they can be misleading because the elements to which they are assigned might change completely. As a consequence, evo- lution becomes incomprehensible, partial transformation is hampered, and sufﬁcient management of inter-model relationships (e.g. traceability links) is impeded. This article presents an approach to identify model elements or even complete model fragments over time. It establishes a ﬁne- grained history representation to describe model evolution. The representation contains identiﬁcation links between the elements of different model revisions allowing us to identify elements of a given revision in other revisions or variants of the model. Due to the explicit expression of model evo- lution, it further enables the capturing of changes that have been applied to the ﬁne-grained elements inside a model.",
        "keywords": [
            "Model evolution",
            "Traceability",
            "Identiﬁcation"
        ],
        "authors": [
            "Sven Wenzel"
        ],
        "file_path": "data/sosym-all/s10270-012-0311-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An integrated conceptual model for information system security risk management supported by enterprise architecture management",
        "submission-date": "2016/09",
        "publication-date": "2018/02",
        "abstract": "Risk management is today a major steering tool for any organisation wanting to deal with information system (IS) security. However, IS security risk management (ISSRM) remains a difﬁcult process to establish and maintain, mainly in a context of multi-regulations with complex and inter-connected IS. We claim that a connection with enterprise architecture management (EAM) contributes to deal with these issues. A ﬁrst step towards a better integration of both domains is to deﬁne an integrated EAM-ISSRM conceptual model. This paper is about the elaboration and validation of this model. To do so, we improve an existing ISSRM domain model, i.e. a conceptual model depicting the domain of ISSRM, with the concepts of EAM. The validation of the EAM-ISSRM integrated model is then performed with the help of a validation group assessing the utility and usability of the model.",
        "keywords": [
            "Risk management",
            "Security",
            "Enterprise architecture",
            "ArchiMate"
        ],
        "authors": [
            "Nicolas Mayer\nJocelyn Aubert\nEric Grandry\nChristophe Feltus\nElio Goettelmann\nRoel Wieringa"
        ],
        "file_path": "data/sosym-all/s10270-018-0661-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for EMMSAD’2019 special section",
        "submission-date": "2020/11",
        "publication-date": "2021/01",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) conference series organized 24 events from 1996 to 2019, associated with CAiSE (Conference on Advanced Information Systems Engineering). From 2009, EMMSAD has become a two-days working conference.From2017,EMMSADbestpapersareinvitedtosubmit extended versions for considering their publication in the Journal of Software and Systems Modeling (SoSyM). The main topics of the EMMSAD series have the focus on models and modeling methods for software and information systems development, requirements engineering, enterprise modeling and architecture, and business process management. The conferencefurtheraddressesevaluationofmodelingmethods through a variety of empirical and nonempirical approaches. The aims, topics, and history of EMMSAD can be also found on its website at http://www.emmsad.org/. ",
        "keywords": [],
        "authors": [
            "Iris Reinhartz-Berger",
            "Jelena Zdravkovic"
        ],
        "file_path": "data/sosym-all/s10270-020-00845-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models@run.time: a guided tour of the state of the art and research challenges",
        "submission-date": "2018/02",
        "publication-date": "2019/01",
        "abstract": "More than a decade ago, the research topic models@run.time was coined. Since then, the research area has received increasing\nattention. Given the proliﬁc results during these years, the current outcomes need to be sorted and classiﬁed. Furthermore,\nmany gaps need to be categorized in order to further develop the research topic by experts of the research area but also\nnewcomers. Accordingly, the paper discusses the principles and requirements of models@run.time and the state of the art of\nthe research line. To make the discussion more concrete, a taxonomy is deﬁned and used to compare the main approaches and\nresearch outcomes in the area during the last decade and including ancestor research initiatives. We identiﬁed and classiﬁed 275\npapers on models@run.time, which allowed us to identify the underlying research gaps and to elaborate on the corresponding\nresearch challenges. Finally, we also facilitate sustainability of the survey over time by offering tool support to add, correct\nand visualize data.",
        "keywords": [
            "Models@run.time",
            "Self-reﬂection",
            "Causal connection",
            "Systematic literature review"
        ],
        "authors": [
            "Nelly Bencomo",
            "Sebastian Götz",
            "Hui Song"
        ],
        "file_path": "data/sosym-all/s10270-018-00712-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Speciﬁcation and automated validation of staged reconﬁguration processes for dynamic software product lines",
        "submission-date": "2014/10",
        "publication-date": "2015/05",
        "abstract": "Dynamic software product lines (DSPLs) propose elaborated design and implementation principles for engineering highly conﬁgurable runtime-adaptive systems in a sustainable and feature-oriented way. For this, DSPLs add to classical software product lines (SPL) the notions of (1) staged (pre-)conﬁgurations with dedicated binding times for each individual feature, and (2) continuous run-time reconﬁgurations of dynamic features throughout the entire product life cycle. Especially in the context of safety-and mission-critical systems, the design of reliable DSPLs requires capabilities for accurately specifying and validating arbitrary complex constraints among conﬁguration parame-ters and/or respective reconﬁguration options. Compared to classical SPL domain analysis which is usually based on Boolean constraint solving, DSPL validation, therefore, further requires capabilities for checking temporal properties of reconﬁguration processes. In this article, we present a comprehensive approach for modeling and automatically verifying essential validity properties of staged reconﬁgura-tion processes with complex binding time constraints during DSPL domain engineering. The novel modeling concepts introduced are motivated by (re-)conﬁguration constraints apparent in a real-world industrial case study from the automation engineering domain, which are not properly expressibleandanalyzableusingstate-of-the-artSPLdomain modeling approaches. We present a prototypical tool imple-mentation based on the model checker SPIN and present evaluation results obtained from our industrial case study, demonstrating the applicability of the approach.",
        "keywords": [
            "Dynamic software product lines",
            "Staged conﬁguration",
            "Model-based domain engineering and validation",
            "Model checking"
        ],
        "authors": [
            "Malte Lochau",
            "Johannes Bürdek",
            "Stefan Hölzle",
            "Andy Schürr"
        ],
        "file_path": "data/sosym-all/s10270-015-0470-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending single- to multi-variant model transformations by trace-based propagation of variability annotations",
        "submission-date": "2019/07",
        "publication-date": "2020/03",
        "abstract": "Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.",
        "keywords": [
            "Model transformation",
            "Software product line",
            "Annotative variability"
        ],
        "authors": [
            "Bernhard Westfechtel",
            "Sandra Greiner"
        ],
        "file_path": "data/sosym-all/s10270-020-00791-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An executable formal semantics for UML-RT",
        "submission-date": "2012/08",
        "publication-date": "2014/02",
        "abstract": "We propose a formal semantics for UML-RT, a UML proﬁle for real-time and embedded systems. The formal semantics is given by mapping UML-RT models into a language called kiltera, a real-time extension of the π-calculus. Previous attempts to formalize the semantics of UML-RT have fallen short by considering only a very small subset of the language and providing fundamentally incomplete semantics based on incorrect assumptions, such as a one-to-one correspondence between “capsules” and threads. Our semantics is novel in several ways: (1) it deals with both state machine diagrams and capsule diagrams; (2) it deals with aspects of UML-RT that have not been formalized before, such as thread allocation, service provision points, and service access points; (3) it supports an action language; and (4) the translation has been implemented in the form of a transformation from UML-RT models created with IBM’s RSA-RTE tool, into kiltera code. To our knowledge, this is the most comprehensive formal semantics for UML-RT to date.",
        "keywords": [
            "UML-RT",
            "RTE",
            "Modelling",
            "Semantics"
        ],
        "authors": [
            "Ernesto Posse",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-014-0399-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using language workbenches and domain-speciﬁc languages for safety-critical software development",
        "submission-date": "2017/11",
        "publication-date": "2018/05",
        "abstract": "Language workbenches support the efﬁcient creation, integration, and use of domain-speciﬁc languages. Typically, they\nexecute models by code generation to programming language code. This can lead to increased productivity and higher\nquality. However, in safety-/mission-critical environments, generated code may not be considered trustworthy, because of\nthe lack of trust in the generation mechanisms. This makes it harder to justify the use of language workbenches in such an\nenvironment. In this paper, we demonstrate an approach to use such tools in critical environments. We argue that models\ncreated with domain-speciﬁc languages are easier to validate and that the additional risk resulting from the transformation to\ncode can be mitigated by a suitably designed transformation and veriﬁcation architecture. We validate the approach with an\nindustrial case study from the healthcare domain. We also discuss the degree to which the approach is appropriate for critical\nsoftware in space, automotive, and robotics systems.",
        "keywords": [
            "Domain-speciﬁc languages",
            "Safety-critical software development",
            "Case study",
            "Language workbenches"
        ],
        "authors": [
            "Markus Voelter",
            "Bernd Kolb",
            "Klaus Birken",
            "Federico Tomassetti",
            "Patrick Alff",
            "Laurent Wiart",
            "Andreas Wortmann",
            "Arne Nordmann"
        ],
        "file_path": "data/sosym-all/s10270-018-0679-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending the Uniﬁed Modeling Language for ontology development",
        "submission-date": "2002/02",
        "publication-date": "2002/12",
        "abstract": "There is rapidly growing momentum for web enabled agents that reason about and dynamically integrate the appropriate knowledge and services at run-time. The dynamic integration of knowledge and services depends on the existence of explicit declarative semantic models (ontologies). We have been building tools for ontology development based on the Uniﬁed Modeling Language (UML). This allows the many mature UML tools, models and expertise to be applied to knowledge representation systems, not only for visualizing complex ontologies but also for managing the ontology development process. UML has many features, such as proﬁles, global modularity and extension mechanisms that are not generally available in most ontology languages. However, ontology languages have some features that UML does not support. Our paper iden-tiﬁes the similarities and diﬀerences (with examples) between UML and the ontology languages RDF and DAML+OIL. To reconcile these diﬀerences, we pro-pose a modiﬁcation to the UML metamodel to address some of the most problematic diﬀerences. One of these is the ontological concept variously called a property, relation or predicate. This notion corresponds to the UML concepts of association and attribute. In ontology languages properties are ﬁrst-class modeling elem-ents, but UML associations and attributes are not ﬁrst-class. Our proposal is backward-compatible with existing UML models while enhancing its viability for ontology modeling. While we have focused on RDF and DAML+OIL in our research and development activities, the same issues apply to many of the knowledge represen-tation languages. This is especially the case for semantic network and concept graph approaches to knowledge representations.",
        "keywords": [
            "Ontology",
            "Semantic web",
            "Agents",
            "OO modeling",
            "UML",
            "RDF",
            "DAML"
        ],
        "authors": [
            "Kenneth Baclawski",
            "Mieczyslaw K. Kokar",
            "Paul A. Kogut",
            "Lewis Hart",
            "Jeﬀrey Smith",
            "Jerzy Letkowski",
            "Pat Emery"
        ],
        "file_path": "data/sosym-all/s10270-002-0008-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On challenges of model transformation from UML to Alloy",
        "submission-date": "2008/03",
        "publication-date": "2008/12",
        "abstract": "The Uniﬁed Modeling Language (UML) is the de facto language used in the industry for software speciﬁcations. Once an application has been speciﬁed, Model Driven Architecture (MDA) techniques can be applied to generate code from such speciﬁcations. Since implementing a sys-tem based on a faulty design requires additional cost and effort, it is important to analyse the UML models at ear-lier stages of the software development lifecycle. This paper focuses on utilizing MDA techniques to deal with the analysis of UML models and identify design faults within a spec-iﬁcation. Specifically, we show how UML models can be automatically transformed into Alloy which, in turn, can be automatically analysed by the Alloy Analyzer. The proposed approach relies on MDA techniques to transform UML mod-els to Alloy. This paper reports on the challenges of the model transformation from UML class diagrams and OCL to Alloy. Those issues are caused by fundamental differences in the design philosophy of UML and Alloy. To facilitate better the representation of Alloy concepts in the UML, the paper draws on the lessons learnt and presents a UML proﬁle for Alloy.",
        "keywords": [],
        "authors": [
            "Kyriakos Anastasakis",
            "Behzad Bordbar",
            "Geri Georg",
            "Indrakshi Ray"
        ],
        "file_path": "data/sosym-all/s10270-008-0110-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section of BPMDS’2013: coping with complexity in business processes",
        "submission-date": "2015/03",
        "publication-date": "2015/05",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a 2-day working conference attached to Conference on Advanced Information Systems Engineering (CAiSE). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of information systems theory. Today, business processes have to cope with increasing complexity in several areas. This special section is targeted at both researchers and practitioners in the information systems community with a focus on “Coping with Complexity in Business Processes”.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-015-0468-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Redesign of UML class diagrams: a formal approach",
        "submission-date": "2005/05",
        "publication-date": "2007/11",
        "abstract": "Contracts provide a precise way of specifying object-oriented systems. When a class structure is modiﬁed, the corresponding contracts must be modiﬁed accordingly. This paper presents a method of transforming contracts, which allows the extension of a mapping deﬁned on a few model elements, to—what we call—an interpretation function, and to use this function to automatically translate OCL-constraints. Interestingly, such functions preserve reasoning using propositional calculi, resolution, equations, and induction. Interpretation functions can be used to trace model elements throughout multiple redesigns of UML class diagrams in both the forward, and the backward direction. The applicability of our approach is demonstrated in several examples, including some of Fowler’s refactoring patterns.",
        "keywords": [
            "UML",
            "OCL",
            "Formal methods",
            "Refactoring",
            "Requirements tracing"
        ],
        "authors": [
            "Piotr Kosiuczenko"
        ],
        "file_path": "data/sosym-all/s10270-007-0068-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistency management in industrial continuous model-based development settings: a reality check",
        "submission-date": "2021/10",
        "publication-date": "2022/04",
        "abstract": "This article presents the state of practice of consistency management in thirteen industrial model-based development settings. Our analysis shows a tight coupling between adopting shorter development cycles and increasingly pressing consistency management challenges. We ﬁnd that practitioners desire to adopt shorter development cycles, but immature modeling practices slow them down. We describe the different patterns that emerge from the various industrial settings. There is an opportunity for researchers to provide practitioners with a migration path toward practices that enable more automated consistency management, and ultimately, continuous model-based development.",
        "keywords": [
            "Model-based development",
            "Consistency management",
            "Agile"
        ],
        "authors": [
            "Robbert Jongeling",
            "Federico Ciccozzi",
            "Jan Carlson",
            "Antonio Cicchetti"
        ],
        "file_path": "data/sosym-all/s10270-022-01000-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based requirements speciﬁcation of real-time systems with UML, SysML and MARTE",
        "submission-date": "2015/09",
        "publication-date": "2016/04",
        "abstract": "Activities of speciﬁcation, analysis and design of real-time systems (RTS) are highly dependent on an effec-tive understanding of the application domain and on the thorough representation of their basic requirements. Model-based approaches using modeling languages such as UML are often applied to contribute to handle complexity of RTS development. However, UML alone does not com-pletely represent important features associated with these systems, such as relationship with hardware elements and an effective representation of timing constraints. This article explores the combined use of UML and its proﬁles SysML and MARTE for modeling hardware and software requirements of RTS, with application to a case of controlling urban road trafﬁc. The SysML proﬁle alone does not present the representation of temporal, behavioral and performance requirements. The MARTE proﬁle provides key resources to specify non-functional requirements for RTS, in addition to a clear description of the various relevant aspects of require-Communicated by Prof. Jean-Michel Bruel. ments deﬁnition of RTS, as for instance, temporal aspects and constraints. The main objective is to present the com-binedapplicationofSysMLwithMARTEstereotypes,which enables the speciﬁcation of different features of individual software requirements. Thus, in addition to the factors men-tioned above, we can say that the proposed approach has an important role to specify RTS at different levels of detail and levels of abstraction.",
        "keywords": [
            "Real-time systems",
            "Requirements engineering",
            "UML",
            "SysML",
            "MARTE"
        ],
        "authors": [
            "Fabíola Gonçalves C. Ribeiro",
            "Carlos E. Pereira",
            "Achim Rettberg",
            "Michel S. Soares"
        ],
        "file_path": "data/sosym-all/s10270-016-0525-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for the special section on MODELS 2022",
        "submission-date": "2025/05",
        "publication-date": "2025/07",
        "abstract": "The MODELS conference series is the premier venue for model-driven software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. The ACM/IEEE 25th International Conference on Model Driven Engineering Languages and Systems (MODELS 2022) took place in Montreal, Canada, from 23 to 28 October 2022. A total of 125 papers were submitted to the conference. The Foundations track of MODELS 2022 received 86 submissions of which 23 were accepted, while the Practice and Innovation track received 39 submissions of which 12 were accepted. Together, both tracks had an acceptance rate of 28%. It is a tradition that authors of the best papers at each MODELS conference edition are invited to submit revised and extended versions of their papers for publication in a special section in SoSyM. The selection of these papers is based on the reviews provided by the Program Committee members and on the reception of the papers at the conference. This special section comprises the twelve articles that resulted from this invitation and review process. Each article was subject to the full SoSyM review cycle and the authors received anonymous feedback from three expert reviewers. As a result, each article has been thoroughly revised and substantially extended compared to its conference version.",
        "keywords": [],
        "authors": [
            "Nelly Bencomo",
            "Houari Sahraoui",
            "Eugene Syriani",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-025-01303-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assert and negate revisited: Modal semantics for UML sequence diagrams",
        "submission-date": "2006/08",
        "publication-date": "2007/05",
        "abstract": "Live Sequence Charts (LSC) extend Message Sequence Charts (MSC), mainly by distinguishing possible from necessary behavior. They thus enable the speciﬁcation of rich multi-modal scenario-based properties, such as mandatory, possible and forbidden scenarios. The sequence diagrams of UML 2.0 enrich those of previous versions of UML by two new operators, assert and negate, for specifying required and forbidden behaviors, which appear to have been inspired by LSC. The UML 2.0 semantics of sequence diagrams, however, being based on pairs of valid and invalid sets of traces, is inadequate, and prevents the new operators from being used effectively.\nWe propose an extension of, and a different semantics for this UML language—Modal Sequence Diagrams (MSD)— based on the universal/existential modal semantics of LSC. In particular, in MSD assert and negate are really modalities, not operators. We deﬁne MSD as a UML 2.0 proﬁle, thus paving the way to apply formal veriﬁcation, synthesis, and scenario-based execution techniques from LSC to the mainstream UML standard.",
        "keywords": [
            "UML Interactions",
            "Sequence diagrams",
            "Live sequence charts",
            "Standardization",
            "Formal semantics"
        ],
        "authors": [
            "David Harel",
            "Shahar Maoz"
        ],
        "file_path": "data/sosym-all/s10270-007-0054-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting different process views through a Shared Process Model",
        "submission-date": "2014/03",
        "publication-date": "2015/01",
        "abstract": "Different stakeholders in the business process management (BPM) life cycle benefit from having different views onto a particular process model. Each view can show, and offer to change, the details relevant to the particular stakeholder, leaving out the irrelevant ones. However, introducing different views on a process model entail the problem to synchronize changes in case that one view evolves. This problem is especially relevant and challenging for views at different abstraction levels. In this paper, we propose a Shared Process Model that provides different stakeholder views at different abstraction levels and synchronizes changes made to any view. We present detailed requirements and a solution design for the Shared Process Model. We also present an overview of our prototypical implementation to demonstrate the feasibility of the approach. Finally, we report on a comprehensive evaluation of the approach on real Business–IT modeling scenarios.",
        "keywords": [
            "Business process modeling",
            "Business",
            "IT gap",
            "Model synchronization"
        ],
        "authors": [
            "Jochen Küster",
            "Hagen Völzer",
            "Cédric Favre",
            "Moisés Castelo Branco",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-015-0453-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Comparing relational model transformation technologies: implementing Query/View/Transformation with Triple Graph Grammars",
        "submission-date": "2008/03",
        "publication-date": "2009/07",
        "abstract": "The Model Driven Architecture (MDA) is an approach to develop software based on different models. There are separate models for the business logic and for platform speciﬁc details. Moreover, code can be generated automatically from these models. This makes transformations a core technology for MDA and for model-based software engineering approaches in general. Query/View/Transformation (QVT) is the transformation technology recently proposed for this purpose by the OMG. Triple Graph Grammars (TGGs) are another transformation technology proposed in the mid-nineties, used for example in the FUJ-ABA CASE tool. In contrast to many other transformation technologies, both QVT and TGGs declaratively deﬁne the relation between two models. With this definition, a transformation engine can execute a transformation in either direction and, based on the same definition, can also propagate changes from one model to the other. In this paper, we compare the concepts of the declarative languages of QVT and TGGs. It turns out that TGGs and declarative QVT have many concepts in common. In fact, QVT-Core can be mapped to TGGs. We show that QVT-Core can be implemented by transforming QVT-Core mappings to TGG rules, which can then be executed by a TGG transformation engine that performs the actual QVT transformation. Furthermore, we discuss an approach for mapping QVT-Relations to TGGs. Based on the semantics of TGGs, we clarify semantic gaps that we identiﬁed in the declarative languages of QVT and, furthermore, we show how TGGs can beneﬁt from the concepts of QVT.",
        "keywords": [
            "MDA",
            "Model-based software engineering",
            "Model transformation",
            "Query/View/Transformation (QVT)",
            "Triple Graph Grammar (TGG)"
        ],
        "authors": [
            "Joel Greenyer",
            "Ekkart Kindler"
        ],
        "file_path": "data/sosym-all/s10270-009-0121-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Query/View/Transformation (QVT), Triple Graph Grammar (TGG)"
        }
    },
    {
        "title": "Reusing model validation methods for the continuous validation of digital twins of cyber-physical systems",
        "submission-date": "2023/09",
        "publication-date": "2024/10",
        "abstract": "One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reﬂects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) We provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) We demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.",
        "keywords": [
            "Modeling and simulation",
            "Model validation",
            "Validation metrics",
            "Digital twin"
        ],
        "authors": [
            "Joost Mertens",
            "Joachim Denil"
        ],
        "file_path": "data/sosym-all/s10270-024-01225-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Transformation challenges: from software models to performance models",
        "submission-date": "2011/09",
        "publication-date": "2013/10",
        "abstract": "A software model can be analysed for non-functional requirements by extending it with suitable annotations and transforming it into analysis models for the corresponding non-functional properties. For quantitative performance evaluation, suitable annotations are standardized in the “UML Proﬁle for Modeling and Analysis of Real-Time Embedded systems” (MARTE) and its predecessor, the “UML Proﬁle for Schedulability, Performance and Time”. A range of different performance model types (such as queueing networks, Petri nets, stochastic process algebra) may be used for analysis. In this work, an intermediate “Core Scenario Model” (CSM) is used in the transformation from the source software model to the target performance model. CSM focuses on how the system behaviour uses the system resources. The semantic gap between the software model and the performance model must be bridged by (1) information supplied in the performance annotations, (2) in interpretation of the global behaviour expressed in the CSM and (3) in the process of constructing the performance model. Flexibility is required for specifying sets of alternative cases, for choosing where this bridging information is supplied, and for overriding values. It is also essential to be able to trace the source of values used in a particular performance estimate. The performance model in turn can be used to verify responsiveness and scalability of a software system, to discover architectural limitations at an early stage of development, and to develop efﬁcient performance tests. This paper describes how the semantic gap between software models in UML+MARTE and performance models (based on queueing or Petri nets) can be bridged using transformations based on CSMs, and how the transformation challenges are addressed.",
        "keywords": [
            "Software performance",
            "Performance analysis",
            "Model transformation",
            "UML",
            "MARTE proﬁle",
            "Layered queueing networks"
        ],
        "authors": [
            "Murray Woodside",
            "Dorina C. Petriu",
            "José Merseguer",
            "Dorin B. Petriu",
            "Mohammad Alhaj"
        ],
        "file_path": "data/sosym-all/s10270-013-0385-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal analysis of human operator behavioural patterns in interactive surveillance systems",
        "submission-date": "2006/05",
        "publication-date": "2007/12",
        "abstract": "An important area of Human Reliability Assessment in interactive systems is the ability to understand the causes of human error and to model their occurrence. This paper investigates a new approach to analysis of task failures based on patterns of operator behaviour, in contrast with more traditional event-based approaches. It considers, as a case study, a formal model of an Air Trafﬁc Control system operator’s task which incorporates a simple model of the high-level cognitive processes involved. The cognitive model is formalised in the CSP process algebra. Various patterns of behaviour that could lead to task failure are descri-bed using temporal logic. Then a model-checking technique is used to verify whether the set of selected behavioural pat-terns is sound and complete with respect to the deﬁnition of task failure. The decomposition is shown to be incomplete and a new behavioural pattern is identiﬁed, which appears to have been overlooked in the informal analysis of the problem. This illustrates how formal analysis of operator models can yield fresh insights into how failures may arise in interactive systems.",
        "keywords": [],
        "authors": [
            "Antonio Cerone",
            "Simon Connelly",
            "Peter Lindsay"
        ],
        "file_path": "data/sosym-all/s10270-007-0072-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Knowledge and software modeling using UML",
        "submission-date": "2002/09",
        "publication-date": "2004/04",
        "abstract": "Ontology can be considered as a comprehen-sive knowledge model which enables the developer to practice knowledge, instead of code, reuse. In the devel-opment of knowledge-based systems, diﬀerent modeling languages are employed at diﬀerent stages of the develop-ment process. By using a common modeling language for the knowledge and software models, knowledge instead of software reuse can be achieved. We illustrate the process by ﬁrst presenting an ontology developed for an industrial domain and then investigate Uniﬁed Modeling Language (UML) as an ontology modeling tool. Since any model ex-pressed in UML can be translated into a software model, the transition from the knowledge model to system im-plementation is better supported with the proposed ap-proach. The industrial domain of selecting a remediation technique for petroleum contaminated sites is adopted for the illustration case study.",
        "keywords": [
            "Ontology",
            "Knowledge Reuse",
            "Uniﬁed Modeling Language (UML)"
        ],
        "authors": [
            "Christine W. Chan"
        ],
        "file_path": "data/sosym-all/s10270-004-0057-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling research in recent years: special section on ECMFA 2017 and ECMFA 2018",
        "submission-date": "2020/07",
        "publication-date": "2020/08",
        "abstract": "The modeling community has made signiﬁcant progress in recent years regarding developing software and systems with enhanced productivity and quality. Leveraging the critical entities emerging in the development process and their correspondences allows us to design, analyze, and develop software and systems relying on formalized notations that are amenable to computer-based automation. The European Conference on Modelling Foundations and Applications (ECMFA) is one of the primary scientiﬁc venues where almost any aspect of modeling is discussed among world-class experts from academia and industry.",
        "keywords": [],
        "authors": [
            "Anthony Anjorin",
            "Alfonso Pierantonio",
            "Salvador Trujillo",
            "Huascar Espinoza Ortiz"
        ],
        "file_path": "data/sosym-all/s10270-020-00821-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Clafer: unifying class and feature modeling",
        "submission-date": "2013/09",
        "publication-date": "2014/12",
        "abstract": "We present Clafer (class, feature, reference), a\nclass modeling language with ﬁrst-class support for feature\nmodeling. We designed Clafer as a concise notation for meta-\nmodels, feature models, mixtures of meta- and feature mod-\nels (such as components with options), and models that cou-\nple feature models and meta-models via constraints (such\nas mapping feature conﬁgurations to component conﬁgura- \ntions or model templates). Clafer allows arranging models\ninto multiple specialization and extension layers via con- \nstraints and inheritance. We identify several key mechanisms\nallowing a meta-modeling language to express feature mod- \nels concisely. Clafer uniﬁes basic modeling constructs, such\nas class, association, and property, into a single construct,\ncalled clafer. We provide the language with a formal seman-\ntics built in a structurally explicit way. The resulting seman-\ntics explains the meaning of hierarchical models whereby\nproperties can be arbitrarily nested in the presence of inher-\ntance and feature modeling constructs. The semantics also\nenables building consistent automated reasoning support for\nthe language: To date, we implemented three reasoners for\nClafer based on Alloy, Z3 SMT, and Choco3 CSP solvers.",
        "keywords": [
            "Language design",
            "Feature modeling",
            "OOM",
            "Semantics",
            "Uniﬁcation"
        ],
        "authors": [
            "Kacper Ba˛k",
            "Zinovy Diskin",
            "Michał Antkiewicz",
            "Krzysztof Czarnecki",
            "Andrzej Wa˛sowski"
        ],
        "file_path": "data/sosym-all/s10270-014-0441-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML Proﬁle for the Design, Quality Assessment and Deployment of Data-intensive Applications",
        "submission-date": "2018/02",
        "publication-date": "2019/04",
        "abstract": "Big Data or Data-Intensive applications (DIAs) seek to mine, manipulate, extract or otherwise exploit the potential intelligence hidden behind Big Data. However, several practitioner surveys remark that DIAs potential is still untapped because of very difﬁcult and costly design, quality assessment and continuous reﬁnement. To address the above shortcoming, we propose the use of a UML domain-speciﬁc modeling language or proﬁle speciﬁcally tailored to support the design, assessment and continuous deployment of DIAs. This article illustrates our DIA-speciﬁc proﬁle and outlines its usage in the context of DIA performance engineering and deployment. For DIA performance engineering, we rely on the Apache Hadoop technology, while for DIA deployment, we leverage the TOSCA language. We conclude that the proposed proﬁle offers a powerful language for data-intensive software and systems modeling, quality evaluation and automated deployment of DIAs on private or public clouds.",
        "keywords": [
            "UML",
            "Proﬁle",
            "Data-intensive applications",
            "Software design",
            "Big Data",
            "Performance assessment",
            "Model-driven deployment",
            "Apache Hadoop",
            "TOSCA language"
        ],
        "authors": [
            "Diego Perez-Palacin",
            "José Merseguer",
            "José I. Requeno",
            "M. Guerriero",
            "Elisabetta Di Nitto",
            "D. A. Tamburri"
        ],
        "file_path": "data/sosym-all/s10270-019-00730-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Beyond loop bounds: comparing annotation languages for worst-case execution time analysis",
        "submission-date": "2009/01",
        "publication-date": "2010/04",
        "abstract": "Worst-case execution time (WCET) analysis is concerned with computing a precise-as-possible bound for the maximum time the execution of a program can take. This information is indispensable for developing safety-critical real-time systems, e.g., in the avionics and automotive ﬁelds. Starting with the initial works of Chen, Mok, Puschner, Shaw, and others in the mid and late 1980s, WCET analysis turned intoawell-establishedandvibrantﬁeldofresearchanddevel-opment in academia and industry. The increasing number and diversity of hardware and software platforms and the ongo-ing rapid technological advancement became drivers for the development of a wide array of distinct methods and tools for WCET analysis. The precision, generality, and efﬁciency of these methods and tools depend much on the expressive-ness and usability of the annotation languages that are used to describe feasible and infeasible program paths. In this article we survey the annotation languages which we con-sider formative for the ﬁeld. By investigating and comparing their individual strengths and limitations with respect to a set of pivotal criteria, we provide a coherent overview of the state of the art. Identifying open issues, we encourage further research. This way, our approach is orthogonal and comple-mentary to a recent approach of Wilhelm et al. who provide a thorough survey of WCET analysis methods and tools that have been developed and used in academia and industry.",
        "keywords": [
            "Worst-case execution time (WCET) analysis",
            "Annotation languages",
            "Path-oriented",
            "constraint-oriented",
            "and hierarchy-oriented WCET annotation languages",
            "WCET annotation language challenge"
        ],
        "authors": [
            "Raimund Kirner",
            "Jens Knoop",
            "Adrian Prantl",
            "Markus Schordan",
            "Albrecht Kadlec"
        ],
        "file_path": "data/sosym-all/s10270-010-0161-0.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Bridging MDE and AI: a systematic review of domain-speciﬁc languages and model-driven practices in AI software systems engineering",
        "submission-date": "2023/07",
        "publication-date": "2024/09",
        "abstract": "Technical systems are becoming increasingly complex due to the increasing number of components, functions, and involve-ment of different disciplines. In this regard, model-driven engineering techniques and practices tame complexity during the development process by using models as primary artifacts. Modeling can be carried out through domain-speciﬁc languages whose implementation is supported by model-driven techniques. Today, the amount of data generated during product devel-opment is rapidly growing, leading to an increased need to leverage artiﬁcial intelligence algorithms. However, using these algorithms in practice can be difﬁcult and time-consuming. Therefore, leveraging domain-speciﬁc languages and model-driven techniques for formulating AI algorithms or parts of them can reduce these complexities and be advantageous. This study aims to investigate the existing model-driven approaches relying on domain-speciﬁc languages in support of the engineering of AI software systems to sharpen future research further and deﬁne the current state of the art. We conducted a Systemic Literature Review (SLR), collecting papers from ﬁve major databases resulting in 1335 candidate studies, eventually retaining 18 primary studies. Each primary study will be evaluated and discussed with respect to the adoption of (1) MDE principles and practices and (2) the phases of AI development support aligned with the stages of the CRISP-DM methodology. The study’s ﬁndings show that language workbenches are of paramount importance in dealing with all aspects of modeling language development (metamodel, concrete syntax, and model transformation) and are leveraged to deﬁne domain-speciﬁc languages (DSL) explicitly addressing AI concerns. The most prominent AI-related concerns are training and modeling of the AI algorithm, while minor emphasis is given to the time-consuming preparation of the data sets. Early project phases that support interdisciplinary communication of requirements, such as the CRISP-DM Business Understanding phase, are rarely reﬂected. The study found that the use of MDE for AI is still in its early stages, and there is no single tool or method that is widely used. Additionally, current approaches tend to focus on speciﬁc stages of development rather than providing support for the entire development process. As a result, the study suggests several research directions to further improve the use of MDE for AI and to guide future research in this area.",
        "keywords": [
            "Model-driven engineering",
            "Artiﬁcial intelligence",
            "MDE4AI",
            "Domain-speciﬁc language",
            "SLR",
            "Literature review",
            "Machine learning"
        ],
        "authors": [
            "Simon Rädler",
            "Luca Berardinelli",
            "Karolin Winter",
            "Abbas Rahimi",
            "Stefanie Rinderle-Ma"
        ],
        "file_path": "data/sosym-all/s10270-024-01211-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special issue on MODELS 2010",
        "submission-date": "2010/10",
        "publication-date": "2013/02",
        "abstract": "Welcome to this special issue of Software and Systems Modeling devoted to selected papers of MODELS 2010. The MODELS series of conferences is the premier venue for exchange of innovative ideas and practical experience focusing on a very important new technical discipline: model-driven software and systems engineering. The expansion of this discipline is a direct consequence of the increasing significance and success of model-based methods in practice.",
        "keywords": [],
        "authors": [
            "Dorina C. Petriu"
        ],
        "file_path": "data/sosym-all/s10270-013-0324-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Performance modeling and analysis of message-oriented event-driven systems",
        "submission-date": "2011/07",
        "publication-date": "2012/02",
        "abstract": "Message-oriented event-driven systems are becoming increasingly ubiquitous in many industry domains including telecommunications, transportation and supply chain management. Applications in these areas typically have stringent requirements for performance and scalability. To guarantee adequate quality-of-service, systems must be subjected to a rigorous performance and scalability analysis before they are put into production. In this paper, we present a comprehensive modeling methodology for message-oriented event-driven systems in the context of a case study of a representative application in the supply chain management domain. The methodology, which is based on queueing Petri nets, provides a basis for performance analysis and capacity planning. We study a deployment of the SPECjms2007 standard benchmark on a leading commercial middleware platform. A detailed system model is built in a step-by-step fashion and then used to predict the system performance under various workload and configuration scenarios. After the case study, we present a set of generic performance modeling patterns that can be used as building blocks when modeling message-oriented event-driven systems. The results demonstrate the effectiveness, practicality and accuracy of the proposed modeling and prediction approach.",
        "keywords": [
            "Event-based",
            "Performance model",
            "Performance evaluation",
            "Message-oriented middleware",
            "Performance pattern"
        ],
        "authors": [
            "Kai Sachs",
            "Samuel Kounev",
            "Alejandro Buchmann"
        ],
        "file_path": "data/sosym-all/s10270-012-0228-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving query performance on dynamic graphs",
        "submission-date": "2020/01",
        "publication-date": "2020/11",
        "abstract": "Querying large models efﬁciently often imposes high demands on system resources such as memory, processing time, disk access or network latency. The situation becomes more complicated when data are highly interconnected, e.g. in the form of graph structures, and when data sources are heterogeneous, partly coming from dynamic systems and partly stored in databases. These situations are now common in many existing social networking applications and geo-location systems, which require specialized and efﬁcient query algorithms in order to make informed decisions on time. In this paper, we propose an algorithm to improve the memory consumption and time performance of this type of queries by reducing the amount of elements to be processed, focusing only on the information that is relevant to the query but without compromising the accuracy of its results. To this end, the reduced subset of data is selected depending on the type of query and its constituent ﬁlters. Three case studies are used to evaluate the performance of our proposal, obtaining signiﬁcant speedups in all cases.",
        "keywords": [
            "Data stream processing",
            "Dynamic graphs",
            "Performance optimization",
            "Precomputing systems",
            "Data queries"
        ],
        "authors": [
            "Gala Barquero",
            "Javier Troya",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-020-00832-3.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Understanding Declare models: strategies, pitfalls, empirical results",
        "submission-date": "2013/10",
        "publication-date": "2014/09",
        "abstract": "Declarative approaches to business process modeling are regarded as well suited for highly volatile environments, as they enable a high degree of flexibility. However, problems in understanding and maintaining declarative process models often impede their adoption. Likewise, little research has been conducted into the understanding of declarative process models. This paper takes a first step toward addressing this fundamental question and reports on an empirical investigation consisting of an exploratory study and a follow-up study focusing on the system analysts’ sense-making of declarative process models that are specified in Declare. For this purpose, we distributed real-world Declare models to the participating subjects and asked them to describe the illustrated process and to perform a series of sense-making tasks. The results of our studies indicate that two main strategies for reading Declare models exist: either considering the execution order of the activities in the process model, or orienting by the layout of the process model. In addition, the results indicate that single constraints can be handled well by most subjects, while combinations of constraints pose significant challenges. Moreover, the study revealed that aspects that are similar in both imperative and declarative process modeling languages at a graphical level, while having different semantics, cause considerable troubles. This research not only helps guiding the future development of tools for supporting system analysts, but also gives advice on the design of declarative process modeling notations and points out typical pitfalls to teachers and educators of future systems analysts.",
        "keywords": [
            "Declarative process models",
            "Empirical research",
            "Understandability"
        ],
        "authors": [
            "Cornelia Haisjackl",
            "Irene Barba",
            "Stefan Zugal",
            "Pnina Soffer",
            "Irit Hadar",
            "Manfred Reichert",
            "Jakob Pinggera",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-014-0435-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based tool support for Tactical Data Links: an experience report from the defence domain",
        "submission-date": "2014/05",
        "publication-date": "2015/08",
        "abstract": "The Tactical Data Link (TDL) allows the exchange of information between cooperating platforms as part of an integrated command and control (C2) system. Information exchange is facilitated by adherence to a complex, message-based protocol deﬁned by document-centric standards. In this paper, we report on a recent body of work investigating migration from a document-centric to a model-centric approach within the context of the TDL domain, motivated by a desire to achieve a positive return on investment. The model-centric approach makes use of the Epsilon technology stack and provides a signiﬁcant improvement to both the level of abstraction and rigour of the network design. It is checkable by a machine and, by virtue of an MDA-like approach to the separation of domains and model transformation between domains, is open to integration with other models to support more complex workﬂows, such as by providing the results of interoperability analyses in human-readable domain-speciﬁc reports conforming to an accepted standard.",
        "keywords": [
            "Tactical data link",
            "Model-based development",
            "Interoperability",
            "Metamodelling",
            "Model management",
            "Eclipse Modeling Framework",
            "Epsilon"
        ],
        "authors": [
            "Suraj Ajit",
            "Chris Holmes",
            "Julian Johnson",
            "Dimitrios S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-015-0480-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon"
        }
    },
    {
        "title": "User journey games: automating user-centric analysis",
        "submission-date": "2023/03",
        "publication-date": "2024/03",
        "abstract": "The servitization of business is moving industry to business models driven by customer demand. Customer satisfaction is connected with ﬁnancial rewards, forcing companies to invest in their users’ experience. User journeys describe how users maneuver through a service. Today, user journeys are typically modeled graphically, and lack formalization and analysis support. This paper proposes a formalization of user journeys as weighted games between the user and the service provider and a systematic data-driven method to derive these user journey games from system logs, using process mining techniques. As the derived games may contain cycles, we deﬁne an algorithm to transform user journeys games with cycles into acyclic weighted games, which can be model checked using Uppaal Stratego to uncover potential challenges in a company’s interactions with its users and derive company strategies to guide users through their journeys. Finally, we propose a user journey sliding-window analysis to detect changes in the user journey over time by model checking a sequence of generated games. Our analysis pipeline has been evaluated on an industrial case study; it revealed design challenges within the studied service and could be used to derive actionable recommendations for improvement.",
        "keywords": [
            "User journeys",
            "Data-driven model construction",
            "Time-series analysis",
            "Games",
            "Model checking",
            "UPPAAL."
        ],
        "authors": [
            "Paul Kobialka\nS. Lizeth Tapia Tarifa\nGunnar R. Bergersen\nEinar Broch Johnsen"
        ],
        "file_path": "data/sosym-all/s10270-024-01148-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Style-based modeling and refinement of service-oriented architectures",
        "submission-date": "2005/05",
        "publication-date": "2006/04",
        "abstract": "Service-oriented architectures (SOA) provide a flexible and dynamic platform for implementing business solutions. In this paper, we address the modeling of such architectures by refining business-oriented architectures, which abstract from technology aspects, into service-oriented ones, focusing on the ability of dynamic reconfiguration (binding to new services at run-time) typical for SOA. The refinement is based on conceptual models of the platforms involved as architectural styles, formalized by graph transformation systems. Based on a refinement relation between abstract and platform-specific styles we investigate how to realize business-specific scenarios on the SOA platform by automatically deriving refined, SOA-specific reconfiguration scenarios.",
        "keywords": [
            "Service-oriented architecture",
            "Architectural style",
            "Architecture refinement",
            "Graph transformation"
        ],
        "authors": [
            "Luciano Baresi",
            "Reiko Heckel",
            "Sebastian Th¨one",
            "D´aniel Varr´o"
        ],
        "file_path": "data/sosym-all/s10270-006-0001-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A demonstration-based model transformation approach to automate model scalability",
        "submission-date": "2011/09",
        "publication-date": "2013/09",
        "abstract": "An important aspect during software development is the ability to evolve and scale software models in order to handle design forces, such as enlarging and upgrading system features, or allocating more resources to handle additional users. Model scalability is the ability to refactor a base model, by adding or replicating the base model elements, connections or substructures, in order to build a larger and more complex model to satisfy new design requirements. Although a number of modeling tools have been developed to create and edit models for different purposes, mechanisms to scale models have not been well supported. In most situations, models are manually scaled using the basic point-and-click editing operations provided by the modeling environment. Manual model scaling is often tedious and error-prone, especially when the model to be scaled has hundreds or thousands of elements and the scaling process involves entirely manual operations. Although model scaling tasks can be automated by using model transformation languages, writing model transformation rules requires learning a model transformation language, as well as possessing a great deal of knowledge about the metamodel. Model transformation languages and metamodel concepts are often difficult for domain experts to understand. This requirement to learn a complex model transformation language exerts a negative influence on the usage of models by domain experts in software development. For instance, domain experts may be prevented from contributing to model scalability tasks from which they have significant domain experience. This paper presents a demonstration-based approach to automate model scaling. Instead of writing model transformation rules explicitly, users demonstrate how to scale models by directly editing the concrete model instances and simulate the model replication processes. By recording a user’s operations, an inference engine analyzes the user’s demonstration and generalizes model transformation patterns automatically, which can be reused to scale up other model instances. Using this approach, users are able to automate scaling tasks without learning a complex model transformation language. In addition, because the demonstration is performed on model instances, users are isolated from the underlying abstract metamodel definitions.",
        "keywords": [
            "Model evolution",
            "Model scalability",
            "Model transformation by demonstration"
        ],
        "authors": [
            "Yu Sun",
            "Jeff Gray",
            "Jules White"
        ],
        "file_path": "data/sosym-all/s10270-013-0374-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Semantics of OCL speciﬁed with QVT",
        "submission-date": "2007/03",
        "publication-date": "2008/03",
        "abstract": "The Object Constraint Language (OCL) has been\nfor many years formalized both in its syntax and seman-\ntics in the language standard. While the ofﬁcial definition\nof OCL’s syntax is already widely accepted and strictly sup-\nported by most OCL tools, there is no such agreement on\nOCL’s semantics, yet. In this paper, we propose an approach\nbased on metamodeling and model transformations for for-\nmalizing the semantics of OCL. Similarly to OCL’s ofﬁcial\nsemantics, our semantics formalizes the semantic domain of\nOCL, i.e. the possible values to which OCL expressions can\nevaluate, by a metamodel. Contrary to OCL’s ofﬁcial seman-\ntics, the evaluation of OCL expressions is formalized in our\napproach by model transformations written in QVT. Thanks\nto the chosen format, our semantics definition for OCL can be\nautomatically transformed into a tool, which evaluates OCL\nexpressions in a given context. Our work on the formaliza-\ntion of OCL’s semantics resulted also in the identiﬁcation\nand better understanding of important semantic concepts, on\nwhich OCL relies. These insights are of great help when OCL\nhas to be tailored as a constraint language of a given DSL.\nWe show on an example, how the semantics of OCL has to\nredefined in order to become a constraint language in a\ndatabase domain.",
        "keywords": [
            "QVT",
            "OCL Semantics",
            "Graph-transformations",
            "DSL"
        ],
        "authors": [
            "Slaviša Markovi´c",
            "Thomas Baar"
        ],
        "file_path": "data/sosym-all/s10270-008-0083-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Analysis of variability models: a systematic literature review",
        "submission-date": "2019/06",
        "publication-date": "2020/11",
        "abstract": "Dealing with variability, during Software Product Line Engineering (SPLE), means trying to allow software engineers to\ndevelop a set of similar applications based on a manageable range of variable functionalities according to expert users’ needs.\nParticularly, variability management (VM) is an activity that allows ﬂexibility and a high level of reuse during software\ndevelopment. In the last years, we have witnessed a proliferation of methods, techniques and supporting tools for VM in\ngeneral, and for its analysis in particular. More precisely, a speciﬁc ﬁeld has emerged, named (automated) variability analysis,\nfocusing on verifying variability models across the SPLE’s phases. In this paper, we introduce a systematic literature review of\nexisting proposals (as primary studies) focused on analyzing variability models. We deﬁne a classiﬁcation framework, which\nis composed of 20 sub-characteristics addressing general aspects, such as scope and validation, as well as model-speciﬁc\naspects, such as variability primitives, reasoner type. The framework allows to look at the analysis of variability models\nduring its whole life cycle—from design to derivation—according to the activities involved during an SPL development.\nAlso, the framework helps us answer three research questions deﬁned for showing the state of the art and drawing challenges\nfor the near future. Among the more interesting challenges, we can highlight the needs of more applications in industry, the\nexistence of more mature tools, and the needs of providing more semantics in the way of variability primitives for identifying\ninconsistencies in the models.",
        "keywords": [
            "Variability analysis",
            "Software Product Line",
            "Variability management",
            "Supporting tools"
        ],
        "authors": [
            "Matias Pol’la",
            "Agustina Buccella",
            "Alejandra Cechich"
        ],
        "file_path": "data/sosym-all/s10270-020-00839-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "IoT meets BPM: a bidirectional communication architecture for IoT-aware process execution",
        "submission-date": "2018/10",
        "publication-date": "2020/03",
        "abstract": "Business processes are frequently executed within application systems that involve humans, computer systems as well as objects of the Internet of Things (IoT). Nevertheless, the usage of IoT technology for system supported process execution is still constrained by the absence of a common system architecture that manages the communication between both worlds. In this paper, we introduce an integrated approach for IoT-aware business process execution that exploits IoT for BPM by providing IoT data in a process-compatible way, providing an IoT data provenance framework, considering IoT data for interaction in a pre-deﬁned process model, and providing wearable user interfaces with context-speciﬁc IoT data provision. The approach has been implemented on top of contemporary BPM modeling concepts and system technology. The introduced technique has evaluated extensively in different use cases in industry.",
        "keywords": [
            "Process Execution",
            "Internet of Things",
            "Wearables"
        ],
        "authors": [
            "Stefan Schönig",
            "Lars Ackermann",
            "Stefan Jablonski",
            "Andreas Ermer"
        ],
        "file_path": "data/sosym-all/s10270-020-00785-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deep speciﬁcation and proof preservation for the CoqTL transformation language",
        "submission-date": "2021/03",
        "publication-date": "2022/05",
        "abstract": "Executable engines for relational model-transformation languages evolve continuously because of language extension, performance improvement and bug ﬁxes. While new versions generally change the engine semantics, end-users expect to get backward-compatibility guarantees, so that existing transformations do not need to be adapted at every engine update. The CoqTL model-transformation language allows users to deﬁne model transformations, theorems on their behavior and machine-checked proofs of these theorems in Coq. Backward-compatibility for CoqTL involves also the preservation of these proofs. However, proof preservation is challenging, as proofs are easily broken even by small refactorings of the code they verify. In this paper, we present the solution we designed for the evolution of CoqTL. We provide a deep speciﬁcation of the transformation engine, including a set of theorems that must hold against the engine implementation. Then, at each milestone in the engine development, we certify the new version of the engine against this speciﬁcation, by providing proofs of the impacted theorems. The certiﬁcation formally guarantees end-users that all the proofs they write using the provided theorems will be preserved through engine updates. We illustrate the structure of the deep speciﬁcation theorems, we produce a machine-checked certiﬁcation of three versions of CoqTL against it, and we show examples of user proofs that leverage this speciﬁcation and are thus preserved through the updates. Finally, we discuss the evolution of the deep speciﬁcation by an extension mechanism, we present an evolution that introduces trace links in the speciﬁcation, and we show which user proofs are preserved through speciﬁcation evolutions.",
        "keywords": [
            "MDE",
            "Model transformation",
            "Programming language implementation",
            "Certiﬁcation",
            "Theorem proving",
            "Coq"
        ],
        "authors": [
            "Zheng Cheng",
            "Massimo Tisi"
        ],
        "file_path": "data/sosym-all/s10270-022-01004-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "CoqTL"
        }
    },
    {
        "title": "A Methodological Approach for Object-Relational Database Design using UML",
        "submission-date": "2002/01",
        "publication-date": "2003/01",
        "abstract": "The most common way of designing databases is by means of a conceptual model, such as E/R, without taking into account other views of the system. New object-oriented design languages, such as UML (Uniﬁed Modelling Language), allow the whole system, including the database schema, to be modelled in a uniform way. Moreover, as UML is an extendable language, it allows for any necessary introduction of new stereotypes for specific applications. Proposals exist to extend UML with stereotypes for database design but, unfortunately, they are focused on relational databases. However, new applications require complex objects to be represented in complex relationships, object-relational databases being more appropriate for these requirements. The framework of this paper is an Object-Relational Database Design Methodology, which defines new UML stereotypes for Object-Relational Database Design and proposes some guidelines to translate a UML conceptual schema into an object-relational schema. The guidelines are based on the SQL:1999 object-relational model and on Oracle8i as a product example.",
        "keywords": [
            "UML extensions",
            "Stereotypes",
            "Object-Relational Databases",
            "Database Design",
            "Object Persistence",
            "Design Methodology",
            "UML",
            "SQL:1999",
            "Oracle"
        ],
        "authors": [
            "Esperanza Marcos",
            "Belén Vela",
            "José María Cavero"
        ],
        "file_path": "data/sosym-all/s10270-002-0001-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ESUML-EAF: a framework to develop an energy-efﬁcient design model for embedded software",
        "submission-date": "2012/08",
        "publication-date": "2013/03",
        "abstract": "There is a growing interest in developing embed- ded systems that consume low energy in such application areas as mobile communications or wireless sensor networks. To especially provide the complex and diverse functions of embedded software with limited energy consumption, many studies of low-energy software are being performed. The existing studies to analyze energy consumption of embed- ded software have mainly focused on source code. How- ever, some studies recently explored model-based energy consumption analysis to fulﬁll the requirement of energy consumption in the early phase of software development process. This paper proposes a model-based energy con- sumption analysis framework to develop an energy-efﬁcient design model of embedded software. The proposed frame- work can analyze energy consumption without building an additional analysis model in software development and pro- vide the chance to fulﬁll the energy consumption require- ments in the early phase of the software development process, which can reduce the feedback efforts.",
        "keywords": [
            "UML",
            "Energy-efﬁcient design model",
            "Embedded software"
        ],
        "authors": [
            "Doo-Hwan Kim",
            "Jang-Eui Hong"
        ],
        "file_path": "data/sosym-all/s10270-013-0337-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to theme issue on model-driven engineering of component-based software systems",
        "submission-date": "2017/02",
        "publication-date": "2017/03",
        "abstract": "This theme issue aims at providing a forum for disseminating latest trends in the use and combination of model-driven engineering (MDE) and component-based software engineering (CBSE). One of the main aims of MDE is to increase productivity in the development of complex systems, while reducing the time to market. Regarding CBSE, one of the main goals is to deliver and then support the exploitation of reusable “off-the-shelf” software components to be incorporated into larger applications. An effective interplay of MDE and CBSE can bring beneﬁts to both communities: on the one hand, the CBSE community would beneﬁt from implementation and automation capabil-ities of MDE, and on the other hand, MDE would beneﬁt from the foundational nature of CBSE. In total, we received 23 submissions to this theme issue, and each submission was reviewed by at least three reviewers. Thanks to the high qual-ity of the submissions that we received, we could eventually accept six papers for publication.",
        "keywords": [],
        "authors": [
            "Federico Ciccozzi",
            "Jan Carlson",
            "Patrizio Pelliccione",
            "Massimo Tivoli"
        ],
        "file_path": "data/sosym-all/s10270-017-0589-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating relational database transactions from eb3 attribute definitions",
        "submission-date": "2006/08",
        "publication-date": "2008/10",
        "abstract": "eb3 is a trace-based formal language created for the speciﬁcation of information systems. In eb3, each entity and association attribute is independently deﬁned by a recursive function on the valid traces of external events. This paper describes an algorithm that generates, for each external event, a transaction that updates the value of affected attributes in their relational database representation. The beneﬁts are two-fold: eb3 attribute speciﬁcations are automatically translated into executable programs, eliminating system design and implementation steps; the construction of information systems is streamlined, because eb3 speciﬁcations are simpler and shorter to write than corresponding traditional speciﬁcations, design and implementations. In particular, the paper shows that simple eb3 constructs can replace complex SQL queries which are typically difﬁcult to write.",
        "keywords": [
            "Information systems",
            "Attributes",
            "Pattern matching",
            "SELECT statements",
            "Transactions"
        ],
        "authors": [
            "Frédéric Gervais",
            "Marc Frappier",
            "Régine Laleau"
        ],
        "file_path": "data/sosym-all/s10270-008-0104-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for the special section on MODELS 2021",
        "submission-date": "2021/10",
        "publication-date": "2023/05",
        "abstract": "The MODELS conference series is the premier venue for model-based software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. MODELS 2021 took place online (originally planned in Fukuoka, Japan), from October 10 to October 15, 2021, as the ACM/IEEE 24th International Conference on Model Driven Engineering Languages and Systems. This special section presents the 14 articles that resulted from an invitation based on the best papers at the conference.",
        "keywords": [],
        "authors": [
            "Shiva Nejati",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-023-01108-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An epistemic approach to the formal speciﬁcation of statistical machine learning",
        "submission-date": "2020/03",
        "publication-date": "2020/09",
        "abstract": "We propose an epistemic approach to formalizing statistical properties of machine learning. Speciﬁcally, we introduce a formal model for supervised learning based on a Kripke model where each possible world corresponds to a possible dataset and modal operators are interpreted as transformation and testing on datasets. Then, we formalize various notions of the classiﬁcation performance, robustness, and fairness of statistical classiﬁers by using our extension of statistical epistemic logic. In this formalization, we show relationships among properties of classiﬁers, and relevance between classiﬁcation performance and robustness. As far as we know, this is the ﬁrst work that uses epistemic models and logical formulas to express statistical properties of machine learning, and would be a starting point to develop theories of formal speciﬁcation of machine learning.",
        "keywords": [
            "Modal logic",
            "Possible world semantics",
            "Machine learning",
            "Classiﬁcation performance",
            "Robustness",
            "Fairness"
        ],
        "authors": [
            "Yusuke Kawamoto"
        ],
        "file_path": "data/sosym-all/s10270-020-00825-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Gamifying model-based engineering: the PapyGame experience",
        "submission-date": "2022/08",
        "publication-date": "2023/03",
        "abstract": "Modeling is an essential and challenging activity in any engineering environment. It implies some hard-to-train skills such as abstraction and communication. Teachers, project leaders, and tool vendors have a hard time teaching or training their students, co-workers, or users. Gamiﬁcation refers to the exploitation of gaming mechanisms for serious purposes, like pro-moting behavioral changes, soliciting participation and engagement in activities, etc. We investigate the introduction of gaming mechanisms in modeling tasks with the primary goal of supporting learning/training. The result has been the realization of a gamiﬁed modeling environment named PapyGame. In this article, we present the approach adopted for PapyGame imple-mentation, the details on the gamiﬁcation elements involved, and the derived conceptual architecture required for applying gamiﬁcation in any modeling environment. Moreover, to demonstrate the beneﬁts of using PapyGame for learning/training modeling, a set of user experience evaluations have been conducted. Correspondingly, we report the obtained results together with a set of future challenges we consider as critical to make gamiﬁed modeling a more effective education/training approach.",
        "keywords": [
            "Model-based engineering",
            "Education",
            "Gamiﬁcation",
            "Papyrus"
        ],
        "authors": [
            "Antonio Bucchiarone",
            "Maxime Savary-Leblanc",
            "Xavier Le Pallec",
            "Antonio Cicchetti",
            "Sébastien Gérard",
            "Simone Bassanelli",
            "Federica Gini",
            "Annapaola Marconi"
        ],
        "file_path": "data/sosym-all/s10270-023-01091-8.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Foundations for Streaming Model Transformations by Complex Event Processing",
        "submission-date": "2015/05",
        "publication-date": "2016/05",
        "abstract": "Streaming model transformations represent a novel class of transformations to manipulate models whose elements are continuously produced or modiﬁed in high volume and with rapid rate of change. Executing streaming transformations requires efﬁcient techniques to recognize activated transformation rules over a live model and a potentially inﬁnite stream of events. In this paper, we propose foundations of streaming model transformations by innovatively integrating incremental model query, complex event processing (CEP) and reactive (event-driven) transformation techniques. Complex event processing allows to identify relevant patterns and sequences of events over an event stream. Our approach enables event streams to include model change events which are automatically and continuously populated by incremental model queries. Furthermore, a reactive rule engine carries out transformations on identiﬁed complex event patterns. We provide an integrated domain-speciﬁc language with precise semantics for capturing complex event patterns and streaming transformations together with an execution engine, all of which is now part of the Viatra reactive transformation framework. We demonstrate the feasibility of our approach with two case studies: one in an advanced model engineering workﬂow; and one in the context of on-the-ﬂy gesture recognition.",
        "keywords": [
            "Streaming model transformations",
            "Complex event processing",
            "Live models",
            "Change-driven transformations",
            "Reactive transformations"
        ],
        "authors": [
            "István Dávid",
            "István Ráth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-016-0533-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Business process improvement with AB testing and reinforcement learning: grounded theory-based industry perspectives",
        "submission-date": "2023/10",
        "publication-date": "2024/11",
        "abstract": "In order to better facilitate the need for continuous business process improvement, the application of DevOps principles has been proposed. In particular, the AB-BPM methodology applies AB testing—a DevOps practice—and reinforcement learning to increase the speed and quality of business process improvement efforts. In this paper, we provide an industry perspective on this approach, assessing prerequisites, suitability, requirements, risks, and additional aspects of the AB-BPM methodology and supporting tools. Our qualitative study follows the grounded theory research methodology, including 16 semi-structured interviews with BPM practitioners. The main ﬁndings indicate: (1) a need for expert control during reinforcement learning-driven experiments in production, (2) the importance of involving the participants and aligning the method culturally with the respective setting, (3) the necessity of an integrated process execution environment, and (4) the long-term potential of the methodology for effective and efﬁcient validation of algorithmically (co-)created business process variants, and their continuous management.",
        "keywords": [
            "Business process improvement",
            "Process redesign",
            "Reinforcement learning",
            "AB testing",
            "Grounded theory",
            "Business process management"
        ],
        "authors": [
            "Aaron Friedrich Kurz",
            "Timotheus Kampik",
            "Luise Pufahl",
            "Ingo Weber"
        ],
        "file_path": "data/sosym-all/s10270-024-01229-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Multi-agent systems with ANote",
        "submission-date": "2003/12",
        "publication-date": "2004/08",
        "abstract": "An important issue in getting the agent technology into mainstream software development is the development of appropriate methodologies for developing agent-oriented systems. This paper presents an approach to model distributed systems based on a goal-oriented requirements acquisition. These models are acquired as instances of a conceptual meta-model. The latter can be represented as a graph where each node captures a concept such as, e.g., goal, action, agent, or scenario, and where the edges capture semantic links between such abstractions. This approach is supported by a modeling language, the ANote, which presents views that capture the most important modeling aspects according to the concept currently under consideration.",
        "keywords": [
            "Software agents",
            "Multi-agent system analysis",
            "Modeling language",
            "Views"
        ],
        "authors": [
            "Ricardo Choren",
            "Carlos Lucena"
        ],
        "file_path": "data/sosym-all/s10270-004-0065-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Gamiﬁcation of business process modeling education: an experimental analysis",
        "submission-date": "2023/08",
        "publication-date": "2024/04",
        "abstract": "Gamiﬁcation, the practice of using game elements in non-recreational contexts to increase user participation and interest, has been applied more and more throughout the years in software engineering. Business process modeling is a skill considered fundamental for software engineers, with Business Process Modeling Notation (BPMN) being one of the most commonly used notations for this discipline. BPMN modeling is present in different curricula in speciﬁc Master’s Degree courses related to software engineering but is usually seen by students as an unappealing or uninteresting activity. Gamiﬁcation could potentially solve this issue, though there have been no relevant attempts in research yet. This paper aims at collecting preliminary insights on how gamiﬁcation affects students’ motivation in performing BPMN modeling tasks and—as a consequence—their productivity and learning outcomes. A web application for modeling BPMN diagrams augmented with gamiﬁcation mechanics such as feedback, rewards, progression, and penalization has been compared with a non-gamiﬁed version that provides more limited feedback in an experiment involving 200 students. The diagrams modeled by the students are collected and analyzed after the experiment. Students’ opinions are gathered using a post-experiment questionnaire. Statistical analysis showedthatgamiﬁcationleadsstudentstocheckmoreoftenfortheirsolutions’correctness,increasingthesemanticcorrectness of their diagrams, thus showing that it can improve students’ modeling skills. The results, however, are mixed and require additional experiments in the future to ﬁne-tune the tool for actual classroom use.",
        "keywords": [
            "Gamiﬁcation",
            "BPMN modeling",
            "Teaching",
            "Information systems"
        ],
        "authors": [
            "Giacomo Garaccione",
            "Riccardo Coppola",
            "Luca Ardito",
            "Marco Torchiano"
        ],
        "file_path": "data/sosym-all/s10270-024-01171-3.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "How are informal diagrams used in software engineering? An exploratory study of open-source and industrial practices",
        "submission-date": "2024/03",
        "publication-date": "2024/12",
        "abstract": "In software engineering practice, models created for communication and documentation are often informal. This limits the applicability of powerful model-driven engineering mechanisms. Understanding the motivations and use of informal diagrams can improve modelling techniques and tools, by bringing together the beneﬁts of both informal diagramming and modelling using modelling languages and modelling tools. In this paper, we report on an initial exploration effort to investigate the use of informal diagramming in both open-source software repositories and industrial software engineering practices. We carried out a repository mining study on open-source software repositories seeking informal diagrams and classiﬁed them according to what they represent and how they are used. Additionally, we describe industrial practices that rely to some extent on informal diagramming, as gathered through unstructured interviews with practitioners. We compare the ﬁndings from these data sources and discuss how informal diagrams are used in practice.",
        "keywords": [
            "Informal diagramming",
            "Repository mining",
            "Flexible modelling"
        ],
        "authors": [
            "Robbert Jongeling",
            "Antonio Cicchetti",
            "Federico Ciccozzi"
        ],
        "file_path": "data/sosym-all/s10270-024-01252-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mining team compositions for collaborative work in business processes",
        "submission-date": "2015/10",
        "publication-date": "2016/10",
        "abstract": "Process mining aims at discovering processes by extracting knowledge about their different perspectives from event logs. The resource perspective (or organisational perspective) deals, among others, with the assignment of resources to process activities. Mining in relation to this perspective aims to extract rules on resource assignments for the process activities. Prior research in this area is limited by the assumption that only one resource is responsible for each process activity, and hence, collaborative activities are disregarded. In this paper, we leverage this assumption by developing a process mining approach that is able to discover team compositions for collaborative process activities from event logs. We evaluate our novel mining approach in terms of computational performance and practical applicability.",
        "keywords": [
            "Business process management",
            "Declarative process mining",
            "Event log analysis",
            "Resource perspective",
            "Teamwork"
        ],
        "authors": [
            "Stefan Schönig",
            "Cristina Cabanillas",
            "Claudio Di Ciccio",
            "Stefan Jablonski",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-016-0567-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "GoRIM: a model-driven method for enhancing regulatory intelligence",
        "submission-date": "2020/08",
        "publication-date": "2021/11",
        "abstract": "Regulators are under constant pressure to demonstrate if and how the regulations they administer, which impose many requirements on various systems and processes, achieve intended societal outcomes. Traditionally, regulators have relied on impact assessments, risk analysis, and cost–beneﬁt analysis to assess compliance with regulations. These methods, however, are effort and time intensive and focus on the efﬁciency of regulatory processes rather than on the effectiveness of the regulatory initiatives meant to improve compliance to regulations and the latter’s impact on intended societal outcomes. Goal-oriented modelling and data analytics approaches provide the basis for the development of more sophisticated methods and tools to better address the needs of regulators. This paper introduces the goal-oriented regulatory intelligence method (GoRIM), which enables effective management of regulations through modelling and data analytics. Through continuous monitoring, assessing, and reporting on efﬁciency and effectiveness aspects, GoRIM is meant to facilitate the analysis of feedback loops between regulations, regulatory initiatives, and societal outcomes. To demonstrate the applicability and perceived usefulness of GoRIM in addressing the ﬁrst feedback loop between regulations and initiatives, we evaluated it through three case studies involving regulators from different contexts, with positive results. GoRIM extends the concept of regulatory intelligence beyond the analysis of compliance. It also provides practical guidelines and tools to regulators for making, in a timely way, evidence-based decisions related to the addition, modiﬁcation, or repeal of regulations and related regulatory initiatives. In addition, GoRIM helps better identify software and information needs for enabling such decisions.",
        "keywords": [
            "Data analytics",
            "Evidence-based decision-making",
            "Goal-oriented modelling",
            "GRL",
            "Regulations modelling",
            "Regulatory intelligence"
        ],
        "authors": [
            "Okhaide Akhigbe",
            "Daniel Amyot",
            "Gregory Richards",
            "Lysanne Lessard"
        ],
        "file_path": "data/sosym-all/s10270-021-00949-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrated revision and variation control for evolving model-driven software product lines",
        "submission-date": "2018/01",
        "publication-date": "2019/02",
        "abstract": "Software engineering projects are faced with abstraction, which is achieved by software models, historical evolution, which is addressed by revision control, and variability, which is managed with the help of software product line engineering. Addressing these phenomena by separate tools ignores obvious overlaps and therefore fails at exploiting synergies between revision and variation control for models. In this article, we present a conceptual framework for integrated revision and variation control of model-driven software projects. The framework reuses the abstractions of revision graphs and feature models and follows an iterative, revision-control-like approach to software product line engineering called product-based product line development. A single version (i.e., a variant of a selected revision) is made available in a workspace, where the user may apply arbitrary modiﬁcations. Based on a user-provided speciﬁcation of the affected variants, the changes are automatically written back to a transparent repository that relies on an internal multi-version storage. The uniform handling of revisions and variants of models is achieved by transparently mapping version concepts to a semantic base layer, which is deﬁned upon propositional logic. At the heart of the conceptual framework is a dynamic ﬁltered editing model, which allows that the versioned artifacts and the feature model co-evolve. We contribute algorithms for checkout and commit, which satisfy a set of consistency constraints referring to variant speciﬁcations in an evolving feature model. This article furthermore addresses the orchestration of collaborative development by distributed replication and the well formedness of text and model artifacts to be checked out into the workspace. The Eclipse-based tool SuperMod demonstrates the feasibility of the conceptual framework. It allows the user to reuse arbitrary editing tools for text-based programming and/or Ecore-based modeling languages. An evaluation based on three case studies investigates the properties of SuperMod with a speciﬁc focus on ﬁltered editing. The evaluation demonstrates that the dynamic ﬁltered editing model reduces the cognitive complexity and the amount of user interaction necessary for variation control when compared to unﬁltered model-driven approaches to software product line engineering.",
        "keywords": [
            "Model versioning",
            "Model-driven product lines",
            "Variation control systems",
            "Tool integration",
            "Integrated historical and logical versioning"
        ],
        "authors": [
            "Felix Schwägerl",
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-019-00722-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction: iDOCEM",
        "submission-date": "2024/08",
        "publication-date": "2024/08",
        "abstract": "In this article, the title of the paper was published as “iDO-CEM”. It should be corrected as “iDOCEM: deﬁning a common terminology for object-centric event logging and data-centric process modelling”.\nThe original article has been updated.",
        "keywords": [],
        "authors": [
            "Charlotte Verbruggen",
            "Alexandre Goossens",
            "Johannes De Smedt",
            "Jan Vanthienen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-024-01200-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Refactoring OCL annotated UML class diagrams",
        "submission-date": "2006/03",
        "publication-date": "2007/05",
        "abstract": "Refactoring of UML class diagrams is an emerging research topic and heavily inspired by refactoring of program code written in object-oriented implementation languages. Current class diagram refactoring techniques concentrate on the diagrammatic part but neglect OCL constraints that might become syntactically incorrect by changing the underlying class diagram. This paper formalizes the most important refactoring rules for class diagrams and classifies them with respect to their impact on attached OCL constraints. For refactoring rules that have an impact on OCL constraints, we formalize the necessary changes of the attached constraints. Our refactoring rules are specified in a graph-grammar inspired formalism. They have been implemented as QVT transformation rules. We finally discuss for our refactoring rules the problem of syntax preservation and show, by using the KeY-system, how this can be resolved.",
        "keywords": [
            "Refactoring",
            "QVT",
            "Imperative OCL",
            "Graph-transformations",
            "Syntax preserving refactoring rules",
            "Source code verification"
        ],
        "authors": [
            "Slaviša Markovi´c",
            "Thomas Baar"
        ],
        "file_path": "data/sosym-all/s10270-007-0056-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Advanced prefetching and caching of models with PrefetchML",
        "submission-date": "2017/03",
        "publication-date": "2018/03",
        "abstract": "Caching and prefetching techniques have been used for decades in database engines and ﬁle systems to improve the perfor- mance of I/O-intensive application. A prefetching algorithm typically beneﬁts from the system’s latencies by loading into main memory elements that will be needed in the future, speeding up data access. While these solutions can bring a signiﬁcant improvement in terms of execution time, prefetching rules are often deﬁned at the data level, making them hard to understand, maintain, and optimize. In addition, low-level prefetching and caching components are difﬁcult to align with scalable model persistence frameworks because they are unaware of potential optimizations relying on the analysis of metamodel-level infor- mation and are less present in NoSQL databases, a common solution to store large models. To overcome this situation, we propose PrefetchML, a framework that executes prefetching and caching strategies over models. Our solution embeds a DSL to conﬁgure precisely the prefetching rules to follow and a monitoring component providing insights on how the prefetching execution is working to help designers optimize his performance plans. Our experiments show that PrefetchML is a suitable solution to improve query execution time on top of scalable model persistence frameworks. Tool support is fully available online as an open-source Eclipse plug-in.",
        "keywords": [
            "Prefetching",
            "MDE",
            "DSL",
            "Scalability",
            "Persistence framework",
            "NoSQL"
        ],
        "authors": [
            "Gwendal Daniel",
            "Gerson Sunyé",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-018-0671-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhanced graph rewriting systems for complex software domains",
        "submission-date": "2013/12",
        "publication-date": "2014/09",
        "abstract": "Methodologies for correct by construction rec-onﬁgurations can efﬁciently solve consistency issues in dynamic software architecture. Graph-based models are appropriate for designing such architectures and methods. At the same time, they may be unfit to characterize a system from a non-functional perspective. This stems from efﬁciency and applicability limitations in handling time-varying character-istics and their related dependencies. In order to lift these restrictions, an extension to graph rewriting systems is proposed herein. The suitability of this approach, as well as the restraints of currently available ones, is illustrated, analyzed, and experimentally evaluated with reference to a concrete example. This investigation demonstrates that the conceived solution can (i) express any kind of algebraic dependencies between evolving requirements and properties; (ii) significantly ameliorate the efﬁciency and scalability of system modiﬁcations with respect to classic methodologies; (iii) provide an efﬁcient access to attribute values; (iv) be fruitfully exploited in software management systems; and (v) guarantee theoretical properties of a grammar, like its termination.",
        "keywords": [
            "Constrained and attributed rewriting systems",
            "Graph rewriting systems",
            "Non-functional requirements",
            "Dynamic software architecture",
            "Correctness by construction"
        ],
        "authors": [
            "Cédric Eichler",
            "Thierry Monteil",
            "Patricia Stolf",
            "Luigi Alfredo Grieco",
            "Khalil Drira"
        ],
        "file_path": "data/sosym-all/s10270-014-0433-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A method of reﬁnement in UML-B",
        "submission-date": "2012/05",
        "publication-date": "2013/12",
        "abstract": "UML-B is a ‘UML-like’ graphical front-end for Event-B that provides support for object-oriented and state machine modelling concepts, which are not available in Event-B. In particular, UML-B includes class diagram and state machine diagram editors with automatic generation of corresponding Event-B. In Event-B, reﬁnement is used to relate system models at different abstraction levels. The same reﬁnement concepts are also applicable in UML-B but require special consideration due to the higher-level modelling concepts. In previous work, we described a case study to introduce support for reﬁnement in UML-B. We now provide a more complete presentation of the technique of reﬁnement in UML-B including a formalisation of the reﬁnement rules and a deﬁnition of the extensions to the abstract syntax of UML-B notation. The provision of gluing invariants to discharge the proof obligations associated with a reﬁnement is a signiﬁcant step in providing veriﬁable models. We discuss and compare two approaches for constructing gluing invariants in the context of UML-B reﬁnement.",
        "keywords": [
            "Visual modelling languages",
            "Formal speciﬁcation",
            "UML-B",
            "Event-B",
            "Class diagram",
            "State machine"
        ],
        "authors": [
            "Mar Yah Said",
            "Michael Butler",
            "Colin Snook"
        ],
        "file_path": "data/sosym-all/s10270-013-0391-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A system-theoretic assurance framework for safety-driven systems engineering",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "The complexity of safety-critical systems is continuously increasing. To create safe systems despite the complexity, the\nsystem development requires a strong integration of system design and safety activities. A promising choice for integrating\nsystem design and safety activities are model-based approaches. They can help to handle complexity through abstraction,\nautomation, and reuse and are applied to design, analyze, and assure systems. In practice, however, there is often a disconnect\nbetween the model-based design and safety activities. At the same time, there is often a delay until recent approaches are\navailable in model-based frameworks. As a result, the advantages of the models are often not fully utilized. Therefore, this\narticle proposes a framework that integrates recent approaches for system design (model-based systems engineering), safety\nanalysis (system-theoretic process analysis), and safety assurance (goal structuring notation). The framework is implemented\nin the systems modeling language (SysML), and the focus is placed on the connection between the safety analysis and\nsafety assurance activities. It is shown how the model-based integration enables tool assistance for the systematic creation,\nanalysis, and maintenance of safety artifacts. The framework is demonstrated with the system design, safety analysis, and\nsafety assurance of a collision avoidance system for aircraft. The model-based nature of the design and safety activities is\nutilized to support the systematic generation, analysis, and maintenance of safety artifacts.",
        "keywords": [
            "MBSE",
            "Safety",
            "STPA",
            "SysML",
            "GSN"
        ],
        "authors": [
            "Alexander Ahlbrecht",
            "Jasper Sprockhoff",
            "Umut Durak"
        ],
        "file_path": "data/sosym-all/s10270-024-01209-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Optimization framework for DFG-based automated process discovery approaches",
        "submission-date": "2020/02",
        "publication-date": "2021/02",
        "abstract": "The problem of automatically discovering business process models from event logs has been intensely investigated in the past two decades, leading to a wide range of approaches that strike various trade-offs between accuracy, model complexity, and execution time. A few studies have suggested that the accuracy of automated process discovery approaches can be enhanced by means of metaheuristic optimization techniques. However, these studies have remained at the level of proposals without validation on real-life datasets or they have only considered one metaheuristic in isolation. This article presents a metaheuristic optimization framework for automated process discovery. The key idea of the framework is to construct a directly-follows graph (DFG) from the event log, to perturb this DFG so as to generate new candidate solutions, and to apply a DFG-based automated process discovery approach in order to derive a process model from each DFG. The framework can be instantiated by linking it to an automated process discovery approach, an optimization metaheuristic, and the quality measure to be optimized (e.g., ﬁtness, precision, F-score). The article considers several instantiations of the framework corresponding to four optimization metaheuristics, three automated process discovery approaches (Inductive Miner—directly-follows, Fodina, and Split Miner), and one accuracy measure (Markovian F-score). These framework instances are compared using a set of 20 real-life event logs. The evaluation shows that metaheuristic optimization consistently yields visible improvements in F-score for all the three automated process discovery approaches, at the cost of execution times in the order of minutes, versus seconds for the baseline approaches.",
        "keywords": [
            "Automated process discovery",
            "Metaheuristic optimization",
            "Process mining"
        ],
        "authors": [
            "Adriano Augusto",
            "Marlon Dumas",
            "Marcello La Rosa",
            "Sander J. J. Leemans",
            "Seppe K. L. M. vanden Broucke"
        ],
        "file_path": "data/sosym-all/s10270-020-00846-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalization of UML state machines using temporal logic",
        "submission-date": "2003/03",
        "publication-date": "2003/11",
        "abstract": "The main purpose of this paper is to approach the use of formal methods in computing. In more specific terms, we use a temporal logic to formalize the most fundamental aspects of the semantics of UML state machines. We pay special attention to the dynamic aspects of the different operations associated with states and transitions, as well as the behaviour of transitions related with composite states. This, to the best of our knowledge, has not been done heretofore using temporal logic.\n\nOur formalization is based on a temporal logic that combines points, intervals, and dates. Moreover this new temporal logic is built over an innovative and simple topological semantics, which simplifies the metatheory development.",
        "keywords": [
            "Statechart diagrams",
            "interval temporal logic",
            "specification",
            "formal semantics",
            "UML"
        ],
        "authors": [
            "Carlos Rossi",
            "Manuel Enciso",
            "Inmaculada P. de Guzm´an"
        ],
        "file_path": "data/sosym-all/s10270-003-0029-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Counterexample classification",
        "submission-date": "2022/06",
        "publication-date": "2023/07",
        "abstract": "In model checking, when a model fails to satisfy the desired speciﬁcation, a typical model checker provides a counterexample that illustrates how the violation occurs. In general, there exist many diverse counterexamples that exhibit distinct violating behaviors, which the user may wish to examine before deciding how to repair the model. Unfortunately, (1) the number of counterexamples may be too large to enumerate one by one, and (2) many of these counterexamples are redundant, in that they describe the same type of violating behavior. In this paper, we propose a technique called counterexample classiﬁcation. The goal of classiﬁcation is to cover the space of all counterexamples into a ﬁnite set of counterexample classes, each of which describes a distinct type of violating behavior for the given speciﬁcation. These classes are then presented as a summary of possible violating behaviors in the system, freeing the user from manually having to inspect or analyze numerous counterexamples to extract the same information. We have implemented a prototype of our technique on top of an existing formal modeling and veriﬁcation tool, the Alloy Analyzer, and evaluated the effectiveness of the technique on case studies involving the well-known Needham–Schroeder and TCP protocols with promising results.",
        "keywords": [
            "Model checking",
            "Formal modelling",
            "debugging"
        ],
        "authors": [
            "Cole Vick",
            "Eunsuk Kang",
            "Stavros Tripakis"
        ],
        "file_path": "data/sosym-all/s10270-023-01118-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The shape of feature code: an analysis of twenty C-preprocessor-based systems",
        "submission-date": "2014/10",
        "publication-date": "2015/07",
        "abstract": "Feature annotations (e.g., code fragments guarded by #ifdef C-preprocessor directives) control code extensions related to features. Feature annotations have long been said to be undesirable. When maintaining features that control many annotations, there is a high risk of ripple effects. Also, excessive use of feature annotations leads to code clutter, hinder program comprehension and harden maintenance. To prevent such problems, developers should monitor the use of feature annotations, for example, by setting acceptable thresholds. Interestingly, little is known about how to extract thresholds in practice, and which values are representative for feature-related metrics. To address this issue, we analyze the statistical distribution of three feature-related metrics collected from a corpus of 20 well-known and long-lived C-preprocessor-based systems from different domains. We consider three metrics: scattering degree of feature constants, tangling degree of feature expressions, and nesting depth of preprocessor annotations. Our ﬁndings show that feature scattering is highly skewed; in 14 systems (70%), the scattering distributions match a power law, making averages and standard deviations unreliable limits. Regarding tangling and nesting, the values tend to follow a uniform distribution; although outliers exist, they have little impact on the mean, suggesting that central statistics measures are reliable thresholds for tangling and nesting. Following our ﬁndings, we then propose thresholds from our benchmark data, as a basis for further investigations.",
        "keywords": [
            "Software families",
            "Preprocessor",
            "Feature-related metrics",
            "Thresholds",
            "Power-law distribution"
        ],
        "authors": [
            "Rodrigo Queiroz",
            "Leonardo Passos",
            "Marco Tulio Valente",
            "Claus Hunsen",
            "Sven Apel",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-015-0483-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mesoscale performance simulation of multicore processor systems",
        "submission-date": "2011/07",
        "publication-date": "2012/01",
        "abstract": "Modern microprocessor design relies heavily on detailed full-chip performance simulations to evaluate complex trade-offs. Typically, different design alternatives are tried out for a specific sub-system or component, while keeping the rest of the system unchanged. We observe that full-chip simulations for such studies is overkill. This paper introduces mesoscale simulation, which employs high-level modeling for the unchanged parts of a design and uses detailed cycle-accurate simulations for the components being modified. This combination of high-level and low-level modeling enables accuracy on par with detailed full-chip modeling while achieving much higher simulation speeds than detailed full-chip simulations. Consequently, mesoscale models can be used to quickly explore vast areas of the design space with high fidelity. We describe a proof-of-concept mesoscale implementation of the memory subsystem of the Cell/B.E. processor and discuss results from running various workloads.",
        "keywords": [
            "Microprocessor design",
            "Performance simulations",
            "Performance modeling"
        ],
        "authors": [
            "Peter Altevogt",
            "Tibor Kiss",
            "Mike Kistler",
            "Ram Rangan"
        ],
        "file_path": "data/sosym-all/s10270-012-0231-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Analysing refactoring dependencies using graph transformation",
        "submission-date": "2005/02",
        "publication-date": "2007/01",
        "abstract": "Refactoring is a widely accepted technique\nto improve the structure of object-oriented software.\nNevertheless, existing tool support remains restricted\nto automatically applying refactoring transformations.\nDeciding what to refactor and which refactoring to apply\nstill remains a difﬁcult manual process, due to the many\ndependencies and interrelationships between relevant\nrefactorings. In this paper, we represent refactorings as\ngraph transformations, and we propose the technique\nof critical pair analysis to detect the implicit dependen-\ncies between refactorings. The results of this analysis\ncan help the developer to make an informed decision of\nwhich refactoring is most suitable in a given context and\nwhy. We report on several experiments we carried out\nin the AGG graph transformation tool to support our\nclaims.",
        "keywords": [
            "Refactoring",
            "Graph transformation",
            "Critical pair analysis",
            "Dependency analysis",
            "AGG"
        ],
        "authors": [
            "Tom Mens",
            "Gabriele Taentzer",
            "Olga Runge"
        ],
        "file_path": "data/sosym-all/s10270-006-0044-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CEViNEdit: improving the process of creating cognitively effective graphical editors with GMF",
        "submission-date": "2019/02",
        "publication-date": "2020/10",
        "abstract": "The rise of domain-speciﬁc (Visual) languages and the inherent complexity of developing graphical editors for these languages have led to the emergence of proposals that provide support for this task. Most of these proposals are principally based on EMF and GMF, which effectively help to simplify and increase the level of automation of the development process of the editors, but it is important to recall that these proposals have some important disadvantages, mainly related to the learning curve of these technologies, poor documentation or the complexity of providing all the customisation possibilities to the user. In addition, in the process of developing a domain-speciﬁc language, issues related to graphical conventions have historically been undervalued, while most of the effort has been focused on semantic aspects. In fact, deﬁnitions of the concrete (visual) syntax of modelling languages in Software Engineering are usually based on common sense, intuition, the reuse of existing notations or emulation of common practices. In order to alleviate the inherent complexity of the EMF/GMF approach for the development of graphical editors and to support the evaluation of the quality of visual notations of modelling languages, this article presents CEViNEdit, an intuitive tool that simultaneously supports the semi-automatic generation of graphical editors and the assessment of the cognitive effectiveness of the visual notation implemented by the editor.",
        "keywords": [
            "Model-driven engineering (MDE)",
            "Domain-Speciﬁc Language (DSL)",
            "Visual notation",
            "Cognitive effectiveness"
        ],
        "authors": [
            "David Granada",
            "Juan M. Vara",
            "Mercedes Merayo",
            "Esperanza Marcos"
        ],
        "file_path": "data/sosym-all/s10270-020-00833-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Identifying duplicate functionality in textual use cases by aligning semantic actions",
        "submission-date": "2013/12",
        "publication-date": "2014/08",
        "abstract": "Developing high-quality requirements speciﬁcations often demands a thoughtful analysis and an adequate levelofexpertisefromanalysts.Althoughrequirementsmod- eling techniques provide mechanisms for abstraction and clarity, fostering the reuse of shared functionality (e.g., via UML relationships for use cases), they are seldom employed in practice. A particular quality problem of textual requirements, such as use cases, is that of having duplicate pieces of functionality scattered across the speciﬁcations. Duplicate functionality can sometimes improve readability for end users, but hinders development-related tasks such as effort estimation, feature prioritization, and maintenance, among others. Unfortunately, inspecting textual requirements by hand in order to deal with redundant functionality can be an arduous, time-consuming, and error-prone activity for ana- lysts. In this context, we introduce a novel approach called ReqAligner that aids analysts to spot signs of duplication in use cases in an automated fashion. To do so, ReqAligner com- bines several text processing techniques, such as a use case-aware classiﬁer and a customized algorithm for sequence alignment. Essentially, the classiﬁer converts the use cases into an abstract representation that consists of sequences of semantic actions, and then these sequences are compared pairwise in order to identify action matches, which become possible duplications. We have applied our technique to ﬁve real-world speciﬁcations, achieving promising results and identifying many sources of duplication in the use cases.",
        "keywords": [
            "Use case modeling",
            "Use case refactoring",
            "Natural language processing",
            "Sequence alignment",
            "Requirements engineering",
            "Machine learning"
        ],
        "authors": [
            "Alejandro Rago",
            "Claudia Marcos",
            "J. Andres Diaz-Pace"
        ],
        "file_path": "data/sosym-all/s10270-014-0431-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The triptych of conceptual modeling\nA framework for a better understanding of conceptual modeling",
        "submission-date": "2020/09",
        "publication-date": "2020/11",
        "abstract": "We understand this paper as a contribution to the “anatomy” of conceptual models. We propose a signature of conceptual models for their characterization, which allows a clear distinction from other types of models. The motivation for this work arose from the observation that conceptual models are widely discussed in science and practice, especially in computer science, but that their potential is far from being exploited. We combine our proposal of a more transparent explanation of the nature of conceptual models with an approach that classifies conceptual models as a link between the dimension of linguistic terms and the encyclopedic dimension of notions. As a paradigm we use the triptych, whose central tableau represents the model dimension. The effectiveness of this explanatory approach is illustrated by a number of examples. We derive a number of open research questions that should be answered to complete the anatomy of conceptual models.",
        "keywords": [
            "Conceptual modeling",
            "Modeling languages",
            "Model characteristics",
            "Model hierarchies",
            "Language hierarchies",
            "Concept",
            "Notion",
            "Term"
        ],
        "authors": [
            "Heinrich C. Mayr\nBernhard Thalheim"
        ],
        "file_path": "data/sosym-all/s10270-020-00836-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Numeric semantics of class diagrams with multiplicity and uniqueness constraints",
        "submission-date": "2012/03",
        "publication-date": "2012/11",
        "abstract": "We translate class diagrams with multiplicity constraintsanduniquenessattributestoinequalitiesovernon-negative integers. Based on this numeric semantics we check the satisﬁability and consistency of class diagrams and compute minimal models. We show that this approach is efﬁcient and provides succinct user feedback in the case of errors. In an experimental section we demonstrate that general off-the-shelf solvers for integer linear programming perform as well on real-world and synthetic benchmarks as specialised algorithms do, facilitating the extension of the formal model by further numeric constraints like cost functions. Our results are embedded in a research programme on reasoning about class diagrams and are motivated by applications in configuration management. Compared to other (for instance logic-based) approaches our aim is to hide the complexity of formal methods behind familiar user interfaces like class diagrams and to concentrate on problems that can be solved efficiently in order to be able to provide immediate feedback to users.",
        "keywords": [
            "Model engineering",
            "Formal methods",
            "Reasoning about class diagrams",
            "Integer linear programming",
            "Configuration management"
        ],
        "authors": [
            "Ingo Feinerer",
            "Gernot Salzer"
        ],
        "file_path": "data/sosym-all/s10270-012-0294-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Heterogeneous megamodel management using collection operators",
        "submission-date": "2018/06",
        "publication-date": "2019/06",
        "abstract": "Modelmanagementtechniqueshelptamethecomplexitycausedbythemanymodelsusedinlarge-scalesoftwaredevelopment; however, these techniques have focused on operators to manipulate individual models rather than entire collections of them. In this work, we begin to address this gap by adapting the widely used map, reduce and ﬁlter collection operators for collections of models represented by megamodels. Key parts of this adaptation include the special handling of relationships between models and the use of polymorphism to support heterogeneous model collections. We evaluate the complexity of our operators analytically and demonstrate their applicability on six diverse megamodel management scenarios. We describe our tool support for the approach and evaluate its scalability experimentally as well as its applicability on a practical application from the automotive domain.",
        "keywords": [
            "Megamodel",
            "Model management",
            "Heterogeneous"
        ],
        "authors": [
            "Rick Salay",
            "Sahar Kokaly",
            "Alessio Di Sandro",
            "Nick L. S. Fung",
            "Marsha Chechik"
        ],
        "file_path": "data/sosym-all/s10270-019-00738-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing process mining with visual resource analytics",
        "submission-date": "2024/11",
        "publication-date": "Not found",
        "abstract": "Resource analysis in process mining focuses on understanding the behavior and performance of resources involved in business processes. While previous research has provided various insights into resource-related aspects, the visualization of these insights remains insufﬁciently developed. This paper addresses this gap by proposing a novel resource analytics technique that integrates metrics from four critical resource-related areas: resource allocation, resource performance, workload distribution, and capacity utilization. The technique incorporates interactive visualizations and process model views to support process analysts in performing resource analysis tasks such as identifying bottlenecks and inefﬁciencies. Results from a user evaluation demonstrate that the proposed technique enhances the accuracy of resource analysis tasks and is highly regarded for its ease of use and perceived usefulness.",
        "keywords": [
            "Process mining",
            "Resource analysis",
            "Performance analysis",
            "Visual analytics"
        ],
        "authors": [
            "Alana Hoogmoed",
            "Djordje Djurica",
            "Maxim Vidgof",
            "Christoﬀer Rubensson",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-025-01315-z.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A modeling methodology for collaborative evaluation of future automotive innovations",
        "submission-date": "2020/03",
        "publication-date": "2021/04",
        "abstract": "The rapid introduction of innovations plays a major role in automotive industries. Today, once a member of the automotive value chain devises an innovation, the time-to-market could be years due to traditional forms of collaboration. It is thus indispensable for companies to collaborate on reducing the time to find and design innovations in order to remain competitive. One idea is to streamline the innovation process outside current product development cycles (7 + years) within the automotive value chain. In our work, we propose a modeling methodology offering a better collaboration in this innovation design phase. Beginning with the innovation idea, our approach allows capturing requirements, functional, and structural aspects of the innovation under design in an innovation model with defined semantics. This innovation model can then be exchanged between automotive partners, who in their turn can refine and exchange the refined model within an iterative process toward an initial innovation evaluation. Additionally, we propose a generic description of how such innovation models can be captured in SysML, a system modeling standard. Our approach is illustrated by a “wireless car charging” innovation case-study showing how a possible collaboration at different modeling abstraction levels could take place and how consistency of the models exchanged can be verified. The consistency check is exemplified by timing specifications.",
        "keywords": [
            "Innovation Modeling Grid (IMoG)",
            "Automotive innovations",
            "Model-based design",
            "SysML"
        ],
        "authors": [
            "Maher Fakih\nOliver Klemp\nStefan Puch\nKim Grüttner"
        ],
        "file_path": "data/sosym-all/s10270-021-00864-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using boundary objects and methodological island (BOMI) modeling in large-scale agile systems development",
        "submission-date": "2023/05",
        "publication-date": "2024/08",
        "abstract": "Large-scale systems development commonly faces the challenge of managing relevant knowledge between different organizational groups, particularly in increasingly agile contexts. Here, there is a conflict between coordination and group autonomy, and it is challenging to determine what necessary coordination information must be shared by what teams or groups, and what can be left to local team management. We introduce a way to manage this complexity using a modeling framework based on two core concepts: methodological islands (i.e., groups using different development methods than the surrounding organization) and boundary objects (i.e., artifacts that create a common understanding across team borders). However, we found that companies often lack a systematic way of assessing coordination issues and the use of boundary objects between methodological islands. As part of an iterative design science study, we have addressed this gap by producing a modeling framework (BOMI: Boundary Objects and Methodological Islands) to better capture and analyze coordination and knowledge management in practice. This framework includes a metamodel, as well as a list of bad smells over this metamodel that can be leveraged to detect inter-team coordination issues. The framework also includes a methodology to suggest concrete modeling steps and broader guidelines to help apply the approach successfully in practice. We have developed Eclipse-based tool support for the BOMI method, allowing for both graphical and textual model creation, and including an implementation of views over BOMI instance models in order to manage model complexity. We have evaluated these artifacts iteratively together with five large-scale companies developing complex systems. In this work, we describe the BOMI framework and its iterative evaluation in several real cases, reporting on lessons learned and identifying future work. We have produced a matured and stable modeling framework which facilitates understanding and reflection over complex organizational configurations, communication, governance, and coordination of knowledge artifacts in large-scale agile system development.",
        "keywords": [
            "Boundary objects",
            "Agile development",
            "Empirical studies"
        ],
        "authors": [
            "Jörg Holtmann",
            "Jennifer Horkoﬀ",
            "Rebekka Wohlrab",
            "Victoria Vu",
            "Rashidah Kasauli",
            "Salome Maro",
            "Jan-Philipp Steghöfer",
            "Eric Knauss"
        ],
        "file_path": "data/sosym-all/s10270-024-01193-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A case study about the improvement of business process models driven by indicators",
        "submission-date": "2015/02",
        "publication-date": "2015/07",
        "abstract": "Organizations are increasingly concerned about business process model improvement in their efforts to guarantee improved operational efﬁciency. Quality assurance of business process models should be addressed in the most objective manner, e.g., through the application of measures, but the assessment of measurement results is not a straightforward task and it requires the identiﬁcation of relevant indicators and threshold values, which are able to distinguish different levels of process model quality. Furthermore, indicators must support the improvements of the models by using suitable guidelines. In this paper, we present a case study to evaluate the BPMIMA framework for BP model improvement. This framework is composed of empirically validated measures related to quality characteristics of the models, a set of indicators with validated thresholds associated with modeling guidelines and a prototype supporting tool. The obtained data suggest that the redesign by applying guidelines driven by the indicator results was successful, as the understandability and modiﬁability of the models were improved. In addition, the changes in the models according to guidelines were perceived as acceptable by the practitioners who participated in the case study.",
        "keywords": [
            "Businessprocessimprovement",
            "Measurement",
            "Indicators",
            "Redesign guidelines"
        ],
        "authors": [
            "Laura Sánchez-González",
            "Félix García",
            "Francisco Ruiz",
            "Mario Piattini"
        ],
        "file_path": "data/sosym-all/s10270-015-0482-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bridging proprietary modelling and open-source model management tools: the case of PTC Integrity Modeller and Epsilon",
        "submission-date": "2018/07",
        "publication-date": "2019/05",
        "abstract": "While the majority of research on Model-Based Software Engineering revolves around open-source modelling frameworks such as the Eclipse Modelling Framework, the use of commercial and closed-source modelling tools such as RSA, Rhapsody, MagicDraw and Enterprise Architect appears to be the norm in industry at present. This technical gap can prohibit industrial users from reaping the beneﬁts of state-of-the-art research-based tools in their practice. In this paper, we discuss an attempt to bridge a proprietary UML modelling tool (PTC Integrity Modeller), which is used for model-based development of safety-critical systems at Rolls-Royce, with an open-source family of languages for automated model management (Epsilon). We present the architecture of our solution, the challenges we encountered in developing it, and a performance comparison against the tool’s built-in scripting interface. In addition, we use the bridge in a real-world industrial case study that involves the coordination with other bridges between proprietary tools and Epsilon.",
        "keywords": [
            "Model-driven engineering",
            "Model management",
            "Open-source"
        ],
        "authors": [
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Stuart Hutchesson",
            "Beatriz Sanchez Pina",
            "Alan Grigg",
            "Mole Li",
            "Dimitrios S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-019-00732-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon"
        }
    },
    {
        "title": "How effective is UML modeling ? An empirical perspective on costs and beneﬁts",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "Modeling has become a common practice in modern software engineering. Since the mid 1990s the Uniﬁed Modeling Language (UML) has become the de facto standard for modeling software systems. The UML is used in allphasesofsoftwaredevelopment:rangingfromtherequire-ment phase to the maintenance phase. However, empirical evidence regarding the effectiveness of modeling in software development is few and far apart. This paper aims to synthe-size empirical evidence regarding the effectiveness of mod-eling using UML in software development, with a special focus on the cost and beneﬁts.",
        "keywords": [
            "Uniﬁed Modeling Language",
            "Costs and beneﬁts",
            "Quality",
            "Productivity",
            "Effectiveness"
        ],
        "authors": [
            "Michel R. V. Chaudron",
            "Werner Heijstek",
            "Ariadi Nugroho"
        ],
        "file_path": "data/sosym-all/s10270-012-0278-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Statistical prioritization for software product line testing: an experience report",
        "submission-date": "2014/10",
        "publication-date": "2015/07",
        "abstract": "Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.",
        "keywords": [
            "Software product line testing",
            "Prioritization",
            "Statistical testing"
        ],
        "authors": [
            "Xavier Devroey",
            "Gilles Perrouin",
            "Maxime Cordy",
            "Hamza Samih",
            "Axel Legay",
            "Pierre-Yves Schobbens",
            "Patrick Heymans"
        ],
        "file_path": "data/sosym-all/s10270-015-0479-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "DALEC: a framework for the systematic evaluation of data-centric approaches to process management software",
        "submission-date": "2017/07",
        "publication-date": "2018/09",
        "abstract": "The increasing importance of data in business processes has led to the emergence of data-centric business process management, which deviates from the widely used activity-centric paradigm. Data-centric approaches set their focus on data, aiming at supporting data-intensive business processes and increased process ﬂexibility. The objective of this article is to gain profound insights into the maturity of different data-centric approaches as well as their capabilities. In particular, this article will provide a framework for systematically evaluating and comparing data-centric approaches, with regard to the phases of the business process lifecycle. To this end, a systematic literature review (SLR) was conducted with the goal of evaluating the capabilities of data-centric process management approaches. The SLR comprises 38 primary studies which were thoroughly analyzed. The studies were categorized into different approaches, whose capabilities were thoroughly assessed. Special focus was put on the tooling and software of the approaches. The article provides the empirically grounded DALEC framework to evaluate and compare data-centric approaches. Furthermore, the results of the SLR offer insights into existing data-centric approaches and their capabilities. Data-centric approaches promise better support of loosely structured and data-intensive business processes, which may not be adequately represented by activity-centric paradigms.",
        "keywords": [
            "Systematic literature review",
            "Data-centric BPM",
            "DALEC framework",
            "Systematic evaluation"
        ],
        "authors": [
            "Sebastian Steinau",
            "Andrea Marrella",
            "Kevin Andrews",
            "Francesco Leotta",
            "Massimo Mecella",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-018-0695-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ensuring conformance of relational model transformation speciﬁcations and implementations",
        "submission-date": "2011/04",
        "publication-date": "2012/04",
        "abstract": "The correctness of model transformations is a crucial element for model-driven engineering of high-quality software. A prerequisite to verify model transformations at the level of the model transformation speciﬁcation is that an unambiguous formal semantics exists and that the implemen-tation of the model transformation language adheres to this semantics. However, for existing relational model transfor-mation approaches, it is usually not really clear under which constraints particular implementations really conform to the formal semantics. In this paper, we will bridge this gap for the formal semantics of triple graph grammars (TGG) and an existing efﬁcient implementation. While the formal seman-tics assumes backtracking and ignores non-determinism, practical implementations do not support backtracking, re-quire rule sets that ensure determinism, and include further optimizations. Therefore, we capture how the considered TGGimplementationrealizesthetransformationbymeansof operational rules, deﬁne required criteria, and show confor-mance to the formal semantics if these criteria are fulﬁlled.",
        "keywords": [],
        "authors": [
            "Holger Giese",
            "Stephan Hildebrandt",
            "Leen Lambers"
        ],
        "file_path": "data/sosym-all/s10270-012-0247-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "TGG"
        }
    },
    {
        "title": "Precise null-pointer analysis",
        "submission-date": "2009/02",
        "publication-date": "2009/10",
        "abstract": "In Java, C or C++, attempts to dereference the\nnull value result in an exception or a segmentation fault.\nHence, it is important to identify those program points where\nthis undesired behaviour might occur or prove the other pro-\ngram points (and possibly the entire program) safe. To that\npurpose, null-pointer analysis of computer programs checks\nor infers non-null annotations for variables and object\nﬁelds. With few notable exceptions, null-pointer analyses\ncurrently use run-time checks or are incorrect or only verify\nmanually provided annotations. In this paper, we use abstract\ninterpretation to build and prove correct a ﬁrst, ﬂow and con-\ntext-sensitive static null-pointer analysis for Java bytecode\n(and hence Java) which infers non-null annotations. It is\nbased on Boolean formulas, implemented with binary deci-\nsion diagrams. For better precision, it identiﬁes instance or\nstatic ﬁelds that remain always non-null after being initial-\nised. Our experiments show this analysis faster and more pre-\ncise than the correct null-pointer analysis by Hubert, Jensen\nand Pichardie. Moreover, our analysis deals with exceptions,\nwhich is not the case of most others; its formulation is the-\noretically clean and its implementation strong and scalable.\nWe subsequently improve that analysis by using local reason-\ning about ﬁelds that are not always non-null, but happen to\nhold a non-null value when they are accessed. This is a fre-\nquent situation, since programmers typically check a ﬁeld for\nnon-nullness before its access. We conclude with an exam-\nple of use of our analyses to infer null-pointer annotations\nwhich are more precise than those that other inference tools\ncan achieve.",
        "keywords": [
            "Null-pointer analysis",
            "Java bytecode",
            "Static\nanalysis",
            "Abstract interpretation",
            "Automatic software\nveriﬁcation"
        ],
        "authors": [
            "Fausto Spoto"
        ],
        "file_path": "data/sosym-all/s10270-009-0132-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section of BPMDS’2017: enabling business transformation by business process modeling, development and support",
        "submission-date": "2019/11",
        "publication-date": "2019/12",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a two-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of information systems theory. The goals, format and history of BPMDS can be found on the Web site http://www.bpmds.org/. This special section follows the 18th edition of the BPMDS (Business Process Modeling, Development and Support) series, organized in conjunction with CAiSE’17, which was held in Essen, Germany, June 2017. BPMDS’2017 received 24 submissions from 18 countries, and 11 papers were selected and published in Springer LNBIP 287 volume.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-019-00771-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Safe reuse in modelling language engineering using model subtyping with OCL constraints",
        "submission-date": "2021/08",
        "publication-date": "2022/09",
        "abstract": "Low-code software development promises rapid delivery of software cloud applications by employing domain-speciﬁc lan-guages (DSLs), requiring minimal traditional coding. Model-driven engineering (MDE) provides tools, modelling notations and practices suited for engineering such DSLs, both from a syntactic and semantic perspective. However, low-code software development is heavily reliant on software reuse. It is imperative to provide safe mechanisms that guarantee valid semantic reuse of structural components and their behaviour, most often in a stepwise manner. This article presents a semantic reuse technique based on model subtyping over metamodels to manage correct model-driven engineering of DSLs. Model sub-typing is generalized to structural semantics by considering OCL constraints. Moreover, model subtyping is generalized to behavioural semantics by considering speciﬁcations of model transformation operations, which may encode operational or translational semantics. Model subtyping facilitates structural and behavioural reﬁnement. It has been implemented atop a bounded model checker, realizing a semi-decidable procedure for verifying that DSL elements are safely reused. The algo-rithm ﬁnds semantic witnesses of inconsistencies when reﬁnement principles are not satisﬁed, fostering a correct stepwise engineering of DSLs. Moreover, the algorithm produces an extension metamodel that permits the as-is reuse of implementa-tions of model transformation operation speciﬁcations. Finally, the versatility of the model subtyping technique is illustrated with common use cases extracted from the research literature.",
        "keywords": [
            "Model subtyping",
            "DSL engineering",
            "Software speciﬁcation",
            "Stepwise reﬁnement"
        ],
        "authors": [
            "Artur Boronat"
        ],
        "file_path": "data/sosym-all/s10270-022-01028-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "PSL: A semantic domain for ﬂow models",
        "submission-date": "2003/06",
        "publication-date": "2004/11",
        "abstract": "Flow models underlie popular programming languages and many graphical behavior speciﬁcation tools. However, their semantics is typically ambiguous, causing miscommunication between modelers and unexpected implementation results. This article introduces a way to disambiguate common ﬂow modeling constructs, by expressing their semantics as constraints on runtime sequences of behavior execution. It also shows that reduced ambiguity enables more powerful modeling abstractions, such as partial behavior speciﬁcations. The runtime representation considered in this paper uses the Process Speciﬁcation Language (PSL), which is deﬁned in ﬁrst-order logic, making it amenable to automated reasoning.",
        "keywords": [
            "Flow model",
            "Flow semantics",
            "PSL",
            "Process speciﬁcation",
            "Control ﬂow",
            "Data ﬂow",
            "Concurrency",
            "UML",
            "Activity model"
        ],
        "authors": [
            "Conrad Bock",
            "Michael Gruninger"
        ],
        "file_path": "data/sosym-all/s10270-004-0066-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A feature-based classiﬁcation of formal veriﬁcation techniques for software models",
        "submission-date": "2016/07",
        "publication-date": "2017/03",
        "abstract": "Software models are the core development arti-\nfact in model-based engineering (MBE). The MBE paradigm\npromotes the use of software models to describe structure\nand behavior of the system under development and proposes\nthe automatic generation of executable code from the mod-\nels. Thus, defects in the models most likely propagate to\nexecutable code. To detect defects already at the modeling\nlevel, many approaches propose to use formal veriﬁcation\ntechniques to ensure the correctness of these models. These\napproaches are the subject of this survey. We review the state\nof the art of formal veriﬁcation techniques for software mod-\nels and provide a feature-based classiﬁcation that allows us\nto categorize and compare the different approaches.",
        "keywords": [
            "Model-based engineering",
            "Veriﬁcation",
            "Model\nchecking",
            "Theorem proving"
        ],
        "authors": [
            "Sebastian Gabmeyer",
            "Petra Kaufmann",
            "Martina Seidl",
            "Martin Gogolla",
            "Gerti Kappel"
        ],
        "file_path": "data/sosym-all/s10270-017-0591-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Empirical analysis of the tool support for software product lines",
        "submission-date": "2020/04",
        "publication-date": "2022/06",
        "abstract": "For the last ten years, software product line (SPL) tool developers have been facing the implementation of different variability requirements and the support of SPL engineering activities demanded by emergent domains. Despite systematic literature reviews identifying the main characteristics of existing tools and the SPL activities they support, these reviews do not always help to understand if such tools provide what complex variability projects demand. This paper presents an empirical research in which we evaluate the degree of maturity of existing SPL tools focusing on their support of variability modeling characteristics and SPL engineering activities required by current application domains. We ﬁrst identify the characteristics and activities that are essential for the development of SPLs by analyzing a selected sample of case studies chosen from application domains with high variability. Second, we conduct an exploratory study to analyze whether the existing tools support those characteristics and activities. We conclude that, with the current tool support, it is possible to develop a basic SPL approach. But we have also found out that these tools present several limitations when dealing with complex variability requirements demanded by emergent application domains, such as non-Boolean features or large conﬁguration spaces. Additionally, we identify the necessity for an integrated approach with appropriate tool support to completely cover all the activities and phases of SPL engineering. To mitigate this problem, we propose different road map using the existing tools to partially or entirely support SPL engineering activities, from variability modeling to product derivation.",
        "keywords": [
            "Empirical analysis",
            "Case studies analysis",
            "Software product lines",
            "State of the art",
            "Tool support",
            "Tooling road map",
            "Variability modeling"
        ],
        "authors": [
            "José Miguel Horcas",
            "Mónica Pinto",
            "Lidia Fuentes"
        ],
        "file_path": "data/sosym-all/s10270-022-01011-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Thirteen years of SysML: a systematic mapping study",
        "submission-date": "2018/03",
        "publication-date": "2019/05",
        "abstract": "The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research ﬁelds. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main ﬁndings: (i) there is a growing scientiﬁc interest in SysML in the last years particularly in the research ﬁeld of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-speciﬁc requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a speciﬁc UML proﬁle mostly used in systems engineering; however, the language has to be customized to accommodate domain-speciﬁc aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for ﬁnding speciﬁc approaches about SysML.",
        "keywords": [
            "SysML",
            "Systematic mapping study",
            "Systems engineering"
        ],
        "authors": [
            "Sabine Wolny",
            "Alexandra Mazak",
            "Christine Carpella",
            "Verena Geist",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-019-00735-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "HoloFlows: modelling of processes for the Internet of Things in mixed reality",
        "submission-date": "2019/11",
        "publication-date": "2021/01",
        "abstract": "Our everyday lives are increasingly pervaded by digital assistants and smart devices forming the Internet of Things (IoT). While user interfaces to directly monitor and control individual IoT devices are becoming more sophisticated and end-user friendly, applications to connect standalone IoT devices and create more complex IoT processes for automating and assisting users with repetitive tasks still require a high level of technical expertise and programming knowledge. Related approaches for process modelling in IoT mostly suggest extensions to complex modelling languages, require high levels of abstraction and technical knowledge, and rely on unintuitive tools. We present a novel approach for end-user oriented-no-code-IoT process modelling using Mixed Reality (MR) technology: HoloFlows. Users are able to explore the IoT environment and model processes among sensors and actuators as ﬁrst-class citizens by simply “drawing” virtual wires among physical IoT devices. MR technology hereby facilitates the understanding of the physical contexts and relations among the IoT devices and provides a new and more intuitive way of modelling IoT processes. The results of a user study comparing HoloFlows with classical modelling approaches show an increased user experience and decrease in required modelling knowledge and technical expertise to create IoT processes.",
        "keywords": [
            "Process modelling",
            "Mixed reality",
            "Internet of Things",
            "IoT processes",
            "End-user development"
        ],
        "authors": [
            "Ronny Seiger",
            "Romina Kühn",
            "Mandy Korzetz",
            "Uwe Aßmann"
        ],
        "file_path": "data/sosym-all/s10270-020-00859-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on PoEM’2022",
        "submission-date": "2024/06",
        "publication-date": "2024/07",
        "abstract": "This guest editorial presents the papers contributing to the 15th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modelling (PoEM 2022). The best papers were selected for invitation for revision and signiﬁcant expansion. Five papers were ﬁnally accepted in the special section. Collectively, these papers provide an excellent representation of the state of the art of Enterprise Modelling in both research and practice.",
        "keywords": [
            "Enterprise modelling",
            "Meta models",
            "Conceptual modelling",
            "Modelling methods"
        ],
        "authors": [
            "Balbir S. Barn",
            "Kurt Sandkuhl",
            "Souvik Barat",
            "Tony Clark"
        ],
        "file_path": "data/sosym-all/s10270-024-01189-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "DataMock: An Agile Approach for Building Data Models from User Interface Mockups",
        "submission-date": "2015/12",
        "publication-date": "2017/02",
        "abstract": "In modern software development, much time is devoted and much attention is paid to the activity of data modeling and the translation of data models into databases. This has motivated the proposal of different approaches and tools to support this activity, such as semiautomatic approaches that generate data models from requirements artifacts using text analysis and sets of heuristics, among other techniques. However, these approaches still suffer from important limitations, including the lack of support for requirements traceability, the poor support for detecting and solving conﬂicts in domain-speciﬁc requirements, and the considerable effort required for manually checking the generated models. This paper introduces DataMock, an Agile approach that enables the iterative building of data models from requirements speciﬁcations, while supporting traceability and allowing inconsistencies detection in data requirements and speciﬁcations. The paper also describes how the approach effectively allows improving traceabil- ity and reducing errors and effort to build data models in comparison with traditional, state-of-the-art, data modeling approaches.",
        "keywords": [
            "Data modeling",
            "Agile methods",
            "Mockups",
            "Annotations",
            "Requirements engineering",
            "Requirements traceability",
            "Model-driven development"
        ],
        "authors": [
            "José Matías Rivero",
            "Julián Grigera",
            "Damiano Distante",
            "Francisco Montero",
            "Gustavo Rossi"
        ],
        "file_path": "data/sosym-all/s10270-017-0586-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending UML with coordination contracts",
        "submission-date": "2004/09",
        "publication-date": "2005/09",
        "abstract": "Coordination contracts are a software analysis and design construct which enable separation between the stable components of a system and the rules which define the interactions of these components. This separation supports rapid evolution of rules without requiring modification to components. In this paper we show that contracts can be defined in UML, and we define an MDA-based development process which makes use of contracts.",
        "keywords": [
            "Coordination contracts",
            "UML",
            "MDA"
        ],
        "authors": [
            "K. Lano",
            "J. L. Fiadeiro"
        ],
        "file_path": "data/sosym-all/s10270-005-0095-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model transformation intents and their properties",
        "submission-date": "2013/09",
        "publication-date": "2014/07",
        "abstract": "The notion of model transformation intent is proposed to capture the purpose of a transformation. In this paper, a framework for the description of model transformation intents is defined, which includes, for instance, a description of properties a model transformation has to satisfy to qualify as a suitable realization of an intent. Several common model transformation intents are identified, and the framework is used to describe six of them in detail. A case study from the automotive industry is used to demonstrate the usefulness of the proposed framework for identifying crucial properties of model transformations with different intents and to illustrate the wide variety of model transformation intents that an industrial model-driven software development process typically encompasses.",
        "keywords": [
            "Model transformation",
            "Intent",
            "Property",
            "Verification",
            "Description framework"
        ],
        "authors": [
            "Levi Lúcio",
            "Moussa Amrani",
            "Juergen Dingel",
            "Leen Lambers",
            "Rick Salay",
            "Gehan M. K. Selim",
            "Eugene Syriani",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-014-0429-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards clone detection in UML domain models",
        "submission-date": "2010/11",
        "publication-date": "2011/10",
        "abstract": "Code clones (i.e., duplicate fragments of code) have been studied for long, and there is strong evidence that they are a major source of software faults. Anecdotal evidence suggests that this phenomenon occurs similarly in models, suggesting that model clones are as detrimental to model quality as they are to code quality. However, programming language code and visual models have significant differences that make it difﬁcult to directly transfer notions and algorithms developed in the code clone arena to model clones. In this article, we develop and propose a definition of the notion of “model clone” based on the thorough analysis of practical scenarios. We propose a formal definition of model clones, specify a clone detection algorithm for UML domain models, and implement it prototypically. We investigate different similarity heuristics to be used in the algorithm, and report the performance of our approach. While we believe that our approach advances the state of the art significantly, it is restricted to UML models, its results leave room for improvements, and there is no validation by ﬁeld studies.",
        "keywords": [
            "Model clones",
            "Model management",
            "Model evolution",
            "Model maintenance",
            "Model similarity"
        ],
        "authors": [
            "Harald Störrle"
        ],
        "file_path": "data/sosym-all/s10270-011-0217-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling compliance speciﬁcations in linear temporal logic, event processing language and property speciﬁcation patterns: a controlled experiment on understandability",
        "submission-date": "2018/05",
        "publication-date": "2019/02",
        "abstract": "Mature veriﬁcation and monitoring approaches, such as complex event processing and model checking, can be applied for checking compliance speciﬁcations at design time and runtime. Little is known about the understandability of the different formal and technical languages associated with these approaches. This uncertainty regarding understandability might be a major obstacle for the broad practical adoption of those techniques. This article reports a controlled experiment with 215 participants on the understandability of modeling compliance speciﬁcations in representative modeling languages, namely linear temporal logic (LTL), the complex event processing-based event processing language (EPL) and property speciﬁcation patterns (PSP). The formalizations in PSP were overall more correct. That is, the pattern-based approach provides a higher level of understandability than EPL and LTL. More advanced users, however, seemingly are able to cope equally well with PSP and EPL in modeling compliance speciﬁcations.",
        "keywords": [
            "Controlled experiment",
            "Understandability",
            "Linear temporal logic",
            "Property speciﬁcation patterns",
            "Complex event processing",
            "Event processing language"
        ],
        "authors": [
            "Christoph Czepa\nAmirali Amiri\nEvangelos Ntentos\nUwe Zdun"
        ],
        "file_path": "data/sosym-all/s10270-019-00721-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven engineering of SAP core data services - the BIGER2CDS modeling tool",
        "submission-date": "2024/10",
        "publication-date": "2025/08",
        "abstract": "This paper introduces bigER2CDS, a novel model-driven engineering approach and tool support for SAP Core Data Services (CDS). bigER2CDS addresses the need for a higher abstraction level in CDS development, enabling blended, i.e., textual and graphical modeling of CDS Views through a domain-speciﬁc modeling language. Based on web technologies and the Language Server Protocol (LSP), we realized a modeling tool for SAP CDS. Our tool supports the hybrid modeling of CDS and the import of existing SAP CDS view entities for analysis and development support. This model-driven approach aims to enable domain experts to develop CDS views, mitigating the need for extensive programming skills. We report on the development of the ER2CDS domain-speciﬁc language (DSL) and the implementation of the corresponding bigER2CDS modeling tool. Finally, bigER2CDS is evaluated in the form of a controlled experiment and a case study with domain experts and CDS developers. The results show a high usability score for our tool and a willingness by domain experts and CDS developers to use it. The tool can be freely downloaded from the VS Code marketplace: https://marketplace.visualstudio.com/items?itemName=BIGModelingTools.er2cds.",
        "keywords": [
            "Model-driven engineering",
            "SAP Core Data Services",
            "Domain-speciﬁc language",
            "CDS",
            "Modeling tool",
            "LSP",
            "Langium",
            "Sprotty"
        ],
        "authors": [
            "Gallus Huber",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-025-01320-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Component-based veriﬁcation using incremental design and invariants",
        "submission-date": "2013/02",
        "publication-date": "2014/04",
        "abstract": "We propose invariant-based techniques for the efﬁcient veriﬁcation of safety and deadlock-freedom properties of component-based systems. Components and their interactions are described in the BIP language. Global invariants of composite components are obtained by combining local invariants of their constituent components with interaction invariants that take interactions into account. We study new techniques for computing interaction invariants. Some of these techniques are incremental, i.e., interaction invariants of a composite hierarchically structured component are computed by reusing invariants of its constituents. We formalize incremental construction of components in the BIP language as the process of building progressively complex components by adding interactions (synchronization constraints) to atomic components. We provide sufﬁcient conditions ensuring preservation of invariants when new interactions are added. When these conditions are not satisﬁed, we propose methods for generating new invariants in an incremental manner by reusing existing invariants from the constituents in the incremental construction. The reuse of existing invariants reduces considerably the overall veriﬁcation effort. The techniques have been implemented in the D-Finder toolset. Among the experiments conducted, we have been capable of verifying safety properties and deadlock-freedom of sub-systems of the functional level of the DALA autonomous robot. This work goes far beyond the capacity of existing monolithic veriﬁcation tools.",
        "keywords": [
            "Veriﬁcation method",
            "Invariant",
            "Component-based systems",
            "Incremental design",
            "Veriﬁcation tools",
            "Deadlock-freedom",
            "BIP"
        ],
        "authors": [
            "Saddek Bensalem",
            "Marius Bozga",
            "Axel Legay",
            "Thanh-Hung Nguyen",
            "Joseph Sifakis",
            "Rongjie Yan"
        ],
        "file_path": "data/sosym-all/s10270-014-0410-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for EMMSAD’2020 special section",
        "submission-date": "2021/06",
        "publication-date": "2021/06",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) conference series organized 25 events from 1996 to 2020, associated with CAISE (Conference on Advanced Information Systems Engineering). From 2009, EMMSAD has become a two-day working conference. From 2017, EMMSAD best papers are invited to submit extended versions for considering their publication in the Journal of Software and Systems Modeling (SoSyM). The main topics of the EMMSAD series have the focus on models and modeling methods for software and information systems development.",
        "keywords": [],
        "authors": [
            "Iris Reinhartz‑Berger",
            "Jelena Zdravkovic"
        ],
        "file_path": "data/sosym-all/s10270-021-00903-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Language-speciﬁc model checking of UML-RT models",
        "submission-date": "2014/09",
        "publication-date": "2015/07",
        "abstract": "Abstract Model-driven development (MDD) deals with\ncomplexities of modern software development by using mod-\nels. Their veriﬁcation is one of the opportunities of MDD,\nsince it can be performed in the early stages of the develop-\nment.TheprevailingtrendinveriﬁcationofMDDmodelshas\nbeen to translate them to an input language of one of the exist-\ning tools, most notably model checkers. Such an approach\nhas advantages; for instance, we can use tools that achieved\na higher level of maturity, including SPIN, NuSMV and Java\nPathFinder. However, the input languages of model checkers\nare typically not compatible with MDD models, which can\nmake the translations very complex and difﬁcult to maintain.\nMoreover,itismoredifﬁculttotakeadvantageofspeciﬁcfea-\ntures of the structure and semantics of models to, e.g., speed\nup analysis. In this paper, we depart from the translational\ntrendandpresent moredirect anddedicatedapproach. Weuse\nan MDD language, namely UML-RT (used in IBM Rational\nSoftware Architect RealTime Edition), and we introduce a\nveriﬁcation method built around its main features such as\nhierarchical structures, action code and asynchronous com-\nmunication. In our method we use a formalization tailored to\nUML-RT models. This enables very easy transformation of\nmodels, but also reduces the necessary translations of ver-\niﬁcation results and directly supports the most important\nfeatures of UML-RT. The proposed method includes an on-\nthe-ﬂy model checking algorithm based on the original CTL\nlabeling. This algorithm is further optimized to include lazy\ncomposition. In the paper, we present all necessary compo-\nnents of the checking algorithms. Additionally, we also show\ntheresultsofexperimentswithourimplementationusingsev-\neral UML-RT models and CTL formulas. The experiments\nprovide some evidence of the viability of a language-speciﬁc\nanalysis of MDD models and of the effectiveness of our opti-\nmizations in certain cases.",
        "keywords": [
            "UML-RT",
            "Model checking",
            "Lazy composition"
        ],
        "authors": [
            "Karolina Zurowska",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-015-0484-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Heuristics for composite Web service decentralization",
        "submission-date": "2011/09",
        "publication-date": "2012/08",
        "abstract": "A composite service is usually speciﬁed by means of a process model that captures control-ﬂow and data-ﬂow relations between activities that are bound to underlying component services. In mainstream service orchestration platforms, this process model is executed by a centralized orchestrator through which all interactions are channeled. This architecture is not optimal in terms of communication overhead and has the usual problems of a single point of failure. In previous work, we proposed a method for executing composite services in a decentralized manner. However, this and similar methods for decentralized composite service execution do not optimize the communication overhead between the services participating in the composition. This paper studies the problem of optimizing the selection of services assigned to activities in a decentralized composite service, both in terms of communication overhead and overall quality of service, and taking into account collocation and separation constraints that may exist between activities in the composite service. This optimization problem is formulated as a quadratic assignment problem. The paper puts forward a greedy algorithm to compute an initial solution as well as a tabu search heuristic to identify improved solutions. An experimental evaluation shows that the tabu search heuristic achieves signiﬁcant improvements over the initial greedy solution. It is also shown that the greedy algorithm combined with the tabu search heuristic scale up to models of realistic size.",
        "keywords": [
            "Service composition",
            "Decentralized service execution",
            "Quality of service"
        ],
        "authors": [
            "Walid Fdhila",
            "Marlon Dumas",
            "Claude Godart",
            "Luciano García-Bañuelos"
        ],
        "file_path": "data/sosym-all/s10270-012-0262-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating business process simulation and information system simulation for performance prediction",
        "submission-date": "2014/02",
        "publication-date": "2015/03",
        "abstract": "Business process (BP) designs and enterprise information system (IS) designs are often not well aligned. Missing alignment may result in performance problems at run-time, such as large process execution time or overloaded IS resources. The complex interrelations between BPs and ISs are not adequately understood and considered in development so far. Simulation is a promising approach to predict performance of both BP and IS designs. Based on prediction results, design alternatives can be compared and verified against requirements. Thus, BP and IS designs can be aligned to improve performance. In current simulation approaches, BP simulation and IS simulation are not adequately integrated. This results in limited prediction accuracy due to neglected interrelations between the BP and the IS in simulation. In this paper, we present the novel approach Integrated Business IT Impact Simulation (IntBIIS) to adequately reflect the mutual impact between BPs and ISs in simulation. Three types of mutual impact between BPs and ISs in terms of performance are specified. We discuss several solution alternatives to predict the impact of a BP on the performance of ISs and vice versa. It is argued that an integrated simulation of BPs and ISs is best suited to reflect their interrelations. We propose novel concepts for continuous modeling and integrated simulation. IntBIIS is implemented by extending the Palladio tool chain with BP simulation concepts. In a real-life case study with a BP and IS from practice, we validate the feasibility of IntBIIS and discuss the practicability of the corresponding tool support.",
        "keywords": [
            "Business process",
            "Information system",
            "Alignment",
            "Performance"
        ],
        "authors": [
            "Robert Heinrich",
            "Philipp Merkle",
            "Jörg Henss",
            "Barbara Paech"
        ],
        "file_path": "data/sosym-all/s10270-015-0457-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling competences in enterprise architecture: from knowledge, skills, and attitudes to organizational capabilities",
        "submission-date": "2023/03",
        "publication-date": "2024/03",
        "abstract": "Competence-basedapproacheshavereceivedincreasedattention,asthedemandforqualiﬁedpeoplewiththerightcombination\nof competences establishes itself as a major factor of organizational performance. This paper examines how competences\ncan be incorporated into Enterprise Architecture modeling: (i) we identify a key set of competence-related concepts such as\nknowledge, skills, and attitudes, (ii) analyze and relate them using a reference ontology (grounded on the Uniﬁed Foundational\nOntology), and (iii) propose a representation strategy for modeling competences and their constituent elements leveraging\nthe ArchiMate language, discussing how the proposed models can ﬁt in enterprise competence-based practices. Our approach\nis intended to cover two tasks relevant to the combined application of Enterprise Architecture and Competence Modeling:\n‘zooming in’ on competences, revealing the relations between competences, knowledge, skills, attitudes and other personal\ncharacteristics that matter in organizational performance, and ‘zooming out’ of competences, placing them in the wider context\nof other personal competences and overall organizational capabilities. An assessment of the representation is offered in the\nform of an empirical survey.",
        "keywords": [
            "Competences",
            "Ontologies",
            "Competence Modeling",
            "Enterprise Architecture"
        ],
        "authors": [
            "Rodrigo F. Calhau",
            "João Paulo A. Almeida",
            "Satyanarayana Kokkula",
            "Giancarlo Guizzardi"
        ],
        "file_path": "data/sosym-all/s10270-024-01151-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Graphical composite modeling and simulation for multi-aircraft collision avoidance",
        "submission-date": "2019/06",
        "publication-date": "2020/11",
        "abstract": "Modeling and simulation for multi-aircraft collision avoidance to understand the mechanistic behavior is an important activity. Building models using general programming language typically requires specialist knowledge, and this limits the spread of modeling and simulation approach among multi-aircraft collision avoidance scenario. Thus, a software environment is needed to support convenient development of models by assembling components, when analysis demands changes. In this work, the graphical composite modeling and simulation software (GMAS extended) for multi-aircraft collision avoidance is introduced, with the basic graphical components and a graphical assembly editor. We deﬁne the serial and parallel execution semantics of GMASE-based model and then introduce the high-level graphical modeling interface, the low-level runtime engine of GMAS, and the simulation-based decision tree, which transforms a complex decision-making process into a collection of simpler decisions of ﬁnding the no collision or optimal sequence from some initial state to the goal state. To validate its efﬁciency and practicability, a three-aircraft collision avoidance model with TCAS operations is built on GMAS, which shows that using GMAS increases reusability and hiding complexity in graphical programming by splitting complex behavior into data ﬂow and function components. The experimental result proves that GMAS not only provides a better representation for multi-aircraft collision avoidance, but also a useful approach for analyzing the potential collision occurrences.",
        "keywords": [
            "Graphical composite modeling",
            "Multi-aircraft collision avoidance",
            "Event-driven modeling and simulation",
            "Simulation-based decision tree"
        ],
        "authors": [
            "Feng Zhu",
            "Jun Tang"
        ],
        "file_path": "data/sosym-all/s10270-020-00830-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework to support alignment of secure software engineering with legal regulations",
        "submission-date": "2009/01",
        "publication-date": "2010/03",
        "abstract": "Regulation compliance is getting more and more important for software systems that process and manage sensitive information. Therefore, identifying and analysing relevant legal regulations and aligning them with security requirements become necessary for the effective development of secure software systems. Nevertheless, Secure Software Engineering Modelling Languages (SSEML) use different concepts and terminology from those used in the legal domain for the description of legal regulations. This situation, together with the lack of appropriate background and knowledge of laws and regulations, introduces a challenge for software developers. In particular, it makes difficult to perform (i) the elicitation of appropriate security requirements from the relevant laws and regulations; and (ii) the correct tracing of the security requirements throughout the development stages. This paper presents a framework to support the consideration of laws and regulations during the development of secure software systems. In particular, the framework enables software developers (i) to correctly elicit security requirements from the appropriate laws and regulations; and (ii) to trace these requirements throughout the development stages in order to ensure that the design indeed supports the required laws and regulations. Our framework is based on existing work from the area of secure software engineering, and it complements this work with a novel and structured process and a well-defined method. A practical case study is employed to demonstrate the applicability of our work.",
        "keywords": [
            "Secure software engineering",
            "Non-functional properties",
            "Security requirements",
            "Secure Tropos",
            "UMLsec",
            "Modelling regulations",
            "Legal constraints"
        ],
        "authors": [
            "Shareeful Islam",
            "Haralambos Mouratidis",
            "Jan Jürjens"
        ],
        "file_path": "data/sosym-all/s10270-010-0154-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based safety assessment with SysML and component fault trees: application and lessons learned",
        "submission-date": "2019/07",
        "publication-date": "2020/02",
        "abstract": "Mastering the complexity of safety assurance for modern, software-intensive systems is challenging in several domains, such as automotive, robotics, and avionics. Model-based safety analysis techniques show promising results to handle this challenge by automating the generation of required artifacts for an assurance case. In this work, we adapt prominent approaches and propose to augment of SysML models with component fault trees (CFTs) to support the fault tree analysis and the failure mode and effects analysis. While most existing approaches based on CFTs are only targeting the system topology, e.g., UML class diagrams, we propose an integration of CFTs with SysML internal block diagrams as well as SysML activity diagrams. We realized our approach in a prototypical tool. We conclude with best practices and lessons learned that emerged from our case studies with an electronic power steering system and a boost recuperation system.",
        "keywords": [
            "Model-based systems engineering",
            "MBSE",
            "Model-based safety analysis",
            "MBSA",
            "Fault trees",
            "Fault tree analysis",
            "FTA",
            "Component fault tree",
            "CFT",
            "Failure mode and effects analysis",
            "FMEA",
            "Safety",
            "Reliability",
            "Dependability"
        ],
        "authors": [
            "Peter Munk",
            "Arne Nordmann"
        ],
        "file_path": "data/sosym-all/s10270-020-00782-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An ontological metamodel for cyber-physical system safety, security, and resilience coengineering",
        "submission-date": "2020/06",
        "publication-date": "2021/06",
        "abstract": "Cyber-physical systems are complex systems that require the integration of diverse software, ﬁrmware, and hardware to be practical and useful. This increased complexity is impacting the management of models necessary for designing cyber-physical systems that are able to take into account a number of “-ilities”, such that they are safe and secure and ultimately resilient to disruption of service. We propose an ontological metamodel for system design that augments an already existing industry metamodel to capture the relationships between various model elements (requirements, interfaces, physical, and functional) and safety, security, and resilient considerations. Employing this metamodel leads to more cohesive and structured modeling efforts with an overall increase in scalability, usability, and uniﬁcation of already existing models. In turn, this leads to a mission-oriented perspective in designing security defenses and resilience mechanisms to combat undesirable behaviors. We illustrate this metamodel in an open-source GraphQL implementation, which can interface with a number of modeling languages. We support our proposed metamodel with a detailed demonstration using an oil and gas pipeline model.",
        "keywords": [],
        "authors": [
            "Georgios Bakirtzis",
            "Tim Sherburne",
            "Stephen Adams",
            "Barry M. Horowitz",
            "Peter A. Beling",
            "Cody H. Fleming"
        ],
        "file_path": "data/sosym-all/s10270-021-00892-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Systematic literature review of the objectives, techniques, kinds, and architectures of models at runtime",
        "submission-date": "2013/04",
        "publication-date": "2013/12",
        "abstract": "In the context of software development, models provide an abstract representation of a software system or a part of it. In the software development process, they are primarily used for documentation and communication purposes in analysis, design, and implementation activities. Model-Driven Engineering (MDE) further increases the importance of models, as in MDE models are not only used for documentation and communication, but as central artefacts of the software development process. Various recent research approaches take the idea of using models as central artefacts one step further by using models at runtime to cope with dynamic aspects of ever-changing software and its environment. In this article, we analyze the usage of models at runtime in the existing research literature using the Systematic Literature Review (SLR) research method. The main goals of our SLR are building a common classiﬁcation and surveying the existing approaches in terms of objectives, techniques, architectures, and kinds of models used in these approaches. The contribution of this article is to provide an overview and classiﬁcation of current research approaches using models at runtime and to identify research areas not covered by models at runtime so far.",
        "keywords": [
            "Models",
            "Runtime",
            "Literature review"
        ],
        "authors": [
            "Michael Szvetits",
            "Uwe Zdun"
        ],
        "file_path": "data/sosym-all/s10270-013-0394-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model checking of spacecraft operational designs: a scalability analysis",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "Ensuring the correct and safe behavior of a spacecraft is a main objective in space-system design. Since spacecraft consist of highly complex and tightly integrated components developed by large teams of engineers from various different disciplines, this is a challenging task. Increasingly, formal veriﬁcation methods such as model checking are applied to establish the correctness of safety-critical parts or subsystems. Generally, the often limited scalability of model checking due to the state-space explosion problem hinders the wide-spread adoption of this technique. In this paper, we systematically examine the scalability of model checking for verifying behavioral models that arise within early space-system design phases. For this, we created a representative model for the mode management of a satellite that can be scaled in terms of its size and the complexity of interactions between system components. The model can be transformed into the input languages of various model-checking tools, which enables a comparative study of various model-checking algorithms and also facilitates analyzing the impact of different communication schemes on the scalability. The evaluation shows promising results regarding the applicability of model checking within the spacecraft design process.",
        "keywords": [
            "Aerospace",
            "Space systems",
            "State machines",
            "Model checking",
            "Scalability"
        ],
        "authors": [
            "Philipp Chrszon",
            "Paulina Maurer",
            "George Saleip",
            "Sascha Müller",
            "Philipp M. Fischer",
            "Andreas Gerndt",
            "Michael Felderer"
        ],
        "file_path": "data/sosym-all/s10270-025-01281-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Imperative versus declarative constraint speciﬁcation languages: a controlled experiment",
        "submission-date": "2019/09",
        "publication-date": "2020/05",
        "abstract": "Model-based software engineering gains further attention these days. To better support it, the use of constraint languages is important in order to bridge expressiveness gaps and eliminate ambiguity. Nevertheless, the use of model-based constraint languages, like the Object Constraint Language (OCL), is quite limited and the speciﬁcation of constraints is left to the implementation stage. One option for these practices might be the misconception that model-based constraint languages are difficult to work with. In this paper, we examine the usages of representative constraint languages, namely OCL, for model-based constraint languages, and Java, for implementation-based constraint languages. In particular, we examine their usage in understanding and developing constraints. We evaluate these usages via a controlled experiment with 110 Information Systems Engineering undergraduate students. We found out that using OCL outperforms using Java for both understanding and developing constraints. Yet, the students had more conﬁdence with Java. The results indicate that the aforementioned misconception is wrong and there is a need for further education regarding model-based constraints languages, so to get more practice and conﬁdence.",
        "keywords": [
            "Modeling",
            "Constraint language",
            "OCL",
            "Java",
            "Evaluation",
            "Controlled experiment",
            "Imperative language",
            "Declarative language"
        ],
        "authors": [
            "Azzam Maraee",
            "Arnon Sturm"
        ],
        "file_path": "data/sosym-all/s10270-020-00796-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A query-retyping approach to model transformation co-evolution",
        "submission-date": "2019/02",
        "publication-date": "2020/06",
        "abstract": "In rule-based approaches, a model transformation deﬁnition tells how an instance of a source metamodel should be transformed to an instance of a target metamodel. As these metamodels undergo changes, model transformations deﬁned over these metamodels may get out of sync. Restoring conformance between model transformations and the metamodels is a complex and error-prone task. In this paper, we propose a formal approach to automatically co-evolve model transformations according to the evolution of the metamodels. The approach is based on encoding the model transformation deﬁnition as a query-retyping combination and the evolution of the metamodels as applications of graph transformation rules. These rules are used to obtain an evolved query over the evolved metamodel together with a new retyping from the target metamodel. We will identify the criteria which need to be fulﬁlled in order to make this automatic co-evolution possible. We provide a tool support for this procedure, in which, from a traceability model that represents the original model transformation deﬁnition, we derive a co-evolved traceability model that represents the evolved transformation deﬁnition. Moreover, we use a case study to evaluate the approach with a set of commonly performed metamodel evolutions.",
        "keywords": [
            "MDE",
            "Migration",
            "Co-evolution",
            "Graph transformations"
        ],
        "authors": [
            "Adrian Rutle",
            "Ludovico Iovino",
            "Harald König",
            "Zinovy Diskin"
        ],
        "file_path": "data/sosym-all/s10270-020-00805-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "What practitioners really want: requirements for visual notations in conceptual modeling",
        "submission-date": "2017/09",
        "publication-date": "2018/02",
        "abstract": "This research was aimed at eliciting the requirements of practitioners who use conceptual modeling in their professional work for the visual notations of modeling languages. While the use of conceptual modeling in practice has been addressed, what practitioners in fact require of the visual notation of the modeling languages they use has received little attention. This work was thus motivated by the need to understand to what extent practitioners’ requirements are acknowledged and accommodated by visual notation research efforts. A mixed-method study was conducted, with a survey being offered over the course of several months to LinkedIn professional groups. The requirements included in the survey were based on a leading design theory for visual notations, the Physics of Notations. After preprocessing, 104 participant responses were analyzed. Data analysis included descriptive coding and qualitative analysis of purposes for modeling and additional requirements beyond the scope of visual design. Statistical and factorial analysis was used to explore potential correlations between the importance of different requirements as perceived by practitioners and the demographic factors (e.g., domain, purpose, topics). The results indicate several correlations between demographic factors and the perceived importance of visual notation requirements, as well as differences in the perceived relative importance of different requirements for models used to communicate with modeling experts as compared to non-experts. Furthermore, the results show an evolution from trends identiﬁed in studies conducted in the previous decade. The identiﬁed correlations with practitioners’ demographics reveal several research challenges that should be addressed, as well as the potential beneﬁts of more purpose-speciﬁc tailoring of visual notation design. Furthermore, the shift in practitioner demographics as compared to those found in earlier work indicates that the research and development of conceptual modeling efforts needs to stay up-to-date with the way practitioners employ conceptual modeling.",
        "keywords": [
            "Visual notations",
            "Requirements",
            "Conceptual modeling"
        ],
        "authors": [
            "Dirk van der Linden",
            "Irit Hadar",
            "Anna Zamansky"
        ],
        "file_path": "data/sosym-all/s10270-018-0667-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A toolkit for model manipulation",
        "submission-date": "2002/02",
        "publication-date": "2003/10",
        "abstract": "We present a toolkit to develop scripts to process software models. It can be used to create applications to check, transform and generate derived artifacts from a model. The toolkit is based on the current OMG standards and it can be used with the Uniﬁed Modeling Language (UML) and other user-deﬁned languages based on the Meta Object Facility.",
        "keywords": [
            "UML",
            "Transformation",
            "Metamodeling",
            "Tools",
            "Scripting"
        ],
        "authors": [
            "Ivan Porres"
        ],
        "file_path": "data/sosym-all/s10270-003-0034-x.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Contract-based modeling and veriﬁcation of timed safety requirements within SysML",
        "submission-date": "2014/07",
        "publication-date": "2015/07",
        "abstract": "In order to cope with the growing complexity of critical real-time embedded systems, systems engineering has adopted a component-based design technique driven by requirements. Yet, such an approach raises several issues since it does not explicitly prescribe how system requirements can be decomposed on components nor how components contribute to the satisfaction of requirements. The envisioned solution is to design, with respect to each requirement and for each involved component, an abstract speciﬁcation, tractable at each design step, that models how the component is concerned by the satisfaction of the requirement and that can be further reﬁned toward a correct implementation. In this paper, we consider such speciﬁcations in the form of contracts. A contract for a component consists in a pair (assumption, guarantee) where the assumption models an abstract behavior of the component’s environment and the guarantee models an abstract behavior of the component given that the environment behaves according to the assumption. Therefore, contracts are a valuable asset for the correct design of systems, but also for mapping and tracing requirements to components, for tracing the evolution of requirements during design and, most importantly, for compositional veriﬁcation of requirements. The aim of this paper is to introduce contract-based reasoning for the design of critical real-time systems made of reactive components modeled with UML and/or SysML. We propose an extension of UML and SysML languages with a syntax and semantics for contracts and the reﬁnement relations that they must satisfy. The semantics of components and contracts is formalized by a variant of timed input/output automata on top of which we build a formal contract-based theory. We prove that the contract-based theory is sound and can be applied for a relatively large class of SysML system models. Finally, we show on a case study extracted from the automated transfer vehicle (http://www.esa.int/ATV) that our contract-based theory allows to verify requirement satisfaction for previously intractable models.",
        "keywords": [
            "Contract-based reasoning",
            "Safety requirement",
            "Component-based design",
            "UML/SysML",
            "Compositional veriﬁcation",
            "Timed input/output automata"
        ],
        "authors": [
            "Iulia Dragomir",
            "Iulian Ober",
            "Christian Percebois"
        ],
        "file_path": "data/sosym-all/s10270-015-0481-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSyM reﬂections of 2017: a journal status report",
        "submission-date": "2018/01",
        "publication-date": "2018/01",
        "abstract": "2017 has been an exciting year for SoSyM, which has now passed its 16th year of publication! We have continued to intensify our collaboration with the MODELS conference by presenting the annual SoSyM Awards at MODELS and strengthening the SoSyM “Journal-First” arrangement with MODELS. The ﬁrst issue of this new year’s volume summarizes the status of SoSyM in terms of recent statistics and milestone events over the past year. As an additional testament to the interest and growth in our ﬁeld, recent full professorship positions have been offered in the area of software and systems modeling, which means the area has now advanced from a subdomain of software engineering, data modeling, formal methods, and related areas to its own sustainable research domain.",
        "keywords": [],
        "authors": [
            "Geri Georg",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-018-0656-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Precise visual modeling: A case-study",
        "submission-date": "2003/12",
        "publication-date": "2005/04",
        "abstract": "We develop an abstract model for our case-study: software to support a “video rental service.” This illustrates how a visual formalism, constraint diagrams, may be used in order to specify such systems precisely.",
        "keywords": [
            "Constraint diagrams",
            "Formal methods"
        ],
        "authors": [
            "John Howse",
            "Steve Schuman"
        ],
        "file_path": "data/sosym-all/s10270-004-0074-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Gamiﬁcation of conceptual modeling education with UML class diagrams: an experimental analysis",
        "submission-date": "2024/07",
        "publication-date": "Not found",
        "abstract": "UML has become, throughout the years, the most popular modeling language for the conceptual design of software. However,\nUML diagrams are frequently ﬂawed with semantic and syntactical errors. One of the main root causes for such issues can\nbe traced back to software modeling education in software engineering curricula, which is typically given less attention\nthan core development activities. The objective of this manuscript is to describe the application of gamiﬁcation (i.e., the\nuse of game-related mechanics in non-gameful contexts) to increase the motivation and engagement of Master’s students in\nlearning the core concepts of UML modeling. Our tool prototype includes typical gamiﬁcation mechanics such as avatars,\nachievements, scoring mechanisms, and leaderboards and incorporates a system for automatic validation of the correctness of\nthe student’s solution. We empirically evaluated the beneﬁts achieved through the tool by performing a controlled experiment\nwith 280 Master’s students. We found that the use of gamiﬁcation signiﬁcantly increased the student commitment to perform\nexercises, the completeness of the exercises, and the semantic quality of the produced diagrams. Through standard usability\nquestionnaires, we also gathered positive responses and attitudes toward the usage of the tool.",
        "keywords": [
            "Gamiﬁcation",
            "UML modeling",
            "Teaching",
            "Information systems"
        ],
        "authors": [
            "Giacomo Garaccione\nRiccardo Coppola\nLuca Ardito\nMarco Torchiano"
        ],
        "file_path": "data/sosym-all/s10270-025-01282-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Recent and simple algorithms for Petri nets",
        "submission-date": "2014/01",
        "publication-date": "2014/07",
        "abstract": "We show how inductive invariants can be used to solve coverability, boundedness and reachability problems for Petri nets. This approach provides algorithms that are conceptually simpler than previously published ones.",
        "keywords": [
            "Petri nets",
            "Veriﬁcation of reachability properties",
            "Simple algorithms"
        ],
        "authors": [
            "Alain Finkel",
            "Jérôme Leroux"
        ],
        "file_path": "data/sosym-all/s10270-014-0426-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Leveraging annotation-based modeling with JUMP",
        "submission-date": "2015/05",
        "publication-date": "2016/05",
        "abstract": "The capability of UML proﬁles to serve as anno-tation mechanism has been recognized in both research and industry. Today’s modeling tools offer proﬁles speciﬁc to platforms, such as Java, as they facilitate model-based engineering approaches. However, considering the large number of possible annotations in Java, manually develop-ing the corresponding proﬁles would only be achievable by huge development and maintenance efforts. Thus, leveraging annotation-based modeling requires an automated approach capable of generating platform-speciﬁc proﬁles from Java libraries. To address this challenge, we present the fully automated transformation chain realized by Jump, thereby continuing existing mapping efforts between Java and UML by emphasizing on annotations and proﬁles. The evaluation of Jump shows that it scales for large Java libraries and gen-erates proﬁles of equal or even improved quality compared to proﬁles currently used in practice. Furthermore, we demon-strate the practical value of Jump by contributing proﬁles that facilitate reverse engineering and forward engineering processes for the Java platform by applying it to a modern-ization scenario.",
        "keywords": [
            "Java annotations",
            "UML proﬁles",
            "Model-based software engineering",
            "Forward engineering",
            "Reverse engineering"
        ],
        "authors": [
            "Alexander Bergmayr",
            "Michael Grossniklaus",
            "Manuel Wimmer",
            "Gerti Kappel"
        ],
        "file_path": "data/sosym-all/s10270-016-0528-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven development of asynchronous message-driven architectures with AsyncAPI",
        "submission-date": "2020/10",
        "publication-date": "2021/12",
        "abstract": "IntheInternet-of-Things(IoT)vision,everydayobjectsevolveintocyber-physicalsystems.Themassiveuseanddeploymentof\nthese systems has given place to the Industry 4.0 or Industrial IoT (IIoT). Due to its scalability requirements, IIoT architectures\nare typically distributed and asynchronous. In this scenario, one of the most widely used paradigms is publish/subscribe,\nwhere messages are sent and received based on a set of categories or topics. However, these architectures face interoperability\nchallenges. Consistency in message categories and structure is the key to avoid potential losses of information. Ensuring this\nconsistency requires complex data processing logic both on the publisher and the subscriber sides. In this paper, we present\nour proposal relying on AsyncAPI to automate the design and implementation of these asynchronous architectures using\nmodel-driven techniques for the generation of (part of) message-driven infrastructures. Our proposal offers two different\nways of designing the architectures: either graphically, by modeling and annotating the messages that are sent among the\ndifferent IoT devices, or textually, by implementing an editor compliant with the AsyncAPI speciﬁcation. We have evaluated\nour proposal by conducting a set of experiments with 25 subjects with different expertise and background. The experiments\nshow that one-third of the subjects were able to design and implement a working architecture in less than an hour without\nprevious knowledge of our proposal, and an additional one-third estimated that they would only need less than two hours in\ntotal.",
        "keywords": [
            "Publish/subscribe",
            "Cyber-physical systems (CPS)",
            "Message-driven architectures",
            "Asynchronous communication",
            "AsyncAPI",
            "Industrial Internet of Things (IIoT)"
        ],
        "authors": [
            "Abel Gómez",
            "Markel Iglesias-Urkia",
            "Lorea Belategi",
            "Xabier Mendialdua",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-021-00945-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling customer-centric value of system architecture investments",
        "submission-date": "2010/11",
        "publication-date": "2012/02",
        "abstract": "System architecture investments aim at improving the quality of the system in alignment with (current and future) business goals. While the costs of architecture changes are routinely calculated, identifying beneﬁts of architecture changes and translating them to a monetary value has been a challenge in practice. Currently, architecture value estimation is largely based on cost-savings or on risk mitigation, without much reliance on potential customer beneﬁts. This article reports on our experience in modeling the customer value and evaluating its potential use in choosing between different system architectures in two case studies conducted in an organization developing healthcare systems. To model the customer value, we exploit best practices in management and marketing. Management tools, in particular strategy maps and balanced scorecards, are used to identify customer-centric beneﬁts caused by architecture design decisions. Furthermore, two marketing concepts, customer value-in-use and customer segments, are adopted to quantify the value of architecture changes for a single customer and multiple customers, respectively. The paper shows that using the customer value in addition to the existing value indicators in the organization has several advantages but also calls for future improvements to be adopted in practice.",
        "keywords": [
            "Customer value",
            "Architecture investments",
            "Software architecture",
            "Decision making",
            "Customer value-in-use",
            "Customer segments",
            "Value-based software engineering",
            "Strategy maps",
            "Balanced scorecards",
            "Case study"
        ],
        "authors": [
            "Ana Ivanovi´c",
            "Pierre America",
            "Chris Snijders"
        ],
        "file_path": "data/sosym-all/s10270-012-0235-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial for FACS 2021 special section (SoSyM)",
        "submission-date": "2023/01",
        "publication-date": "2023/02",
        "abstract": "This special section of the International Journal on Software and Systems Modeling (SoSyM) contains a selection of the best papers accepted at the 17th edition of the international conference series on Formal Aspects of Component Software (FACS). This conference was held online on October 28–29, 2021. The aim of the conference series is to study how formal methods can be applied to component-based software and system development. Component-based software development proposes sound engineering principles and techniques to cope with the complexity of modern software systems. Many challenging conceptual and technological issues remain in component-based software development theory and practice. Furthermore, the advent of service-oriented and cloud computing, cyber-physical systems, and the Internet of Things comes with new challenges, such as quality of service and robustness to withstand faults, which require revisiting established concepts and developing new ones. Formal methods have provided foundations for component-based software through research on mathematical models for components, composition and adaptation, and rigorous approaches to veriﬁcation, deployment, testing, and certiﬁcation. After the FACS’21 conference took place, the ﬁve best papers were selected, and the authors of these papers were invited to submit a revised and extended version of their work to this special section. After a meticulous review process, we ﬁnally accepted three of the ﬁve invited papers.",
        "keywords": [],
        "authors": [
            "Gwen Salaün"
        ],
        "file_path": "data/sosym-all/s10270-023-01088-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model framework-based domain-speciﬁc composable modeling method for combat system effectiveness simulation",
        "submission-date": "2014/09",
        "publication-date": "2016/01",
        "abstract": "Combat system effectiveness simulation (CoSES) plays an irreplaceable role in the effectiveness measurement of combat systems. According to decades of research and practice, composable modeling and multi-domain modeling are recognized as two major modeling requirements in CoSES. Current effectiveness simulation researches attempt to cope with the structural and behavioral complexity of CoSES based on a uniﬁed technological space, and they are limited to their existing modeling paradigms and fail to meet these two requirements. In this work, we propose a model framework-based domain-speciﬁc composable modeling method to solve this problem. This method builds a common model framework using application invariant knowledge for CoSES, and designs domain-speciﬁc modeling infrastructures for subdomains as corresponding extension points of the framework to support the modeling of application variant knowledge. Therefore, this method supports domain-speciﬁc modeling in multiple subdomains and the composition of subsystem models across different subdomains based on the model framework. The case study shows that this method raises the modeling abstraction level, supports generative modeling, and promotes model reuse and composability.",
        "keywords": [
            "Modeling and simulation",
            "Composable modeling",
            "Domain-speciﬁc modeling",
            "Simulation model framework",
            "System effectiveness simulation"
        ],
        "authors": [
            "Xiao-bo Li",
            "Feng Yang",
            "Yong-lin Lei",
            "Wei-ping Wang",
            "Yi-fan Zhu"
        ],
        "file_path": "data/sosym-all/s10270-015-0513-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dual deep modeling: multi-level modeling with dual potencies and its formalization in F-Logic",
        "submission-date": "2015/02",
        "publication-date": "2016/04",
        "abstract": "An enterprise database contains a global, integrated, and consistent representation of a company’s data. Multi-level modeling facilitates the deﬁnition and maintenance of such an integrated conceptual data model in a dynamic environment of changing data requirements of diverse applications. Multi-level models transcend the traditional separation of class and object with clabjects as the central modeling primitive, which allows for a more ﬂexible and natural representation of many real-world use cases. In deep instantiation, the number of instantiation levels of a clabject or property is indicated by a single potency. Dual deepmodeling(DDM)differentiatesbetweensourcepotency and target potency of a property or association and supports the ﬂexible instantiation and reﬁnement of the property by statements connecting clabjects at different modeling lev-els. DDM comes with multiple generalization of clabjects, subsetting/specialization of properties, and multi-level cardinalityconstraints.ExamplesarepresentedusingaUML-style notation for DDM together with UML class and object diagrams for the representation of two-level user views derived from the multi-level model. Syntax and semantics of DDM are formalized and implemented in F-Logic, supporting the modeler with integrity checks and rich query facilities.",
        "keywords": [
            "Database modeling",
            "Deep instantiation",
            "Clabject",
            "UML",
            "Deep modeling notation"
        ],
        "authors": [
            "Bernd Neumayr",
            "Christoph G. Schuetz",
            "Manfred A. Jeusfeld",
            "Michael Schreﬂ"
        ],
        "file_path": "data/sosym-all/s10270-016-0519-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the Theme Section on enterprise modelling",
        "submission-date": "2013/03",
        "publication-date": "2013/03",
        "abstract": "Modern organizations rely on complex configurations of distributed IT systems that implement key business processes, provide databases, data warehousing, and business intelligence. The current business environment requires organizations to comply with a range of externally defined regulations such as Sarbanes-Oxley and BASEL II. Organizations need to be increasingly agile, robust, and be able to react to complex events, possibly in terms of dynamic reconfiguration. In order to satisfy these complex requirements, large organizations are increasingly using enterprise modelling (EM) technologies to analyse their business units, processes, resources and IT systems, and to show how these elements satisfy the goals of the business. EM describes all aspects of the construction and analysis of organizational models and supports enterprise use cases including: Business alignment where elements of a business are shown to meet its goals and in particular establishing that the IT systems and processes that run the business are consistent with the goals set by the Chief Executive. Business change management where as-is and to-be models are used to plan how a business is to be changed based on a precise definition of business component dependencies.",
        "keywords": [],
        "authors": [
            "Tony Clark",
            "Florian Matthes",
            "Balbir Barn",
            "Alan Brown"
        ],
        "file_path": "data/sosym-all/s10270-013-0327-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Establishing interoperability between EMF and MSDKVS: an M3-level-bridge to transform metamodels and models",
        "submission-date": "2023/03",
        "publication-date": "2024/04",
        "abstract": "Many powerful metamodeling platforms enabling model-driven software engineering (MDSE) exist, each with its strengths, weaknesses, functionalities, programming language(s), and developer community. Platform interoperability would enable users to exploit their mutual beneﬁts. Such interoperability would allow the transformation of metamodels and models created in one platform into equivalent metamodels and models in other platforms. Language engineers could then freely choose the metamodeling platform without risking a lock-in effect. Two well-documented and publicly available metamodeling platforms are the eclipse modeling framework (EMF) and the modeling SDK for visual studio (MSDKVS). In this paper, we propose an M3-level-bridge (M3B) that establishes interoperability between EMF and MSDKVS on the abstract syntax level and on the graphical concrete syntax level. To establish such interoperability we (i) compare the two platforms, (ii) present a conceptual mapping between them, and (iii) implement a bidirectional transformation bridge including both the metamodel and model layer. We evaluate our approach by transforming a collection of publicly available metamodels and automatically generated or manually created models thereof. The transformation outcomes are then used to quantitatively and qualitatively evaluate the transformation’s validity, executability, and expressiveness.",
        "keywords": [
            "MSDKVS",
            "EMF",
            "Metamodeling",
            "Model transformation",
            "MDSE",
            "Sirius",
            "Graphical concrete syntax",
            "Abstract syntax",
            "M3B",
            "DSL"
        ],
        "authors": [
            "Florian Cesal",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-024-01169-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Requirements for modelling tools for teaching",
        "submission-date": "2024/03",
        "publication-date": "2024/07",
        "abstract": "Modelling is an important activity in software development and it is essential that students learn the relevant skills. Modelling relies on dedicated tools and these can be complex to install, conﬁgure, and use—distracting students from learning key modelling concepts and creating accidental complexity for teachers. To address these challenges, we believe that modelling tools speciﬁcally aimed at use in teaching are required. Based on discussions at a working session organised at MODELS 2023 and the results from an internationally shared questionnaire, we report on requirements for such modelling tools for teaching. We also present examples of existing modelling tools for teaching and how they address some of the requirements identiﬁed.",
        "keywords": [
            "Modelling",
            "Education",
            "Tools",
            "Requirements"
        ],
        "authors": [
            "Jörg Kienzle",
            "Steﬀen Zschaler",
            "William Barnett",
            "Timur Sa˘glam",
            "Antonio Bucchiarone",
            "Silvia Abrahão",
            "Eugene Syriani",
            "Dimitris Kolovos",
            "Timothy Lethbridge",
            "Sadaf Mustaﬁz",
            "Soﬁa Meacham"
        ],
        "file_path": "data/sosym-all/s10270-024-01192-y.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Exploiting practical limitations of UML diagrams for model validation and execution",
        "submission-date": "2004/12",
        "publication-date": "2005/06",
        "abstract": "We suggest a framework for UML diagram validation and execution that takes advantage of some of the practical restrictions induced by diagrammatic representations (as compared to Turing equivalent programming languages) by exploiting possible gains in decidability. In particular, within our framework we can prove that an object interaction comes to an end, or that one action is always performed before another. Even more appealingly, we can compute efficiently whether two models are equivalent (aiding in the redesign or refactoring of a model), and what the differences between two models are. The framework employs a simple modelling object language (called MOL) for which we present formal syntax and semantics. A first generation of tools has been implemented that allows us to collect experience with our approach, guiding its further development.",
        "keywords": [
            "Modelling",
            "Modelling language",
            "Validation of models",
            "UML"
        ],
        "authors": [
            "Friedrich Steimann",
            "Heribert Vollmer"
        ],
        "file_path": "data/sosym-all/s10270-005-0097-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Metamodel specialization for graphical language support",
        "submission-date": "2017/03",
        "publication-date": "2018/03",
        "abstract": "Most of current modeling languages are based on graphical diagrams. The concrete graphical syntax of these languages typically is deﬁned informally—by text and diagram examples. Only recently, starting from UML 2.5, a formalism is offered for deﬁning the graphical syntax of UML. This formalism is based on Diagram Deﬁnition standard by OMG, where the main emphasis is on enabling diagram interchange between different tools implementing the given language. While this is crucial for standardized languages such as UML, this aspect is not so important for domain-speciﬁc languages. In this paper, an approach is offered for a simple direct deﬁnition of concrete graphical syntax by means of metamodels. Metamodels are typically used for a language deﬁnition, but mainly the MOF-inspired approach via meta-metamodel instantiation is used. We offer an alternative approach based on core metamodel specialization which leads to a more direct and understandable deﬁnition staying at the same meta-layer. In addition, our approach permits a natural extension—facility for a graphical editor deﬁnition for the given language, which is vital in the world of DSLs. In contrast to most DSL development platforms, which are based on the abstract syntax metamodel of the language and a mapping to graphics, our facility is based directly on the graphical syntax. But we show that in those cases where the relation to the DSL abstract syntax is really required, a mapping from the graphical syntax to abstract syntax can be relatively easily deﬁned by the specialization approach.",
        "keywords": [
            "Metamodeling",
            "Metamodel specialization",
            "Graphical syntax deﬁnition",
            "Graphical DSL",
            "Graphical editors"
        ],
        "authors": [
            "Audris Kalnins",
            "Janis Barzdins"
        ],
        "file_path": "data/sosym-all/s10270-018-0668-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model transformations for migrating legacy deployment models in the automotive industry",
        "submission-date": "2012/11",
        "publication-date": "2013/07",
        "abstract": "Many companies in the automotive industry have adopted model-driven development in their vehicle software development. As a major automotive company, General Motors (GM) has been using a custom-built, domain-specific modeling language, implemented as an internal proprietary metamodel, to meet the modeling needs in its control software development. Since AUTomotive Open System ARchitecture (AUTOSAR) has been developed as a standard to ease the process of integrating components provided by different suppliers and manufacturers, there has been a growing demand to migrate these GM-specific, legacy models to AUTOSAR models. Given that AUTOSAR defines its own metamodel for various system artifacts in automotive software development, we explore applying model transformations to address the challenges in migrating GM-specific, legacy models to their AUTOSAR equivalents. As a case study, we have built and validated a model transformation using the MDWorkbench tool, the Atlas Transformation Language, and the Metamodel Coverage Checker tool. This paper reports on the case study, makes observations based on our experience to assist in the development of similar types of transformations, and provides recommendations for further research.",
        "keywords": [
            "Model-driven development (MDD)",
            "Model transformations",
            "AUTOSAR",
            "Transformation languages and tools",
            "Automotive control software",
            "Black-box testing"
        ],
        "authors": [
            "Gehan M. K. Selim",
            "Shige Wang",
            "James R. Cordy",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-013-0365-1.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "Atlas Transformation Language"
        }
    },
    {
        "title": "A semi-formal description of migrating domain-speciﬁc models with evolving domains",
        "submission-date": "2011/03",
        "publication-date": "2013/01",
        "abstract": "One of the main advantages of deﬁning a domain-speciﬁc modeling language (DSML) is the ﬂexibility to adjust the language deﬁnition to changing requirements or in response to a deeper understanding of the domain. With the industrial applications of domain-speciﬁc modeling environments, models are valuable investments. If the modeling language evolves, these models must be seamlessly migrated to the evolved DSML. Although the changes stemming from the language evolution are not abrupt in nature, migrating existing models to a new language is still a challenging task. Our solution is the Model Change Language (MCL) tool set, which deﬁnes a DSML to describe the migration rules and then performs the model migration automatically. In this paper, we describe the precise semantics of MCL and its execution, along with the conﬂuence of the migration.",
        "keywords": [],
        "authors": [
            "Tihamer Levendovszky",
            "Daniel Balasubramanian",
            "Anantha Narayanan",
            "Feng Shi",
            "Chris van Buskirk",
            "Gabor Karsai"
        ],
        "file_path": "data/sosym-all/s10270-012-0313-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Model Change Language (MCL)"
        }
    },
    {
        "title": "How do humans inspect BPMN models: an exploratory study",
        "submission-date": "2015/10",
        "publication-date": "2016/10",
        "abstract": "Even though considerable progress regarding the technical perspective on modeling and supporting business processes has been achieved, it appears that the human perspective is still often left aside. In particular, we do not have an in-depth understanding of how process models are inspected by humans, what strategies are taken, what challenges arise, and what cognitive processes are involved. This paper contributes toward such an understanding and reports an exploratory study investigating how humans identify and classify quality issues in BPMN process models. Providing preliminary answers to initial research questions, we also indicate other research questions that can be investigated using this approach. Our qualitative analysis shows that humans adapt different strategies on how to identify quality issues. In addition, we observed several challenges appearing when humans inspect process models. Finally, we present different manners in which classification of quality issues was addressed.",
        "keywords": [
            "Process model quality",
            "Process model maintainability",
            "Empirical research",
            "Human-centered support"
        ],
        "authors": [
            "Cornelia Haisjackl",
            "Pnina Soffer",
            "Shao Yi Lim",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-016-0563-8.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Model engineering",
        "submission-date": "2003/07",
        "publication-date": "2003/07",
        "abstract": "The heightened awareness of the beneﬁts that can be derived from model-driven software and system development approaches is evident in discussions that take place in academic conferences, workshops, and industry meetings. On the other hand, there is a signiﬁcant number of developers and researchers who question the feasibility and beneﬁts of model-driven approaches. Model-based software description techniques use models, expressed in a formal language, to describe the architecture of a system, and the behavior of software artifacts.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-003-0025-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model execution tracing: a systematic mapping study",
        "submission-date": "2018/09",
        "publication-date": "2019/02",
        "abstract": "Model-Driven Engineering is a development paradigm that uses models instead of code as primary development artifacts. In\nthis paper, we focus on executable models, which are used to abstract the behavior of systems for the purpose of verifying and\nvalidating (V&V) a system’s properties. Model execution tracing (i.e., obtaining and analyzing traces of model executions)\nis an important enabler for many V&V techniques including testing, model checking, and system comprehension. This may\nexplain the increase in the number of proposed approaches on tracing model executions in the last years. Despite the increased\nattention, there is currently no clear understanding of the state of the art in this research ﬁeld, making it difﬁcult to identify\nresearch gaps and opportunities. The goal of this paper is to survey and classify existing work on model execution tracing, and\nidentify promising future research directions. To achieve this, we conducted a systematic mapping study where we examined\n64 primary studies out of 645 found publications. We found that the majority of model execution tracing approaches has been\ndeveloped for the purpose of testing and dynamic analysis. Furthermore, most approaches target speciﬁc modeling languages\nand rely on custom trace representation formats, hindering the synergy among tools and exchange of data. This study also\nrevealed that most existing approaches were not validated empirically, raising doubts as to their effectiveness in practice.\nOur results suggest that future research should focus on developing a common trace exchange format for traces, designing\nscalable trace representations, as well as conducting empirical studies to assess the effectiveness of proposed approaches.",
        "keywords": [
            "Model-driven engineering",
            "Executable models",
            "Model execution tracing",
            "Dynamic analysis of model-driven\nsystems",
            "Systematic mapping study"
        ],
        "authors": [
            "Fazilat Hojaji",
            "Tanja Mayerhofer",
            "Bahman Zamani",
            "Abdelwahab Hamou-Lhadj",
            "Erwan Bousse"
        ],
        "file_path": "data/sosym-all/s10270-019-00724-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From enterprise architecture to business models and back",
        "submission-date": "2011/11",
        "publication-date": "2012/12",
        "abstract": "In this study, we argue that important IT change processes affecting an organization’s enterprise architecture are also mirrored by a change in the organization’s business model. An analysis of the business model may establish whether the architecture change has value for the business. Therefore, in order to facilitate such analyses, we propose an approach to relate enterprise models speciﬁed in ArchiMate to business models, modeled using Osterwalder’s Business Model Canvas. Our approach is accompanied by a method that supports business model-driven migration from a baseline architecture to a target architecture and is demonstrated by means of a case study.",
        "keywords": [
            "Business modeling",
            "Enterprise architecture",
            "Business Model Canvas",
            "ArchiMate",
            "Business Model Ontology",
            "Cost/revenue analysis"
        ],
        "authors": [
            "M. E. Iacob",
            "L. O. Meertens",
            "H. Jonkers",
            "D. A. C. Quartel",
            "L. J. M. Nieuwenhuis",
            "M. J. van Sinderen"
        ],
        "file_path": "data/sosym-all/s10270-012-0304-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A reﬁnement-based approach to safe smart contract deployment and evolution",
        "submission-date": "2023/02",
        "publication-date": "2024/01",
        "abstract": "In our previous work, we proposed a veriﬁcation framework that shifts from the “code is law” to a new “speciﬁcation is law” paradigm related to the safe evolution of smart contracts. The framework proposed there relaxed the well-established requirement that, once a smart contract is deployed in a blockchain, its code is expected to be immutable. More ﬂexibly, contracts are allowed to be created and upgraded provided they meet a corresponding formal speciﬁcation that was ﬁxed. In the current paper, we extend this framework to allow speciﬁcations to evolve, provided a reﬁnement notion is preserved. We propose a notion of speciﬁcation reﬁnement tailored for smart contracts and a methodology for checking it. In addition to weakening preconditions and strengthening postconditions and invariants, we allow for the change of data representation and interface extension. Thus, we are able to reason about a signiﬁcantly wider class of smart contract evolution histories, when contrasted with the original framework. The new framework is centred around a trusted deployer: an off-chain service that formally veriﬁes and enforces the notions of implementation conformance and speciﬁcation reﬁnement. We have investigated its applicability to the safe deployment and upgrade of contracts implementing widely used Ethereum standards (the ERC20 Token Standard, the ERC3156 Flash Loans, the ERC1155 Multi Token Standard and The ERC721 standard for Non-Fungible Tokens); we handle evolutions possibly involving changes in data representation and interface extensions.",
        "keywords": [
            "Formal veriﬁcation",
            "Smart contracts",
            "Ethereum",
            "Solidity",
            "Safe deployment",
            "Safe upgrade"
        ],
        "authors": [
            "Pedro Antonino",
            "Juliandson Ferreira",
            "Augusto Sampaio",
            "A. W. Roscoe",
            "Filipe Arruda"
        ],
        "file_path": "data/sosym-all/s10270-023-01143-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "DSMCompare: domain-speciﬁc model differencing for graphical domain-speciﬁc languages",
        "submission-date": "2021/02",
        "publication-date": "2022/01",
        "abstract": "During the development of a software project, different developers collaborate on creating and changing models. These models evolve and need to be versioned. Over the past several years, progress has been made in offering dedicated support for model versioning that improves on what is being supported by text-based version control systems. However, there is still need to understand model differences in terms of the semantics of the modeling language, and to visualize the changes using its concrete syntax. To address these issues, we propose a comprehensive approach—called DSMCompare—that considers both the abstract and the concrete syntax of a domain-speciﬁc language (DSL) when expressing model differences, and which supports deﬁning domain-speciﬁc semantics for speciﬁc difference patterns. The approach is based on the automatic extension of the DSL to enable the representation of changes and on the automatic adaptation of its graphical concrete syntax to visualize the differences. In addition, we allow for the deﬁnition of semantic differencing rules to capture recurrent domain-speciﬁc difference patterns. Since these rules can be conﬂicting with each other, we introduce algorithms for conﬂict resolution and rule scheduling. To demonstrate the applicability and effectiveness of our approach, we report on evaluations based on synthetic models and on version histories of models developed by third parties.",
        "keywords": [
            "Model-driven engineering",
            "Model differencing",
            "Domain-speciﬁc languages",
            "Graphical concrete syntax"
        ],
        "authors": [
            "Manouchehr Zadahmad",
            "Eugene Syriani",
            "Omar Alam",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-021-00971-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the theme section on Agile model-driven engineering",
        "submission-date": "2022/05",
        "publication-date": "2022/05",
        "abstract": "Agile methods have become a widely used software development approach across many industry sectors [14], with established benefits in terms of increased responsiveness to change and decreased time to market. Agile practices emphasise lightweight and iterative development, designed to deliver value to customers quickly. The model-driven approach to software development (MDE) originated at about the same time as Agile, in the late 1990s, and has been most widely utilised in high-integrity domains such as vehicle control systems and aerospace. It has the benefits of a rigorous and systematic approach to software construction, based on models which represent the key concepts of the application. Because Agile and MDE both have attractive features, the idea arose of producing an integration of the two approaches, in order to gain the advantages of both. From the perspective of MDE researchers, incorporating Agile practices into MDE appeared to be a way to widen the uptake of MDE to more general software application areas. From the perspective of industrialists using Agile approaches, the introduction of MDE techniques was considered as a means of increasing the rigour and precision of their practices.",
        "keywords": [],
        "authors": [
            "Kevin Lano",
            "Shekoufeh Kolahdouz-Rahimi",
            "Javier Troya",
            "Hessa Alfraihi"
        ],
        "file_path": "data/sosym-all/s10270-022-01016-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Conceptual modelling of temporality and subjectivity as cross-cutting concerns",
        "submission-date": "2024/02",
        "publication-date": "2024/12",
        "abstract": "Conceptual models represent the world, but the world changes over time, and is different, to some extent, depending on who you ask. To cater for these aspects, conceptual models must address the representation of temporality and subjectivity. Although some work has been done to incorporate these aspects into conceptual modelling languages, no mainstream language incorporates explicit support for temporality and subjectivity in relation to the existence and predication on the represented entities. In this article, I propose an approach to conceptualising and implementing temporality and subjectivity into conceptual modelling languages as cross-cutting aspects that provide built-in language primitives to the modeller while not imposing any particular manner to represent time or subjects. The approach has been adopted by ConML, a conceptual modelling language geared towards the humanities and social sciences, implemented in the Bundt software tool, and applied to a number of research projects.",
        "keywords": [
            "Temporality",
            "Subjectivity",
            "Conceptual modelling",
            "Cross-cutting aspects",
            "ConML"
        ],
        "authors": [
            "Cesar Gonzalez-Perez"
        ],
        "file_path": "data/sosym-all/s10270-024-01247-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OCLFO: ﬁrst-order expressive OCL constraints for efﬁcient integrity checking",
        "submission-date": "2017/02",
        "publication-date": "2018/08",
        "abstract": "OCL is the standard language for deﬁning constraints in UML class diagrams. Unfortunately, as we show in this paper, full\nOCL is so expressive that it is not possible to check general OCL constraints efﬁciently. In particular, we show that checking\ngeneral OCL constraints is not only not polynomial, but not even semidecidable. To overcome this situation, we identify\nOCLFO, a fragment of OCL which is expressively equivalent to relational algebra (RA). By equivalent we mean that any\nOCLFO constraint can be checked through a RA query (which guarantees that OCLFO checking is efﬁcient, i.e., polynomial),\nand any RA query encoding some constraint can be written as an OCLFO constraint (which guarantees expressiveness of\nOCLFO). In this paper we deﬁne the syntax of OCLFO, we concisely determine its semantics through set theory, and we prove\nits equivalence to RA. Additionally, we identify the core of this language, i.e., a minimal subset of OCLFO equivalent to RA.",
        "keywords": [
            "OCL",
            "Relational algebra",
            "Integrity checking"
        ],
        "authors": [
            "Enrico Franconi",
            "Alessandro Mosca",
            "Xavier Oriol",
            "Guillem Rull",
            "Ernest Teniente"
        ],
        "file_path": "data/sosym-all/s10270-018-0688-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models for the digital transformation",
        "submission-date": "2017/04",
        "publication-date": "2017/04",
        "abstract": "“Digital transformation” is currently an important trend that penetrates many industrial and societal domains. The phrase is also emerging as a buzzword that allows different stakeholders to inject various forms of innovation into their respective company, business, government, academic institution, or other public services. The nuances of digital transformation are broad and have not yet been deﬁned precisely, but even job advertisements often contain the phrase. Deconstructing the term from its two primary words, we identify two well-known concepts. “Transformation” describes a general process that starts with some initial situation that moves toward a changed, and supposedly better situation. May be that in this case the word transformation is not the best word choice because the underlying transformation may never meet a stable end, but rather undergo a continual set of evolutionary optimizations related to new forms of business, production, logistics, medicine or other changes within the targeted domain. “Digital” suggests that many changes in society, business and industry will be driven by information technologies that allow data to be processed in real-time and even used to intelligently derive information to finally to provide stakeholders with improved knowledge about their processes and products. Downstream digitization would also allow optimization, automation activities and production techniques of various forms.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-017-0596-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model development guidelines for UML-RT: conventions, patterns and antipatterns",
        "submission-date": "2016/01",
        "publication-date": "2016/07",
        "abstract": "Software development guidelines are a set of rules which can help improve the quality of software. These rules are deﬁned on the basis of experience gained by the software developmentcommunityovertime.Thispaperdiscussesaset of design guidelines for model-based development of complex real-time embedded software systems. To be precise, we propose nine design conventions, three design patterns and thirteen antipatterns for developing UML-RT models. These guidelines have been identiﬁed based on our analysis of around 100 UML-RT models from industry and academia. Most of the guidelines are explained with the help of examples, and standard templates from the current state of the art are used for documenting the design rules.",
        "keywords": [
            "UML-RT",
            "Patterns",
            "AntiPatterns",
            "Model development guidelines"
        ],
        "authors": [
            "Tuhin Kanti Das",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-016-0549-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Relational interprocedural veriﬁcation of concurrent programs",
        "submission-date": "2010/06",
        "publication-date": "2012/03",
        "abstract": "We propose a general analysis method for recursive, concurrent programs that track effectively procedure calls and return in a concurrent context, even in the presence of unbounded recursion and inﬁnite-state variables like integers. This method generalizes the relational interprocedural analysis of sequential programs to the concurrent case, and extends it to backward or coreachability analysis. We implemented it for programs with scalar variables and experimented with several classical synchronization protocols in order to illustrate the precision of our technique and also to analyze the approximations it performs.",
        "keywords": [
            "Concurrent program analysis",
            "Interprocedural analysis",
            "Abstract interpretation",
            "Numerical abstract domains",
            "Forward and backward analysis"
        ],
        "authors": [
            "Bertrand Jeannet"
        ],
        "file_path": "data/sosym-all/s10270-012-0230-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A newly introduced Industry Voice Column",
        "submission-date": "2013/06",
        "publication-date": "2013/06",
        "abstract": "We are proud to announce a new column for the journal that is about to start with this very issue, where a first paper is included. We have started it, because we think modeling is an essential part of modern software and systems development processes. It is used in a variety of ways including informal design sketches, the basis of system analysis such as security and safety, and as a basis for industrializing software production from models. The science of software and systems modeling has been developed from the earliest days of software engineering when it was realized that abstraction is a fundamental tool for system design. Modeling research has matured through the development of many well-defined theories and notations, and is well served in terms of prominent conferences and journals including SoSyM. The application of modeling within industry is now widespread and has developed through the application of research results and as a result of standardization efforts, most notably the UML notation. Industrial model based development promises a great deal in terms of the increase of system quality while at the same time reducing the cost of system development. However, many challenges remain to be addressed by the research community. The SoSyM Industry Voice aims to provide a regular update on the application and challenges of model-based approaches to system development in industry. It provides researchers with a valuable resource of information about modeling success stories, standardization updates, and specific technology requirements that can be used to scope the research agenda. It is a regular SoSyM column (approx. 2–5 pages). The column solicits contributions in the following areas: Industrial case studies and experience reports, News from standards organizations, Industrial strategies for deployment of model-based approaches, Commercial and professional aspects of model-based engineering, Vision statements by industrial thought leaders and evangelists, Industrial input to the model-based research agenda, Historical perspectives on model-based approaches.",
        "keywords": [
            "• Industrial case studies and experience reports\n• News from standards organizations\n• Industrial strategies for deployment of model-based approaches\n• Commercial and professional aspects of model-based engineering\n• Vision statements by industrial thought leaders and evangelists\n• Industrial input to the model-based research agenda\n• Historical perspectives on model-based approaches."
        ],
        "authors": [
            "Tony Clark",
            "Gabor Karsai",
            "Roel J. Wieringa",
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-013-0361-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Uncertainty-aware environment simulation of medical devices digital twins",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Smart medical devices are an integral component of the healthcare Internet of Things (IoT), providing patients with various healthcare services through an IoT-based application. Ensuring the dependability of such applications through system and integration-level testing mandates the physical integration of numerous medical devices, which is costly and impractical. In this context, digital twins of medical devices play an essential role in facilitating testing automation. Testing with digital twins without accounting for uncertain environmental factors of medical devices leaves many functionalities of IoT-based healthcare applications untested. In addition, digital twins operating without environmental factors remain out of sync and uncalibrated with their corresponding devices functioning in the real environment. To deal with these challenges, in this paper, we propose a model-based approach (EnvDT) for modeling and simulating the environment of medical devices’ digital twins under uncertainties. We empirically evaluate the EnvDT using three medicine dispensers, Karie, Medido, and Pilly connected to a real-world IoT-based healthcare application. Our evaluation targets analyzing the coverage of environment models and the diversity of uncertain scenarios generated for digital twins. Results show that EnvDT achieves approximately 61% coverage of environment models and generates diverse uncertain scenarios (with a near-maximum diversity value of 0.62) during multiple environmental simulations.",
        "keywords": [
            "Healthcare Internet of Things (IoT)",
            "Environment modeling",
            "Digital twins",
            "Uncertainty"
        ],
        "authors": [
            "Hassan Sartaj",
            "Shaukat Ali",
            "Julie Marie Gjøby"
        ],
        "file_path": "data/sosym-all/s10270-024-01223-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MBFair: a model-based veriﬁcation methodology for detecting violations of individual fairness",
        "submission-date": "2022/11",
        "publication-date": "2024/06",
        "abstract": "Decision-making systems are prone to discrimination against individuals with regard to protected characteristics such as gender and ethnicity. Detecting and explaining the discriminatory behavior of implemented software is difﬁcult. To avoid the possibility of discrimination from the onset of software development, we propose a model-based methodology called MBFair that allows for verifying UML-based software designs with regard to individual fairness. The veriﬁcation in MBFair is performed by generating temporal logic clauses, whose veriﬁcation results enable reporting on the individual fairness of the targeted software. We study the applicability of MBFair using three case studies in real-world settings including a bank services system, a delivery system, and a loan system. We empirically evaluate the necessity of MBFair in a user study and compare it against a baseline scenario in which no modeling and tool support is offered. Our empirical evaluation indicates that analyzing the UML models manually produces unreliable results with a high chance of 46% that analysts overlook true-positive discrimination. We conclude that analysts require support for fairness-related analysis, such as our MBFair methodology.",
        "keywords": [
            "Software fairness",
            "Individual fairness",
            "Model-based veriﬁcation",
            "UML"
        ],
        "authors": [
            "Qusai Ramadan",
            "Marco Konersmann",
            "Amir Shayan Ahmadian",
            "Jan Jürjens",
            "Steﬀen Staab"
        ],
        "file_path": "data/sosym-all/s10270-024-01184-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to the theme issue on metamodelling",
        "submission-date": "2009/08",
        "publication-date": "2009/08",
        "abstract": "Linguistics, Mathematics, and Computer Science are inherently reﬂective disciplines. All of them require care in avoiding paradoxes which may be caused by uncontrolled self-reference. Early “meta” success stories in computer science date back as far as 1960. Metamodelling, as applied in such tools, has a number of associated advantages.",
        "keywords": [],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-009-0124-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extraction and evolution of architectural variability models in plugin-based systems",
        "submission-date": "2012/02",
        "publication-date": "2013/07",
        "abstract": "Variability management is a key issue when building and evolving software-intensive systems, making it possible to extend, configure, customize and adapt such systems to customers’ needs and specific deployment contexts. A wide form of variability can be found in extensible software systems, typically built on top of plugin-based architectures that offer a (large) number of configuration options through plugins. In an ideal world, a software architect should be able to generate a system variant on-demand, corresponding to a particular assembly of plugins. To this end, the variation points and constraints between architectural elements should be properly modeled and maintained over time (i.e., for each version of an architecture). A crucial, yet error-prone and time-consuming, task for a software architect is to build an accurate representation of the variability of an architecture, in order to prevent unsafe architectural variants and reach the highest possible level of flexibility. In this article, we propose a reverse engineering process for producing a variability model (i.e., a feature model) of a plugin-based architecture. We develop automated techniques to extract and combine different variability descriptions, including a hierarchical software architecture model, a plugin dependency model and the software architect knowledge. By computing and reasoning about differences between versions of architectural feature models, software architect can control both the variability extraction and evolution processes. The proposed approach has been applied to a representative, large-scale plugin-based system (FraSCAti), considering different versions of its architecture. We report on our experience in this context.",
        "keywords": [
            "Variability",
            "Product lines",
            "Reverse engineering",
            "Software evolution",
            "Architecture recovery",
            "Configuration management"
        ],
        "authors": [
            "Mathieu Acher",
            "Anthony Cleve",
            "Philippe Collet",
            "Philippe Merle",
            "Laurence Duchien",
            "Philippe Lahire"
        ],
        "file_path": "data/sosym-all/s10270-013-0364-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A method for digital business ecosystem design: situational method engineering in an action research project",
        "submission-date": "2022/03",
        "publication-date": "2022/11",
        "abstract": "Digital business ecosystem (DBE) is a paradigm that enables developing and monitoring novel business models of collabo-rating organisations and individuals using ICT as the foundation. Different from traditional online networked models such as manufacturer, retailer, or franchise centred, using a shared digital environment, DBE fosters heterogeneity, symbiosis, coevo-lution, and self-organisation of its multiple actors, which enables it to span different business domains as well as exhibits diverse interests. For many organisations and individuals, DBE presents a new collaborative approach to leverage offered and desired resources among the involved members to meet each of their goals. As such, it is foreseen to be of high value to the involved actors, but at the same time, it is often complex due to many correlated interactions of these actors and thus difﬁcult to design and manage. Furthermore, the current state of the art shows a lack of methodological guidance for DBE design. We propose a method for DBE design that follows the requirements collected from industry experts and practitioners by applying situational method engineering to enable its modularised design. The method for design is validated by action research in the setting of Digital Vaccine, a Swedish DBE managing health-related services.",
        "keywords": [
            "Digital business ecosystem",
            "Situational method engineering",
            "Method chunks",
            "Action research"
        ],
        "authors": [
            "Chen Hsi Tsai",
            "Jelena Zdravkovic",
            "Fredrik Söder"
        ],
        "file_path": "data/sosym-all/s10270-022-01068-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modularization of model transformations through a phasing mechanism",
        "submission-date": "2007/07",
        "publication-date": "2008/06",
        "abstract": "In recent years a great effort has been devoted\nto understanding the nature of model transformations. As\na result, several mechanisms to improve model transforma-\ntion languages have been proposed. Phasing has been men-\ntioned in some works as a rule scheduling or organization\nmechanism, but without any detail. In this paper, we present\na phasing mechanism in the context of rule-based transfor-\nmation languages. We explain the structure and the behavior\nof the mechanism, and how it can be integrated in a language.\nWe also analyze how the mechanism promotes modularity,\ninternal transformation composition and helps to solve usual\ntransformationproblems.Besides,weshowseveralexamples\nof application to illustrate the usefulness of the mechanism.",
        "keywords": [
            "Model transformation",
            "Transformation\nlanguages",
            "Phasing mechanism",
            "Modularity",
            "Internal\ntransformation composition"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Jesús García Molina"
        ],
        "file_path": "data/sosym-all/s10270-008-0093-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Reducing accidental complexity in domain models",
        "submission-date": "2006/12",
        "publication-date": "2007/06",
        "abstract": "A fundamental principle in engineering, including software engineering, is to minimize the amount of accidental complexity which is introduced into engineering solutions due to mismatches between a problem and the technology used to represent the problem. As model-driven development moves to the center stage of software engineering, it is particularly important that this principle be applied to the technologies used to create and manipulate models, especially models that are intended to be free of solution decisions. At present, however, there is a signiﬁcant mismatch between the “two level” modeling paradigm used to construct mainstream domain models and the conceptual information such models are required to represent—a mis-match that makes such models more complex than they need be. In this paper, we identify the precise nature of the mis-match, discuss a number of more or less satisfactory worka-rounds, and show how it can be avoided.",
        "keywords": [
            "Domain modeling",
            "Model quality",
            "Accidental complexity",
            "Modeling languages",
            "Modeling paradigm",
            "Stereotypes",
            "Powertypes",
            "Deep instantiation"
        ],
        "authors": [
            "Colin Atkinson",
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-007-0061-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ark: a constraint-based method for architectural synthesis of smart systems",
        "submission-date": "2017/09",
        "publication-date": "2019/11",
        "abstract": "As smart systems leverage capabilities of heterogeneous systems for accomplishing complex combined behaviors, they pose\nnew challenges to traditional software engineering practices that considered software architectures to be mostly static and\nstable. The software architecture of a smart system is inherently dynamic due to uncertainty surrounding its operational envi-\nronment. While the abstract architecture offers a way to implicitly describe different forms taken by the software architecture\nat run time, it is still not sufﬁcient to guarantee that all concrete architectures will automatically adhere to it. To address this\nissue, this work presents a formal method named Ark supporting the architectural synthesis of smart systems. This is achieved\nby expressing abstract architectures as a set of constraints that must be valid for any concrete architecture of the smart system.\nThis way, we can beneﬁt from existing model-checking techniques to guarantee that all concrete architectures realized from\nsuch an abstract model will comply with well-formed rules. We also describe how this method can be incorporated to a\nmodel-driven approach for bridging the gap between abstract and concrete architectural models. We demonstrate our method\nin an illustrative case study, showing how Ark can be used to support the synthesis of concrete architectures as well check the\ncorrectness and completeness of abstract architecture descriptions. Finally, we elaborate on future directions to consolidating\na process for the synthesis of run-rime architectures that are correct-by-construction.",
        "keywords": [
            "Smart system",
            "Software architecture",
            "Formal method",
            "Architectural synthesis",
            "Constraints",
            "Alloy"
        ],
        "authors": [
            "Milena Guessi",
            "Flavio Oquendo",
            "Elisa Yumi Nakagawa"
        ],
        "file_path": "data/sosym-all/s10270-019-00764-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Business process management as the “Killer App” for Petri nets",
        "submission-date": "2014/01",
        "publication-date": "2014/06",
        "abstract": "Since their inception in 1962, Petri nets have been used in a wide variety of application domains. Although Petri nets are graphical and easy to understand, they have formal semantics and allow for analysis techniques ranging from model checking and structural analysis to process mining and performance analysis. Over time Petri nets emerged as a solid foundation for Business Process Management (BPM) research. The BPM discipline develops methods, techniques, and tools to support the design, enactment, management, and analysis of operational business processes. Mainstream business process modeling notations and workﬂow management systems are using token-based semantics borrowed from Petri nets. Moreover, state-of-the-art BPM analysis techniques are using Petri nets as an internal representation. Users of BPM methods and tools are often not aware of this. This paper aims to unveil the seminal role of Petri nets in BPM.",
        "keywords": [
            "Business process management",
            "Petri nets",
            "Process modeling",
            "Process mining"
        ],
        "authors": [
            "W. M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-014-0424-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "F-Alloy: a relational model transformation language based on Alloy",
        "submission-date": "2016/06",
        "publication-date": "2017/11",
        "abstract": "Abstract Model transformations are one of the core arti-\nfacts of a model-driven engineering approach. The relational\nlogic language Alloy has been used in the past to verify\nproperties of model transformations. In this paper we intro-\nduce the concept of functional Alloy modules. In essence a\nfunctional Alloy module can be viewed as an Alloy module\nrepresenting a model transformation. We describe a sub-\nlanguage of Alloy called F-Alloy speciﬁcally designed to\nconcisely specify functional Alloy modules. The restrictions\non F-Alloy’s syntax are meant to allow efﬁcient execution\nof the speciﬁed transformation, without the use of back-\ntracking, by an adapted interpretation algorithm. F-Alloy’s\nsemantics is given in this paper as a direct translation\nto Alloy; hence, F-Alloy speciﬁcations are also analyz-\nable using the powerful automatic analysis features of\nAlloy.",
        "keywords": [
            "Model transformation",
            "F-Alloy",
            "Alloy",
            "Analysis",
            "Formal method",
            "Endogenous",
            "Exogenous"
        ],
        "authors": [
            "Loïc Gammaitoni",
            "Pierre Kelsen"
        ],
        "file_path": "data/sosym-all/s10270-017-0630-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "F-Alloy"
        }
    },
    {
        "title": "Model-based qualitative risk assessment for availability of IT infrastructures",
        "submission-date": "2009/06",
        "publication-date": "2010/06",
        "abstract": "For today’s organisations, having a reliable information system is crucial to safeguard enterprise revenues(thinkofon-linebanking,reservationsfore-ticketsetc.). Such a system must often offer high guarantees in terms of its availability; in other words, to guarantee business continuity, IT systems can afford very little downtime. Unfortunately, making an assessment of IT availability risks is difﬁcult: incidents affecting the availability of a marginal component of the system may propagate in unexpected ways to other more essential components that functionally depend on them. General-purpose risk assessment (RA) methods do not provide technical solutions to deal with this problem. In this paper we present the qualitative time dependency (QualTD) model and technique, which is meant to be employed together with standard RA methods for the qualitative assessment of availability risks based on the propagation of availability incidents in an IT architecture. The QualTD model is based on our previous quantitative time dependency (TD) model (Zam-bon et al. in BDIM ’07: Second IEEE/IFIP international Communicated by Prof. Ketil Stølen. workshop on business-driven IT management. IEEE Computer Society Press, pp 75–83, 2007), but provides more ﬂexible modelling capabilities for the target of assessment. Furthermore, the previous model required quantitative data which is often too costly to acquire, whereas QualTD applies only qualitative scales, making it more applicable to indus-trial practice. We validate our model and technique in a real-world case by performing a risk assessment on the authen-tication and authorisation system of a large multinational company and by evaluating the results with respect to the goals of the stakeholders of the system. We also perform a review of the most popular standard RA methods and discuss which type of method can be combined with our technique.",
        "keywords": [
            "Information risk management",
            "Risk assessment",
            "Availability",
            "Information security",
            "System modelling"
        ],
        "authors": [
            "Emmanuele Zambon",
            "Sandro Etalle",
            "Roel J. Wieringa",
            "Pieter Hartel"
        ],
        "file_path": "data/sosym-all/s10270-010-0166-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest Editorial to the Special Issue on Language Engineering for Model-Driven Software Development",
        "submission-date": "2006/08",
        "publication-date": "2006/08",
        "abstract": "Model-driven approaches to software development require precise definitions and tool support for modelling languages, their syntax and semantics, consistency, refinement and transformation. In order to support model-driven development for a variety of application contexts and platforms, efficient ways of designing languages are needed, accepting that languages are evolving and that tools need to be delivered in a timely fashion. In this respect, languages are not unlike software: A discipline of language engineering is required to support their design, implementation, verification and validation, delivering languages of high quality at low cost. An important contribution of any engineering approach, besides the actual technology provided, is the meta knowledge about how different technologies are related and for which classes of problems they provide solutions. Well-known examples of such technologies, used by contributions in this special issue, include UML/MOF-based metamodelling, graph transformation, algebra and logic, data base technology, etc. This special issue, published in two sections in this and the next volume of this journal, presents six contributions from two scientific events.",
        "keywords": [],
        "authors": [
            "Jean Bézivin",
            "Reiko Heckel"
        ],
        "file_path": "data/sosym-all/s10270-006-0028-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based a-posteriori integration of engineering tools for incremental development processes",
        "submission-date": "2003/12",
        "publication-date": "2004/11",
        "abstract": "A-posteriori integration of heterogeneous engin-eering tools supplied by different vendors constitutes a chal-lenging task. In particular, this statement applies to incremental development processes where small changes have to be propagated – potentially bidirectionally – through a set of inter-dependent design documents which have to be kept consistent with each other. Responding to these challenges, we have developed an approach to tool integration which puts strong emphasis on software architecture and model-driven development. Starting from an abstract description of a software architecture, the architecture is gradually reﬁned down to an implementation level. To integrate heterogeneous en-gineering tools, wrappers are constructed for abstracting from technical details and for providing homogenized data access. On top of these wrappers, incremental integration tools provide for inter-document consistency. These tools are based on graph models of the respective document classes and graph transformation rules for maintaining inter-document consis-tency. Altogether, the collection of support tools and the respective infrastructure considerably leverage the problem of composing a tightly integrated development environment from a set of heterogeneous engineering tools.",
        "keywords": [
            "A-posteriori integration",
            "Incremental consistency management",
            "Graph transformation",
            "UML",
            "Wrapping",
            "Software architecture"
        ],
        "authors": [
            "Simon M. Becker",
            "Thomas Haase",
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-004-0071-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based testing of software for automation systems using heuristics and coverage criterion",
        "submission-date": "2016/10",
        "publication-date": "2018/08",
        "abstract": "The aim of this work is to increase the conﬁdence on software for automation systems deﬁning a coverage criterion to measure the quality level of generated tests and the time interval needed to execute them. This coverage criterion called At Least N (ALN) is based on the Effect Predicate Heuristic (EPH) that provides all effect predicate for ISA 5.2 diagrams. The ALN and EPH have been incorporated into the Gungnir tool that was built using model-based testing concepts. The Gungnir uses timed automata to model the speciﬁcation, in the ISA 5.2 diagrams, and the implementation, in the Ladder language. The timed automata models are automatically extracted, data tests are generated and the tool automatically veriﬁes if the implementation is in conformance with the speciﬁcation, given a quality level deﬁned by the user.",
        "keywords": [
            "Model-based testing",
            "Conformance tests",
            "Automation systems",
            "Timed automata",
            "Programmable logic controllers",
            "Ladder",
            "ISA 5.2"
        ],
        "authors": [
            "Rodrigo José Sarmento Peixoto",
            "Leandro Dias da Silva",
            "Angelo Perkusich"
        ],
        "file_path": "data/sosym-all/s10270-018-0690-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An approach for modeling and detecting software performance antipatterns based on ﬁrst-order logics",
        "submission-date": "2011/04",
        "publication-date": "2012/04",
        "abstract": "The problem of interpreting the results of performance analysis is quite critical in the software performance domain. Mean values, variances and probability distributions are hard to interpret for providing feedback to software architects. Instead, what architects expect are solutions to performance problems, possibly in the form of architectural alternatives (e.g. split a software component in two components and re-deploy one of them). In a soft-ware performance engineering process, the path from analysis results to software design or implementation alternatives is still based on the skills and experience of analysts. In this paper, we propose an approach for the generation of feedback based on performance antipatterns. In particular, we focus on the representation and detection of antipatterns. To this goal, we model performance antipatterns as logical predicates and we build an engine, based on such predicates, aimed at detect-ing performance antipatterns in an XML representation of the software system. Finally, we show the approach at work on a case study.",
        "keywords": [
            "Software performance antipatterns",
            "Performance analysis",
            "Software architectures"
        ],
        "authors": [
            "Vittorio Cortellessa",
            "Antinisca Di Marco",
            "Catia Trubiani"
        ],
        "file_path": "data/sosym-all/s10270-012-0246-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Transitive-closure-based model checking (TCMC) in Alloy",
        "submission-date": "2017/10",
        "publication-date": "2020/01",
        "abstract": "We present transitive-closure-based model checking (TCMC): a symbolic representation of the semantics of computational tree logic with fairness constraints (CTLFC) for ﬁnite models in ﬁrst-order logic with transitive closure (FOLTC). TCMC is an expression of the complete model checking problem for CTLFC as a set of constraints in FOLTC without induction, iteration, or invariants. We implement TCMC in the Alloy Analyzer, showing how a transition system can be expressed declaratively and concisely in the Alloy language. Since the total state space is rarely representable due to the state-space explosion problem, we present scoped TCMC where the property is checked for state spaces of a size smaller than the total state space. We address the problem of spurious instances and carefully describe the meaning of results from scoped TCMC with respect to the complete model checking problem. Using case studies, we demonstrate scoped TCMC and compare it with bounded model checking, highlighting how TCMC can check inﬁnite paths.",
        "keywords": [
            "Symbolic model checking",
            "Alloy",
            "Declarative models"
        ],
        "authors": [
            "Sabria Farheen\nNancy A. Day\nAmirhossein Vakili\nAli Abbassi"
        ],
        "file_path": "data/sosym-all/s10270-019-00763-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introductory paper",
        "submission-date": "2004/04",
        "publication-date": "2004/04",
        "abstract": "The evolution of diagrammatic notations usually follows a pattern that, from their usage as illustrations of sentences written in some formal or natural language, leads to the definition of “modeling languages”. To achieve this status, the definition of a diagrammatic language requires formal methods to precisely specify the syntax and semantics of such diagrams. In particular, when visual models of systems or processes constitute executable specification of systems, not only is a non-ambiguous specifications of their static syntax and semantics needed, but also an adequate notion of diagram dynamics. Such a notion must establish links (e.g., morphisms) which relate diagram transformations and transformations of the objects of the underlying domain. The field of Graph Grammars and Graph Transformation Systems has contributed much insight into the solution of these problems, but also other approaches (e.g., meta modeling, constraint-based and other rule-based systems) have been developed to tackle specific issues. This special section on graph transformations and visual modeling techniques gives a vivid account of the diversity of problems and approaches that the community of researchers on visual modeling techniques are pursuing. The contributions here range from theoretical issues to the generation of language specific tools, as well as general techniques for language specification. Much attention is payed to dynamic aspects of diagrammatic languages, either proposing formal semantics for languages specifying system behaviors, or directly expressing behaviors through language transformations.",
        "keywords": [],
        "authors": [
            "Paolo Bottoni",
            "Mark Minas"
        ],
        "file_path": "data/sosym-all/s10270-003-0049-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "GRAPHxx: DSML engineering for knowledge graph building and streamlining with GraphRAG",
        "submission-date": "2024/03",
        "publication-date": "2025/05",
        "abstract": "Symbolic AI has been facing long-term adoption obstacles and a slow uptake caused by the limited availability of frictionless tooling—that can support not only knowledge engineers, but also novice users and educators, with designing and presenting graph exemplars that can be visually communicated, edited and ad hoc processed. There’s still a shortage of tools that can democratize knowledge graph (KG) creation, something that is increasingly needed—ﬁrstly by educators trying to discuss examples with novices without stumbling on OWL jargon at the earliest step, but also for more recent integration cases such as (i) GraphRAG, where private KGs are called to augment or ensure factuality of large language model services; or (ii) in search engine optimization (SEO), where SEO practitioners lacking knowledge engineering background must embed Schema.org graph data into their Web content. Most visual KG tools are visualizers of KGs created by other means—either in OWL-centric ontology editors posing high expertise barriers, or converted from available serializations, or lifted from legacy data sources. The few actual KG editors are mostly OWL editors neglecting to support the creation of schemaless graph datasets as well as of ﬂexible combinations of graph data and schema fragments. This paper reports on a Design Science effort adopting metamodeling means, traditionally employed for the engineering of domain-speciﬁc modeling languages, toward deﬁning a KG development and integration method that facilitates both the visual design of KG exemplars and their operationalization. We aim to balance a diagrammatic look and feel with machine readability of the semantic content being produced—further streamlined in an architecting proposition for integration with LLM services and for the production of Schema.org graph snippets. The DSML was deployed and evaluated as a tool implemented on the ADOxx metamodeling platform, using RDF and LangChain as mediators that streamline the content toward triplestores and LLM services.",
        "keywords": [
            "Knowledge graphs",
            "Domain-speciﬁc modeling language",
            "RDF",
            "Retrieval augmented generation",
            "LLM",
            "ADOxx"
        ],
        "authors": [
            "Andrei Chis",
            "Ana-Maria Ghiran",
            "Robert Andrei Buchmann"
        ],
        "file_path": "data/sosym-all/s10270-025-01297-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling should be an independent scientific discipline",
        "submission-date": "2022/07",
        "publication-date": "2022/08",
        "abstract": "Software modeling started as a paradigm to help developers build better software faster by enabling them to specify, reason and manipulate software systems at a higher-abstraction level while ignoring irrelevant low-level technical details. But this same principle manifests in any other domain that has to deal with complex systems, software-based or not. We argue that bringing to other engineering and scientific fields, our modeling expertise is a win–win opportunity where we can all learn from each other as we all model, but in complementary ways. Nevertheless, to fully unleash the benefits of this collaboration, we must go beyond individual efforts trying to adapt single techniques from one field to another. It requires a deeper reformulation of modeling as a whole. It is time for modeling to become an independent discipline where all fields of knowledge can contribute and benefit from.",
        "keywords": [
            "Modeling",
            "Abstraction",
            "Discipline",
            "Transdisciplinarity",
            "Science",
            "Engineering"
        ],
        "authors": [
            "Jordi Cabot",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-022-01035-8.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A4WSN: an architecture-driven modelling platform for analysing and developing WSNs",
        "submission-date": "2016/07",
        "publication-date": "2018/07",
        "abstract": "This paper proposes A4WSN, an architecture-driven modelling platform for the development and the analysis of wireless sensor networks (WSNs). A WSN consists of spatially distributed sensor nodes that cooperate in order to accomplish a speciﬁc task. Sensor nodes are cheap, small, and battery-powered devices with limited processing capabilities and memory. WSNs are mostly developed directly on the top of the operating system. They are tied to the hardware conﬁguration of the sensor nodes, and their design and implementation can require cooperation with a myriad of system stakeholders with different backgrounds. The peculiarities of WSNs and current development practices bring a number of challenges, ranging from hardware and software coupling, limited reuse, and the late assessment of WSN quality properties. As a way to overcome a number of existing limitations, this study presents a multi-view modelling approach that supports the development and analysis of WSNs. The framework uses different models to describe the software architecture, hardware conﬁguration, and physical deployment of a WSN. A4WSN allows engineers to perform analysis and code generation in earlier stages of the WSN development life cycle. The A4WSN platform can be extended with third-party plug-ins providing additional analysis or code generation engines. We provide evidence of the applicability of the proposed platform by developing PlaceLife, an A4WSN plug-in for estimating the WSN lifetime by taking various physical obstacles in the deployment environment into account. In turn, PlaceLife has been applied to a real-world case study in the health care domain as a running example.",
        "keywords": [
            "MDE",
            "Software engineering",
            "Software architecture",
            "WSNs",
            "Energy"
        ],
        "authors": [
            "Ivano Malavolta",
            "Leonardo Mostarda",
            "Henry Muccini",
            "Enver Ever",
            "Krishna Doddapaneni",
            "Orhan Gemikonakli"
        ],
        "file_path": "data/sosym-all/s10270-018-0687-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model projection relative to submetamodeling dimensions",
        "submission-date": "2022/07",
        "publication-date": "2023/07",
        "abstract": "Model-based engineering (MBE) recognizes models as central in software construction with the possibility of their management in libraries and repositories with proper structuring of their spaces and operations. Due to this success, models (and metamodels) are becoming larger and larger and technics are needed in order to comprehend and exploit them, such as circumscribing sub(meta)models of interest, which is the subject of this paper. Following MBE, there are mainly two ways for circumscribing submodels: only at the model level (by selecting model elements of interest) or through the meta level (by selecting a submetamodeling dimension of interest). In a preceding paper, we deeply studied the first way. Here we concentrate on the second way. Model projection deeply relies on the concepts of submodels and submetamodels with their inclusion qualities for model space structuring and has to be systematically examined from this point of view. It is important to point out that model treatment has to deal with full models (as offered by “off the shelf” libraries) but also with not necessarily well-formed ones, such as unspeciﬁed model chunks, due, for example, to the storage in repositories of incomplete engineering choices or of intermediate results of operations. It is a difficulty to encompass all these forms of models, being well-formed or not, in a homogeneous manner through MBE operations. The operation for “Model projection relative to submetamodeling dimensions” presented here does take this difficulty into account.",
        "keywords": [
            "Model projection",
            "Submodel extraction",
            "Submodel",
            "Submetamodel"
        ],
        "authors": [
            "Bernard Carré\nGilles Vanwormhoudt\nOlivier Caron"
        ],
        "file_path": "data/sosym-all/s10270-023-01116-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Operation-based versioning as a foundation for live executable models",
        "submission-date": "2024/03",
        "publication-date": "2024/10",
        "abstract": "Live modeling is the ability to edit an executable model at run-time, and to subsequently continue the execution instead of having to restart it. Few modeling frameworks support this feature. Much of the research concerning live modeling attempts to bring “liveness” to existing modeling languages and environments, which is a complex, and often ad hoc endeavor. We instead argue to build modeling environments on an operation-based versioning foundation, to not only record edit operations, but also execution steps on an explicit run-time model. This reduces the complexity of patching the run-time state with edit operations to a simple merge-operation, while getting powerful features such as collaborative editing and debugging “for free.”",
        "keywords": [
            "Model-driven engineering",
            "Live programming",
            "Live modeling",
            "Versioning"
        ],
        "authors": [
            "Joeri Exelmans",
            "Ciprian Teodorov",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-024-01212-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Building European software architecture community: how far have we come?",
        "submission-date": "2013/04",
        "publication-date": "2013/04",
        "abstract": "The text on this page does not contain an explicit abstract.",
        "keywords": [],
        "authors": [
            "Muhammad Ali Babar",
            "Ian Gorton",
            "Flavio Oquendo"
        ],
        "file_path": "data/sosym-all/s10270-013-0339-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A domain‑specific modeling milestone",
        "submission-date": "2021/08",
        "publication-date": "2021/08",
        "abstract": "In October 2021, the workshop on Domain-Specific Modeling (held at the SPLASH conference series) celebrates its twentieth anniversary since the first workshop in 2001 (http://​www.​dsmfo​rum.​org/​events/​DSM21/). The history of the workshop can be traced back to the 2000 OOPSLA conference (Minneapolis MN) and a Birds of a Feather (BoF) session that brought together those attendees who were interested in DSM topics. From that original BoF meeting, some of the attendees (Juha-Pekka Tolvanen, Jeff Gray, Steven Kelly, and Joern Bettin) decided that it was time for an official forum for collecting initial research efforts in the DSM area. The first workshop (actually called DSVL—Domain-Specific Visual Languages, and first called “DSM” in 2003) was held in October 2001 in Tampa, FL. That workshop, which almost did not happen due to the proximity of the 9/11 tragedy in the USA, received 20 submissions, from which 14 papers were presented. Details about that first workshop are still available at: http://​www.​dsmfo​rum.​org/​events/​DSVL01/​DSVL01.​html\nAlthough concepts such as metamodeling and other approaches for supporting customized domain-specific modeling languages are commonly discussed today, such was not the case during the formation of the DSM workshop. The early editions of the UML conference were not as receptive to DSM techniques, and the 2001 DSM workshop was the first open forum that welcomed researchers working in the specific area. In fact, it was not until the name change of the 2005 MODELS/UML conference that the modeling community began widespread acceptance of a broad range of DSM approaches over those focused just on UML extension mechanisms (coincidentally, the 2005 MODELS/UML conference had a panel that discussed these differences, called “A DSL Or UML Profile. Which Would You Use?”).\nThe peak period of the DSM workshop was from 2006 to 2008, with an average of nearly 40 participants and 20 paper presentations. As DSM topics became more accepted at other venues, the depth of participation of the DSM workshop waned as the maturity of the area was realized. The DSM workshop also produced three special journal issues, including 5 papers in the Journal of Visual Languages and Computing (June 2004), 6 papers in IEEE Software (July/August 2009) and 6 papers in a SoSyM theme issue (January 2013).\nOver the past 20 years, the DSM workshop witnessed the evolution of the area, with specific observation of the following trends:",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe",
            "Juha-Pekka Tolvanen"
        ],
        "file_path": "data/sosym-all/s10270-021-00921-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Quantitative modelling and analysis of BDI agents",
        "submission-date": "2022/06",
        "publication-date": "2023/08",
        "abstract": "Belief–desire–intention (BDI) agents are a popular agent architecture. We extend conceptual agent notation (Can)—a BDI programming language with advanced features such as failure recovery and declarative goals—to include probabilistic action outcomes, e.g. to reﬂect failed actuators, and probabilistic policies, e.g. for probabilistic plan and intention selection. The extension is encoded in Milner’s bigraphs. Through application of our BigraphER tool and the PRISM model checker, the probability of success (intention completion) under different probabilistic outcomes and plan/event/intention selection strategies can be investigated and compared. We present a smart manufacturing use case. A signiﬁcant result is that plan selection has limited effect compared with intention selection. We also see that the impact of action failures can be marginal—even when failure probabilities are large—due to the agent making smarter choices.",
        "keywords": [
            "BDI agents",
            "Quantitative analysis",
            "Bigraphs",
            "Probabilistic modelling",
            "Robotic software"
        ],
        "authors": [
            "Blair Archibald",
            "Muﬀy Calder",
            "Michele Sevegnani",
            "Mengwei Xu"
        ],
        "file_path": "data/sosym-all/s10270-023-01121-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special issue on “modeling: foundations and applications” (MODELS 2013)",
        "submission-date": "2015/07",
        "publication-date": "2015/11",
        "abstract": "Since its beginnings, the use of models has always been a core principle in Computer Science. Recently, model-based engineering is gaining rapid popularity across various engineering disciplines. The pervasive use of models as the essential artifacts of the development process, and model-driven development of complex systems, has been strengthened by a focus on executable models and automatic transformations supporting the generation of more refined models and implementations. Software models have become industrially accepted best practices in many application areas. Domains like automotive systems and avionics, interactive systems, business engineering, games, and Web-based applications commonly apply a tool-supported, model-based, or model-driven approach toward software development. The potential for early validation and verification, coupled with the generation of production code, has shown to cover a large percentage of implemented functionality with improved productivity and reliability. This increased success of using models in software and systems engineering also opens up new challenges, requiring collaborative research across multiple disciplines, ranging from offering suitable domain-specific modeling concepts to supporting legacy needs through models. The MODELS conference is devoted to model-based development for software and systems engineering, covering all types of modeling languages, methods, tools, and their applications. The 2013 edition of the MODELS conference took place in the “magic” city of Miami, USA, from September 29 to October 4, 2013. The MODELS 2013 conference offered an opportunity for researchers, practitioners, educators, and students to come together, to reflect on and discuss our progress as a community, and to identify the important challenges still to be addressed and overcome. In its sixteenth edition, the MODELS community was challenged to demonstrate the maturity and effectiveness of model-based and model-driven engineering and to explore their limits by investigating new application areas and combinations with other emerging technologies. This challenge resulted in 180 papers submitted to the MODELS 2013 Foundations (130) and Applications Tracks (50). The Foundations Track papers provide significant contributions to the core software modeling body of knowledge in the form of new ideas and results that advance the state of the art. Two categories of Foundations Track papers were invited: technical papers, describing original scientifically rigorous solutions to challenging model-driven development problems, and exploratory papers, describing new, non-conventional modeling research positions or approaches that challenge the status quo and describe solutions that are based on new ways of looking at software modeling problems.",
        "keywords": [],
        "authors": [
            "Ana Moreira",
            "Bernhard Schätz",
            "Peter Clarke",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-015-0500-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How does your model represent the system? A note on model ﬁdelity, underspeciﬁcation, and uncertainty",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Model-driven engineering (MDE) was elaborated over two decades ago based on a very limited set of seminal concepts: system, model, metamodel, and seminal relationships (i.e., representation of, and conforms to), as shown in Fig. 1. Since the early development of MDE, the modeling community has produced a large body of knowledge about conformity. This also allows metamodeling techniques to be used as deﬁnitions of the modeling languages themselves. Beyond a pure conformity relation between model and meta-model, various detailed “typing” relationships have been explored to support a better decoupling and modularity of models, and thus a certain degree of reuse. Some ﬂexibility in conformity has been explored to support different levels of formality during modeling activities, hence supporting the process from informal to formal modeling. However, very little literature can be found on the representativity of the model with regard to the system under study, raising the question: “How well does a given model actually represent the system?” Indeed, the core modeling activity has been conceptually deﬁned, e.g., by Stachowiack in terms of characteristics of a model: a model is an abstraction of an original (i.e., the “system”), and fulﬁll a speciﬁc purpose with regard to this original. Several relaxations have been identiﬁed. For example, the system might still be under construction and does not exist yet, a model may have many purposes (dependent on the different stakeholders) and a model may be composed of other models. However, all of these techniques do not provide a formal way to capture and reason about the actual representativity of the models built. In this editorial, we discuss some ways of qualifying the representativity relationship in terms of ﬁdelity and under-speciﬁcation, leading to some degrees of (possibly explicit) uncertainty.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "JeffGray",
            "Jean-Marc Jézéquel",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01210-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Metric propositional neighborhood logics on natural numbers",
        "submission-date": "2010/06",
        "publication-date": "2011/02",
        "abstract": "Interval logics formalize temporal reasoning on interval structures over linearly (or partially) ordered domains, where time intervals are the primitive ontological entities and truth of formulae is deﬁned relative to time intervals, rather than time points. In this paper, we introduce and study Metric Propositional Neighborhood Logic (MPNL) over natural numbers. MPNL features two modalities referring, respectively, to an interval that is “met by” the current one and to an interval that “meets” the current one, plus an inﬁnite set of length constraints, regarded as atomic propositions, to constrain the length of intervals. We argue that MPNL can be successfully used in different areas of computer science to combine qualitative and quantitative interval temporal reasoning, thus providing a viable alternative to well-established logical frameworks such as Duration Calculus. We show that MPNL is decidable in double exponential time and expressively complete with respect to a well-deﬁned sub-fragment of the two-variable fragment FO2[N, =, <, s] of first-order logic for linear orders with successor function, interpreted over natural numbers. Moreover, we show that MPNL can be extended in a natural way to cover full FO2[N, =, <, s], but, unexpectedly, the latter (and hence the former) turns out to be undecidable.",
        "keywords": [
            "Metric temporal logic",
            "Interval logic",
            "Decidability",
            "Complexity",
            "Expressiveness"
        ],
        "authors": [
            "Davide Bresolin",
            "Dario Della Monica",
            "Valentin Goranko",
            "Angelo Montanari",
            "Guido Sciavicco"
        ],
        "file_path": "data/sosym-all/s10270-011-0195-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Producing robust use case diagrams via reverse engineering of use case descriptions",
        "submission-date": "2006/03",
        "publication-date": "2007/01",
        "abstract": "In a use case driven development process,\na use case model is utilized by a development team to\nconstruct an object-oriented software system. The large\ndegree of informality in use case models, coupled with\nthe fact that use case models directly affect the quality of\nall aspects of the development process, is a very danger-\nous combination. Naturally, informal use case models\nare prone to contain problems, which lead to the injec-\ntion of defects at a very early stage in the development\ncycle. In this paper, we propose a structure that will aid\nthe detection and elimination of potential defects caused\nby inconsistencies present in use case models. The struc-\nture contains a small set of formal constructs that will\nallow use case models to be machine readable while\nretaining their readability by retaining a large degree\nof unstructured natural language. In this paper we also\npropose a process which utilizes the structured use cases\nto systematically generate their corresponding use case\ndiagrams and vice versa. Finally a tool provides support\nfor the new structure and the new process. To demon-\nstrate the feasibility of this approach, a simple study is\nconducted using a mock online hockey store system.",
        "keywords": [],
        "authors": [
            "Mohamed El-Attar",
            "James Miller"
        ],
        "file_path": "data/sosym-all/s10270-006-0039-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guided architecture trade space exploration: fusing model-based engineering and design by shopping",
        "submission-date": "2020/03",
        "publication-date": "2021/06",
        "abstract": "Advances in model-based system engineering have greatly increased the predictive power of models and the analyses that can be run on them. At the same time, designs have become more modular and component-based. It can be difﬁcult to manually explore all possible system designs due to the sheer number of possible architectures and conﬁgurations; trade space exploration has arisen as a solution to this challenge. In this work, we present a new software tool: the Guided Architecture Trade Space Explorer (GATSE), which connects an existing model-based engineering language (AADL) and modeling environment (OSATE) to an existing trade space exploration tool (ATSV). GATSE, AADL, and OSATE are all designed to be easily extended by users, which enables relatively straightforward domain-customizations. ATSV, combined with these customizations, lets system designers “shop” for candidate architectures and interactively explore the architectural trade space according to any quantiﬁable quality attribute or system characteristic. We evaluate GATSE according to an established framework for variable system architectures, and demonstrate its use on an avionics subsystem.",
        "keywords": [
            "Design space exploration",
            "Search-based system engineering",
            "Model-based engineering",
            "Guided optimization",
            "Architecture analysis and design language (AADL)",
            "Open source AADL tool environment (OSATE)",
            "ARL trade space visualizer (ATSV)"
        ],
        "authors": [
            "Sam Procter\nLutz Wrage"
        ],
        "file_path": "data/sosym-all/s10270-021-00889-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework for deadlock detection in core ABS",
        "submission-date": "2013/11",
        "publication-date": "2015/01",
        "abstract": "We present a framework for statically detecting deadlocks in a concurrent object-oriented language with asynchronous method calls and cooperative scheduling of method activations. Since this language features recursion and dynamic resource creation, deadlock detection is extremely complex and state-of-the-art solutions either give imprecise answers or do not scale. In order to augment precision and scalability, we propose a modular framework that allows several techniques to be combined. The basic component of the framework is a front-end inference algorithm that extracts abstract behavioral descriptions of methods, called contracts, which retain resource dependency information. This component is integrated with a number of possible different back-ends that analyze contracts and derive deadlock information. As a proof-of-concept, we discuss two such back-ends: (1) an evaluator that computes a ﬁxpoint semantics and (2) an evaluator using abstract model checking.",
        "keywords": [
            "Type inference",
            "Deadlock analysis",
            "Asynchronous method invocation",
            "Concurrent object groups"
        ],
        "authors": [
            "Elena Giachino",
            "Cosimo Laneve",
            "Michael Lienhardt"
        ],
        "file_path": "data/sosym-all/s10270-014-0444-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Removing redundant multiplicity constraints in UML class models",
        "submission-date": "2017/10",
        "publication-date": "2018/09",
        "abstract": "Models are central for the development and management of complex systems. In order to be useful along the entire life cycle of software they must provide reliable support and enable automatic management. For that purpose, they must be precise, consistent, correct, and be subject to stringent quality veriﬁcation and control criteria. Model automation calls for deep formal study of models, so that tools can provide inclusive support to users. This paper deals with optimization of multiplicity constraints in Class Models, i.e., models that provide abstraction on the static structure of software. The paper introduces an in-depth analysis of redundancy of multiplicity constraints, i.e., multiplicities that cannot be realized in any legal instance. The analysis includes: (1) a formal study of gaps of redundancies in multiplicity intervals; (2) algorithmic and rule-based methods for removing redundancies in multiplicity constraints; (3) a formal study of completeness of the algorithmic procedures, with respect to most UML class model constraints. The algorithmic procedures are implemented in our FiniteSatUSE tool. To the best of our knowledge, there is no previous study of properties of multiplicity redundancy, no completeness analysis of tightening methods, and no systematic study of these features with respect to most UML class model constraints.",
        "keywords": [
            "Class models",
            "MBSE",
            "Formal semantics",
            "Multiplicity constraints",
            "Correctness",
            "Quality",
            "Redundancy",
            "Veriﬁcation",
            "Boundary tight",
            "Tightening methods",
            "Model optimization",
            "Multiplicity tightening",
            "Tightening rules"
        ],
        "authors": [
            "Mira Balaban",
            "Azzam Maraee"
        ],
        "file_path": "data/sosym-all/s10270-018-0696-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the relationship between modeling and programming languages",
        "submission-date": "2012/01",
        "publication-date": "2012/01",
        "abstract": "At the MODELS 2011 conference in New Zealand, Colin Atkinson, held a panel on “When will Code become Irrele-vant?”. The panelists were requested to answer the following six questions:\n1. Do you agree with the implied assumption in the panel abstract that “code” is different to “models, at least in the minds of practicing software engineers?\n2. If so, do you agree with the premise that code is still the primary artifact, or at least still an important artifact, in software engineering?\n3. If so, do you think it matters?\n4. If so, when if ever, will the situation change and what will it take to make it happen?\n5. What can the research community do to help bring this about?\n6. What will the future of modeling look like? Does modeling have a future as an independent activity or will it fade away in importance?\nSoftware developers create and use models for a variety of purposes. For example, the following are three common forms of uses:\n• Models as sketches: Developers ﬁnd it useful to sketch descriptions of requirements, design or deployment concepts on whiteboards or paper when discussing their ideas with other developers or customer representatives. This use of models supports exploratory development of concepts and ideas that may or may not later ﬁnd their way into more formal models or implementations.\n• Models as analysis artifacts: Developers build analyzable models to check speciﬁed properties (e.g., consistency and satisﬁability properties), to predict implementation qualities (e.g., performance), or to simulate implemented behavior. Included in this category of models are formal, non-executable speciﬁcations that can be statically analyzed, and executable models that support more dynamic forms of analysis. These models typically contain only the information needed to analyze target properties, and thus may not include information that is needed to generate full implementations.\n• Models as the basis for code generation or synthesis of software artifacts: Models can be built for the purpose of generating implementations, test cases, deployment or software conﬁguration scripts, or other software artifacts. These models must contain all necessary information in a form that allow a generator to mechanically synthesize software artifacts.",
        "keywords": [],
        "authors": [
            "Bernhard Rumpe",
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-011-0224-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML – the Good, the Bad or the Ugly? Perspectives from a panel of experts",
        "submission-date": "2005/01",
        "publication-date": "2005/01",
        "abstract": "This paper presents a panel discussion on the health of UML (Unified Modeling Language) version 2.0, featuring perspectives from experts involved in its creation. The panelists discuss the pros and cons of UML2.0, its usability, semantics, and its potential as a foundation for Model Driven Architecture (MDA).",
        "keywords": [],
        "authors": [
            "Brian Henderson-Sellers",
            "Steve Cook",
            "Steve Mellor",
            "Joaquin Miller",
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-004-0076-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A proﬁle and tool for modelling safety information with design information in SysML",
        "submission-date": "2013/05",
        "publication-date": "2014/02",
        "abstract": "Communication both between development teams and between individual developers is a common source of safety-related faults in safety–critical system design. Communication between experts in different ﬁelds can be particularly challenging due to gaps in assumed knowledge, vocabulary and understanding. Faults caused by communication failures must be removed once found, which can be expensive if they are found late in the development process. Aiding communication earlier in development can reduce faults and costs. Modelling languages for design have been shown through practical experience to improve communication through better information presentation and increased information consistency. In this paper, we describe a SysML proﬁle designed for modelling the safety-related concerns of a system. The proﬁle models common safety concepts from safety standards and safety analysis techniques integrated withsystemdesigninformation.Wedemonstratethatthepro-ﬁle is capable of modelling the concepts through examples. Wealsoshowtheuseofsupportingtoolstoaidtheapplication of the proﬁle through analysis of the model and generation of reports presenting safety information in formats appropriate to the target reader. Through increased traceability and inte- gration, the proﬁle allows for greater consistency between safety information and system design information and can aid in communicating that information to stakeholders.",
        "keywords": [
            "SysML",
            "UML/SysML proﬁle",
            "Safety analysis",
            "System design"
        ],
        "authors": [
            "Geoffrey Biggs",
            "Takeshi Sakamoto",
            "Tetsuo Kotoku"
        ],
        "file_path": "data/sosym-all/s10270-014-0400-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to the theme section on model-driven engineering for digital twins",
        "submission-date": "2025/03",
        "publication-date": "2025/04",
        "abstract": "Digital twins are virtual, digital representations of real-world systems or objects that are kept in sync with data from the real-world system and can be used for advanced analysis, predictive exploration, control, and (semi-) automated transformation of these systems and objects [1]. Digital twins promise tremendous potential in domains such as automotive, avionics, manufacturing, and healthcare.\nDigital twins are based on models representing aspects of a real system. Whether the model is the basis of what-if simulation, or more sophisticated control and adaptation, digital twins present an interesting research challenge for the modeling community in terms of quality-based development and deployment. Digital twin engineering is currently ad hoc [2], which is a challenge for quality-controlled development, deployment, and operation. Hence, MDE is crucial to leveraging the potential of digital twins fully and systematically.\nThis theme section of SoSyM provides a platform for digital twin researchers and practitioners to report emerging results, evidence of success and good practice, and to outline roadmaps to deliver high-quality digital twins using model-driven engineering.",
        "keywords": [],
        "authors": [
            "Djamel Eddine Khelladi",
            "Tony Clark",
            "Vinay Kulkarni",
            "Steffen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-025-01288-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Meta-modelling and graph grammars for multi-paradigm modelling in AToM3",
        "submission-date": "2003/01",
        "publication-date": "2004/04",
        "abstract": "This paper presents the combined use of meta-modelling and graph grammars for the generation of visual modelling tools for simulation formalisms. In meta-modelling, formalisms are described at a meta-level. This information is used by a meta-model processor to generate modelling tools for the described formalisms. We combine meta-modelling with graph grammars to extend the model manipulation capabilities of the gen-erated modelling tools: edit, simulate, transform into another formalism, optimize and generate code. We store all (meta-)models as graphs, and thus, express model ma-nipulations as graph grammars. We present the design and implementation of these concepts in AToM3 (A Tool for Multi-formalism, Meta-Modelling). AToM3 supports modelling of complex systems using diﬀerent formalisms, all meta-modelled in their own right. Models in diﬀerent formalisms may be transformed into a single common formalism for further processing. These transformations are speciﬁed by graph grammars. Mosterman and Vangheluwe [18] in-troduced the term multi-paradigm modelling to denote the combination of multiple formalisms, multiple ab-straction levels, and meta-modelling. As an example of multi-paradigm modelling we present a meta-model for the Object-Oriented Continuous Simulation Language OOCSMP, in which we combine ideas from UML class diagrams (to express the OOCSMP model structure), Causal Block Diagrams (CBDs), and Statecharts (to specify the methods of the OOCSMP classes). A graph grammar is able to generate OOCSMP code, and then a compiler for this language (C-OOL) generates Java app-lets for the simulation execution.",
        "keywords": [
            "Meta-modelling",
            "Multi-formalism",
            "Multi-paradigm modelling",
            "Graph grammars",
            "Model transformation",
            "Code generation",
            "Causal block diagrams",
            "Statecharts",
            "AToM3",
            "OOCSMP"
        ],
        "authors": [
            "Juan de Lara",
            "Hans Vangheluwe",
            "Manuel Alfonseca"
        ],
        "file_path": "data/sosym-all/s10270-003-0047-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "graph grammars"
        }
    },
    {
        "title": "Applying design patterns in the search-based optimization of software product line architectures",
        "submission-date": "2016/08",
        "publication-date": "2017/08",
        "abstract": "The design of the product line architecture (PLA) is a difﬁcult activity that can beneﬁt from the application of design patterns and from the use of a search-based optimization approach, which is generally guided by different objectives related, for instance, to cohesion, coupling and PLA extensibility. The use of design patterns for PLAs is a recent research ﬁeld, not completely explored yet. Some works apply the patterns manually and for a speciﬁc domain. Approaches to search-based PLA design do not consider the usage of these patterns. To allow such use, this paper introduces a mutation operator named “Pattern-Driven Mutation Operator” that includes methods to automatically identify suitable scopes and apply the patterns Strategy, Bridge and Mediator with the search-based approach multi-objective optimization approach for PLA. A metamodel is proposed to represent and identify suitable scopes to receive each one of the patterns, avoiding the introduction of architectural anomalies. Empirical results are also presented, showing evidences that the use of the proposed operator produces a greater diversity of solutions and improves the quality of the PLAs obtained in the search-based optimization process, regarding the values of software metrics.",
        "keywords": [
            "Design pattern",
            "Search-based software engineering",
            "Software product line architecture"
        ],
        "authors": [
            "Giovani Guizzo",
            "Thelma Elita Colanzi",
            "Silvia Regina Vergilio"
        ],
        "file_path": "data/sosym-all/s10270-017-0614-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A security requirements modelling language for cloud computing environments",
        "submission-date": "2017/09",
        "publication-date": "2019/09",
        "abstract": "This paper presents a novel security modelling language and a set of original analysis techniques, for capturing and analysing security requirements for cloud computing environments. The novelty of the language lies in the integration of concepts from cloud computing, with concepts from security and goal-oriented requirements engineering to elicit, model and analyse security requirements for cloud infrastructures. We then propose three analysis techniques, which support an automated process where given a model of a cloud computing system, developed with the proposed language, will enhance the model with new security knowledge, for example threats and vulnerabilities, mitigation strategies and assets and actor responsibilities. This is, to the best of our knowledge, the first attempt in the literature to develop a language for cloud computing security modelling and analysis, based on such integration, and support it with a set of automated techniques that enhanced the stakeholder-created models with security knowledge. The proposed modelling language and techniques are illustrated through walking examples and a case study based on our work in the VisiOn European project.",
        "keywords": [
            "Security modelling language",
            "Secure Tropos",
            "Cloud computing security",
            "Cloud threat analysis"
        ],
        "authors": [
            "Haralambos Mouratidis",
            "Shaun Shei",
            "Aidan Delaney"
        ],
        "file_path": "data/sosym-all/s10270-019-00747-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Repeat, reorder, rephrase: data augmentation for process information extraction",
        "submission-date": "2024/11",
        "publication-date": "2025/06",
        "abstract": "Automatic retrieval of formal business process models from their natural language descriptions is a well-established way to facilitate the time- and cost-intensive modeling procedure. Yet, a lack of data usable for developing and training new retrieval methods is impeding progress in this ﬁeld of research. This issue can be overcome by either using methods less reliant on high-quality data, such as large language models, or creating bigger datasets. The latter is often preferable in the context of business process modeling, especially when internal workﬂows of organizations have to be treated conﬁdentially. It is the more data-intensive solution, though, which is costly. Data augmentation techniques aim to improve both quality and quantity of existing datasets, by deliberate perturbations resulting in new, synthetic data. In this article, we present a collection of data augmentation techniques, which are speciﬁcally selected for the task of improving data quality in the context of process information extraction. We show why data augmentation techniques from the wider ﬁeld of natural language processing are often not applicable to process information extraction, and how the resulting data differ in terms of linguistic variety, structure, and feature space coverage. In our experiments, data augmentation results in an absolute improvement in the F1 measure of 5.7% for extracting process-relevant entities from text and 4.5% for extracting relations between those entities. We make all code available at https://github.com/JulianNeuberger/pet-data-augmentation and results for our experiments at https://zenodo.org/doi/10.5281/zenodo.10941423.",
        "keywords": [
            "Business Process Extraction",
            "Data Augmentation",
            "Natural Language Processing"
        ],
        "authors": [
            "Julian Neuberger",
            "Lars Ackermann",
            "Stefan Jablonski"
        ],
        "file_path": "data/sosym-all/s10270-025-01305-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ReFlO: an interactive tool for pipe-and-ﬁlter domain speciﬁcation and program generation",
        "submission-date": "2013/03",
        "publication-date": "2014/03",
        "abstract": "ReFlO is a framework and interactive tool to record and systematize domain knowledge used by experts to derive complex pipe-and-ﬁlter (PnF) applications. Domain knowledge is encoded as transformations that alter PnF graphs by reﬁnement (adding more details), ﬂattening (removing modular boundaries), and optimization (substitut-ing inefﬁcient PnF graphs with more efﬁcient ones). All three kinds of transformations arise in reverse-engineering legacy PnF applications. We present the conceptual foundation and tool capabilities of ReFlO, illustrate how parallel PnF appli-cations are designed and generated, and how domain-speciﬁc libraries of transformations are developed.",
        "keywords": [
            "MDE",
            "Tools",
            "Software architectures",
            "Design by transformation",
            "Reﬁnement",
            "Optimization",
            "Graph transformations"
        ],
        "authors": [
            "Rui C. Gonçalves",
            "Don Batory",
            "João L. Sobral"
        ],
        "file_path": "data/sosym-all/s10270-014-0403-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A pattern-based approach for improving model quality",
        "submission-date": "2013/04",
        "publication-date": "2014/01",
        "abstract": "UML class diagrams play a central role in modeling activities, and it is essential that class diagrams keep their high quality all along a product life cycle. Correctness problems in class diagrams are mainly caused by complex interactions among class-diagram constraints. Detection, identification, and repair of such problems require background training. In order to improve modelers’ capabilities in these directions, we have constructed a catalog of anti-patterns of correctness and quality problems in class diagrams, where an anti-pattern analyzes a typical constraint interaction that causes a correctness or a quality problem and suggests possible repairs. This paper argues that exposure to correctness anti-patterns improves modeling capabilities. The paper introduces the catalog and its pattern language, and describes experiments that test the impact of awareness to modeling problems in class diagrams (via concrete examples and anti-patterns) on the analysis capabilities of modelers. The experiments show that increased awareness implies increased identification. The improvement is remarkably noticed when the awareness is stimulated by anti-patterns, rather than by concrete examples.",
        "keywords": [
            "Anti-patterns",
            "Pattern languages",
            "Pattern awareness",
            "Experiments",
            "Modeling problems",
            "Analysis capabilities",
            "Software engineering education",
            "Correctness",
            "Quality"
        ],
        "authors": [
            "Mira Balaban",
            "Azzam Maraee",
            "Arnon Sturm",
            "Pavel Jelnov"
        ],
        "file_path": "data/sosym-all/s10270-013-0390-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Graphic modeling in Distributed Autonomous and Asynchronous Automata ­(DA3)",
        "submission-date": "2020/04",
        "publication-date": "2021/11",
        "abstract": "Automated verification of distributed systems becomes very important in distributed computing. The graphical insight into the system in the early and late stages of the project is essential. In the design phase, the visual input helps to articulate the collaborative distributed components clearly. The formal verification gives evidence of correctness or malfunction, but in the latter case, graphical simulation of counterexample helps for better understanding design errors. For these purposes, we invented Distributed Autonomous and Asynchronous Automata ­(DA3), which have the same semantics as the formal verification base—Integrated Model of Distributed Systems (IMDS). The IMDS model reflects the natural characteristics of distributed systems: unicasting, locality, autonomy, and asynchrony. Distributed automata have all of these features because they share the same semantics as IMDS. In formalism, the unified system definition has two views: the server view of the cooperating distributed nodes and the agent view of the migrating agents performing distributed computations. The automata have two formally equivalent forms that reflect two views: Server ­DA3 for observing servers exchanging messages, and Agent ­DA3 for tracking agents, which visit individual servers in their progress of distributed calculations. We present the ­DA3 formulation based on the IMDS formalism and their application to design and verify distributed systems in the Dedan environment. ­DA3 formalism is compared with other concepts of distributed automata known from the literature.",
        "keywords": [
            "Distributed systems",
            "Distributed system modeling",
            "Distributed automata",
            "Graphic modeling",
            "Formal methods"
        ],
        "authors": [
            "Wiktor B. Daszczuk"
        ],
        "file_path": "data/sosym-all/s10270-021-00917-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A graph-based algorithm for consistency maintenance in incremental and interactive integration tools",
        "submission-date": "2005/02",
        "publication-date": "2007/01",
        "abstract": "Development processes in engineering disciplines are inherently complex. Throughout the development process, the system to be built is mod-eled from different perspectives, on different levels of abstraction, and with different intents. Since state-of-the-art development processes are highly incremental and iterative, models of the system are not constructed in one shot; rather, they are extended and improved repeatedly. Furthermore, models are related by man-ifold dependencies and need to be maintained mutu-ally consistent with respect to these dependencies. Thus, tools are urgently needed which assist developers in maintaining consistency between inter-dependent and evolving models. These tools have to operate incremen-tally, i.e., they have to propagate changes performed on one model into related models which are affected by these changes. In addition, they need to support user interactions in settings where the effects of changes can-not be determined automatically and deterministically. We present an algorithm for incremental and interac-tive consistency maintenance which meets these require-ments. The algorithm is based on graphs, which are used as the data model for representing the models to be inte-grated, and graph transformation rules, which describe the modiﬁcations of the graphs to be performed on a high level of abstraction.",
        "keywords": [
            "Incremental consistency maintenance",
            "Graph transformation",
            "Triple graph grammars"
        ],
        "authors": [
            "Simon M. Becker",
            "Sebastian Herold",
            "Sebastian Lohmann",
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-006-0045-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Managing the evolution of data-intensive Web applications by model-driven techniques",
        "submission-date": "2009/11",
        "publication-date": "2011/02",
        "abstract": "The adoption of Model-Driven Engineering (MDE) in the development of Web Applications permitted to decouple the functional description of applications from the underlying implementation platform. This is of paramount relevance for preserving the intellectual property encoded in models and making applications, languages and processes resilient to technological changes. This paper proposes a model-driven approach for supporting the migration and evolution of data-intensive Web applications. In particular, model differencing techniques are considered to realize a migration facility capable of detecting the modiﬁcations a model underwent during its lifecycle and to automatically derive from them the programs that are capable of migrating/adapting also those aspects which are not directly derivable from the source models, as for instance the data persistently stored in a database and the page layout usually written usinggraphictemplates.Theapproachisvalidatedbyconsidering applications described with the beContent and WebML modeling languages.",
        "keywords": [
            "Migration",
            "Data-intensive Web applications",
            "Model differencing",
            "Coupled evolution",
            "Ecore"
        ],
        "authors": [
            "Antonio Cicchetti",
            "Davide Di Ruscio",
            "Ludovico Iovino",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-011-0193-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling the Complex Living World",
        "submission-date": "2006/03",
        "publication-date": "2006/03",
        "abstract": "In the software development domain, models of software systems are typically used to understand and predict properties of the system or to produce implementations. A model is normally created before the software system is implemented and, in many cases, acts as a specification of behavioral and structural properties that must be present in implemented systems. In theory, implementations can be generated from formal models in a manner that ensures their correctness with respect to the models. In other non-engineering disciplines, models are used differently. For example, physicists use models primarily to understand and explain phenomena that occur in the world around them. They build models that are consistent with their observations of the phenomena, and they test the models to determine their fidelity and the circumstances under which the models make accurate predictions. Unlike software models, formal models of physical systems typically describe continuous behavior (with very few non-continuous disruptions) and therefore use concepts from continuous mathematics (e.g., differential equations). It may seem that scientists in non-engineering disciplines have very little use for software modeling techniques, but the complex problems that are currently tackled in the Life Sciences area indicate otherwise. Scientists in the Life Sciences tackle highly-complex problems that involve study of organs, cells, proteins and organic molecules that exhibit continuous as well as discrete, non-deterministic behavior that can be described in terms of state transitions. Bio-technological models describe state-based phenomena and are primarily used to understand the phenomena and to predict behavior in a variety of situations. Accurate models pave the way for the engineering of medicines and for the development of sophisticated “biological tools” to further improve our lives.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-006-0008-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling and veriﬁcation of a telecommunication application using live sequence charts and the Play-Engine tool",
        "submission-date": "2005/12",
        "publication-date": "2008/01",
        "abstract": "We apply the scenario-based approach to modeling, via the language of live sequence charts (LSCs) and the Play-Engine tool to a real-world complex telecommunication service, Depannage. It allows a user to call for help from a doctor, the ﬁre brigade, a car maintenance service, etc. These kinds of services are built on top of an embedded platform, using both new and existing service components, and their complexity stems from their distributed architecture, the various time constraints they entail, and their rapidly evolving underlying systems. A well known problem in this class of telecommunication applications is that of feature interaction, whereby a new feature might cause problems in the execution of existing features. Our approach provides a methodology for high-level modeling of telecommunication applications that can help in detecting feature interaction at early development stages. We exhibit the results of applying the methodology to the speciﬁcation, animation and formal veriﬁcation of the Depannage service.",
        "keywords": [
            "Live sequence charts (LSCs)",
            "Requirements engineering",
            "Veriﬁcation",
            "Telecommunication"
        ],
        "authors": [
            "Pierre Combes",
            "David Harel",
            "Hillel Kugler"
        ],
        "file_path": "data/sosym-all/s10270-007-0069-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven runtime environment for Web applications",
        "submission-date": "2004/05",
        "publication-date": "2005/06",
        "abstract": "Abstract A large part of software development these days\ndeals with building so-called Web applications. Many of\nthese applications are database-powered and exhibit a page\nlayout and navigational structure that is close to the class\nstructure of the entities being managed by the system.\nAlso, there is often only limited application-speciﬁc busi-\nness logic. This makes the usual three-tier architectural ap-\nproach unappealing, because it results in a lot of unneces-\nsary overhead. One possible solution to this problem is the\nuse of model-driven architecture (MDA). A simple platform-\nindependent domain model describing only the entity struc-\nture of interest could be transformed into a platform-speciﬁc\nmodel that incorporates a persistence mechanism and a user\ninterface. Yet, this raises a number of additional problems\ncaused by the one-way, multi-transformational nature of the\nMDA process. To cope with these problems, the authors pro-\npose the notion of a model-driven runtime (MDR) environ-\nment that is able to execute a platform-independent model\nfor a speciﬁc purpose instead of transforming it. The pa-\nper explains the concepts of an MDR that interprets OCL-\nannotated class diagrams and state machines to realize Web\napplications. It shows the authors’ implementation of the ap-\nproach, the Infolayer system, which is already used by a\nnumber of applications. Experiences from these applications\nare described, and the approach is compared to others.",
        "keywords": [
            "UML",
            "OCL",
            "Action semantics",
            "MDA",
            "Web applications",
            "UML virtual machnies"
        ],
        "authors": [
            "Stefan Haustein",
            "Joerg Pleumann"
        ],
        "file_path": "data/sosym-all/s10270-005-0093-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dealing with forward and backward jumps in workﬂow management systems",
        "submission-date": "2002/10",
        "publication-date": "2003/02",
        "abstract": "Workﬂow management systems (WfMS) offer a promising technology for the realization of process-centered application systems. A deﬁciency of existing WfMS is their inadequate support for dealing with exceptional deviations from the standard procedure. In the ADEPT project, therefore, we have developed advanced concepts for workﬂow modeling and execution, which aim at the increase of ﬂexibility in WfMS. On the one hand we allow workﬂow designers to model exceptional execution paths already at buildtime provided that these deviations are known in advance. On the other hand authorized users may dynamically deviate from the pre-modeled workﬂow at runtime as well in order to deal with unforeseen events. In this paper, we focus on forward and backward jumps needed in this context. We describe sophisticated modeling concepts for capturing deviations in workﬂow models already at buildtime, and we show how forward and backward jumps (of diﬀerent semantics) can be correctly applied in an ad-hoc manner during runtime as well. We work out basic requirements, facilities, and limitations arising in this context. Our experiences with applications from diﬀerent domains have shown that the developed concepts will form a key part of process ﬂexibility in process-centered information systems.",
        "keywords": [
            "Workﬂow management",
            "Adaptive workﬂow",
            "Exception handling",
            "Forward/backward jump"
        ],
        "authors": [
            "Manfred Reichert",
            "Peter Dadam",
            "Thomas Bauer"
        ],
        "file_path": "data/sosym-all/s10270-003-0018-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "VPML: an approach to detect design patterns of MOF-based modeling languages",
        "submission-date": "2012/05",
        "publication-date": "2013/03",
        "abstract": "A design pattern is a recurring and well-understooddesignfragment.Inamodel-drivenengineeringmethodology, detecting occurrences of design patterns supports the activities of model comprehension and maintenance. With the recent explosion of domain-speciﬁc modeling languages, each with its own syntax and semantics, there has been a corresponding explosion in approaches to detecting design patterns that are so much tailored to those many languages that they are difﬁcult to reuse. This makes developing generic analysis tools extremely hard. Such a generic tool is however desirable to reduce the learning curve for pattern design-ers as they specify patterns for different languages used to model different aspects of a system. In this paper, we propose a uniﬁed approach to detecting design patterns of MOF-based modeling languages. MOF is increasingly used to deﬁne modeling languages, including UML and BPMN. In our approach, a pattern is modeled with a Visual Pattern Modeling Language and mapped to a corresponding QVT-Relations transformation. Such a transformation runs over an input model where pattern occurrences are to be detected and reports those occurrences in a result model. The approach is prototyped on Eclipse and validated in two large case studies that involve detecting design patterns—speciﬁcally a subset of GoF patterns in a UML model and a subset of Control Flow patterns in a BPMN model. Results show that the approach is adequate for modeling complex design patterns for MOF-based modeling languages and detecting their occurrences with high accuracy and performance.",
        "keywords": [
            "Design pattern",
            "Modeling",
            "MOF",
            "UML",
            "BPMN",
            "QVT",
            "GoF",
            "VPML"
        ],
        "authors": [
            "Maged Elaasar",
            "Lionel C. Briand",
            "Yvan Labiche"
        ],
        "file_path": "data/sosym-all/s10270-013-0325-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-Relations, VPML"
        }
    },
    {
        "title": "Integrated model-driven development of self-adaptive user interfaces",
        "submission-date": "2019/02",
        "publication-date": "2020/01",
        "abstract": "Modern user interfaces (UIs) are increasingly expected to be plastic, in the sense that they retain a constant level of usability, even when subjected to context changes at runtime. Self-adaptive user interfaces (SAUIs) have been promoted as a solution for context variability due to their ability to automatically adapt to the context-of-use at runtime. The development of SAUIs is a challenging and complex task as additional aspects like context management and UI adaptation have to be covered. In classical model-driven UI development approaches, these aspects are not fully integrated and hence introduce additional complexity as they represent crosscutting concerns. In this paper, we present an integrated model-driven development approach where a classical model-driven development of UIs is coupled with a model-driven development of context-of-use and UI adaptation rules. We base our approach on the core UI modeling language IFML and introduce new modeling languages for context-of-use (ContextML) and UI adaptation rules (AdaptML). The generated UI code, based on the IFML model, is coupled with the context and adaptation services, generated from the ContextML and AdaptML model, respectively. The integration of the generated artifacts, namely UI code, context, and adaptation services in an overall rule-based execution environment, enables runtime UI adaptation. The beneﬁt of our approach is demonstrated by two case studies, showing the development of SAUIs for different application scenarios and a usability study which has been conducted to analyze end-user satisfaction of SAUIs.",
        "keywords": [
            "Model-driven UI development",
            "UI adaptation",
            "Self-adaptive UIs",
            "Context-aware applications"
        ],
        "authors": [
            "Enes Yigitbas",
            "Ivan Jovanovikj",
            "Kai Biermeier",
            "Stefan Sauer",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-020-00777-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction: MBFair: a model-based veriﬁcation methodology for detecting violations of individual fairness",
        "submission-date": "2024/06",
        "publication-date": "2025/01",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Qusai Ramadan",
            "Marco Konersmann",
            "Amir Shayan Ahmadian",
            "Jan Jürjens",
            "Steffen Staab"
        ],
        "file_path": "data/sosym-all/s10270-024-01262-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Graph and model transformation tools for model migration\nEmpirical results from the transformation tool contest",
        "submission-date": "2011/04",
        "publication-date": "2012/04",
        "abstract": "We describe the results of the Transformation Tool Contest 2010 workshop, in which nine graph and model transformation tools were compared for specifying model migration. The model migration problem—migration of UML activity diagrams from version 1.4 to version 2.2—is non-trivial and practically relevant. The solutions have been compared with respect to several criteria: correctness, conciseness, understandability, appropriateness, maturity and support for extensions to the core migration task. We describe in detail the comparison method, and discuss the strengths andweaknesses of thesolutions withaspecial focus on the differences between graph and model transformation for model migration. The comparison results demonstrate tool and language features that strongly impact the efﬁcacy of solutions, such as support for retyping of model elements. The results are used to motivate an agenda for future model migration research (including suggestions for areas in which the tools need to be further improved).",
        "keywords": [
            "Model transformation",
            "Graph transformation",
            "Co-evolution"
        ],
        "authors": [
            "Louis M. Rose",
            "Markus Herrmannsdoerfer",
            "Steffen Mazanek",
            "Pieter Van Gorp",
            "Sebastian Buchwald",
            "Tassilo Horn",
            "Elina Kalnina",
            "Andreas Koch",
            "Kevin Lano",
            "Bernhard Schätz",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-012-0245-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Facilitating the migration to the microservice architecture via model-driven reverse engineering and reinforcement learning",
        "submission-date": "2021/03",
        "publication-date": "2022/02",
        "abstract": "The microservice architecture has gained remarkable attention in recent years. Microservices allow developers to implement and deploy independent services, so they are a naturally effective architecture for continuously deployed systems. Because of this, several organizations are undertaking the costly process of manually migrating their traditional software architectures to microservices. The research in this paper aims at facilitating the migration from monolithic software architectures to microservices. We propose a framework which enables software developers/architects to migrate their software systems more efficiently by helping them remodularize the source code of their systems. The framework leverages model-driven reverse engineering to obtain a model of the legacy system and reinforcement learning to propose a mapping of this model toward a set of microservices.",
        "keywords": [
            "Microservice architecture",
            "Reinforcement learning",
            "Model-driven reverse engineering",
            "Migration"
        ],
        "authors": [
            "MohammadHadi Dehghani",
            "Shekoufeh Kolahdouz-Rahimi",
            "Massimo Tisi",
            "Dalila Tamzalit"
        ],
        "file_path": "data/sosym-all/s10270-022-00977-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Distributed implementation of message sequence charts",
        "submission-date": "2011/11",
        "publication-date": "2013/06",
        "abstract": "This work revisits the problem of program synthesis from speciﬁcations described by high-level message sequence charts. We first show that in the general case, synthesis by a simple projection on each component of the system allows more behaviors in the implementation than in the specification. We then show that differences arise from loss of ordering among messages and show that behaviors can be preserved by addition of communication controllers that intercept messages to add stamping information before resending them and deliver messages to processes in the order described by the specification.",
        "keywords": [
            "Scenarios",
            "Implementation",
            "Distributed system synthesis"
        ],
        "authors": [
            "Rouwaida Abdallah",
            "Loïc Hélouët",
            "Claude Jard"
        ],
        "file_path": "data/sosym-all/s10270-013-0357-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Performance analysis of aspect-oriented UML models",
        "submission-date": "2006/02",
        "publication-date": "2007/04",
        "abstract": "Aspect-Oriented Modeling (AOM) techniques allow software designers to isolate and address separately solutions for crosscutting concerns (such as security, reliability, new functional features, etc.). Current AOM research is concerned not only with the separate expression of concerns and their composition into a complete system model, but also with the analysis of different properties of such models. This paper proposes an approach for analyzing the performance effects of a given aspect on the overall system performance, after the composition of the aspect model with the system’s primary model. Performance analysis of UML models is enabled by the “UML Performance Proﬁle for Schedulability, Performance and Time” (SPT) standardized by OMG, which deﬁnes a set of quantitative performance annotations to be added to a UML model. The ﬁrst step of the proposed approach is to add performance annotations to both the primary and the aspect models. An aspect model is generic at ﬁrst, and therefore its performance annotations must be parameterized. A generic model is converted into a context-speciﬁc aspect model with concrete values assigned to its performance annotations. The latter is composed with the primary model, generating a complete annotated UML model. The composition is performed in both structural and behavioural views. A novel approach for composing activity diagrams based on graph-rewriting concepts is proposed in the paper. The next step is to transform automatically the composed model into a Layered Queueing Network (LQN) performance model, by using techniques developed in pre-vious work. The proposed approach is illustrated with a case study system, whose primary model is enhanced with some security features by using AOM. The performance effects of the security aspect under consideration are analyzed in two design alternatives, by solving and analyzing the LQN model of the composed system.",
        "keywords": [],
        "authors": [
            "Dorina C. Petriu",
            "Hui Shen",
            "Antonino Sabetta"
        ],
        "file_path": "data/sosym-all/s10270-007-0053-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generation of hazard relation diagrams: formalization and tool support",
        "submission-date": "2018/11",
        "publication-date": "2020/06",
        "abstract": "Developing safety-critical, software-intensive embedded systems are characterized by the need to identify hazards and to define hazard-mitigating requirements at the earliest possible stage of development, i.e., during requirements engineering. These hazard-mitigating requirements must be adequate in the sense that they must specify the functionality required by the stakeholders in addition to rendering the system sufficiently safe during operation. The adequacy of hazard-mitigating requirements is determined during requirements validation. Yet, the validation of the adequacy of hazard-mitigating requirements is burdened by the fact that hazards and contextual information about hazards are a work product of safety assessment, and hazard-mitigating requirements are a work product of requirements engineering. These work products are poorly integrated such that during validation, the information needed to determine the adequacy of hazard-mitigating requirements is not avail-able to stakeholders. In consequence, there is the risk that inadequate hazard-mitigating requirements remain covert and the system is falsely considered safe. To alleviate this issue, we have previously proposed (Tenbergen et al., in: Proceedings of the 21st international working conference on requirements engineering: foundation for software quality, pp 17–32, 2015), improved, and evaluated (Tenbergen et al. in Requir Eng J 23(2):291–329, 2018. https​://doi.org/10.1007/s0076​6-017-0267-9) a novel diagram type called “Hazard Relation Diagrams.” In this paper, we present a semiautomated formal approach and tool support for their generation. We make use of a running example to illustrate the concepts.",
        "keywords": [
            "Safety requirements",
            "Hazards",
            "Hazard-mitigating requirements",
            "Safety assessment",
            "Validation",
            "Reviews",
            "Inspections",
            "Mitigation",
            "Adequacy",
            "Modeling",
            "Safety-critical embedded systems",
            "Model-based engineering",
            "Hazard relation diagrams"
        ],
        "authors": [
            "Bastian Tenbergen",
            "Thorsten Weyer"
        ],
        "file_path": "data/sosym-all/s10270-020-00799-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the application of process management and process mining to Industry 4.0",
        "submission-date": "2023/01",
        "publication-date": "2024/04",
        "abstract": "The continuous evolution of digital technologies applied to the more traditional world of industrial automation led to Industry 4.0, which envisions production processes subject to continuous monitoring and able to dynamically respond to changes that can affect the production at any stage (resilient factory). The concept of agility, which is a core element of Industry 4.0, is defined as the ability to quickly react to breaks and quickly adapt to changes. Accurate approaches should be implemented aiming at managing, optimizing and improving production processes. In this vision paper, we show how process management (BPM) can benefit from the availability of raw data from the industrial internet of things to obtain agile processes by using a top-down approach based on automated synthesis and a bottom-up approach based on mining.",
        "keywords": [
            "Industry 4.0",
            "Process management",
            "BPM",
            "Process mining",
            "Process adaptation"
        ],
        "authors": [
            "Flavia Monti",
            "Jerin George Mathew",
            "Francesco Leotta",
            "Agnes Koschmider",
            "Massimo Mecella"
        ],
        "file_path": "data/sosym-all/s10270-024-01175-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mixed-semantics composition of statecharts for the component-based design of reactive systems",
        "submission-date": "2019/06",
        "publication-date": "2020/07",
        "abstract": "The increasing complexity of reactive systems can be mitigated with the use of components and composition languages in model-driven engineering. Designing composition languages is a challenge itself as both practical applicability (support for different composition approaches in various application domains), and precise formal semantics (support for veriﬁcation and code generation) have to be taken into account. In our Gamma Statechart Composition Framework, we designed and implemented a composition language for the synchronous, cascade synchronous and asynchronous composition of statechart-based reactive components. We formalized the semantics of this composition language that provides the basis for generating composition-related Java source code as well as mapping the composite system to a back-end model checker for formal veriﬁcation and model-based test case generation. In this paper, we present the composition language with its formal semantics, putting special emphasis on design decisions related to the language and their effects on veriﬁability and applicability. Furthermore, we demonstrate the design and veriﬁcation functionality of the composition framework by presenting case studies from the cyber-physical system domain.",
        "keywords": [
            "Component-based design",
            "Statecharts",
            "Composition language",
            "Formal semantics",
            "Formal veriﬁcation"
        ],
        "authors": [
            "Bence Graics",
            "Vince Molnár",
            "András Vörös",
            "István Majzik",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-020-00806-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying static code analysis for domain-speciﬁc languages",
        "submission-date": "2018/09",
        "publication-date": "2019/04",
        "abstract": "The use of code quality control platforms for analysing source code is increasingly gaining attention in the developer com-\nmunity. These platforms are prepared to parse and check source code written in a variety of general-purpose programming\nlanguages. The emergence of domain-speciﬁc languages enables professionals from different areas to develop and describe\nproblem solutions in their disciplines. Thus, source code quality analysis methods and tools can also be applied to software\nartefacts developed with a domain-speciﬁc language. To evaluate the quality of domain-speciﬁc language code, every soft-\nware component required by the quality platform to parse and query the source code must be developed. This becomes a\ntime-consuming and error-prone task, for which this paper describes a model-driven interoperability strategy that bridges the\ngap between the grammar formats of source code quality parsers and domain-speciﬁc text languages. This approach has been\ntested on the most widespread platforms for designing text-based languages and source code analysis. This interoperability\napproach has been evaluated on a number of speciﬁc contexts in different domain areas.",
        "keywords": [
            "Text-based languages",
            "Static analysis",
            "Model-driven interoperability",
            "Xtext",
            "SonarQube"
        ],
        "authors": [
            "Iván Ruiz-Rube",
            "Tatiana Person",
            "Juan Manuel Dodero",
            "José Miguel Mota",
            "Javier Merchán Sánchez-Jara"
        ],
        "file_path": "data/sosym-all/s10270-019-00729-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Investigating a ﬁle transfer protocol using CSP and B",
        "submission-date": "2005/05",
        "publication-date": "2005/05",
        "abstract": "In this paper a ﬁle transmission protocol spe-ciﬁcation is developed using the combination of two for-mal methods: CSP and B. The aim is to demonstrate that it is possible to integrate two well established formal methods whilst maintaining their individual advantages. We discuss how to compositionally verify the speciﬁca-tion and ensure that it preserves some abstract prop-erties. We also discuss how the structure of the speci-ﬁcation follows a particular style which may be gener-ally applicable when modelling other protocols using this combination.",
        "keywords": [
            "CSP",
            "B",
            "Combining formalisms",
            "Compo-sitional veriﬁcation"
        ],
        "authors": [
            "Neil Evans",
            "Helen Treharne"
        ],
        "file_path": "data/sosym-all/s10270-005-0084-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An algebra of product families",
        "submission-date": "2009/03",
        "publication-date": "2009/08",
        "abstract": "Experience from recent years has shown that it is often advantageous not to build a single product but rather a family of similar products that share at least one common functionality while having well-identiﬁed variabilities. Such product families are built from elementary features that reach from hardware parts to software artefacts such as requirements, architectural elements or patterns, components, middleware, or code. We use the well established mathematical structure of idempotent semirings as the basis for a product family algebra that allows a formal treatment of the above notions. A particular application of the algebra concerns the multi-view reconciliation problem that arises when complex systems are modelled. We use algebraic integration constraints linking features in one view to features in the same or a different view and show in several examples the suitability of this approach for a wide class of integration constraint formulations. Our approach is illustrated with a Haskell prototype implementation of one particular model of product family algebra.",
        "keywords": [
            "Product family",
            "Product line",
            "Idempotent semiring",
            "Multi-view reconciliation",
            "Formal family speciﬁcation",
            "Feature modelling"
        ],
        "authors": [
            "Peter Höfner",
            "Ridha Khedri",
            "Bernhard Möller"
        ],
        "file_path": "data/sosym-all/s10270-009-0127-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models as the subject of education",
        "submission-date": "2020/07",
        "publication-date": "2020/07",
        "abstract": "The title of the editorial for SoSyM vol. 16 (9) was “Models as the Subject of Research” [1], which focused on modeling as a pure topic for research. In a similar theme to that past editorial, we also believe that the modeling community has much to contribute on the topic of education, as related specifically to modeling, and also how modeling is used within the general context of computer science (CS) and software engineering (SE) education. However, the global interest and influence of modeling as a topic of education seems to be silent outside of the modeling community. We ask, “Why?”",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-020-00818-1.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "SEFM: software engineering and formal methods",
        "submission-date": "2014/02",
        "publication-date": "2014/02",
        "abstract": "This paper discusses the importance of formal methods in software engineering, highlighting the need for rigorous techniques in software development and validation. It introduces a special issue collecting papers from the 9th International Conference on Software Engineering and Formal Methods (SEFM) held in Montevideo, Uruguay, aiming to bridge the gap between formal methods research and practical software engineering.",
        "keywords": [],
        "authors": [
            "Gilles Barthe",
            "Alberto Pardo",
            "Gerardo Schneider"
        ],
        "file_path": "data/sosym-all/s10270-014-0404-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for EMMSAD’2021 special section",
        "submission-date": "2022/09",
        "publication-date": "2022/10",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) conference series organized 26 events from 1996 to 2021, associated with CAISE (Conference on Advanced Information Systems Engineering). From 2009, EMMSAD has become a two-day working conference. From 2017, EMMSAD best papers are invited to submit extended versions for considering their publication in the Journal of Software and Systems Modeling (SoSyM). The main topics of the EMMSAD series have the focus on models and modeling methods for the analysis and development of software information systems of any kind.",
        "keywords": [],
        "authors": [
            "Iris Reinhartz-Berger",
            "Jelena Zdravkovic",
            "Asif Gill"
        ],
        "file_path": "data/sosym-all/s10270-022-01058-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven process enactment for NFV systems with MAPLE",
        "submission-date": "2019/02",
        "publication-date": "2020/02",
        "abstract": "The network functions virtualization (NFV) advent is making way for the rapid deployment of network services (NS) for telecoms. Automation of network service management is one of the main challenges currently faced by the NFV community. Explicitly deﬁning a process for the design, deployment, and management of network services and automating it is therefore highly desirable and beneﬁcial for NFV systems. The use of model-driven orchestration means has been advocated in this context. As part of this effort to support automated process execution, we propose a process enactment approach with NFV systems as the target application domain. Our process enactment approach is megamodel-based. An integrated process modelling and enactment environment, MAPLE, has been built into Papyrus for this purpose. Process modelling is carried out with UML activity diagrams. The enactment environment transforms the process model to a model transformation chain, and then orchestrates it with the use of megamodels. In this paper, we present our approach and environment MAPLE, its recent extension with new features as well as application to an enriched case study consisting of NS design and onboarding process.",
        "keywords": [
            "Process enactment",
            "Megamodelling",
            "Papyrus",
            "Network functions virtualization (NFV)"
        ],
        "authors": [
            "Sadaf Mustaﬁz",
            "Omar Hassane",
            "Guillaume Dupont",
            "Ferhat Khendek",
            "Maria Toeroe"
        ],
        "file_path": "data/sosym-all/s10270-020-00783-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reengineering component-based software systems with Archimetrix",
        "submission-date": "2012/01",
        "publication-date": "2013/04",
        "abstract": "Many software development, planning, or analysis tasks require an up-to-date software architecture documentation. However, this documentation is often outdated, unavailable, or at least not available as a formal model which analysis tools could use. Reverse engineering methods try to fill this gap. However, as they process the system’s source code, they are easily misled by design deficiencies (e.g., violations of component encapsulation) which leaked into the code during the system’s evolution. Despite the high impact of design deficiencies on the quality of the resulting software architecture models, none of the surveyed related works is able to cope with them during the reverse engineering process. Therefore, we have developed the Archimetrix approach which semiautomatically recovers the system’s concrete architecture in a formal model while simultaneously detecting and removing design deficiencies. We have validated Archimetrix on a case study system and two implementation variants of the CoCoME benchmark system. Results show that the removal of relevant design deficiencies leads to an architecture model which more closely matches the system’s conceptual architecture.",
        "keywords": [
            "Reengineering",
            "Reverse engineering",
            "Software architecture",
            "Component-based software systems",
            "Architecture reconstruction",
            "Design deficiencies",
            "Deficiency detection",
            "Code metrics",
            "CoCoME"
        ],
        "authors": [
            "Markus von Detten",
            "Marie Christin Platenius",
            "Steffen Becker"
        ],
        "file_path": "data/sosym-all/s10270-013-0341-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-checking software library API usage rules",
        "submission-date": "2013/11",
        "publication-date": "2015/05",
        "abstract": "Modern software increasingly relies on using\nthird-party libraries which are accessed via application\nprogramming interfaces (APIs). Libraries usually impose\nconstraints on how API functions can be used (API usage\nrules) and programmers have to obey these API usage rules.\nHowever, API usage rules often are not well documented\nor documented informally. In this work, we show how to\nuse the SCTPL and SLTPL logics to precisely and formally\nspecify API usage rules in libraries, where SCTPL/SLTPL\ncan be seen as an extension of the branching/linear tempo-\nral logic CTL/LTL with variables, quantiﬁers and predicates\nover the stack. This allows library providers to formally\ndescribe API usage rules without knowing how their libraries\nwill be used by programmers. We propose an automated\napproach to check whether programs using libraries violate\nAPI usage rules or not. Our approach consists in modeling\nprograms as pushdown systems (PDSs) and checking API\nusage rules by SCTPL/SLTPL model-checking for PDSs. To\nmake the model-checking procedure more efﬁcient and pre-\ncise, we propose an abstraction that reduces drastically the\nsize of the program model and integrate may-alias analysis\ninto our approach to reduce false alarms. Moreover, we char-\nacterize two sublogics rSCTPL and rSLTPL of SCTPL and\nSLTPL that are preserved by the abstraction. We implement\nour techniques in a tool and apply the tool to check sev-\neral open-source programs. Our tool ﬁnds several previously\nunknown bugs in several programs. The may-alias analysis\navoids most of the false alarms that occur using SCTPL or\nSLTPL model-checking techniques without may-alias analy-\nsis.",
        "keywords": [
            "Pushdown systems",
            "Model-checking",
            "Software API usage rules"
        ],
        "authors": [
            "Fu Song\nTayssir Touili"
        ],
        "file_path": "data/sosym-all/s10270-015-0473-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modular language product lines: concept, tool and analysis",
        "submission-date": "2023/05",
        "publication-date": "2024/05",
        "abstract": "Modelling languages are intensively used in paradigms like model-driven engineering to automate all tasks of the development\nprocess. These languages may have variants, in which case the need arises to deal with language families rather than with\nindividual languages. However, specifying the syntax and semantics of each language variant separately in an enumerative\nway is costly, hinders reuse across variants, and may yield inconsistent semantics between variants. Hence, we propose a novel, \nmodular and compositional approach to describing product lines of modelling languages. It enables the incremental deﬁnition\nof language families by means of modules comprising meta-model fragments, graph transformation rules, and rule extensions.\nLanguage variants are conﬁgured by selecting the desired modules, which entails the composition of a language meta-model\nand a set of rules deﬁning its semantics. This paper describes: a theory for checking well-formedness, instantiability, and\nconsistent semantics of all languages within the family; an implementation as an Eclipse plugin; and an evaluation reporting\ndrastic speciﬁcation size and analysis time reduction in comparison to an enumerative approach.",
        "keywords": [
            "Model-driven engineering",
            "Graph transformation",
            "Product lines",
            "Meta-modelling",
            "Software language\nengineering"
        ],
        "authors": [
            "Juan de Lara\nEsther Guerra\nPaolo Bottoni"
        ],
        "file_path": "data/sosym-all/s10270-024-01179-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A lightweight approach to nontermination inference using Constrained Horn Clauses",
        "submission-date": "2022/06",
        "publication-date": "2024/03",
        "abstract": "Nontermination is an unwanted program property for some software systems, and a safety property for other systems. In either case, automated discovery of preconditions for nontermination is of interest. We introduce NtHorn, a fast lightweight nontermination analyser, which is able to deduce non-trivial sufﬁcient conditions for nontermination. Using Constrained Horn Clauses (CHCs) as a vehicle, we show how established techniques for CHC program transformation and abstract interpretation can be exploited for the purpose of nontermination analysis. NtHorn is comparable in effectiveness to the state-of-the-art nontermination analysis tools, as measured on standard competition benchmark suites (consisting of integer manipulating programs), while typically solving problems faster by one order of magnitude.",
        "keywords": [
            "Nontermination",
            "Precondition inference",
            "Constrained Horn clauses",
            "Program transformation",
            "Abstract interpretation"
        ],
        "authors": [
            "Bishoksan Kaﬂe",
            "Graeme Gange",
            "Peter Schachte",
            "Harald Søndergaard",
            "Peter J. Stuckey"
        ],
        "file_path": "data/sosym-all/s10270-024-01161-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A survey of approaches for verifying model transformations",
        "submission-date": "2012/01",
        "publication-date": "2013/06",
        "abstract": "As with other software development artifacts, model transformations are not bug-free and so must be systematically veriﬁed. Their nature, however, means that transformations require specialist veriﬁcation techniques. This paper brings together current research on model transformation veriﬁcation by classifying existing approaches along two dimensions. Firstly, we present a coarse-grained classiﬁcation based on the technical details of the approach (e.g., testing, theorem proving, model checking). Secondly, we present a ﬁner-grained classiﬁcation which categorizes approaches according to criteria such as level of formality, transformation language, properties veriﬁed. The purpose of the survey is to bring together research in model transformation veriﬁcation to act as a resource for the community. Furthermore, based on the survey, we identify a number of trends in current and past research on model transformation veriﬁcation.",
        "keywords": [
            "Model transformations",
            "Veriﬁcation",
            "Survey"
        ],
        "authors": [
            "Lukman Ab. Rahim",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-013-0358-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Softw Syst Model (2011) 10:143–145",
        "submission-date": "2010/07",
        "publication-date": "2010/07",
        "abstract": "Originally motivated by the need to increase the reliability and robustness of a software and hardware design a few decades ago, formal methods have now established themselves as a distinct important area of computer science. However, the original software engineering motivation of formal methods has been largely weaken throughout the years by the very high cost of their application to system development. As a result, formal methods are now moving towards two different directions, involving two fundamentally distinct research communities. On one side are those who are interested in theoretical aspects of formal methods and focus on the definition of complex mathematical frameworks, which are only loosely inspired by practical problems. On the other side are those who are interested in looking for practical problems, not just in software and hardware development but also in the application areas such as system biology and chemistry, where using formal methods is expected to be worthy in terms of time, effort and financial cost.",
        "keywords": [],
        "authors": [
            "Antonio Cerone",
            "Stefan Gruner"
        ],
        "file_path": "data/sosym-all/s10270-010-0169-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Knowledge-based construction of distributed constrained systems",
        "submission-date": "2013/11",
        "publication-date": "2015/02",
        "abstract": "The problem of deriving distributed implementations from global specifications has been extensively studied for a number of application domains. We explore it here from the knowledge perspective: A process may decide to take a local action when it has enough knowledge to do so. Such knowledge may be acquired by communication through primitives available on the platform or by static analysis. In this paper, we want to combine control and distribution, that is, we need to impose some global control constraint on a system executed in a distributed fashion. To reach that goal, we compare two approaches: either build a centralized controlled system, distribute its controller and then implement this controlled system on a distributed platform; or alternatively, directly enforce the control constraint while implementing the distributed system on the platform. We show how to achieve a solution following the second approach and explain why this is a pragmatic and more efficient strategy than the other, previously proposed one.",
        "keywords": [
            "Distributed implementations",
            "Knowledge",
            "Controlled system",
            "Correct-by-construction",
            "Implementation relation",
            "Knowledge preservation"
        ],
        "authors": [
            "Susanne Graf",
            "Sophie Quinton"
        ],
        "file_path": "data/sosym-all/s10270-014-0451-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Styles in business process modeling: an exploration and a model",
        "submission-date": "2012/09",
        "publication-date": "2013/05",
        "abstract": "Business process models are an important means to design, analyze, implement, and control business processes. As with every type of conceptual model, a business process model has to meet certain syntactic, semantic, and pragmatic quality requirements to be of value. For many years, such quality aspects were investigated by centering on the properties of the model artifact itself. Only recently, the process of model creation is considered as a factor that inﬂuences the resulting model’s quality. Our work contributes to this stream of research and presents an explorative analysis of the process of process modeling (PPM). We report on two large-scale modeling sessions involving 115 students. In these sessions, the act of model creation, i.e., the PPM, was automatically recorded. We conducted a cluster analysis on this data and identiﬁed three distinct styles of modeling. Further, we investigated how both task- and modeler-speciﬁc factors inﬂuence particular aspects of those modeling styles. Based thereupon, we propose a model that captures our insights. It lays the foundations for future research that may unveil how high-quality process models can be established through better modeling support and modeling instruction.",
        "keywords": [
            "Business process modeling",
            "Process of process modeling",
            "Modeling styles",
            "Cluster analysis"
        ],
        "authors": [
            "Jakob Pinggera",
            "Pnina Soffer",
            "Dirk Fahland",
            "Matthias Weidlich",
            "Stefan Zugal",
            "Barbara Weber",
            "Hajo A. Reijers",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-013-0349-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An integrated semantics for reasoning about SysML design models using refinement",
        "submission-date": "2014/11",
        "publication-date": "2015/09",
        "abstract": "SysML is a variant of UML for systems design. Several formalisations of SysML (and UML) are available. Our work is distinctive in two ways: a semantics for refinement and for a representative collection of elements from the UML4SysML profile (blocks, state machines, activities, and interactions) used in combination. We provide a means to analyse and refine design models specified using SysML. Thisfacilitatesthediscoveryofproblemsearlierinthesystem development lifecycle, reducing time, and costs of production. Here, we describe our semantics, which is defined using a state-rich process algebra and implemented in a tool for automatic generation of formal models. We also show how the semantics can be used for refinement-based analysis and development. Our case study is a leadership-election protocol, a critical component of an industrial application. Our major contribution is a framework for reasoning using refinement about systems specified by collections of SysML diagrams.",
        "keywords": [
            "Process algebra",
            "CML",
            "Refinement",
            "Automation",
            "SysML",
            "Semantics"
        ],
        "authors": [
            "Lucas Lima",
            "Alvaro Miyazawa",
            "Ana Cavalcanti",
            "Márcio Cornélio",
            "Juliano Iyoda",
            "Augusto Sampaio",
            "Ralph Hains",
            "Adrian Larkham",
            "Vaughan Lewis"
        ],
        "file_path": "data/sosym-all/s10270-015-0492-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the Software Engineering and Formal Methods 2013 special issue",
        "submission-date": "2015/03",
        "publication-date": "2015/05",
        "abstract": "We are in the world in which society is increasingly dependent on software, and so, the quality of this software is more important than ever. Unfortunately, the development of high-quality software is becoming increasingly challenging as complexity grows and systems are often concurrent and distributed. The Software Engineering and Formal Methods communities have developed a range of approaches that help address this problem, but initially there was relatively little interaction between these areas and some saw them as rivals. Thankfully, these attitudes have gradually changed, with the communities accepting that each makes a useful contribution in tackling an important problem.",
        "keywords": [],
        "authors": [
            "Mario Bravetti",
            "Robert M. Hierons",
            "Mercedes G. Merayo"
        ],
        "file_path": "data/sosym-all/s10270-015-0467-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving user productivity in modeling tools by explicitly modeling workﬂows",
        "submission-date": "2017/04",
        "publication-date": "2018/05",
        "abstract": "Software engineering aims to create software tools that allow people to solve particular problems in an easy and efﬁcient way. In this regard, model-driven engineering (MDE) enables to generate software tools, by systematically modeling and transforming models. To do so, MDE relies on language workbenches: Integrated Development Environment for engineering modeling languages, designing models, executing them, and verifying them. However, the usability of these tools is far from efﬁcient. Common MDE activities, such as creating a domain-speciﬁc language or developing a model transformation, are non-trivial and often require repetitive tasks. This results in unnecessary risings of development time. The goal of this paper is to increase the productivity of modelers in their daily activities by automating the tasks performed in current MDE tools. We propose an MDE-based solution where the user deﬁnes a reusable workﬂow that can be parameterized at run-time and executed. We have implemented workﬂows in the graphical modeling tool AToMPM. An empirical evaluation shows that the users’ productivity is signiﬁcantly improved.",
        "keywords": [
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Enactment",
            "Model transformation",
            "User study"
        ],
        "authors": [
            "Miguel Gamboa",
            "Eugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-018-0678-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Verifying object-based graph grammars\nAn assume-guarantee approach",
        "submission-date": "2004/11",
        "publication-date": "2006/06",
        "abstract": "The development of concurrent and reactive\nsystems is gaining importance since they are well-suited\nto modern computing platforms, such as the Internet.\nHowever, the development of correct concurrent and\nreactive systems is a non-trivial task. Object-based graph\ngrammar (OBGG) is a visual formal language suitable\nfor the speciﬁcation of this class of systems. In previous\nwork, a translation from OBGG to PROMELA (the in-\nput language of the SPIN model checker) was deﬁned,\nenabling the veriﬁcation of OBGG models using SPIN.\nIn this paper we extend this approach in two different\nways: (1) the approach for property speciﬁcation is\nimproved, enabling to prove properties not only about\npossible OBGG derivations, but also about the internal\nstate of involved objects; (2) an approach is deﬁned to\ninterpret PROMELA traces as OBGG derivations, gen-\nerating graphical counter-examples for properties that\nare not true for a given OBGG model. Another con-\ntribution of this paper is (3) the deﬁnition of a method\nfor model checking partial systems (isolated objects or\na set of objects) using an assume-guarantee approach.\nA gas station system modeled with OBGGs is used to\nillustrate the contributions.",
        "keywords": [
            "Model checking",
            "Partial systems",
            "Graph\ngrammars",
            "Object-based systems",
            "Reactive systems"
        ],
        "authors": [
            "Fernando Luís Dotti",
            "Leila Ribeiro",
            "Osmar Marchi dos Santos",
            "Fábio Pasini"
        ],
        "file_path": "data/sosym-all/s10270-006-0014-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating deductive veriﬁcation and symbolic execution for abstract object creation in dynamic logic",
        "submission-date": "2013/11",
        "publication-date": "2014/12",
        "abstract": "We present a fully abstract weakest precondi-\ntion calculus and its integration with symbolic execution.\nOur assertion language allows both specifying and verifying\nproperties of objects at the abstraction level of the program-\nming language, abstracting from a speciﬁc implementation\nof object creation. Objects which are not (yet) created never\nplay any role. The corresponding proof theory is discussed\nand justiﬁed formally by soundness theorems. The usage of\nthe assertion language and proof rules is illustrated with an\nexample of a linked list reachability property. All proof rules\npresented are fully implemented in a version of the KeY ver-\niﬁcation system for Java programs.",
        "keywords": [
            "Speciﬁcation",
            "Veriﬁcation",
            "Program logic",
            "Dynamic logic",
            "Object creation"
        ],
        "authors": [
            "Stijn de Gouw",
            "Frank de Boer",
            "Wolfgang Ahrendt",
            "Richard Bubel"
        ],
        "file_path": "data/sosym-all/s10270-014-0446-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting data-aware processes with MERODE",
        "submission-date": "2022/02",
        "publication-date": "2023/03",
        "abstract": "Most data-aware process modelling approaches have been developed from a process perspective and lack a full-ﬂedged data modelling approach. In addition, the evaluation of data-centric process approaches reveals that, even though their value is acknowledged, their usability is a point of concern. This paper presents a data-aware process modelling approach combining full-ﬂedged domain modelling based on UML class diagrams and state charts with BPMN and DMN. The approach is illustrated by means of an elaborated example with multiple business processes on top of a joint domain model. A proof-of-concept has been implemented using the MERODE code generator, linking the resulting prototype application to a Camunda BPM engine, making use of RESTful web services. The proposed approach is evaluated against 20 requirements for data-aware processes and demonstrates that the majority of these are already satisﬁed by the out-of-the-box combination of the Camunda BPM engine with the prototype generated from a MERODE domain model.",
        "keywords": [
            "Conceptual modelling",
            "Process modelling",
            "Data-aware processes",
            "Model-driven engineering"
        ],
        "authors": [
            "Monique Snoeck",
            "Charlotte Verbruggen",
            "Johannes De Smedt",
            "Jochen De Weerdt"
        ],
        "file_path": "data/sosym-all/s10270-023-01095-4.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Supporting UML-based development of embedded systems by formal techniques",
        "submission-date": "2005/12",
        "publication-date": "2007/02",
        "abstract": "We describe an approach to support UML-based development of embedded systems by formal techniques. A subset of UML is extended with timing annotations and given a formal semantics. UML models are translated, via XMI, to the input format of formal tools, to allow timed and non-timed model checking and interactive theorem proving. Moreover, the Play-Engine tool is used to execute and analyze requirements by means of live sequence charts. We apply the approach to a part of an industrial case study, the MARS system, and report about the experiences, results and conclusions.",
        "keywords": [
            "Formal methods",
            "UML",
            "Embedded systems",
            "Real-time"
        ],
        "authors": [
            "Jozef Hooman",
            "Hillel Kugler",
            "Iulian Ober",
            "Anjelika Votintseva",
            "Yuri Yushtein"
        ],
        "file_path": "data/sosym-all/s10270-006-0043-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Experimenting with modeling-speciﬁc word embeddings",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "The application of machine learning techniques to address MDE problems often requires transforming raw information (e.g., software models) to a numerical representation which can be used by machine learning algorithms. To this end, pretrained embeddings are a key technology to facilitate the construction of such applications. However, previous works have demonstrated that these embeddings struggle to generalize effectively in the MDE domain due to their training on general-purpose corpora. To tackle this issue, we developed WordE4MDE, which are specialized word embeddings trained specifically on modeling documents. In this study, we aim to overcome several limitations of WordE4MDE and conduct additional experiments to assess its efﬁcacy. Key limitations we address include: (1) mitigating the out-of-vocabulary issue through the utilization of sub-word embeddings, (2) adding contextualization to the embeddings by training a BERT model on our specific modeling corpus and (3) addressing the constraint of limited training data by investigating the augmentation of our modeling corpus with StackOverﬂow and StackExchange data.",
        "keywords": [
            "Embeddings",
            "Classiﬁcation",
            "Clustering",
            "Recommendation",
            "Machine Learning",
            "Model-Driven Engineering"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Carlos Durá",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-024-01250-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enabling automated requirements reuse and configuration",
        "submission-date": "2017/02",
        "publication-date": "2017/11",
        "abstract": "A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.",
        "keywords": [
            "Requirements",
            "Product line",
            "Configuration",
            "Reuse",
            "Feature model"
        ],
        "authors": [
            "Yan Li",
            "Tao Yue",
            "Shaukat Ali",
            "Li Zhang"
        ],
        "file_path": "data/sosym-all/s10270-017-0641-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the STAF 2015 special section",
        "submission-date": "2018/06",
        "publication-date": "2018/07",
        "abstract": "Software Technologies: Applications and Foundations (STAF) is a federation of a number of leading conferences on software technologies. It provides a loose umbrella organization for practical software technologies conferences. In 2015 the STAF federated event has been held in L’Aquila (Italy) with a special focus on practical and foundational aspects of software technology, from object-oriented design, testing, mathematical approaches to modeling and veriﬁcation, model transformation, graph transformation, model-driven engineering, aspect-oriented development, and tools. Besides the main and satellite events (14 overall), three conferences: The International Conference on Model Transformation (ICMT 2015), The European Conference on Modelling Foundations and Applications (ECMFA 2015), The International Conference on Tests & Proofs (TAP 2015) collected papers on relevant topics of software and system modelingandveriﬁcationtechniques;amongthem,10papers have been selected and extended to be part of this special section.",
        "keywords": [],
        "authors": [
            "Jasmin Blanchette",
            "Francis Bordeleau",
            "Alfonso Pierantonio",
            "Nikolai Kosmatov",
            "Gabriele Taentzer",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-018-0686-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting inheritance hierarchy changes in model-based regression test selection",
        "submission-date": "2016/09",
        "publication-date": "2017/12",
        "abstract": "Models can be used to ease and manage the development, evolution, and runtime adaptation of a software system. When models are adapted, the resulting models must be rigorously tested. Apart from adding new test cases, it is also important to perform regression testing to ensure that the evolution or adaptation did not break existing functionality. Since regression testing is performed with limited resources and under time constraints, regression test selection (RTS) techniques are needed to reduce the cost of regression testing. Applying model-level RTS for model-based evolution and adaptation is more convenient than using code-level RTS because the test selection process happens at the same level of abstraction as that of evolution and adaptation. In earlier work, we proposed a model-based RTS approach called MaRTS to be used with a ﬁne-grained model-based adaptation framework that targets applications implemented in Java. MaRTS uses UML models consisting of class and activity diagrams. It classiﬁes test cases as obsolete, reusable, or retestable based on changes made to UML class and activity diagrams of the system being adapted. However, MaRTS did not take into account the changes made to the inheritance hierarchy in the class diagram and the impact of these changes on the selection of test cases. This paper extends MaRTS to support such changes and demonstrates that the extended approach performs as well as or better than code-based RTS approaches in safely selecting regression test cases. While MaRTS can generally be used during any model-driven development or model-based evolution activity, we have developed it in the context of runtime adaptation. We evaluated the extended MaRTS on a set of applications and compared the results with code-based RTS approaches that also support changes to the inheritance hierarchy. The results showed that the extended MaRTS selected all the test cases relevant to the inheritance hierarchy changes and that the fault detection ability of the selected test cases was never lower than that of the baseline test cases. The extended MaRTS achieved comparable results to a graph-walk code-based RTS approach (DejaVu) and showed a higher reduction in the number of selected test cases when compared with a static analysis code-based RTS approach (ChEOPSJ).",
        "keywords": [
            "Executable UML models",
            "Inheritance hierarchy",
            "Model-based adaptation",
            "Model-based regression test selection",
            "Model validation",
            "Runtime adaptation",
            "UML activity diagram",
            "UML class diagram"
        ],
        "authors": [
            "Mohammed Al-Refai",
            "Sudipto Ghosh",
            "Walter Cazzola"
        ],
        "file_path": "data/sosym-all/s10270-017-0636-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mutation testing for temporal alloy models (extended version)",
        "submission-date": "2024/04",
        "publication-date": "Not found",
        "abstract": "Writing declarative models has numerous beneﬁts, ranging from automated reasoning and correction of design-level properties before systems are built, to automated testing and debugging of their implementations after they are built. Alloy is a declarative modeling language that is well-suited for verifying system designs. A key strength of Alloy is its scenario-ﬁnding toolset, the Analyzer, which allows users to explore all valid scenarios that adhere to the model’s constraints up to a user-provided scope. Despite the Analyzer, writing correct Alloy models remains a difﬁcult task, partly due to Alloy’s expressive operators, which allow for succinct formulations of complex properties but can be difﬁcult to reason over manually. To further add to the complexity, Alloy’s grammar was recently expanded to support linear temporal logic, increasing both the expressibility of Alloy and the burden for accurately expressing properties. To address this, this paper presents μAlloyτ, an extension to Alloy’s mutation testing framework that accounts for the newly introduced temporal logic, including updating μAlloyτ’s test generation capability to produce temporal test cases. Experimental results reveal μAlloyτ is efﬁcient at generating and checking mutations and μAlloyτ’s automatically generated tests are effective at detecting faulty temporal models.",
        "keywords": [
            "Alloy",
            "Mutation testing",
            "Test generation"
        ],
        "authors": [
            "Ana Jovanovic",
            "Allison Sullivan"
        ],
        "file_path": "data/sosym-all/s10270-024-01220-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Literature review of reuse in business process modeling",
        "submission-date": "2011/11",
        "publication-date": "2012/09",
        "abstract": "Business process models play an important role\nin the analysis and improvement of the performance of an\nenterprise. Evidently, the quality of a business process model\nhas a direct effect on the business performance. This evidence\nhas motivated both the academic and industrial communi-\nties to look for suitable methods for creating good quality\nbusiness process models. In particular, there is a wide agree-\nment that reuse can accelerate the design process and pro-\nduce high quality solutions by adopting best practices and\nagreed-up-on solutions. However, faced with various types\nof reusable artifacts, business process designers need a set of\ncriteria to determine which type would suit best their needs\nand design context. To assist designers in their choice, we\nfirst present a set of criteria inﬂuencing the design phase in\nterms of effort required and the quality of the resulting model.\nSecondly, we use this set of criteria to present a state of the\nart on the most signiﬁcant reusable design artifacts.",
        "keywords": [
            "Workﬂow pattern",
            "Activity pattern",
            "Action pattern",
            "Semantic business process pattern",
            "Reference model",
            "Business process modeling"
        ],
        "authors": [
            "Nahla Zaaboub Haddar",
            "Lobna Makni",
            "Hanene Ben Abdallah"
        ],
        "file_path": "data/sosym-all/s10270-012-0286-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Expert’s voice: The BabyUML discipline of programming (where a Program = data + Communication + Algorithms)",
        "submission-date": "2006/03",
        "publication-date": "2006/03",
        "abstract": "I want increased conﬁdence in my programs. I want my own and other people’s programs to be more readable. I want a new discipline of programming that augments my thought processes. Therefore, I create and explore a new discipline of programming in my BabyUML laboratory. I select, simplify and twist UML and other languages to demonstrate how they help bridge the gap between me as a programmer and the objects running in my computer The focus is on the run time objects; their structure, their interaction, and their individual behaviors.",
        "keywords": [
            "Object-oriented programming",
            "Object oriented methods",
            "Data structures",
            "Object communication",
            "Object algorithms",
            "Latently-typed languages",
            "Stored program object computers"
        ],
        "authors": [
            "Trygve Reenskaug"
        ],
        "file_path": "data/sosym-all/s10270-006-0005-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guidelines for representing complex cardinality constraints in binary and ternary relationships",
        "submission-date": "2010/11",
        "publication-date": "2012/02",
        "abstract": "Ternary relationships represent the association among three entities whose constraints database designers do not always know how to manage. In other words, it is very difﬁcult for the designer to detect, represent and add constraints in a ternary relationship according to the domain requirements. To remedy the shortcomings in capturing the semantics required for the representation of this kind of relationship, the present paper discusses a practical method to motivate the designer’s use of ternary relationships in a methodological framework. The method shows how to calculate cardinality constraints in binary and ternary relationships and to preserve the associated semantics until the implementation phase of the database development method.",
        "keywords": [
            "Ternary associations",
            "Conceptual models",
            "Logical models",
            "Model transformations",
            "Database methodology"
        ],
        "authors": [
            "Dolores Cuadra",
            "Paloma Martínez",
            "Elena Castro",
            "Harith Al-Jumaily"
        ],
        "file_path": "data/sosym-all/s10270-012-0234-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic Model Transformation and Formal Veriﬁcation for Function Block of IEC 61499",
        "submission-date": "2024/05",
        "publication-date": "2025/06",
        "abstract": "The IEC 61499 standard describes the structure and behavior of distributed control systems, providing a design language\nat the system level and a speciﬁcation for distributed systems. In the design of the control ﬂow, the function block of the\nIEC 61499 standard will use an execution control chart to describe its behavior. An operation state machine is designed to\ndescribe the operation of the function block. It is necessary to consider the interaction between the execution control chart\nand the operation state machine for control behavior descriptions. The function block model cannot be directly used as the\ninput of model checkers, so it needs to be transformed formally and semantically equivalently to the input models of formal\nveriﬁcation tools. This paper proposes a method to automatically transform the function block model into a ﬁnite automata\nmodel. We use bisimulation to prove that the behavior of the transformed ﬁnite automata model is consistent with that of the\nfunction block. We further demonstrate that the transformed ﬁnite automata model is used as the input to the model checker\nfor formal veriﬁcation.",
        "keywords": [
            "IEC61499 standard",
            "Automatic model transformation",
            "Formal veriﬁcation",
            "Model checking"
        ],
        "authors": [
            "Yean-Ru Chen",
            "Chia-Hao Hsu",
            "Tien-Fu Li",
            "Cheng-Yuan Lin",
            "Shao-Chia Weng",
            "Min-Yan Tsai"
        ],
        "file_path": "data/sosym-all/s10270-025-01316-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Adopting the concept of a function as an underlying semantic paradigm for modeling languages",
        "submission-date": "2023/11",
        "publication-date": "2023/11",
        "abstract": "There are researchers and practitioners in the areas of modeling and modeling language design who focus mainly on a syntactic point of view. For example, they may look at language design for language agglomerates like UML or SysML, as well as more customized DSLs, to consider the optimal syntactic constructs that are needed to cover all of the different kinds of phenomena occurring in a real-world problem context. Other researchers may focus on the semantic point of view to understand the meaning of a specific model as expressed in a language. A general challenge is when the same syntactic construct can be interpreted differently in various contexts of usage.\n\nA well known but often confusing example is the class diagram. A class “Person” in such a diagram may have different meanings based on the phase of development in which it is used. If the diagram has been defined during business requirements elicitation, the class actually represents human beings. In this case, “Persons” are real objects in the world of physical things. When the same class diagram is then used for design, the very same class “Person” suddenly describes the data structure that is capable of collecting data about human beings andthus apurelyvirtual concept emerges. Somewhere between requirements elicitation and design, the interpretationaddsastepofindirectionthatisnotreﬂectedinthesyntax itself. Other prominent examples use different interpretations of various modeling elements, which frequently lead to confusion. The situation is even more challenging when physical systems are accompanied with digital twins, where software components intelligently control a physical representation.\n\nIt would be helpful to have an underlying paradigm that connects all of the different interpretations and views (e.g., structure, data structure, interaction, behavior, state, agents, or activities) that are represented through various modeling techniques.\n\nIn the context of the upcoming SysML 2.0 deﬁnition, it is evident that a semantically sound and useful integration could play a major role toward addressing this challenge of differing interpretations. One possibility is to consider the often used paradigm of “function”.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01140-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Informal description and analysis of geographic requirements: an approach based on problems",
        "submission-date": "2005/07",
        "publication-date": "2006/08",
        "abstract": "Software requirements describe a problem in the real world that a software system is intended to solve. Describing requirements is challenging because usually too much attention is given to the ﬁnal soft-ware product instead of concentrating on the problem itself and the real world. The area of geographic appli-cations is no exception. Existing approaches to software development that are speciﬁc to the geographic area, for example, GIS tools, spatial databases, geographic query languages, and spatial data structures, are suitable for designing and implementing geographic applications and are, therefore, solution-oriented. We present a prob-lem-oriented approach for requirements description of geographic applications. Most geographic applications are composed of well-known geographic subproblems. The proposed approach provides classes of common geographic subproblems that can be used to promote analysis and description of real-world problems. Each class of problems is presented as a problem frame show-ing domain properties, requirements and speciﬁcations. The problem frames discussed in this work are based on Jackson’s general purpose problem frames and are tailored here for the geographic area. The approach is validated through a case study.",
        "keywords": [],
        "authors": [
            "Maria Augusta V. Nelson",
            "Paulo S. C. Alencar",
            "Donald D. Cowan"
        ],
        "file_path": "data/sosym-all/s10270-006-0031-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling presentation layers of web applications for testing",
        "submission-date": "2008/08",
        "publication-date": "2009/08",
        "abstract": "Websoftwareapplicationshavebecomecomplex,\nsophisticated programs that are based on novel computing\ntechnologies. Their most essential characteristic is that they\nrepresent a different kind of software deployment—most of\nthe software is never delivered to customers’ computers, but\nremains on servers, allowing customers to run the software\nacross the web. Although powerful, this deployment model\nbrings new challenges to developers and testers. Checking\nstatic HTML links is no longer sufﬁcient; web applications\nmust be evaluated as complex software products. This paper\nfocuses on three aspects of web applications that are unique\nto this type of deployment: (1) an extremely loose form of\ncoupling that features distributed integration, (2) the abil-\nity that users have to directly change the potential ﬂow of\nexecution, and (3) the dynamic creation of HTML forms.\nTaken together, these aspects allow the potential control\nﬂow to vary with each execution, thus the possible con-\ntrol ﬂows cannot be determined statically, prohibiting several\nstandard analysis techniques that are fundamental to many\nsoftware engineering activities. This paper presents a new\nway to model web applications, based on software couplings\nthat are new to web applications, dynamic ﬂow of control,\ndistributed integration, and partial dynamic web application\ndevelopment. This model is based on the notion of atomic\nsections, which allow analysis tools to build the analog of\na control ﬂow graph for web applications. The atomic sec-\ntion model has numerous applications in web applications;\nthis paper applies the model to the problem of testing web\napplications.",
        "keywords": [
            "Web applications",
            "Web modeling",
            "Test criteria"
        ],
        "authors": [
            "Jeff Offutt",
            "Ye Wu"
        ],
        "file_path": "data/sosym-all/s10270-009-0125-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward an ontology for EA modeling and EA model quality",
        "submission-date": "2023/04",
        "publication-date": "2024/02",
        "abstract": "Models have long since been used, in different shapes and forms, to understand, communicate about, and (re)shape, the world around us; including many different social, economic, biological, chemical, physical, and digital aspects. This is also the case in the context of enterprise architecture (EA), where we see a wide range of models in many different shapes and forms being used as well. Researchers in EA modeling usually introduce their own lexicon, and perspective of what a model actually is, while accepting (often implicitly) the accompanying ontological commitments. Similarly, practitioners of EA modeling implicitly also commit to (different) ontologies, resulting in models that have an uncertain ontological standing. This is because, for the subject domain of enterprise architecture models (as opposed to the content of such models), no single ontology has gained major traction. As a result, studies into aspects of enterprise architecture models, such as “model quality” and “return on modeling effort”, are fragmented, and cannot readily be compared or combined. This paper proposes a comprehensive applied ontology, speciﬁcally geared to enterprise architecture modeling. Ontologies represent structured knowledge about a particular subject domain. It allows for study into, and reasoning about, that subject domain. Our ontology is derived from a theory of modeling, while clarifying concepts such as “enterprise architecture model”, and introduces novel concepts such as “model audience” and “model objective”. Furthermore, the relevant interrelations between these different concepts are identiﬁed and deﬁned. The resulting ontology for enterprise architecture models is represented in OntoUML, and shown to be consistent with the foundational ontology for modeling, Uniﬁed Foundational Ontology.",
        "keywords": [
            "Enterprise architecture",
            "Ontology",
            "Domain model",
            "Enterprise architecture modeling",
            "Enterprise architecture model",
            "Architecture",
            "Model quality"
        ],
        "authors": [
            "Jan A. H. Schoonderbeek",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-023-01146-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Measurement and classification of inter-actor dependencies in goal models",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Goal-oriented requirements engineering approaches aim to capture desired goals and strategies of relevant stakeholders during early requirements engineering stages, using goal models. Socio-technical systems (STSs) involve a rich interplay of human actors (traditional stakeholders, described as actors in goal models) and technical systems. Actors may depend on each other for goals to be achieved, activities to be performed, and resources to be supplied. These dependencies create new opportunities by extending actors’ capabilities but may make the actor vulnerable if the dependee fails to deliver the dependum (knowingly or unintentionally). This paper proposes a novel quantitative metric, called Actor Interaction Metric (AIM), to measure inter-actor dependencies in Goal-oriented Requirements Language (GRL) models. The metric is used to categorize inter-actor dependencies into positive (beneﬁcial), negative (harmful), and neutral (no impact). Furthermore, the AIM metric is used to identify the most harmful/beneﬁcial dependency for each actor. The proposed approach is implemented in a tool targeting the textual GRL language, part of the User Requirements Notation (URN) standard. We evaluate experimentally our approach using 13 GRL models, with positive results on applicability and scalability.",
        "keywords": [
            "Goal-oriented requirements",
            "Metric",
            "Inter-actor dependencies",
            "GRL",
            "URN"
        ],
        "authors": [
            "Jameleddine Hassine",
            "Muhammad Tukur"
        ],
        "file_path": "data/sosym-all/s10270-021-00961-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating user interface generation approaches: model-based versus model-driven development",
        "submission-date": "2017/06",
        "publication-date": "2018/10",
        "abstract": "Advances in software design possibilities have led to a growing interest in the study of user interfaces (UIs). Many Model-Based User Interface Development Environments (MB-UIDEs) have been proposed to deal with the generation of the UIs (semi-) automatically by using models with different levels of abstraction. Often, this generation is limited to the UI part of an application. However, achieving true model-driven development (MDD) requires the co-development of application and UI and, hence, needs to go a step further. This paper analyzes a large set of existing MB-UIDEs, evaluates, from a critical perspective, to what extent they can be considered full MDD environments, and adequately addresses the co-design of UI and application. Following the guidelines proposed by Kitchenham and Charters (Engineering 2, 2007), we performed a systematic literature review. A total of 96 papers were examined. Based on these papers, an assessment framework containing 10 criteria with speciﬁc metrics to evaluate MB-UIDEs was deﬁned and 30 different environments were evaluated following these criteria. The evaluation identiﬁes several gaps in the existing state of the art and highlights the areas of promising improvement. The evaluation shows that, although a strong progress has being achieved over the last years, the existing environments do not yet fully exploit the beneﬁts and potentialities of MDD, nor do they adequately integrate UI design with application logic design and generation. Further research needs to be done to support the MDD of UIs and the co-design of the underlying application. The difﬁculty of use of the existing MB-UIDEs, the lack of UI design ﬂexibility, and the lack of complete and integrated development support are the main research gaps that need to be addressed.",
        "keywords": [
            "Model-based user interface software tools",
            "User interface generation",
            "Model-driven development",
            "Integration with application development"
        ],
        "authors": [
            "Jenny Ruiz",
            "Estefanía Serral",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-018-0698-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An extensible approach to implicit incremental model analyses",
        "submission-date": "2017/11",
        "publication-date": "2019/01",
        "abstract": "As systems evolve, analysis results based on models of the system must be updated, in many cases as fast as possible. Since usually only small parts of the model change, large parts of the analysis’ intermediate results could be reused in an incremental fashion. Manually invalidating these intermediate results at the right places in the analysis is a non-trivial and error-prone task that conceals the codes intention. A possible solution for this problem is implicit incrementality, i.e., an incremental algorithm is derived from the batch speciﬁcation, aiming for an increased performance without the cost of degraded maintainability. Current approaches are either specialized to a subset of analyses or require explicit state management. In this paper, we propose an approach to implicit incremental model analysis capable of integrating custom dynamic algorithms. For this, we formalize incremental derivation using category theory, gaining type-safety and correctness properties. We implement an extensible implicit incremental computation system and validate its applicability by integrating incremental queries. We evaluate the performance using a micro-benchmark and a community benchmark where the integration of explicit query incrementalization was multiple orders of magnitude faster than rerunning the analysis after every change.",
        "keywords": [
            "Incremental computation",
            "Model-driven",
            "Monads"
        ],
        "authors": [
            "Georg Hinkel",
            "Robert Heinrich",
            "Ralf Reussner"
        ],
        "file_path": "data/sosym-all/s10270-019-00719-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on PoEM’2020",
        "submission-date": "2022/05",
        "publication-date": "2022/06",
        "abstract": "This paper is a guest editorial to the special section on PoEM’2020, a working conference on the Practice of Enterprise Modeling. It discusses the conference, its focus on Enterprise Modeling in the Digital Age, and the selection process for papers published in this special section of the Journal of Software and Systems Modeling.",
        "keywords": [
            "Enterprise modeling",
            "Conceptual modeling"
        ],
        "authors": [
            "J¯anis Grabis",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-022-01017-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the persistent rumors of the programmer’s imminent demise",
        "submission-date": "2023/07",
        "publication-date": "2023/11",
        "abstract": "Since the dawn of programming, several developments in programming language design and programming methodology have been hailed as the end of the profession of programmer; they have all proven to be exaggerated rumors, to echo the words attributed to Mark Twain. In this short paper, we ponder the question of whether the emergence of large language models finally realizes these prophecies? Also, we discuss why even if this prophecy is finally realized, it does not change the job of the researcher in programming.",
        "keywords": [
            "Programming languages",
            "Automatic programming",
            "Programming profession",
            "Large language models"
        ],
        "authors": [
            "Hessam Mohammadi",
            "Wided Ghardallou",
            "Elijah Brick",
            "Ali Mili"
        ],
        "file_path": "data/sosym-all/s10270-023-01136-y.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Ten years of software and systems modeling",
        "submission-date": "2012/09",
        "publication-date": "2012/09",
        "abstract": "This editorial reflects on the ten years of the Journal of Software and Systems Modeling (SoSyM), reviewing changes in the field and looking towards future developments. It introduces a special issue containing ten invited papers from well-respected experts, focusing on the state-of-practice and potential future research agendas.",
        "keywords": [],
        "authors": [
            "Gregor Engels",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-012-0280-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling assistants based on information reuse: a user evaluation for language engineering",
        "submission-date": "2021/03",
        "publication-date": "2023/04",
        "abstract": "Model-driven engineering (MDE) uses models as ﬁrst-class artefacts during the software development lifecycle. MDE often relies on domain-speciﬁc languages (DSLs) to develop complex systems. The construction of a new DSL implies a deep understanding of a domain, whose relevant knowledge may be scattered in heterogeneous artefacts, like XML doc-uments, (meta-)models, and ontologies, among others. This heterogeneity hampers their reuse during (meta-)modelling processes. Under the hypothesis that reusing heterogeneous knowledge helps in building more accurate models, more efﬁciently, in previous works we built a (meta-)modelling assistant called Extremo. Extremo represents heterogeneous information sources with a common data model, supports its uniform querying and reusing information chunks for building (meta-)models. To understand how and whether modelling assistants—like Extremo—help in designing a new DSL, we conducted an empirical study, which we report in this paper. In the study, participants had to build a meta-model, and we measured the accuracy of the artefacts, the perceived usability and utility and the time to completion of the task. Interestingly, our results show that using assistance did not lead to faster completion times. However, participants using Extremo were more effective and efﬁcient, produced meta-models with higher levels of completeness and correctness, and overall perceived the assistant as useful. The results are not only relevant to Extremo, but we discuss their implications for future modelling assistants.",
        "keywords": [
            "Modelling",
            "Modelling assistants",
            "Language engineering",
            "Modelling process",
            "Empirical studies"
        ],
        "authors": [
            "Ángel Mora Segura",
            "Juan de Lara",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-023-01094-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The evolution of model editors: browser- and cloud-based solutions",
        "submission-date": "2016/04",
        "publication-date": "2016/04",
        "abstract": "In March 2006, Google purchased Upstartle to gain access to their browser-based word processor called Writely [1]. This acquisition from over a decade ago led to what we now know as Google Docs, which ushered in a new form of collabo-rative authoring tools. The idea of using a Web browser as an editing platform, coupled with the storage options avail-able within the cloud, provides powerful new capabilities that have transformed the way we interact with colleagues to design and create documents, as well as all other sorts of artifacts. Specialized text processing solutions, like the LaTeX-focused Overleaf environment [2], bring a fresh new approach to collaboration using long-standing traditional tools.Furthermore,browser-andcloud-basedauthoringtools have penetrated many domains. For example, in computer science education, tools such as Scratch help new program-mers learn block-based coding in a browser, where programs are stored in the cloud with a large repository (over 13.M shared Scratch programs are available at the time of this writ-ing) of user-shared examples [3].",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-016-0524-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SOCAM: a service-oriented computing architecture modeling method",
        "submission-date": "2021/01",
        "publication-date": "2021/11",
        "abstract": "Software architecture models are considered ﬁrst-class artifacts in current software engineering best practices. Thus, usable\nand well-understood modeling methods are required for software architects. For this aim, several speciﬁc software architecture\nmodeling methods as well as generic design methods included in software development methodologies are available. However,\nwe believe, there is the lack of more speciﬁc guidance in current software architecture methods. One of the principal causes\nof such a lack of speciﬁc guidance is the general-purpose nature of these methods. Therefore, further efforts are required to\ndefine domain-speciﬁc software architecture methods. In this paper, we present SOCAM, a software architecture modeling\nmethod for Web Service-Oriented Systems. We illustrate the use of SOCAM with a customization of the well-known SOA\ntest application case: the Sun Adventure Builder system. A comparative analysis of SOCAM with other methods reveals a\nnumber of beneﬁts of our method over the other approaches. Also, a survey research method evaluation conﬁrms some of\nthese beneﬁts such as the fact that SOCAM is perceived as more useful than certain general-purpose methods.",
        "keywords": [
            "Software architecture",
            "SOC",
            "SOA",
            "Web-based business systems"
        ],
        "authors": [
            "Paola Y. Reyes-Delgado",
            "Hector A. Duran-Limon",
            "Manuel Mora",
            "Laura C. Rodriguez-Martinez"
        ],
        "file_path": "data/sosym-all/s10270-021-00946-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models and temporal logical speciﬁcations for timed component connectors",
        "submission-date": "2005/02",
        "publication-date": "2006/08",
        "abstract": "Component-based software engineering advocates construction of software systems through composition of coordinated autonomous components. Signiﬁcant beneﬁts of this approach include software reuse, simpler and faster construction, enhanced reli- ability, and dramatic reductions in the complexity of construction of provably correct critical systems, many of which involve real-time concerns. Effective, ﬂexible component composition by itself still poses a challenge today and yet the special nature of real-time constraints makes component-based construction of real-time systems even more demanding. The coordination language Reo supports compositional system construction through connectors that exogenously coor- dinate the interactions among the constituent compo- nents which unawarely comprise a complex system, into a coherent collaboration. The simple, yet surprisingly rich, calculus of channel composition that underlies Reo offers a ﬂexible framework for compositional construction of coordinating component connectors with real-time properties. In this paper, we present an operational semantics for the channel-based component connectors of Reo in terms of Timed Constraint Auto- mata and introduce a temporal-logic for speciﬁcation and veriﬁcation of their real-time properties.",
        "keywords": [
            "Coordination",
            "Real-time",
            "Composition",
            "Reo",
            "Constraint automata",
            "Timed automata",
            "Linear temporal logic",
            "Timed data streams"
        ],
        "authors": [
            "Farhad Arbab",
            "Christel Baier",
            "Frank de Boer",
            "Jan Rutten"
        ],
        "file_path": "data/sosym-all/s10270-006-0009-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "VMTL: a language for end-user model transformation",
        "submission-date": "2015/10",
        "publication-date": "2016/07",
        "abstract": "Model transformation is a key enabling technology of Model-Driven Engineering (MDE). Existing model transformation languages are shaped by and for MDE practitioners—a user group with needs and capabilities which are not necessarily characteristic of modelers in general. Consequently, these languages are largely ill-equipped for adoption by end-user modelers in areas such as requirements engineering, business process management, or enterprise architecture. We aim to introduce a model transformation language addressing the skills and requirements of end-user modelers. With this contribution, we hope to broaden the application scope of model transformation and MDE technology in general. We discuss the proﬁle of end-user modelers and propose a set of design guidelines for model transformation languages addressing them. We then introduce Visual Model Transformation Language (VMTL) following these guidelines. VMTL draws on our previous work on the usability-oriented Visual Model Query Language. We implement VMTL using the Henshin model transformation engine, and empirically investigate its learnability via two user experiments and a think-aloud protocol analysis. Our experiments, although conducted on computer science students exhibiting only some of the characteristics of end-user modelers, show that VMTL compares favorably in terms of learnability with two state-of the-art model transformation languages: Epsilon and Henshin. Our think-aloud protocol analysis conﬁrms many of the design decisions adopted for VMTL, while also indicating possible improvements.",
        "keywords": [
            "End-user modelers",
            "Transparent model transformation",
            "VMTL",
            "Henshin",
            "Epsilon",
            "Learnability",
            "Experiment",
            "Think-aloud protocol"
        ],
        "authors": [
            "Vlad Acre¸toaie",
            "Harald Störrle",
            "Daniel Strüber"
        ],
        "file_path": "data/sosym-all/s10270-016-0546-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "VMTL, Henshin, Epsilon"
        }
    },
    {
        "title": "Controllable and decomposable multidirectional synchronizations",
        "submission-date": "2020/08",
        "publication-date": "2021/04",
        "abstract": "Studying large-scale collaborative systems engineering projects across teams with differing intellectual property clearances,\nor healthcare solutions where sensitive patient data needs to be partially shared, or similar multi-user information systems over\ndatabases, all boils down to a common mathematical framework. Updateable views (lenses) and more generally bidirectional\ntransformations are abstractions to study the challenge of exchanging information between participants with different read\naccess privileges. The view provided to each participant must be different due to access control or other limitations, yet also\nconsistent in a certain sense, to enable collaboration towards common goals. A collaboration system must apply bidirectional\nsynchronization to ensure that after a participant modiﬁes their view, the views of other participants are updated so that\nthey are consistent again. While bidirectional transformations (synchronizations) have been extensively studied, there are\nnew challenges that are unique to the multidirectional case. If complex consistency constraints have to be maintained, \nsynchronizations that work ﬁne in isolation may not compose well. We demonstrate and characterize a failure mode of\nthe emergent behaviour, where a consistency restoration mechanism undoes the work of other participants. On the other\nend of the spectrum, we study the case where synchronizations work especially well together: we characterize very well-\nbehaved multidirectional transformations, a non-trivial generalization from the bidirectional case. For the former challenge, we \nintroduceanovel concept of controllability, whilefor thelatter one, weproposeanovel formal notionof faithful decomposition.\nAdditionally, the paper proposes several novel properties of multidirectional transformations.",
        "keywords": [
            "Bidirectional transformation",
            "Very well behaved",
            "Collaborative engineering"
        ],
        "authors": [
            "Gábor Bergmann"
        ],
        "file_path": "data/sosym-all/s10270-021-00879-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Location-aware business process modeling and execution",
        "submission-date": "2023/10",
        "publication-date": "2024/11",
        "abstract": "Locally distributed processes include several process participants working on tasks at different locations, e.g., craftspeople working on construction sites. Compared to classical IT environments, new challenges emerge due to the spatial context of a process. Real-time location data from Internet of Things (IoT) devices can help businesses implement more efﬁcient and effective processes through business process management (BPM). However, only small parts of existing research have touched on those advantages, while the architecture and implementation of actual executable location-aware processes area has only been vaguely considered. Therefore, we introduce and present a non-exhaustive list of patterns for using location data in BPM while also including an actual implementation of a location-aware approach using a multilayer system architecture based on standard BPM technology. These can be used to leverage the location perspective of process entities as contextual data in BPM.",
        "keywords": [
            "Process execution",
            "Location-awareness",
            "Distributed processes"
        ],
        "authors": [
            "Leo Poss",
            "Stefan Schönig"
        ],
        "file_path": "data/sosym-all/s10270-024-01224-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From use case maps to executable test procedures: a scenario-based approach",
        "submission-date": "2016/06",
        "publication-date": "2017/08",
        "abstract": "Testing embedded systems software has become a costly activity as these systems become more complex to fulfill rising needs. Testing processes should be both effective and affordable. An ideal testing process should begin with validated requirements and begin as early as possible so that requirements defects can be fixed before they propagate and become more difficult to address. Furthermore, the testing process should facilitate test procedures creation and automate their execution. We propose a novel methodology for testing functional requirements. The methodology activities include standard notations, such as UCM for modeling scenarios derived from requirements, TDL for describing test cases and TTCN-3 for executing test procedures; other test scripting languages can also be used with our methodology. Furthermore, the automation of the methodology generates test artifacts through model transformation. The main goals of this test methodology are to leverage requirements represented as scenarios, to replace the natural language test case descriptions with test scenarios in TDL, and to generate executable test procedures. Demonstration of the feasibility of the proposed approach is based on a public case study. An empirical evaluation of our approach is given using a case study from the avionics domain.",
        "keywords": [
            "Model-driven testing",
            "Testing methodology",
            "Embedded systems",
            "Test generation",
            "TTCN-3",
            "TDL",
            "UCM"
        ],
        "authors": [
            "Nader Kesserwan",
            "Rachida Dssouli",
            "Jamal Bentahar",
            "Bernard Stepien",
            "Pierre Labrèche"
        ],
        "file_path": "data/sosym-all/s10270-017-0620-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From subsets of model elements to submodels A characterization of submodels and their properties",
        "submission-date": "2012/01",
        "publication-date": "2013/04",
        "abstract": "Model-driven engineering (MDE) generalized the status of models from documentation or model-driven architecture (MDA) modeling steps to full artifacts, members of a so-called structured “model space”. We concentrate here on the submodel relationship which contributes a lot to this structuring effort. Many works and MDE practices resort to this notion and call for its precise characterization, which is the intent of this paper. A typical situation is model management through repositories. We start from the deﬁnition of a model as a set of model elements plus a set of dependency constraints that it asserts over these elements. This allows to isolate the notions of closed, covariant and invariant submodels. As a major result, we show that submodel transitivity can be guaranteed thanks to submodel invariance. This formalization offers keys to analyze operations which manipulate submodels. For example, we deeply study the operator which consists in extracting a model from another one, when selecting some subset of its elements. The same can be applied to many other model operations and the last part of the paper is dedicated to a synthesis on related works which could proﬁt from this characterization. More practically, we show how the results were exploited in our Eclipse modeling environment.",
        "keywords": [
            "Submodel",
            "Set-theoretic formalization",
            "Model extraction",
            "Model composition"
        ],
        "authors": [
            "Bernard Carré",
            "Gilles Vanwormhoudt",
            "Olivier Caron"
        ],
        "file_path": "data/sosym-all/s10270-013-0340-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "METAMORPH: formalization of domain-speciﬁc conceptual modeling methods—an evaluative case study, juxtaposition and empirical assessment",
        "submission-date": "2021/11",
        "publication-date": "2022/10",
        "abstract": "Models have evolved from mere pictures supporting human understanding and communication to sophisticated knowledge structures processable by machines and establish value through their processing capabilities. This entails an inevitable need for computer-understandable modeling languages and causes formalization to be a crucial part in the lifecycle of engineering a modeling method. An appropriate formalism must be a means for providing a structural deﬁnition to enable a theoretical investigation of conceptual modeling languages and a unique, unambiguous way of specifying the syntax and semantics of an arbitrary modeling language. For this purpose, it must be generic and open to capturing any domain and any functionality. This paper provides a pervasive description of the formalism MetaMorph based on logic and model theory—an approach fulﬁlling the requirements above for modeling method engineering. The evaluation of the formalism is presented following three streams of work: First, two evaluative case studies illustrate the applicability of MetaMorph formalism concept by concept on the modeling language ProVis from the domain of stochastic education and the well-known Entity-Relationship language. ProVis as well as ER comprise only a few objects and relation types but with high interconnection and expressive power and are therefore considered interesting specimens for formalization. Second, a comprehensive juxtaposition of MetaMorph to three other formalization approaches based on different foundational theories is outlined concept by concept to underpin the formalism design. Third, an empirical evaluation has been performed, assessing the usability and adequacy of the formalism within a classroom assessment. The results allow for conclusions on the completeness, intuitiveness, and complexity as well as on interdependencies with engineers’ skills.",
        "keywords": [
            "Conceptual modeling",
            "Agile modeling method engineering",
            "Domain-speciﬁc modeling language",
            "Formal language",
            "Logic",
            "Evaluation"
        ],
        "authors": [
            "Victoria Döller",
            "Dimitris Karagiannis",
            "Wilfrid Utz"
        ],
        "file_path": "data/sosym-all/s10270-022-01047-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial",
        "submission-date": "2002/10",
        "publication-date": "2003/11",
        "abstract": "This editorial provides a report on the first year of the Software and System Modeling (SoSyM) journal. It discusses the journal's launch, growth, and challenges, as well as acknowledging the contributions of reviewers, editors, and publication staff.",
        "keywords": [],
        "authors": [
            "Robert B. France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-003-0041-y.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Unifying classes and processes",
        "submission-date": "2005/06",
        "publication-date": "2005/06",
        "abstract": "Previously, we presented Circus, an integration of Z, CSP, and Morgan’s refinement calculus, with a semantics based on the unifying theories of programming. Circus provides a basis for development of state-rich concurrent systems; it has a formal semantics, a refinement theory, and a development strategy. The design of Circus is our solution to combining data and behavioural specifications. Here, we further explore this issue in the context of object-oriented features. Concretely, we present an object-oriented extension of Circus called OhCircus. We present its syntax, describe its semantics, explain the formalisation of method calls, and discuss our approach to refinement.",
        "keywords": [
            "Z",
            "CSP",
            "Refinement",
            "Integration"
        ],
        "authors": [
            "Ana Cavalcanti",
            "Augusto Sampaio",
            "Jim Woodcock"
        ],
        "file_path": "data/sosym-all/s10270-005-0085-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design methods for the new database era: a systematic literature review",
        "submission-date": "2018/09",
        "publication-date": "2019/06",
        "abstract": "Over the last decade, a range of new database solutions and technologies have emerged, in line with the new types of applications and requirements that they facilitate. Consequently, various new methods for designing these new databases have evolved, in order to keep pace with progress in the field. In this paper, we systematically review these methods, with a view to better understanding their suitability for designing new database solutions. The study shows that while research in the field has expanded continuously, a range of factors still require further attention. The study identified important criteria in database design and analyzed existing studies accordingly. This analysis will assist in defining and recommending key areas for future research, guiding the evolution of design methods, their usability and adaptability in real-world scenarios. The study found that current database design methods do not address non-functional requirements; tend to refer to a preselected database; and are lacking in their evaluation.",
        "keywords": [
            "Database design",
            "Database modeling",
            "NoSQL",
            "Design methods"
        ],
        "authors": [
            "Noa Roy‑Hubara",
            "Arnon Sturm"
        ],
        "file_path": "data/sosym-all/s10270-019-00739-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for EMMSAD’2018 special section",
        "submission-date": "2019/10",
        "publication-date": "2019/12",
        "abstract": "The exploring modeling methods for systems analysis and development (EMMSAD) series has produced 23 events, associated with conference on advanced information systems engineering (CAiSE), from 1996 to 2018. From 2009, EMMSAD has become a two-day working conference. The topics addressed by the EMMSAD series focus on modeling methods for software and information systems development [4], enterprise management [3], and business process management [1]. It further refers to evaluation of modeling methods through a variety of empirical and non-empirical approaches (a review and comparative analysis of such evaluation techniques can be found at [6]). The aims, topics, and history of EMMSAD can be found on the Website at http://www.emmsa​d.org/. ",
        "keywords": [],
        "authors": [
            "Iris Reinhartz‑Berger",
            "Sérgio Guerreiro"
        ],
        "file_path": "data/sosym-all/s10270-019-00769-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "What makes life for process mining analysts difﬁcult? A reﬂection of challenges",
        "submission-date": "2023/01",
        "publication-date": "2023/11",
        "abstract": "Over the past few years, several software companies have emerged that offer process mining tools to assist enterprises in gaining insights into their process executions. However, the effective application of process mining technologies depends on analysts who need to be proﬁcient in managing process mining projects and providing process insights and improvement opportunities. To contribute to a better understanding of the difﬁculties encountered by analysts and to pave the way for the development of enhanced and tailored support for them, this work reveals the challenges they perceive in practice. In particular, we identify 23 challenges based on interviews with 41 analysts, which we validate using a questionnaire survey. We provide insights into the relevancy of the process mining challenges and present mitigation strategies applied in practice to overcome them. While mitigation strategies exist, our ﬁndings imply the need for further research to provide support for analysts along all phases of process mining projects on the individual level, but also the technical, group, and organizational levels.",
        "keywords": [
            "Process mining",
            "Challenges",
            "Mitigation strategies",
            "Process analysis",
            "Work practices"
        ],
        "authors": [
            "Lisa Zimmermann",
            "Francesca Zerbato",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-023-01134-0.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A survey on the design space of end-user-oriented languages for specifying robotic missions",
        "submission-date": "2019/11",
        "publication-date": "2021/02",
        "abstract": "Mobile robots are becoming increasingly important in society. Fulﬁlling complex missions in different contexts and envi-ronments, robots are promising instruments to support our everyday live. As such, the task of deﬁning the robot’s mission is moving from professional developers and roboticists to the end-users. However, with the current state-of-the-art, deﬁning missions is non-trivial and typically requires dedicated programming skills. Since end-users usually lack such skills, many commercial robots are nowadays equipped with environments and domain-speciﬁc languages tailored for end-users. As such, the software support for deﬁning missions is becoming an increasingly relevant criterion when buying or choosing robots. Improving these environments and languages for specifying missions toward simplicity and ﬂexibility is crucial. To this end, we need to improve our empirical understanding of the current state-of-the-art of such languages and their environments. In this paper, we contribute in this direction. We present a survey of 30 mission speciﬁcation environments for mobile robots that come with a visual and end-user-oriented language. We explore the design space of these languages and their environments, identify their concepts, and organize them as features in a feature model. We believe that our results are valuable to prac-titioners and researchers designing the next generation of mission speciﬁcation languages in the vibrant domain of mobile robots.",
        "keywords": [
            "Speciﬁcation environments",
            "Language concepts",
            "Visual languages",
            "Robotic missions",
            "Empirical study"
        ],
        "authors": [
            "Swaib Dragule",
            "Thorsten Berger",
            "Claudio Menghi",
            "Patrizio Pelliccione"
        ],
        "file_path": "data/sosym-all/s10270-020-00854-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based development",
        "submission-date": "2007/11",
        "publication-date": "2007/11",
        "abstract": "This issue contains the second part of the regular papers that have been nurtured from Models’05 conference.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-007-0071-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Application of reﬂection in a model transformation language",
        "submission-date": "2008/11",
        "publication-date": "2009/11",
        "abstract": "Computational reﬂection is a well-known tech-\nnique applied in many existing programming languages\nranging from functional to object-oriented languages. In this\npaper we study the possibilities and beneﬁts of introducing\nand using reﬂection in a rule-based model transformation\nlanguage. The paper identiﬁes some language abstractions to\nachieve structural and behavioral reﬂection. Reﬂective fea-\ntures are motivated by examples of problems derived from\nthe experience with currently used transformation languages.\nExample solutions are given by using an experimental lan-\nguage with reﬂective capabilities. The paper also outlines\npossible implementation strategies for adding reﬂection to a\nlanguage and discusses their advantages and disadvantages.",
        "keywords": [
            "Reﬂection",
            "Model transformation languages",
            "MDE",
            "MISTRAL"
        ],
        "authors": [
            "Ivan Kurtev"
        ],
        "file_path": "data/sosym-all/s10270-009-0138-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "MISTRAL"
        }
    },
    {
        "title": "Model‑based test case generation and prioritization: a systematic literature review",
        "submission-date": "2020/12",
        "publication-date": "2021/09",
        "abstract": "Model-based test case generation (MB-TCG) and prioritization (MB-TCP) utilize models that represent the system under test (SUT) for test generation and prioritization in software testing. They are based on model-based testing (MBT), a technique that facilitates automation in testing. Automated testing is indispensable for testing complex and industrial-size systems because of its advantages over manual testing. In recent years, MB-TCG and MB-TCP publications have shown an encourag-ing growth. However, the empirical studies done to validate these approaches must not be taken lightly because they reflect the results' validity and whether these approaches are generalizable to the industrial context. This systematic review aims at identifying and reviewing the state-of-the-art for MB-TCG, MB-TCP, and the approaches that combined MB-TCG and MB-TCP. The needs for this review were used to design the research questions. Keywords extracted from the research questions were utilized to search for studies in the literature that will answer the research questions. Prospective studies also underwent a quality assessment to ensure that only studies with sufficient quality were selected. All the research data of this review are also available in a public repository for full transparency. 122 primary studies were finalized and selected. There were 100, 15, and seven studies proposed for MB-TCG, MB-TCP, and MB-TCG and MB-TCP combination approaches, respectively. One of the main findings is that the most common limitations in the existing approaches are the dependency on specifica-tions, the need for manual interventions, and the scalability issue.",
        "keywords": [
            "Model-based testing",
            "Test case prioritization",
            "Test case generation",
            "Systematic literature review"
        ],
        "authors": [
            "Muhammad Luqman Mohd‑Shafie",
            "Wan Mohd Nasir Wan Kadir",
            "Horst Lichter",
            "Muhammad Khatibsyarbini",
            "Mohd Adham Isa"
        ],
        "file_path": "data/sosym-all/s10270-021-00924-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Software and systems modeling with graph transformations theme issue of the Journal on Software and Systems Modeling",
        "submission-date": "2012/06",
        "publication-date": "2012/06",
        "abstract": "Over the years model-based development has rapidly gained popularity in various engineering disciplines. Numerous efforts have resulted in the invention of an abundance of appropriate modeling concepts, languages, and tools. Today modeling activities often span multiple disciplines and have to be addressed by collaborative efforts across disci-plines such as industrial automation, business engineering, hardware/softwareco-design,real-timesystemdevelopment, Web 2.0 application design, and so forth. As a consequence, model-based development techniques related to the analysis, synchronization, and execution of families of models that are concurrently developed by different engineers on differ-ent levels of abstraction play a major role in many software and systems development projects. Graphs, on the other hand, are among the simplest and most universal models for a variety of systems, not just in computer science, but throughout engineering and the life sciences. Graph transformations combine the idea of graphs as a universal modeling paradigm with a rule-based mathematically well-founded approach to specify their evolution.",
        "keywords": [],
        "authors": [
            "Andy Schürr",
            "Arend Rensink"
        ],
        "file_path": "data/sosym-all/s10270-012-0254-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "IAT/ML: a metamodel and modelling approach for discourse analysis",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "Languagetechnologiesaregainingmomentumastextualinformationsaturatessocialnetworksandmediaoutlets,compounded\nby the growing role of fake news and disinformation. In this context, approaches to represent and analyse public speeches,\nnews releases, social media posts and other types of discourses are becoming crucial. Although there is a large body of\nliterature on text-based machine learning, it tends to focus on lexical and syntactical issues rather than semantic or pragmatic.\nBeing useful, these advances cannot tackle the nuanced and highly context-dependent problems of discourse evaluation that\nsociety demands. In this paper, we present IAT/ML, a metamodel and modelling approach to represent and analyse discourses.\nIAT/ML focuses on semantic and pragmatic issues, thus tackling a little researched area in language technologies. It does so by\ncombining three different modelling approaches: ontological, which focuses on what the discourse is about; argumentation,\nwhich deals with how the text justiﬁes what it says; and agency, which provides insights into the speakers’ beliefs, desires\nand intentions. Together, these three modelling approaches make IAT/ML a comprehensive solution to represent and analyse\ncomplex discourses towards their understanding, evaluation and fact checking.",
        "keywords": [
            "Natural language",
            "Discourse",
            "Argumentation",
            "Ontologies",
            "Metamodel",
            "IAT/ML"
        ],
        "authors": [
            "Cesar Gonzalez-Perez",
            "Martín Pereira-Fariña",
            "Beatriz Calderón-Cerrato",
            "Patricia Martín-Rodilla"
        ],
        "file_path": "data/sosym-all/s10270-024-01208-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The next evolution of MDE: a seamless integration of machine learning into domain modeling",
        "submission-date": "2016/08",
        "publication-date": "2017/05",
        "abstract": "Machine learning algorithms are designed to resolveunknownbehaviorsbyextractingcommonalitiesover massive datasets. Unfortunately, learning such global behaviors can be inaccurate and slow for systems composed of heterogeneous elements, which behave very differently, for instance as it is the case for cyber-physical systems and Internet of Things applications. Instead, to make smart decisions, such systems have to continuously reﬁne the behavior on a per-element basis and compose these small learning units together. However, combining and composing learned behaviors from different elements is challenging and requires domain knowledge. Therefore, there is a need to structure and combine the learned behaviors and domain knowl-edge together in a ﬂexible way. In this paper we propose to weave machine learning into domain modeling. More speciﬁcally, we suggest to decompose machine learning into reusable, chainable, and independently computable small learning units, which we refer to as microlearning units. These microlearning units are modeled together with and at the same level as the domain data. We show, based on a smart grid case study, that our approach can be signiﬁcantly more accurate than learning a global behavior, while the per-formance is fast enough to be used for live learning.",
        "keywords": [
            "Domain modeling",
            "Live learning",
            "Model-driven engineering",
            "Metamodeling",
            "Cyber-physical systems",
            "Smart grids"
        ],
        "authors": [
            "Thomas Hartmann",
            "Assaad Moawad",
            "Francois Fouquet",
            "Yves Le Traon"
        ],
        "file_path": "data/sosym-all/s10270-017-0600-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special issue on UML 2004",
        "submission-date": "2006/03",
        "publication-date": "2006/07",
        "abstract": "The UML 2004 was held in Lisbon (Portugal), 11–15, October 2004 and was the seventh conference in the series of annual UML conferences. The 2004 edition of the UML conference proved to be a milestone event since it was the last conference under the UML banner, and the announcement of the new format, MoDELS (MOdel Driven Engineering, Languages and Systems).",
        "keywords": [],
        "authors": [
            "Thomas Baar",
            "Ana Moreira"
        ],
        "file_path": "data/sosym-all/s10270-006-0021-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Completion of SysML state machines from Given–When–Then requirements",
        "submission-date": "2023/08",
        "publication-date": "2024/11",
        "abstract": "MDE enables the centrality of the models in semi-automated development processes. However, its level of usage in industrial settings is still not adequate for the beneﬁts MDE can introduce. This paper proposes a semi-automatic approach for the completion of high-level models in the lifecycle of critical systems, which exhibit an event-driven behaviour. The proposal suggests a speciﬁcation guideline that starts from a partial SysML model of a system and on a set of requirements, expressed in the well-known Given–When–Then paradigm. On the basis of such requirements, the approach enables the semi-automatic generation of new SysML state machines model elements. Accordingly, the approach focuses on the completion of the state machines by adding proper transitions (with triggers, guards and effects) among pre-existing states. Also, traceability modelling elements are added to the model. Two case studies demonstrate the feasibility of the proposed approach.",
        "keywords": [
            "Behaviour-driven development",
            "Requirements engineering",
            "SysML",
            "Critical systems design",
            "Event-driven systems design"
        ],
        "authors": [
            "Maria Stella de Biase",
            "Simona Bernardi",
            "Stefano Marrone",
            "José Merseguer",
            "Angelo Palladino"
        ],
        "file_path": "data/sosym-all/s10270-024-01228-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A formal veriﬁcation framework for static analysis",
        "submission-date": "2013/11",
        "publication-date": "2015/07",
        "abstract": "Static analysis tools, such as resource analyzers, give useful information on software systems, especially in real-time and safety-critical applications. Therefore, the question of the reliability of the obtained results is highly important. State-of-the-art static analyzers typically combine a range of complex techniques, make use of external tools, and evolve quickly. To formally verify such systems is not a realistic option. In this work, we propose a different approach whereby, instead of the tools, we formally verify the results of the tools. The central idea of such a formal veriﬁcation framework for static analysis is the method-wise translation of the information about a program gathered during its static analysis into speciﬁcation contracts that contain enough information for them to be veriﬁed automatically. We instantiate this framework with costa, a state-of-the-art static analysis system for sequential Java programs, for producing resource guarantees and KeY, a state-of-the-art veriﬁcation tool, for formally verifying the correctness of such resource guarantees. Resource guarantees allow to be certain that programs will run within the indicated amount of resources, which may refer to memory consumption, number of instructions executed, etc. Our results show that the proposed tool cooperation can be used for automatically producing veriﬁed resource guarantees.",
        "keywords": [
            "Cost analysis",
            "Closed-form upper bounds",
            "Resource analysis",
            "Resource guarantees"
        ],
        "authors": [
            "Elvira Albert\nRichard Bubel\nSamir Genaim\nReiner Hähnle\nGermán Puebla\nGuillermo Román-Díez"
        ],
        "file_path": "data/sosym-all/s10270-015-0476-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design for service compatibility Behavioural compatibility checking and diagnosis",
        "submission-date": "2012/08",
        "publication-date": "2012/08",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Georg Grossmann",
            "Michael Schreﬂ",
            "Markus Stumptner"
        ],
        "file_path": "data/sosym-all/s10270-012-0267-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling for the cloud",
        "submission-date": "2010/03",
        "publication-date": "2010/03",
        "abstract": "Cloud computing is poised to become a major driving force behind European and American businesses. Long-standing projects like the SETI@Home project and facilities such as SourceForge leverage third party distributed storage and computational resources to deliver services. Companies are seeking to commercialize this approach to service delivery through the use of cloud computing technologies. Cloud computing commerce can take several forms: customers can rent an infrastructure, a platform, or predeﬁned services. While predeﬁned cloud-based services for email, blogs, wikis, and media storage are well known, more complex business oriented applications like customer relationship management are starting to appear. While companies such as Amazon, Google, and Force.com are providing services for and from the cloud there are aspects of cloud com- puting that can beneﬁt from research in the model-driven software development area. For example, software and sys- tem modeling research can yield results that address prob- lems related to the safety and integrity of data (where the user does not control the physical location of the storage anymore), efﬁciency of storage and retrieval, and decou- pling of applications from underlying operating systems, and other computing platforms and infrastructures. Software and system modeling research can also produce results that head-off future problems related to migration of services to new cloud computing environments that will inevitably arise as technologies evolve. For example, enabling inter- operability across cloud computing environments, and inte- grating mobile and cloud-based applications are challenging problems that will arise.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-010-0159-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Change propagation and bidirectionality in internal transformation DSLs",
        "submission-date": "2016/06",
        "publication-date": "2017/08",
        "abstract": "Despite good results in several industrial projects, model-driven engineering (MDE) has not been widely adopted in industry. Although MDE has existed for more than a decade now, the lack of tool support is still one of the major problems, according to studies by Staron and Mohaghegi (Staron, in: Model driven engineering languages and systems, Springer, Berlin, 2006; Mohagheghi et al. in Empir Softw Eng 18(1):89–116, 2013). Internal languages offer a solution to this problem for model transformations, which are a key part of MDE. Developers can use exist-ing tools of host languages to create model transformations in a familiar environment. These internal languages, however, typically lack key features such as change propagation or bidirectional transformations. In our opinion, one reason is that existing formalisms for these properties are not well suited for textual languages. In this paper, we present a new formalism describing incremental, bidirectional model syn-chronizations using synchronization blocks. We prove the ability of this formalism to detect and repair inconsistencies and show its hippocraticness. We use this formalism to create a single internal model transformation language for unidirec-tional and bidirectional model transformations with optional change propagation. In total, we currently provide 18 opera-tion modes based on a single speciﬁcation. At the same time, the language may reuse tool support for C#. We validate the applicabilityofourlanguageusingasyntheticexamplewitha transformation from ﬁnite state machines to Petri nets where we achieved speedups of up to multiple orders of magnitude compared to classical batch transformations.",
        "keywords": [
            "Model-driven engineering",
            "Model synchronization",
            "Domain-speciﬁc language",
            "Change propagation",
            "Bidirectional",
            "Incremental"
        ],
        "authors": [
            "Georg Hinkel",
            "Erik Burger"
        ],
        "file_path": "data/sosym-all/s10270-017-0617-6.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "None"
        }
    },
    {
        "title": "Handling causality and schedulability when designing and prototyping cyber-physical systems",
        "submission-date": "2020/02",
        "publication-date": "2021/02",
        "abstract": "Cyberphysicalsystemsarebuiltupondigitalandanalogcircuits,makingitnecessarytohandledifferentmodelsofcomputation during their design and veriﬁcation (e.g., by simulation). When designing these systems, an important aspect to consider is the causality between the different domains. For this, we introduce a new model-driven framework able to identify causality problems and to suggest a valid schedule between the analog and digital domains. Once a valid schedule has been computed, our framework can generate cycle and bit accurate virtual prototypes (in SystemC/SystemC AMS) from high-level SysML models.",
        "keywords": [
            "Cyber-physical systems",
            "Virtual prototyping",
            "Co-simulation"
        ],
        "authors": [
            "Rodrigo Cortés Porto1",
            "2",
            "Daniela Genius1",
            "Ludovic Apvrille3"
        ],
        "file_path": "data/sosym-all/s10270-021-00866-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An executable metamodel refactoring catalog",
        "submission-date": "2022/06",
        "publication-date": "2022/08",
        "abstract": "Like any software artifacts, metamodels are evolving entities that constantly change over time for different reasons. Changing metamodels by keeping them consistent with other existing artifacts is an error-prone and tedious activity without the availability of automated support. In this paper, we foster the adoption of metamodel refactorings collected in a curated catalog. The Edelta framework is proposed as an operative environment to provide modelers with constructs for specifying basic refactorings and evolution operators, to deﬁne a complete metamodel refactoring catalog. The proposed environment has been used to implement the metamodel refactorings available in the literature and make them executable. A detailed discussion on how modelers can use and contribute to the deﬁnition of the catalog is also given.",
        "keywords": [
            "Metamodels",
            "Evolution",
            "Refactoring",
            "Catalog"
        ],
        "authors": [
            "Lorenzo Bettini",
            "Davide Di Ruscio",
            "Ludovico Iovino",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-022-01034-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Edelta"
        }
    },
    {
        "title": "Guest editorial for EMMSAD’2023 special section",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "The Exploring Modeling Methods for Systems Analysis and Development (EMMSAD) conference series organized 29 events\nfrom 1996 to 2024, associated with Conference on Advanced Information Systems Engineering. In 2009, EMMSAD became\na two-day working conference. Since 2017, the authors of EMMSAD’s best papers are invited to submit extended versions\nof their paper, for consideration to be published in the Journal of Software and Systems Modeling. The main topics of the\nEMMSAD series focus on models and modeling methods for the analysis and development of software information systems\nof any kind. These are organized into ﬁve tracks: (1) Foundations of Modeling and Method Engineering; (2) Enterprise, Business, Process, and Capability Modeling; (3) Information Systems and Requirements Modeling; (4) Domain-Speciﬁc and Knowledge Modeling; and (5) Evaluation of Models and Modeling Approaches. The aims, topics, and history of EMMSAD\ncan be also found on its website at http://www.emmsad.org/.",
        "keywords": [],
        "authors": [
            "Dominik Bork",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-024-01213-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Fault localization in DSLTrans model transformations by combining symbolic execution and spectrum-based analysis",
        "submission-date": "2022/09",
        "publication-date": "2023/09",
        "abstract": "The veriﬁcation of model transformations is important for realizing robust model-driven engineering technologies and quality-assured automation. Many approaches for checking properties of model transformations have been proposed. Most of them have focused on the effective and efﬁcient detection of property violations by contract checking. However, there remains the fault localization step between identifying a failing contract for a transformation based on veriﬁcation feedback and precisely identifying the faulty rules. While there exist fault localization approaches in the model transformation veriﬁcation literature, these require the creation and maintenance of test cases, which imposes an additional burden on the developer. In this paper, we combine transformation veriﬁcation based on symbolic execution with spectrum-based fault localization techniques for identifying the faulty rules in DSLTrans model transformations. This fault localization approach operates on the path condition output of symbolic transformation checkers instead of requiring a set of test input models. In particular, we introduce a workﬂow for running the symbolic execution of a model transformation, evaluating the deﬁned contracts for satisfaction, and computing different measures for tracking the faulty rules. We evaluate the effectiveness of spectrum-based analysis techniques for tracking faulty rules and compare our approach to previous works. We evaluate our technique by introducing known mutations into ﬁve model transformations. Our results show that the best spectrum-based analysis techniques allow for effective fault localization, showing an average EXAM score below 0.30 (less than 30% of the transformation needs to be inspected). These techniques are also able to locate the faulty rule in the top-three ranked rules in 70% of all cases. The impact of the model transformation, the type of mutation and the type of contract on the results is discussed. Finally, we also investigate the cases where the technique does not work properly, including discussion of a potential pre-check to estimate the prospects of the technique for a certain transformation.",
        "keywords": [],
        "authors": [
            "Bentley James Oakes",
            "Javier Troya",
            "Jessie Galasso",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-023-01123-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "DSLTrans"
        }
    },
    {
        "title": "Grand challenges in model-driven engineering: an analysis of the state of the research",
        "submission-date": "2019/11",
        "publication-date": "2020/01",
        "abstract": "In 2017 and 2018, two events were held—in Marburg, Germany, and San Vigilio di Marebbe, Italy, respectively—focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (MDE). The events brought together experts from industry, academia, and the open-source community to assess what has changed in research in MDE over the last 10 years, what challenges remain, and what new challenges have arisen. This article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis. These challenges could lead to research initiatives for the community going forward.",
        "keywords": [
            "Model-driven engineering",
            "Grand challenge",
            "Research roadmap"
        ],
        "authors": [
            "Antonio Bucchiarone",
            "Jordi Cabot",
            "Richard F. Paige",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-019-00773-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to theme section on interplay of model-driven and component-based software engineering",
        "submission-date": "2020/06",
        "publication-date": "2020/07",
        "abstract": "This theme section aims to disseminate the latest trends in the use and combination of Model-Driven Engineering (MDE) and Component-Based Software Engineering (CBSE). On the one hand, MDE aims to increase productivity in the development of complex systems while reducing the time to market. On the other hand, CBSE aims to deliver and then support the exploitation of reusable “off-the-shelf” software components that can be incorporated into larger applications. An effective interplay of MDE and CBSE can yield benefits to both communities: the CBSE community would benefit from implementation and automation capabilities of MDE, the MDE community would benefit from the foundational nature of CBSE.",
        "keywords": [],
        "authors": [
            "Federico Ciccozzi",
            "Antonio Cicchetti",
            "Andreas Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-020-00812-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mining Frequent Structures in Conceptual Models",
        "submission-date": "2024/06",
        "publication-date": "2025/04",
        "abstract": "The challenge of using structured methods to represent knowledge is a well-documented issue in conceptual modeling and has been the focus of extensive research. It is widely recognized that adopting modeling patterns offers an effective structural approach for designing conceptual models. Patterns, in this context, refer to generalizable, recurring structures that provide solutions to common design problems. They signiﬁcantly enhance both the understanding and improvement of the modeling process. Numerous experimental studies have demonstrated the undeniable value of using patterns in conceptual modeling. Despite this, the task of identifying patterns in conceptual models remains highly complex, and there is currently no systematic method for pattern discovery. To address this gap, this paper proposes a general approach for discovering frequent structures in conceptual modeling languages as a means to support pattern identiﬁcation. Speciﬁcally, we focus on uncovering recurring structures that reﬂect the usage patterns of a given conceptual modeling language. As proof of concept, we implement our approach by focusing on two widely used conceptual modeling languages. This implementation includes an exploratory tool that integrates a frequent subgraph mining algorithm with graph manipulation techniques, such as graph visualization, graph clustering, and graph transformation. The tool processes multiple conceptual models and identiﬁes recurrent structures based on various criteria. We validate the tool using two state-of-the-art curated datasets: one consisting of models encoded in OntoUML and the other in ArchiMate. The primary objective of our approach is to provide a support tool for language engineers. This tool can be used to identify both effective and ineffective modeling practices, enabling the reﬁnement and evolution of conceptual modeling languages. Furthermore, it facilitates the reuse of accumulated expertise, ultimately supporting the creation of higher-quality models in a given language.",
        "keywords": [
            "Conceptual modeling",
            "Mining conceptual models",
            "Frequent subgraph mining",
            "Recurrent modeling structures",
            "Modeling patterns"
        ],
        "authors": [
            "Mattia Fumagalli",
            "Tiago Prince Sales",
            "Pedro Paulo F. Barcelos",
            "Giovanni Micale",
            "Philipp-Lorenz Glaser",
            "Dominik Bork",
            "Vadim Zaytsev",
            "Diego Calvanese",
            "Giancarlo Guizzardi"
        ],
        "file_path": "data/sosym-all/s10270-025-01295-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A fundamental approach to model versioning based on graph modiﬁcations: from theory to implementation",
        "submission-date": "2011/04",
        "publication-date": "2012/04",
        "abstract": "In model-driven engineering, models are primary artifacts that can evolve heavily during their life cycle. Therefore, versioning of models is a key technique to be offered by integrated development environments for model-driven engineering. In contrast to text-based versioning systems, we present an approach that takes model structures and their changes over time into account. Considering model structures as graphs, we deﬁne a fundamental approach where model revisions are considered as graph modiﬁcations consisting of delete and insert actions. Two different kinds of conﬂict detection are presented: (1) the check for operation-based conﬂicts between different graph modiﬁcations, and (2) the check for state-based conﬂicts on merged graph modiﬁcations. For the merging of graph modiﬁcations, a two-phase approach is proposed: First, operational conﬂicts are temporarily resolved by always giving insertion priority over deletion to keep as much information as possible. There- after, this tentative merge result is the basis for manual con- ﬂict resolution as well as for the application of repair actions that resolve state-based conﬂicts. If preferred by the user, giving deletion priority over insertion might be one solution. The fundamental concepts are illustrated by versioning sce- narios for simpliﬁed statecharts. Furthermore, we show an implementation of this fundamental approach to model ver- sioning based on the Eclipse Modeling Framework as tech- nical space.",
        "keywords": [
            "Model versioning",
            "Graph modiﬁcation",
            "Conﬂict detection",
            "Conﬂict resolution"
        ],
        "authors": [
            "Gabriele Taentzer",
            "Claudia Ermel",
            "Philip Langer",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-012-0248-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the theme issue on variability modeling of software-intensive systems",
        "submission-date": "2015/08",
        "publication-date": "2017/01",
        "abstract": "Variability in software-intensive systems (such as engine control or driver assistance systems in modern vehicles, ﬂight control systems in the aviation sector) is a major challenge when developing high quality systems efﬁciently and in short release cycles. New technologies and architectural paradigms will make the development and operation of variable-products even more difﬁcult and risky in future. This theme issue focuses broadly on innovative work in the area of variability modeling and management. We have particularly invited contributions with a strong variability modeling aspect, but also addressing the wider area of variability management, e.g., requirements, architecture, analysis, implementation, evolution and teaching of variability modeling. The purpose is to present new results for mastering variability throughout the whole lifecycle of systems, system families, and product lines.",
        "keywords": [],
        "authors": [
            "Andrzej W˛asowski",
            "Thorsten Weyer"
        ],
        "file_path": "data/sosym-all/s10270-015-0501-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A veriﬁed catalogue of OCL optimisations",
        "submission-date": "2019/02",
        "publication-date": "2019/07",
        "abstract": "OCL is widely used by model-driven engineering tools with different purposes like writing integrity constraints for meta-models, as a navigation language in model transformation languages or to deﬁne transformation speciﬁcations. Another scenario is the automatic generation of OCL code by a repair system. These generated expressions tend to be complex and unreadable due to the nature of the generative process. However, to be useful this code should be simple and resemble manually written code as much as possible when a developer must manually maintain it. There exists refactorings approaches for manually written OCL code, but there is no tool targeted to the optimisation of OCL expressions which have been automatically synthesised. Moreover, there is no available catalogue of OCL refactorings which can be integrated seamlessly into a tool. In this work, we contribute a set of refactorings intended to optimise OCL expressions, notably covering cases likely to arise in generated OCL code. We also contribute the implementation of these refactorings, built as a generic transformation catalogue using bent¯o, a transformation reuse tool for ATL. This makes it possible to specialise the catalogue for any OCL variant based on Ecore. Moreover, we propose a method to verify the correctness of the implemented catalogue based on translation validation and model ﬁnding. We describe the design and implementation of the catalogue and evaluate it by optimising a large amount of OCL expressions and proving the correctness of each optimisation execution. We also derive working implementations of the catalogue for ATL, EMF/OCL and SimpleOCL made available in a tool called BeautyOCL.",
        "keywords": [
            "Model transformations",
            "OCL",
            "Refactoring",
            "Veriﬁcation"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-019-00740-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL, OCL"
        }
    },
    {
        "title": "A unifying framework for homogeneous model composition",
        "submission-date": "2018/01",
        "publication-date": "2019/01",
        "abstract": "The growing use of models for separating concerns in complex systems has lead to a proliferation of model composition operators. These composition operators have traditionally been deﬁned from scratch following various approaches differing in formality, level of detail, chosen paradigm, and styles. Due to the lack of proper foundations for deﬁning model composition (concepts, abstractions, or frameworks), it is difﬁcult to compare or reuse composition operators. In this paper, we stipulate the existence of a unifying framework that reduces all structural composition operators to structural merging, and all composition operators acting on discrete behaviors to event scheduling. We provide convincing evidence of this hypothesis by discussing how structural and behavioral homogeneous model composition operators (i.e., weavers) can be mapped onto this framework. Based on this discussion, we propose a conceptual model of the framework and identify a set of research challenges, which, if addressed, lead to the realization of this framework to support rigorous and efﬁcient engineering of model composition operators for homogeneous and eventually heterogeneous modeling languages.",
        "keywords": [
            "Model composition",
            "Symmetric merge",
            "Event scheduling",
            "Event structures",
            "Separation of concerns"
        ],
        "authors": [
            "Jörg Kienzle",
            "Gunter Mussbacher",
            "Benoit Combemale",
            "Julien Deantoni"
        ],
        "file_path": "data/sosym-all/s10270-018-00707-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Claimed advantages and disadvantages of (dedicated) model transformation languages: a systematic literature review",
        "submission-date": "2019/11",
        "publication-date": "2020/07",
        "abstract": "There exists a plethora of claims about the advantages and disadvantages of model transformation languages compared to general-purpose programming languages. With this work, we aim to create an overview over these claims in the literature and systematize evidence thereof. For this purpose, we conducted a systematic literature review by following a systematic process for searching and selecting relevant publications and extracting data. We selected a total of 58 publications, categorized claims about model transformation languages into 14 separate groups and conceived a representation to track claims and evidence through the literature. From our results, we conclude that: (i) the current literature claims many advantages of model transformation languages but also points towards certain deﬁcits and (ii) there is insufﬁcient evidence for claimed advantages and disadvantages and (iii) there is a lack of research interest into the veriﬁcation of claims.",
        "keywords": [
            "Model transformation language",
            "DSL",
            "Model transformation",
            "MDSE",
            "Advantages",
            "Disadvantages"
        ],
        "authors": [
            "Stefan Götz",
            "Matthias Tichy",
            "Raﬀaela Groner"
        ],
        "file_path": "data/sosym-all/s10270-020-00815-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design and automation of a COSMIC measurement procedure based on UML models",
        "submission-date": "2018/01",
        "publication-date": "2019/04",
        "abstract": "Context. Many organizations are adopting the COSMIC method to size software products for estimating and controlling their development costs and performances. Using a functional size measurement method requires specialized expertise and can be time-consuming. Objectives. Since UML is the de facto industrial modeling language standard for object-oriented systems, it is very useful to understand how to exploit UML models for measuring software systems and for developing tools that can automatically derive the COSMIC size from them. This paper provides an answer to these needs. Method. We present a measurement procedure to derive the COSMIC functional size from UML software artifacts and a tool, named J-UML COSMIC, for the automation of the procedure. Based on the observation that different development processes are characterized by the use of different UML models, the tool has been designed to work with different UML artifacts (such as use case models, package diagrams, component diagrams, class diagrams, activity diagrams, and sequence diagrams) and to adapt to the speciﬁc employed process. To assess the measurement procedure and J-UML COSMIC, we have carried out two case studies and compared the measurement results provided by the tool with the ones obtained by experts applying the standard COSMIC method. Results. Using the proposed measurement procedure the tool is able to identify from UML software models all the COSMIC concepts and data movements identiﬁed by the experts. Moreover, the tool allows us to obtain incremental accurate measurements when new models are considered or existing ones are detailed. Conclusions. The designed approach is able to automatically measure the functional size starting from UML artifacts and providing higher accurate results when more data is available.",
        "keywords": [
            "Functional size measurement",
            "Automation tool",
            "COSMIC-ISO 19761",
            "Uniﬁed modeling language"
        ],
        "authors": [
            "Gabriele De Vito",
            "Filomena Ferrucci",
            "Carmine Gravino"
        ],
        "file_path": "data/sosym-all/s10270-019-00731-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting the internet-based evaluation of research software with cloud infrastructure",
        "submission-date": "2009/10",
        "publication-date": "2010/05",
        "abstract": "Due to license restrictions and installation issues, it is often not feasible to experiment with software without making substantial investments. Especially in the case of legacy tools, it turns out that even free software is often too costly (i.e., time-consuming) to be installed for evaluating the quality of a research contribution. After organizing a series of events related to software modeling, we have constructed (and started to use) SHARE, a system for sharing practically any type of software artifact to reviewers and to other participants who have very limited time available. The system relies on cloud-computing technologies to provide online access to interactive environments containing all the tools, documentation, input and output models to reproduce alleged research results. The system also enables one to clone such an environment and add additional models or tools in order to extend a contribution or pinpoint a problem. In retrospect, we observe that the approach is not limited to software modeling and SHARE is in fact gaining acceptance in other ﬁelds already.",
        "keywords": [
            "Reproducible research",
            "Model transformation",
            "Tool contest",
            "Peer review",
            "Cloud computing"
        ],
        "authors": [
            "Pieter Van Gorp",
            "Paul Grefen"
        ],
        "file_path": "data/sosym-all/s10270-010-0163-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Opportunities in intelligent modeling assistance",
        "submission-date": "2020/06",
        "publication-date": "2020/07",
        "abstract": "Modeling is requiring increasingly larger efforts while becoming indispensable given the complexity of the problems we are solving. Modelers face high cognitive load to understand a multitude of complex abstractions and their relationships. There is an urgent need to better support tool builders to ultimately provide modelers with intelligent modeling assistance that learns from previous modeling experiences, automatically derives modeling knowledge, and provides context-aware assistance. However, current intelligent modeling assistants (IMAs) lack adaptability and ﬂexibility for tool builders, and do not facilitate understanding the differences and commonalities of IMAs for modelers. Such a patchwork of limited IMAs is a lost opportunity to provide modelers with better support for the creative and rigorous aspects of software engineering. In this expert voice, we present a conceptual reference framework (RF-IMA) and its properties to identify the foundations for intelligent modeling assistance. For tool builders, RF-IMA aims to help build IMAs more systematically. For modelers, RF-IMA aims to facilitate comprehension, comparison, and integration of IMAs, and ultimately to provide more intelligent support. We envision a momentum in the modeling community that leads to the implementation of RF-IMA and consequently future IMAs. We identify open challenges that need to be addressed to realize the opportunities provided by intelligent modeling assistance.",
        "keywords": [
            "Model-based software engineering",
            "Intelligent modeling assistance",
            "Integrated development environment",
            "Artiﬁcial intelligence",
            "Development data",
            "Feedback"
        ],
        "authors": [
            "Gunter Mussbacher",
            "Benoit Combemale",
            "Jörg Kienzle",
            "Silvia Abrahão",
            "Hyacinth Ali",
            "Nelly Bencomo",
            "Márton Búr",
            "Loli Burgueño",
            "Gregor Engels",
            "Pierre Jeanjean",
            "Jean-Marc Jézéquel",
            "Thomas Kühn",
            "Sébastien Mosser",
            "Houari Sahraoui",
            "Eugene Syriani",
            "Dániel Varró",
            "Martin Weyssow"
        ],
        "file_path": "data/sosym-all/s10270-020-00814-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reasoning over time into models with DataTime",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Models at runtime have been initially investigated for adaptive systems. Models are used as a reﬂective layer of the current state of the system to support the implementation of a feedback loop. More recently, models at runtime have also been identified as key for supporting the development of full-ﬂedged digital twins. However, this use of models at runtime raises new challenges, such as the ability to seamlessly interact with the past, present, and future states of the system. In this paper, we propose a framework called DataTime to implement models at runtime which capture the state of the system according to the dimensions of both time and space, here modeled as a directed graph where both nodes and edges bear local states (i.e., values of properties of interest). DataTime offers a unifying interface to query the past, present, and future (predicted) states of the system. This unifying interface provides (i) an optimized structure of the time series that capture the past states of the system, possibly evolving over time, (ii) the ability to get the last available value provided by the system’s sensors, and (iii) a continuous micro-learning over graph edges of a predictive model to make it possible to query future states, either locally or more globally, thanks to a composition law. The framework has been developed and evaluated in the context of the Intelligent Public Transportation Systems of the city of Rennes (France). This experimentation has demonstrated how DataTime can be used for managing data from the past, the present, and the future and facilitate the development of digital twins.",
        "keywords": [
            "Models at runtime",
            "Digital twins",
            "Data analysis",
            "Intelligent Public Transportation Systems"
        ],
        "authors": [
            "Gauthier Lyan",
            "Jean-Marc Jézéquel",
            "David Gross-Amblard",
            "Romain Lefeuvre",
            "Benoit Combemale"
        ],
        "file_path": "data/sosym-all/s10270-023-01080-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Composable partial multiparty session types for open systems",
        "submission-date": "2022/03",
        "publication-date": "2022/09",
        "abstract": "Session types are a well-established framework for the speciﬁcation of interactions between components of a distributed systems. An important issue is how to determine the type for an open system, i.e., obtained by assembling subcomponents, some of which could be missing. To this end, we introduce partial sessions and partial (multiparty) session types. Partial sessions can be composed, and the type of the resulting system is derived from those of its components without knowing any suitable global type nor the types of missing parts. To deal with this incomplete information, partial session types represent the subjective views of the interactions from participants’ perspectives; when sessions are composed, different partial views can be merged if compatible, yielding a uniﬁed view of the session. Incompatible types, due to, e.g., miscommunications or deadlocks, are detected at the merging phase. In fact, in this theory the distinction between global and local types vanishes. We apply these types to a process calculus for which we prove subject reduction and progress, so that well-typed systems never violate the prescribed constraints. In particular, we introduce a generalization of the progress property, in order to accommodate the case when a partial session cannot progress not due to a deadlock, but because some participants are still missing. Therefore, partial session types support the development of systems by incremental assembling of components.",
        "keywords": [
            "Multiparty session types",
            "Process algebras",
            "Open systems"
        ],
        "authors": [
            "Claude Stolze",
            "Marino Miculan",
            "Pietro Di Gianantonio"
        ],
        "file_path": "data/sosym-all/s10270-022-01040-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CMMN evaluation: the modelers’ perceptions of the main notation elements",
        "submission-date": "2020/04",
        "publication-date": "2021/05",
        "abstract": "Case Management Model and Notation (CMMN) has been introduced as a graphical modeling language targeting the modeling of human-centric processes. Despite its growing reputation since 2016, when the OMG standant was released, the usage and the adoption potential of CMMN is not yet evaluated. The goal of this paper is to evaluate CMMN language and the contribution of its main notation elements to its future adoption, based on the experience of modelers. A CMMN workshop was conducted, where groups of modelers modeled two different human-centric, real-world processes with CMMN. The effectiveness and efﬁciency of the language and modelers’ usage experience were evaluated. Their perception of the role of the CMMN notation elements to their future adoption CMMN have been recorded through a survey. A multi-criteria decision making method (Analytic Hierarchy Process–AHP) was utilized for analyzing the answers and generating the results. The evaluation results showed that CMMN language could be adopted for modeling non-structural processes and the study participants showed a positive attitude towards adopting CMMN driven by the fact that they overall perceived it as useful. To the best of our knowledge, this is the ﬁrst attempt to evaluate CMMN language’s usability and prospects of adoption. Moreover, this is the first empirical study that explores the syntax of a process modeling language and its effect on its usage and adoption. Overall, since interest in CMMN is increasing, this work could inspire future researchers and practitioners to further explore the CMMN usage and adoption potential.",
        "keywords": [
            "CMMN",
            "Human-centric processes",
            "Modeling language evaluation",
            "AHP method"
        ],
        "authors": [
            "Ioannis Routis",
            "Cleopatra Bardaki",
            "Georgia Dede",
            "Mara Nikolaidou",
            "Thomas Kamalakis",
            "Dimosthenis Anagnostopoulos"
        ],
        "file_path": "data/sosym-all/s10270-021-00880-3.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A framework for embedded software portability and veriﬁcation: from formal models to low-level code",
        "submission-date": "2022/06",
        "publication-date": "2024/02",
        "abstract": "Porting software to new target architectures is a common challenge, particularly when dealing with low-level functionality in drivers or OS kernels that interact directly with hardware. Traditionally, adapting code for different hardware platforms has been a manual and error-prone process. However, with the growing demand for dependability and the increasing hardware diversity in systems like the IoT, new software development approaches are essential. This includes rigorous methods for verifying and automatically porting Real-Time Operating Systems (RTOS) to various devices. Our framework addresses this challenge through formal methods and code generation for embedded RTOS. We demonstrate a hardware-speciﬁc part of a kernel model in Event-B, ensuring correctness according to the speciﬁcation. Since hardware details are only added in late modeling stages, we can reuse most of the model and proofs for multiple targets. In a proof of concept, we reﬁne the generic model for two different architectures, also ensuring safety and liveness properties. We then showcase automatic low-level code generation from the model. Finally, a hardware-independent factorial function model illustrates more potential of our approach.",
        "keywords": [
            "Embedded systems",
            "RTOS",
            "Formal methods",
            "Event-B",
            "Veriﬁcation",
            "Code generation",
            "Portability"
        ],
        "authors": [
            "Renata Martins Gomes",
            "Bernhard Aichernig",
            "Marcel Baunach"
        ],
        "file_path": "data/sosym-all/s10270-023-01144-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Efﬁcient analysis of pattern-based constraint speciﬁcations",
        "submission-date": "2008/06",
        "publication-date": "2009/08",
        "abstract": "Precision and consistency are important prereq-uisites for class models to conform to their intended domain semantics. Precision can be achieved by augmenting models with design constraints and consistency can be achieved by avoiding contradictory constraints. However, there are differ-ent views of what constitutes a contradiction for design constraints. Moreover, state-of-the-art analysis approaches for proving constrained models consistent either scale poorly or require the use of interactive theorem proving. In this paper, we present a heuristic approach for efﬁciently analyz-ing constraint speciﬁcations built from constraint patterns. This analysis is based on precise notions of consistency for constrained class models and exploits the semantic properties of constraint patterns, thereby enabling syntax-based consis-tency checking in polynomial-time. We introduce a consis-tencycheckerimplementingtheseideasandwereportoncase studies in applying our approach to analyze industrial-scale models. These studies show that pattern-based constraint development supports the creation of concise speciﬁcations and provides immediate feedback on model consistency.",
        "keywords": [
            "UML",
            "OCL",
            "Constraints",
            "Patterns",
            "Consistency"
        ],
        "authors": [
            "Michael Wahler",
            "David Basin",
            "Achim D. Brucker",
            "Jana Koehler"
        ],
        "file_path": "data/sosym-all/s10270-009-0123-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based intelligent user interface adaptation: challenges and future directions",
        "submission-date": "2021/05",
        "publication-date": "2021/07",
        "abstract": "Adapting the user interface of a software system to the requirements of the context of use continues to be a major challenge, particularly when users become more demanding in terms of adaptation quality. A considerable number of methods have, over the past three decades, provided some form of modelling with which to support user interface adaptation. There is, however, a crucial issue as regards in analysing the concepts, the underlying knowledge, and the user experience afforded by these methods as regards comparing their beneﬁts and shortcomings. These methods are so numerous that positioning a new method in the state of the art is challenging. This paper, therefore, deﬁnes a conceptual reference framework for intelligent user interface adaptation containing a set of conceptual adaptation properties that are useful for model-based user interface adaptation. The objective of this set of properties is to understand any method, to compare various methods and to generate new ideas for adaptation. We also analyse the opportunities that machine learning techniques could provide for data processing and analysis in this context, and identify some open challenges in order to guarantee an appropriate user experience for end-users. The relevant literature and our experience in research and industrial collaboration have been used as the basis on which to propose future directions in which these challenges can be addressed.",
        "keywords": [
            "Context of use",
            "Intelligent user interface",
            "Machine learning",
            "Model-based software engineering",
            "Model-driven engineering",
            "User interface adaptation",
            "Conceptual reference framework"
        ],
        "authors": [
            "Silvia Abrahão",
            "Emilio Insfran",
            "Arthur Sluÿters",
            "Jean Vanderdonckt"
        ],
        "file_path": "data/sosym-all/s10270-021-00909-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards high-level fuzzy control speciﬁcations for building automation systems",
        "submission-date": "2018/09",
        "publication-date": "2019/09",
        "abstract": "The control logic underlying building automation systems has consisted, traditionally, of embedded discrete programs created\nusing either low-level or proprietary scripting languages, or using general purpose fourth-generation visual languages like\nSimulink. It is also well known that programs developed in this way are hard to evolve, test, and maintain. These difﬁculties\nare intensiﬁed when continuous control problems have to be tackled or when the actuation must vary continually subject\nto the sensor inputs. Such is the case in day-lighting or occupancy-based control applications. In this paper, we propose a\ndeclarative high-level Domain-Speciﬁc Language that aims to reduce the effort required to specify the control logic of building\nautomation systems. Our language combines fuzzy logic and temporal logic, enabling to deﬁne the behaviour in terms of\ndomain abstractions. Finally, the approach has been validated in two ways: (i) in a case study that simulates the control system\nof an automated ofﬁce room and (ii) by means of an empirical study to conﬁrm usability (with a System Usability Scale\nquestionnaire) and effectiveness, here regarded from the perspective of correctness, of the proposed language with respect to\na well-known language like Simulink.",
        "keywords": [
            "Ambient intelligence",
            "Context-aware systems",
            "Building automation",
            "Fuzzy control systems",
            "Domain-Speciﬁc Languages"
        ],
        "authors": [
            "Juan C. Vidal",
            "Paulo Carreira",
            "Vasco Amaral",
            "Joao Aguiam",
            "João Sousa"
        ],
        "file_path": "data/sosym-all/s10270-019-00755-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward testing from ﬁnite state machines with symbolic inputs and outputs",
        "submission-date": "2016/09",
        "publication-date": "2017/08",
        "abstract": "After 60 or so years of development, the theory of checking experiments for FSM still continues to attract a lot of attention of research community. One of the reasons is that it offers test generation techniques which under well-deﬁned assumptions guarantee complete fault coverage for a given fault model of a speciﬁcation FSM. Checking experiments have already been extended to remove assumptions that the speciﬁcation Mealy machine need to be reduced, deterministic, and completely speciﬁed, while keeping the input, output and state sets ﬁnite. In our recent work, we investigated possibilities of removing the assumption about the ﬁniteness of the input set, introducing the model FSM with symbolic inputs. In this paper, we report the results that further lift the theory of checking experiments for Mealy machines with symbolic inputs and symbolic outputs. The former are predicates deﬁned over input variables and the latter are output variable valuations computed by assignments on input variables. Both types of variables can have large or even inﬁnite domains. Inclusion of assignments in the model complicates even output fault detection, as different assignments may produce the same output valuations for some input valuations. We address this issue by using a transition cover composed of symbolic inputs on which the assignments produce different outputs. The enhanced transition cover is then used in checking experiments, which detect assignment/output faults and more general transition faults under certain assumptions.",
        "keywords": [
            "Finite state machines",
            "Extended ﬁnite state machines",
            "Symbolic automata",
            "Conformance testing",
            "Checking experiments",
            "Fault model-based test generation"
        ],
        "authors": [
            "Alexandre Petrenko"
        ],
        "file_path": "data/sosym-all/s10270-017-0613-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An interdisciplinary comparison of sequence modeling methods for next-element prediction",
        "submission-date": "2018/10",
        "publication-date": "2020/04",
        "abstract": "Data of sequential nature arise in many application domains in the form of, e.g., textual data, DNA sequences, and software execution traces. Different research disciplines have developed methods to learn sequence models from such datasets: (i) In the machine learning ﬁeld methods such as (hidden) Markov models and recurrent neural networks have been developed and successfully applied to a wide range of tasks, (ii) in process mining process discovery methods aim to generate human-interpretable descriptive models, and (iii) in the grammar inference ﬁeld the focus is on ﬁnding descriptive models in the form of formal grammars. Despite their different focuses, these ﬁelds share a common goal: learning a model that accurately captures the sequential behavior in the underlying data. Those sequence models are generative, i.e., they are able to predict what elements are likely to occur after a given incomplete sequence. So far, these ﬁelds have developed mainly in isolation from each other and no comparison exists. This paper presents an interdisciplinary experimental evaluation that compares sequence modeling methods on the task of next-element prediction on four real-life sequence datasets. The results indicate that machine learning methods, which generally do not aim at model interpretability, tend to outperform methods from the process mining and grammar inference ﬁelds in terms of accuracy.",
        "keywords": [
            "Process mining",
            "Machine learning",
            "Grammar inference",
            "Sequence prediction"
        ],
        "authors": [
            "Niek Tax",
            "Irene Teinemaa",
            "Sebastiaan J. van Zelst"
        ],
        "file_path": "data/sosym-all/s10270-020-00789-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing secure business process design with security process patterns",
        "submission-date": "2017/11",
        "publication-date": "2019/07",
        "abstract": "Business process deﬁnition and analysis are an important activity for any organisation. As research has demonstrated, well-deﬁned business processes can reduce cost, improve productivity and provide organisations with competitive advantages. In the last few years, the need to ensure the security of business processes has been identiﬁed as a major research challenge. Limited security expertise of business process developers together with a clear lack of appropriate methods and techniques to support the security analysis of business processes is important prohibitors to providing answers to that research challenge. This paper introduces the ﬁrst attempt in the literature to produce a novel pattern-based approach to support the design and analysis of secure business processes. Our work draws on elements from the security requirements engineering area and the security patterns area, combined with business process modelling, and it produces a set of process-level security patterns which are used to implement security in a given business process model. Such an approach advances the existing literature by providing a structured way of operationalising security at the business process level of abstraction. The applicability of the work is illustrated through an application to a real-life information system, and the effectiveness and usability of the work are evaluated via a workshop-based experiment. The evaluation clearly indicates that non-experts are able to comprehend and utilise the developed patterns to construct secure business process designs.",
        "keywords": [
            "Security requirements engineering",
            "Business process modelling",
            "Security process patterns",
            "Business process security"
        ],
        "authors": [
            "Nikolaos Argyropoulos",
            "Haralambos Mouratidis",
            "Andrew Fish"
        ],
        "file_path": "data/sosym-all/s10270-019-00743-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on SEFM 2009",
        "submission-date": "2012/03",
        "publication-date": "Not found",
        "abstract": "This guest editorial introduces the special section focusing on the Seventh IEEE International Conference on Software Engineering and Formal Methods (SEFM 2009). It discusses the importance of formal methods in software engineering, the scope of the special section, and the selection process of the included papers. The editorial highlights the integration of software engineering techniques with formal methods and aims to appeal to both researchers and postgraduate students.",
        "keywords": [],
        "authors": [
            "Padmanabhan Krishnan",
            "Dang Van Hung",
            "Antonio Cerone"
        ],
        "file_path": "data/sosym-all/s10270-012-0238-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Emerging OCL tools",
        "submission-date": "2002/06",
        "publication-date": "2003/10",
        "abstract": "The Object Constraint Language (OCL) is\na notational language for analysis and design of software\nsystems, which is used in conjunction with the Uniﬁed\nModelling Language (UML) to specify the semantics of\nthe building blocks precisely. OCL can also be used by\nother languages, notations, methods and software tools\nin order to specify restrictions and other expressions of\ntheir models. Likewise, OCL is used by the Object Man-\nagement Group (OMG) in the deﬁnition of other fast\nspreading industrial standards such as Meta Object Facil-\nity (MOF) or XML Metadata Interchange (XMI).\nSupport tools aimed at making this language easier\nto use are becoming available. These tools are capable\nof supporting and handling OCL expressions. This paper\npresents a comparative study of the main tools currently\navailable, both commercial and freely available ones. The\nstudy is very practical, with the advantages and disad-\nvantages of the diﬀerent tools being pointed out. The\nevaluations made may be of use in helping those develop-\ners and analysts who already use the language, as well as\nthose who intend to use it in the near future, to choose the\nOCL tool which best adapts to their requirements.",
        "keywords": [
            "Object Constraint Language",
            "OCL tools",
            "Analysis",
            "Comparison"
        ],
        "authors": [
            "Ambrosio Toval",
            "V´ıctor Requena",
            "Jos´e Luis Fern´andez"
        ],
        "file_path": "data/sosym-all/s10270-003-0031-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EDITORIAL",
        "submission-date": "2003/10",
        "publication-date": "2005/09",
        "abstract": "The UML series of conferences began in 1998, in the early days of the Uniﬁed Modelling Language (UML). Since then, both the language and its community have changed out of all recognition. The application of UML has been much wider than was originally envisaged by most of those involved. It has spawned an extraordinary number of discussions, tools, books, variants, and not least, academic papers. UML itself is, at the time of writing, about to complete the upgrade to UML2.0. At the time of the conference to which this special issue refers, the revision was in progress; some papers in this special issue refer to UML2.0, others to UML1.x. However, many of the most interesting developments in the ﬁeld are not speciﬁc to UML at all. This is reﬂected in the new change to the name of the conference: from 2005, it will be known as MoDELS (Model Drivel Engineering, Languages and Systems), not as UML. Several of the papers in this special issue already make the point that their work applies outside UML.",
        "keywords": [],
        "authors": [
            "Perdita Stevens",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-005-0086-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Recommender systems in model-driven engineering",
        "submission-date": "2020/12",
        "publication-date": "2021/07",
        "abstract": "Recommender systems are information ﬁltering systems used in many online applications like music and video broadcasting and e-commerce platforms. They are also increasingly being applied to facilitate software engineering activities. Following this trend, we are witnessing a growing research interest on recommendation approaches that assist with modelling tasks and model-based development processes. In this paper, we report on a systematic mapping review (based on the analysis of 66 papers) that classiﬁes the existing research work on recommender systems for model-driven engineering (MDE). This study aims to serve as a guide for tool builders and researchers in understanding the MDE tasks that might be subject to recommendations, the applicable recommendation techniques and evaluation methods, and the open challenges and opportunities in this ﬁeld of research.",
        "keywords": [
            "Model-driven engineering",
            "Recommender systems",
            "Systematic mapping review"
        ],
        "authors": [
            "Lissette Almonte",
            "Esther Guerra",
            "Iván Cantador",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-021-00905-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SustainScrum: integrating sustainability assessment in a tailored Scrum process for computing quantitative sustainability indicators",
        "submission-date": "2024/03",
        "publication-date": "2025/02",
        "abstract": "In the wake of a rapidly growing global focus on sustainable practices, the year 2023 marked a signiﬁcant regulatory mile-\nstone, with the enactment of the Corporate Sustainability Reporting Directive. This directive signiﬁcantly expands the scope\nof sustainability reporting, encompassing a broader array of enterprises, including large-scale, and small- and medium-sized\nEnterprises within the EU. This regulatory development has profound implications for the software industry, as these com-\npanies need to provide comprehensive sustainability reporting for their software products. However, the industry still lacks\nmodels, tools, and methodologies for quantitatively assessing sustainability indicators during an agile software develop-\nment process. In this paper, we introduce SustainScrum, a customized Scrum process model that incorporates sustainability\nevaluations into the development lifecycle of software products. More precisely, SustainScrum integrates the assessment of\nsustainability aspects into the backlog, user story management, and development stages of the Scrum process. Its primary\nobjective is to ensure that sustainability considerations are systematically captured, evaluated, and addressed throughout the\ndevelopment process. It integrates the computation of quantitative sustainability indicators thereby advancing the ability to\naddress sustainability challenges within software engineering practices. We performed an initial validation, investigating the\napplicability of SustainScrum on an open-source, publicly available requirements data set for agile development.",
        "keywords": [
            "Sustainability",
            "Agile development",
            "Process model",
            "Scrum"
        ],
        "authors": [
            "Alexandra Mazak-Huemer",
            "Michael Vierhauser",
            "Iris Groher"
        ],
        "file_path": "data/sosym-all/s10270-025-01266-5.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "UML speciﬁcation of access control policies and their formal veriﬁcation",
        "submission-date": "2004/01",
        "publication-date": "2006/10",
        "abstract": "Security requirements have become an integral part of most modern software systems. In order to produce secure systems, it is necessary to provide software engineers with the appropriate systematic support. We propose a methodology to integrate the speciﬁcation of access control policies into Uniﬁed Modeling Language (UML) and provide a graph-based formal semantics for theUMLaccess control speciﬁcationwhich permits to reason about the coherence of the access control speciﬁcation. The main concepts in the UML access control speciﬁcation are illustrated with an example access control model for distributed object systems.",
        "keywords": [],
        "authors": [
            "Manuel Koch",
            "Francesco Parisi-Presicce"
        ],
        "file_path": "data/sosym-all/s10270-006-0030-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework for automated multi-stage and multi-step product configuration of cyber-physical systems",
        "submission-date": "2019/08",
        "publication-date": "2020/06",
        "abstract": "Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabili-ties and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.",
        "keywords": [
            "Cyber-physical systems",
            "Product line engineering",
            "Automated configuration",
            "Multi-stage and multi-step configuration process",
            "Constraint classification",
            "Variability modeling",
            "Real-world case studies"
        ],
        "authors": [
            "Safdar Aqeel Safdar",
            "Hong Lu",
            "Tao Yue",
            "Shaukat Ali",
            "Kunming Nie"
        ],
        "file_path": "data/sosym-all/s10270-020-00803-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model synchronization based on triple graph grammars: correctness, completeness and invertibility",
        "submission-date": "2012/04",
        "publication-date": "2013/01",
        "abstract": "Triple graph grammars (TGGs) have been used successfully to analyze correctness and completeness of bidirectional model transformations, but a corresponding formal approach to model synchronization has been missing. This paper closes this gap by providing a formal synchronization framework with bidirectional update propagation operations. TheyaregeneratedfromagivenTGG, whichspeciﬁes the language of all consistently integrated source and target models. As our main result, we show that the generated synchronization framework is correct and complete, provided that forward and backward propagation operations are deterministic. Correctness essentially means that the propagation operations preserve and establish consistency while completeness ensures that the operations are deﬁned for all possible inputs. Moreover, we analyze the conditions under which the operations are inverse to each other. All constructions and results are motivated and explained by a running example,",
        "keywords": [
            "Model synchronization",
            "Correctness",
            "Bidirectional model transformation",
            "Triple graph grammars"
        ],
        "authors": [
            "Frank Hermann",
            "Hartmut Ehrig",
            "Fernando Orejas",
            "Krzysztof Czarnecki",
            "Zinovy Diskin",
            "Yingfei Xiong",
            "Susann Gottmann",
            "Thomas Engel"
        ],
        "file_path": "data/sosym-all/s10270-012-0309-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model template for reachability-based containment checking of imprecise observations in timed automata",
        "submission-date": "2023/07",
        "publication-date": "2024/09",
        "abstract": "Verifying safety requirements by model checking becomes increasingly important for safety-critical applications. For the validity of such proof in practice, the model needs to capture the actual behavior of the real system, which could be tested by containment checks of real observation traces. Basic equivalence checks, however, are not applicable if the system is only partially or imprecisely observable, if the model abstracts from explicit states with symbolic semantics, or if the checks are not expressible in the logics supported by a model checker. In this article, we solve the problem of observation containment checking in timed automata via reachability checking on tester systems. We introduce the logic SRL (sequence reachability logic) to express observations as sequences of delayed reachability properties. Through SBLL (introduced by Aceto et al.) as intermediate logic, we synthesize a set of matcher model templates for partial and imprecise observations and further extend these templates for the case of limited state accessibility in a model. For the obtained matching traces, we deﬁne the back-transformation into the original model domain and formally prove the correctness of the transformation. We implemented the observation matching approach, and apply it to a set of 7 demo and 3 case study models with different levels of observability. The results show that all positive and negative observations are correctly classiﬁed, and that the most advanced matcher model instance still offers average run times between 0.1 and 1s in all but 3 scenarios.",
        "keywords": [
            "Containment checking",
            "Reachability problem",
            "Model transformation",
            "Timed automata",
            "Observation matching"
        ],
        "authors": [
            "Sascha Lehmann",
            "Sibylle Schupp"
        ],
        "file_path": "data/sosym-all/s10270-024-01205-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Empirical study on the effectiveness and efﬁciency of model-driven architecture techniques",
        "submission-date": "2018/02",
        "publication-date": "2019/01",
        "abstract": "Previous studies have reported conﬂicting opinions on the feasibility of model-driven architecture (MDA). Studies have investigated the mechanics of MDA, but few have examined its effectiveness and efﬁciency from a developer’s perception. This study conducted empirical research in which a system was implemented by subjects using MDA; afterward, evaluated its perceived efﬁciency and effectiveness. In the model construction phase, Uniﬁed Modeling Language and Object Constraint Language were perceived as effective and efﬁcient. In the model transformation phase, the query/view/transformation standard was perceived as marginally efﬁcient rather than effective, and the round-trip engineering phase was not perceived as effective or efﬁcient. These ﬁndings are explained using 12 themes identiﬁed in subjects’ opinions. This study may help scholars understand the importance of efﬁciency and effectiveness on MDA techniques and facilitate the development of more acceptable and practical MDA.",
        "keywords": [
            "Model-driven architecture",
            "Effectiveness",
            "Efﬁciency",
            "Model transformation",
            "Uniﬁed Modeling Language",
            "Query/view/transformation"
        ],
        "authors": [
            "Shin-Shing Shin"
        ],
        "file_path": "data/sosym-all/s10270-018-00711-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Query/view/transformation"
        }
    },
    {
        "title": "SoSyM special section on service-based software engineering",
        "submission-date": "2005/12",
        "publication-date": "2006/04",
        "abstract": "Services offer the possibility to describe the functionality of a system exceeding the limited modularization supplied by component-based approaches. For that reason, service-based systems engineering has traditionally proven useful in the development of telecommunication systems, by separating individual services – or features – with high degree of interaction between the components of the system and describing the complete behavior of the system when combined. Due to their modular character, services are especially suited for the development of configurable or extendable systems, supporting product lines as well as incremental development. Thus, beyond telecommunication, especially the notion of web services has received a lot of interest as a means for publishing and accessing software functions via web standards to yield flexibly configurable systems. For similar reasons, increasingly the notion of a service is also gaining ground in other application domains, such as spontaneous networks, ubiquitous computing, and even safety critical systems from the automotive or avionics domain. Precise specification and correct implementation of requirements for services are essential in most of these application domains. Many notions of service refer only to syntactic interfaces. Typical examples include the service notions inherent in WSDL/SOAP and technologies such as .NET/J2EE building on these service notions. This is inadequate for more elaborate service specifications that include, for instance, quality-of-service properties. In many approaches, services are not treated as first-class modeling elements, say, in UML/UML-RT and SDL. Therefore, especially in the application domains mentioned above, a suitable notion of service is needed to support service-based software engineering beyond simplistic syntactic approaches.",
        "keywords": [
            "Service",
            "Semantics",
            "Model",
            "Description",
            "Architecture",
            "Composition"
        ],
        "authors": [
            "Manfred Broy",
            "Heinrich Hussmann",
            "Ingolf H. Kr¨uger",
            "Bernhard Sch¨atz"
        ],
        "file_path": "data/sosym-all/s10270-006-0002-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial",
        "submission-date": "2007/08",
        "publication-date": "2007/10",
        "abstract": "This edition and the next of “Software and Systems Modeling” contain eight papers discussing the use of models in software and systems development. These articles are based on papers presented at MODELS 2005, the eighth international conference on Model Driven Engineering Languages and Systems held in Montego Bay, Jamaica, 2–7 October 2005. This was the inaugural edition of MODELS, which is the follow-on conference series to the highly popular UML series of conferences. MODELS represents the broadening of the UML series to include topics such as languages, methods, and systems relevant to the development and use of complex systems.",
        "keywords": [],
        "authors": [
            "Lionel Briand",
            "Geri Georg"
        ],
        "file_path": "data/sosym-all/s10270-007-0067-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Software Language Extension Problem",
        "submission-date": "2019/09",
        "publication-date": "2019/12",
        "abstract": "The problem of software language extension and composition drives much of the research in Software Language Engineering (SLE). Although various solutions have already been proposed, there is still little understanding of the speciﬁc ins and outs of this problem, which hinders the comparison and evaluation of existing solutionsIn this SoSyM Expert Voice, we introduce the Language Extension Problem as a way to better qualify the scope of the challenges related to language extension and compositionThe formulation of the problem is similar to the seminal Expression Problem introduced by Wadler in the late 1990s and lifts it from the extensibility of single constructs to the extensibility of groups of constructs, i.e., software languages. We provide a comprehensive deﬁnition of the actual constraints when considering language extension and believe the Language Extension Problem will drive future research in SLE, the same way the original Expression Problem helped to understand the strengths and weaknesses of programming languages and drove much research in programming languages.",
        "keywords": [
            "Domain-speciﬁc language",
            "Extension",
            "Composition",
            "Expression problem"
        ],
        "authors": [
            "Manuel Leduc",
            "Thomas Degueule",
            "Eric Van Wyk",
            "Benoit Combemale"
        ],
        "file_path": "data/sosym-all/s10270-019-00772-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML/OCL framework for the analysis of graph transformation rules",
        "submission-date": "2008/11",
        "publication-date": "2009/08",
        "abstract": "In this paper we present an approach for the analysis of graph transformation rules based on an intermediate OCL representation. We translate different rule semantics into OCL, together with the properties of interest (like rule applicability, conﬂicts or independence). The intermediate representation serves three purposes: (1) it allows the seamless integration of graph transformation rules with the MOF and OCL standards, and enables taking the meta-model and its OCL constraints (i.e. well-formedness rules) into account when verifying the correctness of the rules; (2) it permits the interoperability of graph transformation concepts with a number of standards-based model-driven development tools; and (3) it makes available a plethora of OCL tools to actually perform the rule analysis. This approach is especially useful to analyse the operational semantics of Domain Spe-ciﬁc Visual Languages. We have automated these ideas by providing designers with tools for the graphical speciﬁcation and analysis of graph transformation rules, including a back-annotation mechanism that presents the analysis results in terms of the original language notation.",
        "keywords": [
            "Graph transformation",
            "OCL",
            "Meta-modelling",
            "Domain Speciﬁc Visual Languages",
            "Veriﬁcation and validation"
        ],
        "authors": [
            "Jordi Cabot",
            "Robert Clarisó",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-009-0129-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A concern architecture view for aspect-oriented software design",
        "submission-date": "2005/11",
        "publication-date": "2006/10",
        "abstract": "Although aspect-oriented programming is becoming popular, support for the independent description of aspect designs and for the incremental design of aspects themselves has been neglected. A conceptual framework for the design of aspects is presented, where aspects are viewed as augmentations that map an existing design into a new one with changes or additions. The principles of a Concern Architecture model are defined both to group designs of aspects, and to make explicit their dependencies and potential interferences in the design of a system with multiple aspects. The aspects are described generically, where any design element can be either required or provided. The required elements resemble formal parameters, and their binding to an existing design shows the context in which the provided parts are to modify that design. Overlap and a partial order among aspects and concerns are visualized in a Concern Architecture Diagram. An instantiation of the ideas as a UML profile is outlined, and the design of a digital sound recorder is used to demonstrate the utility of the approach.",
        "keywords": [
            "Aspect orientation",
            "Design concepts",
            "UML"
        ],
        "authors": [
            "Mika Katara",
            "Shmuel Katz"
        ],
        "file_path": "data/sosym-all/s10270-006-0032-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Change patterns\nCo-evolving requirements and architecture",
        "submission-date": "2011/03",
        "publication-date": "2012/08",
        "abstract": "Change, such as in the requirements or the\nassumptions of a system, has a far-reaching impact across\nseveral software artifacts. This paper argues that patterns of\nco-evolution (or change patterns) can be observed between\nintertwined pairs of artifacts, like the requirements speci-\nfication and the architectural design. The paper introduces\nchange patterns as a precise framework to systematically\ncapture and handle change. The approach is based on model-\ndriven engineering concepts and is accompanied by a tool-\nsupported process. Changing trust assumptions are presented\nas an example of security-related evolution, and are used to\nillustrate the approach. The approach is empirically validated\nby means of a controlled experiment involving 12 subjects,\nand a case study involving an industrial partner.",
        "keywords": [
            "Co-evolution",
            "Model-driven engineering",
            "Security requirements",
            "Software architecture"
        ],
        "authors": [
            "Koen Yskout",
            "Riccardo Scandariato",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-012-0276-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On tracing reactive systems",
        "submission-date": "2009/06",
        "publication-date": "2010/03",
        "abstract": "We present a rich and highly dynamic technique for analyzing, visualizing, and exploring the execution traces of reactive systems. The two inputs are a designer’s inter-object scenario-based behavioral model, visually described using a UML2-compliant dialect of live sequence charts (LSC), and an execution trace of the system. Our method allows one to visualize, navigate through, and explore, the activation and progress of the scenarios as they “come to life” during execution. Thus, a concrete system’s run-time is recorded and viewed through abstractions provided by behavioral models used for its design, tying the visualization and exploration of system execution traces to model-driven engineering. We support both event-based and real-time-based tracing, and use details-on-demand mechanisms, multi-scaling grids, and gradient coloring methods. Novel model exploration techniques include semantics-based navigation, ﬁltering, and trace comparison. The ideas are implemented and tested in a prototype tool called the Tracer.",
        "keywords": [
            "Software visualization",
            "UML interactions",
            "Sequence diagrams",
            "Live sequence charts",
            "Model-based traces",
            "Dynamic analysis"
        ],
        "authors": [
            "Shahar Maoz",
            "David Harel"
        ],
        "file_path": "data/sosym-all/s10270-010-0151-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A generic LSTM neural network architecture to infer heterogeneous model transformations",
        "submission-date": "2020/07",
        "publication-date": "2021/05",
        "abstract": "Models capture relevant properties of systems. During the models’ life-cycle, they are subjected to manipulations with different\ngoals such as managing software evolution, performing analysis, increasing developers’ productivity, and reducing human\nerrors. Typically, these manipulation operations are implemented as model transformations. Examples of these transformations\nare (i) model-to-model transformations for model evolution, model refactoring, model merging, model migration, model\nreﬁnement, etc., (ii) model-to-text transformations for code generation and (iii) text-to-model ones for reverse engineering.\nThese operations are usually manually implemented, using general-purpose languages such as Java, or domain-speciﬁc\nlanguages (DSLs) such as ATL or Acceleo. Even when using such DSLs, transformations are still time-consuming and error-\nprone. We propose using the advances in artiﬁcial intelligence techniques to learn these manipulation operations on models and\nautomate the process, freeing the developer from building speciﬁc pieces of code. In particular, our proposal is a generic neural\nnetwork architecture suitable for heterogeneous model transformations. Our architecture comprises an encoder–decoder long\nshort-term memory with an attention mechanism. It is fed with pairs of input–output examples and, once trained, given an\ninput, automatically produces the expected output. We present the architecture and illustrate the feasibility and potential of\nour approach through its application in two main operations on models: model-to-model transformations and code generation.\nThe results conﬁrm that neural networks are able to faithfully learn how to perform these tasks as long as enough data are\nprovided and no contradictory examples are given.",
        "keywords": [
            "Model manipulation",
            "Code generation",
            "Model transformation",
            "Artiﬁcial intelligence",
            "Machine learning",
            "Neural networks"
        ],
        "authors": [
            "Loli Burgueño",
            "Jordi Cabot",
            "Shuai Li",
            "Sébastien Gérard"
        ],
        "file_path": "data/sosym-all/s10270-021-00893-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section of SoSyM dedicated to 50 years of Petri nets",
        "submission-date": "2014/03",
        "publication-date": "2014/11",
        "abstract": "Very few concepts of computer science are so closely linked to the names of their creators as Petri nets are to their inventor Carl Adam Petri. More than 50 years ago, in June 1962, he laid the foundation for a modeling technique that has been continuously worked on ever since. With his stable foundation, numerous and broadly applicable concepts and techniques, special cases, generalizations and adaptations, Petri nets are used in a variety of ﬁelds inside and outside computer science. Just as ﬁnite automata and their numerous variants are the formal basis for many modeling languages for sequential systems, Petri nets are a basic and precise description of essential concepts and phenomena of discrete distributed systems, which implicitly or explicitly inﬂuenced many custom-designed modeling languages. Efﬁcient techniques for proving and checking relevant properties of a system model can be transferred from Petri nets to other modeling languages. The same is true for information and results on those properties that cannot be ascertained or require a very high algorithmic effort. Thus, results of Petri net theory are relevant in many areas where the name “Petri” is not explicitly mentioned. Vice versa, new and custom-designed problems are often easier to analyze and solve in the more general context of Petri nets. This was the impetus for discussing the basic concepts of Petri nets and highlight current ﬁelds of successful application in this special section.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-014-0439-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "FLEXMI: a generic and modular textual syntax for domain-speciﬁc modelling",
        "submission-date": "2022/02",
        "publication-date": "2022/11",
        "abstract": "Domain-speciﬁc languages allow engineers and domain experts to express problems and design solutions using domain-focused vocabularies and abstractions, by means of graphical or textual syntaxes. In the case of textual syntaxes, language engineers can opt for creating a language-speciﬁc syntax by deﬁning and maintaining a BNF-style grammar, or use an existing general-purpose reﬂective syntax such as the XML Metadata Interchange (XMI) or the Human Usable Textual Notation (HUTN), which do not require any development and maintenance effort, but which are more verbose and cannot be customised. We present Flexmi: a new general-purpose textual syntax for deﬁning models that conform to Eclipse Modelling Framework’s Ecore-based metamodels. Flexmi offers XML and YAML/JSON syntax ﬂavours, it can be fuzzily parsed to reduce verbosity, and it includes a templating system to facilitate encapsulation of reusable composite model element structures, thus enabling more concise model speciﬁcations. We have evaluated Flexmi for verbosity and model loading performance against XMI, HUTN, and a bespoke (i.e. custom) textual syntax for Ecore (Emfatic). Our results indicate that the use of fuzzy parsing and templates allow Flexmi to achieve a signiﬁcant reduction in the verbosity of models compared to XMI/HUTN and can become almost as concise as a bespoke textual syntax, with a moderate performance penalty.",
        "keywords": [
            "Domain-speciﬁc languages",
            "Generic textual syntaxes",
            "Model-driven engineering"
        ],
        "authors": [
            "Dimitris Kolovos",
            "Alfonso de la Vega"
        ],
        "file_path": "data/sosym-all/s10270-022-01064-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Spider Graphs: a graph transformation system for spider diagrams",
        "submission-date": "2012/08",
        "publication-date": "2013/09",
        "abstract": "The use of diagrammatic logic as a reasoning mechanism to produce inferences on subsets of some universe could provide a way to overcome the current limitations of visual modelling methods, which have to be integrated with textual languages to express complex constraints. On the other hand, graph transformations are becoming widespread as a way to express formal semantics for visual modelling languages, so that a mechanisation of diagrammatic logic based on graph transformation would facilitate language integration, based on a common underlying machinery. In this paper, we propose such a mechanisation for spider diagrams (SDs), an established language for reasoning with diagrams modelling relations between sets and constraints on their cardinalities. The concrete syntax of SDs extends that of Euler diagrams that use closed curves and the enclosed regions to represent sets and their intersections. The language is augmented with reasoning rules, i.e. syntactic transformation rules corresponding to logical inference rules. However, these rules are typically defined in procedural terms, so that a completely formal specification and an adequate mechanisation of them has not been achieved yet. We propose an abstract syntax for SDs in terms of typed graphs and define the corresponding language of Spider Graphs (SGs), expressing reasoning rules for SDs as graph transformation units. This enables a direct realisation of the reasoning system via graph transformation tools without resorting to ad hoc implementations, and we provide an implementation in AGG. Techniques for static analysis become available to reason on proof strategies and on possible optimisations.",
        "keywords": [
            "Diagrammatic reasoning",
            "Graph transformations",
            "Spider diagrams",
            "Spider Graphs",
            "Reasoning strategies"
        ],
        "authors": [
            "Paolo Bottoni",
            "Andrew Fish",
            "Francesco Parisi Presicce"
        ],
        "file_path": "data/sosym-all/s10270-013-0381-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Spider Graphs"
        }
    },
    {
        "title": "Reusing semi-speciﬁed behavior models in systems analysis and design",
        "submission-date": "2006/01",
        "publication-date": "2008/02",
        "abstract": "As the structural and behavioral complexity of systems has increased, so has interest in reusing modules in early development phases. Developing reusable modules and then weaving them into speciﬁc systems has been addressed by many approaches, including plug-and-play software component technologies, aspect-oriented techniques, design patterns, superimposition, and product line techniques. Most of these ideas are expressed in an object-oriented framework, so they reuse behaviors after dividing them into methods that are owned by classes. In this paper, we present a crosscutting reuse approach that applies object-process methodology (OPM). OPM, which uniﬁes system structure and behavior in a single view, supports the notion of a process class that does not belong to and is not encapsulated in an object class, but rather stands alone, capable of getting input objects and producing output objects. The approach features the ability to specify modules generically and concretize them in the target application. This is done in a three-step process: designing generic and target modules, weaving them into the system under development, and reﬁning the combined speciﬁcation in a way that enables the individual modules to be modiﬁed after their reuse. Rules for specifying and combining modules are deﬁned and exempliﬁed, showing the ﬂexibility and beneﬁts of this approach.",
        "keywords": [
            "Software reuse",
            "Aspect-oriented software engineering",
            "Aspect-oriented modeling",
            "Object-Process Methodology",
            "Modularity"
        ],
        "authors": [
            "Iris Reinhartz-Berger",
            "Dov Dori",
            "Shmuel Katz"
        ],
        "file_path": "data/sosym-all/s10270-007-0079-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Agile MERODE: a model-driven software engineering method for user-centric and value-based development",
        "submission-date": "2021/11",
        "publication-date": "2022/06",
        "abstract": "Agile is often associated with a lack of architectural thinking causing technical debt but has the advantage of user centricity and a strong focus on value. Model-driven software engineering (MDSE) strongly performs for building a quality architecture and code, but lacks focus on user requirements and tends to consider development as a monolithic whole. The combination of Agile and MDSE has been explored, but a convincing integrated method has not been proposed yet. This paper addresses this gap by exploring the speciﬁc combination of MERODE—as an example of a proven MDSE method—with Scrum, a reference agile method offering a concrete (sprint-based) life cycle management on the basis of user stories. The method resulting of this integration is called Agile MERODE; it is driven by user stories, themselves associated with behavior-driven development scenarios. It allows for domain-driven design and permits fast development from domain models by means of code generation. An illustrative example further clariﬁes the practical application of Agile MERODE, while a case study shows the planning game application in the case’s context. While the approach, in its entirety, allows reducing technical debt by building the architecture in a logical, consistent and complete manner, introducing MDSE involves a trade-off with pure value-driven development. Agile MERODE contributes to the state of the art by showing how to increase user centricity in MDSE, how to align model-driven engineering with the Scrum cycle, and how to reduce the technical debt of agile developments yet remaining value-focused.",
        "keywords": [
            "Model-driven engineering",
            "Agile",
            "MERODE",
            "User story",
            "User stories",
            "BDD",
            "Behavior-driven development"
        ],
        "authors": [
            "Monique Snoeck",
            "Yves Wautelet"
        ],
        "file_path": "data/sosym-all/s10270-022-01015-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to the theme section on model-based testing",
        "submission-date": "2018/09",
        "publication-date": "2018/10",
        "abstract": "This theme on model-based testing (MBT) was organized in the context of advances in model-based testing (A-MOST) workshop series. Now in its fourteenth edition, this workshop covers all aspects of MBT from theoretical developments to industrial implementations. Following the twelfth edition in Chicago (2016), we invited the MBT community to submit their latest and ﬁnest research in the ﬁeld via an open call. We selected seven articles that are presented in this theme section.",
        "keywords": [],
        "authors": [
            "Mike Papadakis",
            "Shaukat Ali",
            "Gilles Perrouin"
        ],
        "file_path": "data/sosym-all/s10270-018-0699-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Eugenia: towards disciplined and automated development of GMF-based graphical model editors",
        "submission-date": "2013/07",
        "publication-date": "2015/02",
        "abstract": "EMF and GMF are powerful frameworks for implementing tool support for modelling languages in Eclipse. However, with power comes complexity, implementing a graphical editor for a modelling language using EMF and GMF requires developers to handcraft and maintain several detailed interconnected models through a loosely guided, labour-intensive, and error-prone process. We demonstrate how the application of metamodel annotation and model transformation techniques can help to manage the complexity of GMF and EMF and deliver signiﬁcant productivity, quality, and maintainability beneﬁts. We present Eugenia, an open-source tool that implements the proposed approach, illustrate its functionality with an example, evaluate it through an empirical study, and report on the community’s response to the tool.",
        "keywords": [
            "Graphical modelling",
            "Model transformation",
            "Eclipse",
            "GMF"
        ],
        "authors": [
            "Dimitrios S. Kolovos",
            "Antonio García-Domínguez",
            "Louis M. Rose",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-015-0455-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From misuse cases to mal-activity diagrams: bridging the gap between functional security analysis and design",
        "submission-date": "2011/03",
        "publication-date": "2012/03",
        "abstract": "Secure software engineering is concerned with developing software systems that will continue delivering its intended functionality despite a multitude of harmful software technologies that can attack these systems from anywhere and at anytime. Misuse cases and mal-activity diagrams are two techniques to model functional security requirements address security concerns early in the develop-ment life cycle. This allows system designers to equip their systems with security mechanisms built within system design rather than relying on external defensive mechanisms. In a model-driven engineering process, misuse cases are expected to drive the construction of mal-activity diagrams. However, a systematic approach to transform misuse cases into mal-activity diagrams is missing. Therefore, this process remains dependent on human skill and judgment, which raises the risk of developing mal-activity diagrams that are inconsistent with the security requirements described in misuse cases, leading to the development of an insecure system. This paper presents an authoring structure for misuse cases and a transformation technique to systematically perform this desired model transformation. A study was conducted to evaluate the proposed technique using 46 attack stories outlined in a book by a former well-known hacker (Mitnick and Simon in The art of deception: controlling the human element of security, Wiley, Indianapolis, 2002). The results indicate that applying the proposed technique produces correct mal-activ-ity diagrams from misuse cases.",
        "keywords": [
            "Model transformation",
            "Misuse cases",
            "Mal-activity diagrams",
            "Metamodels"
        ],
        "authors": [
            "Mohamed El-Attar"
        ],
        "file_path": "data/sosym-all/s10270-012-0240-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the automation-supported derivation of domain-speciﬁc UML proﬁles considering static semantics",
        "submission-date": "2019/05",
        "publication-date": "2021/05",
        "abstract": "In the light of standardization, the model-driven engineering (MDE) is becoming increasingly important for the development of DSLs, in addition to traditional approaches based on grammar formalisms. Metamodels deﬁne the abstract syntax and static semantics of a DSL and can be created by using the language concepts of the Meta Object Facility (MOF) or by deﬁning a UML proﬁle.\nBoth metamodels and UML proﬁles are often provided for standardized DSLs, and the mappings of metamodels to UML proﬁles are usually speciﬁed informally in natural language, which also applies for the static semantics of metamodels and/or UML proﬁles, which has the disadvantage that ambiguities can occur, and that the static semantics must be manually translated into a machine-processable language.\nTo address these weaknesses, we propose a new automated approach for deriving a UML proﬁle from the metamodel of a DSL. One novelty is that subsetting or redeﬁning metaclass attributes are mapped to stereotype attributes whose values are computed at runtime via automatically created OCL expressions. The automatic transfer of the static semantics of a DSL to a UML proﬁle is a further contribution of our approach. Our DSL Metamodeling and Derivation Toolchain (DSL-MeDeTo) implements all aspects of our proposed approach in Eclipse. This enabled us to successfully apply our approach to the two DSLs Test Description Language (TDL) and Speciﬁcation and Description Language (SDL).",
        "keywords": [],
        "authors": [
            "Alexander Kraas"
        ],
        "file_path": "data/sosym-all/s10270-021-00890-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "OCL"
        }
    },
    {
        "title": "On the formal interpretation and behavioural consistency checking of SysML blocks",
        "submission-date": "2014/10",
        "publication-date": "2015/12",
        "abstract": "The Systems Modeling Language (SysML) is a semi-formal, graphical modelling language used in the speciﬁcation and design of systems. We describe how Communicating Sequential Processes (CSP) and its associated reﬁnement checker, FDR3, may be used to underpin an approach that facilitates the reﬁnement checking of the behavioural consistency of SysML diagrams. We achieve this by utilising CSP as a semantic domain for reasoning about SysML behavioural aspects: activities and state machines are given a formal, process-algebraic semantics. These behaviours execute within the context of the structural diagrams to which they relate, and this is reﬂected in the CSP descriptions that depict their characteristic patterns of interaction. We describe how CSP and FDR3 can be used in conjunction with SysML in a formal, top-down approach to systems engineering. Moreover, the compositionality afforded by CSP alleviates the state space explosion problem frequently encountered with complex formal models and complements the top-down approach of SysML. Typically, a system is composed from constituent systems using the concept of blocks. SysML permits two alternative interpretations with regard to the behaviour of the resulting composition. We argue that the use of a process-algebraic formalism enables us to explore the relationships between these interpretations in a more rigorous fashion than would otherwise be the case.",
        "keywords": [
            "SysML",
            "CSP",
            "State machines",
            "Activities",
            "Blocks"
        ],
        "authors": [
            "Jaco Jacobs",
            "Andrew Simpson"
        ],
        "file_path": "data/sosym-all/s10270-015-0511-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The 2011 “State of the Journal” Report\nEditorial for the SoSyM Issue 2012/01: Part 2",
        "submission-date": "2011/12",
        "publication-date": "2011/12",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Robert France",
            "Geri Georg",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-011-0225-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Effective application of process improvement patterns to business processes",
        "submission-date": "2013/09",
        "publication-date": "2014/12",
        "abstract": "Improving the operational effectiveness and efﬁ-ciency of processes is a fundamental task of business process management (BPM). There exist many proposals of process improvement patterns (PIPs) as practices that aim at support-ing this goal. Selecting and implementing relevant PIPs are therefore an important prerequisite for establishing process-aware information systems in enterprises. Nevertheless, there is still a gap regarding the validation of PIPs with respect to their actual business value for a speciﬁc application scenario before implementation investments are incurred. Based on empirical research as well as experiences from BPM projects, this paper proposes a method to tackle this challenge. Our approach toward the assessment of process improvement pat-terns considers real-world constraints such as the role of senior stakeholders or the cost of adapting available IT sys-tems. In addition, it outlines process improvement poten-tials that arise from the information technology infrastruc-ture available to organizations, particularly regarding the combination of enterprise resource planning with business process intelligence. Our approach is illustrated along a real-world business process from human resource manage-ment. The latter covers a transactional volume of about 29,000 process instances over a period of 1year. Overall, our approach enables both practitioners and researchers to reasonably assess PIPs before taking any process implemen-tation decision.",
        "keywords": [
            "Business process modeling",
            "Business process design",
            "Business process optimization",
            "Business process intelligence",
            "Business process quality"
        ],
        "authors": [
            "Matthias Lohrmann",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-014-0443-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial for the SoSyM issue 2006/02",
        "submission-date": "2006/05",
        "publication-date": "2006/05",
        "abstract": "This issue consists of two parts: The ﬁrst part has three regular papers. The second part contains a special sec-tion on Service-Based Software Engineering edited by Manfred Broy, Heinrich Hussmann, Ingolf H. Krüger and Bernhard Schätz.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-006-0019-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Why Johnny can’t model",
        "submission-date": "2009/03",
        "publication-date": "2009/03",
        "abstract": "I recently taught an undergraduate C++ course for second year undergraduate students. Teaching this course provided me with some insight into why some students have difﬁculty grasping the abstraction and modeling concepts introduced in the advanced software development courses we offer to third and fourth year students. While grading their programming assignments it became evident that students were not thinking of their solutions in an object-oriented (OO) manner. The students were becoming skilled at stringing together C++ statements to produce working programs, but they were having problems thinking about solutions in terms of collaborating objects. Many students used objects more as passive maintainers of data rather than as active participants in a collaborative effort to accomplish functional goals. For example, classes with references to other classes were rare; many classes had only basic get and set methods, and had attributes that uniquely identiﬁed objects. What was surprising was the extensive use of globally declared data structures in some of the programs. If students do not understand how to effectively use OOP concepts to solve problems then it is not surprising that they have problems building good models of OO solutions. Their underdeveloped OOP skills make it difﬁcult for them to distinguish good and bad OO abstractions. The link between modeling and programming skills should not be undervalued. A good modeler should also be a skilled programmer. A great modeler is invariably an expert programmer. On the other hand, good programmers are not necessarilygoodmodelers.Highly-skilledprogrammersmay rely on mentally-held patterns and abstractions when constructing programs, but using those patterns and abstractions to produce good models is an acquired skill. Good programming knowledge should not be equated to good knowledge of the syntax and semantics of a programming language. A few students in my C++ class did have good knowledge of the C++ syntax and semantics, but they also had poor OOP skills. Learning how to program is not the same as learning the syntax and semantics of a programming language. Similarly, learning how to model is not the same as learning the syntax and semantics of a modeling language such as the UML. As an analogy, consider how abstractions are developed and used in mathematics. For example, Category Theory provides abstractions over mathematical structures (e.g., sets) and their relationships (e.g., functions). The developers of these abstractions had in-depth knowledge of the mathematical structures (including in-depth knowledge of their manipulations) they were abstracting over. Few will argue that effective use of these abstractions requires at least a good understanding of the mathematical structures that the theory abstracts over. Attempting to use Category Theory without such knowledge can confuse rather than enlighten. Given the above, how we can better lay the foundation on whichstudentsdevelopgoodmodelingskills?Weshouldcer- tainly continue to expose students to programming concepts as early as possible in the curriculum. A problem with many introductory programming courses is that emphasis has been more on covering programming language syntax and seman- tics at the expense of material that addresses how abstrac- tions provided by a programming language can be used to develop good quality solutions. The growing complexity of programming languages is partially to blame for this state of affairs, but I would argue that teaching students how to pro- gram trumps the need to expose students to a wide range of program language features. The decision to cover a language",
        "keywords": [],
        "authors": [
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-009-0117-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A systematic literature review on IoT-aware business process modeling views, requirements and notations",
        "submission-date": "2021/07",
        "publication-date": "2022/10",
        "abstract": "The Internet of things has been adopted in several sectors both influencing how people work and enhancing organizations’ business processes. This resulted in the rise of relevant research topics such as IoT-aware business processes. The modeling of these processes makes it possible to better understand working scenarios and to support the adoption of model-driven development approaches for IoT-aware and process-oriented software systems. Since much research has been performed on this topic, a better awareness of the current status is needed. This paper reports a systematic literature review to develop a map on modeling notations for IoT-aware business processes. The survey mainly adopts an academic point of view, resulting in the detailed analysis of 84 research works from the leading computer science digital libraries. The output of the review is in the form of schemes and reflections. In particular, our research aims to shed light on (1) the relevant modeling views referring to different types of IoT-aware business processes; (2) the IoT requirements supported by the modeling notations; and (3) the modeling notations proposed and/or adopted to model IoT-aware business processes. Finally, our research work highlights possible future research lines needing further investigations.",
        "keywords": [
            "IoT-aware business process",
            "IoT",
            "Business process management",
            "Modeling notation"
        ],
        "authors": [
            "Ivan Compagnucci",
            "Flavio Corradini",
            "Fabrizio Fornari",
            "Andrea Polini",
            "Barbara Re",
            "Francesco Tiezzi"
        ],
        "file_path": "data/sosym-all/s10270-022-01049-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven method for enacting the design-time QoS analysis of business processes",
        "submission-date": "2011/10",
        "publication-date": "2013/05",
        "abstract": "Business Process Management (BPM) is a holistic approach for describing, analyzing, executing, managing, and improving large enterprise business processes. A business process can be seen as a flow of tasks that are orchestrated to accomplish well-defined goals such as goods production or services delivery. From an IT perspective, BPM is closely related to a business process automation approach carried out by use of IT standards and technologies, such as service-oriented architectures (SOAs) and Web Services. This paper specifically focuses on fully automated business processes that are defined and executed as orchestrations of software services. In a BPM context, the ability to predict at design time the business process behavior assumes a strategic relevance, both to early assess whether or not the business goals are achieved and to gain a competitive advantage. A business process is typically specified by use of Business Process Modeling Notation (BPMN), the standard language for the high-level description of business processes. Unfortunately, BPMN does not support the characterization of the business process in terms of nonfunctional or QoS properties, such as performance and reliability. To overcome such a limitation, this paper introduces Performability-enabled BPMN (PyBPMN), a lightweight BPMN extension for the specification of performance and reliability properties. PyBPMN enables the design time prediction of the business processes behavior, in terms of performance and reliability properties. Such prediction activity requires the use of models that are to be first built and then evaluated. In this respect, this work introduces a model-driven method that exploits PyBPMN to predict, at design time, the performance and the reliability of a business process, either to select the process configuration that provides the best behavior or to check if a given configuration satisfies the overall requirements. The proposed model-driven method that enacts the automated analysis of a business process behavior embraces the complete business process development cycle, from the specification phase down to the implementation phase. The paper also describes how the proposed model-driven method is implemented. The several model transformations at the core of the method have been implemented by use of QVT, and the standard language for specifying model transformations provided by OMG’s MDA. The availability of such automated model transformations allows business analysts to predict the process behavior with no extra effort and without being required to own specific skills of performance or reliability theory, as shown by use of an example application.",
        "keywords": [
            "Business process",
            "MDA",
            "BPMN",
            "Performance",
            "QoS",
            "LQN"
        ],
        "authors": [
            "Paolo Bocciarelli",
            "Andrea D’Ambrogio"
        ],
        "file_path": "data/sosym-all/s10270-013-0345-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "The extended EA ModelSet—a FAIR dataset for researching and reasoning enterprise architecture modeling practices",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Conceptual modeling research is increasingly investigating the application of artiﬁcial intelligence (AI) and machine learning (ML) to automate tasks like model creation, completion, analysis, and processing. This trend also applies to enterprise architecture (EA) research. In contrast to its neighboring disciplines, such as business process management, EA lacks proper guidelines, patterns, and best practices to create high-quality EA models. A currently limiting factor for conducting AI-based research to bridge these gaps is the scarcity of openly available models of adequate quality and quantity. With this paper, our aim is to address this limitation by introducing the extended EA ModelSet, a curated and FAIR repository of enterprise architecture models represented in the ArchiMate modeling language that can be used by the research and practitioner community. We report on our efforts to build the EA ModelSet and elaborate on exemplary future empirical and ML-based research that can facilitate the dataset. We hope that this paper sparks a community effort toward the further development and maintenance of the EA ModelSet.",
        "keywords": [
            "Enterprise modeling",
            "Machine learning",
            "FAIR",
            "Enterprise architecture",
            "Dataset",
            "Artiﬁcial intelligence",
            "Conceptual modeling",
            "ArchiMate"
        ],
        "authors": [
            "Philipp-Lorenz Glaser",
            "Emanuel Sallinger",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-025-01278-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "KIPO: the knowledge-intensive process ontology",
        "submission-date": "2012/09",
        "publication-date": "2014/04",
        "abstract": "A business process is a sequence of activities that aims at creating products or services, granting value to the customer, and is generally represented by a business process model. Business process models play an important role in bridging the gap between the business domain and the information technology, increasing the weight of business model-ing as ﬁrst step of software development. However, the tra-ditional way of representing a process is not suitable for the so-called Knowledge-Intensive Processes (KIP). This type of process comprises sequences of activities based on intensive acquisition,sharing,storageand(re)useofknowledge,sothat the amount of value added to the organization depends on the actor knowledge. Current research in the literature points to the lack of approaches to make this kind of process explicit and strategies for handling information that is necessary for their understanding and support. The goal of this paper is to present KIPO—a knowledge-intensive process ontology, which encompasses a clear and semantically rich deﬁnition of KIPs, and to discuss the results of a case study to eval-uate KIPO with regard to its applicability and capability of making all relevant knowledge embedded in a KIP explicit.",
        "keywords": [
            "Knowledge-intensive process",
            "Knowledge-intensive process ontology",
            "Process representation",
            "Foundational ontology"
        ],
        "authors": [
            "Juliana Baptista dos Santos França",
            "Joanne Manhães Netto",
            "Juliana do E. S. Carvalho",
            "Flávia Maria Santoro",
            "Fernanda Araujo Baião",
            "Mariano Pimentel"
        ],
        "file_path": "data/sosym-all/s10270-014-0397-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Continuous Evolution of Digital Twins using the DarTwin Notation",
        "submission-date": "2023/09",
        "publication-date": "2024/11",
        "abstract": "Despite best efforts, various challenges remain in the creation and maintenance processes of digital twins (DTs). One of those primary challenges is the constant, continuous and omnipresent evolution of systems, their user’s needs and their environment, demanding the adaptation of the developed DT systems. DTs are developed for a speciﬁc purpose, which generally entails the monitoring, analysis, simulation or optimisation of a speciﬁc aspect of an actual system, referred to as the actual twin (AT). As such, when the twin system changes, that is either the AT itself changes, or the scope/purpose of a DT is modiﬁed, the DTs usually evolve in close synchronicity with the AT. As DTs are software systems, the best practices or methodologies for software evolution can be leveraged. This paper tackles the challenge of maintaining a (set of) DT(s) throughout the evolution of the user’s requirements and priorities and tries to understand how this evolution takes place. In doing so, we provide two contributions: (i) we develop DarTwin, a visual notation form that enables reasoning on a twin system, its purposes, properties and implementation, and (ii) we introduce a set of architectural transformations that describe the evolution of DT systems. The development of these transformations is driven and illustrated by the evolution and transformations of a family home’s DT, whose purpose is expanded, changed and re-prioritised throughout its ongoing lifecycle. Additionally, we evaluate the transformations on a laboratory-scale gantry crane’s DT.",
        "keywords": [
            "System evolution",
            "System design",
            "Composability",
            "Digital twin",
            "Smart home"
        ],
        "authors": [
            "Joost Mertens",
            "Stefan Klikovits",
            "Francis Bordeleau",
            "Joachim Denil",
            "Øystein Haugen"
        ],
        "file_path": "data/sosym-all/s10270-024-01216-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "STAIRS towards formal design with sequence diagrams",
        "submission-date": "2004/05",
        "publication-date": "2005/10",
        "abstract": "The paper presents STAIRS [1], an approach to the compositional development of UML interactions supporting the speciﬁcation of mandatory as well as potential behavior. STAIRS has been designed to facilitate the use of interactions for requirement capture as well as test speciﬁcation. STAIRS assigns a precise interpretation to the various steps in incremental system development based on an approach to reﬁnement known from the ﬁeld of formal methods and provides thereby a foundation for compositional analysis. An interaction may characterize three main kinds of traces. A trace may be (1) positive in the sense that it is valid, legal or desirable, (2) negative meaning that it is invalid, illegal or undesirable, or (3) inconclusive meaning that it is considered irrelevant for the interaction in question. The basic increments in system development proposed by STAIRS, are structured into three main kinds referred to as supplementing, narrowing and detailing. Supplementing categorizes inconclusive traces as either positive or negative. Narrowing reduces the set of positive traces to capture new design decisions or to match the problem more adequately. Detailing involves introducing a more detailed description without signiﬁcantly altering the externally observable behavior.",
        "keywords": [
            "UML interactions",
            "Formal semantics",
            "Explicit non-determinism",
            "Reﬁnement",
            "Sequence diagrams"
        ],
        "authors": [
            "Øystein Haugen",
            "Knut Eilif Husa",
            "Ragnhild Kobro Runde",
            "Ketil Stølen"
        ],
        "file_path": "data/sosym-all/s10270-005-0087-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards an integrated graph-based semantics for UML",
        "submission-date": "2005/08",
        "publication-date": "2008/08",
        "abstract": "This paper shows how a central part of the Uniﬁed Modeling Language (UML) can be integrated into a single visual semantic model. It discusses UML models compo-sed of class, object, state, sequence and collaboration dia-grams and presents an integrated semantics of these models. As formal basis the theoretically well-founded area of graph transformation is employed which supports a visual and rule-based transformation of UML model states. For the transla-tion of a UML model into a graph transformation system the operations in class diagrams and the transitions in state diagrams are associated with graph transformation rules that are then combined into one system in order to obtain a single coherent semantic description. Operation calls in sequence and collaboration diagrams can be associated with applica-tions of graph transformation rules in the constructed graph transformation system so that valid sequence and collabora-tion diagrams correspond to derivations, i.e., to sequences of graph transformation rule applications. The main aim of this paper is to provide a formal framework that supports visual simulation of integrated UML speciﬁcations in which system statesandstatechangesaremodeledinastraightforwardway.",
        "keywords": [
            "UML diagram",
            "Graph transformation",
            "Formal semantics"
        ],
        "authors": [
            "Sabine Kuske",
            "Martin Gogolla",
            "Hans-Jörg Kreowski",
            "Paul Ziemann"
        ],
        "file_path": "data/sosym-all/s10270-008-0101-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling the interplay of security, privacy and trust in sociotechnical systems: a computer-aided design approach",
        "submission-date": "2018/03",
        "publication-date": "2019/07",
        "abstract": "Personal data have become a central asset for multiple enterprise applications and online services offered by private companies, public organisations or a combination of both. The sensitivity of such data and the continuously growing legislation that accompanies their management dictate the development of methods that allow the development of more secure, trustworthy software systems with focus on privacy protection. The contribution of this paper is the deﬁnition of a novel requirements engineering method that supports both early and late requirements speciﬁcation, giving emphasis on security, privacy and trust. The novelty of our work is that it provides the means for software designers and security experts to analyse the system-to-be from multiple aspects, starting from identifying high-level goals to the deﬁnition of business process composition, and elicitation of mechanisms to fortify the system from external threats. The method is supported by two CASE tools. To demonstrate the applicability and usefulness of our work, the paper shows its applications to a real-world case study.",
        "keywords": [
            "Security",
            "Privacy",
            "Trust",
            "Sociotechnical systems",
            "CASE tools"
        ],
        "authors": [
            "Mattia Salnitri",
            "Konstantinos Angelopoulos",
            "Michalis Pavlidis",
            "Vasiliki Diamantopoulou",
            "Haralambos Mouratidis",
            "Paolo Giorgini"
        ],
        "file_path": "data/sosym-all/s10270-019-00744-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multi-level modeling: cornerstones of a rationale",
        "submission-date": "2020/05",
        "publication-date": "2022/01",
        "abstract": "This expert voice paper presents a comprehensive rationale of multi-level modeling. It aims not only at a systematic assessment of its prospects, but also at encouraging applications of multi-level modeling in business information systems and at providing a motivation for future research. The assessment is developed from a comparison of multi-level modeling with object-oriented, general-purpose modeling languages (GPMLs) and domain-speciﬁc modeling languages (DSMLs). To foster a differentiated evaluation, we propose a multi-perspective framework that accounts, among others, for essential design conﬂicts, different types of users, as well as economic aspects. Besides the assessment of the additional abstraction offered by multi-level modeling, the evaluation also identiﬁes speciﬁc drawbacks and remaining challenges. Based on the results of the comparative assessment, in order to foster the adoption and further development of multi-level modeling, we discuss the prospects of supplementing multi-level modeling languages with multi-level programming languages and suggest possible dissemination strategies customized for different groups of users. The paper concludes with an outline of future research.",
        "keywords": [
            "Essential design conﬂicts",
            "Multi-perspective evaluation framework",
            "Multi-level programming languages",
            "Integration of models and code",
            "Multi-level dissemination strategies"
        ],
        "authors": [
            "Ulrich Frank"
        ],
        "file_path": "data/sosym-all/s10270-021-00955-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A benchmark for OCL engine accuracy, determinateness, and efﬁciency",
        "submission-date": "2009/05",
        "publication-date": "2010/09",
        "abstract": "Since several years, the Object Constraint Language (OCL) is a central component in modeling and transformation languages like the Uniﬁed Modeling Language, the Meta Object Facility, and Query View Transformation. Consequently, approaches MDE (Model-Driven Engineering) depend on this language. OCL is present not only in areas inﬂuenced by the OMG but also in the Eclipse Modeling Framework (EMF). Thus the quality of OCL and its realization in tools seems to be crucial for the success of model-driven development. Surprisingly, up to now a benchmark for OCL to measure quality properties has not been proposed. This paper puts forward in the ﬁrst part the concepts of a comprehensive OCL benchmark. Our benchmark covers (1) OCL engine accuracy (e.g., for the handling of the undeﬁned value, the use of variables and the implementation of OCL standard operations), (2) OCL engine determinateness properties (e.g., for the collection operations ‘any’ and ‘ﬂatten’), and (3) OCL engine efﬁciency (for data type and user-deﬁned operations). In the second part, this paper empirically evaluates the proposed benchmark concepts by examining several OCL tools. The paper clariﬁes a number of differences in handling particular language features and under speciﬁcations in the OCL standard.",
        "keywords": [
            "OCL",
            "Benchmark",
            "UML",
            "MDE",
            "Accuracy",
            "Determinateness",
            "Efﬁciency"
        ],
        "authors": [
            "Mirco Kuhlmann",
            "Lars Hamann",
            "Martin Gogolla",
            "Fabian Büttner"
        ],
        "file_path": "data/sosym-all/s10270-010-0174-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-based design approach for simulation and virtual prototyping of automotive control systems using port-Hamiltonian systems",
        "submission-date": "2016/03",
        "publication-date": "2017/12",
        "abstract": "Cyber–physical systems (CPS) such as automotive control systems consist of various interacting cyber and physical com-\nponents. Heterogeneous domains, composition of multiple components, complex dynamics, and nonlinearities result in\nsigniﬁcant challenges for design, modeling, and simulation of CPS. Model-based design can be used to address such chal-\nlenges, but it is very important to use physically accurate heterogeneous models that can be composed to represent the overall\nsystem behavior. Further, it is important to preserve the properties derived from analyses based on the mathematical models\nin the control system implementation in order to reduce costly testing and design changes late in the development cycle. This\npaper proposes a model-based design methodology for automotive control software using port-Hamiltonian systems (PHS).\nPHS are used to model the vehicle dynamics, speed and steering control systems, and the interactions between physical and\ncyber components. Passivity analysis is used to design the controllers and ensure system stability. More importantly, the\nproposed approach guarantees that passivity is preserved after time-discretization and quantization of the controllers. The\nmodels are then used for code generation and compilation, scheduling, and software deployment, ensuring that passivity is\npreserved by the control system implementation. We evaluate the methodology using an automotive control design case study\nimplemented on a hardware-in-the-loop simulation platform and present simulation results to demonstrate its effectiveness.",
        "keywords": [
            "Cyber",
            "physical systems",
            "Model-based design",
            "Port-Hamiltonian systems",
            "Passivity",
            "Automotive control software"
        ],
        "authors": [
            "Siyuan Dai",
            "Zhenkai Zhang",
            "Xenofon Koutsoukos"
        ],
        "file_path": "data/sosym-all/s10270-017-0646-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A method for testing and validating executable statechart models",
        "submission-date": "2016/08",
        "publication-date": "2018/05",
        "abstract": "Statecharts constitute an executable language for modelling event-based reactive systems. The essential complexity of\nstatechart models solicits the need for advanced model testing and validation techniques. In this article, we propose a method\nimed at enhancing statechart design with a range of techniques that have proven their usefulness to increase the quality\nand reliability of source code. The method is accompanied by a process that ﬂexibly accommodates testing and validation\ntechniques such as test-driven development, behaviour-driven development, design by contract, and property statecharts that\ncheck for violations of behavioural properties during statechart execution. The method is supported by the Sismic tool, an\nopen-source statechart interpreter library in Python, which supports all the aforementioned techniques. Based on this tool-\ning, we carry out a controlled user study to evaluate the feasibility, usefulness and adequacy of the proposed techniques for\nstatechart testing and validation.",
        "keywords": [
            "Statechart",
            "Executable modeling",
            "Behaviour-driven development",
            "Design by contract",
            "Runtime veriﬁcation"
        ],
        "authors": [
            "Tom Mens",
            "Alexandre Decan",
            "Nikolaos I. Spanoudakis"
        ],
        "file_path": "data/sosym-all/s10270-018-0676-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integration of clinical and genomic data to enhance precision medicine: a case of study applied to the retina-macula",
        "submission-date": "2021/11",
        "publication-date": "2022/09",
        "abstract": "Age-related macular degeneration is a complex, multifactorial, and neurodegenerative disease that is the third cause of blindness after cataracts and glaucoma. To date, there are no effective remedies available for treating the disease. Therefore, the main goal of the scientiﬁc community is to uncover the underlying role that both genetics and environmental factors play in the development of the disease. Nevertheless, the complexity of the domain, the heterogeneity of the information, and the massive amounts of existing data hinder the daily work of clinical experts to provide an accurate diagnosis and treatment. In this work, we present how clinicians can beneﬁt from the development of ontologically well-grounded information systems to support the management of both clinical and genomic data. First, we summarize the results obtained in a previous work that cover the clinical perspective using an information system called G-MAC, that has been specially developed for the management of clinical data. Then, we present the results of an exhaustive study of the genetic factors of age-related macular degeneration by using an information system that was developed with the aim of enhancing the management of complex genomic data. Finally, we state how the connection of both perspectives through the use of conceptual models can beneﬁt clinicians and patients through a more accurate Medicine of Precision.",
        "keywords": [
            "Conceptual modeling",
            "Information systems",
            "Precision medicine",
            "Age-related macular degeneration",
            "Genomics"
        ],
        "authors": [
            "José Fabián Reyes Román",
            "Ana León Palacio",
            "Alberto García Simón",
            "Rubén Cabrera Beyrouti",
            "Oscar Pastor"
        ],
        "file_path": "data/sosym-all/s10270-022-01039-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integration of data validation and user interface concerns in a DSL for web applications",
        "submission-date": "2009/11",
        "publication-date": "2010/09",
        "abstract": "Data validation rules constitute the constraints that data input and processing must adhere to in addition to the structural constraints imposed by a data model. Web mod-eling tools do not make all types of data validation explicit in their models, hampering full code generation and model expressivity. Web application frameworks do not offer a consistent interface for data validation. In this paper, we present a solution for the integration of declarative data val-idation rules with user interface models in the domain of web applications, unifying syntax, mechanisms for error han-dling, and semantics of validation checks, and covering value well-formedness, data invariants, input assertions, and actionassertions.WehaveimplementedtheapproachinWeb-DSL, a domain-speciﬁc language for the definition of web applications.",
        "keywords": [
            "Web application",
            "Domain-speciﬁc language",
            "Data validation"
        ],
        "authors": [
            "Danny M. Groenewegen",
            "Eelco Visser"
        ],
        "file_path": "data/sosym-all/s10270-010-0173-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "AI simulation by digital twins: systematic survey, reference framework, and mapping to a standardized architecture",
        "submission-date": "2024/10",
        "publication-date": "2025/06",
        "abstract": "Insufﬁcient data volume and quality are particularly pressing challenges in the adoption of modern subsymbolic AI. To alleviate these challenges, AI simulation uses virtual training environments in which AI agents can be safely and efﬁciently developed with simulated, synthetic data. Digital twins open new avenues in AI simulation, as these high-ﬁdelity virtual replicas of physical systems are equipped with state-of-the-art simulators and the ability to further interact with the physical system for additional data collection. In this article, we report on our systematic survey of digital twin-enabled AI simulation. By analyzing 22 primary studies, we identify technological trends and derive a reference framework to situate digital twins and AI components. Based on our ﬁndings, we derive a reference framework and provide architectural guidelines by mapping it onto the ISO 23247 reference architecture for digital twins. Finally, we identify challenges and research opportunities for prospective researchers.",
        "keywords": [
            "AI",
            "Artiﬁcial intelligence",
            "Data science",
            "Deep neural networks",
            "Digital twins",
            "Lifecycle model",
            "Machine learning",
            "Neural networks",
            "Reinforcement learning",
            "SLR",
            "Subsymbolic AI",
            "Survey",
            "Training"
        ],
        "authors": [
            "Xiaoran Liu",
            "Istvan David"
        ],
        "file_path": "data/sosym-all/s10270-025-01306-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Converting metamodels to graph grammars: doing without advanced graph grammar features",
        "submission-date": "2012/12",
        "publication-date": "2013/09",
        "abstract": "In this paper, we present a method to convert a metamodel in the form of a UML class diagram into a context-sensitive graph grammar whose language comprises precisely the set of model graphs (UML object diagrams) that conform to the input metamodel. Compared to other approaches that deal with the same problem, we use a graph grammarformalismthatdoesnotemployanyadvancedgraph grammar features, such as application conditions, precedence rules, and production schemes. Specifically, we use Rekers and Schürr’s Layered Graph Grammars, which may be regarded as a pure generalization of standard context-sensitive string grammars. We show that elementary grammatical features, i.e., grammar labels and context-sensitive graph rewrite rules, sufﬁce to represent metamodels with arbitrary multiplicities and inheritance. Inspired by attribute string grammars, we also propose a graph-grammar-based approach to the semantic analysis of model graphs.",
        "keywords": [
            "Metamodel",
            "UML",
            "Graph grammar",
            "Parsability",
            "Parsing",
            "Semantic analysis"
        ],
        "authors": [
            "Luka Fürst",
            "Marjan Mernik",
            "Viljan Mahniˇc"
        ],
        "file_path": "data/sosym-all/s10270-013-0380-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalizing the four-layer metamodeling stack with METAMORPH: potential and beneﬁts",
        "submission-date": "2021/03",
        "publication-date": "2022/03",
        "abstract": "Enterprise modeling deals with the increasing complexity of processes and systems by operationalizing model content and by linking complementary models and languages, thus amplifying the model value beyond mere comprehensible pictures. To enable this ampliﬁcation and turn models into computer-processable structures, a comprehensive formalization is needed. This paper presents the formalism MetaMorph based on typed ﬁrst-order logic and provides a perspective on the potential and beneﬁts of formalization that arise for a variety of research issues in conceptual modeling. MetaMorph deﬁnes modeling languages as formal languages with a signature Σ—comprising object types, relation types, and attributes through types and function symbols—and a set of constraints. Four case studies are included to show the effectiveness of this approach. Applying the MetaMorph formalism to the next level in the hierarchy of models, we create M2FOL, a formal modeling language for metamodels. We show that M2FOL is self-describing and therefore complete the formalization of the full four-layer metamodelingstack.Onthebasisofourgenericformalismapplicabletoarbitrarymodelinglanguages,weexaminefourcurrent research topics—modeling with power types, language interleaving & consistency, operations on models, and automatic translation of formalizations to platform-speciﬁc code—and how to approach them with the MetaMorph formalism. This shows that the rich knowledge stack on formal languages in logic offers new tools for old problems.",
        "keywords": [
            "Conceptual modeling",
            "Metamodeling",
            "Modeling language",
            "Formal language",
            "Predicate logic"
        ],
        "authors": [
            "Victoria Döller"
        ],
        "file_path": "data/sosym-all/s10270-022-00986-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial for the theme issue on model-based interoperability",
        "submission-date": "2011/02",
        "publication-date": "2011/02",
        "abstract": "The commercial benefits claimed for software based on visual and graphical modeling languages are well documented. Many domain specific modeling tools exist and are being used as point solutions. Tailoring of notations to the specific application domain and combined use of several languages define the nature of the approach, and constitute the source of the achievable benefits. Unfortunately, data representations and the mechanisms used to integrate modeling languages tend to be highly tool specific. This compromises the use of modeling languages in building tool chains that may contain components from several suppliers.",
        "keywords": [],
        "authors": [
            "Tony Clark",
            "Jorn Bettin"
        ],
        "file_path": "data/sosym-all/s10270-010-0184-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-based architecture for interactive run-time monitoring",
        "submission-date": "2019/06",
        "publication-date": "2020/01",
        "abstract": "We present a model-based architecture for monitoring executions of models of real-time and embedded systems. This architecture is highly conﬁgurable and allows for the combination of various run-time monitoring tools, not only for observing the system execution, but also for interacting with it. Using a variety of case studies, we illustrate the use of the architecture for connecting the code generated from a model with a range of external tools for different purposes, including execution animation and run-time veriﬁcation. However, the external tool can not only consume information from the execution, but also generate input for it and thus inﬂuence and steer it.",
        "keywords": [
            "Model-driven engineering",
            "Run-time monitoring",
            "Observation",
            "Model instrumentation",
            "UML-RT"
        ],
        "authors": [
            "Nicolas Hili",
            "Mojtaba Bagherzadeh",
            "Karim Jahed",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-020-00780-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ChatGPT in software modeling",
        "submission-date": "2023/05",
        "publication-date": "2023/05",
        "abstract": "We wonder—how many SoSyM readers have tried using OpenAI’sChatGPTtoformulatepromptstoexploretheinter-esting results that may be produced? It is really fascinating to consider the results when the query is deﬁned appropriately. Of course, we abstained from using ChatGPT to write any part of this editorial, but if we actually would generate portions of our text, we wonder how difﬁcult it would be for readers to identify the generated parts. ChatGPT is rather chatty, but even the produced text can be adapted by asking it for a speciﬁc answering style. The options and future extensions seem to be endless. In the context of this editorial, we do not consider legal or ethical issues of ChatGPT—these have to be sorted out eventually. We also do not discuss the problems of having ChatGPT text, or text generated by other Large Language Models (LLMs), in scientiﬁc papers. Rather, in this editorial, we focus more positively and ask the main question: How will ChatGPT be able to help us in a development process, especially in the tasks of developing or using models for analysis, production or understanding of software and systems?",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01106-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Interactive web interfaces modeling, simulation and analysis using Colored Petri Nets",
        "submission-date": "2016/04",
        "publication-date": "2017/03",
        "abstract": "Interaction modeling is a relevant activity during software development processes. Created relying on Petri Nets theory and aiming to represent discrete time events, Colored Petri Nets (CPNs) are a graphical formal language developed and widely employed for system modeling. While traditional CPNs only have elements with ordinary stylization and behaviors, in this article we explore the key ideas behind Web Interaction Modeling Using Colored Petri Nets (wiCPN), a modeling style developed with focus on representing Web interactions as an incremental improvement of CPNs. We review wiCPN’s reﬁnements over CPNs and the modeling of the Web interface of Classroom eXperience (CX), a ubiquitous educational platform, thus verifying the modelspropertiestoensureitwasabletorepresentthedifferent access levels among its users and how wiCPN displayed suitability to comprehend this requirement on the gener-ated model. We have also improved the originally developed model with the modiﬁcation of elements to make it ﬁnite and fully analyzable. Also, we added temporization capabil-ities to the model and ran corresponding user simulation to observe the average time that users with different roles tend to spend during interactions. We compared wiCPN results with Uniﬁed Modeling Language (UML) Activity and Use Case diagrams, observing, as outcomes, that the generated model represented CX’s interactive ﬂow correctly and maintained a concise notation—a single wiCPN diagram was sufﬁcient to depict the same interactive ﬂow that, in UML, would require several diagrams, something that could overload the design team in actual software development scenarios. We also included new user experiments comprising qualitative results from experts. Finally, we created a reachability graph for the new model and generated a full state space report, analyzing Petri Nets properties such as boundedness, live-ness and home marking.",
        "keywords": [
            "Web interaction modeling",
            "Human",
            "computer interaction",
            "Web interfaces",
            "Formal methods",
            "Colored Petri Nets"
        ],
        "authors": [
            "Taffarel Brant-Ribeiro",
            "Rafael D. Araújo",
            "Igor E. Mendonça",
            "Michel S. Soares",
            "Renan G. Cattelan"
        ],
        "file_path": "data/sosym-all/s10270-017-0593-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A graph-based framework for model-driven optimization facilitating impact analysis of mutation operator properties",
        "submission-date": "2021/10",
        "publication-date": "2023/01",
        "abstract": "Optimization problems in software engineering typically deal with structures as they occur in the design and maintenance of software systems. In model-driven optimization (MDO), domain-speciﬁc models are used to represent these structures while evolutionary algorithms are often used to solve optimization problems. However, designing appropriate models and evolutionary algorithms to represent and evolve structures is not always straightforward. Domain experts often need deep knowledge of how to conﬁgure an evolutionary algorithm. This makes the use of model-driven meta-heuristic search difﬁcult and expensive. We present a graph-based framework for MDO that identiﬁes and clariﬁes core concepts and relies on mutation operators to specify evolutionary change. This framework is intended to help domain experts develop and study evolutionary algorithms based on domain-speciﬁc models and operators. In addition, it can help in clarifying the critical factors for conducting reproducible experiments in MDO. Based on the framework, we are able to take a ﬁrst step toward identifying and studying important properties of evolutionary operators in the context of MDO. As a showcase, we investigate the impact of soundness and completeness at the level of mutation operator sets on the effectiveness and efﬁciency of evolutionary algorithms.",
        "keywords": [
            "Search-Based Software Engineering",
            "Model-Driven Engineering",
            "Evolutionary Computation"
        ],
        "authors": [
            "Stefan John",
            "Jens Kosiol",
            "Leen Lambers",
            "Gabriele Taentzer"
        ],
        "file_path": "data/sosym-all/s10270-022-01078-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Asynchronous session subtyping as communicating automata refinement",
        "submission-date": "2020/02",
        "publication-date": "2021/01",
        "abstract": "We study the relationship between session types and behavioural contracts, representing Communicating Finite State Machines (CFSMs), under the assumption that processes communicate asynchronously. Session types represent a syntax-based approach for the description of communication protocols, while behavioural contracts, formally expressing CFSMs, follow an operational approach. We show the existence of a fully abstract interpretation of session types into a fragment of contracts that maps session subtyping into binary compliance-preserving CFSMs/behavioural contract refinement. In this way, on the one hand, we enrich the theory of session types with an operational characterization and, on the other hand, we use recent undecidability results for asynchronous session subtyping to obtain an original undecidability result for asynchronous CFSMs/behavioural contract refinement.",
        "keywords": [
            "Session types",
            "Behavioural contracts",
            "Communicating finite-state machines"
        ],
        "authors": [
            "Mario Bravetti",
            "Gianluigi Zavattaro"
        ],
        "file_path": "data/sosym-all/s10270-020-00838-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design of blockchain-based applications using model-driven engineering and low-code/no-code platforms: a structured literature review",
        "submission-date": "2022/10",
        "publication-date": "2023/06",
        "abstract": "The creation of blockchain-based software applications requires today considerable technical knowledge, particularly in software design and programming. This is regarded as a major barrier in adopting this technology in business and making it accessible to a wider audience. As a solution, low-code and no-code approaches have been proposed that require only little or no programming knowledge for creating full-fledged software applications. In this paper we extend a review of academic approaches from the discipline of model-driven engineering as well as industrial low-code and no-code development platforms for blockchains. This includes a content-based, computational analysis of relevant academic papers and the derivation of major topics. In addition, the topics were manually evaluated and refined. Based on these analyses we discuss the spectrum of approaches in this field and derive opportunities for further research.",
        "keywords": [
            "Blockchain",
            "Low-code",
            "No-code",
            "Model-driven engineering",
            "Software development"
        ],
        "authors": [
            "Simon Curty\nFelix Härer\nHans-Georg Fill"
        ],
        "file_path": "data/sosym-all/s10270-023-01109-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Understanding and improving UML package merge",
        "submission-date": "2007/03",
        "publication-date": "2007/12",
        "abstract": "Package merge allows the content of one package to be combined with that of another package. Package merge is used extensively in the UML 2 speciﬁcation to modularize the definition of the UML 2 meta model and to deﬁne the four compliance levels of UML 2. Package merge is a novel construct in UML and currently not well understood. This paper summarizes our work to understand and improve package merge. First, we identify ambiguous and missing rules in the package merge definition and suggest corrections. Then, we formalize package merge and analyze it with respect to some desirable properties. Our analyses employs Alloy, a ﬁrst-order modelling language with tool support, and concepts from mathematical logic which allow us to develop a general taxonomy of package extension mechanisms. The analyses reveal the unexpected failure of important properties.",
        "keywords": [
            "UML",
            "Semantics formalization",
            "Model composition",
            "Metamodeling techniques"
        ],
        "authors": [
            "J. Dingel",
            "Z. Diskin",
            "A. Zito"
        ],
        "file_path": "data/sosym-all/s10270-007-0073-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Linking models and their storage artifacts",
        "submission-date": "2011/06",
        "publication-date": "2011/06",
        "abstract": "Many developers working on model-based development projects experience a problem that highlights the need for better model management technologies. If someone wants to adapt an OO class that was generated from a model, the ﬁrst thing is to locate the “sources” used to generate the class (i.e., the model elements used to produce the class implementa-tion). This turned out to be a surprisingly difﬁcult and time-consuming task. It is worth shedding some light on why this task typically is difﬁcult if only to remind us that research on software modeling needs to extend its focus to include model management issues if we are to see more widespread adoption of software-modeling approaches.",
        "keywords": [],
        "authors": [
            "Bernhard Rumpe",
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-011-0208-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Empirically evaluating OCL and Java for specifying constraints on UML models",
        "submission-date": "2013/11",
        "publication-date": "2014/11",
        "abstract": "The Object Constraint Language (OCL) has been applied, along with UML models, for various purposes such as supporting model-based testing, code generation, and automated consistency checking of UML models. However, a lot of challenges have been raised in the literature regarding its applicability in industry such as extensive training, slow learning curve, and significant effort to use OCL due to lack of familiarity of practitioners. To confirm these challenges, empirical evidence is needed, which is severely lacking in the literature. To build such preliminary evidence, we report a controlled experiment that was designed to evaluate OCL by comparing it with Java; a programming language that has also been used to specify constraints on UML models. Results show that the participants using OCL perform as good as the participants working with Java in terms of three objective quality metrics (i.e., completeness, conformance and redundancy) and two subjective metrics (i.e., applicability and confidence level). In addition, the participants using OCL performed consistently well for all the constraints of varying complexity, while fluctuating results were obtained for the participants using Java for the same constraints. Based on the empirical evidence, we can conclude that it does not make much difference to use OCL or Java for specifying constraints on UML models. However, the participants working with OCL performed consistently well on specifying constraints of varying complexity suggesting that OCL can be used to model complicated constraints (commonly observed in industrial applications) with the same quality as for simpler constraints. Moreover, additional analyses on the constraints when using Java and OCL tools revealed that tools are needed to specify fully correct constraints that can be used to support automation.",
        "keywords": [
            "OCL",
            "Java",
            "Controlled experiment",
            "Empirical study",
            "Constraints"
        ],
        "authors": [
            "Tao Yue",
            "Shaukat Ali"
        ],
        "file_path": "data/sosym-all/s10270-014-0438-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SoSyM at three",
        "submission-date": "2004/11",
        "publication-date": "2004/11",
        "abstract": "This issue marks the end of three years since the ﬁrst issue of the International Journal on Software and System Modeling (SoSyM) was published. We sincerely thank the authors, reviewers, and editors who have contributed to the success of the journal as a media for collecting and disseminating high quality papers on software and system modeling. As we have done on previous anniversaries, we take this opportunity to give a “state of the journal” report and to acknowledge the reviewers, editors, and publication staﬀthat have contributed to another year of publication that has exceeded our expectations.",
        "keywords": [],
        "authors": [
            "Robert B. France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-004-0075-9.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Harvesting models from web 2.0 databases",
        "submission-date": "2009/11",
        "publication-date": "2011/02",
        "abstract": "Data rather than functionality are the sources of competitive advantage for Web2.0 applications such as wikis, blogs and social networking websites. This valuable information might need to be capitalized by third-party applications or be subject to migration or data analysis. Model-Driven Engineering (MDE) can be used for these purposes. However, MDE first requires obtaining models from the wiki/blog/website database (a.k.a. model harvesting). This can be achieved through SQL scripts embedded in a program. However, this approach leads to laborious code that exposes the iterations and table joins that serve to build the model. By contrast, a Domain-Speciﬁc Language (DSL) can hide these “how” concerns, leaving the designer to focus on the “what”, i.e. the mapping of database schemas to model classes. This paper introduces Schemol, a DSL tailored for extracting models out of databases which considers Web2.0 specifics. Web2.0 applications are often built on top of general frameworks (a.k.a. engines) that set the databaseschema(e.g.,MediaWiki,Blojsom).Hence,table names offer little help in automating the extraction process. In addition, Web2.0 data tend to be annotated. User-provided data (e.g., wiki articles, blog entries) might contain semantic markups which provide helpful hints for model extraction. Unfortunately, these data end up being stored as opaque strings. Therefore, there exists a considerable conceptual gap between the source database and the target metamodel. Schemol offers extractive functions and view-like mechanisms to confront these issues. Examples using Blojsom as the blog engine are available for download.",
        "keywords": [
            "Model-driven engineering",
            "Web2.0",
            "Harvesting",
            "Data re-engineering",
            "Databases"
        ],
        "authors": [
            "Oscar Díaz",
            "Gorka Puente",
            "Javier Luis Cánovas Izquierdo",
            "Jesús García Molina"
        ],
        "file_path": "data/sosym-all/s10270-011-0194-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Schemol"
        }
    },
    {
        "title": "Learning minimal automata with recurrent neural networks",
        "submission-date": "2023/02",
        "publication-date": "2024/03",
        "abstract": "In this article, we present a novel approach to learning ﬁnite automata with the help of recurrent neural networks. Our goal\nis not only to train a neural network that predicts the observable behavior of an automaton but also to learn its structure,\nincluding the set of states and transitions. In contrast to previous work, we constrain the training with a speciﬁc regularization\nterm. We iteratively adapt the architecture to learn the minimal automaton, in the case where the number of states is unknown.\nWe evaluate our approach with standard examples from the automata learning literature, but also include a case study of\nlearning the ﬁnite-state models of real Bluetooth Low Energy protocol implementations. The results show that we can ﬁnd\nan appropriate architecture to learn the correct minimal automata in all considered cases.",
        "keywords": [
            "Automata learning",
            "Machine learning",
            "Recurrent neural networks",
            "Bluetooth Low Energy",
            "Model inference"
        ],
        "authors": [
            "Bernhard K. Aichernig",
            "Sandra König",
            "Cristinel Mateis",
            "Andrea Pferscher",
            "Martin Tappler"
        ],
        "file_path": "data/sosym-all/s10270-024-01160-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing the OPEN Process Framework with service-oriented method fragments",
        "submission-date": "2011/04",
        "publication-date": "2011/11",
        "abstract": "Service orientation is a promising paradigm that enables the engineering of large-scale distributed software systems using rigorous software development processes. The existing problem is that every service-oriented software development project often requires a customized development process that provides speciﬁc service-oriented software engineering tasks in support of requirements unique to that project. To resolve this problem and allow situational method engineering, we have deﬁned a set of method fragments in support of the engineering of the project-speciﬁc service-oriented software development processes. We have derived the proposed method fragments from the recurring features of 11 prominent service-oriented software development methodologies using a systematic mining approach. We have added these new fragments to the repository of OPEN Process Framework to make them available to software engi-neers as reusable fragments using this well-known method repository.",
        "keywords": [
            "Service-oriented software development",
            "OPEN Process Framework",
            "OPF repository",
            "Method fragment",
            "Situational method engineering"
        ],
        "authors": [
            "Mahdi Fahmideh Gholami",
            "Mohsen Shariﬁ",
            "Pooyan Jamshidi"
        ],
        "file_path": "data/sosym-all/s10270-011-0222-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to the theme issue on multi-level modeling",
        "submission-date": "2016/09",
        "publication-date": "2016/10",
        "abstract": "Multi-level modeling, i.e., the explicit use of multiple levels of classification in modeling, is a conservative extension of the well-established, traditional two-level object-oriented paradigm. Two-level object-oriented technology has been tremendously successful in both modeling (e.g., UML) and programming (e.g., Java). However, it has been shown that attempting to capture certain domains or systems with only two classification levels (i.e., objects and their types) results in accidental complexity that stems from an impedance mismatch between the subject at hand and the solution technology used to capture it [2]. Examples for domains that can be much more elegantly captured using multiple classification levels are biological taxonomies, process (meta-) modeling, enforced software architectures, and systems with dynamic type levels [4].",
        "keywords": [],
        "authors": [
            "Colin Atkinson",
            "Thomas Kühne",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-016-0565-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A UML and OWL description of Bunge’s upper-level ontology model",
        "submission-date": "2006/02",
        "publication-date": "2008/03",
        "abstract": "A prominent high-level ontology is that proposed by Mario Bunge. While it has been extensively used for research in IS analysis and conceptual modelling, it has not been employed in the more formal settings of semantic web research. We claim that its speciﬁcation in natural language is the key inhibitor to its wider use. Consequently, this paper offers a description of this ontology in open, standardized knowledge representation formats. The ontology is described both in UML and OWL in order to address needs of both semantic web and conceptual modelling communities.",
        "keywords": [],
        "authors": [
            "Joerg Evermann"
        ],
        "file_path": "data/sosym-all/s10270-008-0082-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalization of the classiﬁcation pattern: survey of classiﬁcation modeling in information systems engineering",
        "submission-date": "2015/02",
        "publication-date": "2016/04",
        "abstract": "Formalization is becoming more common in all stages of the development of information systems, as a better understanding of its beneﬁts emerges. Classiﬁcation systems are ubiquitous, no more so than in domain model-ing. The classiﬁcation pattern that underlies these systems provides a good case study of the move toward formaliza-tion in part because it illustrates some of the barriers to formalization, including the formal complexity of the pat-tern and the ontological issues surrounding the “one and the many.” Powersets are a way of characterizing the (com-plex) formal structure of the classiﬁcation pattern, and their formalization has been extensively studied in mathemat-ics since Cantor’s work in the late nineteenth century. One can use this formalization to develop a useful benchmark. There are various communities within information systems engineering (ISE) that are gradually working toward a for-malization of the classiﬁcation pattern. However, for most of these communities, this work is incomplete, in that they have not yet arrived at a solution with the expressiveness of the powerset benchmark. This contrasts with the early smooth adoption of powerset by other information systems communities to, for example, formalize relations. One way of understanding the varying rates of adoption is recogniz-ing that the different communities have different historical baggage. Many conceptual modeling communities emerged from work done on database design, and this creates hur-dles to the adoption of the high level of expressiveness of powersets. Another relevant factor is that these communities also often feel, particularly in the case of domain modeling, a responsibility to explain the semantics of whatever formal structures they adopt. This paper aims to make sense of the formalization of the classiﬁcation pattern in ISE and surveys its history through the literature, starting from the relevant theoretical works of the mathematical literature and gradu-ally shifting focus to the ISE literature. The literature survey follows the evolution of ISE’s understanding of how to for-malize the classiﬁcation pattern. The various proposals are assessed using the classical example of classiﬁcation; the Linnaean taxonomy formalized using powersets as a bench-mark for formal expressiveness. The broad conclusion of the survey is that (1) the ISE community is currently in the early stages of the process of understanding how to formalize the classiﬁcation pattern, particularly in the requirements for expressiveness exempliﬁed by powersets, and (2) that there is an opportunity to intervene and speed up the process of adoption by clarifying this expressiveness. Given the central place that the classiﬁcation pattern has in domain model-ing, this intervention has the potential to lead to signiﬁcant improvements.",
        "keywords": [
            "Classiﬁcation system",
            "Classiﬁcation",
            "Powerset",
            "Powertype",
            "Set theory"
        ],
        "authors": [
            "Chris Partridge",
            "Sergio de Cesare",
            "Andrew Mitchell",
            "James Odell"
        ],
        "file_path": "data/sosym-all/s10270-016-0521-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From low-level programming to full-ﬂedged industrial model-based development: the story of the Rubus Component Model",
        "submission-date": "2023/05",
        "publication-date": "2023/05",
        "abstract": "Developing distributed real-time systems is a complex task that has historically entailed specialized handcraft. In this paper,\nwe propose a retrospective on the (r)evolutionary changes that led to the transition from low-level programming to industrial\nfull-ﬂedged model-based development embodied by the Rubus Component Model and its tool-ecosystem. We focus on the\nneeds, challenges, and solutions of a 15-year-long evolution journey of a software development approach that has gone from\nlow-level and manual programming to a highly automated environment offering modeling, analysis, and development of\nvehicular software systems with multi-criticality for deployment on single- and multi-core platforms.",
        "keywords": [
            "Component model",
            "Model-based development",
            "Vehicular embedded systems real",
            "time systems"
        ],
        "authors": [
            "Alessio Bucaioni",
            "Federico Ciccozzi",
            "Amleto Di Salle",
            "Mikael Sjödin"
        ],
        "file_path": "data/sosym-all/s10270-023-01107-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scientiﬁc workﬂow execution in the cloud using a dynamic runtime model",
        "submission-date": "2021/12",
        "publication-date": "2023/06",
        "abstract": "To explain speciﬁc phenomena, scientists perform a sequence of tasks, e.g., to gather, analyze and interpret data, forming a scientiﬁc workﬂow. Depending on the complexity of the workﬂow, scientists require access to various kinds of tools, applications and infrastructures for individual tasks. Current approaches are often limited to managing these resources at design time, requiring the scientist to preemptively set up applications essential for their workﬂow. Therefore, a dynamic provisioning and conﬁguration of computing resources are required that fulﬁlls these needs at runtime. In this paper, we present a dynamic runtime model that couples workﬂow tasks with their individual applications and infrastructure requirements. This runtime model is used as a knowledge base by a model-driven workﬂow execution engine orchestrating the sequence of tasks and their infrastructure. We exhibit that the simplicity of the runtime model supports the creation of highly tailored infrastructures, the integration of self-developed applications, as well as a human-in-the-loop allowing scientists to monitor and interact with the workﬂow at runtime. To tackle the heterogeneity of cloud provider interfaces, we implement the workﬂow runtime model by extending the Open Cloud Computing Interface cloud standard, which provides an extensible data model as well as a uniform interface to manage cloud resources. We demonstrate the applicability of our approach using three case studies and discuss the beneﬁts of the runtime model from a user and system perspective.",
        "keywords": [
            "Runtime model",
            "Workﬂow",
            "Cloud",
            "OCCI"
        ],
        "authors": [
            "Johannes Erbel",
            "Jens Grabowski"
        ],
        "file_path": "data/sosym-all/s10270-023-01112-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating formal model veriﬁcation tools in an industrial context: the case of a smart device life cycle management system",
        "submission-date": "2023/05",
        "publication-date": "2024/08",
        "abstract": "The formal veriﬁcation of the properties of semi-formal models can make it easier to ensure their security and safety. However,\nthis task is generally cumbersome for non-specialists in formal veriﬁcation, particularly in an industrial context. This paper\nintroduces an evaluation of four formal veriﬁcation tools on an industrial case, called a Life Cycle Management System\n(LCMS). This LCMS makes it possible to deploy Product-Service Systems (PSSs) to customers using Systems-on-Chip\n(SoC). A PSS is a business model in which products and services are tightly connected and whose objective is to optimize the\nuse of products, with a positive environmental impact. A SoC can embed hardware security; however, a LCMS must be secure\nfrom end to end, which requires a veriﬁcation not only of the used protocol (in this case, a blockchain-based protocol), but also\nof the whole architecture. For that purpose, semi-formal UML models of a LCMS were ﬁrst speciﬁed and designed with their\nassociated properties, then improved in order to be formally veriﬁable. Despite being more complex, they remain capable of\nbeing processed by dedicated tools. In this paper, Verifpal and ProVerif, two formal cryptographic protocol veriﬁers, are used\nand evaluated for the cryptographic protocol and AnimUML (developed by one of the authors) and HugoRT, two veriﬁcation\ntools for behavior and UML for the architectural model are evaluated. These tools are assessed and compared according to\ntheir coverage of properties and state spaces, limitations, and usability for non-specialists. Some limitations of the approach\nitself are also provided.",
        "keywords": [
            "UML veriﬁcation",
            "Cryptographic protocols",
            "Formal veriﬁcation tools",
            "Formally veriﬁable models",
            "Life cycle\nmanagement systems",
            "Smart devices"
        ],
        "authors": [
            "Maxime Méré",
            "Frédéric Jouault",
            "Loïc Pallardy",
            "Richard Perdriau"
        ],
        "file_path": "data/sosym-all/s10270-024-01201-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model clone detection for rule-based model transformation languages",
        "submission-date": "2016/11",
        "publication-date": "2017/10",
        "abstract": "Cloning is a convenient mechanism to enable reuse across and within software artifacts. On the downside, it is also a practice related to severe long-term maintainability impediments, thus generating a need to identify clones in affected artifacts. A large variety of clone detection techniques have been proposed for programming and modeling languages; yet no specific ones have emerged for model transformation languages. In this paper, we explore clone detection for rule-based model transformation languages, including graph-based ones, such as Henshin, and hybrid ones, such as ATL. We introduce use cases for such techniques in the context of constructive and analytical quality assurance, and a set of key requirements we derived from these use cases. To address these requirements, we describe our customization of existing model clone detection techniques: We consider eScan, an a-priori-based technique, ConQAT, a heuristic technique, and a hybrid technique based on a combination of eScan and ConQAT. We compare these techniques in a comprehensive experimental evaluation, based on three realistic Henshin rule sets, and a comprehensive body of examples from the ATL transformation zoo. Our results indicate that our customization of ConQAT enables the efficient detection of the considered clones, without sacrificing accuracy. With our contributions, we present the first evidence on the usefulness of model clone detection for the quality assurance of model transformations and pave the way for future research efforts at the intersection of model clone detection and model transformation.",
        "keywords": [
            "Quality assurance",
            "Model clone detection",
            "Model transformation",
            "ATL",
            "Henshin"
        ],
        "authors": [
            "Daniel Strüber",
            "Vlad Acretoaie",
            "Jennifer Plöger"
        ],
        "file_path": "data/sosym-all/s10270-017-0625-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Henshin, ATL"
        }
    },
    {
        "title": "Refactoring object constraint language speciﬁcations",
        "submission-date": "2005/02",
        "publication-date": "2006/07",
        "abstract": "The object constraint language (OCL) plays an important role in the elaboration of precise models. Although OCL was designed to be both formal and simple, OCL speciﬁcations may be difﬁcult to understand and evolve, particularly those containing complex or duplicated expressions. In this paper, we discuss how refactoringtechniques canbeappliedinorder toimprove the understandability and maintainability of OCL spec-iﬁcations. In particular, we present several potentially bad constructions often found in OCL speciﬁcations and a collection of refactorings that can be applied to replace such constructions by better ones. We also brieﬂy dis-cuss how refactorings can be automated and how model regression testing can be used to increase our conﬁdence that the semantics of an OCL speciﬁcation has been pre-served after manually performed refactorings.",
        "keywords": [],
        "authors": [
            "Alexandre Correa",
            "Cláudia Werner"
        ],
        "file_path": "data/sosym-all/s10270-006-0023-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lessons learned from developing mbeddr: a case study in language engineering with MPS",
        "submission-date": "2016/07",
        "publication-date": "2017/01",
        "abstract": "Language workbenches are touted as a promising technology to engineer languages for use in a wide range of domains, from programming to science to business. However, not many real-world case studies exist that evaluate the suitability of language workbench technology for this task. This paper contains such a case study. In particular, we evaluate the development of mbeddr, a collection of integrated languages and language extensions built with the Jetbrains MPS language workbench. mbeddr consists of 81 languages, with their IDE support, 34 of them C extensions. The mbeddr languages use a wide variety of notations—textual, tabular, symbolic and graphical—and the C extensions are modular; new extensions can be added without changing the existing implementation of C. mbeddr’s development has spanned 10 person-years sofar, andthetool is usedinpracticeandcontinues to be developed. This makes mbeddr a meaningful case study of non-trivial size and complexity. The evaluation is centered around ﬁve research questions: language modularity, notational freedom and projectional editing, mechanisms for managing complexity, performance and scalability issues and the consequences for the development process. We draw generally positive conclusions; language engineering with MPS is ready for real-world use. However, we also identify a number of areas for improvement in the state of the art in language engineering in general, and in MPS in particular.",
        "keywords": [
            "Language engineering",
            "Language extension",
            "Language workbenches",
            "Domain-speciﬁc language",
            "Case study",
            "Languages",
            "Experimentation"
        ],
        "authors": [
            "Markus Voelter",
            "Bernd Kolb",
            "Tamás Szabó",
            "Daniel Ratiu",
            "Arie van Deursen"
        ],
        "file_path": "data/sosym-all/s10270-016-0575-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Matching events and activities by integrating behavioral aspects and label analysis",
        "submission-date": "2015/10",
        "publication-date": "2017/05",
        "abstract": "Nowadays, business processes are increasingly supported by IT services that produce massive amounts of event data during the execution of a process. These event data can be used to analyze the process using process mining techniques to discover the real process, measure conformance to a given process model, or to enhance existing models with performance information. Mapping the produced events to activities of a given process model is essential for conformance checking, annotation and understanding of process mining results. In order to accomplish this mapping with low manual effort, we developed a semi-automatic approach that maps events to activities using insights from behavioral analysis and label analysis. The approach extracts Declare constraints from both the log and the model to build matching constraints to efficiently reduce the number of possible mappings. These mappings are further reduced using techniques from natural language processing, which allow for a matching based on labels and external knowledge sources. The evaluation with synthetic and real-life data demonstrates the effectiveness of the approach and its robustness toward non-conforming execution logs.",
        "keywords": [
            "Process mining",
            "Event mapping",
            "Business process intelligence",
            "Constraint satisfaction",
            "Declare",
            "Natural language processing"
        ],
        "authors": [
            "Thomas Baier",
            "Claudio Di Ciccio",
            "Jan Mendling",
            "Mathias Weske"
        ],
        "file_path": "data/sosym-all/s10270-017-0603-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Information science and the logic of models",
        "submission-date": "2009/03",
        "publication-date": "2009/05",
        "abstract": "Various ways exist in which a branch of science may be understood... In fact, the reality underlying the application of the systems of information science can only be approached at any point in time via models, which means that in respect of adequacy and intent, the modes of the statements made by information science are in most cases a qualiﬁed network of judgements about models.",
        "keywords": [],
        "authors": [
            "Bernd Mahr"
        ],
        "file_path": "data/sosym-all/s10270-009-0119-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From network trafﬁc data to business activities: a conceptualization and a recognition approach",
        "submission-date": "2022/03",
        "publication-date": "2023/11",
        "abstract": "Event logs are the main source for business process mining techniques. However, readily available logs are produced only by part of the existing systems, which may not always be part of an investigated environment. Furthermore, logs that are created by a given information system may reﬂect only parts of the full process, while other parts may span additional systems. We suggest that data generated by communication network trafﬁc that is associated with business processes can ﬁll this gap, both in availability and in span. However, trafﬁc data are technically oriented and noisy, and there is a huge conceptual gap between these data and business meaningful event logs. Considering the above, we set the following aims. First, to assess whether the gap between technical-level trafﬁc data and conceptual-level business activities can be bridged. Once this is established, to automatically recognize business activities within network trafﬁc data, considering that these data hold interleaving activities that are performed in parallel. To address the first aim, we develop a conceptual model of trafﬁc behavior that corresponds to a business activity. We use simulated trafﬁc data annotated by the originating activity and perform an iterative process of abstracting and ﬁltering the data, along with the application of process discovery. As a result, we obtained distinct process models for speciﬁc activity types and a generic higher-level model of trafﬁc behavior in a business activity. To address the second aim, relying on the insights gained from the conceptual models, we propose a method utilizing sequence learning to identify activity types, and their boundaries (start and end) within network trafﬁc data. Evaluation shows that the proposed approach has a high precision and recall in classifying packets by activities, even while these activities are performed in parallel to each other and their data are interleaved.",
        "keywords": [
            "Activity recognition",
            "Event abstraction",
            "Sequence models",
            "Process mining",
            "Interleaved data",
            "Network trafﬁc"
        ],
        "authors": [
            "Moshe Hadad",
            "Gal Engelberg",
            "Pnina Soffer"
        ],
        "file_path": "data/sosym-all/s10270-023-01135-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for the special section on MODELS 2016",
        "submission-date": "2019/04",
        "publication-date": "2019/05",
        "abstract": "The MODELS conference series continues to be the premier venue for model-based software and systems engineering covering all aspects of modeling, from theory to languages and methods to tools and applications. This special section presents the ﬁve articles that resulted from an invitation to authors of the best papers at MODELS 2016, each having undergone a full SoSyM review cycle and substantial revision.",
        "keywords": [],
        "authors": [
            "Jörg Kienzle",
            "Alexander Pretschner"
        ],
        "file_path": "data/sosym-all/s10270-019-00733-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Promoting traits into model-driven development",
        "submission-date": "2015/02",
        "publication-date": "2015/11",
        "abstract": "Traits, as sets of behaviors, can provide a good mechanism for reusability. However, they are limited in important ways and are not present in widely used programming and modeling languages and hence are not readily available for use by mainstream developers. In this paper, we add UML associations and other modeling concepts to traits and apply them to Java and C++ through model-driven development. We also extend traits with required interfaces so dependencies at the semantics level become part of their usage, rather than simple syntactic capture. All this is accomplished in Umple, a textual modeling language based upon UML that allows adding programming constructs to the model. We applied the work to two case studies. The results show that we can promote traits to the modeling level along with the improvement in ﬂexibility and reusability.",
        "keywords": [
            "Reusability",
            "Traits",
            "Modeling",
            "Umple",
            "UML"
        ],
        "authors": [
            "Vahdat Abdelzad",
            "Timothy C. Lethbridge"
        ],
        "file_path": "data/sosym-all/s10270-015-0505-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Student experience with software modeling tools",
        "submission-date": "2018/01",
        "publication-date": "2019/01",
        "abstract": "Modeling is a key concept in software engineering education, since students need to learn it in order to be able to produce large-\nscale and reliable software. Quality tools are needed to support modeling in education, but existing tools vary considerably\nboth in their features and in their strengths and weaknesses. The objective of the research presented in this paper was to\nhelp professors and students choose tools by determining which strengths and weaknesses matter most to students, which\ntools exhibit which of these strengths and weaknesses, and how difﬁcult to use are various tools for students. To achieve this\nobjective, we conducted a survey of the use of modeling tools among students in software engineering courses from Brazil,\nCanada, USA, Spain, Denmark, UK and China. We report the results regarding the 31 UML tools that 117 participants have\nused, focusing on the nine tools that the students have used most heavily. Common beneﬁts quoted by students in choosing\na tool include simplicity of installing and learning, being free, supporting the most important notations and providing code\ngeneration. The most cited complaints about tools include lack of feedback, being slow to use, difﬁculty drawing the diagrams,\nnot interacting well with other tools and being complex to use. This research also compares the results with the ﬁndings of\nanother survey conducted among professors who taught modeling. The results should beneﬁt tool developers by suggesting\nways they could improve their tools. The results should also help inform the selection of tools by educators and students.",
        "keywords": [
            "Software modeling tools",
            "Software engineering education",
            "Survey"
        ],
        "authors": [
            "Luciane T. W. Agner",
            "Timothy C. Lethbridge",
            "Inali W. Soares"
        ],
        "file_path": "data/sosym-all/s10270-018-00709-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Translation of ATL to AGT and application to a code generator for Simulink",
        "submission-date": "2016/06",
        "publication-date": "2017/07",
        "abstract": "Analysing and reasoning on model transformations has become very relevant for various applications such as ensuring the correctness of transformations. ATL is a model transformation language with rich semantics and a focus on usability, making its analysis not straightforward. Conversely, algebraic graph transformation (AGT) is an approach with strong theoretical foundations allowing for formal analyses that would be valuable in the context of ATL. In this paper, we propose a translation of ATL to the AGT framework in the objective of bringing theoretical analyses of AGT to ATL transformations. We show that this transformation supports a sufﬁcient subset of ATL to be used on an industrial application example: QGen, a qualiﬁable Simulink® to source code generator developed at AdaCore. In addition to this example, we validate our proposal by translating a set of feature-rich ATL transformations to the Henshin AGT framework. We execute the ATL and AGT versions on the same set of models and verify that the result is the same.",
        "keywords": [
            "ATL",
            "Henshin",
            "Algebraicgraphtransformation",
            "OCL",
            "Nested graph conditions",
            "Analysis of model\ntransformations"
        ],
        "authors": [
            "Elie Richa",
            "Etienne Borde",
            "Laurent Pautet"
        ],
        "file_path": "data/sosym-all/s10270-017-0607-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL, AGT"
        }
    },
    {
        "title": "Real-time collaborative multi-level modeling by conflict-free replicated data types",
        "submission-date": "2022/04",
        "publication-date": "2022/11",
        "abstract": "The need for real-time collaborative solutions in model-driven engineering has been increasing over the past years. Conflict-free replicated data types (CRDT) provide scalable and robust replication mechanisms that align well with the requirements of real-time collaborative environments. In this paper, we propose a real-time collaborative multi-level modeling framework to support advanced modeling scenarios, built on a collection of custom CRDT, specifically tailored for the needs of modeling environments. We demonstrate the benefits of the framework through an illustrative modeling case and compare it with other state-of-the-art modeling frameworks.",
        "keywords": [
            "Collaborative modeling",
            "Real-time collaboration",
            "Multi-level modeling",
            "Conflict-free replicated data types",
            "Model-driven engineering"
        ],
        "authors": [
            "Istvan David\nEugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-022-01054-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven approach to machine learning and software modeling for the IoT",
        "submission-date": "2021/03",
        "publication-date": "2022/01",
        "abstract": "Models are used in both Software Engineering (SE) and Artiﬁcial Intelligence (AI). SE models may specify the architecture at different levels of abstraction and for addressing different concerns at various stages of the software development life-cycle, from early conceptualization and design, to veriﬁcation, implementation, testing and evolution. However, AI models may provide smart capabilities, such as prediction and decision-making support. For instance, in Machine Learning (ML), which is currently the most popular sub-discipline of AI, mathematical models may learn useful patterns in the observed data and can become capable of making predictions. The goal of this work is to create synergy by bringing models in the said communities together and proposing a holistic approach to model-driven software development for intelligent systems that require ML. We illustrate how software models can become capable of creating and dealing with ML models in a seamless manner. The main focus is on the domain of the Internet of Things (IoT), where both ML and model-driven SE play a key role. In the context of the need to take a Cyber-Physical System-of-Systems perspective of the targeted architecture, an integrated design environment for both SE and ML sub-systems would best support the optimization and overall efﬁciency of the implementation of the resulting system. In particular, we implement the proposed approach, called ML-Quadrat, based on ThingML, and validate it using a case study from the IoT domain, as well as through an empirical user evaluation. It transpires that the proposed approach is not only feasible, but may also contribute to the performance leap of software development for smart Cyber-Physical Systems (CPS) which are connected to the IoT, as well as an enhanced user experience of the practitioners who use the proposed modeling solution.",
        "keywords": [
            "Model-driven software engineering",
            "Domain-speciﬁc modeling",
            "Analytics modeling",
            "Machine learning",
            "Internet of things",
            "Cyber-physical systems"
        ],
        "authors": [
            "Armin Moin",
            "Moharram Challenger",
            "Atta Badii",
            "Stephan Günnemann"
        ],
        "file_path": "data/sosym-all/s10270-021-00967-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Online adaptation for autonomous unmanned systems driven by requirements satisfaction model",
        "submission-date": "2021/03",
        "publication-date": "2022/02",
        "abstract": "Autonomous unmanned systems (AUSs) emerge to replace human operators for achieving better safety, efﬁciency, and effectiveness in harsh and difﬁcult missions. They usually run in a highly open and dynamic operating environment, in which some unexpected situations may occur, leading to violations of predeﬁned requirements. In order to maintain stable performance, the AUS control software needs to predict in advance whether the requirements will be violated and then make adaptations tomaximizerequirements satisfaction. Wepropose Captain, amodel-drivenandcontrol-basedonlineadaptation approach, for the AUS control software. At the modeling phase, apart from the system behavior model and the operating environment model, we construct a requirements satisfaction model. At runtime, based on the requirements satisfaction model, Captain ﬁrst predicts whether the requirements will be violated in the upcoming situation; then identiﬁes the unsatisﬁable requirements that need to be accommodated; and ﬁnally, ﬁnds an optimal adaptation for the upcoming situation. We evaluate Captain in both simulated scenarios and the real world. For the former, we use two cases of UAV Delivery and UUV Ocean Surveillance, whose results demonstrate the Captain’s robustness, scalability, and real-time performance. For the latter, we have successfully implemented Captain in the DJI Matrice 100 UAV with real-world workloads.",
        "keywords": [
            "Requirements satisfaction model",
            "Runtime adaptation",
            "Models@runtime",
            "Autonomous unmanned systems"
        ],
        "authors": [
            "Yixing Luo",
            "Yuan Zhou",
            "Haiyan Zhao",
            "Zhi Jin",
            "Tianwei Zhang",
            "Yang Liu",
            "Danny Barthaud",
            "Yijun Yu"
        ],
        "file_path": "data/sosym-all/s10270-022-00981-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using empirical studies to mitigate symbol overload in iStar extensions",
        "submission-date": "2018/10",
        "publication-date": "2019/12",
        "abstract": "Modelling languages are frequently extended to include new constructs to be used together with the original syntax. New \nconstructs may be proposed by adding textual information, such as UML stereotypes, or by creating new graphical represen-\ntations. Thus, these new symbols need to be expressive and proposed in a careful way to increase the extension’s adoption. \nA method to create symbols for the original constructs of a modelling language was proposed and has been used to create \nthe symbols when a new modelling language is designed. We argue this method can be used to recommend new symbols for \nthe extension’s constructs. However, it is necessary to make some adjustments since the new symbols will be used with the \nexisting constructs of the modelling language original syntax. In this paper, we analyse the usage of this adapted method to \npropose symbols to mitigate the occurrence of overloaded symbols in the existing iStar extensions. We analysed the existing \niStar extensions in an SLR and identified the occurrence of symbol overload among the existing constructs. We identified a \nset of fifteen overloaded symbols in existing iStar extensions. We used these concepts with symbol overload in a multi-stage \nexperiment that involved users in the visual notation design process. The study involved 262 participants, and its results \nrevealed that most of the new graphical representations were better than those proposed by the extensions, with regard to \nsemantic transparency. Thus, the new representations can be used to mitigate this kind of conflict in iStar extensions. Our \nresults suggest that next extension efforts should consider user-generated notation design techniques in order to increase the \nsemantic transparency.",
        "keywords": [
            "Model-based engineering",
            "Semiotic clarity principle",
            "Symbol overload",
            "Experiment",
            "Modelling language \nextensions",
            "iStar"
        ],
        "authors": [
            "Enyo Gonçalves",
            "Camilo Almendra",
            "Miguel Goulão",
            "João Araújo",
            "Jaelson Castro"
        ],
        "file_path": "data/sosym-all/s10270-019-00770-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting method engineering with a low-code approach: the LOMET tool",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "Method engineering emerged in the 1990s as a discipline aiming to design, construct, and adapt methods, techniques, and tools for the development of information systems. By executing a method step by step, users can follow a well-deﬁned process to achieve the intended results for which the method was created. To create methods in a more guided and systematic manner, a framework of methods can serve as a template. This allows individuals to leverage the expertise of method engineers who have consolidated their best practices within these frameworks. However, the creation and adoption of a method can be challenging in the absence of tools to support these activities. Additionally, method engineers may lack the programming skills required to implement such tools. In this context, we extend an approach inspired by the low-code paradigm for method engineering. By integrating construction rules for guidance (called here protocols), the goal of this approach is to assist method engineers in creating new methods or adapting existing frameworks. It automatically provides tool support, enabling method experts to effectively execute the method. This paper builds upon previous work and presents the approach through a proof-of-concept implementation, LOMET. We present a second version of LOMET, which has been reﬁned based on feedback received during an empirical evaluation conducted through semi-structured interviews.",
        "keywords": [
            "Method engineering",
            "Low-code",
            "Framework of methods",
            "Method execution"
        ],
        "authors": [
            "Raquel Araújo de Oliveira",
            "Mario Cortes-Cornax",
            "Agnès Front"
        ],
        "file_path": "data/sosym-all/s10270-024-01203-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Theme section on models and evolution",
        "submission-date": "2025/02",
        "publication-date": "2025/04",
        "abstract": "This theme section falls in the context of the Models and Evolution international workshop. It addresses novel theories, techniques, and tools to support evolution through and in the context of modeling. The section presents papers on topics such as Rubus Component Model evolution, process model repair, legacy system modernization, and adaptive caching for operation-based versioning of models.",
        "keywords": [],
        "authors": [
            "Dalila Tamzalit",
            "Ludovico Iovino"
        ],
        "file_path": "data/sosym-all/s10270-025-01284-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The 2013 “State of the Journal” report",
        "submission-date": "2014/01",
        "publication-date": "2014/01",
        "abstract": "SoSyM continues to experience an increasing number of submissions. In 2013, 245 manuscripts were submitted, of which 67% were reviewed for regular issues, while the other 33% were submitted for special or theme sections or industry voice. The average number of days from initial submission to a final decision (that is, a final accept or reject) was 213 days. It should be noted that this time includes the time authors spend on making major and subsequent minor revisions. The number of months to online publication of an accepted paper is between three to four weeks. We will continue to work on improving the review turnaround time.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Geri Georg",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-014-0396-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A component-based end-to-end simulation of the Linux ﬁle system",
        "submission-date": "2011/06",
        "publication-date": "2012/06",
        "abstract": "The Linux ﬁle system is designed with components utilizing a layered architecture. The upper components hide details of the lower components, and each layer presents uniﬁed and simple interfaces to the layers above and below. This design helps Linux to be ﬂexible as well as to provide support for multiple types of storage devices. In this paper, this component architecture is used to develop a realistic simulation without having to model lower level details of the hardware layer or particular storage devices. A detailed simulation-based performance model of the Linux ext3 ﬁle system has been developed using Colored Petri Nets. The extensive validation study using the model obtains results that are close to the expected behavior of the real ﬁle system. The model demonstrates that ﬁle system parameters have a significant impact on the I/O performance.",
        "keywords": [
            "Colored Petri Net",
            "File system modeling",
            "File system simulation",
            "Petri Net",
            "Linux ﬁle system",
            "L2 cache model"
        ],
        "authors": [
            "Hai Nguyen",
            "Amy Apon"
        ],
        "file_path": "data/sosym-all/s10270-012-0253-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Change-driven model transformations",
        "submission-date": "2010/03",
        "publication-date": "2011/03",
        "abstract": "In this paper, we investigate change-driven model transformations, a novel class of transformations, which are directly triggered by complex model changes carried out by arbitrary transactions on the model (e.g. editing opera-tion, transformation, etc). After a classiﬁcation of relevant change scenarios, we identify challenges for change-driven transformations. As the main technical contribution of the current paper, we deﬁne an expressive, high-level language for specifying change-driven transformations as an exten-sion of graph patterns and graph transformation rules. This language generalizes previous results on live model trans-formations by offering trigger events for arbitrarily complex model changes, and dedicated reactions for speciﬁc kinds of changes, making this way the concept of change to be a ﬁrst-class citizen of the transformation language. We dis-cuss how the underlying transformation engine needs to be adapted in order to use the same language uniformly for dif-ferent change scenarios. The technicalities of our approach will be discussed on a (1) model synchronization case study with non-materialized target models and (2) a case study on detecting the violation of evolutionary (temporal) constraints in the security requirements engineering domain.",
        "keywords": [
            "Incremental model transformation",
            "Change models",
            "Change-driven transformations"
        ],
        "authors": [
            "Gábor Bergmann",
            "István Ráth",
            "Gergely Varró",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-011-0197-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Synchronization of abstract and concrete syntax in domain-speciﬁc modeling languages",
        "submission-date": "2009/01",
        "publication-date": "2009/08",
        "abstract": "Moderndomain-speciﬁcmodeling(DSM)frame-works provide reﬁned techniques for developing new languages based on the clear separation of conceptual elements of the language (called abstract syntax) and their graphical visual representation (called concrete syntax). This separation is usually achieved by recording traceability informa-tion between the abstract and concrete syntax using mapping models. However, state-of-the-art DSM frameworks impose severe restrictions on traceability links between elements of the abstract syntax and the concrete syntax. In the cur-rent paper, we propose a mapping model which allows to deﬁne arbitrarily complex mappings between elements of the abstract and concrete syntax. Moreover, we demonstrate how live model transformations can complement mapping mod-els in providing bidirectional synchronization and implicit traceability between models of the abstract and the concrete syntax. In addition, we introduce a novel architecture for DSM environments which enables these concepts, and pro-vide an overview of the tool support.",
        "keywords": [
            "Domain-speciﬁc modeling languages",
            "Model synchronization",
            "Live model transformations",
            "Traceability"
        ],
        "authors": [
            "István Ráth",
            "András Ökrös",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-009-0122-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Cost-sensitive precomputation of real-time-aware reconﬁguration strategies based on stochastic priced timed games",
        "submission-date": "2023/05",
        "publication-date": "2024/08",
        "abstract": "In many recent application domains, software systems must repeatedly reconﬁgure themselves at runtime to satisfy changing contextual requirements. To decide which next conﬁguration is presumably best suited is a very challenging task as it involves not only functional requirements but also non-functional properties (NFP). NFP include multiple, potentially contradicting, criteria like real-time constraints and cost measures like energy consumption. Effectiveness of context-aware reconﬁguration decisions further depends on mostly uncertain future contexts which makes greedy one-step decision heuristics potentially misleading. Moreover, the computational runtime overhead for reconﬁguration planning should not nullify the beneﬁts. Nevertheless, entirely pre-planning reconﬁguration decisions during design time is also not feasible due to missing knowledge about runtime contexts. In this article, we propose a model-based technique for precomputing context-aware reconﬁguration decisions under partially uncertain real-time constraints and cost measures. We employ a game-theoretic approach based on stochastic priced timed game automata as reconﬁguration model. This formal model allows us to automatically synthesize winning strategies for the ﬁrst player (the system) which efﬁciently delivers presumably best-ﬁtting reconﬁguration decisions as reactions to moves of the second player (the context) at runtime. Our tool implementation copes with the high computational complexity of strategy synthesis by utilizing the statistical model checker Uppaal Stratego to approximate near-optimal solutions. We applied our tool to a real-world example consisting of a reconﬁgurable robot support system for the construction of aircraft fuselages. Our evaluation results show that Uppaal Stratego is indeed able to precompute effective reconﬁguration strategies within a reasonable amount of time.",
        "keywords": [
            "Stochastic priced timed games",
            "Proactive self-adaptation",
            "Strategy synthesis",
            "Statistical model checking"
        ],
        "authors": [
            "Hendrik Göttmann",
            "Birte Caesar",
            "Lasse Beers",
            "Malte Lochau",
            "Andy Schürr",
            "Alexander Fay"
        ],
        "file_path": "data/sosym-all/s10270-024-01195-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Runtime translation of OCL-like statements on Simulink models: Expanding domains and optimising queries",
        "submission-date": "2020/05",
        "publication-date": "2021/08",
        "abstract": "Open-source model management frameworks such as OCL and ATL tend to focus on manipulating models built atop the Eclipse Modelling Framework (EMF), a de facto standard for domain speciﬁc modelling. MATLAB Simulink is a widely used proprietary modelling framework for dynamic systems that is built atop an entirely different technical stack to EMF. To leverage the facilities of open-source model management frameworks with Simulink models, these can be transformed into an EMF-compatible representation. Downsides of this approach include the synchronisation of the native Simulink model and its EMF representation as they evolve; the completeness of the EMF representation, and the transformation cost which can be crippling for large Simulink models. We propose an alternative approach to bridge Simulink models with open-source model management frameworks that uses an “on-the-ﬂy” translation of model management constructs into MATLAB statements. Our approach does not require an EMF representation and can mitigate the cost of the upfront transformation on large models. To evaluate both approaches we measure the performance of a model validation process with Epsilon (a model management framework) on a sample of large Simulink models available on GitHub. Our previous results suggest that, with our approach, the total validation time can be reduced by up to 80%. In this paper, we expand our approach to support the management of Simulink requirements and dictionaries, and we improve the approach to perform queries on collections of model elements more efﬁciently. We demonstrate the use of the Simulink requirements and dictionaries with a case study and we evaluate the optimisations on collection queries with an experiment that compares the performance of a set of queries on models with different sizes. Our results suggest an improvement by up to 99% on some queries.",
        "keywords": [
            "Model driven engineering",
            "Interoperability",
            "Epsilon",
            "MATLAB Simulink",
            "Query optimisation",
            "Eclipse Modelling Framework"
        ],
        "authors": [
            "Beatriz A. Sanchez",
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Dimitris Kolovos",
            "Richard F. Paige",
            "Justin C. Cooper",
            "Jason Hampson"
        ],
        "file_path": "data/sosym-all/s10270-021-00910-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon, OCL"
        }
    },
    {
        "title": "Realizing Model Transformation Chain interoperability",
        "submission-date": "2009/11",
        "publication-date": "2010/10",
        "abstract": "A single Model Transformation Chain (MTC) takes a high-level input model rooted in the problem domain and through one or more transformation steps produces a low-level output model rooted in the solution domain. To build a single “almighty” MTC that is in charge of every design, implementation and speciﬁc platform concern is a complex task. Instead, we can use several smaller MTCs that are easier to develop and maintain, because each MTC is independently developed focusing on a speciﬁc concern. However, the MTCs must interoperate to produce complete applications; this inherently creates dependencies between them, because each MTC generates a part of the ﬁnal low-level model. In this paper, we propose an external and explicit mechanism to track dependencies between the MTCs (i.e., the MTCs are oblivious to the mechanism), which is used to automatically derive correspondence relationships between the ﬁnal models generated by each MTC. The contribution of our mechanism is the reduction of complexity of building interoperable MTCs because the derived correspondences are resolved after the transformations execution, in the solu-tion domain where the semantics of every concept is well-deﬁned. The resolution process consists of (1) checking the consistency between the models, (2) producing communica-tion bridges or (3) guiding the composition of the models. This paper presents three case studies to illustrate the deriva-tion and resolution of correspondence relationships through the MTCs.",
        "keywords": [
            "Software Engineering",
            "Model-driven Engineering",
            "Model Transformation Chains",
            "Interoperability"
        ],
        "authors": [
            "Andrés Yie",
            "Rubby Casallas",
            "Dirk Deridder",
            "Dennis Wagelaar"
        ],
        "file_path": "data/sosym-all/s10270-010-0179-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reference architectures modelling and compliance checking",
        "submission-date": "2021/08",
        "publication-date": "2022/08",
        "abstract": "Reference architectures (RAs) are successfully used to represent families of concrete software architectures in several domains such as automotive, banking, and the Internet of Things. RAs inspire architects when designing concrete architectures, and they help to guarantee compliance with architectural decisions, regulatory requirements, as well as architectural qualities. Despite their importance, reference architectures still suffer from a number of open technical issues, including (i) the lack of a common interpretation, a precise notation for their representation and documentation, and (ii) the lack of conformance mechanisms for checking the compliance of concrete architectures to their related reference architecture, architectural decisions, regulatory requirements, etc. This paper addresses these two issues by introducing a model-driven approach that leverages (i) a domain-independent metamodel for the representation of reference architectures and (ii) the combination of model transformation and weaving techniques for the automatic conformance checking of concrete architectures. We evaluate the applicability, effectiveness, and generalizability of our approach using illustrative examples from the web browsers and automotive domains, including an assessment from an independent practitioner.",
        "keywords": [
            "Model-driven Engineering",
            "Software architecture",
            "Reference architecture"
        ],
        "authors": [
            "Alessio Bucaioni",
            "Amleto Di Salle",
            "Ludovico Iovino",
            "Ivano Malavolta",
            "Patrizio Pelliccione"
        ],
        "file_path": "data/sosym-all/s10270-022-01022-z.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Analysing the Linux kernel feature model changes using FMDiff",
        "submission-date": "2014/10",
        "publication-date": "2015/05",
        "abstract": "Evolving a large scale, highly variable system is a challenging task. For such a system, evolution operations often require to update consistently both their implementation and its feature model. In this context, the evolution of the feature model closely follows the evolution of the system. The purpose of this work is to show that ﬁne-grained feature changes can be used to guide the evolution of the highly variable system. In this paper, we present an approach to obtain ﬁne-grained feature model changes with its supporting tool “FMDiff”. Our approach is tailored for Kconﬁg-based variability models and proposes a feature change classiﬁcation detailing changes in features, their attributes and attribute values. We apply our approach to the Linux kernel feature model, extracting feature changes occurring in sixteen ofﬁcial releases. In contrast to previous studies, we found that feature modiﬁcations are responsible for most of the changes. Then, by taking advantage of the multi-platform aspect of the Linux kernel, we observe the effects of a feature change across the different architecture-speciﬁc feature models of the kernel. We found that between 10 and 50% of feature changes impact all the architecture-speciﬁc feature models, offering a new perspective on studies of the evolution of the Linux feature model and development practices of its developers.",
        "keywords": [
            "Software product line",
            "Feature model",
            "Evolution"
        ],
        "authors": [
            "Nicolas Dintzner",
            "Arie van Deursen",
            "Martin Pinzger"
        ],
        "file_path": "data/sosym-all/s10270-015-0472-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving the definition of general constraints in UML",
        "submission-date": "2007/03",
        "publication-date": "2008/01",
        "abstract": "An important aspect in the speciﬁcation of conceptual schemas is the definition of general constraints that cannot be expressed by the predeﬁned constructs provided by conceptual modeling languages. This is generally achieved by using general-purpose languages like OCL. In this paper we propose a new approach that facilitates the definition of such general constraints in UML. More precisely, we deﬁne a proﬁle that extends the set of predeﬁned UML constraints by adding certain types of constraints that are commonly used in conceptual schemas. We also show how our proposal facili-tates reasoning about the constraints and their automatic code generation, study the application of our ideas to the speciﬁ-cation of two real-life applications, and present a prototype tool implementation.",
        "keywords": [
            "Conceptual modeling",
            "Integrity constraints",
            "UML proﬁle"
        ],
        "authors": [
            "Dolors Costal",
            "Cristina Gómez",
            "Anna Queralt",
            "Ruth Raventós",
            "Ernest Teniente"
        ],
        "file_path": "data/sosym-all/s10270-007-0078-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using contexts to extract models from code",
        "submission-date": "2014/06",
        "publication-date": "2015/05",
        "abstract": "Behaviour models facilitate the understanding and analysis of software systems by providing an abstract view of their behaviours and also by enabling the use of validation and veriﬁcation techniques to detect errors. However, depending on the size and complexity of these systems, constructing models may not be a trivial task, even for experienced developers. Model extraction techniques can automatically obtain models from existing code, thus reducing the effort and expertise required of engineers and helping avoid errors often present in manually constructed models. Existing approaches for model extraction often fail to produce faithful models, either because they only consider static information, which may include infeasible behaviours, or because they are based only on dynamic information, thus relying on observed executions, which usually results in incomplete models. This paper describes a model extraction approach based on the concept of contexts, which are abstractions of concrete states of a program, combining static and dynamic information. Contexts merge some of the advantages of using either type of information and, by their combination, can overcome some of their problems. The approach is partially implemented by a tool called LTS Extractor, which translates information collected from execution traces produced by instrumented Java code to labelled transition systems (LTS), which can be analysed in an existing veriﬁcation tool. Results from case studies are presented and discussed, showing that, considering a certain level of abstraction and a set of execution traces, the produced models are correct descriptions of the programs from which they were extracted. Thus, they can be used for a variety of analyses, such as program understanding, validation, veriﬁcation, and evolution.",
        "keywords": [
            "Behaviour models",
            "Model extraction",
            "Model analysis"
        ],
        "authors": [
            "Lucio Mauro Duarte",
            "Jeff Kramer",
            "Sebastian Uchitel"
        ],
        "file_path": "data/sosym-all/s10270-015-0466-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Proactive modeling: a new model intelligence technique",
        "submission-date": "2013/08",
        "publication-date": "2015/04",
        "abstract": "This article discusses a model intelligence tech-nique called proactive modeling. The goal of proactive modeling is to reduce the amount of manual modeling required when using a graphical DSML and to assist in step-by-step creation of a model. Proactive modeling accomplishes this goal by examining the metamodels syntax and constraints, automatically executing model modiﬁcations, and prompting the modeler for assistance when more than one valid model modiﬁcation exists, but none are neces-sary. We have integrated proactive modeling into the generic modeling environment (GME) as a generic add-on that can operate on any domain-speciﬁc modeling language imple-mented in GME. Lastly, results from applying proactive modeling to several DSMLs in GME show that it can reduce modeling effort.",
        "keywords": [
            "Proactive modeling",
            "Model intelligence",
            "Domain-speciﬁc modeling language",
            "Model-driven engineering"
        ],
        "authors": [
            "Tanumoy Pati",
            "Sowmya Kolli",
            "James H. Hill"
        ],
        "file_path": "data/sosym-all/s10270-015-0465-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "What makes a good modeling research contribution?",
        "submission-date": "2023/11",
        "publication-date": "2024/04",
        "abstract": "The modeling ﬁeld is rapidly evolving and expanding to address new research topics and to connect with new disciplines. As such, what constituted a good modeling research contribution ten years ago may not be the same today. We try to distill some insights of what we (and the community we aim to represent) consider today as key elements of a good research paper in the ﬁeld of software and systems modeling. Such insights—which will need to evolve and adapt with time—will be useful not just for authors of new papers, but also for reviewers and editors.",
        "keywords": [
            "Modeling",
            "Science",
            "Relevance",
            "Writing",
            "Community"
        ],
        "authors": [
            "Richard F. Paige",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-024-01177-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From model transformation to incremental bidirectional model synchronization",
        "submission-date": "2007/03",
        "publication-date": "2008/03",
        "abstract": "The model-driven software development paradigm requires that appropriate model transformations are applicable in different stages of the development process. The transformations have to consistently propagate changes between the different involved models and thus ensure a proper model synchronization. However, most approaches today do not fully support the requirements for model synchronization and focus only on classical one-way batch-oriented transformations. In this paper, we present our approach for an incremental model transformation which supports model synchronization. Our approach employs the visual, formal, and bidirectional transformation technique of triple graph grammars. Using this declarative speciﬁcation formalism, we focus on the efﬁcient execution of the transformation rules and how to achieve an incremental model transformation for synchronization purposes. We present an evaluation of our approach and demonstrate that due to the speedup for the incremental processing in the average case even larger models can be tackled.",
        "keywords": [
            "Model transformation",
            "Incremental model synchronization",
            "Triple graph grammars"
        ],
        "authors": [
            "Holger Giese",
            "Robert Wagner"
        ],
        "file_path": "data/sosym-all/s10270-008-0089-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the theme issue on models for quality of software architecture",
        "submission-date": "2013/08",
        "publication-date": "2014/01",
        "abstract": "High quality of system architecture is essential for the long-term success of a software product. Software engineers need to design and implement secure, maintainable, usable, efficient, and reliable software systems. They have to design architectures that can fulfill quality requirements, judge quality tradeoffs, and ensure that the implementation adheres to the architecture. In case of long-living software systems, they have to monitor, evaluate, and improve quality continuously throughout the entire software lifecycle. Models play a key role in supporting software engineers to master these tasks. The goal of this special theme issue is to emphasize the deep relationship between system modeling and the quality of system architecture. For example, how can model-driven techniques for system development be used to assess and improve the quality of system architecture? What are the connections between software modeling tools and quality assessment tools and environments? Papers in this theme issue are a step toward bridging the gap between theory and practice of architecture quality and system modeling.",
        "keywords": [],
        "authors": [
            "Dorina C. Petriu",
            "Jens Happe"
        ],
        "file_path": "data/sosym-all/s10270-013-0373-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Coordinating large distributed relational process structures",
        "submission-date": "2019/11",
        "publication-date": "2020/11",
        "abstract": "Representing a business process as a collaboration of interacting processes has become feasible with the emergence of\ndata-centric business process management paradigms. Usually, these interacting processes have relations and, thereby, form\na complex relational process structure. The interactions of processes within this relational process structure need to be\ncoordinated to arrive at a meaningful overall business goal. However, relational process structures may become arbitrarily\nlarge. With the use of cloud technology, they may additionally be distributed over multiple nodes, allowing for scalability.\nCoordination processes have been proposed to coordinate relational process structures, where processes may have one-to-many and\nmany-to-many relations at run-time. This paper shows how multiple coordination processes can be used in a decentralized fashion to more efﬁciently coordinate large, distributed process structures. The main challenge of using multiple coordination processes is to effectively realize the coordination responsibility of each coordination process. Key components of the solution are the subsidiary principle and the hierarchy of the relational process structure. Finally, an implementation of the coordination process concept based on microservices was developed, which allows for fast and concurrent enactment of multiple, decentralized coordination processes in large, distributed process structures.",
        "keywords": [
            "Process interactions",
            "Relational process structure",
            "Coordination process",
            "Distributed process execution",
            "BPM in the cloud"
        ],
        "authors": [
            "Sebastian Steinau",
            "Kevin Andrews",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-020-00835-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lifting transformational models of product lines: a case study",
        "submission-date": "2008/11",
        "publication-date": "2009/10",
        "abstract": "Model driven engineering (MDE) of software product lines (SPLs) merges two increasing important paradigms that synthesize programs by transformation. MDE creates programs by transforming models, and SPLs elaborate programs by applying transformations called features. In this paper, we present the design and implementation of a transformational model of a product line of scalar vector graphics and JavaScript applications. We explain how we simpliﬁed our implementation by lifting selected features and their compositions from our original product line (whose implementations were complex) to features and their compositions of another product line (whose speciﬁcations were simple). We used operators to map higher-level features and their compositions to their lower-level counterparts. Doing so exposed commuting relationships among feature compositions in both product lines that helped validate our model and implementation.",
        "keywords": [
            "Transformation reuse",
            "Code generation",
            "Model composition",
            "High-level transformations",
            "Features",
            "Product-lines",
            "Model driven engineering"
        ],
        "authors": [
            "Greg Freeman",
            "Don Batory",
            "Greg Lavender",
            "Jacob Neal Sarvela"
        ],
        "file_path": "data/sosym-all/s10270-009-0131-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How low-code platforms support digital twins of processes",
        "submission-date": "2025/05",
        "publication-date": "2025/07",
        "abstract": "Digital Twin of Processes, also deﬁned as process digital twins (PDTs), are emerging as a feasible solution for modeling, monitoring, and optimizing business processes by providing real-time, data-driven insights into operational workﬂows. However, designing, developing, and maintaining PDTs can be complex and resource-intensive, often requiring highly specialized expertise in software engineering and domain-speciﬁc processes. This paper proposes insights and guidelines into using low-code development platforms (LCDPs) to simplify and expedite the modeling and deployment of PDTs, leveraging intuitive, visual development environments and pre-built components. We identiﬁed 11 core characteristics that deﬁne PDTs and assessed the potential of LCDPs to support their design, development, and execution. The applicability of this framework is demonstrated through three case studies of littering management, order management, and guest invitations, where we evaluate how well LCDPs address the key requirements of PDT implementation. Our results indicate that while LCDPs offer signiﬁcant advantages in terms of ease of adoption and cost efﬁciency, several challenges remain, particularly around scalability and process performance. At the same time, we propose lessons learned from these experiences that could help address these challenges in future implementations.",
        "keywords": [
            "Process digital twin",
            "Low-code development platform",
            "Workﬂow management system",
            "Workﬂow modeling"
        ],
        "authors": [
            "Arianna Fedeli",
            "Amleto Di Salle",
            "Daniela Micucci",
            "Luciana Rebelo",
            "Maria Teresa Rossi",
            "Leonardo Mariani",
            "Ludovico Iovino"
        ],
        "file_path": "data/sosym-all/s10270-025-01310-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the uniﬁcation power of models",
        "submission-date": "2003/11",
        "publication-date": "2005/05",
        "abstract": "In November 2000, the OMG made public the MDATMinitiative, a particular variant of a new global trend called MDE (Model Driven Engineering). The basic ideas of MDA are germane to many other approaches such as generative programming, domain speciﬁc languages, model-integrated computing, generic model management, software factories, etc. MDA may be deﬁned as the realization of MDE principles around a set of OMG standards like MOF, XMI, OCL, UML, CWM, SPEM, etc. MDE is presently making several promises about the potential beneﬁts that could be reaped from a move from code-centric to model-based practices. When we observe these claims, we may wonder when they may be satisﬁed: on the short, medium or long term or even never perhaps for some of them. This paper tries to propose a vision of the development of MDE based on some lessons learnt in the past 30 years in the development of object technology. The main message is that a basic principle (“Everything is an object”) was most helpful in driving the technology in the direction of simplicity, generality and power of integration. Similarly in MDE, the basic principle that “Everything is a model” has many interesting properties, among others the capacity to generate a realistic research agenda. We postulate here that two core relations (representation and conformance) are associated to this principle, as inheritance and instantia-tion were associated to the object uniﬁcation principle in the class-based languages of the 80’s. We suggest that this may be most useful in understanding many questions about MDE in general and the MDA approach in particular. We provide some illustrative examples. The personal position taken in this paper would be useful if it could generate a critical debate on the research directions in MDE.",
        "keywords": [
            "MDE",
            "MDA",
            "Models",
            "Metamodels"
        ],
        "authors": [
            "Jean B´ezivin"
        ],
        "file_path": "data/sosym-all/s10270-005-0079-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scalable process discovery and conformance checking",
        "submission-date": "2015/10",
        "publication-date": "2016/07",
        "abstract": "Considerable amounts of data, including process events, are collected and stored by organisations nowadays. Discovering a process model from such event data and ver-iﬁcation of the quality of discovered models are important steps in process mining. Many discovery techniques have been proposed, but none of them combines scalability with strong quality guarantees. We would like such techniques to handle billions of events or thousands of activities, to produce sound models (without deadlocks and other anomalies), and to guarantee that the underlying process can be rediscov-ered when sufﬁcient information is available. In this paper, we introduce a framework for process discovery that ensures these properties while passing over the log only once and introduce three algorithms using the framework. To measure the quality of discovered models for such large logs, we introduce a model–model and model–log comparison framework that applies a divide-and-conquer strategy to measure recall, ﬁtness, and precision. We experimentally show that these discovery and measuring techniques sacriﬁce little compared to other algorithms, while gaining the ability to cope with event logs of 100,000,000 traces and processes of 10,000 activities on a standard computer.",
        "keywords": [
            "Big data",
            "Scalable process mining",
            "Block-structured process discovery",
            "Directly-follows graphs",
            "Algorithm evaluation",
            "Rediscoverability",
            "Conformance checking"
        ],
        "authors": [
            "Sander J. J. Leemans",
            "Dirk Fahland",
            "Wil M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-016-0545-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Linking physicality and computation",
        "submission-date": "2016/02",
        "publication-date": "2017/12",
        "abstract": "Cyber-physical systems have developed into a very active research ﬁeld, with a broad range of challenges and research directions going from requirements, to implementation and simulation, as well as validation and veriﬁcation to guarantee essential properties. In this survey paper, we focus exclusively on the following fundamental issue: how to link physicality and computation, continuous time-space dynamics with discrete untimed ones? We consider that cyber-physical system design ﬂow involves the following three main steps: (1) cyber-physical systems modeling; (2) discretization for executability; and (3) simulation and implementation. We review—and strive to provide insight into possible approaches for addressing—the key issues, for each of these three steps.",
        "keywords": [
            "Cyber-physical systems design",
            "Structural equational modeling",
            "Modelica",
            "Linear graphs",
            "Bond graphs",
            "Idealization",
            "Abstraction",
            "Hybrid dataﬂow networks",
            "Discretization",
            "Language embedding"
        ],
        "authors": [
            "Simon Bliudze",
            "Sébastien Furic",
            "Joseph Sifakis",
            "Antoine Viel"
        ],
        "file_path": "data/sosym-all/s10270-017-0642-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MDA Tool Components: a proposal for packaging know-how in model driven development",
        "submission-date": "2006/04",
        "publication-date": "2007/05",
        "abstract": "As the Model Driven Development (MDD) and Product Line Engineering (PLE) appear as major trends for reducing software development complexity and costs, an important missing stone becomes more visible: there is no standard and reusable assets for packaging the know-how and artifacts required when applying these approaches. To overcome this limit, we introduce in this paper the notion of MDA Tool Component, i.e., a packaging unit for encapsulating business know-how and required resources in order to support speciﬁc modeling activities on a certain kind of model. The aim of this work is to provide a standard way for representing this know-how packaging unit. This is done by introducing a two-layer MOF-compliant metamodel. Whilst the ﬁrst layer focuses on the deﬁnition of the structure and contents of the MDA Tool Component, the second layer introduces a language independent way for describing its behavior. An OMG RFP (Request For Proposal) has been issued in order to standardize this approach.",
        "keywords": [
            "MDD",
            "Packaging of know-how",
            "MDA Tool Component",
            "Reusability"
        ],
        "authors": [
            "Reda Bendraou",
            "Philippe Desfray",
            "Marie-Pierre Gervais",
            "Alexis Muller"
        ],
        "file_path": "data/sosym-all/s10270-007-0058-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "AuRUS: explaining the validation of UML/OCL conceptual schemas",
        "submission-date": "2011/12",
        "publication-date": "2013/06",
        "abstract": "The validation and the veriﬁcation of concep-tual schemas have attracted a lot of interest during the last years, and several tools have been developed to automate this process as much as possible. This is achieved, in general, by assessing whether the schema satisﬁes different kinds of desirable properties which ensure that the schema is correct. In this paper we describe AuRUS, a tool we have developed to analyze UML/OCL conceptual schemas and to explain their (in)correctness. When a property is satisﬁed, AuRUS provides a sample instantiation of the schema showing a particular situation where the property holds. When it is not, AuRUS provides an explanation for such unsatisﬁability, i.e., a set of integrity constraints which is in contradiction with the property.",
        "keywords": [
            "Validation",
            "Conceptual modeling",
            "UML",
            "OCL",
            "Automated reasoning",
            "Explanation"
        ],
        "authors": [
            "Guillem Rull",
            "Carles Farré",
            "Anna Queralt",
            "Ernest Teniente",
            "Toni Urpí"
        ],
        "file_path": "data/sosym-all/s10270-013-0350-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From process mining to augmented process execution",
        "submission-date": "2023/07",
        "publication-date": "2023/11",
        "abstract": "Business process management (BPM) is a well-established discipline comprising a set of principles, methods, techniques, and tools to continuously improve the performance of business processes. Traditionally, most BPM decisions and activities are undertaken by business stakeholders based on manual data collection and analysis techniques. This is time-consuming and potentially leads to suboptimal decisions, as only a restricted subset of data and options are considered. Over the past decades, a rich set of data-driven techniques has emerged to support and automate various activities and decisions across the BPM lifecycle, particularly within the process mining ﬁeld. More recently, the uptake of artiﬁcial intelligence (AI) methods for BPM has led to a range of approaches for proactive business process monitoring. Given their common data requirements and overlapping goals, process mining and AI-driven approaches to business process optimization are converging. This convergence is leading to a promising emerging concept, which we call (AI-)augmented process execution: a collection of data analytics and artiﬁcial intelligence methods for continuous and automated improvement and adaptation of business processes. This article gives an outline of research at the intersection between process mining and AI-driven process optimization, classiﬁes the researched techniques based on their scope and objectives, and positions augmented process execution as an additional layer on top of this stack.",
        "keywords": [
            "Business process management",
            "Predictive analytics",
            "Prescriptive analytics",
            "Autonomous systems"
        ],
        "authors": [
            "David Chapela-Campa",
            "Marlon Dumas"
        ],
        "file_path": "data/sosym-all/s10270-023-01132-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "SoSyM reﬂections of 2016: a journal status report",
        "submission-date": "2016/00",
        "publication-date": "2017/01",
        "abstract": "The past year has been an exciting time for SoSyM with the celebration of our 15th Anniversary, the addi- tion of new Editors, and further collaboration with the MODELS conference (e.g., SoSyM awards at MODELS and the SoSyM Journal-ﬁrst arrangement). The ﬁrst issue of this new year’s volume summarizes the status of SoSyM in terms of recent statistics and milestone events over the past year.",
        "keywords": [],
        "authors": [
            "Geri Georg",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-017-0582-0.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "An elucidation of blended modeling from an industrial perspective",
        "submission-date": "2024/11",
        "publication-date": "2024/12",
        "abstract": "Model-Driven Engineering (MDE) has been widely adopted across various industrial sectors due to its ability to manage the complexity of modern engineering products. However, traditional modeling languages and tools are often limited to a single, specific concrete syntax, which poses challenges for the diverse stakeholders involved in the modeling process.. To address these limitations, the emerging field of blended modeling introduces the use of multiple concrete syntaxes, and in some cases, even multiple abstract syntaxes, for representing the same information. In this expert perspective, we present generalized, technology-agnostic concepts developed within a European research and development project focused on blended modeling. Specifically, we contribute a standardized terminology and ontology for blended modeling, along with a methodology for creating blended modeling environments. These concepts were developed through collaboration between academic and industrial partners, who aligned on the motivations and benefits of this approach. The insights gained from this project are not only relevant to blended MDE but also can be applied to traditional MDE practices.",
        "keywords": [
            "Model-Driven Engineering",
            "Blended modeling",
            "Terminologies",
            "Ontologies",
            "Methodologies"
        ],
        "authors": [
            "Jörg Holtmann",
            "Federico Ciccozzi",
            "Wim Bast",
            "Joost van Pinxten"
        ],
        "file_path": "data/sosym-all/s10270-024-01246-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the interpretation of binary associations in the Uniﬁed Modelling Language",
        "submission-date": "2002/02",
        "publication-date": "2002/09",
        "abstract": "Binary associations between classiﬁers are among the most fundamental of UML concepts. However, there is considerable room for disagreement concerning what an association is, semantically; it turns out that at least two diﬀerent notions are called Associations. This confusion of concepts gives rise to unnecessary compli-cation and ambiguity in the language, which have impli-cations for the modeller because they can result in seri-ous misunderstandings of static structure diagrams; sim-ilarly, they have implications for tool developers. In this paper we explore these issues, suggest improvements and clariﬁcations, and demonstrate side-beneﬁts that would accrue.",
        "keywords": [
            "UML",
            "Association",
            "Link"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-002-0002-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal semantics of static and temporal state-oriented OCL constraints",
        "submission-date": "2003/02",
        "publication-date": "2003/07",
        "abstract": "The textual Object Constraint Language (OCL) is primarily intended to specify restrictions over UML class diagrams, in particular class invariants, operation pre-, and postconditions. Based on several improvements in the deﬁnition of the language concepts in last years, a proposal for a new version of OCL has recently been published [43]. That document provides an extensive OCL semantic description that constitutes a tight integration into UML. However, OCL still lacks a semantic integration of UML Statecharts, although it can already be used to refer to states in OCL expressions. This article presents an approach that closes this gap and introduces a formal semantics for such integration through a mathematical model. It also presents the deﬁn-ition of a temporal OCL extension by means of a UML Proﬁle based on the metamodel of the latest OCL proposal. Our OCL extension enables modelers to specify behavioral state-oriented real-time constraints. It provides an intuitive understanding and readability at application level since common OCL syntax and concepts are pre-served. A well-deﬁned formal semantics is given through the mapping of temporal OCL expressions to temporal logics formulae.",
        "keywords": [
            "Object Constraint Language",
            "UML Statecharts",
            "UML Proﬁle",
            "Real-time constraints",
            "Temporal logics"
        ],
        "authors": [
            "Stephan Flake",
            "Wolfgang Mueller"
        ],
        "file_path": "data/sosym-all/s10270-003-0026-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model checking multi-level and recursive nets",
        "submission-date": "2015/03",
        "publication-date": "2016/01",
        "abstract": "With the increasing complexity of the problems and systems arising nowadays, the use of multi-level models is becoming more frequent in practice. However, there are still few reports in the literature concerning methods for analyzing such models without ﬂattening the multi-level structure. For instance, several variants of multi-level Petri nets have been applied for modeling interaction protocols and mobility in multi-agent systems and coordination of cross-organizational workﬂows. But there are few automated tools for analyzing the behavior of these nets. In this paper we explain how to detect faults in models based on a representative class of multi-level nets: the nested Petri nets. We translate a nested net into a veriﬁable model that preserves its modular structure, a PROMELA program. This allows the use of SPIN model checker to verify properties related to termination, boundedness and reachability.",
        "keywords": [
            "Multi-level modeling",
            "Nested Petri nets",
            "Model checking",
            "SPIN"
        ],
        "authors": [
            "Mirtha Lina Fernández Venero",
            "Flávio Soares Corrêa da Silva"
        ],
        "file_path": "data/sosym-all/s10270-015-0509-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bidirectional model transformations in QVT: semantic issues and open questions",
        "submission-date": "2008/04",
        "publication-date": "2008/12",
        "abstract": "We consider the OMG’s queries, views and transformations standard as applied to the speciﬁcation of bidirectional transformations between models. We discuss what is meant by bidirectional transformations, and the model-driven development scenarios in which they are needed. We analyse the fundamental requirements on tools which support such transformations, and discuss some semantic issues which arise. In particular, we show that any transformation language sufﬁcient to the needs of model-driven development would have to be able to express non-bijective transformations. We argue that a considerable amount of basic research is needed before suitable tools will be fully realisable, and suggest directions for this future research.",
        "keywords": [
            "Bidirectional model transformation",
            "QVT",
            "Model-driven development",
            "Semantics"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-008-0109-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "Feature-based classification of bidirectional transformation approaches",
        "submission-date": "2012/07",
        "publication-date": "2015/01",
        "abstract": "Bidirectional model transformation is a key technology in model-driven engineering (MDE), when two models that can change over time have to be kept constantly consistent with each other. While several model transformation tools include at least a partial support to bidirectionality, it is not clear how these bidirectional capabilities relate to each other and to similar classical problems in computer science, from the view update problem in databases to bidirectional graph transformations. This paper tries to clarify and visualize the space of design choices for bidirectional transformations from an MDE point of view, in the form of a feature model. The selected list of existing approaches are characterized by mapping them to the feature model. Then, the feature model is used to highlight some unexplored research lines in bidirectional transformations.",
        "keywords": [
            "Bidirectional transformation",
            "Feature model",
            "Domain analysis"
        ],
        "authors": [
            "Soichiro Hidaka",
            "Massimo Tisi",
            "Jordi Cabot",
            "Zhenjiang Hu"
        ],
        "file_path": "data/sosym-all/s10270-014-0450-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multi-perspective enterprise modeling: foundational concepts, prospects and future research challenges",
        "submission-date": "2011/11",
        "publication-date": "2012/08",
        "abstract": "The paper presents a method for multi-perspective enterprise modeling (MEMO) and a corresponding (meta-) modeling environment. An extensive analysis of requirements for enterprise modeling serves to motivate and assess the method. The method is based on an elaborate conception of multi-perspective enterprise models and on an extensible language architecture. The language architecture is comprised of a meta modeling language and an extensible set of integrated domain-speciﬁc modeling languages (DSML). The DSML are supplemented with process models and with guidelines for their reﬂective use. The corresponding modeling environment integrates editors for various DSML into multi-language model editors. It includes a meta model editor which enables the convenient use, development and extension of the set of supported DSML and supports the generation of respective graphical model edi-tors. Thus, it also serves as a foundation for method engineering. MEMO covers both software engineering as well as social, managerial and economic aspects of the ﬁrm. The presentation of MEMO is supplemented with a comparative overview of other approaches to enterprise modeling. The paper concludes bys summarizing fundamental technical, epistemological and political challenges for enterprise modeling research and discusses potential paths for future research.",
        "keywords": [
            "Enterprise modeling",
            "Domain-speciﬁc modeling language (DSML)",
            "Method engineering",
            "Modeling tool",
            "Reference model"
        ],
        "authors": [
            "Ulrich Frank"
        ],
        "file_path": "data/sosym-all/s10270-012-0273-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Promoting social diversity for the automated learning of complex MDE artifacts",
        "submission-date": "2021/04",
        "publication-date": "2022/01",
        "abstract": "Software modeling activities typically involve a tedious and time-consuming effort by specially trained personnel. This lack of automation hampers the adoption of model-driven engineering (MDE). Nevertheless, in the recent years, much research work has been dedicated to learn executable MDE artifacts instead of writing them manually. In this context, mono- and multi-objective genetic programming (GP) has proven being an efﬁcient and reliable method to derive automation knowledge by using, as training data, a set of examples representing the expected behavior of an artifact. Generally, conformance to the training example set is the main objective to lead the learning process. Yet, single ﬁtness peak, or local optima deadlock, a common challenge in GP, hinders the application of GP to MDE. In this paper, we propose a strategy to promote populations’ social diversity during the GP learning process. We evaluate our approach with an empirical study featuring the case of learning well-formedness rules in MDE with a multi-objective genetic programming algorithm. Our evaluation shows that integration of social diversity leads to more efﬁcient search, faster convergence, and more generalizable results. Moreover, when the social diversity is used as crowding distance, this convergence is uniform through a hundred of runs despite the probabilistic nature of GP. It also shows that genotypic diversity strategies cannot achieve comparable results.",
        "keywords": [
            "Genetic programming",
            "Model-driven engineering",
            "Social diversity"
        ],
        "authors": [
            "Edouard R. Batot",
            "Houari Sahraoui"
        ],
        "file_path": "data/sosym-all/s10270-021-00969-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Cross-platform edge deployment of machine learning models: a model-driven approach",
        "submission-date": "2024/07",
        "publication-date": "2025/01",
        "abstract": "Deploying machine learning (ML) models on edge devices presents unique challenges, arising from the different environments used for developing ML models and those required for their deployment, leading to a gray area of competence and expertise between ML engineers and application developers. In this paper, we explore the use of model-driven engineering to simplify the deployment of ML models on edge devices, speciﬁcally smartphones. We present a DSL for the speciﬁcation of the ML serving pipelines (pre- and postprocessing of data before and after inference), together with a model interpretation approach that allows to make changes to the pipeline during runtime, thus removing the need to re-release an application upon changes to a pipeline. We followed a design science approach, in which we elicited requirements through an initial artifact study and interviews with engineers at an industrial partner. This was followed by the design and implementation of a lightweight, JSON-based domain-speciﬁc language designed to describe ML serving pipelines, along with an accompanying Flutter library to execute the pipelines during runtime. A preliminary evaluation with four developers shows the potential of this approach to increase development speed, decrease the amount of code required to make changes to an ML serving pipeline, and make less-experienced engineers more conﬁdent contributing to the domain.",
        "keywords": [
            "Model-driven engineering",
            "MDE4AI",
            "AI engineering",
            "Mobile applications"
        ],
        "authors": [
            "Albin Karlsson Landgren",
            "Philip Perhult Johnsen",
            "Daniel Strüber"
        ],
        "file_path": "data/sosym-all/s10270-025-01273-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling event-based communication in component-based software architectures for performance predictions",
        "submission-date": "2012/02",
        "publication-date": "2013/03",
        "abstract": "Event-based communication is used in different\ndomains including telecommunications, transportation, and\nbusiness information systems to build scalable distributed\nsystems. Such systems typically have stringent requirements\nfor performance and scalability as they provide business and\nmission critical services. While the use of event-based com-\nmunication enables loosely-coupled interactions between\ncomponents and leads to improved system scalability, it\nmakes it much harder for developers to estimate the sys-\ntem’s behavior and performance under load due to the decou-\npling of components and control ﬂow. In this paper, we\npresent our approach enabling the modeling and performance\nprediction of event-based systems at the architecture level.\nApplying a model-to-model transformation, our approach\nintegrates platform-speciﬁc performance inﬂuences of the\nunderlying middleware while enabling the use of differ-\nent existing analytical and simulation-based prediction tech-\nniques. In summary, the contributions of this paper are:\n(1) the development of a meta-model for event-based com-\nmunication at the architecture level, (2) a platform aware\nmodel-to-model transformation, and (3) a detailed evaluation\nof the applicability of our approach based on two represen-\ntative real-world case studies. The results demonstrate the\neffectiveness, practicability and accuracy of the proposed\nmodeling and prediction approach.",
        "keywords": [
            "Event-based",
            "Performance model",
            "Performance evaluation",
            "Software architecture",
            "Component-based"
        ],
        "authors": [
            "Christoph Rathfelder",
            "Benjamin Klatt",
            "Kai Sachs",
            "Samuel Kounev"
        ],
        "file_path": "data/sosym-all/s10270-013-0316-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A modelling and simulation based process for dependable systems design",
        "submission-date": "2006/02",
        "publication-date": "2007/04",
        "abstract": "Complex real-time system design needs to address dependability requirements, such as safety, reliability, and security. We introduce a modelling and simulation based approach which allows for the analysis and prediction of dependability constraints. Dependability can be improved by making use of fault tolerance techniques. The de-facto example, in the real-time system literature, of a pump control system in a mining environment is used to demonstrate our model-based approach. In particular, the system is modelled using the Discrete EVent system Speciﬁcation (DEVS) formalism, and then extended to incorporate fault tolerance mechanisms. The modularity of the DEVS formalism facilitates this extension. The simulation demonstrates that the employed fault tolerance techniques are effective. That is, the system performs satisfactorily despite the presence of faults. This approach also makes it possible to make an informed choice between different fault tolerance techniques. Performance metrics are used to measure the reliability and safety of the system, and to evaluate the dependability achieved by the design. In our model-based development process, modelling, simulation and eventual deployment of the system are seamlessly integrated.",
        "keywords": [],
        "authors": [
            "Miriam Zia",
            "Sadaf Mustaﬁz",
            "Hans Vangheluwe",
            "Jörg Kienzle"
        ],
        "file_path": "data/sosym-all/s10270-007-0050-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven approach for vulnerability evaluation of modern physical protection systems",
        "submission-date": "2015/06",
        "publication-date": "2016/12",
        "abstract": "Modern physical protection systems integrate a number of security systems (including procedures, equipments, and personnel) into a single interface to ensure an adequate level of protection of people and critical assets against malevolent human actions. Due to the critical functions of a protection system, the quantitative evaluation of its effectiveness is an important issue that still raises several challenges.Inthispaperweproposeamodel-drivenapproach to support the design and the evaluation of physical protection systems based on (a) UML models representing threats, protection facilities, assets, and relationships among them, and (b) the automatic construction of a Bayesian Network model to estimate the vulnerability of different system configurations. Hence, the proposed approach is useful both in the context of vulnerability assessment and in designing new security systems as it enables what-if and cost–benefit analyses. A real-world case study is further illustrated in order to validate and demonstrate the potentiality of the approach. Specifically, two attack scenarios are considered against the depot of a mass transit transportation system in Milan, Italy.",
        "keywords": [
            "Physical security",
            "Vulnerability",
            "CIP_VAM UML profile",
            "Bayesian Network",
            "Model transformation",
            "Railway infrastructure system"
        ],
        "authors": [
            "Annarita Drago",
            "Stefano Marrone",
            "Nicola Mazzocca",
            "Roberto Nardone",
            "Annarita Tedesco",
            "Valeria Vittorini"
        ],
        "file_path": "data/sosym-all/s10270-016-0572-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A search-based approach for detecting circular dependency bad smell in goal-oriented models",
        "submission-date": "2020/10",
        "publication-date": "2022/01",
        "abstract": "Goal-oriented models are gaining signiﬁcant attention from researchers and practitioners in various domains, especially in software requirements engineering. Similar to other software engineering models, goal models are subject to bad practices (i.e., bad smells). Detecting and rectifying these bad smells would improve the quality of these models. In this paper, we formally deﬁne the circular dependency bad smell and then develop an approach based on the simulated annealing (SA) search-based algorithm to detect its instances. Furthermore, we propose two mechanisms (namely, pruning and pairing) to improve the effectiveness of the proposed approach. We empirically evaluate three algorithm combinations, i.e., (1) the base SA search algorithm, (2) the base SA search algorithm augmented with pruning mechanism, and (3) the base SA search algorithm augmented with pruning and pairing mechanisms, using several case studies. Results show that simulated annealing augmented with pruning and pairing is the most effective approach, while the simulated annealing augmented with pruning mechanism is more effective than the base SA search algorithm. We also found that the proposed pruning and pairing mechanisms provide a signiﬁcant improvement in the detection of circular dependency bad smell, in terms of computation time and accuracy.",
        "keywords": [
            "Circular dependency",
            "Model-driven engineering",
            "Requirements",
            "Simulated annealing",
            "GRL"
        ],
        "authors": [
            "Mawal A. Mohammed",
            "Mohammad Alshayeb",
            "Jameleddine Hassine"
        ],
        "file_path": "data/sosym-all/s10270-021-00965-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven engineering of middleware-based ubiquitous services",
        "submission-date": "2011/10",
        "publication-date": "2013/04",
        "abstract": "Supporting the execution of service-oriented applications over ubiquitous networks specifically calls for a service-oriented middleware (SOM), which effectively enables ubiquitous networking while benefiting from the diversity and richness of the networking infrastructure. However, developing ubiquitous applications that exploit the specific features offered by a SOM might be a time-consuming task, which demands a deep knowledge spanning from the application domain concepts down to the underlying middleware technicalities. In this paper, first we present the model-driven development process underpinning ubiSOAP, a SOM for the ubiquitous networking domain. Then, based on the domain concepts defined by the conceptual model of ubiSOAP, its architecture and its technicalities, we propose a domain-specific environment, called ubiDSE, that aids the development of applications that exploits the ubiSOAP features, from design to implementation. ubiDSE allows developers to focus on the main behavior of the modeled systems, rather than on complex details inherent to ubiquitous environments. As part of ubiDSE, specific tools are provided to automatically generate skeleton code for service-oriented applications to be executed on ubiSOAP-enabled devices, hence facilitating the exploitation of ubiSOAP by developers.",
        "keywords": [
            "Service-oriented development",
            "Model-driven service engineering",
            "Service-oriented middleware",
            "Ubiquitous computing"
        ],
        "authors": [
            "Marco Autili",
            "Mauro Caporuscio",
            "Valérie Issarny",
            "Luca Berardinelli"
        ],
        "file_path": "data/sosym-all/s10270-013-0344-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction to: Enforcing ﬁne-grained access control for secure collaborative modelling using bidirectional transformations",
        "submission-date": "2017/11",
        "publication-date": "2018/01",
        "abstract": "The article “Enforcing ﬁne-grained access control for secure collaborativemodellingusingbidirectionaltransformations”, written by Csaba Debreceni, Gábor Bergmann, István Ráth, Dániel Varró, was originally published electronically on the publisher’s internet portal (https://link.springer.com/journal/10270) on [11/21/2017 6:24:42 AM] without open access.",
        "keywords": [],
        "authors": [
            "Csaba Debreceni",
            "Gábor Bergmann",
            "István Ráth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-017-0650-5.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Approaches to modeling business processes: a critical analysis of BPMN, workﬂow patterns and YAWL",
        "submission-date": "2011/08",
        "publication-date": "2011/09",
        "abstract": "We investigate three approaches describing mod-\nmodels of business processes: the OMG standard BPMN in its\nrecent version 2.0, the workﬂow patterns of the Workﬂow\nPattern Initiative and their reference implementation YAWL.\nWe show how the three approaches fail to provide practitio-\nners with a suitable means precisely and faithfully to capture\nbusiness scenarios and to analyze, communicate and manage\nthe resulting models. On the positive side, we distill from the\ndiscussion six criteria which can help to recognize practical\nand reliable tool-supported business process description and\nmodeling systems.",
        "keywords": [
            "Business process modeling",
            "BPMN",
            "Workﬂow patterns",
            "YAWL"
        ],
        "authors": [
            "Egon Börger"
        ],
        "file_path": "data/sosym-all/s10270-011-0214-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Spectra: a speciﬁcation language for reactive systems",
        "submission-date": "2020/02",
        "publication-date": "2021/04",
        "abstract": "We introduce Spectra, a new speciﬁcation language for reactive systems, speciﬁcally tailored for the context of reactive synthesis. The meaning of Spectra is deﬁned by a translation to a kernel language. Spectra comes with the Spectra Tools, a set of analyses, including a synthesizer to obtain a correct-by-construction implementation, several means for executing the resulting controller, and additional analyses aimed at helping engineers write higher-quality speciﬁcations. We present the language in detail and give an overview of its tool set. Together with the language and its tool set, we present four collections of many, non-trivial, large speciﬁcations, written by undergraduate computer science students for the development of autonomous Lego robots and additional example reactive systems. The collected speciﬁcations can serve as benchmarks for future studies on reactive synthesis. We present the speciﬁcations, with observations and lessons learned about the potential use of reactive synthesis by software engineers.",
        "keywords": [
            "Reactive synthesis",
            "GR(1)",
            "Speciﬁcation language"
        ],
        "authors": [
            "Shahar Maoz",
            "Jan Oliver Ringert"
        ],
        "file_path": "data/sosym-all/s10270-021-00868-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integrating smart contracts into the modeling paradigm to harness the potential of models",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Despite the increasing interest in blockchain and smart contracts, their inherent complexity has impeded widespread adoption. In order to mitigate this issue, this work introduces SmaC, a model-based framework for the development of smart contracts in Solidity that enables the treatment of contracts as models, opening up new possibilities for their enhancement and maintenance. A key beneﬁt of SmaC is its ability to impose a development pattern, which contributes to improved code quality and reduced vulnerabilities. The framework’s effectiveness is evaluated through several case studies, showing how model-driven engineering can mitigate contracts inherent complexity and promote better collaboration between developers and domain experts. As this work will demonstrate, when smart contracts are treated as models, a vast array of possibilities unfolds.",
        "keywords": [
            "Model-driven engineering",
            "Domain-speciﬁc languages",
            "Smart contracts"
        ],
        "authors": [
            "Cristian Gómez-Macías",
            "Francisco Javier Pérez-Blanco",
            "David Granada",
            "Juan Manuel Vara"
        ],
        "file_path": "data/sosym-all/s10270-024-01260-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Redescription mining-based business process deviance analysis",
        "submission-date": "2023/01",
        "publication-date": "2024/11",
        "abstract": "Business processes often deviate from their expected or desired behavior. Such deviations can be either positive or negative, depending on whether or not they lead to better process performance. Deviance mining addresses the problem of identifying such deviations and explaining why a process deviates. In this paper, we propose a novel approach to identify and explain the causes of deviant process executions based on the technique of redescription mining, which extracts knowledge in the form of logical rules. By analyzing, comparing, and ﬁltering these rules, the reasons for the deviant behaviors of a business process are identiﬁed both in general and for particular process instances. Afterward, the results of this analysis are transformed into a concise and well-readable natural language text that can be used by business analysts and process owners to optimize processes in a reasoned manner. We evaluate our approach from different angles using four process models and provide some advice for further optimization.",
        "keywords": [
            "Deviance mining",
            "Redescription mining",
            "Process mining",
            "Natural language generation"
        ],
        "authors": [
            "Engjëll Ahmeti",
            "Martin Käppel",
            "Stefan Jablonski"
        ],
        "file_path": "data/sosym-all/s10270-024-01231-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Test model coverage analysis under uncertainty: extended version",
        "submission-date": "2020/02",
        "publication-date": "2021/02",
        "abstract": "In model-based testing, we may have to deal with a non-deterministic model, e.g. because abstraction was applied, or because the software under test itself is non-deterministic. The same test case may then trigger multiple possible execution paths, depending on some internal decisions made by the software. Consequently, performing precise test analyses, e.g. to calculate the test coverage, are not possible.. This can be mitigated if developers can annotate the model with estimated probabilities for taking each transition. A probabilistic model checking algorithm can subsequently be used to do simple probabilistic coverage analysis. However, in practice developers often want to know what the achieved aggregate coverage is, which unfortunately cannot be re-expressed as a standard model checking problem. This paper presents an extension to allow efﬁcient calculation of probabilistic aggregate coverage, and also of probabilistic aggregate coverage in combination with k-wise coverage.",
        "keywords": [
            "Probabilistic model based testing",
            "Probabilistic test coverage",
            "Testing non-deterministic systems"
        ],
        "authors": [
            "I. S. W. B. Prasetya\nRick Klomp"
        ],
        "file_path": "data/sosym-all/s10270-020-00848-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "RoboChart: modelling and veriﬁcation of the functional behaviour of robotic applications",
        "submission-date": "2018/06",
        "publication-date": "2019/01",
        "abstract": "Robots are becoming ubiquitous: from vacuum cleaners to driverless cars, there is a wide variety of applications, many with potential safety hazards. The work presented in this paper proposes a set of constructs suitable for both modelling robotic applications and supporting veriﬁcation via model checking and theorem proving. Our goal is to support roboticists in writing models and applying modern veriﬁcation techniques using a language familiar to them. To that end, we present RoboChart, a domain-speciﬁc modelling language based on UML, but with a restricted set of constructs to enable a simpliﬁed semantics and automated reasoning. We present the RoboChart metamodel, its well-formedness rules, and its process-algebraic semantics. We discuss veriﬁcation based on these foundations using an implementation of RoboChart and its semantics as a set of Eclipse plug-ins called RoboTool.",
        "keywords": [
            "State machines",
            "Formal semantics",
            "Process algebra",
            "CSP",
            "Model checking",
            "Timed properties",
            "Domain-speciﬁc language for robotics"
        ],
        "authors": [
            "Alvaro Miyazawa",
            "Pedro Ribeiro",
            "Wei Li",
            "Ana Cavalcanti",
            "Jon Timmis",
            "Jim Woodcock"
        ],
        "file_path": "data/sosym-all/s10270-018-00710-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model transformations and tool integration",
        "submission-date": "2003/12",
        "publication-date": "2004/11",
        "abstract": "Model transformations are increasingly recog-\nnised as being of signiﬁcant importance to many areas of\nsoftware development and integration. Recent attention\non model transformations has particularly focused on the\nOMG’s Queries/Views/Transformations (QVT) Request\nfor Proposals (RFP). In this paper I motivate the need for\ndedicated approaches to model transformations, particu-\nlarly for the data involved in tool integration, outline the\nchallenges involved, and then present a number of tech-\nologies and techniques which allow the construction of\nﬂexible, powerful and practical model transformations.",
        "keywords": [
            "Modelling",
            "Transformations",
            "Model transformations",
            "QVT",
            "Tool integration"
        ],
        "authors": [
            "Laurence Tratt"
        ],
        "file_path": "data/sosym-all/s10270-004-0070-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT"
        }
    },
    {
        "title": "What will it take? A view on adoption of model-based methods in practice",
        "submission-date": "2011/12",
        "publication-date": "2012/08",
        "abstract": "Model-basedengineering(MBE)hasbeentouted\nas a new and substantively different approach to software\ndevelopment, characterized by higher levels of abstraction\nand automation compared to traditional methods. Despite\nthe availability of published veriﬁable evidence that it can\nsigniﬁcantly boost both developer productivity and product\nquality in industrial projects, adoption of this approach has\nbeen surprisingly slow. In this article, we review the causes\nbehind this, both technical and non-technical, and outline\nwhat needs to happen for MBE to become a reliable main-\nstream approach to software development.",
        "keywords": [
            "Model-based engineering"
        ],
        "authors": [
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-012-0261-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An interactive tool for UML class model evolution in database applications",
        "submission-date": "2012/08",
        "publication-date": "2013/09",
        "abstract": "In the context of model-driven development of\ndatabase applications with UML, the (usually relational)\ndatabase schema is obtained automatically from the appli-\ncation’s structural (class) UML model. Changes in require-\nments often lead to modiﬁcations of the application’s struc-\ntural model. Such changes, in turn, have to be propagated\nto the underlying database schema. Very often, especially\nwhen the system is in production with a large volume of\nusers’ live data, the data is considered to be valuable enough\nto be preserved through these changes. This paper describes\nan approach to cope with the problem of model evolution\nwith the ultimate requirement to preserve the data stored in\nthe database. The algorithm interactively determines differ-\nences between structural UML models before and after the\nchanges and resolves those differences into transformations\nin the relational database domain.",
        "keywords": [
            "Model evolution",
            "Schema evolution",
            "Model differencing",
            "Schema matching",
            "Object-relational mapping"
        ],
        "authors": [
            "Vukasin Milovanovic",
            "Dragan Milicev"
        ],
        "file_path": "data/sosym-all/s10270-013-0378-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving active participation during enterprise operations modeling with an extended story-card-method and participative modeling software",
        "submission-date": "2022/06",
        "publication-date": "2023/03",
        "abstract": "The COVID-19 pandemic emphasized the need for process automation, using agile software development practices. However,\nwhen agile methods are used in scaled contexts, many software development efforts fail, mainly due to lacking requirements\nengineering practices. When business-oriented software needs to be developed within a scaled context, the story-card method\n(SCM), developed as part of a previous study, assists in structuring emerging software requirements within a taxonomy that\nrepresents enterprise operation. The SCM helps agile team members to develop a common understanding about enterprise\noperation when they construct the enterprise operation taxonomy. Digital participatory enterprise modeling (PEM) may\nincrease collaboration and understanding among team members, especially when team members are geographically dis-\npersed, when they co-model their understanding of enterprise operations. Using design science research to further evolve the\nexisting SCM, we identiﬁed two concerns regarding the existing SCM: (1) The modeling software did not encourage active\nparticipation during modeling, and (2) Low quality of the resulting cooperation structure diagram (CSD) that is used to derive\nan enterprise operation taxonomy, i.e., the need to further extend the existing SCM. As main contribution of this article, we\naddressed previous deﬁciencies of the SCM, developing an extended SCM (eSCM), based on principles and guidelines that\nwould encourage online participation during PEM, also providing a comprehensive case to demonstrate the eSCM. As a\nsecond contribution, we used survey-feedback from research participants, as well as activity tracking to evaluate whether the\nmodeling tool encouraged active PEM. Our third contribution is to evaluate the quality of the resulting CSDs with suggestions\nfor future improvement.",
        "keywords": [
            "Participative enterprise modeling",
            "Participative modeling software",
            "Scaled agile",
            "DEMO"
        ],
        "authors": [
            "Marne De Vries",
            "Petra Opperman"
        ],
        "file_path": "data/sosym-all/s10270-023-01083-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section on uncertainty, modeling, CPSs and AI: honoring Prof. Antonio Vallecillo",
        "submission-date": "2025/03",
        "publication-date": "2025/04",
        "abstract": "This special section contains high-quality papers in the topics of Prof. Antonio Vallecillo’s research. This set of papers has been compiled in appreciation to Prof. Vallecillo’s career and his contributions to the software engineering community in general and the modeling community in particular. After his recent retirement, this special section opens with a hindsight to Prof. Vallecillo’s fruitful and varied career, where he made contributions in several different dimensions. Then, a set of selected papers in the ﬁelds of uncertainty, modeling, CPSs and AI to honor Prof. Vallecillo’s contributions is presented.",
        "keywords": [
            "Model-based software engineering",
            "Modeling foundations",
            "Uncertainty modeling",
            "CPS",
            "Artiﬁcial intelligence"
        ],
        "authors": [
            "Javier Troya\nAlfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-025-01285-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Fast test suite-driven model-based fault localisation with application to pinpointing defects in student programs",
        "submission-date": "2016/06",
        "publication-date": "2017/07",
        "abstract": "Fault localisation, i.e. the identiﬁcation of program locations that cause errors, takes signiﬁcant effort and cost. We describe a fast model-based fault localisation algorithm that, given a test suite, uses symbolic execution methods to fully automatically identify a small subset of program locations where genuine program repairs exist. Our algorithm iterates over failing test cases and collects locations where an assignment change can repair exhibited faulty behaviour. Our main contribution is an improved search through the test suite, reducing the effort for the symbolic execution of the models and leading to speed-ups of more than two orders of magnitude over the previously published implementation by Griesmayer et al. We implemented our algorithm for C programs, using the KLEE symbolic execution engine, and demonstrate its effectiveness on the Siemens TCAS variants. Its performance is in line with recent alternative model-based fault localisation techniques, but narrows the location set further without rejecting any genuine repair locations where faults can be ﬁxed by changing a single assignment. We also show how our tool can be used in an educational context to improve self-guided learning and accelerate assessment. We apply our algorithm to a large selection of actual student coursework submissions, provid-ing precise localisation within a sub-second response time. We show this using small test suites, already provided in the coursework management system, and on expanded test suites, demonstrating the scalability of our approach. We also show compliance with test suites does not reliably grade a class of “almost-correct” submissions, which our tool highlights, as being close to the correct answer. Finally, we show an extension to our tool that extends our fast localisation results to a selection of student submissions that contain two faults.",
        "keywords": [
            "Automated debugging",
            "Model-based fault localisation",
            "Symbolic execution",
            "Automated assessment"
        ],
        "authors": [
            "Geoff Birch",
            "Bernd Fischer",
            "Michael Poppleton"
        ],
        "file_path": "data/sosym-all/s10270-017-0612-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A repository for scalable model management",
        "submission-date": "2012/03",
        "publication-date": "2013/03",
        "abstract": "Applying model-driven engineering (MDE) in industrial-scale systems requires managing complex models which may be very large. These models must be persisted in a way that allows their manipulation by client applications without fully loading them. In this paper, we propose Morsa, a model repository that provides scalable manipulation of large models through load on demand and incremental store; model persistence is supported by a NoSQL database. We discuss some load on demand and incremental store algorithms as well as a database design. A prototype that integrates transparently with EMF is presented, and its evaluation demonstrates that it is capable of fully managing large models with a limited amount of memory. Moreover, a set of benchmarks has been executed, exhibiting better performance than the EMF XMI file-based persistence and the most widely used model repository, CDO.",
        "keywords": [
            "MDE",
            "Model persistence",
            "Model repositories",
            "Model scalability",
            "Large models",
            "NoSQL",
            "Document databases"
        ],
        "authors": [
            "Javier Espinazo Pagán",
            "Jesús Sánchez Cuadrado",
            "Jesús García Molina"
        ],
        "file_path": "data/sosym-all/s10270-013-0326-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework for relating syntactic and semantic model differences",
        "submission-date": "2016/01",
        "publication-date": "2016/08",
        "abstract": "Abstract Model differencing is an important activity in model-based development processes. Differences need to be detected, analyzed, and understood to evolve systems and explore alternatives. Two distinct approaches have been studied in the literature: syntactic differencing, which compares the concrete or abstract syntax of models, and semantic differencing, which compares models in terms of their meaning. Syntactic differencing identifies change operations that transform the syntactical representation of one model to the syntactical representation of the other. However, it does not explain their impact on the meaning of the model. Semantic model differencing is independent of syntactic changes and presents differences as elements in the semantics of one model but not the other. However, it does not reveal the syntactic changes causing these semantic differences. We define Diffuse, a language-independent, abstract framework, which relates syntactic change operations and semantic difference witnesses. We formalize fundamental relations of necessary, exhibiting, and sufficient sets of change operations and analyze their properties. We further demonstrate concrete instances of the Diffuse framework for three different popular modeling languages, namely class diagrams, activity diagrams, and feature models. The Diffuse framework provides a novel foundation for combining syntactic and semantic differencing.",
        "keywords": [
            "Model differencing",
            "Semantics",
            "Model evolution"
        ],
        "authors": [
            "Shahar Maoz",
            "Jan Oliver Ringert"
        ],
        "file_path": "data/sosym-all/s10270-016-0552-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How to deﬁne modeling languages?",
        "submission-date": "2022/09",
        "publication-date": "2023/03",
        "abstract": "At the end of September 2022 at RWTH Aachen, Germany, there was a meeting attended by several experts on software language development with a speciﬁc emphasis on modeling. The LangDev meetings are particularly dedicated to the exchange of new ideas and innovations around the deﬁnition and use of Domain-Speciﬁc Languages (DSLs). Developers and users from multiple research and tooling groups attended, including representation from Essential, Freon (formerly ProjectIT), Gemoc, Langium, LinGo, MontiCore, MPS, Rascal, SpooFax, StarLasu, and Stratego. Attendees presented and discussed new trends and developments in their domains. Several practitioners also demonstrated new applications and uses of DSLs in various application domains, ranging from the European Union Digital COVID(-19) Certiﬁcate (the EU Digital Covid Certiﬁcate), to industrial printing systems, and digital twins, to name a few. LangDev 22 presented a very good balance among practitioners and researchers, as well as industrial and academic partners. The meeting welcomed a large number of new and young participants, which is a good sign for the community. Industrial partners included language workbench providers, and language workbench users, with impressive applications ranging from the EU DCC to examples in scientiﬁc computing. As the theme for this editorial, we want to share our impressions from that meeting as well as some general observations.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01098-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Coupling solvers with model transformations to generate explorable model sets",
        "submission-date": "2020/04",
        "publication-date": "2021/02",
        "abstract": "Model transformation is an effective technique to produce target models from source models. Most transformation approaches focus on generating a single target model from a given source model. However, there are situations where a collection of possible target models is preferred over a single one. Such situations arise when some choices cannot be encoded in the transformation. Then, search techniques can be used to help ﬁnd a target model having speciﬁc properties. In this paper, we presentanapproachthatcombinesmodeltransformationandconstraintprogrammingtogenerateexplorablesetsofmodels.We extend previous work by adding support for multiple solvers, as well as extending ATL, a declarative transformation language used to write such transformations. We evaluate our approach and language on a task scheduling case study including both scheduling constraints and schedule visualization.",
        "keywords": [
            "Model transformation",
            "Constraint solving",
            "Model set exploration"
        ],
        "authors": [
            "Théo Le Calvar",
            "Fabien Chhel",
            "Frédéric Jouault",
            "Frédéric Saubion"
        ],
        "file_path": "data/sosym-all/s10270-021-00867-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Extending interaction overview diagrams with activity diagram constructs",
        "submission-date": "2007/05",
        "publication-date": "2009/02",
        "abstract": "UML2.0 introduced interaction overview diag-rams (IODs) as a way of specifying relationships between UML interactions. IODs are a variant of activity diagrams that show control ﬂow between a set of interactions. The nodes in an IOD are either inline interactions or references to an interaction. A number of recent papers have deﬁned a formal semantics for IODs. These are restricted, however, to interactions that can be speciﬁed using basic sequence diagrams. This excludes the many rich modeling constructs available in activity diagrams such as interruptible regions, activity groups, concurrent node executions, and ﬂow ﬁnal nodes. It is non-trivial to allow such constructs in IODs be-cause their meaning has to be interpreted in the context of interaction sequences rather than activities. In this paper, we consider how some of these activity diagram constructs can be used practically in IODs. We motivate the integration of these constructs into IODs using a NASA air trafﬁc control subsystem and deﬁne a formal semantics for these constructs that builds on an existing semantics definition for IODs.",
        "keywords": [
            "UML",
            "Interactions",
            "Activity diagrams",
            "Formal semantics"
        ],
        "authors": [
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-009-0114-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal translation of YAWL workﬂow models to the Alloy formal speciﬁcations: a testing application",
        "submission-date": "2021/07",
        "publication-date": "2022/11",
        "abstract": "Within microservice architecture-based systems, some microservices are integrated to build the software. The integration of these services may be deﬁned based on a workﬂow model. There are also a variety of different languages available for deﬁning these workﬂow models. BPMN and YAWL are two such options. It is important that testers test the integration of these microservices. This paper proposes the formal method as the solution for integration testing. This method translates the workﬂow model to the Alloy. The algorithm has suggested a translation of workﬂow models to formal speciﬁcations. This speciﬁcation takes into consideration both structural and behavioral aspects. The ﬁrst perspective is about general structures, while the second is about the behavior of the objects in a speciﬁc model. We have proved the correctness of the suggested speciﬁcations. For this purpose, the paper has shown that formal deﬁnitions are sound and are complete with nine theorems for these properties. The translation from YAWL to Alloy is deﬁned based on their BNF grammar. The generated model is an appropriate source for different purposes containing software testing. The suggested method for software testing is model-driven testing. Logical predicates deﬁne the structure of Alloy models. This method uses these logical predicates for generating tests. The test method has used RACC coverage as an example criterion. Alloy Analyzer tests the model by generating test predicates.",
        "keywords": [
            "Integration testing",
            "Alloy",
            "Formal methods",
            "YAWL",
            "Workﬂow models",
            "Microservice architecture"
        ],
        "authors": [
            "Mehran Rivadeh",
            "Seyed-Hassan Mirian-Hosseinabadi"
        ],
        "file_path": "data/sosym-all/s10270-022-01043-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Enhancing software model encoding for feature location approaches based on machine learning techniques",
        "submission-date": "2020/02",
        "publication-date": "2021/08",
        "abstract": "Feature location is one of the main activities performed during software evolution. In our previous works, we proposed an approach for feature location in models based on machine learning, providing evidence that machine learning techniques can obtain better results than other retrieval techniques for feature location in models. However, to apply machine learning techniques optimally, the design of an encoding is essential to be able to identify the best realization of a feature. In this work, we present more thorough research about software model encoding for feature location approaches based on machine learning. As part of this study, we have provided two new software model encodings and compared them with the source encoding. The ﬁrst proposed encoding is an extension of the source encoding to take advantage of not only the main concepts and relations of a domain but also the properties of these concepts and relations. The second proposed encoding is inspired by the characteristics used in benchmark datasets for research on Learning to Rank. Afterward, the new encodings are used to compare three different machine learning techniques (RankBoost, Feedforward Neural Network, and Recurrent Neural Network). The study also considers whether a domain-independent encoding such as the ones proposed in this work can outperform an encoding that is speciﬁcally designed to exploit human experience and domain knowledge. Furthermore, the results of the best encoding and the best machine learning technique were compared to two traditional approaches that have been widely applied for feature location as well as for traceability link recovery and bug localization. The evaluation is based on two real-world case studies, one in the railway domain and the other in the induction hob domain. An approach for feature location in models evaluates these case studies with the different encodings and machine learning techniques. The results show that when using the second proposed encoding and RankBoost, the approach outperforms the results of the other encodings and machine learning techniques and the results of the traditional approaches. Speciﬁcally, the approach achieved the best results for all the performance indicators, providing a mean precision value of 90.11%, a recall value of 86.20%, a F-measure value of 87.22%, and a MCC value of 0.87. The statistical analysis of the results shows that this approach signiﬁcantly improves the results and increases the magnitude of the improvement. The promising results of this work can serve as a starting point toward the use of machine learning techniques in other engineering tasks with software models, such as traceability or bug location.",
        "keywords": [
            "Software models",
            "Feature location",
            "Machine learning",
            "Learning to Rank",
            "Neural networks",
            "Encoding"
        ],
        "authors": [
            "Ana C. Marcén",
            "Francisca Pérez",
            "Óscar Pastor",
            "Carlos Cetina"
        ],
        "file_path": "data/sosym-all/s10270-021-00920-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling context-aware and intention-aware in-car infotainment systems\nConcepts and modeling processes",
        "submission-date": "2016/02",
        "publication-date": "2016/07",
        "abstract": "It is fundamental to understand users’ intentions\nto support them when operating a computer system with a\ndynamically varying set of functions, e.g., within an in-car\ninfotainment system. The system needs to have sufﬁcient\ninformation about its own and the user’s context to pre-\ndict those intentions. Although the development of current\nin-car infotainment systems is already model-based, explic-\nitly gathering and modeling contextual information and user\nintentions is currently not supported. However, manually\ncreating software that understands the current context and\npredicts user intentions is complex, error-prone and expen-\nsive.Model-baseddevelopmentcanhelpinovercomingthese\nissues. In this paper, we present an approach for modeling\na user’s intention based on Bayesian networks. We sup-\nport developers of in-car infotainment systems by providing\nmeans to model possible user intentions according to the cur-\nrent context. We further allow modeling of user preferences\nand show how the modeled intentions may change during\nrun-time as a result of the user’s behavior. We demonstrate\nfeasibility of our approach using an industrial case study of\nan intention-aware in-car infotainment system. Finally, we\nshow how modeling of contextual information and modeling\nuser intentions can be combined by using model transforma-\ntion.",
        "keywords": [
            "Context-aware",
            "Intention-aware",
            "Modeling",
            "Infotainment"
        ],
        "authors": [
            "Daniel Lüddecke",
            "Christoph Seidl",
            "Jens Schneider",
            "Ina Schaefer"
        ],
        "file_path": "data/sosym-all/s10270-016-0543-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multi-paradigm modelling for cyber–physical systems: a descriptive framework",
        "submission-date": "2020/04",
        "publication-date": "2021/06",
        "abstract": "The complexity of cyber–physical systems (CPSs) is commonly addressed through complex workﬂows, involving models in a plethora of different formalisms, each with their own methods, techniques, and tools. Some workﬂow patterns, combined with particular types of formalisms and operations on models in these formalisms, are used successfully in engineering practice. To identify and reuse them, we refer to these combinations of workﬂow and formalism patterns as modelling paradigms. This paper proposes a unifying (Descriptive) Framework to describe these paradigms, as well as their combinations. This work is set in the context of Multi-Paradigm Modelling (MPM), which is based on the principle to model every part and aspect of a system explicitly, at the most appropriate level(s) of abstraction, using the most appropriate modelling formalism(s) and workﬂows. The purpose of the Descriptive Framework presented in this paper is to serve as a basis to reason about these formalisms, workﬂows, and their combinations. One crucial part of the framework is the ability to capture the structural essence of a paradigm through the concept of a paradigmatic structure. This is illustrated informally by means of two example paradigms commonly used in CPS: Discrete Event Dynamic Systems and Synchronous Data Flow. The presented framework also identiﬁes the need to establish whether a paradigm candidate follows, or qualiﬁes as, a (given) paradigm. To illustrate the ability of the framework to support combining paradigms, the paper shows examples of both workﬂow and formalism combinations. The presented framework is intended as a basis for characterisation and classiﬁcation of paradigms, as a starting point for a rigorous formalisation of the framework (allowing formal analyses), and as a foundation for MPM tool development.",
        "keywords": [
            "Multi-paradigm modelling",
            "Foundations of model-based systems engineering",
            "Cyber",
            "physical systems"
        ],
        "authors": [
            "Moussa Amrani",
            "Dominique Blouin",
            "Robert Heinrich",
            "Arend Rensink",
            "Hans Vangheluwe",
            "Andreas Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-021-00876-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction to: Low-code development and model-driven engineering: Two sides of the same coin?",
        "submission-date": "2022/08",
        "publication-date": "2022/08",
        "abstract": "Section 3 states that \"Codebots [7] uses UML to specify domain models that are consumed to automatically generate target artefacts, including complete REST APIs, client libraries, Swagger API documentation, and a JSON Schema deﬁnition for each domain object.\" This statement is incorrect as Codebots uses a domain-speciﬁc language, not UML, to specify domain models. Therefore, this sentence should instead read \"Codebots [7] uses a domain-speciﬁc language to specify domain models that are consumed to automatically generate target artefacts, including complete REST APIs, client libraries, Swagger API documentation, and a JSON Schema deﬁnition for each domain object.\"",
        "keywords": [],
        "authors": [
            "Davide Di Ruscio",
            "Dimitris Kolovos",
            "Juan de Lara",
            "Alfonso Pierantonio",
            "Massimo Tisi",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-022-01038-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Assessing the impact of meta-model evolution: a measure and its automotive application",
        "submission-date": "2016/05",
        "publication-date": "2017/05",
        "abstract": "Domain-speciﬁc meta-models play an important role in the design of large software systems by deﬁning language for the architectural models. Such common modeling languages are particularly important if multiple actors are involved in the development process as they assure interoperability between modeling tools used by different actors. The main objective of this paper is to facilitate the adoption of new domain-speciﬁc meta-model versions, or a subset of the new architectural features they support, by the architectural modeling tools used by different actors in the development of large software systems. In order to achieve this objective, we developed a simple measure of meta-model evolution (named NoC—Number of Changes) that captures atomic modiﬁcation between different versions of the analyzed meta-model. We evaluated the NoC measure on the evolution of the AUTOSAR meta-model, a domain-speciﬁc meta-model used in the design of automotive system architectures. The evaluation shows that the measure can be used as an indicator of effort needed to update meta-model-based tools to support different actors in modeling new architectural features. Our detailed results show the impact of 14 new AUTOSAR features on the modeling tools used by the main actors in the automotive developmentprocess.Wevalidatedourresultsbyﬁndingasigniﬁcant correlation between the results of the NoC measure and the actual effort needed to support these features in the modeling tools reported by the modeling practitioners from four AUTOSAR tool vendors and the AUTOSAR tooling team at Volvo Cars. Generally, our study shows that quantitative analysis of domain-speciﬁc meta-model evolution using a simple measure such as NoC can be used as an indicator of the required updates in the meta-model-based tools that are needed to support new meta-model versions. However, our study also shows that qualitative analysis that may include an inspection of the actual meta-model changes is needed for more accurate assessment.",
        "keywords": [
            "Domain-speciﬁc meta-models",
            "Modeling tools",
            "Architectural features",
            "Software evolution",
            "Measurement",
            "Automotive software",
            "AUTOSAR"
        ],
        "authors": [
            "Darko Durisic",
            "Miroslaw Staron",
            "Matthias Tichy",
            "Jörgen Hansson"
        ],
        "file_path": "data/sosym-all/s10270-017-0601-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The ForeMoSt approach to building valid model-based safety arguments",
        "submission-date": "2022/03",
        "publication-date": "2022/11",
        "abstract": "Safety assurance cases (ACs) are structured arguments designed to comprehensively show that a system is safe. ACs are often model-based, meaning that a model of the system is a primary subject of the argument. ACs use reasoning steps called strategies to decompose high-level claims about system safety into reﬁned subclaims that can be directly supported by evidence. Strategies are often informal and difﬁcult to rigorously evaluate in practice, and consequently, AC arguments often contain reasoning errors. This has led to the deployment of unsafe systems, and caused severe real-world consequences. These errors can be mitigated by formalizing and verifying AC strategies using formal methods; however, these techniques are difﬁcult to use without formal methods expertise. To mitigate potential challenges faced by engineers when developing and interpreting formal ACs, we present ForeMoSt, our tool-supported framework for rigorously validating AC strategies using the Lean theorem prover. The goal of the framework is to straddle the level of abstraction used by the theorem prover and by software engineers. We use case studies from the literature to demonstrate that ForeMoSt is able to (i) augment and validate ACs from the research literature, (ii) support AC development for systems with large models, and (iii) support different model types.",
        "keywords": [
            "Safety",
            "Assurance cases",
            "Strategies",
            "Theorem proving",
            "Lean"
        ],
        "authors": [
            "Torin Viger",
            "Logan Murphy",
            "Alessio Di Sandro",
            "Claudio Menghi",
            "Ramy Shahin",
            "Marsha Chechik"
        ],
        "file_path": "data/sosym-all/s10270-022-01063-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "FloBP: a model-driven approach for developing and executing IoT-enhanced business processes",
        "submission-date": "2023/06",
        "publication-date": "2024/02",
        "abstract": "The capability to integrate Internet of Things (IoT) technologies into business processes (BPs) has emerged as a transformative paradigm, offering unprecedented opportunities for organisations to enhance their operational efﬁciency and productivity. Interacting with the physical world and leveraging real-world data to make more informed business decisions is of greatest interest, and the idea of IoT-enhanced BPs promises to automate and improve business activities and permit them to adapt to the physical environment of execution. Nonetheless, combining these two domains is challenging, and it requires new modelling methods that do not increase notation complexity and provide independent execution between the process and the underlying device technology. In this work, we propose FloBP, a model-driven engineering approach separating concerns between the IoT and BPs, providing a structured and systematic approach to modelling and executing IoT-enhanced BPs. Applying the separation of concerns through an interdisciplinary team is needed to ensure that the approach covers all necessary process aspects, including technological and modelling ones. The FloBP approach is based on modelling tools and a microservices architecture to deploy BPMN models, and it facilitates integration with the physical world, providing ﬂexibility to support multiple IoT device technologies and their evolution. A smart canteen scenario describes and evaluates the approach’s feasibility and its possible adoption by various stakeholders. The performed evaluation concludes that the application of FloBP facilitates the modelling and development of IoT-enhanced BPs by sharing and reusing knowledge among IoT and BP experts.",
        "keywords": [
            "Internet of Things",
            "Model-driven engineering",
            "Business process",
            "Feature model",
            "IoT-enhanced business process",
            "Microservices"
        ],
        "authors": [
            "Arianna Fedeli",
            "Fabrizio Fornari",
            "Andrea Polini",
            "Barbara Re",
            "Victoria Torres",
            "Pedro Valderas"
        ],
        "file_path": "data/sosym-all/s10270-024-01150-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the impact of size to the understanding of UML diagrams",
        "submission-date": "2015/04",
        "publication-date": "2016/05",
        "abstract": "Background Practical experience suggests that usage and understanding of UML diagrams is greatly affected by the quality of their layout. While existing research failed to provide conclusive and comprehensive evidence in support of this hypothesis, our own previous work provided substantial evidence to this effect, also suggesting diagram size as a relevant factor, for a range of diagram types and layouts.\nAims Since there is no generally accepted precise notion of “diagram size,” we ﬁrst need to operationalize this concept,analyzeitsimpactondiagramunderstanding,andderive practical advice from our ﬁndings.\nMethod Wedeﬁnethreealternative,plausiblemetrics.Since they are all highly correlated on a large sample of UML diagrams, we opt for the simplest one. We use it to re-analyze existing experimental data on diagram understanding.\nResults We ﬁnd a strong negative correlation between diagram size and modeler performance. Our results are statistically highly signiﬁcant and exhibit a very large degree of validity. We utilize these results to derive a recommendation on diagram sizes that are, on average, optimal for model understanding. These recommendations are implemented in a plug-in to a widely used modeling tool, providing continuous feedback about diagram size to modelers.\nConclusions The effect sizes are varying, but generally suggest that the impact of size matches or exceeds that of other factors in diagram understanding. With the guideline and tool, modelers are steered toward avoiding too large diagrams.",
        "keywords": [
            "Diagram understanding",
            "Diagram size metrics",
            "Cognitive load",
            "Experiment",
            "Gestalt principles"
        ],
        "authors": [
            "Harald Störrle"
        ],
        "file_path": "data/sosym-all/s10270-016-0529-x.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Connecting databases with process mining: a meta model and toolset",
        "submission-date": "2016/12",
        "publication-date": "2018/02",
        "abstract": "Process mining techniques require event logs which, in many cases, are obtained from databases. Obtaining these event logs is not a trivial task and requires substantial domain knowledge. In addition, an extracted event log provides only a single view on the database. To change our view, e.g., to focus on another business process and generate another event log, it is necessary to go back to the source of data. This paper proposes a meta model to integrate both process and data perspectives, relating one to the other. It can be used to generate different views from the database at any moment in a highly ﬂexible way. This approach decouples the data extraction from the application of analysis techniques, enabling the application of process mining in different contexts.",
        "keywords": [
            "Process mining",
            "Database",
            "Data schema",
            "Meta model",
            "Event extraction"
        ],
        "authors": [
            "Eduardo González López de Murillas",
            "Hajo A. Reijers",
            "Wil M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-018-0664-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Static slicing of Use Case Maps requirements models",
        "submission-date": "2017/07",
        "publication-date": "2018/06",
        "abstract": "Requirements speciﬁcation is a crucial stage in many software development life cycles. As requirements speciﬁcations evolve, they often become more complex. The development of methods to assist the comprehension and maintenance of requirements speciﬁcations has gained much attention in the past 20years. However, there is much room for improvement for model-based speciﬁcations. The Use Case Maps (UCM) language, part of the ITU-T Z.151 User Requirements Notation standard, is a visual modeling notation that aims to describe requirements at a high level of abstraction. A UCM speciﬁcation is used to integrate and capture both functional aspects (based on causal scenarios representing behavior) and architectural aspects (actors and system components responsible for scenario activities). As UCM models evolve and grow, they rapidly become hard to understand and to maintain. In this paper, we propose a static slicing technique to enhance the comprehension of UCM models. The developed slicing approach is implemented within the jUCMNav tool. We validate the proposed approach using a mock system and three publicly available UCM speciﬁcations. The results suggest that, on average, the models can be reduced by about 70% through slicing without losing information required for comprehension and maintenance activities. A small experiment involving 9 participants also suggests that the understandability of UCM speciﬁcations and comprehension speed have both improved substantially by using jUCMNav’s new slicing feature.",
        "keywords": [
            "Requirements speciﬁcation",
            "Use Case Maps",
            "Slicing",
            "User Requirements Notation",
            "Comprehension",
            "Maintenance"
        ],
        "authors": [
            "Taha Binalialhag",
            "Jameleddine Hassine",
            "Daniel Amyot"
        ],
        "file_path": "data/sosym-all/s10270-018-0680-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The meaning of multiplicity of n-ary associations in UML",
        "submission-date": "2002/01",
        "publication-date": "2002/12",
        "abstract": "The concept of multiplicity in UML derives from that of cardinality in entity-relationship modeling techniques. The UML documentation deﬁnes this concept but at the same time acknowledges some lack of obvious-ness in the speciﬁcation of multiplicities for n-ary associations. This paper shows an ambiguity in the deﬁnition given by UML documentation and proposes a clariﬁcation to this deﬁnition, as well as the use of outer and inner multiplicities as a simple extension to the current nota-tion to represent other multiplicity constraints, such as participation constraints, that are equally valuable in understanding n-ary associations.",
        "keywords": [
            "UML",
            "multiplicity",
            "cardinality",
            "ternary association",
            "n-ary association",
            "ternary relationship",
            "n-ary relationship",
            "data modeling",
            "entity-relationship modeling"
        ],
        "authors": [
            "Gonzalo G´enova",
            "Juan Llorens",
            "Paloma Mart´ınez"
        ],
        "file_path": "data/sosym-all/s10270-002-0009-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Conﬂict management techniques for model merging: a systematic mapping review",
        "submission-date": "2021/09",
        "publication-date": "2022/10",
        "abstract": "Model merging conﬂicts occur when different stakeholders aim to integrate their contradicting changes that are applied con-currently to update software models. We conduct an extensive systematic mapping study on conﬂict management techniques and relevant collaboration attributes to the versioning and merging models from 2001 to the middle of 2021. This study follows the standard guidelines within the software engineering domain. We analyzed a total of 105 articles extracted from an initial pool of more than 1800 articles to infer a taxonomy for conﬂict management techniques. We use this taxonomy to classify existing approaches to understand characteristics, shortcomings, and challenges on conﬂict management techniques in merging models. It also provides a solid foundation for future work in this area. We show that syntactic conﬂicts are the most studied type and that the top three popular conﬂict detection techniques are constraint violation, change overlapping, and pattern matching. We observe the lack of a comprehensive state-of-the-art comparison between academic or industrial tools, as well as the need for real-world case studies. Finally, we show that recent trends have focused on online collaboration, where teams of stakeholders work on large-scale models.",
        "keywords": [
            "Conﬂict management",
            "Model merging conﬂict",
            "Collaborative modeling",
            "Model driven engineering",
            "Taxonomy",
            "Systematic mapping"
        ],
        "authors": [
            "Mohammadreza Sharbaf",
            "Bahman Zamani",
            "Gerson Sunyé"
        ],
        "file_path": "data/sosym-all/s10270-022-01050-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistency requirements in business process modeling: a thorough overview",
        "submission-date": "2017/02",
        "publication-date": "2017/11",
        "abstract": "The ﬁeld of business process modeling has been beset by inter-model consistency problems which are mainly due to the existence of multiple variants of the same busi-ness process, for instance when models have been produced by different actors, or through the time by a same (or dif-ferent) actor(s), as well as the possibility of its modeling from discrete and complementary perspectives (using dif-ferent lenses). Accordingly, our overall aim in this paper is to provide a thorough overview of consistency requirements in business process modeling, which is strongly needed not only for the sake of a comprehensive investigation of this challenging subject, but also for the sake of empowering sig-niﬁcant contributions to it. In order to do so, we opted for a systematic literature review of consistency among busi-ness process models as starting point and basis to attain the intended overview and to guide our contributions in this ﬁeld.",
        "keywords": [
            "Business process modeling",
            "Consistency requirements",
            "Inter-model consistency",
            "Systematic literature review",
            "Context-awareness",
            "Map formalism"
        ],
        "authors": [
            "Afef Awadid",
            "Selmin Nurcan"
        ],
        "file_path": "data/sosym-all/s10270-017-0629-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Using two case studies to explore the applicability of VIATRA for the model-driven engineering of mechatronic production systems",
        "submission-date": "2020/07",
        "publication-date": "2022/02",
        "abstract": "The engineering of mechatronic production systems is complex and requires various disciplines (e.g., systems, mechanical, electrical and software engineers). Model-driven engineering (MDE) supports systems development and the exchange of information based on models and transformations. However, the integration and adoption of different modeling approaches are becoming challenges when it comes to cross-disciplinary work. VIATRA is a long-living enduring and mature modeling framework that offers rich model transformation features to develop MDE applications. This study investigates the extent to which VIATRA can be applied in the engineering of mechatronic production systems. For this purpose, two model transformation case studies are presented: “SysML–AutomationML” and “SysML4Mechatronics–AutomationML.” Both case studies are representative of structural modeling and interdisciplinary data exchange during the development of mechatronic production systems. These case studies are derived from other researchers in the community. A VIATRA software prototype implements these case studies as a batch-oriented transformation and serves as one basis for evaluating VIATRA. To report on our observations and ﬁndings, we built on an evaluation framework from the MDE community. This framework considers 14 different characteristics (e.g., maturity, size, execution time, modularity, learnability), according to the Goal-Question-Metric paradigm. To be able to evaluate our ﬁndings, we compared VIATRA to ATL. We applied all cases to a lab-size mechatronic production system. We found that, with VIATRA, the same functions for model transformation applications can be achieved as with ATL, which is popular for model transformations in both the MDE and the mechatronic production systems commu-nity. VIATRA combines the relational, imperative, and graph-based paradigms and enables the development and execution of model-to-model (M2M) and model-to-text (M2T) transformations. Furthermore, the VIATRA internal DSL is based on Xtend and Java, making VIATRA attractive and intuitive for users with less experience in modeling than in object-oriented programming. Thus, VIATRA leads to an interesting alternative for the model-driven engineering of mechatronic production systems. It has the potential to reduce the complexity during the development of model transformations. To conclude, this paper evaluates the applicability of VIATRA, its strengths and limitations. It provides lessons learned and insights that can stimulate further research in the MDE for mechatronic production systems.",
        "keywords": [
            "Model Transformations",
            "Model-Driven Engineering",
            "VIATRA",
            "Mechatronic Production Systems",
            "Applicability Study"
        ],
        "authors": [
            "Gennadiy Koltun",
            "Mathis Pundel"
        ],
        "file_path": "data/sosym-all/s10270-021-00962-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "VIATRA"
        }
    },
    {
        "title": "Model-based ideal testing of hardware description language (HDL) programs",
        "submission-date": "2020/07",
        "publication-date": "2021/11",
        "abstract": "An ideal test is supposed to show not only the presence of bugs but also their absence. Based on the Fundamental Test Theory of Goodenough and Gerhart (IEEE Trans Softw Eng SE-1(2):156–173, 1975), this paper proposes an approach to model-based ideal testing of hardware description language (HDL) programs based on their behavioral model. Test sequences are generated from both original (fault-free) and mutant (faulty) models in the sense of positive and negative testing, forming a holistic test view. These test sequences are then executed on original (fault-free) and mutant (faulty) HDL programs, in the sense of mutation testing. Using the techniques known from automata theory, test selection criteria are developed and formally show that they fulﬁll the major requirements of Fundamental Test Theory, that is, reliability and validity. The current paper comprises a preparation step (consisting of the sub-steps model construction, model mutation, model conversion, and test generation) and a composition step (consisting of the sub-steps pre-selection and construction of Ideal test suites). All the steps are supported by a toolchain that is already implemented and is available online. To critically validate the proposed approach, three case studies (a sequence detector, a trafﬁc light controller, and a RISC-V processor) are used and the strengths and weaknesses of the approach are discussed. The proposed approach achieves the highest mutation score in positive and negative testing for all case studies in comparison with two existing methods (regular expression-based test generation and context-based random test generation), using four different techniques.",
        "keywords": [
            "Model-based testing",
            "Ideal testing",
            "Mutation testing",
            "Behavioral model",
            "Hardware description language"
        ],
        "authors": [
            "Onur Kilincceker",
            "Ercument Turk",
            "Fevzi Belli",
            "Moharram Challenger"
        ],
        "file_path": "data/sosym-all/s10270-021-00934-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Requirements-driven deployment\nCustomizing the requirements model for the host environment",
        "submission-date": "2011/05",
        "publication-date": "2012/07",
        "abstract": "Deployment is a main development phase which\nconfigures a software to be ready for use in a certain environ-\nment. The ultimate goal of deployment is to enable users to\nachieve their requirements while using the deployed soft-\nware. However, requirements are not uniform and differ\nbetween deployment environments. In one environment, cer-\ntain requirements could be useless or redundant, thereby\nmaking some software functionalities superﬂuous. In another\nenvironment, instead, some requirements could be impossi-\nble to achieve and, thus, additional functionalities would be\nrequired. We advocate that ensuring ﬁtness between require-\nments and the system environment is a basic and critical step\nto achieve a comprehensive deployment process. We pro-\npose a tool-supported modelling and analysis approach to\ntailor a requirements model to each environment in which\nthe system is to be deployed. We study the case of a con-\ntextual goal model, which is a requirements model that cap-\ntures the relationship between the variability of requirements\n(goal variability space) and the varying states of a deploy-\nment environment (context variability space). Our analysis\nrelies on sampling a deployment environment to discover its\ncontext variability space and use it to identify loci in the con-\ntextual goal model where a modiﬁcation has to take place.",
        "keywords": [
            "Requirements engineering",
            "Contextual requirements",
            "Deployment",
            "Context-sensitive systems modelling"
        ],
        "authors": [
            "Raian Ali",
            "Fabiano Dalpiaz",
            "Paolo Giorgini"
        ],
        "file_path": "data/sosym-all/s10270-012-0255-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Benchmarking bidirectional transformations: theory, implementation, application, and assessment",
        "submission-date": "2019/03",
        "publication-date": "2019/09",
        "abstract": "Bidirectional transformations (bx) are relevant for a wide range of application domains. While bx problems may be solved with unidirectional languages and tools, maintaining separate implementations of forward and backward synchronizers with mutually consistent behavior can be difﬁcult, laborious, and error-prone. To address the challenges involved in handling bx problems, dedicated languages and tools for bx have been developed. Due to their heterogeneity, however, the numerous and diverse approaches to bx are difﬁcult to compare, with the consequence that fundamental differences and similarities are not yet well understood. This motivates the need for suitable benchmarks that facilitate the comparison of bx approaches. This paper provides a comprehensive treatment of benchmarking bx, covering theory, implementation, application, and assessment. At the level of theory, we introduce a conceptual framework that deﬁnes and classiﬁes architectures of bx tools. At the level of implementation, we describe Benchmarx, an infrastructure for benchmarking bx tools which is based on the conceptual framework. At the level of application, we report on a wide variety of solutions to the well-known Families-to-Persons benchmark, which were developed and compared with the help of Benchmarx. At the level of assessment, we reﬂect on the usefulness of the Benchmarx approach to benchmarking bx, based on the experiences gained from the Families-to-Persons benchmark.",
        "keywords": [
            "Bidirectional transformation",
            "Benchmark",
            "Model synchronization",
            "Framework"
        ],
        "authors": [
            "Anthony Anjorin",
            "Thomas Buchmann",
            "Bernhard Westfechtel",
            "Zinovy Diskin",
            "Hsiang-Shang Ko",
            "Romina Eramo",
            "Georg Hinkel",
            "Leila Samimi-Dehkordi",
            "Albert Zündorf"
        ],
        "file_path": "data/sosym-all/s10270-019-00752-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Data warehouse concepts for model artifacts?",
        "submission-date": "2012/04",
        "publication-date": "2012/04",
        "abstract": "Manufacturing industries, in particular avionics and automotive industries, that have embraced model-driven system development approaches, are faced with the challenge of getting development tools that address different, but related aspects of the development process to interoperate. This challenge led to increased interest in and work on tool-integration approaches that enable the creation of “tool chains”. These approaches are based on the assumption that it is best to use tools that are specialized for speciﬁc purposes. An appropriate way to integrate the tools is to provide facilities for transferring the outputs of one tool to another specialized tool. This typically requires transformation of data produced from one tool to a format suitable for input to other tools. As an example, the development of software for a cyber-physical system (CPS) based on a communication bus (e.g., software found in cars) is based on a dominant bus architecture. We have seen in current development approaches any pieceofdata(signal)ﬂowingbetweencontroldevicesthatare connected with a bus needs a unique name (identiﬁer). This enforces that signals be maintained in a centralized catalog to help ensure qualities such as consistency and completeness, even though they never ﬂow on the same bus and do not interfere at all in the CPS and ruins ﬂexible adaption of the CPS.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-012-0244-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Efficient construction of family-based behavioral models from adaptively learned models",
        "submission-date": "2023/08",
        "publication-date": "2024/08",
        "abstract": "Family-based behavioral models capture the behavior of a software product line (SPL) in a single model, incorporating the variability among the products. In representing these models, a common technique is to annotate well-known behavioral modeling notations with features, e.g., featured ﬁnite state machine (FFSM) as an extension to the well-known ﬁnite state machine notation. It is not always the case that family-based behavioral models are prepared before developing an SPL, or kept up-to-date during the development and maintenance. Model learning is helpful in such situations. Taking advantage of the commonality among the SPL products, it is possible to reuse the product models in learning the behavior of the entire SPL. In this paper, the process of constructing FFSM models for SPLs is enhanced. Model learning is performed using an adaptive learning algorithm called PL*. Regarding the model learning step, we introduce a new heuristic method for determining the product learning orders with high learning efﬁciency. The proposed heuristic takes into account the complexity of features added by each product and improves the previous heuristics for learning order. To construct the whole family-based behavioral model of an SPL, the behavioral models of individual products are iteratively merged into the whole family-based model. A similarity metric is used to determine which states of the two models are merged with each other. By providing a formalization for the existing FFSMDi f f algorithm for this purpose, we prove that in the FFSM constructed by this algorithm, the choice of the similarity metric does not affect the observable behavior of the constructed FFSM. We study the efﬁciency of three similarity metrics, two of which are local metrics, in the sense that they determine the similarity of two states only in terms of their adjacent transitions. On the other hand, a global similarity metric takes into account not only the adjacent transitions, but also the similarity of their adjacent states. It is shown by experimentation on two case studies that local similarity metrics can result in constructing FFSMs as concise as the FFSM resulting from the global similarity metric. The results also show that local similarity metrics increase the efﬁciency and scalability while maintaining the effectiveness of the FFSM construction.",
        "keywords": [
            "Adaptive model learning",
            "Software product lines",
            "Behavioral model",
            "Featured finite state machine"
        ],
        "authors": [
            "Shaghayegh Tavassoli",
            "Ramtin Khosravi"
        ],
        "file_path": "data/sosym-all/s10270-024-01199-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for EMMSAD’2022 special section",
        "submission-date": "2023/09",
        "publication-date": "2023/12",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) conference series organized 27 events from 1996 to 2022, associated with CAISE (Conference on Advanced Information Systems Engineering). In 2009, EMMSAD became a two-day working conference. Since 2017, EMMSAD best papers are invited to submit extended versions for consideration of their publication in the Journal of Software and Systems Modeling (SoSyM). The main topics of the EMMSAD series have the focus on models and modeling methods for the analysis and development of software information systems of any kind.",
        "keywords": [],
        "authors": [
            "Iris Reinhartz‑Berger",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-023-01130-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An experiment in model-driven conceptual database design",
        "submission-date": "2017/09",
        "publication-date": "2018/03",
        "abstract": "The article presents the results of an experiment we conducted with database professionals in order to evaluate an approach to\nautomatic design of the initial conceptual database model based on collaborative business process models. The source business\nprocess model is represented by BPMN, while the target conceptual model is represented by the UML class diagram. The\nresults conﬁrm those already obtained in a case-study-based evaluation, as well as those of an earlier controlled experiment\nconducted with undergraduate students. The evaluation implies that the proposed approach and implemented generator enable\nautomatic generation of the target conceptual model with a high percentage of completeness and precision. The experiment\nalso conﬁrms that the automatically generated model can be efﬁciently used as a starting point for manual design of the target\nmodel, since it signiﬁcantly shortens the estimated efforts and actual time spent to obtain the target model in contrast to the\nmanual design from scratch.",
        "keywords": [
            "BPMN",
            "Collaborative business process model",
            "Conceptual database model",
            "Evaluation",
            "Experiment",
            "Model-driven",
            "UML"
        ],
        "authors": [
            "Drazen Brdjanin",
            "Goran Banjac",
            "Danijela Banjac",
            "Slavko Maric"
        ],
        "file_path": "data/sosym-all/s10270-018-0672-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Type inference in ﬂexible model-driven engineering using classiﬁcation algorithms",
        "submission-date": "2016/06",
        "publication-date": "2018/01",
        "abstract": "Flexible or bottom-up model-driven engineering (MDE) is an emerging approach to domain and systems modelling. Domain experts, who have detailed domain knowledge, typically lack the technical expertise to transfer this knowledge using traditional MDE tools. Flexible MDE approaches tackle this challenge by promoting the use of simple drawing tools to increase the involvement of domain experts in the language deﬁnition process. In such approaches, no metamodel is created upfront, but instead the process starts with the deﬁnition of example models that will be used to infer the metamodel. Pre-deﬁned metamodels created by MDE experts may miss important concepts of the domain and thus restrict their expressiveness. However, the lack of a metamodel, that encodes the semantics of conforming models has some drawbacks, among others that of having models with elements that are unintentionally left untyped. In this paper, we propose the use of classiﬁcation algorithms to help with the inference of such untyped elements. We evaluate the proposed approach in a number of random generated example models from various domains. The correct type prediction varies from 23 to 100% depending on the domain, the proportion of elements that were left untyped and the prediction algorithm used.",
        "keywords": [
            "Model-driven engineering",
            "Flexible model-driven engineering",
            "Bottom-up metamodelling",
            "Type inference",
            "Classiﬁcation and regression trees",
            "Random forests"
        ],
        "authors": [
            "Athanasios Zolotas",
            "Nicholas Matragkas",
            "Sam Devlin",
            "Dimitrios S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-018-0658-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Explicit versus implicit models: What are good languages for modeling?",
        "submission-date": "2022/04",
        "publication-date": "2022/04",
        "abstract": "Although modeling is used in almost all science and engineering disciplines, the explicit deﬁnition of modeling languages is an invention of modern computing technology. It was necessary to deﬁne modeling languages precisely so that computers could process the models described in the languages, as a way to support various analysis and synthesis needs (e.g., model analysis and code generation). However, these were not the ﬁrst languages that were deﬁned pre- cisely. Natural languages, for example English in the Oxford dictionary and German in the Duden, received a rather formal deﬁnition prior to digital automation with computers. The ideas of grammars and related lexicographic deﬁnitions were transferred from the natural language context to the ﬁrst computer languages, namely programming languages and speciﬁcation languages, where it received much more precision. And for diagrammatic modeling languages, the concept of a meta-model was popularized, often as a variant of a class diagram.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-01001-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Distributed model validation with Epsilon",
        "submission-date": "2020/03",
        "publication-date": "2021/03",
        "abstract": "Scalable performance is a major challenge with current model management tools. As the size and complexity of models and\nmodel management programs increases and the cost of computing falls, one solution for improving performance of model\nmanagement programs is to perform computations on multiple computers. In this paper, we demonstrate a low-overhead\ndata-parallel approach for distributed model validation in the context of an OCL-like language. Our approach minimises\ncommunication costs by exploiting the deterministic structure of programs and can take advantage of multiple cores on each\n(heterogeneous) machine with highly conﬁgurable computational granularity. Our performance evaluation shows that the\nimplementation is extremely low overhead, achieving a speed up of 24.5× with 26 computers over the sequential case, and\n122× when utilising all six cores on each computer.",
        "keywords": [
            "Model-driven engineering",
            "Model validation",
            "Distributed computing",
            "Model management",
            "Parallelism"
        ],
        "authors": [
            "Sina Madani",
            "Dimitris Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-021-00878-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "“No shit” or “Oh, shit!”: responses to observations on the use of UML in professional practice",
        "submission-date": "2014/08",
        "publication-date": "2014/08",
        "abstract": "This paper follows a paper, “UML in Practice” presentedatICSE2013.Itsummarizesandreﬂectsonthedis-cussion and additional investigation that arose from “UML in Practice.” The paper provides a condensed recap of “UML in Practice” ﬁndings, explains what data were collected from which sources to inform this paper, and describes how the data were analyzed. It reports on the discussion that has arisen, summarizing responses from industry practitioners, academicsteachingsoftwareengineering,andtheUMLcom-munity, and considers how those responses reﬂect on the original observations. The responses to “UML in Practice” divide (crudely) between two perspectives: (1) the observations made are familiar and unsurprizing, and match personal experience (“No shit”); or (2) the observations threaten long-held beliefs about UML use, and in particular about the status of UML as the de facto standard of software engineering, implying a need to change personal practice (“Oh, shit!”).",
        "keywords": [
            "UML",
            "Software development",
            "Software design",
            "Notation",
            "Empirical studies"
        ],
        "authors": [
            "Marian Petre"
        ],
        "file_path": "data/sosym-all/s10270-014-0430-4.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Variability in UML language and semantics",
        "submission-date": "2011/08",
        "publication-date": "2011/08",
        "abstract": "Practitioners, who use UML as a sketching language are generally not too concerned about the precision of their models, but developers who build UML models to rigorously analyze software properties (e.g., to analyze the consistency of design constraints) or that can be mechanically transformed to implementations requiring tools and tool chains that are based on a precisely defined UML semantics (see this issue’s Expert Voice by Manfred Broy and María Victoria Cengarle as well as the regular paper on the many semantics of sequence diagrams by Zoltán Micskei and Hélène Waeselynck).Thisneedmotivatesmuchoftheworkondeﬁning appropriate formal semantics for the UML.\nThere is a significantly large body of work on formalizing the UML—both syntactical appearance, internal representation and semantics (in terms of meaning), and the collective experience suggests that defining appropriate semantics for the UML has both a technical and a strong political/social aspect. This non-technical aspect is concerned primarily with determining what constitutes an “appropriate” language. The problem is that different stakeholders, including UML modelers from different domains, tool vendors with specific ready to use solutions, have varying views of what constitutes an appropriate UML language and its semantics.\nIt is not easily possible to support these sometimes competing views in a single language. This led to the view of UML as a “family of languages” and to the introduction of profile mechanisms and “semantic variation points” that can be used for specializing the syntax and semantics of UML.\nB. Rumpe (B)\nRWTH Aachen, Aachen, Germany\ne-mail: bernhard.rumpe@sosym.org\nR. France (B)\nColorado State University, Fort Collins, CO, USA\ne-mail: france@cs.colostate.edu\nThe UML currently has a wide variety of these semantic variation points indicating points in the language definition that can be tailored to better support the many forms of usage of UML. Although this form of tailoring may be convenient for developers, it makes the development of generic tools and tool chains considerably more complex and makes it almost impossible to provide a well formed, rather complete and precise semantics for the UML as a whole. Furthermore, the UML does currently not provide good mechanisms for introducing and describing variations or selecting concrete sub-variants yet.\nManaging variability within a language, such as the UML can be likened to manage variability of a software product line. Indeed, it is useful to regard the UML as a product line of languages to explore how techniques for managing variability in product lines (e.g., feature diagrams) can be used to explicitly manage variability in UML. We recently invested some efforts in studying this technique and our work suggests that it can very well be used to make the UML, or at least some derivatives of UML, more precise and easier to use.\nIt can also help developers understand similarities and differences across different UML derivatives. One can envisage configuring a UML tool using a configuration that describes a particular UML derivative, the required tool functionality, enhancedanalysisalgorithmsordomain-speciﬁcrestrictions, thedesiredformofcodeandtestgeneration,amongotherfeatures.OnecanalsoenvisagethattheUMLstandarddeﬁnesits semantic variation points explicitly using feature diagrams.\nFrom its many possible forms of uses, it seems clear that the UML will not have a single syntactic form or semantics that adequately serves its community, but understanding and managing variations in the language UML might allow us to cope with this drawback.\nThe time to explore language variability to allow modelers deal with precise and well-assisted language variations.",
        "keywords": [],
        "authors": [
            "Bernhard Rumpe",
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-011-0210-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic derivation of conceptual database models from differently serialized business process models",
        "submission-date": "2019/09",
        "publication-date": "2020/07",
        "abstract": "The existing tools that aim to derive data models from business process models are typically able to process the source models represented by one single notation and also serialized in one specific way. However, the standards (e.g., BPMN) enable different serialization formats and also provide serialization flexibility, which leads to various implementations of the standard in different modeling tools and results in differently serialized models in practice, which therefore significantly constraints usability of the existing model-driven tools. In this article, we present an approach to automatic derivation of conceptual database models from business process models represented by different notations, with particular focus on differently serialized process models. A deterministic rule-based approach is proposed to overcome the serialization specificities and to enable extraction of characteristic elements from differently serialized process models. Based on the proposed approach, we implemented an online web-based model-driven tool named AMADEOS, which is able to automatically derive conceptual database models from process models represented by different notations and also differently serialized. The experimental results show that the proposed approach and implemented tool enable successful extraction of specific elements from differently serialized process models and enable derivation of the target conceptual database models with very high completeness and precision.",
        "keywords": [
            "AMADEOS",
            "BPMN",
            "Business process model",
            "Conceptual database model",
            "Extractor",
            "Robustness",
            "Serialization",
            "Structural differences"
        ],
        "authors": [
            "Drazen Brdjanin",
            "Stefan Ilic",
            "Goran Banjac",
            "Danijela Banjac",
            "Slavko Maric"
        ],
        "file_path": "data/sosym-all/s10270-020-00808-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Conformance checking in UML artifact-centric business process models",
        "submission-date": "2017/01",
        "publication-date": "2018/05",
        "abstract": "Business artifacts have appeared as a new paradigm to capture the information required for the complete execution and\nreasoning of a business process. Likewise, conformance checking is gaining popularity as a crucial technique that enables\nevaluating whether recorded executions of a process match its corresponding model. In this paper, conformance checking\ntechniques are incorporated into a general framework to specify business artifacts. By relying on the expressive power of\nan artifact-centric speciﬁcation, BAUML, which combines UML state and activity diagrams (among others), the problem\nof conformance checking can be mapped into the Petri net formalism and its results be explained in terms of the original\nartifact-centric speciﬁcation. In contrast to most existing approaches, ours incorporates data constraints into the Petri nets, thus\nachieving conformance results which are more precise. We have also implemented a plug-in, within the ProM framework,\nwhich is able to translate a BAUML into a Petri net to perform conformance checking. This shows the feasibility of our\napproach.",
        "keywords": [
            "Conformance checking",
            "Artifact-centric BPM",
            "BAUML framework",
            "Process mining"
        ],
        "authors": [
            "Montserrat Estañol",
            "Jorge Munoz-Gama",
            "Josep Carmona",
            "Ernest Teniente"
        ],
        "file_path": "data/sosym-all/s10270-018-0681-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OSTRICH: a rich template language for low-code development (extended version)",
        "submission-date": "2022/03",
        "publication-date": "2022/12",
        "abstract": "Low-code platforms aim at allowing non-experts to develop complex systems and knowledgeable developers to improve their productivity in orders of magnitude. The greater gain comes from using components developed by experts capturing common patterns across all layers of the application, from the user interface to the data layer and integration with external systems. Often, cloning sample code fragments is the only alternative in such scenarios, requiring extensive adaptation to reach the intended use. Such customization activities require deep knowledge outside of the comfort zone of low code. To effectively speed up the reuse, composition, and adaptation of pre-deﬁned components, low-code platforms need to provide safe and easy-to-use language mechanisms. This paper introduces OSTRICH, a strongly typed rich templating language for a low-code platform (OutSystems) that builds on metamodel annotations and allows the correct instantiation of templates. We conservatively extend the existing metamodel and ensure that the resulting code is always well-formed. The results we present include a novel type safety veriﬁcation of template deﬁnitions, and template arguments, providing model consistency across application layers. We implemented this template language in a prototype of the OutSystems platform and ported nine of the top ten most used sample code fragments, thus improving the reuse of professionally designed components.",
        "keywords": [
            "Metamodel templating",
            "Typechecking templates",
            "Parameter constraints",
            "Low-code",
            "Development productivity",
            "Model reuse"
        ],
        "authors": [
            "Hugo Lourenço",
            "Carla Ferreira",
            "João Costa Seco",
            "Joana Parreira"
        ],
        "file_path": "data/sosym-all/s10270-022-01066-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "OSTRICH"
        }
    },
    {
        "title": "Characteristics, potentials, and limitations of open-source Simulink projects for empirical research",
        "submission-date": "2020/09",
        "publication-date": "2021/04",
        "abstract": "Simulink is an example of a successful application of the paradigm of model-based development into industrial practice. Numerous companies create and maintain Simulink projects for modeling software-intensive embedded systems, aiming at early validation and automated code generation. However, Simulink projects are not as easily available as code-based ones, which proﬁt from large publicly accessible open-source repositories, thus curbing empirical research. In this paper, we investigate a set of 1734 freely available Simulink models from 194 projects and analyze their suitability for empirical research. We analyze the projects considering (1) their development context, (2) their complexity in terms of size and organization within projects, and (3) their evolution over time. Our results show that there are both limitations and potentials for empirical research. On the one hand, some application domains dominate the development context, and there is a large number of models that can be considered toy examples of limited practical relevance. These often stem from an academic context, consist of only a few Simulink blocks, and are no longer (or have never been) under active development or maintenance. On the other hand, we found that a subset of the analyzed models is of considerable size and complexity. There are models comprising several thousands of blocks, some of them highly modularized by hierarchically organized Simulink subsystems. Likewise, some of the models expose an active maintenance span of several years, which indicates that they are used as primary development artifacts throughout a project’s lifecycle. According to a discussion of our results with a domain expert, many models can be considered mature enough for quality analysis purposes, and they expose characteristics that can be considered representative for industry-scale models. Thus, we are conﬁdent that a subset of the models is suitable for empirical research. More generally, using a publicly available model corpus or a dedicated subset enables researchers to replicate ﬁndings, publish subsequent studies, and use them for validation purposes. We publish our dataset for the sake of replicating our results and fostering future empirical research.",
        "keywords": [
            "Simulink",
            "Open source",
            "Empirical research",
            "Sample study"
        ],
        "authors": [
            "Alexander Boll",
            "Florian Brokhausen",
            "Tiago Amorim",
            "Timo Kehrer",
            "Andreas Vogelsang"
        ],
        "file_path": "data/sosym-all/s10270-021-00883-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Specification of invariability in OCL",
        "submission-date": "2008/05",
        "publication-date": "2011/10",
        "abstract": "The Object Constraint Language (OCL) is a high-level, object-oriented language for contractual system spec-iﬁcations. Despite its expressivity, OCL does not provide primitives for a compact speciﬁcation of invariability. In this paper, problems with invariability speciﬁcation are listed and some weaknesses of existing solutions are pointed out. The question of invariability speciﬁcation is addressed and a simple but expressive extension of OCL is proposed. It allows a view-orientedspeciﬁcationofinvariabilityconstraints,wher-eby we restrict the notion of view to reducts based on order-sorted algebras. The semantics of this extension is deﬁned in terms of standard OCL.",
        "keywords": [
            "OCL",
            "UML",
            "Invariability",
            "Frame problem",
            "Views"
        ],
        "authors": [
            "Piotr Kosiuczenko"
        ],
        "file_path": "data/sosym-all/s10270-011-0215-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Applying MDD in the content management system domain\nScenarios, tooling, and a mixed-method empirical assessment",
        "submission-date": "2020/03",
        "publication-date": "2021/02",
        "abstract": "Content management systems (CMSs) such as Joomla and WordPress dominate today’s web. Enabled by standardized\nextensions, administrators can build powerful web applications for diverse customer demands. However, developing CMS\nextensions requires sophisticated technical knowledge, and the complex code structure of an extension gives rise to errors\nduring typical development and migration scenarios. Model-driven development (MDD) seems to be a promising paradigm\nto address these challenges; however, it has not found adoption in the CMS domain yet. Systematic evidence of the beneﬁt of\napplying MDD in this domain could facilitate its adoption; however, an empirical investigation of this beneﬁt is currently lack-\ning. In this paper, we present a mixed-method empirical investigation of applying MDD in the CMS domain, based on an\ninterview suite, a controlled experiment, a ﬁeld experiment, and case studies. During the experiments, we used JooMDD,\nan MDD infrastructure instantiation for CMS extensions. This infrastructure, which is also presented in this work, consists\nof a DSL with model editors, code generators, and reverse engineering facilities. We consider three scenarios of developing\nnew (both independent and dependent) CMS extensions and of migrating existing ones to a new major platform version.\nThe experienced developers in our interviews acknowledge the relevance of these scenarios and report on experiences that\render them suitable candidates for a successful application of MDD. We found a particularly high relevance of the migration\nscenario. Our experiments largely conﬁrm the potentials and limits of MDD as identiﬁed for other domains. In particular,\nwe found a productivity increase up to factor 11.7 and a quality increase up to factor 2.4 during the development of CMS\nextensions. Furthermore, our observations highlight the importance of good tooling that seamlessly integrates with already\nused tool environments and processes.",
        "keywords": [
            "Model-driven development",
            "Content management systems",
            "Empirical assessment"
        ],
        "authors": [
            "Dennis Priefer",
            "Wolf Rost",
            "Daniel Strüber",
            "Gabriele Taentzer",
            "Peter Kneisel"
        ],
        "file_path": "data/sosym-all/s10270-021-00872-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The sustainability assessment framework toolkit: a decade of modeling experience",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Software intensive systems play a crucial role in most, if not all, aspects of modern society. As such, both their sustainability and their role in supporting sustainable processes must be realized by design. To this aim, the architecture of software intensive systems should be designed to support sustainability goals; and measured to understand how effectively they do so. In this paper, we present the sustainability assessment framework (SAF) Toolkit—a set of instruments we developed to support software architects and design decision makers in modeling sustainability as a software quality property. The SAF Toolkit is the result of our experience gained in more than a decade of case studies in collaboration with industrial partners. We illustrate the toolkit with examples that come from some of such studies. We extract our lessons learned, our current research, and future plans to extend the SAF Toolkit for further architecture modeling and measurement.",
        "keywords": [
            "Software architecture design",
            "Software sustainability",
            "Quality assessment",
            "Framework"
        ],
        "authors": [
            "Patricia Lago",
            "Nelly Condori Fernandez",
            "Iffat Fatima",
            "Markus Funke",
            "Ivano Malavolta"
        ],
        "file_path": "data/sosym-all/s10270-024-01230-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Inter-modelling with patterns",
        "submission-date": "2009/11",
        "publication-date": "2011/02",
        "abstract": "Inter-modelling is the activity of modelling relations between two or more modelling languages. The result of this activity is a model that describes the way in which model instances of these languages can be related. Many tasks in model-driven development can be classified as inter-modelling, for example designing model-to-model transformations, defining model matching and traceability relations, specifying model merging and model weaving, as well as describing mechanisms for inter-model consistency management and model synchronization. This paper presents our approach to inter-modelling in a declarative, relational, visual, and formal style. The approach relies on declarative patterns describing allowed or forbidden relations between two modelling languages. Such specification is then compiled into different operational mechanisms that are tailor-made for concrete inter-modelling scenarios. Up to now, we have used the approach to generate forward and backward transformations from a pattern specification. In this paper we demonstrate that the same specification can be used to derive mechanisms for other inter-modelling tasks, such as model matching and model traceability. In these scenarios the goals are generating the traces between two existing models, checking whether two models are correctly traced, and modifying the traces between two models if they are incorrect.",
        "keywords": [
            "Inter-modelling",
            "Model-to-model transformation",
            "Model matching",
            "Traceability",
            "Graph transformation",
            "Graph constraints"
        ],
        "authors": [
            "Esther Guerra",
            "Juan de Lara",
            "Fernando Orejas"
        ],
        "file_path": "data/sosym-all/s10270-011-0192-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A reference requirements set for public service provision enterprise architectures",
        "submission-date": "2011/11",
        "publication-date": "2012/12",
        "abstract": "Electronic Government (eGov) is a political priority worldwide. One of the core objectives of eGov is the online public services provision (PSP). However, many of eGov PSP systems fail in realizing their objectives. Enterprise Architectures (EA) could contribute to overcome some of the relevant obstacles. The objective of this paper is to derive a reference requirements set for eGov PSP that can be used in EA development. Aiming at capitalizing on existing knowledge, we conduct a systematic literature review on eGov PSP systems requirements. This results in identifying a unified requirements set, i.e. 186 requirements, and stakeholders set, i.e. 19 stakeholders, for eGov PSP systems. Based on these findings, we determine 16 overview use cases demonstrating the basic functionality of such systems. Our findings are modeled using ArchiMate 2.0 notation. The identified requirements set can be used by virtually any public organization providing public services for developing its own EA. As a result, it can lead to the reduction of eGov PSP project failures, the decrease of software development costs and the improvement of its effectiveness and quality. Furthermore, it can be used as a basis to develop a complete reference EA for the eGov PSP domain.",
        "keywords": [
            "E-government",
            "Public service provision",
            "Requirements engineering",
            "Enterprise architecture",
            "ArchiMate"
        ],
        "authors": [
            "Efthimios Tambouris",
            "Eleni Kaliva",
            "Michail Liaros",
            "Konstantinos Tarabanis"
        ],
        "file_path": "data/sosym-all/s10270-012-0303-7.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Reference models: how can we leverage them?",
        "submission-date": "2021/10",
        "publication-date": "2021/11",
        "abstract": "This editorial reflects some observations from discussions at the MODELS 2021 conference and its workshops, which were held virtually in Japan during the period of October 10–15, 2021. As the Editors-in-Chief of the journal most associated with the interest of MODELS, we are very much appreciative of all the deep efforts of the organizers and their continued interest in collaborating with SoSyM. A large number (34) of members from the modeling community were involved in the general organization of MODELS 2021. Our deepest gratitude goes to the General Chairs, Zhenjiang Hu (Peking University), Tomoji Kishi (Waseda University), and Naoyasu Ubayashi (Kyushu University), as well as the PC chairs, Shiva Nejati (University of Ottawa) and Daniel Varro (McGill University as well as Budapest University of Tech-nology and Economics). The organization was superb, and the success of the conference suggests that topics related to software and systems modeling continue to grow in interest each year. The diversity of topics around the idea of modeling in development and understanding of software and systems becomes broader with each conference edition. We were also very excited to extend the collaboration with MODELS through the Journal First option (a record 30 SoSyM papers were presented in MODELS sessions) and the continuation of the Most Impactful Papers (MIP) from the past decade (a list of the MIP awards from the current and past years can be found at https://www.sosym.org/awards). During the informal period in one of the sessions, there was a discussion on the definition of “reference models” and what purposes they serve in general use. To prime this discussion, we provide below the definition from Wikipedia: A reference model—in systems, enterprise, and software engineering—is an abstract framework or domain-specific ontology consisting of an interlinked set of clearly defined concepts produced by an expert or body of experts to encourage clear communication. A reference model can represent the component parts of any consistent idea, from business functions to system components, as long as it represents a complete set. This frame of reference can then be used to communicate ideas clearly among members of the same community.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-021-00948-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Circular systems engineering",
        "submission-date": "2023/12",
        "publication-date": "2024/02",
        "abstract": "The perception of the value and propriety of modern engineered systems is changing. In addition to their functional and\nextra-functional properties, nowadays’ systems are also evaluated by their sustainability properties. The next generation of\nsystems will be characterized by an overall elevated sustainability—including their post-life, driven by efﬁcient value retention\nmechanisms. Current systems engineering practices fall short of supporting these ambitions and need to be revised appropri-\nate. In this paper, we introduce the concept of circular systems engineering, a novel paradigm for systems sustainability,\nand deﬁne two principles to successfully implement it: end-to-end sustainability and bipartite sustainability. We outline typ-\nical organizational evolution patterns that lead to the implementation and adoption of circularity principles, and outline key\nchallenges and research opportunities.",
        "keywords": [
            "Circular economy",
            "Digital thread",
            "Digital twins",
            "Sustainability",
            "Systems engineering"
        ],
        "authors": [
            "Istvan David",
            "Dominik Bork",
            "Gerti Kappel"
        ],
        "file_path": "data/sosym-all/s10270-024-01154-4.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A systematic approach to constructing families of incremental topology control algorithms using graph transformation",
        "submission-date": "2016/07",
        "publication-date": "2017/03",
        "abstract": "In the communication system domain, constructing and maintaining network topologies via topology control algorithms is an important crosscutting research area. Network topologies are usually modeled using attributed graphs whose nodes and edges represent the network nodes and their interconnecting links. A key requirement of topology control algorithms is to fulﬁll certain consistency and optimization properties to ensure a high quality of service. Still, fewattemptshavebeenmadetoconstructivelyintegratethese properties into the development process of topology control algorithms. Furthermore, even though many topology control algorithms share substantial parts (such as structural patterns or tie-breaking strategies), few works constructively leverage these commonalities and differences of topology control algorithms systematically. In previous work, we addressed the constructive integration of consistency proper- ties into the development process. We outlined a constructive, model-driven methodology for designing individual topol- ogy control algorithms. Valid and high-quality topologies are characterized using declarative graph constraints; topology control algorithms are speciﬁed using programmed graph transformation. We applied a well-known static analysis technique to reﬁne a given topology control algorithm in a way that the resulting algorithm preserves the speciﬁed graph constraints. In this paper, we extend our constructive methodology by generalizing it to support the speciﬁcation of families of topology control algorithms. To show the feasibility of our approach, we reengineering six exist- ing topology control algorithms and develop e-kTC, a novel energy-efﬁcient variant of the topology control algorithm kTC. Finally, we evaluate a subset of the speciﬁed topol- ogy control algorithms using a new tool integration of the graph transformation tool eMoflon and the Simonstra- tor network simulation framework.",
        "keywords": [
            "Graph transformation",
            "Graph constraints",
            "Static analysis",
            "Model-driven engineering",
            "Wireless networks",
            "Network simulation"
        ],
        "authors": [
            "Roland Kluge",
            "Michael Stein",
            "Gergely Varró",
            "Andy Schürr",
            "Matthias Hollick",
            "Max Mühlhäuser"
        ],
        "file_path": "data/sosym-all/s10270-017-0587-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Early-stage analysis of cyber-physical production systems through collaborative modelling",
        "submission-date": "2018/09",
        "publication-date": "2019/09",
        "abstract": "This paper demonstrates the flexible methodology of modelling cyber-physical systems (CPSs) using the INTO-CPS tech-nology through co-simulation based on Functional Mock-up Units (FMUs). It explores a novel method with two main co-simulation phases: homogeneous and heterogeneous. In the first phase, high-level, abstract FMUs are produced for all subsystems using a single discrete-event formalism (the VDM-RT language and Overture tool). This approach permits early co-simulation of system-level behaviours and serves as a basis for dialogue between subsystem teams and agreement on interfaces. During the second phase, model refinements of subsystems are gradually introduced, using various simulation tools capable of exporting FMUs. This heterogeneous phase permits high-fidelity models of all subsystems to be produced in appropriate formalisms. This paper describes the use of this methodology to develop a USB stick production line, representing a smart system of systems. The experiments are performed under the assumption that the orders are received in a Gaussian or Uniform distribution. The focus is on the homogeneous co-simulation phase, for which the method demonstrates two important roles: first, the homogeneous phase identifies the right interaction protocols (signals) among the various subsys-tems, and second, the conceptual (system-level) parameters identified before the heterogeneous co-simulation phase reduce the huge size of the design space and create stable constraints, later reflected in the physical implementation.",
        "keywords": [
            "Co-simulation",
            "Cyber-physical production systems",
            "Homogeneous and heterogeneous modelling",
            "Design space exploration"
        ],
        "authors": [
            "Mihai Neghina",
            "Constantin-Bala Zamfirescu",
            "Ken Pierce"
        ],
        "file_path": "data/sosym-all/s10270-019-00753-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Advanced and efﬁcient execution trace management for executable domain-speciﬁc modeling languages",
        "submission-date": "2016/06",
        "publication-date": "2017/05",
        "abstract": "Executable Domain-Speciﬁc Modeling Lang-uages (xDSMLs) enable the application of early dynamic veriﬁcation and validation (V&V) techniques for behavioral models. At the core of such techniques, execution traces are used to represent the evolution of models during their execu-tion. In order to construct execution traces for any xDSML, generic trace metamodels can be used. Yet, regarding trace manipulations, generic trace metamodels lack efﬁciency in time because of their sequential structure, efﬁciency in memory because they capture superﬂuous data, and usability because of their conceptual gap with the considered xDSML. Our contribution is a novel generative approach that deﬁnes a multidimensional and domain-speciﬁc trace metamodel enabling the construction and manipulation of execution traces for models conforming to a given xDSML. Efﬁciency in time is improved by providing a variety of navigation paths within traces, while usability and memory are improved by narrowing the scope of trace metamodels to ﬁt the consid-ered xDSML. We evaluated our approach by generating a trace metamodel for fUML and using it for semantic differ-encing, which is an important V&V technique in the realm of model evolution. Results show a signiﬁcant performance improvement and simpliﬁcation of the semantic differencing rules as compared to the usage of a generic trace metamodel.",
        "keywords": [
            "Model execution",
            "Domain-speciﬁc languages",
            "Execution trace"
        ],
        "authors": [
            "Erwan Bousse",
            "Tanja Mayerhofer",
            "Benoit Combemale",
            "Benoit Baudry"
        ],
        "file_path": "data/sosym-all/s10270-017-0598-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reusing metamodels and notation with Diagram Deﬁnition",
        "submission-date": "2014/10",
        "publication-date": "2016/06",
        "abstract": "It is increasingly common for language speciﬁ-cations to describe visual forms (concrete syntax) separately from underlying concepts (abstract syntax). This is typically to enable interchange of visual information between graphi-cal modeling tools, such as positions of nodes and routings of lines. Often overlooked is that separation of visual forms and abstract concepts enables languages to deﬁne multiple visual forms for the same underlying concepts and for the same visual form to be used for similar underlying concepts in different languages (many-to-many relationships between concrete and abstract syntax). Visual forms can be adapted to communities using different notations for the same con-cepts and can be used to integrate communities using the same notation for similar concepts. Models of concrete syn-tax have been available for some time, but are rarely used to capture these many-to-many relationships with abstract syntax. This paper shows how to model these relationships using concrete graphical syntax expressed in the Diagram Deﬁnition standard, examining cases drawn from the Uni-ﬁed Modeling Language and the Business Process Model and Notation. This gives deﬁners of graphical languages a way to specify visual forms for multiple communities.",
        "keywords": [
            "Notation",
            "Metamodel",
            "Diagram Deﬁnition",
            "Syntax"
        ],
        "authors": [
            "Conrad Bock",
            "Maged Elaasar"
        ],
        "file_path": "data/sosym-all/s10270-016-0537-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Implementing a graph transformation engine in relational databases",
        "submission-date": "2004/11",
        "publication-date": "2006/06",
        "abstract": "We present a novel approach to implement a graph transformation engine based on standard relational database management systems (RDBMSs). The essence of the approach is to create database views for each rule and to handle pattern matching by inner join operations while handling negative application conditions by left outer join operations. Furthermore, the model manipulation prescribed by the application of a graph transformation rule is also implemented using elementary data manipulation statements (such as insert, delete). As a result, we obtain a robust and fast transformation engine especially suitable for (1) extending modeling tools with an underlying RDBMS repository and (2) embedding model transformations into large distributed applications where models are frequently persisted in a relational database and transaction handling is required to handle large models consistently.",
        "keywords": [
            "Tool support",
            "Graph transformation",
            "Pattern matching",
            "Relational databases"
        ],
        "authors": [
            "Gergely Varró",
            "Katalin Friedl",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-006-0015-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalizing requirements with object models and temporal constraints",
        "submission-date": "2009/03",
        "publication-date": "2009/08",
        "abstract": "Flaws in requirements often have a negative impact on the subsequent development phases. In this paper, we present a novel approach for the formal representation and validation of requirements, which we used in an industrial project. The formalism allows us to represent and reason about object models and their temporal evolution. The key ingredients are class diagrams to represent classes of objects, their relationships and their attributes, fragments of ﬁrst order logic to constrain the possible conﬁgurations of such objects, and temporal logic operators to deal with the dynamic evolution of the conﬁgurations. The approach to formal validation allows to check whether the requirements are consistent, if they are compatible with some scenarios, and if they guarantee some implicit properties. The validation procedure is based on satisﬁability checking, which is carried out by means of ﬁnite instantiation and model checking techniques.",
        "keywords": [
            "Formal requirement engineering",
            "Temporal logic",
            "Railway domain",
            "European Train Control System (ETCS)"
        ],
        "authors": [
            "Alessandro Cimatti",
            "Marco Roveri",
            "Angelo Susi",
            "Stefano Tonetta"
        ],
        "file_path": "data/sosym-all/s10270-009-0130-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Kompren: modeling and generating model slicers",
        "submission-date": "2012/03",
        "publication-date": "2012/11",
        "abstract": "Among model comprehension tools, model slicers are tools that extract a subset of model elements, for a specific purpose. Model slicers provide a mechanism to isolate and focus on parts of the model, thereby improving the overall analysis process. However, existing slicers are dedicated to a specific modeling language. This is an issue when we observe that new domain specific modeling languages, for which we want slicing abilities, are created almost on a daily basis. This paper proposes the Kompren language to model and generate model slicers for any DSL (e.g.modelingforsoftwaredevelopmentorforcivilengineer-ing) and for different purposes (e.g. monitoring and model comprehension). We detail the semantics of the Kompren language and of the model slicer generator. This provides a set of expected properties about the slices that are extracted by the different forms of the slicer. Then we illustrate these different forms of slicers on case studies from various domains.",
        "keywords": [
            "Model slicing",
            "Domain specific language"
        ],
        "authors": [
            "Arnaud Blouin",
            "Benoît Combemale",
            "Benoit Baudry",
            "Olivier Beaudoux"
        ],
        "file_path": "data/sosym-all/s10270-012-0300-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Kompren"
        }
    },
    {
        "title": "CoqTL: a Coq DSL for rule-based model transformation",
        "submission-date": "2018/12",
        "publication-date": "2019/11",
        "abstract": "In model-driven engineering, model transformation (MT) veriﬁcation is essential for reliably producing software artifacts.\nWhile recent advancements have enabled automatic Hoare-style veriﬁcation for non-trivial MTs, there are certain veriﬁcation\ntasks (e.g. induction) that are intrinsically difﬁcult to automate. Existing tools that aim at simplifying the interactive veriﬁcation\nof MTs typically translate the MT speciﬁcation (e.g. in ATL) and properties to prove (e.g. in OCL) into an interactive theorem\nprover. However, since the MT speciﬁcation and proof phases happen in separate languages, the proof developer needs a\ndetailed knowledge of the translation logic. Naturally, any error in the MT translation could cause unsound veriﬁcation, i.e. the\nMT executed in the original environment may have different semantics from the veriﬁed MT. We propose an alternative solution\nby designing and implementing an internal domain-speciﬁc language, namely CoqTL, for the speciﬁcation of declarative MTs\ndirectly in the Coq interactive theorem prover. Expressions in CoqTL are written in Gallina (the speciﬁcation language of Coq),\nincreasing the possibilities of reusing native Coq libraries in the transformation deﬁnition and proof. CoqTL speciﬁcations\ncan be directly executed by our transformation engine encoded in Coq, or a certiﬁed implementation of the transformation\ncan be generated by the native Coq extraction mechanism. We ensure that CoqTL has the same expressive power of Gallina\n(i.e. if a MT can be computed in Gallina, then it can also be represented in CoqTL). In this article, we introduce CoqTL,\nevaluate its practical applicability on a use case, and identify its current limitations.",
        "keywords": [
            "Model-driven engineering",
            "Model transformation",
            "Domain-speciﬁc language",
            "Interactive theorem proving",
            "Coq"
        ],
        "authors": [
            "Zheng Cheng",
            "Massimo Tisi",
            "Rémi Douence"
        ],
        "file_path": "data/sosym-all/s10270-019-00765-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "CoqTL"
        }
    },
    {
        "title": "Low-code development and model-driven engineering: Two sides of the same coin?",
        "submission-date": "2021/09",
        "publication-date": "2022/01",
        "abstract": "The last few years have witnessed a signiﬁcant growth of so-called low-code development platforms (LCDPs) both in gaining traction on the market and attracting interest from academia. LCDPs are advertised as visual development platforms, typically running on the cloud, reducing the need for manual coding and also targeting non-professional programmers. Since LCDPs share many of the goals and features of model-driven engineering approaches, it is a common point of debate whether low-code is just a new buzzword for model-driven technologies, or whether the two terms refer to genuinely distinct approaches. To contribute to this discussion, in this expert-voice paper, we compare and contrast low-code and model-driven approaches, identifying their differences and commonalities, analysing their strong and weak points, and proposing directions for cross-pollination.",
        "keywords": [
            "Low-code development",
            "No-code development",
            "Model-driven engineering"
        ],
        "authors": [
            "Davide Di Ruscio",
            "Dimitris Kolovos",
            "Juan de Lara",
            "Alfonso Pierantonio",
            "Massimo Tisi",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-021-00970-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to theme section on open environmental software systems modeling",
        "submission-date": "2022/04",
        "publication-date": "2022/08",
        "abstract": "This theme section aims to disseminate the latest research results in the area of open environmental software systems modeling. Software-intensive systems, such as cyber-physical systems and self-adaptive systems, need to be able to constantly operate in open and dynamic environments, which requires them to continuously evolve, handle internal and external uncertainties, and so on. Traditional software engineering methodologies need to be extended for tackling these new challenges, and modeling is a promising technique to do that. This theme section contains two papers that tackle these issues, that were accepted after a thorough peer-reviewing process. Moreover, it contains an “Expert’s Voice” contribution on the uncertainty interaction problem, which is a relevant issue in the systems considered in this theme section.",
        "keywords": [
            "System modeling",
            "Uncertainty",
            "Open environment",
            "Autonomous",
            "Self-adaptation"
        ],
        "authors": [
            "Tao Yue",
            "Paolo Arcaini",
            "Ji Wu",
            "Xiaowei Huang"
        ],
        "file_path": "data/sosym-all/s10270-022-01032-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based design: a report from the trenches of the DARPA Urban Challenge",
        "submission-date": "2008/10",
        "publication-date": "2009/03",
        "abstract": "The impact of model-based design on the software engineering community is impressive, and recent research in model transformations, and elegant behavioral speciﬁcations of systems has the potential to revolutionize the way in which systems are designed. Such techniques aim to raise the level of abstraction at which systems are speciﬁed, to remove the burden of producing application-speciﬁc programs with general-purpose programming. For complex real-time systems, however, the impact of model-driven approaches is not nearly so widespread. In this paper, we present a perspective of model-based design researchers who joined with software experts in robotics to enter the DARPA Urban Challenge, and to what extent model-based design techniques were used. Further, we speculate on why, according to our experience and the testimonies of many teams, the full promises of model-based design were not widely realized for the competition. Finally, we present some thoughts for the future of model-based design in complex systems such as these, and what advancements in modeling are needed to motivate small-scale projects to use model-based design in these domains.",
        "keywords": [],
        "authors": [
            "Jonathan Sprinkle",
            "J. Mikael Eklund",
            "Humberto Gonzalez",
            "Esten Ingar Grøtli",
            "Ben Upcroft",
            "Alex Makarenko",
            "Will Uther",
            "Michael Moser",
            "Robert Fitch",
            "Hugh Durrant-Whyte",
            "S. Shankar Sastry"
        ],
        "file_path": "data/sosym-all/s10270-009-0116-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On model compatibility with referees and contexts",
        "submission-date": "2011/03",
        "publication-date": "2012/04",
        "abstract": "A model-based engineering discipline presup-\nposes that models are organised by creating relationships\nbetween them. While there has been considerable work on\nunderstanding what it means to instantiate one model from\nanother, little is known about when a model should be con-\nsidered to be a specialisation of another one. This article\nmotivates and discusses ways of deﬁning specialisation rela-\ntionships between models, languages, and transformations\nrespectively. Consideration is given to both structural and\nbehavioural compatibility concerns. Several alternatives of\ndefining a specialisation relationship are considered and dis-\ncussed. The article furthermore discusses the notions of ref-\neree and context in order to validate and define specialisation\nrelationships. The ideas and discussions presented in this arti-\ncle are meant to provide a further stepping stone towards a\nsystematic basis for organising models.",
        "keywords": [
            "Model inheritance",
            "Model compatibility",
            "Language engineering",
            "Model evolution",
            "Subtyping",
            "Refinement",
            "Referee",
            "Context"
        ],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-012-0241-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Encoding process discovery problems in SMT",
        "submission-date": "2014/10",
        "publication-date": "2016/06",
        "abstract": "Information systems, which are responsible for\ndriving many processes in our lives (health care, the web,\nmunicipalities, commerce and business, among others), store\ninformation in the form of logs which is often left unused.\nProcess mining, a discipline in between data mining and\nsoftware engineering, proposes tailored algorithms to exploit\nthe information stored in a log, in order to reason about the\nprocesses underlying an information system. A key challenge\nin process mining is discovery: Given a log, derive a formal\nprocess model that can be used afterward for a formal analy-\nsis. In this paper, we provide a general approach based on\nsatisﬁability modulo theories (SMT) as a solution for this\nchallenging problem. By encoding the problem into the log-\nical/arithmetic domains and using modern SMT engines, it\nis shown how two separate families of process models can\nbe discovered. The theory of this paper is accompanied with\na tool, and experimental results witness the signiﬁcance of\nthis novel view of the process discovery problem.",
        "keywords": [
            "Process discovery",
            "SMT application",
            "Causal\nnets",
            "Petri nets"
        ],
        "authors": [
            "Marc Solé\nJosep Carmona"
        ],
        "file_path": "data/sosym-all/s10270-016-0536-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Generating domain models from natural language text using NLP: a benchmark dataset and experimental comparison of tools",
        "submission-date": "2023/08",
        "publication-date": "2024/05",
        "abstract": "Software requirements speciﬁcation describes users’ needs and expectations on some target system. Requirements documents are typically represented by unstructured natural language text. Such texts are the basis for the various subsequent activities in software development, such as software analysis and design. As part of software analysis, domain models are made that describe the key concepts and relations between them. Since the analysis process is performed manually by business analysts, it is time-consuming and may introduce mistakes. Recently, researchers have worked toward automating the synthesis of domain models from textual software requirements. Current studies on this topic have limitations in terms of the volume and heterogeneity of experimental datasets. To remedy this, we provide a curated dataset of software requirements to be utilized as a benchmark by algorithms that transform textual requirements documents into domain models. We present a detailed evaluation of two text-to-model approaches: one based on a large-language model (ChatGPT) and one building on grammatical rules (txt2Model). Our evaluation reveals that both tools yield promising results with relatively high F-scores for modeling the classes, attributes, methods, and relationships, with txt2Model performing better than ChatGPT on average. Both tools have relatively lower performance and high variance when it comes to the relation types. We believe our dataset and experimental evaluation pave to way to advance the ﬁeld of automated model generation from requirements.",
        "keywords": [
            "Software functional requirements",
            "Software models",
            "Text-to-model transformation",
            "Benchmark dataset"
        ],
        "authors": [
            "Fatma Bozyigit",
            "Tolgahan Bardakci",
            "Alireza Khalilipour",
            "Moharram Challenger",
            "Guus Ramackers",
            "Önder Babur",
            "Michel R. V. Chaudron"
        ],
        "file_path": "data/sosym-all/s10270-024-01176-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Theme issue on Integrated Formal Methods",
        "submission-date": "2015/11",
        "publication-date": "2016/MM",
        "abstract": "This theme issue of the Software and Systems Modeling journal is dedicated to the topic of Integrated Formal Methods. Formal methods allow the modeling and analysis of various aspects of a system. The aim of this theme issue is to provide a resource that describes the state of the art in integrated formal methods and to outline a roadmap that addresses key challenges in this area.",
        "keywords": [],
        "authors": [
            "Einar Broch Johnsen",
            "Luigia Petre"
        ],
        "file_path": "data/sosym-all/s10270-015-0510-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML customization versus domain-speciﬁc languages",
        "submission-date": "2018/06",
        "publication-date": "2018/06",
        "abstract": "We recently collaborated again with 40 developers on a large automotive architecture project. In addition, we discussed some of the characteristics of the project with other modelers and consultants. Several observations emerged that we would like to share.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-018-0685-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dirk Draheim, Gerald Weber: Form-oriented analysis. A new methodology to model form-based applications",
        "submission-date": "2005/05",
        "publication-date": "2005/05",
        "abstract": "In spite of the widespread use of Graphical User Interfaces (GUI), form-based user interfaces still prevail in professional software applications, especially in the business management area. Ordinary internet shopping also makes users navigate through a series of online forms while placing an order and specifying payment details. When working with online forms, users often encounter fatigue and frustration caused by poor design of the form. Form-based interaction is speciﬁc, because it preserves the unfashionable submit-response style interface, revitalized nowadays on the web in countless form-based applications. In principle, they should be operated on a self-service assumption, enabling users to complete their tasks without external help, without frustration and in a minimal number of necessary steps.",
        "keywords": [],
        "authors": [
            "Marcin Sikorski"
        ],
        "file_path": "data/sosym-all/s10270-005-0080-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Testing cockpit display systems of aircraft using a model-based approach",
        "submission-date": "2020/02",
        "publication-date": "2021/01",
        "abstract": "Avionics are highly critical systems that require extensive testing to comply with international safety standards. Cockpit display systems (CDS) are a mandatory part of modern cockpits of both manned and unmanned aircraft. The information from various avionics components is displayed on CDS using a variety of ﬂight instruments. An important part of testing avionics systems is to evaluate whether the displayed information on the CDS is correct or not. A common industrial practice is to manually test CDS, which is time-consuming, labor-intensive, and error-prone. In this paper, we propose a model-based approach to automate the CDS testing of aircraft. The proposed approach tests the CDS at two levels: (i) at the system level to verify that the CDS are working correctly and (ii) at system integration level of CDS when these are integrated with various avionics components. As a part of our approach, we develop a UML proﬁle to model various elements of the CDS. The models are then used to support the automated testing process. We evaluate our approach on two industrial case studies, the ﬁrst case study represents a primary ﬂight display (PFD) of an aircraft and the second one is the CDS of the ground control station (GCS-CDS) of an unmanned aerial vehicle. The evaluation results show that three potential faults are identiﬁed in the PFD and four major faults are found in the GCS-CDS.",
        "keywords": [
            "Model-based testing (MBT)",
            "Safety-critical systems",
            "Cockpit display systems (CDS)",
            "Unmanned aerial vehicle (UAV)",
            "Ground control station (GCS)"
        ],
        "authors": [
            "Hassan Sartaj",
            "Muhammad Zohaib Iqbal",
            "Muhammad Uzair Khan"
        ],
        "file_path": "data/sosym-all/s10270-020-00844-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Accelerating task completion in mobile ofﬂoading systems through adaptive restart",
        "submission-date": "2015/06",
        "publication-date": "2016/05",
        "abstract": "Mobile application ofﬂoading is an efﬁcient technique to unload the burden of intensive computation from thin clients to powerful servers. In a mobile ofﬂoading system, cloud computing is utilized to complete some heavy tasks which are migrated from resource-constrained mobile devices to the Cloud. To assure system performance, the quality of the wireless network connection plays an important role. In previous work we experimentally explored the impact of packet loss and delay in wireless networks on the completion time of an ofﬂoading task. We investigated a local restart mechanism to mitigate these effects. In the presence of unreliable communication, once the waiting time for the response of a cloud server exceeds a given threshold, exploiting the local resources of a mobile client can accelerate the task completion. In this paper, we upgrade the restart mechanism by allowing several ofﬂoading retries before a job eventually is locally restarted and ﬁnally completed in the client device itself. This is an adaptive restart scheme which aims ﬁrst at completing the job using restart with ofﬂoading. If several successive ofﬂoading attempts fail the job is completed locally. Adaptively selecting the right retry threshold and automatically restarting at the appropriate moment can balance out undesired effects. This paper extends Wang and Wolter (Proceedings of the 6th ACM/SPEC international conference on performance engineering. ACM, pp 3–13, 2015) by adding an adaptive retry scheme, a mathematical derivation of the optimal limit for ofﬂoading attempts so as to minimize the task completion time using a greedy method, and by the results of a practical evaluation study which shows the efﬁciency and beneﬁts of the adaptive restart scheme.",
        "keywords": [
            "Mobile ofﬂoading",
            "Restart",
            "Unreliable network"
        ],
        "authors": [
            "Qiushi Wang",
            "Katinka Wolter"
        ],
        "file_path": "data/sosym-all/s10270-016-0531-3.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Use Case Maps as a property speciﬁcation language",
        "submission-date": "2006/11",
        "publication-date": "2007/12",
        "abstract": "Although a signiﬁcant body of research in the\narea of formal veriﬁcation and model checking tools of soft-\nware and hardware systems exists, the acceptance of these\ntools by industry and end-users is rather limited. Beside the\ntechnical problem of state space explosion, one of the main\nreasons for this limited acceptance is the unfamiliarity of\nusers with the required speciﬁcation notation. Requirements\nhave to be typically expressed as temporal logic formalisms\nand notations. Property speciﬁcation patterns were success-\nfully introduced to bridge this gap between users and model\nchecking tools. They also enable non-experts to write formal\nspeciﬁcations that can be used for automatic model checking.\nIn this paper, we propose an abstract high level pattern-based\napproach to the description of property speciﬁcations based\non Use Case Maps (UCM). We present a set of commonly\nused properties with their speciﬁcations that are described in\nterms of occurrence, ordering and temporal scopes of actions.\nFurthermore, our approach also supports the description of\nproperties with respect to their architectural scope. We pro-\nvide a mapping of our UCM property speciﬁcation patterns in\nterms of CTL, TCTL and Architectural TCTL (ArTCTL), an\nextension to TCTL, introduced in this research that provides\ntemporal logics with architectural scopes. We illustrate the\nuse of our pattern system for requirement speciﬁcations of\nan IP Header compression feature.",
        "keywords": [
            "Formal veriﬁcation",
            "Temporal logic",
            "Property speciﬁcation",
            "Use Case Maps",
            "Temporal and architectural scope"
        ],
        "authors": [
            "Jameleddine Hassine",
            "Juergen Rilling",
            "Rachida Dssouli"
        ],
        "file_path": "data/sosym-all/s10270-007-0076-6.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "A manifesto for applicable formal methods",
        "submission-date": "2023/06",
        "publication-date": "2023/08",
        "abstract": "Recently, formal methods have been used in large industrial organisations (including AWS, Facebook/Meta, and Microsoft) and have proved to be an effective part of a software engineering process ﬁnding important bugs. Perhaps because of that, practitioners are interested in using them more often. Nevertheless, formal methods are far less applied than expected, particularly for safety-critical systems where they are strongly recommended and have the most signiﬁcant potential. We hypothesise that formal methods still seem not applicable enough or ready for their intended use in such areas. In critical software engineering, what do we mean when we speak of a formal method? And what does it mean for such a method to be applicable both from a scientiﬁc and practical viewpoint? Based on what the literature tells about the ﬁrst question, with this manifesto, we identify key challenges and lay out a set of guiding principles that, when followed by a formal method, give rise to its mature applicability in a given scope. Rather than exercising criticism of past developments, this manifesto strives to foster increased use of formal methods in any appropriate context to the maximum beneﬁt.",
        "keywords": [
            "Formal methods",
            "Formal veriﬁcation",
            "Software engineering",
            "Tools",
            "Research evaluation",
            "Research transfer"
        ],
        "authors": [
            "Mario Gleirscher",
            "Jaco van de Pol",
            "Jim Woodcock"
        ],
        "file_path": "data/sosym-all/s10270-023-01124-2.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special issue on MODELS 2008",
        "submission-date": "2011/05",
        "publication-date": "Not found",
        "abstract": "Welcome to this special issue of “Software and Systems Modeling,” devoted to selected papers of the eleventh ACM/IEEE International Conference on Model-Driven Engineering Languages and Systems (MODELS’08). MODELS is the premier annual conference featuring research results and practical experiences in model-driven engineering of complex, software-intensive systems.",
        "keywords": [],
        "authors": [
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-011-0202-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Meta-environment and executable meta-language using smalltalk: an experience report",
        "submission-date": "2007/03",
        "publication-date": "2008/03",
        "abstract": "Object-oriented modelling languages such as\nEMOF are often used to specify domain speciﬁc meta-\nmodels. However, these modelling languages lack the abil-\nity to describe behavior or operational semantics. Several\napproaches have used a subset of Java mixed with OCL\nas executable meta-languages. In this experience report we\nshow how we use Smalltalk as an executable meta-language\nin the context of the Moose reengineering environment. We\npresent how we implemented EMOF and its behavioral\naspects. Over the last decade we validated this approach\nthrough incrementally building a meta-described reengineer-\ning environment. Such an approach bridges the gap between\na code-oriented view and a meta-model driven one. It avoids\nthe creation of yet another language and reuses the infrastruc-\ntureandrun-timeoftheunderlyingimplementationlanguage.\nIt offers an uniform way of letting developers focus on their\ntasks while at the same time allowing them to meta-describe\ntheir domain model. The advantage of our approach is that\ndevelopers use the same tools and environment they use for\ntheir regular tasks. Still the approach is not Smalltalk speciﬁc\nbut can be applied to language offering an introspective API\nsuch as Ruby, Python, CLOS, Java and C#.",
        "keywords": [
            "Meta behavior description",
            "Reﬂective\nlanguage",
            "Executable modeling language",
            "Smalltalk"
        ],
        "authors": [
            "Stéphane Ducasse",
            "Tudor Girba",
            "Adrian Kuhn",
            "Lukas Renggli"
        ],
        "file_path": "data/sosym-all/s10270-008-0081-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using reactive links to propagate changes across engineering models",
        "submission-date": "2023/05",
        "publication-date": "2024/06",
        "abstract": "Collaborative model-driven development is a de facto practice to create software-intensive systems in several domains (e.g., aerospace, automotive, and robotics). However, when multiple engineers work concurrently, keeping all model artifacts synchronized and consistent is difﬁcult. This is even harder when the engineering process relies on a myriad of tools and domains (e.g., mechanic, electronic, and software). Existing work tries to solve this issue from different perspectives, such as using trace links between different artifacts or computing change propagation paths. However, these solutions mainly provide additional information to engineers, still requiring manual work for propagating changes. Yet, most modeling tools are limited regarding the traceability between different domains, while also lacking the efﬁciency and granularity required during the development of software-intensive systems. Motivated by these limitations, in this work, we present a solution based on what we call “reactive links”, which are highly granular trace links that propagate change between property values across models in different domains, managed in different tools. Differently from traditional “passive links”, reactive links automatically propagate changes when engineers modify models, assuring the synchronization and consistency of the artifacts. The feasibility, performance, and ﬂexibility of our solution were evaluated in three practical scenarios, from two partner organizations. Our solution is able to resolve all cases in which change propagation among models were required. We observed a great improvement of efﬁciency when compared to the same propagation if done manually. The contribution of this work is to enhance the engineering of software-intensive systems by reducing the burden of manually keeping models synchronized and avoiding inconsistencies that potentially can originate from collaborative engineering in a variety of tool from different domains.",
        "keywords": [
            "Change propagation",
            "Model-driven engineering",
            "Heterogeneous models",
            "Collaboration",
            "Multi-domain traceability"
        ],
        "authors": [
            "Cosmina-Cristina Ra¸tiu",
            "Wesley K. G. Assunção",
            "Edvin Herac",
            "Rainer Haas",
            "Christophe Lauwerys",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-024-01186-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Report on the State of the SoSyM Journal (2024 summary)",
        "submission-date": "2025/02",
        "publication-date": "2025/02",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Stéphanie Challita",
            "Benoit Combemale",
            "Huseyin Ergin",
            "JeﬀGray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-025-01268-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Business processes in the agile organisation: a socio-technical perspective",
        "submission-date": "2014/10",
        "publication-date": "2015/11",
        "abstract": "This paper takes a cross-disciplinary view of the ontology of “business process”: how the concept is treated in the IS research literature and how related concepts (with stronger human behavioural orientation) from organisation and management sciences can potentially inform this IS perspective. In particular, is there room for socio-technical concepts such as technology affordance, derived from the constructivist tradition, in improving our understanding of operational business processes, particularly human-centric business processes? The paper presents a theoretical framework for understanding the role of business processes in organisational agility that distinguishes between the process-as-designed and the process-as-practiced. How this practice aspect of business processes also leads to the improvisation of various information technology enablers, is explored using a socio-technical lens. The posited theoretical framework is illustrated and validated with data drawn from an interpretive empirical case study of a large IT services company. The research suggests that processes within the organisation evolve both by top-down design and by the bottom-up routinisation of practice and that the tension between these is driven by the need for ﬂexibility.",
        "keywords": [
            "Socio-technical systems",
            "Organisational agility",
            "Business process",
            "Technology affordance"
        ],
        "authors": [
            "Charles Crick",
            "Eng K. Chew"
        ],
        "file_path": "data/sosym-all/s10270-015-0506-9.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the theme issue on domain-speciﬁc modeling in theory and applications",
        "submission-date": "2014/01",
        "publication-date": "2014/01",
        "abstract": "Interest in domain-speciﬁc modeling (DSM) comes from the aspiration to signiﬁcantly improve the productivity and quality of software development by raising the level of abstraction beyond programming. This is done by specifying the solution directly using domain concepts, rather than lower level programminglanguageandspeciﬁcplatformconceptsthatintro-duce layers of accidental complexity. In the past, productivity gains have been sought from new programming languages. Today, domain-speciﬁc modeling languages (DSMLs) provide a solution for continuing to raise the level of abstraction beyond coding, making development faster and easier.",
        "keywords": [],
        "authors": [
            "Juha-Pekka Tolvanen",
            "Matti Rossi",
            "Jeff Gray"
        ],
        "file_path": "data/sosym-all/s10270-013-0319-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Specifying dynamic software system architectures",
        "submission-date": "2020/10",
        "publication-date": "2021/03",
        "abstract": "The inexorable penetration of software into practically every facet of modern society calls for sophisticated architectural \nstyles, including ones that can support architectures with dynamically shifting structures, which are required to cope with \nthe dynamics of their applications. With the advent of modern Internet-based systems operating in real time, these types of \nsystems are becoming more widespread. Unfortunately, to date there has been insufficient theoretical work on suitable archi-\ntectural design patterns for such systems. This work describes two such patterns: the dynamic part pattern and the dynamic \nrole pattern, both of which have been proven in earlier-generation dynamic real-time systems. In addition to describing the \nform and semantics of these design patterns, this work proposes a notational form suitable for specifying them in component-\nand-connector-style architecture description languages in a clear and unambiguous manner. The practical application of the \ntwo patterns is illustrated using a running example.",
        "keywords": [
            "Architectural description languages",
            "Dynamic system structure",
            "Architectural design patterns"
        ],
        "authors": [
            "Bran Selić"
        ],
        "file_path": "data/sosym-all/s10270-021-00875-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards Standardized benchmarks of LLMs in software modeling tasks: a conceptual framework",
        "submission-date": "2024/07",
        "publication-date": "Not found",
        "abstract": "The integration of Large Language Models (LLMs) in software modeling tasks presents both opportunities and challenges. This Expert Voice addresses a signiﬁcant gap in the evaluation of these models, advocating for the need for standardized benchmarking frameworks. Recognizing the potential variability in prompt strategies, LLM outputs, and solution space, we propose a conceptual framework to assess their quality in software model generation. This framework aims to pave the way for standardization of the benchmarking process, ensuring consistent and objective evaluation of LLMs in software modeling. Our conceptual framework is illustrated using UML class diagrams as a running example.",
        "keywords": [
            "Modeling",
            "LLMs",
            "Benchmarking"
        ],
        "authors": [
            "Javier Cámara",
            "Lola Burgueño",
            "Javier Troya"
        ],
        "file_path": "data/sosym-all/s10270-024-01206-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section on ICMT at STAF 2018",
        "submission-date": "2019/12",
        "publication-date": "2020/01",
        "abstract": "Modelling is a key element in reducing the complexity of software systems during their development and maintenance. Model transformations are essential for elevating models from documentation elements to ﬁrst-class artifacts. Transformations also play a key role in analysing models to reveal conceptual ﬂaws or highlight quality bottlenecks and in integrating heterogeneous tools into uniﬁed tool chains.",
        "keywords": [],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Arend Rensink"
        ],
        "file_path": "data/sosym-all/s10270-020-00775-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How do organizational capabilities mature? maturity level characteristics for organizational capabilities",
        "submission-date": "2024/11",
        "publication-date": "2025/06",
        "abstract": "In a dynamic business environment, organizations must continuously evolve to sustain a competitive edge by developing new capabilities while also enhancing the existing ones. Maturity models have become a useful management tool for enhancing organizational capabilities, offering a structured framework to assess and improve capabilities over time. By deﬁning distinct stages of development, maturity models help organizations identify their current state and plan future improvements. However, despite their widespread use in domains like software development and project management, a signiﬁcant theoretical gap remains in the conceptualization of maturity levels, especially those related to organizational capabilities. This paper presents a maturity level characterization model (MLCM) for organizational capabilities, addressing the gap in existing research by providing a model that can be used to develop capability maturity models. Drawing from the Dreyfus model of skill acquisition, this model deﬁnes six stages of maturity, ranging from unaware to expert, and offers a theoretically grounded instantiation template for developing maturity levels for organizational capabilities. The research utilizes design science research methodology, with the model being applied and evaluated in the context of advanced data analytics capabilities. Two evaluation questionnaires among 18 practitioners conﬁrm the model’s validity, relevance, completeness, clarity, and usefulness, providing a solid foundation for developing capability maturity levels. Theoretically, this research advances maturity modeling literature by conceptualizing organizational capability maturity by using the Dreyfus model of skill acquisition as a foundational meta-theory, and combining perspectives from institutional theory. Practically, it provides a useful tool for both researchers and practitioners aiming to evaluate and improve organizational capabilities in various domains.",
        "keywords": [
            "Capability maturity models",
            "Maturity levels",
            "Organizational capability",
            "Dreyfus model"
        ],
        "authors": [
            "Ginger Korsten",
            "Baris Ozkan",
            "Banu Aysolmaz",
            "Daan Mul",
            "Oktay Turetken"
        ],
        "file_path": "data/sosym-all/s10270-025-01309-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A metamodeling language supporting subset and union properties",
        "submission-date": "2006/09",
        "publication-date": "2007/06",
        "abstract": "In this article, we describe successive versions of a metamodeling language using a set-theoretic formalization. We focus on language extension mechanisms, particularly on the relatively new subset and union properties of MOF 2.0 and the UML 2.0 Infrastructure. We use Liskov substitutability as the rationale for our formalization. We also show that property redeﬁnitions are not a safe language extension mechanism. Each language version provides new features,andwenotehowsuchfeaturescannotbemixedarbi-trarily. Instead, constraints over the metamodel and model structures must be established. We expect that this article provides a better understanding of the foundations of MOF 2.0, which is necessary to deﬁne new extensions, model transformation languages and tools.",
        "keywords": [
            "Subsets",
            "Unions",
            "Redeﬁnitions",
            "UML",
            "Package merges",
            "Extension mechanisms",
            "Graphs",
            "Graph theory",
            "Software modeling languages",
            "Metamodeling",
            "MOF"
        ],
        "authors": [
            "Marcus Alanen",
            "Ivan Porres"
        ],
        "file_path": "data/sosym-all/s10270-007-0049-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Describing and assessing availability requirements in the early stages of system development",
        "submission-date": "2012/06",
        "publication-date": "2013/09",
        "abstract": "Non-functional aspects such as timing constraints, availability, and fault tolerance are critical in the design and implementation of distributed real-time systems. As a result, it is becoming crucial to model and analyze non-functional requirements at the early stages of the software development life cycle. The widespread interest in dependability modeling and analysis techniques at the requirements phase provides the major motivation for this research. This paper presents a novel approach to describe and validate high-level availability requirements using the Use Case Maps (UCM) language of the ITU-T User Requirements Notation standard. The proposed approach relies on a mapping of availability architectural tactics to UCM models. The resulting extensions are described using a metamodel and are implemented within the jUCMNav tool. Early assessment and characterization of the means to achieve availability are then performed using a matrix representation allowing for feature-based availability composition and reasoning. We demonstrate the applicability of our approach through a case study of lawful intercept and ACL-based forwarding features on IP routers.",
        "keywords": [
            "Non-functional requirements",
            "Availability",
            "URN",
            "Use Case Maps",
            "Availability analysis",
            "Architectural tactics"
        ],
        "authors": [
            "Jameleddine Hassine"
        ],
        "file_path": "data/sosym-all/s10270-013-0382-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling Paradigms",
        "submission-date": "2005/06",
        "publication-date": "2005/06",
        "abstract": "This issue contains three papers in its regular section and includes a special section that consists of extended versions of the best papers from the “St.Eve” (State versus Event-based Modeling) Workshop. In the state-based modeling paradigm, behavior is conceptualized and described in terms of state changes. Behavior in data intensive applications (e.g., business systems) and in control intensive systems (e.g., embedded controllers) ﬁt well in this paradigm. Event-based modeling is particularly suited to describing systems in terms of interactions across their constituent interfaces in the early development phases (e.g., in requirements and high-level architecture phases). Focusing on interactions across interfaces in the early phases is good practice. It allows a developer to abstract out irrelevant internal details while gaining an early understanding of required interactions and constraints on how parts interact in an application. The focus on understanding interactions across interfaces in the early stages can also lead to early convergence of stable application architectures.\n\nState and event-based modeling paradigms are not the only paradigms that are used when modeling software based systems and their context. For example, neither approach is well-suited to modeling workﬂows. In addition, physical or logical distribution and deployment, and threads of activity cannot be adequately described using events or states. It should not come as a surprise that the development of an application may require the use of multiple modeling paradigms. Integrating multiple modeling paradigms is one of the great challenges of model-drive development. An eﬀective integration must be based on a deep understanding of the relationships among modeling paradigms. Workshops such as “St.Eve” help the community develop such an understanding.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-005-0096-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "DEVS-based formalism for the modeling of routing processes",
        "submission-date": "2020/05",
        "publication-date": "2021/10",
        "abstract": "The Discrete Event System Speciﬁcation (DEVS) is a modular and hierarchical Modeling and Simulation (M&S) formalism\nbased on systems theory that provides a general methodology for the construction of reusable models. Well-deﬁned M&S\nstructures have a positive impact when building simulation models because they can be applied systematically. However, even\nwhen DEVS can be used to model routing situations, the structures that emerge from this kind of problem are signiﬁcant due\nto the handling of the ﬂow of events. Often, the modeler ends with a lot of simulation models that refer to variants of the\nsame component. The goal of this paper is to analyze the routing process domain from a conceptual modeling perspective\nthrough the use of a new DEVS extension called Routed DEVS (RDEVS). The RDEVS formalism is conceptually deﬁned as\na subclass of DEVS that manages a set of identiﬁed events inside a model network where each node combines a behavioral\ndescription with a routing policy. In particular, we study the modeling effort required to solve the M&S of routing problems\nscenarios employing a comparison between RDEVS modeling solutions and DEVS modeling strategies. Such a comparison\nis based on measures that promote the capture of the behavioral complexity of the ﬁnal models. The results obtained highlight\nthe modeling beneﬁts of the RDEVS formalism as a constructor of routing processes. The proposed solution reduces the\nmodeling effort involved in DEVS by specifying the event routing process directly in the RDEVS models using design\npatterns. The novel contribution is an advance in the understanding of how DEVS as a system modeling formalism supports\nbest practices of software engineering in general and conceptual modeling in particular. The reusability and ﬂexibility of the\nﬁnal simulation models, along with designs with low coupling and high cohesion, are the main beneﬁts of the proposal that\nimprove the M&S task applying a conceptual modeling perspective.",
        "keywords": [
            "Routed Discrete Event System Speciﬁcation",
            "Conceptual modeling",
            "Modeling effort",
            "Routing problem",
            "Design patterns"
        ],
        "authors": [
            "María Julia Blas",
            "Horacio Leone",
            "Silvio Gonnet"
        ],
        "file_path": "data/sosym-all/s10270-021-00928-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Scenario-based system design with colored Petri nets: an application to train control systems",
        "submission-date": "2014/06",
        "publication-date": "2016/02",
        "abstract": "For the goal of model-based system software development, this paper exploits the formalism of colored Petri nets (CPNs) to design complex systems based on scenarios. The speciﬁcation of UML sequence diagrams which are easily understood by customers, requirement engineers and software developers are adopted to represent scenarios as speciﬁcation models. A scenario is a partial description of the system behavior, describing how users, system components and the environment interact. Thus scenarios need to be synthesized in order to obtain an overall system behavior. A large number of works (e.g., Whittle and Schumann in Proceedings of the 2000 international conference on soft-ware engineering, pp 314–323, 2000; Elkoutbi and Keller in Application and theory of Petri nets, 2000; Damas et al. in Proceedings of the 14th ACM SIGSOFT international symposium on foundations of software engineering, pp 197–207, 2000; Uchitel et al. in IEEE Trans Softw Eng 29(2):99–115, 2003) have investigated scenario synthesis providing approaches or algorithms. These synthesis approaches and algorithms result in either Petri net models (e.g., Elkoutbi and Keller 2000; Ameedeen and Bordbar in 12th international Communicated by Dr. Kevin Lano. This paper is partly supported by the BJTU Founds for Young Scientists under Grant 2015RC072 and the National Natural Science Foundation of China under Grant 61502029. IEEE enterprise distributed object computing conference (EDOC), pp 213–221, 2008) that are mainly suitable for scenario validation or other forms of behavior models (e.g., labeled transition systems in Damas et al. 2000; Uchitel et al. 2003 and statecharts in Krüger et al. in Distributed and parallel embedded systems, pp 61–71, 1999; Whittle et al. 2000) that may be regarded as design models. Petri nets are well known for describing distributed and concurrent complex systems. Furthermore, numerous techniques, e.g., simulation, testing, state space-based techniques, structural methods and model checking, are currently available for analyzing Petri net models. Therefore, design models in the form of Petri nets, integrating all scenarios into a coherent whole and fitting for further detailed design, are promising. To this end, we present a top-down approach to establish hierarchical CPNs in accordance with specified scenarios (i.e., sequence diagrams). This approach makes use of explicitly labeling component states in the sequence diagrams to correlate scenarios. In addition, the techniques of state space analysis and ASK-CTL model checking are used to verify the correctness and consistency of the CPN model with respect to standard and system-specific properties. As the inspiration of the presented approach derives from the development of train control systems, we present an running example of designing the on-board subsystem of a satellite-based train control system to show the feasibility of our approach.",
        "keywords": [
            "Scenario",
            "System design",
            "Modeling",
            "Veriﬁcation",
            "Colored Petri nets",
            "Train control system"
        ],
        "authors": [
            "Daohua Wu",
            "Eckehard Schnieder"
        ],
        "file_path": "data/sosym-all/s10270-016-0517-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An approach for bug localization in models using two levels: model and metamodel",
        "submission-date": "2018/03",
        "publication-date": "2019/03",
        "abstract": "Bug localization is a common task in software engineering, especially when maintaining and evolving software products. This paper introduces a bug localization approach that, in contrast to existing source code approaches, takes advantage of domain information found in the model and the metamodel. Throughout this paper, we present an approach for bug localization in models (BLiM2) that applies the source code ideas for bug localization (textual similarity to the bug description and the Defect Localization Principle) and takes advantage of the domain information from the model and the metamodel. We evaluated our approach in BSH, a real-world industrial case study in the induction hob domain measuring the results in terms of recall, precision, the combination of both the F-measure and the Matthews correlation coefﬁcient. Our study shows that our BLiM2 approach, which combines information from the model and the metamodel for the textual similarity and differentiates between the timespan from the model and metamodel, provides the best results in this work. We also performed a statistical analysis to provide evidence of the signiﬁcance of the results. The values obtained show that there exist signiﬁcant differences in the performance of the best BLiM2 approach with the approach used by our industrial partner. Finally, the effect size statistics reveals that the best BLiM2 approach obtains better results in the 78% of the times in the worst case.",
        "keywords": [
            "Bug localization",
            "Model-driven engineering",
            "Reverse engineering"
        ],
        "authors": [
            "Lorena Arcega",
            "Jaime Font",
            "Øystein Haugen",
            "Carlos Cetina"
        ],
        "file_path": "data/sosym-all/s10270-019-00727-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A transformation contract to generate aspects from access control policies",
        "submission-date": "2009/01",
        "publication-date": "2010/03",
        "abstract": "Access control is an important security issue. It has been addressed since the late 1960s in the early time-sharing computer systems. Many access control models have been proposed since than but of particular interest is Ferraiolo and Khun’s role-based access control model (RBAC). It is a simple and yet general model which has been deeply studied and applied both in industry and in academia. A variety of industrial standards have been proposed based on this model. Generating code for an access control policy is an interesting challenge. Understanding access control as a non-functional concern that cross-cuts the functional part of a systemraises difﬁculties quitesuitablefor asolutionbasedon aspect-oriented programming. In this paper, we address the problems of speciﬁcation and validation of code generation for access control policies targeting an aspect-based infra-structure. We propose an MDA approach. The code generator is a transformation from SecureUML, an RBAC-based mod-eling language, to the language Aspects for Access Control (AAC), an aspect-oriented modeling language proposed in this paper. Metamodels are used to represent the languages and to specify the transformation. A metamodel is used to represent the abstract syntax of a language and the constraints that a given instance model of the metamodel must fulﬁll. This paper is dedicated with affection to the memory of Vinicius Braga.",
        "keywords": [],
        "authors": [
            "Christiano Braga"
        ],
        "file_path": "data/sosym-all/s10270-010-0156-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-integrating development of software systems: a ﬂexible component-based approach",
        "submission-date": "2017/04",
        "publication-date": "2018/06",
        "abstract": "A promising way to develop ﬂexible software systems is to include models that are analyzed, modiﬁed and executed at\nruntimes as an integrated part of the system. Building such model-integrating systems is a challenging task since the respective\nmodeling languages have to be supported comprehensively at runtime, and these systems still need to be developable in\na modular way by composing them from basic building blocks. Model-driven (MDD) and component-based development\n(CBD) are two established orthogonal approaches that can tackle the mentioned challenges. MDD is based on the use of models\nand modeling languages as ﬁrst-class entities to systematically engineer software systems. CBD enables the engineering of\nmodular systems by facilitating a divide-and-conquer approach with reuse. However, combining and aligning the individual\nprinciples from both approaches is an open research problem. In this article, we describe model-integrating development\n(MID), an engineering approach that enables the systematic development of component-based, model-integrating software.\nMID combines principles from MDD and CBD and is based on the central assumption that models and code shall be treated\nequally as ﬁrst-class entities of software throughout its life cycle. In particular, MID leverages the added ﬂexibility that comes\nwith models at runtime, i.e., when models are an integral part of running software. The practicability of the proposed solution\nconcept is rationalized based on a reference implementation that provides the basis for a thoroughly described and critically\ndiscussed feasibility study: a dynamic access control product line. The obtained beneﬁts are presented in a distilled way, and\nfuture research challenges are identiﬁed.",
        "keywords": [
            "Model-integrating development",
            "Model-integrating components",
            "Model-integrating software",
            "Software\ncomponents",
            "Modeling language engineering",
            "Models at runtime",
            "Self-adaptive software"
        ],
        "authors": [
            "Mahdi Derakhshanmanesh",
            "Jürgen Ebert",
            "Marvin Grieger",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-018-0682-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A personal retrospective on language workbenches",
        "submission-date": "2023/01",
        "publication-date": "2023/03",
        "abstract": "Model-driven software engineering and specifically domain-specific languages have contributed to improve the quality of software and the efficiency in the development of software. However, the design and implementation of domain-specific languages requires still an enormous investment. Language workbenches are the most important tools in the field of software language engineering. The introduction of language workbenches has alleviated partly the development effort, but there are still a few major challenges that need to be tackled. This paper presents a personal perspective on the development of tools for language engineering and language workbenches in particular and future challenges to be tackled.",
        "keywords": [
            "Language engineering",
            "DSLs",
            "Programming environment generators",
            "Language workbenches"
        ],
        "authors": [
            "Mark van den Brand"
        ],
        "file_path": "data/sosym-all/s10270-023-01101-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling the obsolescence of models",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Programs, like people, get old. The same is true for models, which can become obsolete due to a diversity of factors such as changing requirements, data drift or evolution of the domain itself. Preventing or addressing obsolescence as early as possible helps to reduce the significant costs, risks, and uncertainties incurred by obsolete models and the software system generated from them. Indeed, obsolescence in models can easily propagate to errors in the system resulting in behavioral uncertainty marked by unforeseen, emergent, or unpredictable behavior. Nevertheless, methods and strategies to identify, anticipate, minimize, and manage model obsolescence are presently lacking. This paper presents an innovative approach to tackle model obsolescence. We have designed a domain-specific language (DSL) to specify potential aging and degradation conditions for model elements. Based on the DSL annotations and the history of changes in a model, we can pinpoint those elements that require validation or risk becoming obsolete. Both the DSL and the engine to calculate the obsolescence status of the elements in a model have been released as part of the open-source BESSER modeling platform.",
        "keywords": [
            "Model obsolescence",
            "Model-driven",
            "Domain-specific language"
        ],
        "authors": [
            "Iván Alfonso",
            "Jean-Sébastien Sottet",
            "Pierre Brimont",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-024-01236-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A generic framework for representing and analyzing model concurrency",
        "submission-date": "2021/03",
        "publication-date": "2023/01",
        "abstract": "Recent results in language engineering simplify the development of tool-supported executable domain-speciﬁc modeling languages (xDSMLs), including editing (e.g., completion and error checking) and execution analysis tools (e.g., debugging, monitoring and live modeling). However, such frameworks are currently limited to sequential execution traces and cannot handleexecutiontraces resultingfromanexecutionsemantics withaconcurrencymodel supportingparallelismor interleaving. This prevents the development of concurrency analysis tools, like debuggers supporting the exploration of model executions resulting from different interleavings. In this paper, we present a generic framework to integrate execution semantics with either implicit or explicit concurrency models, to explore the possible execution traces of conforming models, and to deﬁne strategies for helping in the exploration of the possible executions. This framework is complemented with a protocol to interact with the resulting executions and hence to build advanced concurrency analysis tools. The approach has been implemented within the GEMOC Studio. We demonstrate how to integrate two representative concurrent meta-programming approaches (MoCCML/Java and Henshin), which use different paradigms and underlying foundations to deﬁne an xDSML’s concurrency model. We also demonstrate the ability to deﬁne an advanced concurrent omniscient debugger with the proposed protocol. The paper, thus, contributes key abstractions and an associated protocol for integrating concurrent meta-programming approaches in a language workbench, and dynamically exploring the possible executions of a model in the modeling workbench.",
        "keywords": [
            "Language engineering",
            "Model execution",
            "Model concurrency",
            "Simulation",
            "Concurrent analyses/debugging"
        ],
        "authors": [
            "Steffen Zschaler",
            "Erwan Bousse",
            "Julien Deantoni",
            "Benoit Combemale"
        ],
        "file_path": "data/sosym-all/s10270-022-01073-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Full contract veriﬁcation for ATL using symbolic execution",
        "submission-date": "2016/02",
        "publication-date": "2016/07",
        "abstract": "The Atlas Transformation Language (ATL) is currently one of the most used model transformation languages and has become a de facto standard in model-driven engineering for implementing model transformations. At the same time, it is understood by the community that enhancing methods for exhaustively verifying such transformations allows for a more widespread adoption of model-driven engineering in industry. A variety of proposals for the veriﬁcation of ATL transformations have arisen in the past few years. However, the majority of these techniques are either based on non-exhaustive testing or on proof methods that require human assistance and/or are not complete. In this paper, we describe our method for statically verifying the declarative subset of ATL model transformations. This veriﬁcation is performed by translating the transformation (including features like ﬁlters, OCL expressions, and lazy rules) into our model transformation language DSLTrans. As we handle only the declarative portion of ATL, and DSLTrans is Turing-incomplete, this reduction in expressivity allows us to use a symbolic-execution approach to generate representations of all possible input models to the transformation. We then verify pre-/post-condition contracts on these representations, which in turn veriﬁes the transformation itself. The technique we present in this paper is exhaustive for the subset of declarative ATL model transformations. This means that if the prover indicates a contract holds on a transformation, then the contract’s pre-/post-condition pair will be true for any input model for that transformation. We demonstrate and explore the applicability of our technique by studying several relatively large and complex ATL model transformations, including a model transformation developed in collaboration with our industrial partner. As well, we present our ‘slicing’ technique. This technique selects only those rules in the DSLTrans transformation needed for contract proof, thereby reducing proving time.",
        "keywords": [
            "Model transformation",
            "ATL",
            "Formal veriﬁcation",
            "Symbolic execution",
            "Contracts",
            "Pre-/post-conditions"
        ],
        "authors": [
            "Bentley James Oakes",
            "Javier Troya",
            "Levi Lúcio",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-016-0548-7.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": true,
            "language": "ATL, DSLTrans"
        }
    },
    {
        "title": "Guest editorial to the special issue on “modelling–foundations and applications”",
        "submission-date": "2015/01",
        "publication-date": "2015/01",
        "abstract": "This paper is a guest editorial for a special issue of the Software and Systems Modelling (SoSyM) journal, stemming from the 2012 European Conference on Modelling Foundations and Applications (ECMFA 2012). It outlines the growth of model-driven engineering (MDE) and the role of ECMFA as a leading European conference in the field. The editorial describes the selection process for papers included in the special issue, which were chosen from those presented at ECMFA 2012 based on reviewer feedback and audience impact.",
        "keywords": [],
        "authors": [
            "Antonio Vallecillo",
            "Juha-Pekka Tolvanen"
        ],
        "file_path": "data/sosym-all/s10270-013-0377-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Selecting a process variant modeling approach: guidelines and application",
        "submission-date": "2017/01",
        "publication-date": "2017/12",
        "abstract": "Various modeling approaches have been introduced to manage process diversity in a business context. For practitioners, it is difﬁcult to select an approach suitable for the needs and limitations of their organization due to the limited number of examples and guidelines. In this paper, we report on an action research study to perform a comparative process variant modeling application in a process management consultancy company. This company experienced difﬁculties in maintaining and reusing process deﬁnitions of their customers. We describe how the requirements were determined and led to the selection of two speciﬁc approaches, the Decomposition Driven Method and the Provop approach. We comparatively evaluated the suitability of these approaches to develop variant models for six software project management processes of ﬁve customers. This study contributes to the ﬁeld by presenting an industrial case for process variant modeling, reporting in-depth, real-life applications of two approaches, applying the approaches for hierarchical processes, and presenting guidelines for choosing an approach under comparable conditions.",
        "keywords": [
            "Business process modeling",
            "Process variant modeling",
            "Decomposition driven method",
            "Provop"
        ],
        "authors": [
            "Banu Aysolmaz",
            "Dennis M. M. Schunselaar",
            "Hajo A. Reijers",
            "Ali Yaldiz"
        ],
        "file_path": "data/sosym-all/s10270-017-0648-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "AI-powered model repair: an experience report—lessons learned, challenges, and opportunities",
        "submission-date": "2021/03",
        "publication-date": "2022/02",
        "abstract": "Artificial intelligence has already proven to be a powerful tool to automate and improve how we deal with software development processes. The application of artificial intelligence to model-driven engineering projects is becoming more and more popular; however, within the model repair field, the use of this technique remains mostly an open challenge. In this paper, we explore some existing approaches in the field of AI-powered model repair. From the existing approaches in this field, we identify a series of challenges which the community needs to overcome. In addition, we present a number of research opportunities by taking inspiration from other fields which have successfully used artificial intelligence, such as code repair. Moreover, we discuss the connection between the existing approaches and the opportunities with the identified challenges. Finally, we present the outcomes of our experience of applying artificial intelligence to model repair.",
        "keywords": [
            "Artificial intelligence",
            "Model repair",
            "Challenges",
            "Opportunities"
        ],
        "authors": [
            "Angela Barriga",
            "Adrian Rutle",
            "Rogardt Heldal"
        ],
        "file_path": "data/sosym-all/s10270-022-00983-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An actor-based framework for asynchronous event-based cyber-physical systems",
        "submission-date": "2020/02",
        "publication-date": "2021/04",
        "abstract": "In cyber-physical systems like automotive systems, there are components like sensors, actuators, and controllers that communicate asynchronously with each other. The computational model of actors supports modeling distributed asynchronously communicating systems. We propose the Hybrid Rebeca language to support the modeling of cyber-physical systems. Hybrid Rebeca is an extension of the actor-based language Rebeca. In this extension, physical actors are introduced as new computational entities to encapsulate physical behaviors. To support various means of communication among the entities, the network is explicitly modeled as a separate entity from actors. We develop a tool to derive hybrid automata as the basis for the analysis of Hybrid Rebeca models. We demonstrate the applicability of our approach through a case study in the domain of automotive systems. We use the SpaceEx framework for reachability analysis of the case study. Compared to hybrid automata, our results show that for event-based asynchronous models hybrid Rebeca improves analyzability by reducing the number of real variables, and increases modularity and hence, minimizes the number of changes caused by a modification in the model.",
        "keywords": [
            "Actor model",
            "Cyber-physical systems",
            "Hybrid automata"
        ],
        "authors": [
            "Iman Jahandideh",
            "Fatemeh Ghassemi",
            "Marjan Sirjani"
        ],
        "file_path": "data/sosym-all/s10270-021-00877-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Semi-automated metamodel/model co-evolution: a multi-level interactive approach",
        "submission-date": "2021/03",
        "publication-date": "2022/04",
        "abstract": "Metamodels evolve even more frequently than programming languages. This evolution process may result in a large number\nof instance models that are no longer conforming to the revised metamodel. On the one hand, the manual adaptation of\nmodels after the metamodels’ evolution can be tedious, error-prone, and time-consuming. On the other hand, the automated\nco-evolution of metamodels/models is challenging, especially when new semantics is introduced to the metamodels. While\nsome interactive techniques have been proposed, designers still need to explore a large number of possible revised models,\nwhich makes the interaction time-consuming. Existing interactive tools are limited to interactions with the designers to\nevaluate the impact of the co-evolved models on different objectives of the number of inconsistencies, number of changes\nand the deviation from the initial models. However, designers are also interested to check the impact of introduced changes\non the decision space which is composed by model elements. These interactions help designers to understand the differences\nof the co-evolved models solution that have similar objectives value to select the best one based on their preferences. In this\npaper, we propose an interactive approach that enables designers to select their preference simultaneously in the objective\nand decision spaces. Designers may be interested in looking at co-evolution operations that can improve a speciﬁc objective\nsuch as number of non-conformities with the revised metamodel (objective space), but such operations may be related to\ndifferent model locations (decision space). A set of co-evolution solutions is generated at ﬁrst using multi-objective search\nthat suggests edit operations to designers based on three objectives: minimizing the deviation with the initial model, the\nnumber of non-conformities with the revised metamodel and the number of changes. Then, the approach proposes to the\nuser few regions of interest by clustering the set of recommended co-evolution solutions of the multi-objective search. Also,\nanother clustering algorithm is applied within each cluster of the objective space to identify solutions related to different\nmodel element locations. The objective and decision spaces can now be explored more efﬁciently by the designers, who can\nquickly select their preferred cluster and give feedback on a smaller number of solutions by eliminating similar ones. This\nfeedback is then used to guide the search for the next iterations if the user is still not satisﬁed. We evaluated our approach on\na set of metamodel/model co-evolution case studies and compared it to existing fully automated and interactive co-evolution\ntechniques.",
        "keywords": [
            "Metamodel/model co-evolution",
            "Interactive multi-objective search",
            "Search-based software engineering"
        ],
        "authors": [
            "Wael Kessentini",
            "Vahid Alizadeh"
        ],
        "file_path": "data/sosym-all/s10270-022-00978-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistent change propagation within models",
        "submission-date": "2019/06",
        "publication-date": "2020/08",
        "abstract": "Developers change models with clear intentions—e.g., for refactoring, defects removal, or evolution. However, in doing so,\ndevelopers are often unaware of the consequences of their changes. Changes to one part of a model may affect other parts\nof the same model and/or even other models, possibly created and maintained by other developers. The consequences are\nincomplete changes and with it inconsistencies within or across models. Extensive works exist on detecting and repairing\ninconsistencies. However, the literature tends to focus on inconsistencies as errors in need of repairs rather than on incomplete\nchanges in need of further propagation. Many changes are non-trivial and require a series of coordinated model changes. As\ndevelopers start changing the model, intermittent inconsistencies arise with other parts of the model that developers have not\nyet changed. These inconsistencies are cues for incomplete change propagation. Resolving these inconsistencies should be\ndone in a manner that is consistent with the original changes. We speak of consistent change propagation. This paper leverages\nclassical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only\nsuggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair.\nIn doing so, our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier\nchanges. We argue that consistent change propagation is essential for effective model-driven engineering. Our approach and\nits tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its\nfeasibility and scalability. A comparison with two versioned models shows that our approach identiﬁes actual repair sequences\nthat developers had chosen. Furthermore, an experiment involving 22 participants shows that our change propagation approach\nmeets the workﬂow of how developers handle changes by always computing the sequence of repairs resulting from the change\npropagation.",
        "keywords": [
            "Model-driven engineering",
            "Inconsistency repair",
            "Change propagation",
            "Consistency detection"
        ],
        "authors": [
            "Roland Kretschmer",
            "Djamel Eddine Khelladi",
            "Roberto Erick Lopez-Herrejon",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-020-00823-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Research software engineering and the importance of scientific models",
        "submission-date": "2023/07",
        "publication-date": "2023/07",
        "abstract": "Research Software Engineering is the use of Software Engineering practices to support the software needs when conducting research. This is the definition of a new and emerging field of software engineering. Similar to the original term “software engineering,” the creation of the subfield “Research Software Engineering” (RSE) was necessary because there is an ongoing crisis in the development of software that supports research activities.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01119-z.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Resolving model inconsistencies using automated regression planning",
        "submission-date": "2012/10",
        "publication-date": "2013/02",
        "abstract": "One of the main challenges in model-driven software engineering is to automate the resolution of design model inconsistencies. We propose to use the artiﬁcial intelligence technique of automated planning for the purpose of resolving such inconsistencies through the generation of one or more resolution plans. We implemented Badger, a regression planner in Prolog that generates such plans. We assess its scalability on the resolution of different types of structural inconsistencies in UML models using both generated models and reverse-engineered models of varying sizes, the largest ones containing more than 10,000 model elements. We illustrate the metamodel-independence of our approach by applying it to the resolution of code smells in a Java program. We discuss how the user can adapt the order in which resolution plans are presented by modifying the cost function of the planner algorithm.",
        "keywords": [
            "Automated planning",
            "Inconsistency resolution",
            "Software modeling"
        ],
        "authors": [
            "Jorge Pinna Puissant",
            "Ragnhild Van Der Straeten",
            "Tom Mens"
        ],
        "file_path": "data/sosym-all/s10270-013-0317-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving repair of semantic ATL errors using a social diversity metric",
        "submission-date": "2023/07",
        "publication-date": "2024/04",
        "abstract": "Model transformations play an essential role in the model-driven engineering paradigm. However, writing a correct trans-\nformation requires the user to understand both what the transformation should do and how to enact that change in the\ntransformation. This easily leads to syntactic and semantic errors in transformations which are time-consuming to locate and\nfix. In this article, we extend our evolutionary algorithm (EA) approach to automatically repair transformations containing\nmultiple semantic errors. To prevent the fitness plateaus and the single fitness peak limitations from our previous work, we\ninclude the notion of social diversity as an objective for our EA to promote repair patches tackling errors that are less covered\nby the other patches of the population. We evaluate our approach on four ATL transformations, which have been mutated to\ncontain up to five semantic errors simultaneously. Our evaluation shows that integrating social diversity when searching for\nrepair patches improves the quality of those patches and speeds up the convergence even when up to five semantic errors are\ninvolved.",
        "keywords": [
            "Model-driven engineering",
            "Model transformations",
            "ATL",
            "Evolutionary algorithms",
            "Social diversity"
        ],
        "authors": [
            "Zahra VaraminyBahnemiry",
            "Jessie Galasso",
            "Bentley Oakes",
            "Houari Sahraoui"
        ],
        "file_path": "data/sosym-all/s10270-024-01170-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Automated, interactive, and traceable domain modelling empowered by artiﬁcial intelligence",
        "submission-date": "2021/04",
        "publication-date": "2022/01",
        "abstract": "Model-Based Software Engineering provides various modelling formalisms for capturing the structural, behavioral, conﬁg-uration, and intentional aspects of software systems. One of the most widely used kinds of models—domain models—are used during requirements analysis or the early stages of design to capture the domain concepts and relationships in the form of class diagrams. Modellers perform domain modelling to transform the problem descriptions that express informal requirements in natural language to domain models, which are more concise and analyzable. However, this manual practice of domain modelling is laborious and time-consuming. Existing approaches, which aim to assist modellers by automating or semi-automating the construction of domain models from problem descriptions, fail to address three non-trivial aspects of automated domain modelling. First, automatically extracted domain models from existing approaches are not accurate enough to be used directly or with minor modiﬁcations for software development or teaching purposes. Second, existing approaches do not support modeller-system interactions beyond providing recommendations. Finally, existing approaches do not facilitate the modellers to learn the rationale behind the modelling decisions taken by an extractor system. Therefore, in this paper, we extend our previous work to facilitate bot-modeller interactions. We propose an algorithm to discover alterna-tive conﬁgurations during bot-modeller interactions. Our bot uses this algorithm to ﬁnd alternative conﬁgurations and then present these conﬁgurations in the form of suggestions to modellers. Our bot then updates the domain model in response to the acceptance of these suggestions by a modeller. Furthermore, we evaluate the bot for its effectiveness and performance for the test problem descriptions. Our bot achieves median F1 scores of 86%, 91%, and 90% in the Found Conﬁgurations, Offered Suggestions, and Updated Domain Models categories, respectively. We also show that the median time taken by our bot to ﬁnd alternative conﬁgurations is 55.5ms for the problem descriptions which are similar to the test problem descriptions in terms of model size and complexity. Finally, we conduct a pilot user study to assess the beneﬁts and limitations of our bot and present the lessons learned from our study in preparation for a large-scale user study.",
        "keywords": [
            "Domain model",
            "Natural language (NL)",
            "Natural language processing (NLP)",
            "Machine learning (ML)",
            "Neural networks",
            "Descriptive model",
            "Predictive model",
            "Bot",
            "Modeller interactions",
            "Traceability information model",
            "Traceability knowledge graph"
        ],
        "authors": [
            "Rijul Saini",
            "Gunter Mussbacher",
            "Jin L. C. Guo",
            "Jörg Kienzle"
        ],
        "file_path": "data/sosym-all/s10270-021-00942-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Quick ﬁxing ATL transformations with speculative analysis",
        "submission-date": "2016/02",
        "publication-date": "2016/07",
        "abstract": "Model transformations are central components of most model-based software projects. While ensuring their correctness is vital to guarantee the quality of the solution, current transformation tools provide limited support to statically detect and ﬁx errors. In this way, the identiﬁcation of errors and their correction are nowadays mostly manual activities which incur in high costs. The aim of this work is to improve this situation. Recently, we developed a static analyser that combines program analysis and constraint solving to identify errors in ATL model transformations. In this paper, we present a novel method and system that uses our analyser to propose suitable quick ﬁxes for ATL transformation errors, notably some non-trivial, transformation-speciﬁc ones. Our approach supports speculative analysis to help developers select the most appropriate ﬁx by creating a dynamic ranking of ﬁxes, reporting on the consequences of applying a quick ﬁx, and providing a pre-visualization of each quick ﬁx application. The approach integrates seamlessly with the ATL editor. Moreover, we provide an evaluation based on existing faulty transformations built by a third party, and on automatically generated transformation mutants, which are then corrected with the quick ﬁxes of our catalogue.",
        "keywords": [
            "Model transformation",
            "ATL",
            "Transformation static analysis",
            "Quick ﬁxes",
            "Speculative analysis"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-016-0541-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ATL"
        }
    },
    {
        "title": "Bridging the model-to-code abstraction gap with fuzzy logic in model-based regression test selection",
        "submission-date": "2020/09",
        "publication-date": "2021/07",
        "abstract": "Regression test selection (RTS) approaches reduce the cost of regression testing of evolving software systems. Existing RTS approaches based on UML models use behavioral diagrams or a combination of structural and behavioral diagrams. However, in practice, behavioral diagrams are incomplete or not used. In previous work, we proposed a fuzzy logic based RTS approach called FLiRTS that uses UML sequence and activity diagrams. In this work, we introduce FLiRTS2, which drops the need for behavioral diagrams and relies on system models that only use UML class diagrams, which are the most widely used UML diagrams in practice. FLiRTS2 addresses the unavailability of behavioral diagrams by classifying test cases using fuzzy logic after analyzing the information commonly provided in class diagrams. We evaluated FLiRTS2 on UML class diagrams extracted from 3331 revisions of 13 open-source software systems, and compared the results with those of code-based dynamic (Ekstazi) and static (STARTS) RTS approaches. The average test suite reduction using FLiRTS2 was 82.06%. The average safety violations of FLiRTS2 with respect to Ekstazi and STARTS were 18.88% and 16.53%, respectively. FLiRTS2 selected on average about 82% of the test cases that were selected by Ekstazi and STARTS. The average precision violations of FLiRTS2 with respect to Ekstazi and STARTS were 13.27% and 9.01%, respectively. The average mutation score of the full test suites was 18.90%; the standard deviation of the reduced test suites from the average deviation of the mutation score for each subject was 1.78% for FLiRTS2, 1.11% for Ekstazi, and 1.43% for STARTS. Our experiment demonstrated that the performance of FLiRTS2 is close to the state-of-art tools for code-based RTS but requires less information and performs the selection in less time.",
        "keywords": [
            "Class diagram",
            "Fuzzy logic",
            "Regression test selection",
            "UML"
        ],
        "authors": [
            "Walter Cazzola",
            "Sudipto Ghosh",
            "Mohammed Al-Refai",
            "Gabriele Maurina"
        ],
        "file_path": "data/sosym-all/s10270-021-00899-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The journal on Software and Systems Modeling Matures",
        "submission-date": "2012/09",
        "publication-date": "2012/09",
        "abstract": "This editorial reflects on the ten-year history of the Journal on Software and Systems Modeling (SoSyM), highlighting its origins, evolution, and contribution to the field of software and system modeling.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-012-0287-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A technique for discovering BPMN collaboration diagrams",
        "submission-date": "2023/01",
        "publication-date": "2024/02",
        "abstract": "The process mining domain is actively supported by techniques and tools addressing the discovery of single-participant business processes. In contrast, approaches for discovering collaboration models out of distributed data stored by multiple interacting participants are lacking. In this context, we propose a novel technique for discovering collaboration models from sets of event logs that include data about participants’ interactions. The technique discovers each participant’s process through already available algorithms introduced by the process mining community. Then, it analyzes the logs to extract information on the exchange of messages to automatically combine the discovered processes into a collaboration model representing the distributed system’s behavior and providing analytics on the interactions. The technique has been implemented in a tool evaluated via several experiments on different application domains.",
        "keywords": [
            "BPMN collaborations",
            "Discovery",
            "Messages analysis"
        ],
        "authors": [
            "Flavio Corradini",
            "Sara Pettinari",
            "Barbara Re",
            "Lorenzo Rossi",
            "Francesco Tiezzi"
        ],
        "file_path": "data/sosym-all/s10270-024-01153-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework for families of domain-speciﬁc modelling languages",
        "submission-date": "2010/11",
        "publication-date": "2012/08",
        "abstract": "Domain-speciﬁc modelling langugages, which\nare tailored to the requirements of their users, can signif-icantly increase the acceptance of formal (or at least semi-formal) modelling in scenarios where informal diagrams and\nnatural language descriptions are predominant today. We\nshow in this article how the Resource Description Frame-\nwork (RDF), which is a standard for the fundamental data\nstructures of the Semantic Web, and algebraic graph trans-\nformations on these data structures can be used to realise\nand modify the abstract syntax of models in such domain-\nspeciﬁc languages. We examine a small domain-speciﬁc\nmodelling language for IT infrastructures—inspired by real-\nworld requirements from a banking environment—as an\napplication scenario. From this scenario, we derive four key\nrequirements for a domain-speciﬁc modelling framework:\n(1) distributed modelling, (2) evolution of language deﬁ-\nnitions, (3) migration of legacy models and (4) integration\nof modelling languages. RDF and transformation rules are\nthen used to provide a solution which meets these require-\nments simultaneously, where all kinds of modiﬁcations—\nfrom simple editing steps via model migration to language\nintegration—are realised in an integrated manner by the sin-\ngle, declarative formalism of algebraic graph transformation.",
        "keywords": [
            "Resource Description Framework",
            "Algebraic\ngraph transformation",
            "Domain-speciﬁc modelling language",
            "Language evolution",
            "Model migration",
            "Language integration",
            "Distributed modelling"
        ],
        "authors": [
            "Benjamin Braatz",
            "Christoph Brandt"
        ],
        "file_path": "data/sosym-all/s10270-012-0271-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Participatory modeling from a stakeholder perspective: On the inﬂuence of collaboration and revisions on psychological ownership and perceived model quality",
        "submission-date": "2021/10",
        "publication-date": "2022/08",
        "abstract": "Participatory enterprise modeling is about gathering domain experts and involving them directly in the creation of models, aided by modeling experts. It is meant to increase commitment to and quality of models. This paper presents an exploratory study focusing on the subjective view of the domain experts. We investigated the inﬂuence of direct collaboration versus individual modeling, and the inﬂuence of model revisions by modeling experts on psychological ownership and perceived model quality. We chose process modeling as a particular form of enterprise modeling. Our results give hint that domain experts working individually with a modeling expert perceive model quality as higher than those working collaboratively whereas psychological ownership did not show any difference. Revisions caused changes in the subjects’ assessments only of model quality. Moreover, we will present qualitative results from interviews we led with the participants. They reveal interesting insight on how outcome and perception of the procedure and the method in both settings can be positively inﬂuenced. The interviews also emphasize the special role of the method experts who are sometimes even considered as co-owners of the model.",
        "keywords": [
            "Participatory enterprise modeling",
            "Collaboration",
            "Psychological ownership",
            "Perceived control",
            "Perceived model quality"
        ],
        "authors": [
            "Anne Gutschmidt",
            "Birger Lantow",
            "Ben Hellmanzik",
            "Ben Ramforth",
            "Matteo Wiese",
            "Erko Martins"
        ],
        "file_path": "data/sosym-all/s10270-022-01036-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MIKADO: a smart city KPIs assessment modeling framework",
        "submission-date": "2020/10",
        "publication-date": "2021/08",
        "abstract": "Smart decision making plays a central role for smart city governance. It exploits data analytics approaches applied to collected data, for supporting smart cities stakeholders in understanding and effectively managing a smart city. Smart governance is performed through the management of key performance indicators (KPIs), reﬂecting the degree of smartness and sustainability of smart cities. Even though KPIs are gaining relevance, e.g., at European level, the existing tools for their calculation are still limited. They mainly consist in dashboards and online spreadsheets that are rigid, thus making the KPIs evolution and customizationatediousanderror-proneprocess.Inthispaper,weexploitmodel-drivenengineering(MDE)techniques,through metamodel-based domain-speciﬁc languages (DSLs), to build a framework called MIKADO for the automatic assessment of KPIs over smart cities. In particular, the approach provides support for both: (i) domain experts, by the deﬁnition of a textual DSL for an intuitive KPIs modeling process and (ii) smart cities stakeholders, by the deﬁnition of graphical editors for smart cities modeling. Moreover, dynamic dashboards are generated to support an intuitive visualization and interpretation of the KPIs assessed by our KPIs evaluation engine. We provide evaluation results by showing a demonstration case as well as studying the scalability of the KPIs evaluation engine and the general usability of the approach with encouraging results. Moreover, the approach is open and extensible to further manage comparison among smart cities, simulations, and KPIs interrelations.",
        "keywords": [
            "Smart Cities",
            "MDE",
            "KPI",
            "DSL",
            "Smart Governance"
        ],
        "authors": [
            "Martina De Sanctis",
            "Ludovico Iovino",
            "Maria Teresa Rossi",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-021-00907-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reuse in model-to-model transformation languages: are we there yet?",
        "submission-date": "2012/10",
        "publication-date": "2013/05",
        "abstract": "In the area of model-driven engineering, model transformations are proposed as the technique to systematically manipulate models. For increasing development productivity as well as quality of model transformations, reuse mechanisms are indispensable. Although numerous mechanisms have been proposed, no systematic comparison exists, making it unclear, which reuse mechanisms may be best employed in a certain situation. Thus, this paper provides an in-depth comparison of reuse mechanisms in model-to-model transformation languages and categorizes them along their intended scope of application. Finally, current barriers and facilitators to model transformation reuse are discussed.",
        "keywords": [
            "Reuse mechanisms",
            "Model transformations",
            "Survey",
            "Model-driven engineering"
        ],
        "authors": [
            "A. Kusel",
            "J. Schönböck",
            "M. Wimmer",
            "G. Kappel",
            "W. Retschitzegger",
            "W. Schwinger"
        ],
        "file_path": "data/sosym-all/s10270-013-0343-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "Automatic derivation of BPEL4WS from IDEF0 process models",
        "submission-date": "2005/05",
        "publication-date": "2006/02",
        "abstract": "Integration deﬁnition for function modelling (IDEF0) is one of the most popular notations for modelling business processes. It employs a rather simple and intuitive modelling construct, consisting of boxes representing functions and arrows connecting them signifying ﬂow of information and materials. Web services on the other hand are an emerging technology for implementing distributed systems. Web service orchestration languages, such as Business Process Execution Language for Web Services (BPEL4WS), are the emerging approach for describing processes as networks of coordinated web services. Business processes as captured in IDEF0 models, however, may contain both web services as well as other types of activities which need to be coordinated. By automatically analysing the Extensible Markup Language (XML) deﬁnition of an IDEF0 model, we can identify how web services interact with other activities and at runtime generate code to support the orchestration of web services with the overall business process. The approach proposed is independent of the orchestration language and ensures an implementation independent model for specifying web service orchestrations. This approach also enables the top-down analysis of a business process to its constituent web services and avoids any misalignment problems during design time between the two.",
        "keywords": [
            "IDEF0",
            "UML",
            "Web services",
            "Orchestration",
            "WSDL",
            "BPEL4WS"
        ],
        "authors": [
            "Bill Karakostas",
            "Yannis Zorgios",
            "Charalampos C. Alevizos"
        ],
        "file_path": "data/sosym-all/s10270-006-0003-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling difficulties in creating conceptual data models\nMultimodal studies on individual modeling processes",
        "submission-date": "2022/01",
        "publication-date": "2022/10",
        "abstract": "Conceptual modeling is a learning task essential to students of computer science, software engineering and related programs. Construed as a complex task, surprisingly little is known about the actual act of conceptual modeling, and about modeling difficulties learners experience. Combining complementary modes of observation of learners’ modeling processes, we study modeling difficulties encountered while performing a data modeling task. Using the concept of cognitive breakdown, we analyze audiovisual protocols of the individual modelers’ modeling processes, recordings of their interactions with the employed modeling software tool and survey data of modelers about their perception of encountered modeling difficulties. In an exploratory study and a follow-up study, we identify eight types of modeling difficulties related to modeling entity types, generalization hierarchies, relationship types, attributes and cardinalities. The identified types of modeling difficulties contribute to a better and more complete understanding of data modeling processes intended to inform design science research on modeling assistance for data modelers at different stages of their learning and mastering of conceptual data modeling.",
        "keywords": [
            "Conceptual data modeling",
            "Modeling difficulty",
            "Problem solving",
            "Cognitive breakdown",
            "Mixed methods research"
        ],
        "authors": [
            "Kristina Rosenthal\nStefan Strecker\nMonique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-022-01051-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Comprehensive analysis of FBD test coverage criteria using mutants",
        "submission-date": "2013/06",
        "publication-date": "2014/07",
        "abstract": "Function block diagram (FBD), a graphical mod-eling language for programmable logic controllers, has been widely used to implement safety critical system software such as nuclear reactor protection systems. With the growing importance of structural testing for FBD models, structural test coverage criteria for FBD models have been proposed and evaluated using mutation analysis in our previous work. We extend the previous work by comprehensively analyz-ing the relationships among fault detection effectiveness, test suite size, and coverage level through several research ques-tions. We generate a large number of test suites achieving an FBD test coverage ranging from 0 to 100%, and we also gen-erate many artiﬁcial faults (i.e. mutants) for the FBD models. Our analysis results show that the fault detection effective-ness of the FBD coverage criteria increases with increasing coverage levels, and the coverage criteria are highly effec-tive at detecting faults in all subject models. Furthermore, the test suites generated with the FBD coverage criteria are more effectiveandefﬁcientthantherandomlygeneratedtestsuites. The FBD coverage criteria are strong at detecting faults in Boolean edges, while relatively weak at detecting wrong con-stants in FBD models. Empirical knowledge regarding our experiments provide the validity of using the FBD coverage criteria, and therefore, of FBD model-based testing.",
        "keywords": [
            "FBD model-based testing",
            "Test coverage criteria",
            "Mutation analysis"
        ],
        "authors": [
            "Donghwan Shin",
            "Eunkyoung Jee",
            "Doo-Hwan Bae"
        ],
        "file_path": "data/sosym-all/s10270-014-0428-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling agent organizations using roles",
        "submission-date": "2003/04",
        "publication-date": "2003/06",
        "abstract": "Agent-based and active object systems are no longer contained within the boundaries of a single, small organization. To meet the demands of large-scale software and systems modeling, we need useful analogies for modeling and constructing large-scale systems of autonomous, interactive software entities. In this paper, we employ social and organizational systems theory as a way to guide our understanding of the notion of role and its implications on how agents (and active objects) might behave in group settings.",
        "keywords": [
            "Agent",
            "Role",
            "Active object"
        ],
        "authors": [
            "James J. Odell",
            "H. Van Dyke Parunak",
            "Mitchell Fleischer"
        ],
        "file_path": "data/sosym-all/s10270-003-0017-y.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "On model typing",
        "submission-date": "2006/01",
        "publication-date": "2007/01",
        "abstract": "Where object-oriented languages deal with objects as described by classes, model-driven development uses models, as graphs of interconnected objects, described by metamodels. A number of new languages have been and continue to be developed for this model-based paradigm, both for model transformation and for general programming using models. Many of these use single-object approaches to typing, derived from solutions found in object-oriented systems, while others use metamodels as model types, but without a clear notion of polymorphism. Both of these approaches lead to brittle and overly restrictive reuse characteristics. In this paper we propose a simple extension to object-oriented typing to better cater for a model-oriented context, including a simple strategy for typing models as a collection of interconnected objects. We suggest extensions to existing type system formalisms to support these concepts andtheir manipulation. Usingasimpleexampleweshow how this extended approach permits more ﬂexible reuse, while preserving type safety.",
        "keywords": [
            "MDA",
            "MOF",
            "Metamodelling",
            "Type systems",
            "Typing",
            "Model transformation"
        ],
        "authors": [
            "Jim Steel",
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-006-0036-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Veriﬁcation, validation, and evaluation of modeling methods: experiences and recommendations",
        "submission-date": "2025/02",
        "publication-date": "Not found",
        "abstract": "Modern modeling methods are developed in various ways, some following method engineering approaches and principles, others in more ad-hoc ways based on experience, observation, or by exploration of innovative ideas. In all cases, the assessment of the method quality and ﬁtness to the different situations in which it has to serve is a challenge that the method engineers have to face. In this paper, we aim to share knowledge concerning the process of development of modeling methods with an emphasis on systematic approach to veriﬁcation, validation, and evaluation (VVE). The goal of the paper is to: (1) clarify the differences and key activities and practices of method VVE, (2) to analyze how these have been applied to several modeling methods, and (3) to capture and share key best practices in this process.",
        "keywords": [
            "Modeling method",
            "Veriﬁcation",
            "Validation",
            "Evaluation",
            "Method engineering",
            "Design science research"
        ],
        "authors": [
            "Jolita Ralyté",
            "Georgios Koutsopoulos",
            "Janis Stirna"
        ],
        "file_path": "data/sosym-all/s10270-025-01304-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Improving engineering efﬁciency with PLM/ALM",
        "submission-date": "2012/05",
        "publication-date": "2013/05",
        "abstract": "Rising cost pressure is forcing manufacturers and their suppliers to jointly and consistently master prod-uct development. Our industry case study shows how a leading automotive OEM over time has achieved effective interaction of engineering processes, tools, and people on the basis of product and application life-cycle management (PLM/ALM). Its scope is ﬁrst an introduction to PLM/ALM on the basis of a model-driven engineering (MDE) for one or several products or product families. Second, PLM and ALM need tool support to the degree necessary to ease handling and drive reuse and consistency. Third, introducing MDE needs profound change management. Starting from estab-lishing the relevant engineering processes, we show how they can be effectively automated for best possible usage across the enterprise and even for suppliers. We practically describe how such a profound change process is successfully managed together with impacted engineers and how the concepts can be transferred to other companies. Concrete results for efﬁ-ciency improvement, shorter lead time, and better quality in product development combined with better global engineer-ing underline the business value.",
        "keywords": [
            "Application lifecycle management (ALM)",
            "Product lifecycle management (PLM)",
            "Efﬁciency",
            "Industry\nvoice"
        ],
        "authors": [
            "Christof Ebert"
        ],
        "file_path": "data/sosym-all/s10270-013-0347-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based code generation works: But how far does it go?—on the role of the generator",
        "submission-date": "2024/04",
        "publication-date": "2024/04",
        "abstract": "There are many published examples of successful industrial projects that used code generation from abstract models. However, it seems that the actual use of code generation from explicitly defined models written in modeling languages (e.g., UML, SysML, or a domain-specific language) has not been as successful and widespread as possible. Why is that so? Unfortunately, there are too few in-depth examinations of the problems and challenges that actually prohibit the widespread adoption of model-based code generation. It would be very instructive for the SoSyM community to have deeper insights into these challenges so that we could improve the current state-of-the-art.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01172-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A transformation-based approach to context-aware modelling",
        "submission-date": "2011/04",
        "publication-date": "2012/03",
        "abstract": "Context-aware computing is a paradigm for governing the numerous mobile devices surrounding us. In this computing paradigm, software applications continuously and dynamically adapt to different “contexts” implying different software configurations of such devices. Unfortunately, modelling a context-aware application (CAA) for all possible contexts is only feasible in the simplest of cases. Hence, tool support verifying certain properties is required. In this article, we introduce the CAA model, in which context adaptations are specified explicitly as model transformations. By mapping this model to graphs and graph transformations, we can exploit graph transformation techniques such as critical pair analysis to find contexts for which the resulting application model is ambiguous. We validate our approach by means of an example of a mobile city guide, demonstrating that we can identify subtle context interactions that might go unnoticed otherwise.",
        "keywords": [
            "Context-aware model",
            "Model transformation",
            "Critical pair analysis",
            "Context adaptation",
            "Context coverage"
        ],
        "authors": [
            "Sylvain Degrandsart",
            "Serge Demeyer",
            "Jan Van den Bergh",
            "Tom Mens"
        ],
        "file_path": "data/sosym-all/s10270-012-0239-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated anonymity veriﬁcation of the ThreeBallot and VAV voting systems",
        "submission-date": "2013/11",
        "publication-date": "2015/01",
        "abstract": "In recent years, a large number of secure voting protocols have been proposed in the literature. Often these protocols contain ﬂaws, but because they are complex protocols, rigorous formal analysis has proven hard to come by. Rivest’s ThreeBallot and Vote/Anti-Vote/Vote (VAV) voting systems are important because they aim to provide security (voter anonymity and voter veriﬁability) without requiring cryptography. In this paper, we construct CSP models of ThreeBallot and VAV, and use them to produce the ﬁrst automated formal analysis of their anonymity properties. Along the way, we discover that one of the crucial assumptions under which ThreeBallot and VAV (and many other votingsystems) operate—theshort ballot assumption—is highly ambiguous in the literature. We give various plausible precise interpretations and discover that in each case, the interpretation either is unrealistically strong or else fails to ensure anonymity. We give a revised version of the short ballot assumption for ThreeBallot and VAV that is realistic but still provides a guarantee of anonymity.",
        "keywords": [
            "Formal methods",
            "Voting systems",
            "Anonymity",
            "Automatic veriﬁcation",
            "ThreeBallot",
            "VAV"
        ],
        "authors": [
            "Murat Moran",
            "James Heather",
            "Steve Schneider"
        ],
        "file_path": "data/sosym-all/s10270-014-0445-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From single-objective to multi-objective reinforcement learning-based model transformation",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Model-driven optimization allows to directly apply domain-speciﬁc modeling languages to deﬁne models which are subsequently optimized by applying a predeﬁned set of model transformation rules. Objectives guide the optimization processes which can range from one single objective formulation resulting in one single solution to a set of objectives that necessitates the identiﬁcation of a Pareto-optimal set of solutions. In recent years, a multitude of reinforcement learning approaches has been proposed that support both optimization cases and competitive results for various problem instances have been reported. However, their application to the ﬁeld of model-driven optimization has not gained much attention yet, especially when compared to the extensive application of meta-heuristic search approaches such as genetic algorithms. Thus, there is a lack of knowledge about the applicability and performance of reinforcement learning for model-driven optimization. We therefore present in this paper a general framework for applying reinforcement learning to model-driven optimization problems. In particular, we show how a catalog of different reinforcement learning algorithms can be integrated with existing model-driven optimization approaches that use a transformation rule application encoding. We exemplify this integration by presenting a dedicated reinforcement learning extension for MOMoT. We build on this tool support and investigate several case studies for validating the applicability of reinforcement learning for model-driven optimization and compare the performance against a genetic algorithm. The results show clear advantages of using RL for single-objective problems, especially for cases where the transformation steps are highly dependent on each other. For multi-objective problems, the results are more diverse and case-speciﬁc, which further motivates the usage of model-driven optimization to utilize different approaches to ﬁnd the best solutions.",
        "keywords": [
            "Model-driven optimization",
            "Model transformation",
            "Reinforcement learning",
            "MOMoT"
        ],
        "authors": [
            "Martin Eisenberg",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-024-01233-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A case study on consistency management of business and IT process models in banking",
        "submission-date": "2011/11",
        "publication-date": "2013/03",
        "abstract": "Organizations that adopt process modeling often maintain several co-existing models of the same business process. These models target different abstraction levels and stakeholder perspectives. Maintaining consistency among these models has become a major challenge for such organizations. Although several academic works have discussed this challenge, little empirical investigation exists on how people perform process model consistency management in practice. This paper aims to address this lack by presenting an in-depth empirical study of a business-driven engineering process deployed at a large company in the banking sector. We analyzed more than 70 business process models developed by the company, including their change history, with over 1,000 change requests. We also interviewed 9 business and IT practitioners and surveyed 23 such practitioners to understand concrete difﬁculties in consistency management, the rationales for the speciﬁcation-to-implementation reﬁnements found in the models, strategies that the practitioners use to detect and ﬁx inconsistencies, and how tools could help with these tasks. Our contribution is a set of eight empirical ﬁndings, some of which conﬁrm or contradict previous works on process model consistency management found in the literature. The ﬁndings provide empirical evidence of (1) how business process models are created and maintained, including a set of recurrent patterns used to reﬁne business-level process speciﬁcations into IT-level models; (2) what types of inconsistencies occur; how they are introduced; and what problems they cause; and (3) what stakeholders expect from tools to support consistency management.",
        "keywords": [
            "Business processes",
            "Consistency management",
            "Process reﬁnement patterns",
            "Empirical study"
        ],
        "authors": [
            "Moisés Castelo Branco",
            "Yingfei Xiong",
            "Krzysztof Czarnecki",
            "Jochen Küster",
            "Hagen Völzer"
        ],
        "file_path": "data/sosym-all/s10270-013-0318-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven method for describing and predicting the reliability of composite services",
        "submission-date": "2009/05",
        "publication-date": "2010/02",
        "abstract": "Service-oriented computing is the prominent paradigm for viewing business processes as composed of functions provided by modular and standardized services. Web services are the building blocks for the application of service-oriented computing on the Web and provide the necessary support for the consolidation of multiple services into a single composite service corresponding to the overall process. In such a context, service providers are strategically interested in both describing the quality of service (QoS) characteristics of offered services, to better qualify their offer and gain a significant advantage in the global marketplace, and predicting the level of QoS that can be offered to service consumers when building composite web services that make use of services managed by various service providers. This paper illustrates a model-driven method to automatically describe and predict the QoS of composite web services specified by use of business process execution language (BPEL). The paper specifically addresses the reliability characteristic of the QoS. The proposed method is founded on Q-WSDL, a lightweight WSDL extension for the description of the QoS characteristics of a web service, and exploits Q-WSDL to annotate reliability data onto a BPEL-based UML model of the composite service. The UML model is then used to predict and describe the reliability of the composite web service. The proposed method is illustrated by use of an example application that deals with a composite web service for the migration of PSTN telephone numbers.",
        "keywords": [
            "QoS",
            "Service oriented architecture",
            "WSDL",
            "BPEL",
            "UML",
            "Model-driven prediction"
        ],
        "authors": [
            "Paolo Bocciarelli",
            "Andrea D’Ambrogio"
        ],
        "file_path": "data/sosym-all/s10270-010-0150-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Design patterns for open tool integration",
        "submission-date": "2004/11",
        "publication-date": "2004/11",
        "abstract": "Design tool integration is a highly relevant\narea of software engineering that can greatly improve the\neﬃciency of development processes. Design patterns have\nbeen widely recognized as important contributors to the\nsuccess of software systems. This paper describes and\ncompares two large-grain, architectural design patterns\nthat solve speciﬁc design tool integration problems. Both\npatterns have been implemented and used in real-life en-\ngineering processes.",
        "keywords": [
            "Design patterns",
            "Software architecture",
            "Tool integration framework",
            "Metamodels",
            "Generative programming"
        ],
        "authors": [
            "Gabor Karsai",
            "Andras Lang",
            "Sandeep Neema"
        ],
        "file_path": "data/sosym-all/s10270-004-0073-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven engineering for mobile robotic systems: a systematic mapping study",
        "submission-date": "2020/06",
        "publication-date": "2021/08",
        "abstract": "Mobile robots operate in various environments (e.g. aquatic, aerial, or terrestrial), they come in many diverse shapes and they are increasingly becoming parts of our lives. The successful engineering of mobile robotics systems demands the interdisciplinary collaboration of experts from different domains, such as mechanical and electrical engineering, artiﬁcial intelligence, and systems engineering. Research and industry have tried to tackle this heterogeneity by proposing a multitude of model-driven solutions to engineer the software of mobile robotics systems. However, there is no systematic study of the state of the art in model-driven engineering (MDE) for mobile robotics systems that could guide research or practitioners in ﬁnding model-driven solutions and tools to efﬁciently engineer mobile robotics systems. The paper is contributing to this direction by providing a map of software engineering research in MDE that investigates (1) which types of robots are supported by existing MDE approaches, (2) the types and characteristics of MRSs that are engineered using MDE approaches, (3) a description of how MDE approaches support the engineering of MRSs, (4) how existing MDE approaches are validated, and (5) how tools support existing MDE approaches. We also provide a replication package to assess, extend, and/or replicate the study. The results of this work and the highlighted challenges can guide researchers and practitioners from robotics and software engineering through the research landscape.",
        "keywords": [
            "Model-driven engineering",
            "Mobile robot systems",
            "Systematic mapping study"
        ],
        "authors": [
            "Giuseppina Lucia Casalaro",
            "Giulio Cattivera",
            "Federico Ciccozzi",
            "Ivano Malavolta",
            "Andreas Wortmann",
            "Patrizio Pelliccione"
        ],
        "file_path": "data/sosym-all/s10270-021-00908-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Predictions-on-chip: model-based training and automated deployment of machine learning models at runtime",
        "submission-date": "2020/02",
        "publication-date": "2021/03",
        "abstract": "The design of gas turbines is a challenging area of cyber-physical systems where complex model-based simulations across multiple disciplines (e.g., performance, aerothermal) drive the design process. As a result, a continuously increasing amount of data is derived during system design. Finding new insights in such data by exploiting various machine learning (ML) techniques is a promising industrial trend since better predictions based on real data result in substantial product quality improvements and cost reduction. This paper presents a method that generates data from multi-paradigm simulation tools, develops and trains ML models for prediction, and deploys such prediction models into an active control system operating at runtime with limited computational power. We explore the replacement of existing traditional prediction modules with ML counterparts with different architectures. We validate the effectiveness of various ML models in the context of three (real) gas turbine bearings using over 150,000 data points for training, validation, and testing. We introduce code generation techniques for automated deployment of neural network models to industrial off-the-shelf programmable logic controllers.",
        "keywords": [
            "Prediction-at-runtime",
            "Machine learning",
            "Neural networks",
            "Automated deployment",
            "Code generation",
            "Gas turbine engines"
        ],
        "authors": [
            "Sebastian Pilarski",
            "Martin Staniszewski",
            "Matthew Bryan",
            "Frederic Villeneuve",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-020-00856-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deﬁning and generating multi-level and uncertainty-wise test oracles for cyber-physical systems",
        "submission-date": "2024/03",
        "publication-date": "2025/02",
        "abstract": "Cyber-physical systems (CPSs) blend digital and physical processes. CPS software is the key to realizing their functionalities. This software needs to evolve to deal with different aspects, such as the implementation of new functionalities or bug ﬁxes. Because of this, design–operation methods, colloquially known as “DevOps,” are paramount to be adopted within these systems. During DevOps phases, automating test execution at design time is a key enabler of streamlined software development and software quality improvement. Likewise, monitoring whether a CPS is behaving as expected at operation is similarly important. In DevOps, test oracles play an important role in enabling automated testing, ensuring the reliability of software deployments, providing feedback to developers, etc. However, deﬁning and generating test oracles in the context of DevOps practices in CPSs need to accommodate aspects speciﬁc to CPSs, such as their time-continuous behavior and inherent uncertainties. To this end, in this paper, we propose a domain-speciﬁc language (DSL) to ease the deﬁnition of test oracles and an automated solution for generating a microservice encapsulating the deﬁned test oracles, which is compatible with a DevOps ecosystem for CPSs. We evaluated our DSL with two industrial case study systems and 9 open-source CPSs. Our evaluation results suggest that our DSL can model around 98% of the requirements of these systems through test oracles. Furthermore, it is possible to generate a microservice to be applicable at different test levels within less than 20min, being fast enough to be adopted in practice.",
        "keywords": [
            "Domain-speciﬁc language",
            "Test oracle generation",
            "Cyber-physical systems",
            "Software testing"
        ],
        "authors": [
            "Pablo Valle",
            "Aitor Arrieta",
            "Liping Han",
            "Shaukat Ali",
            "Tao Yue"
        ],
        "file_path": "data/sosym-all/s10270-025-01271-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Veriﬁcation of B+ trees by integration of shape analysis and interactive theorem proving",
        "submission-date": "2012/03",
        "publication-date": "2013/03",
        "abstract": "Interactive proofs of correctness of pointer-manipulating programs tend to be difﬁcult. We propose an approach that integrates shape analysis and interactive theorem proving, namely three-valued logic analyzer (TVLA) and KIV. The approach uses shape analysis to automatically discharge proof obligations for various data structure properties, such as “acyclicity”. To this purpose, we deﬁne a mapping between typed algebraic heaps and TVLA. We verify the main operations of B+ trees by decomposing the problem into three layers: The top-level is an interactive proof of the main recursive procedures. The actual modiﬁcations of the data structure are veriﬁed with shape analysis. TVLA itself relies on problem-speciﬁc constraints and lemmas, that were proven in KIV as a foundation for an overall correct analysis.",
        "keywords": [
            "Theorem proving",
            "Shape analysis",
            "B+ trees",
            "Pointer structures"
        ],
        "authors": [
            "Gidon Ernst",
            "Gerhard Schellhorn",
            "Wolfgang Reif"
        ],
        "file_path": "data/sosym-all/s10270-013-0320-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Translating between Alloy speciﬁcations and UML class diagrams annotated with OCL",
        "submission-date": "2012/03",
        "publication-date": "2013/06",
        "abstract": "Model-driven engineering (MDE) is a software engineering approach based on model transformations at different abstraction levels. It prescribes the development of software by successively transforming the models from abstract (speciﬁcations) to more concrete ones (code). Alloy is an increasingly popular lightweight formal speciﬁcation language that supports automatic veriﬁcation. Unfortunately, its widespread industrial adoption is hampered by the lack of an ecosystem of MDE tools, namely code generators. This paper presents a model transformation from Alloy to UML class diagrams annotated with OCL (UML+OCL) and shows how an existing transformation from UML+OCL to Alloy can be improved to handle dynamic issues. The proposed bidirectional transformation enables a smooth integration of Alloy in the current MDE contexts, by allowing UML+OCL speciﬁcations to be transformed to Alloy for validation and veriﬁcation, to correct and possibly reﬁne them inside Alloy, and to translate them back to UML+OCL for sharing with stakeholders or to reuse current model-driven architecture tools to reﬁne them toward code.",
        "keywords": [
            "MDE",
            "Alloy",
            "UML",
            "OCL"
        ],
        "authors": [
            "Alcino Cunha",
            "Ana Garis",
            "Daniel Riesco"
        ],
        "file_path": "data/sosym-all/s10270-013-0353-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incorporating message weights in UML-based analysis of behavioral dependencies in distributed systems",
        "submission-date": "2008/04",
        "publication-date": "2008/12",
        "abstract": "Behavioral dependency analysis (BDA) and the visualization of dependency information have been identified as a high priority in industrial software systems (in specific, distributed systems). BDA determines the extent to which the functionality of one system entity (e.g., an object or a node) depends on other entities. Among many uses, a BDA is used to perform risk analysis and assessment, load planning, fault tolerance and redundancy provisions in distributed systems. Traditionally, most BDA techniques are based on source code or execution traces of a system. However, as model-driven development is gaining more popularity, there is a need for model-based BDA techniques. To address this need, we proposed in a previous work a metric, referred to as dependency index (DI), for the BDA of distributed objects and nodes based on UML behavioral models (sequence diagrams). However, in our previous BDA work, for simplicity, it was assumed that all messages are equivalent in terms of the dependencies they entail. However, to perform a more realistic BDA on real-world systems, messages must be weighted, e.g., certain messages may be more critical (or important) than others, and thus entail more intensive dependency. To address the above need, we define in this article a family of new BDA metrics, as extensions to our basic DI metric, based on different weighting mechanisms. Through an example application of the proposed metrics, we show that they can be used to predict more realistic dependency information. Communicated by Prof. Roel J. Wieringa.",
        "keywords": [
            "Behavioral dependency analysis",
            "Message weights",
            "Distributed systems",
            "Metrics",
            "UML",
            "Model-driven development",
            "Dependability"
        ],
        "authors": [
            "Vahid Garousi"
        ],
        "file_path": "data/sosym-all/s10270-008-0111-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Human factors in model-driven engineering: future research goals and initiatives for MDE",
        "submission-date": "2024/03",
        "publication-date": "2024/06",
        "abstract": "Software modelling and model-driven engineering (MDE) is traditionally studied from a technical perspective. However, one of the core motivations behind the use of software models is inherently human-centred. Models aim to enable practitioners to communicate about software designs, make software understandable, or make software easier to write through domain-speciﬁc modelling languages. Several recent studies challenge the idea that these aims can always be reached and indicate that human factors play a role in the success of MDE. However, there is an under-representation of research focusing on human factors in modelling. During a GI-Dagstuhl seminar, topics related to human factors in modelling were discussed by 26 expert participants from research and industry. In breakout groups, ﬁve topics were covered in depth, namely modelling human aspects, factors of modeller experience, diversity and inclusion in MDE, collaboration and MDE, and teaching human-aware MDE. We summarise our insights gained during the discussions on the ﬁve topics. We formulate research goals, questions, and propositions that support directing future initiatives towards an MDE community that is aware of and supportive of human factors and values.",
        "keywords": [
            "MDE",
            "Modelling",
            "Human factors",
            "Workshop"
        ],
        "authors": [
            "Grischa Liebel",
            "Jil Klünder",
            "Regina Hebig",
            "Christopher Lazik",
            "Inês Nunes",
            "Isabella Graßl",
            "Jan-Philipp Steghöfer",
            "Joeri Exelmans",
            "Julian Oertel",
            "Kai Marquardt",
            "Katharina Juhnke",
            "Kurt Schneider",
            "Lucas Gren",
            "Lucia Happe",
            "Marc Herrmann",
            "Marvin Wyrich",
            "Matthias Tichy",
            "Miguel Goulão",
            "Rebekka Wohlrab",
            "Reyhaneh Kalantari",
            "Robert Heinrich",
            "Sandra Greiner",
            "Satrio Adi Rukmono",
            "Shalini Chakraborty",
            "Silvia Abrahão",
            "Vasco Amaral"
        ],
        "file_path": "data/sosym-all/s10270-024-01188-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Exploring modeling methods for information systems analysis and design: a data-driven retrospective",
        "submission-date": "2025/02",
        "publication-date": "Not found",
        "abstract": "Modeling for information systems (IS) analysis and design offers broad insights into the advances and challenges of enterprise, business process, software, and conceptual modeling. In celebration of its 30th edition, this paper presents a data-driven retrospective analysis of studies published at the Exploring Modeling Methods for Systems Analysis and Development (EMM-SAD) working conference from 2005 to 2024. EMMSAD has long been a key venue for research on Information Systems (IS) Modeling, covering areas such as conceptual modeling, enterprise modeling, and model-driven engineering, as well as the evaluation of modeling techniques and tools. Using machine learning, specifically Dynamic Topic Modeling (DTM) with BERTopic, this study identifies recurring topics, emerging trends, and shifts in research focus within the IS modeling community. The findings highlight key areas of alignment between IS modeling and the broader modeling landscape, providing insights into the field’s evolution and future research opportunities.",
        "keywords": [
            "IS analysis and design",
            "Dynamic Topic Modeling",
            "BERTopic",
            "Data-driven approach",
            "EMMSAD"
        ],
        "authors": [
            "Iris Reinhartz-Berger",
            "Adir Solomon",
            "Jelena Zdravkovic",
            "John Krogstie",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-025-01302-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extracting LPL privacy policy purposes from annotated web service source code",
        "submission-date": "2020/12",
        "publication-date": "2022/04",
        "abstract": "Privacy policies are a mechanism used to inform users of the World Wide Web about the processing of their personal data. Such processing has special requirements, since personal data are regulated by data protection legislation. For example, a consent or another legal basis is typically needed. Privacy policies are documents used, among other things, to inform the data subject about processing of their personal data. These are formally represented by privacy languages. In this paper, we present a technique for constructing Layered Privacy Language policy data from web service code bases. Theoretically, we model the purposes of processing within web services by extending the privacy language with composition. We also present a formal analysis method for generating privacy policy purposes from the source code of web services. Furthermore, as a practical contribution, we present a static analysis tool that implements the theoretical solution. Finally, we report a brief case study for validating the tool",
        "keywords": [
            "Data protection",
            "privacy engineering",
            "privacy language",
            "static analysis",
            "semantic web",
            "GDPR"
        ],
        "authors": [
            "Kalle Hjerppe",
            "Jukka Ruohonen",
            "Ville Leppänen"
        ],
        "file_path": "data/sosym-all/s10270-022-00998-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Procedure-modular speciﬁcation and veriﬁcation of temporal safety properties",
        "submission-date": "2012/03",
        "publication-date": "2013/03",
        "abstract": "This paper describes ProMoVer, a tool for fully automated procedure-modular veriﬁcation of Java programs equipped with method-local and global assertions that specify safety properties of sequences of method invocations. Modularity at the procedure-level is a natural instantiation of the modular veriﬁcation paradigm, where correctness of global properties is relativized on the local properties of the methods rather than on their implementations. Here, it is based on the construction of maximal models for a program model that abstracts away from program data. This approach allows global properties to be veriﬁed in the presence of code evolution, multiple method implementations (as arising from software product lines), or even unknown method implementations (as in mobile code for open platforms). ProMoVer automates a typical veriﬁcation scenario for a previously developed tool set for compositional veriﬁcation of control ﬂow safety properties, and provides appropriate pre- and post-processing. Both linear-time temporal logic and ﬁnite automata are supported as formalisms for expressing local and global safety properties, allowing the user to choose a suitable format for the property at hand. Modularity is exploited by a mechanism for proof reuse that detects and minimizes the veriﬁcation tasks resulting from changes in the code and the speciﬁcations. The veriﬁcation task is relatively light-weight due to support for abstraction from private methods and automatic extraction of candidate speciﬁcations from method implementations. We evaluate the tool on a number of applications from the domains of Java Card and web-based application.",
        "keywords": [
            "Temporal logic",
            "Model checking",
            "Maximal models"
        ],
        "authors": [
            "Siavash Soleimanifard",
            "Dilian Gurov",
            "Marieke Huisman"
        ],
        "file_path": "data/sosym-all/s10270-013-0321-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Rule-based update transformations and their application to model refactorings",
        "submission-date": "2004/05",
        "publication-date": "2005/09",
        "abstract": "A rule-based update transformation is a model transformation where a single model is transformed in place. A model refactoring is a model transformation that improves the design described in the model. A refactoring should only affect a previously chosen subset of the original model. In this paper, we discuss how to deﬁne and execute model refactorings as rule-based transformations in the context of the UML and MOF standards. We also present an experimental tool to execute this kind of transformation.",
        "keywords": [
            "Model transformation",
            "Model driven engineering"
        ],
        "authors": [
            "Ivan Porres"
        ],
        "file_path": "data/sosym-all/s10270-005-0088-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "DropsBox: the Dresden Open Software Toolbox Domain-speciﬁc modelling tools beyond metamodels and transformations",
        "submission-date": "2010/11",
        "publication-date": "2012/11",
        "abstract": "The Dresden Open Software Toolbox (DropsBox) is a software modelling toolbox consisting of a set of open source tools developed by the Software Technology Group at TU Dresden. The DropsBox is built on top of the Eclipse Platform and the Eclipse Modeling Framework. The DropsBox contributes to the development and application of domain-speciﬁc language changes (DSLs) in model-driven software development. It can be customised by tool and language developers to support various activities of a DSL’s life cycle ranging from language design to language application and evolution. In this paper, we provide an overview of the DSL life cycle, the DropsBox tools, and their interaction on a common example. Furthermore, we discuss our experiences in developing and integrating tools for DropsBox in an academic environment.",
        "keywords": [
            "Domain-speciﬁc modelling environment",
            "Domain-speciﬁc language",
            "Language life cycle",
            "Modelling tool",
            "MDSD",
            "EMF"
        ],
        "authors": [
            "Uwe Aßmann",
            "Andreas Bartho",
            "Christoff Bürger",
            "Sebastian Cech",
            "Birgit Demuth",
            "Florian Heidenreich",
            "Jendrik Johannes",
            "Sven Karol",
            "Jan Polowinski",
            "Jan Reimann",
            "Julia Schroeter",
            "Mirko Seifert",
            "Michael Thiele",
            "Christian Wende",
            "Claas Wilke"
        ],
        "file_path": "data/sosym-all/s10270-012-0284-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Semi-automatic derivation of RESTful choreographies from business process choreographies",
        "submission-date": "2016/12",
        "publication-date": "2018/01",
        "abstract": "Enterprises reach out for collaborations with other organizations in order to offer complex products and services to the market. Such collaboration and coordination between different organizations, for a good share, is facilitated by information technology. The BPMN process choreography is a modeling language for specifying the exchange of information and services between different organizations at the business level. Recently, there is a surging use of the REST architectural style for the provisioning of services on the web, but few systematic engineering approach to design their collaboration. In this paper, we address this gap in a comprehensive way by deﬁning a semi-automatic method for the derivation of RESTful choreographies from process choreographies. The method is based on natural language analysis techniques to derive interactions from the textual information in process choreographies. The proposed method is evaluated in terms of effectiveness resulting in the intervention of a web engineer in only about 10% of all generated RESTful interactions.",
        "keywords": [
            "Business process choreographies",
            "RESTful choreographies",
            "Natural language analysis"
        ],
        "authors": [
            "Adriatik Nikaj",
            "Mathias Weske",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-017-0653-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatically reasoning about metamodeling",
        "submission-date": "2012/04",
        "publication-date": "2013/02",
        "abstract": "Metamodeling is foundational to many modeling frameworks, and so it is important to formalize and reason about it. Ideally, correctness proofs and test-case generation on the metamodeling framework should be automatic. However, it has yet to be shown that extensive automated reasoning on metamodeling frameworks can be achieved. In this paper, we present one approach to this problem: metamodeling frameworks are speciﬁed modularly using algebraic data types and constraint logic programming (CLP). Proofs and test-case generation are encoded as open world query operations and automatically solved.",
        "keywords": [
            "Metamodeling",
            "Formal speciﬁcations",
            "Automated analysis"
        ],
        "authors": [
            "Ethan K. Jackson",
            "Tihamer Levendovszky",
            "Daniel Balasubramanian"
        ],
        "file_path": "data/sosym-all/s10270-013-0315-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A visual language for modeling multiple perspectives of business process compliance rules",
        "submission-date": "2014/10",
        "publication-date": "2016/04",
        "abstract": "A fundamental challenge for enterprises is to ensure compliance of their business processes with imposed compliance rules stemming from various sources, e.g., corporate guidelines, best practices, standards, and laws. In general, a compliance rule may refer to multiple process perspectives including control ﬂow, time, data, resources, and interactions with business partners. On one hand, compliance rules should be comprehensible for domain experts who must deﬁne, verify, and apply them. On the other, these rules should have a precise semantics to avoid ambiguities and enable their automated processing. Providing a visual language is advantageous in this context as it allows hiding formal details and offering an intuitive way of modeling the compliance rules. However, existing visual languages for compliance rule modeling have focused on the control ﬂow perspective so far, but lack proper support for the other process perspectives. To remedy this drawback, this paper introduces the extended Compliance Rule Graph language, which enables the visual modeling of compliance rules with the support of multiple perspectives. Overall, this language will foster the modeling and veriﬁcation of compliance rules in practice.",
        "keywords": [
            "Business process compliance",
            "Extended Compliance Rule Graphs",
            "Business process modeling",
            "Smart processes"
        ],
        "authors": [
            "David Knuplesch",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-016-0526-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formalising privacy regulations with bigraphs",
        "submission-date": "2024/08",
        "publication-date": "2025/04",
        "abstract": "With many governments regulating the handling of user data—the General Data Protection Regulation, the California Consumer Privacy Act, and the Saudi Arabian Personal Data Protection Law—ensuring systems comply with data privacy legislation is of high importance. Checking compliance is a tricky process and often includes many manual elements. We propose that formal methods, that model systems mathematically, can provide strong guarantees to help companies prove their adherence to legislation. To increase usability we advocate a diagrammatic approach, based on bigraphical reactive systems, where privacy experts can explicitly visualise the systems and describe updates, via rewrite rules, that describe system behaviour. The rewrite rules allow ﬂexibility in integrating privacy policies with user-speciﬁed systems. We focus on modelling notions of providing consent, withdrawing consent, purpose limitations, the right to access and sharing data with third parties, and deﬁne privacy properties that we want to prove within the systems. Properties are expressed using the computation tree logic and proved using model checking. To show the generality of the proposed framework, we apply it to two examples: a bank notiﬁcation system, inspired by Monzo’s privacy policy, and a cloud-based home healthcare system based on the Fitbit app’s privacy policy.",
        "keywords": [
            "Privacy regulations",
            "Privacy modelling",
            "Formal modelling",
            "Model checking",
            "Bigraphs"
        ],
        "authors": [
            "Ebtihal Althubiti",
            "Blair Archibald",
            "Michele Sevegnani"
        ],
        "file_path": "data/sosym-all/s10270-025-01293-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated support for deriving test requirements from UML statecharts",
        "submission-date": "2004/05",
        "publication-date": "2005/09",
        "abstract": "Many statechart-based testing strategies result in specifying a set of paths to be executed through a (flattened) statechart. These techniques can usually be easily automated so that the tester does not have to go through the tedious procedure of deriving paths manually to comply with a coverage criterion. The next step is then to take each test path individually and derive test requirements leading to fully specified test cases. This requires that we determine the system state required for each event/transition that is part of the path to be tested and the input parameter values for all events and actions associated with the transitions. We propose here a methodology towards the automation of this procedure, which is based on a careful normalization and analysis of operation contracts and transition guards written with the Object Constraint Language (OCL). It is illustrated by one case study that exemplifies the steps of our methodology and provides a first evaluation of its applicability.",
        "keywords": [],
        "authors": [
            "L. C. Briand",
            "Y. Labiche",
            "J. Cui"
        ],
        "file_path": "data/sosym-all/s10270-005-0090-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Maintaining consistency in networks of models: bidirectional transformations in the large",
        "submission-date": "2018/06",
        "publication-date": "2019/05",
        "abstract": "The model-driven development of systems involves multiple models, metamodels and transformations, and relationships between them. A bidirectional transformation (bx) is usually deﬁned as a means of maintaining consistency between “two (or more)” models. This includes cases where one model may be generated from one or more others, as well as more complex (“symmetric”) cases where models record partially overlapping information. In recent years, binary bx, those relating two models, have been extensively studied. Multiary bx, those relating more than two models, have received less attention. In this paper, we consider how a multiary consistency relation may be deﬁned in terms of binary consistency relations and how consistency restoration may be carried out on a network of models and relationships between them. In particular, we consider the circumstances under which we can prove non-interference between several bidirectional transformations that impact on the same model and how the use of a more reﬁned notion of consistency can help in cases where this is not possible. In the process, we develop an abstract theory of parts of a model that are read or modiﬁed by a bidirectional transformation. We relate the work to megamodelling and discuss further research that is needed.",
        "keywords": [
            "Model-driven development",
            "Bidirectional transformation",
            "Consistency",
            "Megamodel",
            "Model decomposition",
            "Non-interference"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-019-00736-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "OIL: an industrial case study in language engineering with Spoofax",
        "submission-date": "2023/09",
        "publication-date": "2024/06",
        "abstract": "Domain-speciﬁc languages (DSLs) promise to improve the software engineering process, e.g., by reducing software development and maintenance effort and by improving communication, and are therefore seeing increased use in industry. To support the creation and deployment of DSLs, language workbenches have been developed. However, little is published about the actual added value of a language workbench in an industrial setting, compared to not using a language workbench. In this paper, we evaluate the productivity of using the Spoofax language workbench by comparing two implementations of an industrial DSL, one in Spoofax and one in Python, that already existed before the evaluation. The subject is the Open Interaction Language (OIL): a complex DSL for implementing control software with requirements imposed by its industrial context at Canon Production Printing. Our ﬁndings indicate that it is more productive to implement OIL using Spoofax compared to using Python, especially if editor services are desired. Although Spoofax was sufﬁcient to implement OIL, we find that Spoofax should especially improve on practical aspects to increase its adoptability in industry.",
        "keywords": [
            "Language workbench",
            "Language engineering",
            "Case study"
        ],
        "authors": [
            "Olav Bunte",
            "Jasper Denkers",
            "Louis C. M. van Gool",
            "Jurgen J. Vinju",
            "Eelco Visser",
            "Tim A. C. Willemse",
            "Andy Zaidman"
        ],
        "file_path": "data/sosym-all/s10270-024-01185-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Specifying and executing behavioral requirements: the play-in/play-out approach",
        "submission-date": "2002/09",
        "publication-date": "2003/04",
        "abstract": "A powerful methodology for scenario-based speciﬁcation of reactive systems is described, in which the behavior is “played in” directly from the system’s GUI or some abstract version thereof, and can then be “played out”. The approach is supported and illustrated by a tool, which we call the play-engine. As the behav-ior is played in, the play-engine automatically generates a formal version in an extended version of the language of live sequence charts (LSCs). As they are played out, it causes the application to react according to the universal (“must”) parts of the speciﬁcation; the existential (“may”) parts can be monitored to check their successful completion. Play-in is a user-friendly high-level way of specifying behavior and play-out is a rather surprising way of working with a fully operational system directly from its inter-object requirements. The ideas appear to be relevant to many stages of system development, including requirements engineering, speciﬁcation, testing, analysis and implementation.",
        "keywords": [
            "Live sequence charts (LSCs)",
            "Requirements engineering",
            "System modeling and execution",
            "Scenarios",
            "Testing",
            "UML"
        ],
        "authors": [
            "David Harel",
            "Rami Marelly"
        ],
        "file_path": "data/sosym-all/s10270-002-0015-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A closer look at activity relationships to improve business process redesign",
        "submission-date": "2023/11",
        "publication-date": "2024/11",
        "abstract": "Business process models serve as visual representations of the structured sequence of activities, or activity relationships, aimed\nat achieving speciﬁc organizational objectives. These models explicitly depict only a portion of the activity relationships,\nleaving other, so-called hidden relationships to be inferred through transitivity and domain knowledge. Because designers\nlack visibility into these hidden relationships, tasks associated with process redesign (BPR) are more challenging. BPR is\na specialized area within business process management focused on modifying process behavior at the model level. While\nbest practices have been introduced, explicit guidance for implementing them is currently lacking. This paper introduces\nan approach to delineate all possible relationships between any two activities within a BPMN diagram by considering a\ncombination of existential and temporal dependencies. Furthermore, the proposed approach facilitates a detailed speciﬁcation\nof change operations related to BPR and their consequential effects on activity relationships. We illustrate this by applying\nBPR strategies such as resequencing and parallelism to the use case of an order management system.",
        "keywords": [
            "Business process modeling",
            "Activity relationships",
            "Existential dependencies",
            "Business process redesign"
        ],
        "authors": [
            "Kerstin Andree",
            "Dorina Bano",
            "Mathias Weske"
        ],
        "file_path": "data/sosym-all/s10270-024-01234-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Computing refactorings of state machines",
        "submission-date": "2006/02",
        "publication-date": "2007/01",
        "abstract": "For behavior models expressed in statechart-like formalisms, we show how to compute semantically equivalent yet structurally different models. These refactorings are deﬁned by user-provided logical predicates that partition the system’s state space and that characterize coherent parts – modes or control states – of the behavior. We embed the refactorings into an incremental development process that uses a combination of both tables and graphically represented state machines for describing systems.",
        "keywords": [],
        "authors": [
            "Alexander Pretschner",
            "Wolfgang Prenninger"
        ],
        "file_path": "data/sosym-all/s10270-006-0037-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Looking back at UML",
        "submission-date": "2011/10",
        "publication-date": "2012/07",
        "abstract": "This paper sets out in detail the development of the Uniﬁed Modeling Language and its derivatives from its beginning until the present. The paper describes the processes that were used to develop the language, the architecture and intended uses of the language, its strengths and weaknesses, and the steps that are being taken to make it ready for future developments.",
        "keywords": [
            "UML"
        ],
        "authors": [
            "Steve Cook"
        ],
        "file_path": "data/sosym-all/s10270-012-0256-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Search-based model transformation by example",
        "submission-date": "2009/06",
        "publication-date": "2010/09",
        "abstract": "Model transformation (MT) has become an important concern in software engineering. In addition to its role in model-driven development, it is useful in many other situations such as measurement, refactoring, and test-case generation. Roughly speaking, MT aims to derive a target model from a source model by following some rules or principles. So far, the contributions in MT have mostly relied on defining languages to express transformation rules. However, the task of defining, expressing, and maintaining these rules can be difficult, especially for proprietary and non-widely used formalisms. In some situations, companies have accumulated examples from past experiences. Our work starts from these observations to view the transformation problem as one to solve with fragmentary knowledge, i.e. with only examples of source-to-target MTs. Our approach has two main advantages: (1) it always proposes a transformation for a source model, even when rule induction is impossible or difficult to achieve; (2) it is independent from the source and target formalisms; aside from the examples, no extra information is needed. In this context, we propose an optimization-based approach that consists of finding in the examples combinations of transformation fragments that best cover the source model. To that end, we use two strategies based on two search-based algorithms: particle swarm optimization and simulated annealing. The results of validating our approach on industrial projects show that the obtained models are accurate.",
        "keywords": [
            "Search-based software engineering",
            "Automated model transformation",
            "Transformation by example"
        ],
        "authors": [
            "Marouane Kessentini",
            "Houari Sahraoui",
            "Mounir Boukadoum",
            "Omar Ben Omar"
        ],
        "file_path": "data/sosym-all/s10270-010-0175-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "View-based model-driven software development with ModelJoin",
        "submission-date": "2013/06",
        "publication-date": "2014/05",
        "abstract": "Fragmentation of information across instances of different metamodels poses a signiﬁcant problem for software developers and leads to a major increase in effort of transformation development. Moreover, compositions of metamodels tend to be incomplete, imprecise, and erroneous, making it impossible to present it to users or use it directly as input for applications. Customized views satisfy information needs by focusing on a particular concern, and ﬁltering out information that is not relevant to this concern. For a broad establishment of view-based approaches, an automated solution to deal with separate metamodels and the high complexity of model transformations is necessary. In this paper, we present the ModelJoin approach for the rapid creation of views. Using a human-readable textual DSL, developers can deﬁne custom views declaratively without having to write model transformations or deﬁne a bridging metamodel. Instead, a metamodel generator and higher-order transformations create annotated target metamodels and the appropriate transformations on-the-ﬂy. The resulting views, which are based on these metamodels, contain joined instances and can effectively express concerns unforseen during metamodel design. We have applied the ModelJoin approach and validated the textual DSL in a case study using the Palladio Component Model.",
        "keywords": [
            "View-based modeling",
            "Model-driven software development",
            "Model transformation",
            "Model-based query language"
        ],
        "authors": [
            "Erik Burger",
            "Jörg Henss",
            "Martin Küster",
            "Steffen Kruse",
            "Lucia Happe"
        ],
        "file_path": "data/sosym-all/s10270-014-0413-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correct-by-construction synthesis of model transformations using transformation patterns",
        "submission-date": "2011/06",
        "publication-date": "2012/10",
        "abstract": "Model transformations are an essential part\nof model-based development approaches, such as Model-\ndriven Architecture (MDA) and Model-driven Develop-\nment (MDD). Model transformations are used to reﬁne and\nabstract models, to re-express models in a new modelling\nlanguage, and to analyse, refactor, compare and improve\nmodels. Therefore, the correctness of model transformations\nis critically important for successful application of model-\nbased development: software developers should be able to\nrely upon the correct processing of their models by trans-\nformations in the same way that they rely upon compilers\nto produce correct executable versions of their programs.\nIn this paper, we address this problem by deﬁning standard\nstructures for model transformation speciﬁcations and imple-\nmentations, which serve as patterns and strategies for con-\nstructing a wide range of model transformations. These are\nincorporated into a tool-supported process which automati-\ncally synthesises implementations of model transformations\nfrom their speciﬁcations, these implementations are correct-\nby-construction with respect to their speciﬁcations.",
        "keywords": [
            "Model transformation",
            "Patterns",
            "Model-driven\ndevelopment",
            "Veriﬁcation"
        ],
        "authors": [
            "K. Lano",
            "S. Kolahdouz-Rahimi",
            "I. Poernomo",
            "J. Terrell",
            "S. Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-012-0291-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A relational approach to deﬁning and implementing transformations between metamodels",
        "submission-date": "2003/02",
        "publication-date": "2003/09",
        "abstract": "The Model-Driven Architecture initiative of the OMG promotes the idea of transformations in the context of mapping from platform independent to platform speciﬁc models. Additionally, the popularity of XML and the wide spread use of XSLT has raised the proﬁle of model transformation as an important technique for computing. In fact, computing may well be moving to a new paradigm in which models are considered ﬁrst class entities and transformations between them are a major function performed on those models. This paper proposes an approach to deﬁning and implementing model transformations which uses metamodelling patterns to capture the essence of mathematical relations. It shows how these patterns can be used to deﬁne the relationship between two diﬀerent metamodels. A goal of the approach is to enable complete speciﬁcations from which tools can be generated. The paper describes implementations of the examples, which have been partially generated from the deﬁnitions using a tool generation tool. A number of issues emerge which need to be solved in order to achieve the stated goal; these are discussed.",
        "keywords": [
            "Model transformation",
            "UML",
            "Model-driven architecture"
        ],
        "authors": [
            "David Akehurst",
            "Stuart Kent",
            "Octavian Patrascoiu"
        ],
        "file_path": "data/sosym-all/s10270-003-0032-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "More matters on (meta-)modelling: remarks on Thomas Kühne’s “matters\"",
        "submission-date": "2006/02",
        "publication-date": "2006/10",
        "abstract": "This discussion paper refers to the article “Matters on(Meta-)Modeling”of Thomas Kühne. Many of his concepts and proposals are appreciated, as e.g. the clear distinction of type and token models or of prom- inent relationships like instantiation, generalisation or “meta-ness”. However, for some of the presented views and deﬁnitions alternative approaches may be followed. This concerns e.g. the relationship between models and their originals (“systems”) including the distinction of descriptive and prescriptive models, the view on various kinds of instantiation and the way how metamodels are deﬁned. Some of these questions are debated in detail and alternative positions are presented.",
        "keywords": [
            "Modelling",
            "Model and original",
            "Prescriptive/descriptive model",
            "Transformation",
            "Projection",
            "Type model",
            "Token model",
            "Ontological/linguistic instantiation",
            "Metamodel"
        ],
        "authors": [
            "Wolfgang Hesse"
        ],
        "file_path": "data/sosym-all/s10270-006-0033-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A ﬁne-grained analysis of the support provided by UML class diagrams and ER diagrams during data model maintenance",
        "submission-date": "2012/03",
        "publication-date": "2013/01",
        "abstract": "This paper presents the results of an empirical study aiming at comparing the support provided by ER and UML class diagrams during maintenance of data models. We performed one controlled experiment and two replications that focused on comprehension activities (the ﬁrst activity in the maintenance process) and another controlled experiment on modiﬁcation activities related to the implementation of given change requests. The results achieved were analyzed at a ﬁne-grained level aiming at comparing the support given by each single building block of the two notations. Such an analysis is used to identify weaknesses (i.e., building blocks not easy to comprehend) in a notation and/or can justify the need of preferring ER or UML for data modeling. The analysis revealed that the UML class diagrams generally provided a better support for both comprehension and modiﬁcation activities performed on data models as compared to ER diagrams. Nevertheless, the former has some weaknesses related to three building blocks, i.e., multi-value attribute, composite attribute, and weak entity. These ﬁndings suggest that an extension of UML class diagrams should be considered to overcome these weaknesses and improve the support provided by UML class diagrams during maintenance of data models.",
        "keywords": [],
        "authors": [
            "Gabriele Bavota",
            "Carmine Gravino",
            "Rocco Oliveto",
            "Andrea De Lucia",
            "Genoveffa Tortora",
            "Marcela Genero",
            "José A. Cruz-Lemus"
        ],
        "file_path": "data/sosym-all/s10270-012-0312-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling multi-criticality vehicular software systems: evolution of an industrial component model",
        "submission-date": "2019/01",
        "publication-date": "2020/04",
        "abstract": "Software in modern vehicles consists of multi-criticality functions, where a function can be safety-critical with stringent real-\ntime requirements, less critical from the vehicle operation perspective, but still with real-time requirements, or not critical at\nall. Next-generation autonomous vehicles will require higher computational power to run multi-criticality functions and such\na power can only be provided by parallel computing platforms such as multi-core architectures. However, current model-based\nsoftware development solutions and related modelling languages have not been designed to effectively deal with challenges\nspeciﬁc of multi-core, such as core-interdependency and controlled allocation of software to hardware. In this paper, we report\non the evolution of the Rubus Component Model for the modelling, analysis, and development of vehicular software systems\nwith multi-criticality for deployment on multi-core platforms. Our goal is to provide a lightweight and technology-preserving\ntransition from model-based software development for single-core to multi-core. This is achieved by evolving the Rubus\nComponent Model to capture explicit concepts for multi-core and parallel hardware and for expressing variable criticality of\nsoftware functions. The paper illustrates these contributions through an industrial application in the vehicular domain.",
        "keywords": [
            "Model-based engineering",
            "Metamodelling",
            "Single-core",
            "Multi-core",
            "Multi-criticality",
            "Vehicular embedded systems",
            "Real-time systems"
        ],
        "authors": [
            "Alessio Bucaioni",
            "Saad Mubeen",
            "Federico Ciccozzi",
            "Antonio Cicchetti",
            "Mikael Sjödin"
        ],
        "file_path": "data/sosym-all/s10270-020-00795-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EDITORIAL",
        "submission-date": "2004/09",
        "publication-date": "2007/03",
        "abstract": "The field of graph transformations was born in the late 1960s and has been continuously developed and applied in the many areas where the static and the dynamic aspects of models can be naturally described by means of graphical structures and their changes. This special section of the Journal of Software and System Modeling contains three of the papers that were submitted by invitation, and are based on the authors’ contributions to ICGT 2004 and SETra 2004.",
        "keywords": [],
        "authors": [
            "Francesco Parisi-Presicce"
        ],
        "file_path": "data/sosym-all/s10270-007-0052-1.pdf",
        "classification": {
            "is_transformation_paper": true,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Semi-automatic service value network modeling approach based on external public data",
        "submission-date": "2021/06",
        "publication-date": "2022/06",
        "abstract": "Various emerging IT technologies are widely used in the service industry. Thus, an increasing number of new service models have also emerged, including the Internet of Services (IoS). The IoS supports network-based service collaboration and transactions among various service participants from different domains and different organizations, and it is expected to deliver the maximum service value to all stakeholders. To describe the cross-domain, cross-organization, and cross-value chain characteristics of the IoS from a value perspective and support subsequent analysis of the value network and optimization of the IoS, this paper proposes a semi-automatic modeling method for a IoS-oriented value network based on external public data. We ﬁrst propose an intelligent domain entity recognition algorithm based on multidimensional web data to help value network modelers realize effective and efﬁcient recognition of service participants. Then, based on external news data, an intelligent domain relationship extraction algorithm that combines the Bert+BiLSTM+CRF model with the LightGBM model is proposed to effectively and efﬁciently identify the value exchange relationships among service participants, thereby forming an IoS-oriented value network model (IVN). Finally, to extend the cross-domain semantics of the IVN and support analysis of the IVN, we present a domain-speciﬁc value chain extraction algorithm based on typical patterns to complete the cross-domain semantic annotation of the IVN. The effectiveness and efﬁciency of the proposed methods and algorithms are validated through experimental analysis and a case study, which can be of great help in IVN modeling.",
        "keywords": [
            "Service value network",
            "Modeling method",
            "Service value",
            "Value network modeling"
        ],
        "authors": [
            "Jingying Wang",
            "Chao Ma",
            "Huixin Xu",
            "Zhiying Tu",
            "Xiaofei Xu",
            "Hanchuan Xu",
            "Zhongjie Wang"
        ],
        "file_path": "data/sosym-all/s10270-022-01014-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Survey of reliability and availability prediction methods from the viewpoint of software architecture",
        "submission-date": "2006/01",
        "publication-date": "2007/01",
        "abstract": "Many future software systems will be distributed across a network, extensively providing different kinds of services for their users. These systems must be highly reliable and provide services when required. Reliability and availability must be engineered into software from the onset of its development, and potential problems must be detected in the early stages, when it is easier and less expensive to implement modifications. The software architecture design phase is the first stage of software development in which it is possible to evaluate how well the quality requirements are being met. For this reason, a method is needed for analyzing software architecture with respect to reliability and availability. In this paper, we define a framework for comparing reliability and availability analysis methods from the viewpoint of software architecture. Our contribution is the comparison of the existing analysis methods and techniques that can be used for reliability and availability prediction at the architectural level. The objective is to discover which methods are suitable for the reliability and availability prediction of today’s complex systems, what are the shortcomings of the methods, and which research activities need to be conducted in order to overcome these identified shortcomings. The comparison reveals that none of the existing methods entirely fulfill the requirements that are defined in the framework. The comparison framework also defines the characteristics required of new reliability and availability analysis methods. Additionally, the framework is a valuable tool for selecting the best suitable method for architecture analysis. Furthermore, the framework can be extended and used for other evaluation methods as well.",
        "keywords": [
            "Reliability and availability analysis",
            "Software architecture",
            "Software components"
        ],
        "authors": [
            "Anne Immonen",
            "Eila Niemelä"
        ],
        "file_path": "data/sosym-all/s10270-006-0040-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Visual query languages to design complex queries: a systematic literature review",
        "submission-date": "2021/08",
        "publication-date": "2022/12",
        "abstract": "Structured query language (SQL) is a widely used language for accessing both relational and non-relational databases. SQL is the standard form of access in relational databases, while in non-relational databases, SQL is becoming increasingly available and consolidating itself as an access interface for querying data in cluster environments. Despite its declarative syntax, the speciﬁcation of SQL queries is not a trivial task, even for experts, because some queries demand complex constructs (i.e., subqueries, joins, set operations, conditional expressions, grouping restrictions, and recursion). Visual query languages (VQLs) are an alternative to reduce this complexity. However, although several VQLs have been proposed, they are not widely used in practice. By identifying and analyzing the support provided by VQLs that make it possible to design complex SQL queries, this study collected evidence that helps discover the strengths and weaknesses of each VQL, providing useful feedback for other research initiatives that seek to propose improved VQLs. For this purpose, a systematic literature review was carried out. After analyzing 22 relevant studies and performing 462 inspections, this review points to the need for more expressive VQLs, computer-aided software engineering (CASE) tools available to end users, and more rigorous evaluations to investigate the VQL syntax and semantics.",
        "keywords": [
            "Visual query language",
            "Complex query",
            "Conceptual modeling",
            "SQL"
        ],
        "authors": [
            "Edson Silva",
            "Robson Fidalgo",
            "Márcio Ferro",
            "Natália Franco"
        ],
        "file_path": "data/sosym-all/s10270-022-01071-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Boosting bug localization in software models of video games with simulations and component-speciﬁc genetic operations",
        "submission-date": "2023/04",
        "publication-date": "2025/01",
        "abstract": "The development and maintenance of video games present unique challenges that differentiate them from Classic Software Engineering (CSE) such as the increased difﬁculty in locating bugs within video games. This distinction has given rise to Game Software Engineering (GSE), a subﬁeld that intersects software engineering and video games. Our work proposes a novel way for bug localization in video games by evolving simulations via an evolutionary algorithm, which helps to explore the large number of possible simulations. Simulations generate data (i.e., traces) from the behavior of non-player characters (NPCs). NPCs are not controlled by the player and are key components of video games. We hypothesize that such traces can be instrumental in locating bugs. Our approach automatically locates potential buggy model elements from traces. Furthermore, we propose a novel way of applying genetic operations to evolve simulations by selectively combining their components, rather than combining all components as a whole. We evaluate our approach in the commercial video game Kromaia, and the results indicate that evolving simulations using our novel component-speciﬁc genetic operations boosts bug localization. Speciﬁcally, our approach improved the F-measure for all bug categories over randomly combining all components, the baseline (which focuses on CSE and utilizes bug reports), and Random Search by 7.93%, 27.17%, and 46.34%, respectively. This work opens a new research direction for further exploration in bug localization within GSE and potentially in CSE as well. Moreover, it encourages other researchers to explore alternative genetic operations rather than selecting them by default.",
        "keywords": [
            "Bug localization",
            "Video games",
            "Search-based software engineering",
            "Model-driven engineering"
        ],
        "authors": [
            "Rodrigo Casamayor",
            "Lorena Arcega",
            "Francisca Pérez",
            "Carlos Cetina"
        ],
        "file_path": "data/sosym-all/s10270-024-01253-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Uncertainty-Wise Cyber-Physical System test modeling",
        "submission-date": "2016/07",
        "publication-date": "2017/07",
        "abstract": "It is important that a Cyber-Physical System (CPS) with uncertainty in its behavior caused by its unpredictable operating environment, to ensure its reliable operation. One method to ensure that the CPS will handle such uncertainty during its operation is by testing the CPS with model-based testing (MBT) techniques. However, existing MBT techniques do not explicitly capture uncertainty in test ready models, i.e., capturing the uncertain expected behavior of a CPS in the presence of environment uncertainty. To fill this gap, we present an Uncertainty-Wise test-modeling framework, named as UncerTum, to create test ready models to support MBT of CPSs facing uncertainty. UncerTum relies on the definition of a UML profile [the UML Uncertainty Profile (UUP)] and a set of UML Model Libraries extending the UML profile for Modeling and Analysis of Real-Time and Embedded Systems (MARTE). UncerTum also benefits from the UML Testing Profile V.2 to support standard-based MBT. UncerTum was evaluated with two industrial CPS case studies, one real-world case study, and one open-source CPS case study from the following four perspectives: (1) Completeness and Coverage of the profiles and Model Libraries in terms of concepts defined in their underlying uncertainty conceptual model for CPSs, i.e., U-Model and MARTE, (2) Effort required to model uncertainty with UncerTum, and (3) Correctness of the developed test ready models, which was assessed via model execution. Based on the evaluation, we can conclude that we were successful in modeling all the uncertainties identified in the four case studies, which gives us an indication that UncerTum is sufficiently complete. In terms of modeling effort, we concluded that on average UncerTum requires 18.5% more time to apply stereotypes from UUP on test ready models.",
        "keywords": [
            "Uncertainty",
            "Cyber-Physical System",
            "UML",
            "Model-based testing"
        ],
        "authors": [
            "Man Zhang",
            "Shaukat Ali",
            "Tao Yue",
            "Roland Norgren",
            "Oscar Okariz"
        ],
        "file_path": "data/sosym-all/s10270-017-0609-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EB3: an entity-based black-box speciﬁcation method for information systems",
        "submission-date": "2002/05",
        "publication-date": "2003/06",
        "abstract": "This paper describes a formal method for specifying the observable (external) behavior of information systems using a process algebra and input-output traces. Its notation is mainly based on the entity concept, borrowed from the Jackson System Development method, and integrated with the requirements class diagram to represent data structures and associations. The speciﬁcation process promotes modular and incremental description of the behavior of each entity through process abstraction, entity type patterns, and entity attribute function patterns. Valid system input traces result from the composition of entity traces by using parallel composition operations. The association between input traces and outputs through an input-output relation completes the speciﬁcation process.",
        "keywords": [
            "Trace-based speciﬁcations",
            "Black-box speciﬁcations",
            "Process algebra",
            "JSD",
            "Cleanroom",
            "Patterns"
        ],
        "authors": [
            "M. Frappier",
            "R. St-Denis"
        ],
        "file_path": "data/sosym-all/s10270-003-0024-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Mining reading patterns from eye-tracking data: method and demonstration",
        "submission-date": "2018/10",
        "publication-date": "2019/10",
        "abstract": "Understanding how developers interact with different software artifacts when performing comprehension tasks has a potential to improve developers’ productivity. In this paper, we propose a method to analyze eye-tracking data using process mining to ﬁnd distinct reading patterns of how developers interacted with the different artifacts. To validate our approach, we conducted an exploratory study using eye-tracking involving 11 participants. We applied our method to investigate how developers interact with different artifacts during domain and code understanding tasks. To contextualize the reading patterns and to better understand the perceived beneﬁts and challenges participants associated with the different artifacts and their choice of reading patterns, we complemented the eye-tracking data with the data obtained from think aloud. The study used behavior-driven development, a development practice that is increasingly used in Agile software development contexts, as a setting. The study shows that our method can be used to explore developers’ behavior at an aggregated level and identify behavioral patterns at varying levels of granularity.",
        "keywords": [
            "Process mining",
            "Eye-tracking",
            "Reading patterns",
            "Source code",
            "Behavior-driven development"
        ],
        "authors": [
            "Constantina Ioannou",
            "Indira Nurdiani",
            "Andrea Burattin",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-019-00759-4.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Formal modeling of biomedical signal acquisition systems: source of evidence for certiﬁcation",
        "submission-date": "2016/06",
        "publication-date": "2017/08",
        "abstract": "Biomedical signal acquisition systems are software-intensive medical systems composed of processors, transducers, ampliﬁers, ﬁlters, and converters. We present in this article a formal modeling methodology of biomedical signal acquisition systems using Colored Petri Nets (CPN) and based on a frequency-domain approach. In the methodology, a reference model represents the main features of these medium risk systems. We argue that this kind of model is useful to assist manufacturers to reduce the number of defects in systems and to generate safety and effectiveness evi- dence throughout certiﬁcation. Therefore, we describe two main contributions in this article. We provide a reference model of biomedical signal acquisition systems and show how manufacturers can generate evidence by means of an electrocardiography (ECG) case study. We carried out the case study by extending the reference model to represent the behavior of an ECG system using a basic cardiac monitor conﬁguration based on the single-lead, heart rate monitor front end (AD8232) and the low power precision analog microcontroller, ARM cortex M3 with dual sigma-delta con- verters (ADUCM360). We veriﬁed the model against safety requirements with the model checking technique (safety evi- dence) and validated it by comparing output signals with a ﬁltered ECG record available on the PHYSIONET ECG-ID database in the frequency and time domains (effectiveness evidence). This methodology enables manufacturers to iden- tify defects in systems earlier in the development process aiming to decrease costs and development time.",
        "keywords": [
            "Biomedical signal acquisition systems",
            "Colored petri nets",
            "Formal methods",
            "Model checking",
            "Frequency-domain approach"
        ],
        "authors": [
            "Alvaro Sobrinho",
            "Leandro Dias da Silva",
            "Angelo Perkusich",
            "Paulo Cunha",
            "Thiago Cordeiro",
            "Antonio Marcus Nogueira Lima"
        ],
        "file_path": "data/sosym-all/s10270-017-0616-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Message choreography modeling\nA domain-speciﬁc language for consistent enterprise service integration",
        "submission-date": "2010/12",
        "publication-date": "2012/09",
        "abstract": "Service-based applications are based on modern architectures that require careful design of interfaces and protocols to allow smooth integration of service components. These design artifacts are not only useful for implementation, but could also be used for the derivation of integration tests. In order to be applied in these different activities of the development process, they have to conform to existing requirements and other speciﬁcations at different architectural levels. In addition, their internal consistency has to be ensured. In this paper, we present an approach to service integration based on a domain-speciﬁc language for service choreographies. We ﬁrst explain the motivation for our work by deﬁning the industrial context that led to the deﬁnition of a domain-speciﬁc choreography language, called message choreography modeling (MCM). We then provide syntax and semantics for MCM, together with suitable methods for ensuring its consistency. Finally, we report on our experience in applying the described language in practice.",
        "keywords": [
            "Domain speciﬁc modeling languages",
            "Message choreography models",
            "Service choreography",
            "Enterprise SOA"
        ],
        "authors": [
            "Alin Stefanescu",
            "Sebastian Wieczorek",
            "Matthias Schur"
        ],
        "file_path": "data/sosym-all/s10270-012-0272-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Toward a framework for self-adaptive workﬂows in cyber-physical systems",
        "submission-date": "2016/12",
        "publication-date": "2017/11",
        "abstract": "With the establishment of Cyber-physical Systems (CPS) and the Internet of Things, the virtual world of software and services and the physical world of objects and humans move closer together. Despite being a useful means for automation, BPM technologies and workﬂow systems are yet not fully capable of executing processes in CPS. The effects on and possible errors and inconsistencies in the physical world are not considered by “traditional” workﬂow engines. In this work we propose a framework for self-adaptive workﬂows in CPS based on the MAPE-K feedback loop. Within this loop monitoring and analysis of additional sensor and context data is used to check for unanticipated errors in the physical world. Planning and execution of compensation actions restores Cyber-physical Consistency, which leads to an increased resilience of the process execution environment. The framework facilitates the separation of CPS aspects from the “regular” workﬂow views. We show the feasibility of this approach in a smart home scenario and discuss the application of our approach for legacy BPM systems.",
        "keywords": [
            "Workflows for the Internet of Things",
            "Cyber-physical Systems",
            "Self-adaptive Workflows",
            "Real-world processes",
            "Cyber-physical consistency"
        ],
        "authors": [
            "Ronny Seiger",
            "Steffen Huber",
            "Peter Heisig",
            "Uwe Aßmann"
        ],
        "file_path": "data/sosym-all/s10270-017-0639-0.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "Evaluating probabilistic models with uncertain model parameters",
        "submission-date": "2012/01",
        "publication-date": "2012/09",
        "abstract": "Probabilistic models are commonly used to evaluate quality attributes, such as reliability, availability, safety and performance of software-intensive systems. The accuracy of the evaluation results depends on a number of system properties which have to be estimated, such as environmental factors or system usage. Researchers have tackled this problem by including uncertainties in the probabilistic models and solving them analytically or with simulations. The input parameters are commonly assumed to be normally distributed. Accordingly, reporting the mean and variances of the resulting attributes is usually considered sufficient. However, many of the uncertain factors do not follow normal distributions,andanalyticalmethodstoderiveobjectiveuncertainties become impractical with increasing complexity of the probabilistic models. In this work, we introduce a simulation-based approach which uses Discrete Time Markov Chains and probabilistic model checking to accommodate a diverse set of parameter range distributions. The number of simulation runs automatically regulates to the desired significance level and reports the desired percentiles of the values which ultimately characterises a specific quality attribute of the system. We include a case study which illustrates the flexibility of this approach using the evaluation of several probabilistic properties.",
        "keywords": [
            "Software architecture evaluation",
            "Parameter uncertainty",
            "Probabilistic quality models",
            "Monte-Carlo simulation"
        ],
        "authors": [
            "Indika Meedeniya",
            "Irene Moser",
            "Aldeida Aleti",
            "Lars Grunske"
        ],
        "file_path": "data/sosym-all/s10270-012-0277-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Colouring: execution, debug and analysis of QVT-relations transformations through coloured Petri nets",
        "submission-date": "2010/11",
        "publication-date": "2012/11",
        "abstract": "QVT is the standard language sponsored by the OMG to specify model-to-model transformations. It includes three different languages, being QVT-relations (QVT-R) the one with higher-level of abstraction. Unfortunately, there is scarce tool support for it nowadays, with incompatibilities and disagreements between the few tools implementing it, and lacking support for the analysis and veriﬁcation of transformations. Part of this situation is due to the fact that the standard provides only a semi-formal semantics for QVT-R. In order to alleviate this situation, this paper provides a semantics for QVT-R through its compilation into coloured Petri nets. The theory of coloured Petri nets provides useful techniques to analyse transformations (e.g. detecting relation conﬂicts, or checking whether certain structures are generated or not in the target model) as well as to determine their conﬂuence and termination given a starting model. Our semantics is ﬂexible enough to permit the use of QVT-R speciﬁcations not only for transformation and check-only scenarios, but also for model matching and model comparison, not covered in the original standard. As a proof of concept, we report on the use of CPNTools for the execution, debugging, veriﬁcation and validation of transformations, and on a tool chain (named Colouring) to transform QVT-R speciﬁcations and their input models into the input format of CPNTools, as well as to export and visualize the transformation results back as models.",
        "keywords": [
            "Model-driven engineering",
            "Model-to-model transformations",
            "QVT-relations",
            "Coloured Petri nets",
            "Validation and veriﬁcation"
        ],
        "authors": [
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-012-0292-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "QVT-relations (QVT-R)"
        }
    },
    {
        "title": "Localized model transformations for building large-scale transformations",
        "submission-date": "2011/11",
        "publication-date": "2013/09",
        "abstract": "Model-driven engineering (MDE) exploits well-\ndeﬁned, tool-supported modelling languages and operations\napplied to models created using these languages. Model\ntransformation is a critical part of the use of MDE. It has\nbeen argued that transformations must be engineered sys-\ntematically, particularly when the languages to which they\nare applied are large and complicated—e.g., UML 2.x and\nproﬁles such as MARTE—and when the transformation logic\nitself is complex. We present an approach to designing large\nmodel transformations for large languages, based on the\nprinciple of separation of concerns. Speciﬁcally, we deﬁne\na notion of localized transformations that are restricted to\napply to a subset of a modelling language; a composition of\nlocalized transformations is then used to satisfy particular\nMDE objectives, such as the design of very large transfor-\nmations. We illustrate the use of localized transformations\nin a concrete example applied to large transformations for\nsystem-on-chip co-design.",
        "keywords": [
            "Model transformation",
            "Reusable\ntransformation",
            "Transformation chaining"
        ],
        "authors": [
            "Anne Etien",
            "Alexis Muller",
            "Thomas Legrand",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-013-0379-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting domain-speciﬁc model patterns with metamodeling",
        "submission-date": "2007/04",
        "publication-date": "2009/03",
        "abstract": "Metamodeling is a widely applied technique in the ﬁeld of graphical languages to create highly conﬁgurable modeling environments. These environments support the rapid development of domain-speciﬁc modeling languages (DSMLs). Design patterns are efﬁcient solutions for recurring problems. With the proliferation of DSMLs, there is a need for domain-speciﬁc design patterns to offer solutions to problems recurring in different domains. The aim of this paper is to provide theoretical and practical foundations to support domain-speciﬁc model patterns in metamodeling environments. In order to support the treatment of premature model parts, we weaken the instantiation relationship. We provide constructs relaxing the instantiation rules, and we show that these constructs are appropriate and sufﬁcient to express patterns. We provide the necessary modiﬁcations in metamodeling tools for supporting patterns. With the contributed results, a well-founded domain-speciﬁc model pattern support can be realized in metamodeling tools.",
        "keywords": [],
        "authors": [
            "Tihamér Levendovszky",
            "László Lengyel",
            "Tamás Mészáros"
        ],
        "file_path": "data/sosym-all/s10270-009-0118-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automatic data collection for enterprise architecture models",
        "submission-date": "2011/07",
        "publication-date": "2012/06",
        "abstract": "Enterprise Architecture (EA) is an approach used\nto provide decision support based on organization-wide mod-\nmodels is, however, cumbersome as\nmultiple aspects of an organization need to be considered,\nmaking manual efforts time-consuming, and error prone.\nThus, the EA approach would be significantly more prom-\nising if the data used when creating the models could be\ncollected automatically—a topic not yet properly addressed\nby either academia or industry. This paper proposes network\nscanning for automatic data collection and uses an exist-\ning software tool for generating EA models (ArchiMate is\nemployed as an example) based on the IT infrastructure of\nEnterprises. While some manual effort is required to make\nthe models fully useful to many practical scenarios (e.g., to\ndetail the actual services provided by IT components), empir-\nical results show that the methodology is accurate and (in its\ndefault state) require little effort to carry out.",
        "keywords": [
            "Enterprise architecture",
            "Automatic data collection",
            "Network scanning"
        ],
        "authors": [
            "Hannes Holm",
            "Markus Buschle",
            "Robert Lagerström",
            "Mathias Ekstedt"
        ],
        "file_path": "data/sosym-all/s10270-012-0252-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Virtual network embedding: ensuring correctness and optimality by construction using model transformation and integer linear programming techniques",
        "submission-date": "2020/03",
        "publication-date": "2021/01",
        "abstract": "Virtualization technology allows service providers to operate data centers in a cost-effective and scalable manner. The data\ncenter network (substrate network) and the applications executed in the data center (virtual networks) are often modeled as\ngraphs. The nodes of the graphs represent (physical or virtual) servers and switches, and the edges represent communication\nlinks. Nodes and links are annotated with the provided and required resources (e.g., memory and bandwidth). The NP-hard\nvirtual network embedding (VNE) problem deals with the embedding of a set of virtual networks to the substrate network\nsuch that (i) all (resource) constraints of the substrate network are fulﬁlled, and (ii) an objective is optimized (e.g., minimizing\nthe communication costs). The existing two-step highly customizable model-driven virtual network embedding (MdVNE)\napproach combines model transformation (MT) and integer linear programming (ILP) techniques to solve the VNE problem\nbased on a declarative speciﬁcation. MdVNE generates element mapping candidates from an MT speciﬁcation and identiﬁes\nan optimal and correct embeddings using an ILP solver. In the past, developers created the MT and ILP speciﬁcations manually\nand needed to ensure carefully that both are compatible and respect the problem description. In this article, we present a novel\nconstruction methodology for synthesizing the MT and ILP speciﬁcation from a given declarative model-based VNE problem\ndescription. This problem description consists of a metamodel for substrate and virtual networks, additional OCL constraints,\nand an objective function that assigns costs to a given model. This methodology ensures that the derived embeddings are\ncorrect w.r.t. the metamodel and the OCL constraints, and optimal w.r.t. the optimization goal. The novel model-based VNE\nspeciﬁcation is applicable to different network domains, environments, and constraints. Thus, the construction methodology\nallows to automate most of the steps to realize a correct and optimal VNE algorithm compared to a hand-crafted VNE\nimplementation. Furthermore, the simulative evaluation conﬁrms that using MT techniques reduces the time for solving the\nVNE problem considerably in comparison with a purely ILP-based approach.",
        "keywords": [
            "Data center",
            "Virtual network embedding",
            "Model-driven development",
            "Integer linear programming",
            "Model\ntransformation",
            "Graph transformation",
            "Triple-graph grammar",
            "Object Constraint Language"
        ],
        "authors": [
            "Stefan Tomaszek",
            "Roland Speith",
            "Andy Schürr"
        ],
        "file_path": "data/sosym-all/s10270-020-00852-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multi-purpose, multi-level feature modeling of large-scale industrial software systems",
        "submission-date": "2016/01",
        "publication-date": "2016/10",
        "abstract": "Feature models are frequently used to capture the knowledge about conﬁgurable software systems and product lines. However, feature modeling of large-scale systems is challenging as models are needed for diverse pur-poses. For instance, feature models can be used to reﬂect the perspectives of product management, technical solution architecture, or product conﬁguration. Furthermore, mod-els are required at different levels of granularity. Although numerous approaches and tools are available, it remains hard to deﬁne the purpose, scope, and granularity of feature mod-els. This paper ﬁrst reports results and experiences of an exploratory case study on developing feature models for two large-scale industrial automation software systems. We report results on the characteristics and modularity of the fea-ture models, including metrics about model dependencies. Based on the ﬁndings from the study, we developed FORCE, a modeling language, and tool environment that extends an existing feature modeling approach to support models for dif-ferent purposes and at multiple levels, including mappings to the code base. We demonstrate the expressiveness and exten-sibility of our approach by applying it to the well-known Pick and Place Unit example and an injection molding sub-system of an industrial product line. We further show how our approach supports consistency between different feature models. Our results and experiences show that consider-ing the purpose and level of features is useful for modeling large-scale systems and that modeling dependencies between feature models is essential for developing a system-wide per-spective.",
        "keywords": [
            "Feature modeling",
            "Large-scale software systems",
            "Case study"
        ],
        "authors": [
            "Daniela Rabiser",
            "Herbert Prähofer",
            "Paul Grünbacher",
            "Michael Petruzelka",
            "Klaus Eder",
            "Florian Angerer",
            "Mario Kromoser",
            "Andreas Grimmer"
        ],
        "file_path": "data/sosym-all/s10270-016-0564-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Security modeling for service-oriented systems using security pattern reﬁnement approach",
        "submission-date": "2011/09",
        "publication-date": "2012/08",
        "abstract": "Security is one of the critical aspects of current systems, which are based on loosely coupled and technology-agnostic service-oriented architectures (SOA). Though SOA is the driving force for enterprises to open their ends for global business collaborations, nevertheless it evolves many challenges for modeling and enforcing security. One of the main problems for designing secure systems is the lack of consistent frameworks and methodologies for modeling security concerns. Traditional approaches consider security at the end of system development, which evolves ﬂexible and un-conﬁgurable systems, which are too difﬁcult to maintain and manage. The other major problem with current approaches is that they assume pre-deﬁned and hard-coded security patterns and mechanisms for secure system design. Whereas, the evolving SOA systems require conﬁgurable security to realize different security patterns and secu-rity policies in a variety of business scenarios. To solve these problems, it is necessary to model security concerns from the beginning of system modeling in a platform-independent way. This paper proposes a pattern reﬁnement approach for security modeling to achieve conﬁgurable and declarative security, based on the principles of abstraction, reﬁnement, separation-of-concerns and maintainability to achieve ﬂexibleconﬁgurationsofSOAsecurity.Intheproposedapproach, a Domain Expert deﬁnes abstract policies using common security vocabulary and a Security Expert models security with patterns and reﬁnes them for a target architecture in successive systematic reﬁnements. Furthermore, it facilitates thetransformationofabstractsecuritymodelsintoexecutable security policies for the target platforms.",
        "keywords": [
            "Model-driven security",
            "Security patterns",
            "SOA security",
            "Model transformation"
        ],
        "authors": [
            "Mukhtiar Memon",
            "Gordhan D. Menghwar",
            "Mansoor H. Depar",
            "Akhtar A. Jalbani",
            "Waqar M. Mashwani"
        ],
        "file_path": "data/sosym-all/s10270-012-0268-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An architecture framework for enterprise IT service availability analysis",
        "submission-date": "2012/01",
        "publication-date": "2013/01",
        "abstract": "This paper presents an integrated enterprise architecture framework for qualitative and quantitative modeling and assessment of enterprise IT service availability. While most previous work has either focused on formal availability methods such as fault trees or qualitative methods such as maturity models, this framework offers a combination. First, a modeling and assessment framework is described. In addition to metamodel classes, relationships and attributes suitable for availability modeling, the framework also features a formal computational model written in a probabilistic version of the object constraint language. The model is based on 14 systemic factors impacting service availability and also accounts for the structural features of the service architecture. Second, the framework is empirically tested in nine enterprise information system case studies. Based on an initial availability baseline and the annual evolution of the 14 factors of the model, annual availability predictions are made and compared with the actual outcomes as reported in SLA reports and system logs. The practical usefulness of the method is discussed based on the outcomes of a workshop conducted with the participating enterprises, and some directions for future research are offered.",
        "keywords": [
            "Systems availability",
            "Service availability",
            "Downtime",
            "Noisy-OR",
            "System quality analysis",
            "Enterprise Architecture",
            "ArchiMate",
            "Metamodel",
            "OCL"
        ],
        "authors": [
            "Ulrik Franke",
            "Pontus Johnson",
            "Johan König"
        ],
        "file_path": "data/sosym-all/s10270-012-0307-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Language-independent look-ahead for checking multi-perspective declarative process models",
        "submission-date": "2019/12",
        "publication-date": "2021/01",
        "abstract": "Declarative process modelling languages focus on describing a process by restrictions over the behaviour, which must be satisﬁedthroughoutthewholeprocessexecution.Hence,theyarewellsuitedformodellingknowledge-intensiveprocesseswith\nmany decision points. However, such models can be hard to read and understand, which affect the modelling and maintenance\nof the process models tremendously as well as their execution. When executing such declarative (multi-perspective) process\nmodels, it may happen that the execution of activities or the change of data values may result in the non-executability of\ncrucial activities. Hence, it would be beneﬁcial to know all consequences of decisions to give recommendations to the process\nparticipants. A look-ahead attempts to predict the effects of executing an activity towards possible consequences within an a\npriori deﬁned time window. The prediction is based on the current state of the process execution, the intended next event and\nthe underlying process model. While execution engines for single-perspective imperative process models already implement\nsuch functionality, execution approaches, for multi-perspective declarative process models that involve constraints on data\nand resources, are less mature. In this paper, we introduce a simulation-based look-ahead approach for multi-perspective\ndeclarative process models. This approach transforms the problem of a context-aware process simulation into a SAT problem,\nby translating a declarative multi-perspective process model and the current state of a process execution into a speciﬁcation of\nthe logic language Alloy. Via a SAT solver, process trajectories are generated that either satisfy or violate this speciﬁcation.\nThe simulated process trajectories are used to derive consequences and effects of certain decisions at any time of process\nexecution. We evaluate our approach by means of three examples and give some advice for further optimizations.",
        "keywords": [
            "Declarative process models",
            "Multi-perspective",
            "Look-ahead",
            "Model checking",
            "Predictive business process monitoring",
            "SAT solving"
        ],
        "authors": [
            "Martin Käppel",
            "Lars Ackermann",
            "Stefan Schönig",
            "Stefan Jablonski"
        ],
        "file_path": "data/sosym-all/s10270-020-00857-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extending organizational capabilities with Open Data to support sustainable and dynamic business ecosystems",
        "submission-date": "2018/10",
        "publication-date": "2019/09",
        "abstract": "Open Data (OD) is data available in a machine-readable format and without restrictions on the permissions for using or distributing it. OD may include textual artifacts, images, maps, video content, and other. The data can be published and maintained by different entities, both public and private. Despite its power to distribute knowledge freely and availability of a large number of datasets, OD initiatives face important challenges related to its widespread take up. More specifically, OD provisioning is based on a unidirectional linking from OD providers to OD users without considering requirements and preferences of the users. The OD users also lack metadata, and they need to develop specific technical solutions for provid-ing a continuous OD flow and processing, which is particularly difficult when real-time OD are to be used. In this paper, we propose solving these challenges by envisioning a business ecosystem for OD. It is network-based, federated, and supports interplay between OD provisioning and knowledge management. As a methodological solution, we have applied the capability-driven development approach, which allows modeling of OD processing ecosystems, facilitates knowledge exchange about OD usage among members of the ecosystem, and supports configuring information systems for OD processing. The proposal is explicated with a theoretical study of its usability for the service of road maintenance in varying conditions.",
        "keywords": [
            "Open Data",
            "Capability",
            "Context",
            "Requirements",
            "CDD"
        ],
        "authors": [
            "Jānis Kampars",
            "Jelena Zdravkovic",
            "Janis Stirna",
            "Jānis Grabis"
        ],
        "file_path": "data/sosym-all/s10270-019-00756-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Interactive log-delta analysis using multi-range ﬁltering",
        "submission-date": "2020/11",
        "publication-date": "2021/09",
        "abstract": "Process mining is a family of analytical techniques that extract insights from an event log and present them to an analyst. A key analysis task is to understand the distinctive features of different variants of the process and their impact on process performance. Techniques for log-delta analysis (or variant analysis) put a strong emphasis on automatically extracting explanations for differences between variants. A weakness of them is, however, their limited support for interactively exploring the dividing line between typical and atypical behavior. In this paper, we address this research gap by developing and evaluating an interactive technique for log-delta analysis, which we call InterLog. This technique is developed based on the idea that the analyst can interactively deﬁne ﬁlter ranges and that these ﬁlters are used to partition the log L into sub-logs L1 for the selected cases and L2 for the deselected cases. In this way, the analyst can step-by-step explore the log and manually separate the typical behavior from the atypical. We prototypically implement InterLog and demonstrate its application for a real-world event log. Furthermore, we evaluate it in a preliminary design study with process mining experts for usefulness and ease of use.",
        "keywords": [
            "Process mining",
            "Log-delta analysis",
            "Variant analysis",
            "Multi-range ﬁlter",
            "Event logs",
            "Event sequence data"
        ],
        "authors": [
            "Maxim Vidgof",
            "Djordje Djurica",
            "Saimir Bala",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-021-00902-0.pdf",
        "classification": {
            "is_transformation_paper": false,
            "is_transformation_language": false,
            "language": "None"
        }
    },
    {
        "title": "On the use of domain knowledge for process model repair",
        "submission-date": "2021/12",
        "publication-date": "2022/12",
        "abstract": "Processmodelsareimportantforsupportingorganizationsindocumenting,understandingandmonitoringtheirbusiness.When\nthese process models become outdated, they need to be revised to accurately describe the new status quo of the processes\nin the organization. Process model repair techniques help at automatically revising the existing model from behavior traced\nin event logs. So far, such techniques have focused on identifying which parts of the model to change and how to change\nthem, but they do not use knowledge from practitioners to inform the revision. As a consequence, fragments of the model\nmay change in a way that deﬁes existing regulations or represents outdated information that was wrongly considered from the\nevent log. This paper uses concepts from theory revision to provide formal foundations for process model repair that exploits\ndomain knowledge. Speciﬁcally, it conceptualizes (1) what are unchangeable fragments in the model and (2) the role that\nvarious traces in the event log should play when it comes to model repair. A scenario of use is presented that demonstrates\nthe beneﬁts of this conceptualization. The current state of existing process model repair techniques is compared against the\nproposed concepts. The results show that only two existing techniques partially consider the concepts presented in this paper\nfor model repair.",
        "keywords": [
            "Process model repair",
            "Process mining",
            "Concept drift",
            "Theory revision"
        ],
        "authors": [
            "Kate Revoredo"
        ],
        "file_path": "data/sosym-all/s10270-022-01067-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SQME: a framework for modeling and evaluation of software architecture quality attributes",
        "submission-date": "2017/07",
        "publication-date": "2018/05",
        "abstract": "Designing a software architecture that satisﬁes all quality requirements is a difﬁcult task. To determine whether the requirements are achieved, it is necessary to quantitatively evaluate quality attributes on the architecture model. A good evaluation process should have proper answers for these questions: (1) how to feedback the evaluation results to the architecture model (i.e., improve the architecture based on the evaluation results), (2) how to analyze uncertainties in calculations, and (3) how to handle conﬂicts that may exist between the quality preferences of stakeholders. In this paper, we introduce SQME as a framework for automatic evaluation of software architecture models. The framework uses evolutionary algorithms for architecture improvement, evidence theory for uncertainty handling, and EV/TOPSIS for making trade-off decisions. To validate the applicability of the framework, a case study is performed, and a software tool is developed to support the evaluation process.",
        "keywords": [
            "Software architecture",
            "Software quality attributes",
            "Evolutionary algorithms",
            "Evidence theory",
            "EV/TOPSIS"
        ],
        "authors": [
            "Ali Sedaghatbaf",
            "Mohammad Abdollahi Azgomi"
        ],
        "file_path": "data/sosym-all/s10270-018-0684-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models: the fourth dimension of computer science",
        "submission-date": "2021/10",
        "publication-date": "2021/12",
        "abstract": "Models are a universal instrument in science, technology, and daily life. They function as instruments in almost every scenario. Any human activity can be (and is) supported by models, e.g. reason, explain, design, act, predict, explore, communicate, collaborate, interact, orient, direct, guide, socialises, perceive, reﬂect, develop, making sense, teach, learn, imagine, etc. This universal suitability is also the basis for a wide use of models and modelling in Computer Science and Engineering. We claim that models form the fourth dimension in Computer Science. This paper sketches and systematises the main ingredients of the study model and modelling.",
        "keywords": [
            "Model",
            "Study of models and modelling",
            "More"
        ],
        "authors": [
            "Bernhard Thalheim"
        ],
        "file_path": "data/sosym-all/s10270-021-00954-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Does aspect-oriented modeling help improve the readability of UML state machines?",
        "submission-date": "2011/11",
        "publication-date": "2012/11",
        "abstract": "Aspect-oriented modeling (AOM) is a relatively recent and very active ﬁeld of research, whose application has, however, been limited in practice. AOM is assumed to yield several potential beneﬁts such as enhanced modular-ization, easier evolution, increased reusability, and improved readability of models, as well as reduced modeling effort. However, credible, solid empirical evidence of such beneﬁts is lacking. We evaluate the “readability” of state machines when modeling crosscutting behavior using AOM and more speciﬁcally AspectSM, a recently published UML pro-ﬁle. This proﬁle extends the UML state machine notation with mechanisms to deﬁne aspects using state machines. Readability is indirectly measured through defect identiﬁ-cation and ﬁxing rates in state machines, and the scores obtained when answering a comprehension questionnaire about the system behavior. With AspectSM, crosscutting behavior is modeled using so-called “aspect state machines”. Their readability is compared with that of system state machines directly modeling crosscutting and standard behav-ior together. An initial controlled experiment and a much larger replication were conducted with trained graduate students, in two different institutions and countries, to achieve the above objective. We use two baselines of comparisons—standard UML state machines without hierarchical features (ﬂat state machines) and standard state machines with hierarchical/concurrent features (hierarchical S. Ali (B) · T. Yue · L. C. Briand Certus Software V&V Center, Simula Research Laboratory, P.O. Box 134, 1325 Lysaker, Norway e-mail: shaukat@simula.no T. Yue e-mail: tao@simula.no L. C. Briand SnT Centre, University of Luxembourg, Luxembourg, Luxembourg e-mail: lionel.briand@uni.lu state machines). The results showed that defect identiﬁcation and ﬁxing rates are signiﬁcantly better with AspectSM than with both ﬂat and hierarchical state machines. How-ever, in terms of comprehension scores and inspection effort, no signiﬁcant difference was observed between any of the approaches. Results of the experiments suggest that one should use, when possible, aspect state machines along with hierarchical and/or concurrent features of UML state machines to model crosscutting behaviors.",
        "keywords": [
            "Aspect-oriented modeling",
            "UML state machines",
            "Controlled experiment",
            "Defect identiﬁcation and ﬁxing",
            "Comprehension"
        ],
        "authors": [
            "Shaukat Ali",
            "Tao Yue",
            "Lionel C. Briand"
        ],
        "file_path": "data/sosym-all/s10270-012-0293-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluation of a machine learning classiﬁer for metamodels",
        "submission-date": "2020/05",
        "publication-date": "2021/09",
        "abstract": "Modeling is a ubiquitous activity in the process of software development. In recent years, such an activity has reached a high\ndegree of intricacy, guided by the heterogeneity of the components, data sources, and tasks. The democratized use of models has\nled to the necessity for suitable machinery for mining modeling repositories. Among others, the classiﬁcation of metamodels\nintoindependentcategoriesfacilitatespersonalizedsearchesbyboostingthevisibilityofmetamodels.Nevertheless,themanual\nclassiﬁcation of metamodels is not only a tedious but also an error-prone task. According to our observation, misclassiﬁcation\nis the norm which leads to a reduction in reachability as well as reusability of metamodels. Handling such complexity requires\nsuitable tooling to leverage raw data into practical knowledge that can help modelers with their daily tasks. In our previous\nwork, we proposed AURORA as a machine learning classiﬁer for metamodel repositories. In this paper, we present a thorough\nevaluation of the system by taking into consideration different settings as well as evaluation metrics. More importantly, we\nimprove the original AURORA tool by changing its internal design. Experimental results demonstrate that the proposed\namendment is beneﬁcial to the classiﬁcation of metamodels. We also compared our approach with two baseline algorithms,\nnamely gradient boosted decision tree and support vector machines. Eventually, we see that AURORA outperforms the\nbaselines with respect to various quality metrics.",
        "keywords": [
            "Model-driven engineering",
            "Machine learning",
            "Neural networks",
            "GBDT",
            "SVM"
        ],
        "authors": [
            "Phuong T. Nguyen",
            "Juri Di Rocco",
            "Ludovico Iovino",
            "Davide Di Ruscio",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-021-00913-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Standards in software development and modeling",
        "submission-date": "2025/08",
        "publication-date": "2025/08",
        "abstract": "There are important standardization bodies that actually create very good and widely used standards in engineering and development. This is prominent in other engineering domains, but less common in computer science. We may speculate about the reasons, but it may be that computer science is relatively young, and therefore, techniques and methods evolve frequently, and standards may hinder this form of innovation. A second reason may be that in computer science, large companies are developing the de facto standardsthatarenotnecessarilybecomingformalstandards. But in computer science, standards also ensure compatibility, interoperability, reliability, security, reusability, and potentially many other good properties across services, applications, systems, and technologies. And we all know some key categories and examples of relevant standards, such as programming language standards (e.g., ISO/IEC 9899—for C, Java Community Process (JCP) spec—for Java, ECMA-262/ISO/IEC 16262—for JavaScript, HTTP/HTTPS (RFC 9110) protocol—for web communication, and RFC 8259—for JSON). The most relevant standards for Software & Systems Engineering are UML (ﬁrst by the OMG and later by ISO/IEC 19505), IEEE 830 / ISO/IEC/IEEE 29148—for Software Requirements Speciﬁcation, and the newly emerging standards around the digital twin technologies stack that are in discussion by the Digital Twin Consortium (DTC) and the Industrial Digital Twin Association (IDTA).",
        "keywords": [],
        "authors": [
            "Marsha Chechik",
            "Benoit Combemale",
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-025-01312-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A compositional semantics of UML-RSDS",
        "submission-date": "2006/06",
        "publication-date": "2007/08",
        "abstract": "This paper provides a semantics for the UML- RSDS (Reactive System Development Support) subset of UML, using the real-time action logic (RAL) formalism. We show how this semantics can be used to resolve some ambiguities and omissions in UML semantics, and to support reasoningaboutspeciﬁcationsusingtheBformalmethodand tools. We use ‘semantic proﬁles’ to provide precise semantics for different semantic variation points of UML. We also show how RAL can be used to give a semantics to notations for real-time speciﬁcation in UML. Unlike other approaches to UML semantics, which concentrate on the class diagram notation, our semantic representation has behaviour as a central element, and can be used to deﬁne semantics for use cases, state machines and interactions, in addition to class diagrams.",
        "keywords": [
            "UML semantics",
            "UML-RSDS",
            "Model transformations"
        ],
        "authors": [
            "K. Lano"
        ],
        "file_path": "data/sosym-all/s10270-007-0064-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for the special section on SEFM 2020 and 2021",
        "submission-date": "2024/03",
        "publication-date": "Not found",
        "abstract": "The main objective of the International Conference on Software Engineering and Formal Methods (SEFM) is to bring together practitioners and researchers from academia, industry, and government, to advance the state of the art in formal methods, to help in their large-scale application in the software industry, and to encourage their integration with other practical software engineering methods. This special section consists of a selection of papers presented at SEFM 2020 and 2021, the 18th and 19th editions of SEFM, which have been held virtually during the COVID pandemic.",
        "keywords": [],
        "authors": [
            "Frank S. de Boer",
            "Antonio Cerone"
        ],
        "file_path": "data/sosym-all/s10270-024-01168-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Why it is so hard to use models in software development: observations",
        "submission-date": "2013/10",
        "publication-date": "2013/10",
        "abstract": "In a previous editorial, we asked what modeling contributes to the development process. We concluded that models are not so much of value for themselves, but exist to improve certain properties of the product, such as quality or maintainability, or of the process, such as cost-efﬁciency and predictability. In this editorial, we would like to report on speciﬁc ﬁnd-ings that are based on the reported and own experience with some concrete modeling tools and frameworks of different types (without naming them) and draw some conclusions for further tool improvement.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-013-0383-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Fabricatable axis: an approach for modelling customized fabrication machines",
        "submission-date": "2021/03",
        "publication-date": "2022/05",
        "abstract": "Digital fabrication tools such as 3D printers, computer-numerically controlled (CNC) milling machines, and laser cutters are becoming increasingly available, ranging from consumer to industrial versions. Recent studies have shown that users, ranging from researchers, to industry professionals, to hobbyists, are interested in modifying and changing the inherit workﬂows these tools provide. As an answer to this, these users are increasingly modifying and customizing their machines by changing the work envelope, adding different end-effectors, and creating their own fabrication workﬂows in software. However, customizing, modifying and creating digital fabrication machines and the workﬂows they provide require extensive knowledge within multiple different engineering domains and is non-trivial. In this article we present a model-driven approach that enables users to expand their digital fabrication scope by providing a high-level tool that facilitates the customization of fabrication tools. We present The Farbicatable Axis, a model that enables users to create customized linear actuators. The model takes high-level input parameters such as length and gearing-parameters, and outputs a CAD model of a linear motion axis consisting of fabricatable parts. We then present how instances of the Fabricatable Axis can be combined and used to design and implement Fabricatable Machines.",
        "keywords": [
            "Model driven engineering",
            "Digital fabrication",
            "Machine building",
            "CNC",
            "CAD/CAM"
        ],
        "authors": [
            "Frikk H. Fossdal",
            "Rogardt Heldal",
            "Jens Dyvik",
            "Adrian Rutle"
        ],
        "file_path": "data/sosym-all/s10270-022-01007-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introduction to the SoSyM theme issue on models and evolution",
        "submission-date": "2013/04",
        "publication-date": "2013/04",
        "abstract": "Software artifacts are subject to many sources of evolutionary pressure, which range from technical changes due to rapidly evolving technology platforms, to modiﬁcations caused by new requirements and insights emerging from the business domain. These modiﬁcations include changes at all levels, from requirements through architecture and design, to source code, documentation and test suites, and might affect any kinds of models. Therefore, adopting models, techniques, and tools for coping with and managing changes that accompany the evolution of software models is an essential discipline of Software Engineering.",
        "keywords": [],
        "authors": [
            "Dalila Tamzalit",
            "Bernhard Schätz",
            "Alfonso Pierantonio",
            "Dirk Deridder"
        ],
        "file_path": "data/sosym-all/s10270-013-0338-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How do I find reusable models?",
        "submission-date": "2022/03",
        "publication-date": "2023/04",
        "abstract": "Models play a major role in model-based development and serve as the main artifacts that stakeholders aim to achieve. As it is difficult to develop good-quality models, repositories of models start emerging for reuse purposes. Yet, these repositories face several challenges, such as model representation, scalability, heterogeneity, and how to search for models. In this paper, we aim to address the challenge of querying model repositories by proposing a generic search framework that looks for models that match the intention of the user. The framework is based on a greedy search approach using a similarity function that considers type similarity, structure similarity, and label similarity. We evaluate the framework’s efficiency on different model types: UML class diagrams, Human Know-How, and ME maps. We further compare it with existing alternatives. The evaluation indicates that the framework achieved high performance within a bounded time, and the framework can be adapted to different modeling languages for searching related, reusable models.",
        "keywords": [
            "Model-based development",
            "Search",
            "Greedy algorithm",
            "Similarity",
            "Model repositories"
        ],
        "authors": [
            "Maxim Bragilovski",
            "Roni Stern",
            "Arnon Sturm"
        ],
        "file_path": "data/sosym-all/s10270-023-01103-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Feedback on our editorials",
        "submission-date": "2007/05",
        "publication-date": "2007/05",
        "abstract": "In our editorials we give our perspective on the current state of the research and practice with respect to the use of models in software and systems development. In some cases we highlight what we consider to be promising new and emerging research directions, and we sometimes give our perspective on problems arising from immature and incorrect use of models. The editorials are written to stimulate discussion and encourage exploration of new areas of research in modeling software-based systems.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-007-0059-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Effective development of automation systems through domain-speciﬁc modeling in a small enterprise context",
        "submission-date": "2010/11",
        "publication-date": "2012/10",
        "abstract": "High development and maintenance costs and a\nhigh error rate are the major problems in the development of\nautomation systems, which are mainly caused by bad com-\nmunication and inefﬁcient reuse methods. To overcome these\nproblems, we propose a more systematic reuse approach.\nThough systematic reuse approaches such as software prod-\nuct lines are appealing, they tend to involve rather burden-\nsome development and management processes. This paper\nfocuses on small enterprises. Since such companies are often\nunable to perform a “big bang” adoption of the software prod-\nuct line, we suggest an incremental, more lightweight process\nto transition from single-system development to software\nproduct line development. Besides the components of the\ntransition process, this paper discusses tool selection, DSL\ntechnology, stakeholder communication support, and busi-\nness considerations. Although based on problems from the\nautomation system domain, we believe the approach may be\ngeneral enough to be applicable in other domains as well. The\napproach has proven successful in two case studies. First, we\napplied it to a research project for the automation of a logis-\ntics lab model, and in the second case (a real-life industry\ncase), we investigated the approaches suitability for ﬁsh farm\nautomation systems. Several metrics were collected through-\nout the evolution of each case, and this paper presents the\ndata for single system development, clone&own and soft-\nware product line development. The results and observable\neffects are compared, discussed, and ﬁnally summarized in\na list of lessons learned.",
        "keywords": [
            "Domain-speciﬁc modeling",
            "Small enterprise\ncost model",
            "Automation system",
            "Software product line",
            "System development process"
        ],
        "authors": [
            "Andrea Leitner",
            "Christopher Preschern",
            "Christian Kreiner"
        ],
        "file_path": "data/sosym-all/s10270-012-0289-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Teaching conceptual modelling in the age of LLMs: shifting from model creation to model evaluation skills",
        "submission-date": "2025/01",
        "publication-date": "Not found",
        "abstract": "When LLMs are used to assist the development of artefact, it may become difﬁcult to distinguish the genuine contribution by the human modeller from the parts that were generated by the LLM. This is not different for conceptual modelling (CM): an LLM may be asked to generate a CM for a given description. To assess a student’s modelling competences, it’s therefore not sufﬁcient to simply assess the model as an output of their work (i.e. the quality of the created model, along different dimensions). This raises two important questions: How should we assess the modelling capabilities of a student, given that a (large) portion of the model may have been generated by an LLM? And: What skills are needed to create a model? Which ones are essentially human? What if part of these are replaced by LLMs? We posit that when teaching CM, instead of focussing on model creation, we rather need to assess a student’s capability of evaluating, reﬁning and improving models according to requirements, technical constraints, etc. This iterative process of evaluating, reﬁning and improving the model is in line with the utility of modelling as instrument of communication instrument, global design and design exploration and constitutes the essence of modelling skill. We thus need to focus on the capability of critically evaluating a model rather than on model creation, and model creation capabilities will naturally follow from these evaluation capabilities. It is our point that this vision will reinforce the beneﬁts of using LLMs for an improved CM teaching practice.",
        "keywords": [
            "Conceptual modelling",
            "Conceptual modelling education",
            "LLMs",
            "AI-assisted modelling"
        ],
        "authors": [
            "Monique Snoeck",
            "Oscar Pastor"
        ],
        "file_path": "data/sosym-all/s10270-025-01307-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On submodels and submetamodels with their relation\nA uniform formalization through inclusion properties",
        "submission-date": "2015/07",
        "publication-date": "2016/06",
        "abstract": "Model-driven engineering (MDE) recognized\nsoftware models as ﬁrst-class objects with their own relation-\nships and operations, up to constitute full structured model\nspaces. We focus on inclusion capacities through the con-\ncepts of submodel and submetamodel which contribute a\nlot to the structuring effort. Submodels and submetamod-\nels underlie many MDE practices which require their precise\ncharacterization for plain control. A typical application is\nmodel management as offered by model repositories. On the\nbasis of results on submodel inclusion we stated in a preced-\ning paper, we concentrate on the special form of submodels\nwhich are submetamodels and their speciﬁc role in model\nspace structuring. Pointing out that relating submodels and\nsubmetamodels is two ways, their respective inclusion hierar-\nchies will be systematically characterized and symmetrically\ncomparedunderthelogicalrelationshipsofmetamodelmem-\nbership and model well-formedness. As a major result, it will\nbe shown that submodel well-formedness w.r.t submetamod-\nels closely relates to submodel invariance (a property which\nguarantees transitive structure preservation) applied at both\nlevels. The uniform formalization offers algebraic grounding\nto better comprehension and control of model spaces which\nunderlie MDE activities. At a much more practical level,\nreusable technology which takes advantage of established\nresults will be offered.",
        "keywords": [
            "Submodel",
            "Submetamodel",
            "Model space",
            "Set-theoretic formalization",
            "Model repository"
        ],
        "authors": [
            "Bernard Carré\nGilles Vanwormhoudt\nOlivier Caron"
        ],
        "file_path": "data/sosym-all/s10270-016-0540-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for the special section on MODELS 2014",
        "submission-date": "2016/08",
        "publication-date": "2016/09",
        "abstract": "The MODELS conference series is the premier venue for model-based software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. This special section presents the six articles that resulted from an invitation to authors of the best papers at the MODELS 2014 conference to submit revised and extended versions of their papers for publication in SoSyM.",
        "keywords": [],
        "authors": [
            "Juergen Dingel",
            "Wolfram Schulte"
        ],
        "file_path": "data/sosym-all/s10270-016-0561-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Unleashing textual descriptions of business processes",
        "submission-date": "2020/06",
        "publication-date": "2021/05",
        "abstract": "Textual descriptions of processes are ubiquitous in organizations, so that documentation of the important processes can be accessible to anyone involved. Unfortunately, the value of this rich data source is hampered by the challenge of analyzing unstructured information. In this paper we propose a framework to overcome the current limitations on dealing with textual descriptions of processes. This framework considers extraction and analysis and connects to process mining via simulation. The framework is grounded in the notion of annotated textual descriptions of processes, which represents a middle-ground between formalization and accessibility, and which accounts for different modeling styles, ranging from purely imperative to purely declarative. The contributions of this paper are implemented in several tools, and case studies are highlighted.",
        "keywords": [
            "Business process management",
            "Natural language processing",
            "Temporal logics",
            "Process mining",
            "Model checking",
            "Simulation"
        ],
        "authors": [
            "Josep Sànchez-Ferreres",
            "Andrea Burattin",
            "Josep Carmona",
            "Marco Montali",
            "Lluís Padró",
            "Luís Quishpi"
        ],
        "file_path": "data/sosym-all/s10270-021-00886-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for the special section on the 26th international conference on model-driven engineering languages and systems (MODELS 2023)",
        "submission-date": "2025/08",
        "publication-date": "Not found",
        "abstract": "The MODELS conference series is the premier venue for model-driven software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. This special section presents the twelve articles that resulted from the subsequent SoSyM reviewing process, which are extended versions of best papers at MODELS 2023.",
        "keywords": [],
        "authors": [
            "Antonio Cicchetti",
            "Thomas Kühne",
            "Alfonso Pierantonio",
            "Gabriele Taentzer"
        ],
        "file_path": "data/sosym-all/s10270-025-01322-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Execution trace analysis for a precise understanding of latency violations",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Despite the amount of proposed works for the veriﬁcation of embedded systems, understanding the root cause of violations of requirements in simulation or execution traces is still an open issue, especially when dealing with temporal properties such as latencies. Is the violation due to an unfavorable real-time scheduling, to contentions on buses, to the characteristics of functional algorithms or hardware components? The paper introduces the Precise Latency ANalysis approach (PLAN), a new trace analysis technique whose objective is to classify execution transactions according to their impact on latency. To do so, we rely ﬁrst on a model transformation that builds up a dependency graph from an allocation model, thus including hardware and software aspects of a system model. Then, from this graph and an execution trace, our analysis can highlight how software or hardware elements contributed to the latency violation. The paper ﬁrst formalizes the problem before applying our approach to simulation traces of SysML models. A case study deﬁned in the AQUAS European project illustrates the relevance of our approach. Last, a performance evaluation gives computation times for several models and requirements.",
        "keywords": [
            "Embedded systems",
            "Execution trace analysis",
            "Dependency graph",
            "Model-based systems engineering (MBSE)",
            "Timing analysis",
            "Simulation"
        ],
        "authors": [
            "Maysam Zoor",
            "Ludovic Apvrille",
            "Renaud Pacalet",
            "Sophie Coudert"
        ],
        "file_path": "data/sosym-all/s10270-022-01076-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An integrated metamodel-based approach to software model refactoring",
        "submission-date": "2016/06",
        "publication-date": "2017/10",
        "abstract": "Abstract Software refactoring is the process of changing a\nsoftware system in a manner that does not alter its external\nbehavior and yet improving its internal structure. Model-\ndriven architecture and the popularity of the UML enabled\nthe application of refactoring at model level, which was\nearlier applied to software code. In this paper, we propose\na multi-view integrated approach to model-driven refactor-\ning using UML models. We selected a single model from\neach UML view at metamodel level to construct an inte-\ngrated metamodel. We selected class diagram to represent\nthe structural view, sequence diagram to represent the behav-\nioral view and use case diagram to represent the functional\nview. We validated the proposed approach by comparing\nintegrated refactoring approach with refactoring applied to\nmodels individually in terms of quality improvement through\nUML model metrics. Our results indicate that more bad smell\ninstances canbedetectedusingtheintegratedapproachrather\nthan the individual refactoring approach.",
        "keywords": [
            "Refactoring",
            "Metamodel",
            "UML",
            "Model refactoring"
        ],
        "authors": [
            "Mohammed Misbhauddin",
            "Mohammad Alshayeb"
        ],
        "file_path": "data/sosym-all/s10270-017-0628-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The wild-west of modeling (Revisited)",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Modeling is a creative and intellectually focused activity that has a long history and tradition. Ancient Greek philosophers discussed “what is original” and “how to discern the essential nature of a thing” using models. They endeavored to understand the physical world, the biological world, humanity, human medical conditions, and so on by reducing concrete and specific concepts into simpler analogies and rules. One could even argue that the very first models were cave drawings, which show hunting scenes (such as the one on SoSyM’s cover!) to describe the process of hunting successfully for teaching purposes. Cave drawings therefore have a purpose and fulfill the general criteria for being a model. The actual word “modeling” was used already in the twelfth century in Italy, when 1:10 miniature and wooden buildings from churches were called “models” and were used to represent a newly constructed building for stakeholder and developer discussions (and potentially to help raise the money needed for construction) before actually creating the real physical structure.\nModeling is one of the key supports for scientific and engineering disciplines. However, the use of explicit and precisely defined modeling languages is relatively new and has developed primarily in the context of software support for the modeling activity. Interaction with machines has enforced precise syntactic forms; that is, the machine clearly accepts or rejects a model, before any computation is performed or transformation occurs into some other kind of model, executable program, or test infrastructure. A precise semantics requires definition through the behavior of a machine.",
        "keywords": [],
        "authors": [
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-021-00932-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial to theme section on modeling in low-code development platforms",
        "submission-date": "2022/09",
        "publication-date": "2022/09",
        "abstract": "The growing need for secure, trustworthy, and cost-efﬁcient software, as well as recent developments in cloud computing technologies, and the shortage of highly skilled professional software developers have given rise to a new generation of low-code software development platforms, such as Google AppSheet and Microsoft PowerApps. Low-code platforms enable the development and deployment of fully functional applications using mainly visual abstractions and interfaces and requiring little or no procedural code. This makes them accessible to an increasingly digital-native and tech-savvy workforce who can directly and effectively contribute to the software development process, even if they lack a programming background. At the heart of low-code applications are typically models of the structure, the behavior, and the presentation of the application. In addition, low-code application models need to be edited (using graphical and textual interfaces), validated, version-controlled, and eventually transformed or interpreted to deliver user-facing applications. These activities have been of core interest to the modeling community over the last two decades. However, engineering and employing low-code development platforms still encompasses many research topics, including enabling citizen/end-user software development, realizing recommender systems for low-code platforms, interoperability issues between low-code platforms, and scalability issues in low-code development.",
        "keywords": [],
        "authors": [
            "Davide Di Ruscio",
            "Esther Guerra",
            "Massimo Tisi"
        ],
        "file_path": "data/sosym-all/s10270-022-01045-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the theme section on Multi-Paradigm Modeling for Cyber-Physical Systems",
        "submission-date": "2021/03",
        "publication-date": "2021/04",
        "abstract": "This theme section aims to disseminate the latest research results in the area of Multi-Paradigm Modeling for Cyber-Physical Systems (MPM4CPS). MPM has a long tradition within the Model-Driven Engineering community, e.g., several workshops have been held at the MODELS conference for over more than a decade. The MPM4CPS workshop series is a continuation of the successful MPM workshop series with a stronger focus on CPS as especially these systems pose several new challenges on the engineering process and beyond. This theme section covers papers on the foundations and applications of MPM for CPS. In total, we accepted ﬁve submissions for publication in the theme section after a thorough peer-reviewing process.",
        "keywords": [
            "Multi-paradigm modeling",
            "Model-driven engineering",
            "Systems engineering",
            "Cyber-physical systems"
        ],
        "authors": [
            "Eugene Syriani",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-021-00882-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Synchronizing concurrent model updates based on bidirectional transformation",
        "submission-date": "2009/11",
        "publication-date": "2011/01",
        "abstract": "Model-driven software development often involves several related models. When models are updated, the updates need to be propagated across all models to make them consistent. A bidirectional model transformation keeps two models consistent by updating one model in accordance with the other. However, it does not work when the two models are modiﬁed at the same time. In this paper we first examine the requirements for synchronizing concurrent updates. We view a synchronizer for concurrent updates as a function taking the two original models and the two updated models as input, and producing two new models where the updates are synchronized. We argue that the synchronizer should satisfy three properties that we define to ensure a reasonable synchronization behavior. We then propose a new algorithm to wrap any bidirectional transformation into a synchronizer with the help of model difference approaches. We show that synchronizers produced by our algorithm are ensured to satisfy the three properties if the bidirectional transformation satisfies the correctness property and the hippocraticness property. We also show that the history ignorance property contributes to the symmetry of our algorithm. An implementation of our algorithm shows that it worked well in a practical runtime management framework.",
        "keywords": [
            "Model synchronization",
            "Bidirectional transformation",
            "Concurrent updates",
            "Model difference"
        ],
        "authors": [
            "Yingfei Xiong",
            "Hui Song",
            "Zhenjiang Hu",
            "Masato Takeichi"
        ],
        "file_path": "data/sosym-all/s10270-010-0187-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction to: Evaluation of a machine learning classiﬁer for metamodels",
        "submission-date": "2021/11",
        "publication-date": "2021/11",
        "abstract": "Unfortunately, in the original publication, reference 70 was published wrongly as “Rössler, A. M. S., Günnemann, S.: Thingml+: Augmenting model-driven software engineering for the internet of things with machine learning. In: R. Hebig and T. Berger, editors, Proceedings of Workshops co-located with MODELS 2018, Copenhagen, Denmark, October, 14, 2018, volume 2245 of CEUR Workshop Proceedings, pp 521–523. CEUR-WS.org, (2018).” The original article can be found online at https://doi.org/10.1007/s10270-021-00913-x. The correct reference should be “Moin, A., Rössler, S., Günnemann, S.: Thingml+: Augmenting model-driven soft-ware engineering for the internet of things with machine learning. In: R. Hebig and T. Berger, editors, Proceedings of Workshops co-located with MODELS 2018, Copenhagen, Denmark, October, 14, 2018, volume 2245 of CEUR Work-shop Proceedings, pp 521–523. CEUR-WS.org, (2018).",
        "keywords": [],
        "authors": [
            "Phuong T. Nguyen",
            "Juri Di Rocco",
            "Ludovico Iovino",
            "Davide Di Ruscio",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-021-00944-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the theme issue on non-functional system properties in domain specific modeling languages",
        "submission-date": "2010/08",
        "publication-date": "2010/08",
        "abstract": "Complexity of software system has been recognized as the major cause of difficulties in making software system development an engineering discipline. As identified by F. Brooks (F. Brooks, The Mythical Man-Month, Addison Wesley, 1995), complexity of software system consists of the essential complexity, the complexity of problems that are part of requirements placed upon software systems, and accidental complexity caused by the use of current software engineering methods and technologies. Current trends in the use and development of information technology raise the both dimensions of complexity. Potentials recognized in software systems increase the number of their applications, and thus, causing the number and size of problems to grow. Increasing demands for software systems also push different vendors in producing implementation technologies, which promise to have solutions for all problems and such technologies require significant knowledge from software developers, often not very helpful in solving domain problems. As a solution to constant increase of complexity, model driven engineering (MDE) arises as one of the most prominent approaches. Based on ideas of direct representation, where solutions to problems are specified within the problem domain using domain specific terms and models, and automation, where implementations of software systems are (semi)automatically generated from domain specific models, MDE promotes (software) system development by problem-domain experts (e.g. home automation, insurance, performance, reliability) and not implementation experts.",
        "keywords": [],
        "authors": [
            "Marko Boškovi´c",
            "Dragan Gaševi´c",
            "Claus Pahl",
            "Bernhard Schätz"
        ],
        "file_path": "data/sosym-all/s10270-010-0171-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guiding the evolution of product-line configurations",
        "submission-date": "2020/09",
        "publication-date": "2021/07",
        "abstract": "A product line is an approach for systematically managing configuration options of customizable systems, usually by means of features. Products are generated for configurations consisting of selected features. Product-line evolution can lead to unintended changes to product behavior. We illustrate that updating configurations after product-line evolution requires decisions of both, domain engineers responsible for product-line evolution as well as application engineers responsible for configurations. The challenge is that domain and application engineers might not be able to interact with each other. We propose a formal foundation and a methodology that enables domain engineers to guide application engineers through configuration evolution by sharing knowledge on product-line evolution and by defining automatic update operations for configurations. As an effect, we enable knowledge transfer between those engineers without the need for interactions. We evaluate our methodology on four large-scale industrial product lines. The results of the qualitative evaluation indicate that our method is flexible enough for real-world product-line evolution. The quantitative evaluation indicates that we detect product behavior changes for up to 55.3% of the configurations which would not have been detected using existing methods.",
        "keywords": [
            "Product line",
            "Product-line evolution",
            "Guided feature configuration evolution",
            "Product behavior preservation"
        ],
        "authors": [
            "Michael Nieke",
            "Gabriela Sampaio",
            "Thomas Thüm",
            "Christoph Seidl",
            "Leopoldo Teixeira",
            "Ina Schaefer"
        ],
        "file_path": "data/sosym-all/s10270-021-00906-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling in the large: model libraries",
        "submission-date": "2021/05",
        "publication-date": "2021/05",
        "abstract": "Modeling in the Large is a key concept that is vital toward addressing the growing complexity and organizational requirements that are faced by developers when applying modeling techniques to real-world problems. For the most simple products, it is usually not necessary to define and follow a complicated formal development process. Modeling is particularly beneficial if the product is complex, comes in many different variants, or if the product is for a highly regulated domain (e.g., safety and security regulations). In these cases, one model cannot describe the whole product, but many models are needed to define multiple interacting concerns, often requiring several different languages to describe various aspects and viewpoints of the products or parts of the system under development.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-021-00887-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-based reference architecture for complex assistive systems and its application",
        "submission-date": "2023/03",
        "publication-date": "2024/03",
        "abstract": "Complex assistive systems providing human behavior support independent of the age or abilities of users are broadly used in a variety of domains including automotive, production, aviation, or medicine. Current research lacks a common understanding of which architectural components are needed to create assistive systems that use models at runtime. Existing descriptions of architectural components are focused on particular domains, consider only some parts of an assistive system, or do not consider models at runtime. We have analyzed common functional requirements for such systems to be able to propose a set of reusable components, which have to be considered when creating assistive systems that use models. Such components constitute a reference architecture that we propose within this paper. To validate the proposed architecture, we have expressed the architectures of two assistive systems from different domains, namely assistance for elderly people and assistance for operators in smart manufacturing in terms of compliance with such architecture. The proposed reference architecture will facilitate the creation of future assistive systems.",
        "keywords": [
            "Assistive systems",
            "Context-aware",
            "Reference architecture",
            "Model-based software engineering",
            "Daily activities support",
            "Assistive digital twin"
        ],
        "authors": [
            "Judith Michael",
            "Volodymyr A. Shekhovtsov"
        ],
        "file_path": "data/sosym-all/s10270-024-01157-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On theory and management of dependencies between models",
        "submission-date": "2025/06",
        "publication-date": "2025/06",
        "abstract": "Software developers often need to manage dependencies. Unfortunately, software dependencies manifest themselves in various forms, and discussions about dependencies can be challenging due to the very different definitions and relationships that developers may have in mind. To reduce misunderstandings, it may be helpful to categorize the various forms of dependencies.",
        "keywords": [],
        "authors": [
            "Marsha Chechik",
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-025-01301-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework for FMI-based co-simulation of human–machine interfaces",
        "submission-date": "2018/09",
        "publication-date": "2019/09",
        "abstract": "A framework for co-simulation of human–machine interfaces in Cyber-Physical Systems (CPS) is presented. The framework builds on formal (i.e. mathematical) methods. It aims to support the work of formal methods experts in charge of modelling and analysing safety-critical aspects of user interfaces in CPS. To carry out these modelling and analysis activities, formal methods experts usually need to engage with domain experts that may not fully understand the mathematical details of formal analysis results. The framework presented in this work mitigates this communication barrier by allowing formal methods experts to create interactive prototypes driven by formal models. The prototypes closely resemble the visual appearance of the system being developed. They can be used to discuss details of the formal analysis effort without showing any mathematical detail. An existing prototyping toolkit based on formal methods is used as baseline technology. Novel functionalities are developed for automatic generation of interactive prototypes supporting the Functional Mockup Interface (FMI), a de-facto standard technology for simulation of complex systems. Using the FMI interface, the prototypes can be integrated with simulations of other system components. The architecture of the framework is presented, along with a veriﬁcation of core aspects of its functionalities. A case study based on a medical system is used to demonstrate the capabilities of the framework.",
        "keywords": [
            "User interfaces",
            "Prototyping tools",
            "FMI co-simulation",
            "Model-based design"
        ],
        "authors": [
            "Maurizio Palmieri",
            "Cinzia Bernardeschi",
            "Paolo Masci"
        ],
        "file_path": "data/sosym-all/s10270-019-00754-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Editorial for the SoSyM issue 2015/03",
        "submission-date": "2015/06",
        "publication-date": "2015/06",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-015-0478-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction: Automaton-based comparison of Declare process models",
        "submission-date": "2023/01",
        "publication-date": "2023/01",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Nicolai Schützenmeier",
            "Martin Käppel",
            "Lars Ackermann",
            "Stefan Jablonski",
            "Sebastian Petter"
        ],
        "file_path": "data/sosym-all/s10270-022-01079-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From enterprise models to low-code applications: mapping DEMO to Mendix; illustrated in the social housing domain",
        "submission-date": "2023/03",
        "publication-date": "2024/04",
        "abstract": "Due to hyper-competition, technological advancements, regulatory changes, etc, the conditions under which enterprises need to thrive become increasingly turbulent. Consequently, enterprise agility increasingly determines an enterprise’s chances for success. As software development often is a limiting factor in achieving enterprise agility, enterprise agility and software adaptability become increasingly intertwined. As a consequence, decisions that regard ﬂexibility should not be left to software developers alone. By taking a Model-driven Software Development (MDSD) approach, starting from DEMO ontological enterprise models and explicit (enterprise) implementation design decisions, the aim of this research is to bridge the gap from enterprise agility to software adaptability, in such a way that software development is no longer a limiting factor in achieving enterprise agility. Low-code technology is a growing market trend that builds on MDSD concepts and claims to offer a high degree of software adaptability. Therefore, as a ﬁrst step to show the potential beneﬁts to use DEMO ontological enterprise models as a base for MDSD, this research shows the design of a mapping from DEMO models to Mendix for the (automated) creation of a low-code application that also intrinsically accommodates run-time implementation design decisions.",
        "keywords": [
            "Enterprise modeling",
            "Enterprise ontology",
            "DEMO",
            "MDSD",
            "Low-code",
            "Mendix"
        ],
        "authors": [
            "Marien R. Krouwel",
            "Martin Op ’t Land",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-024-01156-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Investigating expressiveness and understandability of hierarchy in declarative business process models",
        "submission-date": "2012/09",
        "publication-date": "2013/06",
        "abstract": "Hierarchy has widely been recognized as a viable approach to deal with the complexity of conceptual models. For instance, in declarative business process models, hierarchy is realized by sub-processes. While technical implementations of declarative sub-processes exist, their application, semantics, and the resulting impact on understandability are less understood yet—this research gap is addressed in this work. More specifically, we discuss the semantics and the application of hierarchy and show how sub-processes enhance the expressiveness of declarative modeling languages. Then, we turn to the influence of hierarchy on the understandability of declarative process models. In particular, we present a cognitive-psychology-based framework that allows to assess the impact of hierarchy on the understandability of a declarative process model. To empirically test the proposed framework, a combination of quantitative and qualitative research methods is followed. While statistical tests provide numerical evidence, think-aloud protocols give insights into the reasoning processes taking place when reading declarative process models.",
        "keywords": [
            "Business process management",
            "Declarative business process models",
            "Modularization",
            "Understandability",
            "Cognitive psychology"
        ],
        "authors": [
            "Stefan Zugal",
            "Pnina Soffer",
            "Cornelia Haisjackl",
            "Jakob Pinggera",
            "Manfred Reichert",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-013-0356-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling and reasoning about uncertainty in goal models: a decision-theoretic approach",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Goal models have been a popular subject of study by researchers in requirements engineering, due to their ability to capture and analyze alternative solutions through which a software system can achieve business objectives. A plethora of analysis methods for automated identiﬁcation of optimal alternatives have been proposed. However, such methods often assume an idealized reality where all tasks are successfully performed when attempted and all goals are eventually satisﬁed with certainty when pursued according to a solution. In reality, some tasks run the risk of failure while others produce chance outcomes. In this paper, we extend the standard goal modeling language to allow representation and reasoning about both uncertainty and preferential utility in goals. Tasks are extended to allow for probabilistic effects and preferential statements of stakeholders are captured and translated into utilities over possible effects. Moreover, solutions are not mere speciﬁcations (functions, quality constraints, and assumptions), but rather policies, that is sequences of situational action decisions, through which stakeholder goals can be fulﬁlled. An AI reasoning tool is adapted and used for identifying optimal policies with respect to the value they offer to stakeholders measured against their probability of failure. Evaluation of the approach includes a simulation study and scalability experiments to assess the applicability of automated reasoning for larger problems.",
        "keywords": [
            "Goal modeling",
            "Markov decision processes (MDP)",
            "DT-Golog",
            "Golog"
        ],
        "authors": [
            "Sotirios Liaskos",
            "Shakil M. Khan",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-021-00968-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling and enforcing secure object ﬂows in process-driven SOAs: an integrated model-driven approach",
        "submission-date": "2011/10",
        "publication-date": "2012/10",
        "abstract": "In this paper, we present an integrated model-driven approach for the speciﬁcation and the enforcement of secure object ﬂows in process-driven service-oriented architectures (SOA). In this context, a secure object ﬂow ensures the conﬁdentiality and the integrity of important objects (such as business contracts or electronic patient records) that are passed between different participants in SOA-based business processes. We specify a formal and generic meta-model for secure object ﬂows that can be used to extend arbitrary process modeling languages. To demonstrate our approach, we present a UML extension for secure object ﬂows. Moreover, we describe how platform-independent models are mapped to platform-speciﬁc software artifacts via automated model transformations. In addition, we give a detailed description of how we integrated our approach with the Eclipse modeling tools.",
        "keywords": [
            "Process modeling",
            "Secure object ﬂows",
            "Security engineering",
            "Service-oriented architecture",
            "Model-driven development",
            "UML",
            "SoaML",
            "Web services"
        ],
        "authors": [
            "Bernhard Hoisl",
            "Stefan Sobernig",
            "Mark Strembeck"
        ],
        "file_path": "data/sosym-all/s10270-012-0263-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extracting ﬁnite state representation of Java programs",
        "submission-date": "2013/05",
        "publication-date": "2014/06",
        "abstract": "We present a static analysis-based technique for\nreverse engineering ﬁnite state machine models from a large\nsubset of sequential Java programs. Our approach enumer-\nates all feasible program paths in a class using symbolic exe-\ncution and records execution summary for each path. Sub-\nsequently, it creates states and transitions by analyzing sym-\nbolic execution summaries. Our approach also detects any\nunhandled exceptions.",
        "keywords": [
            "Software reverse engineering",
            "FSM",
            "System modeling"
        ],
        "authors": [
            "Tamal Sen",
            "Rajib Mall"
        ],
        "file_path": "data/sosym-all/s10270-014-0415-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest Editorial to the Theme Section on Model-Driven Web Engineering",
        "submission-date": "2013/02",
        "publication-date": "2013/02",
        "abstract": "Model-Driven Engineering (MDE) is becoming a widely accepted paradigm for the design and development of complex distributed applications. MDE advocates the use of models and model transformations as key artefacts in all phases of the software process, from system specification and analysis, to design, development and testing. Each model usually addresses one concern, independently from the rest of the issues involved in the construction of the system. Thus, the basic functionality of the system can be separated from its final implementation, and the business logic can be separated from the underlying platform technology, etc. Transformations between models enable the automated implementation of a system from the different models defined for it. The Web Engineering (WE) community soon discovered that the key aspects of MDE (abstraction through modeling; separation of concerns by multi-viewpoint specification, and software development by model transformation) perfectly matched the principles and practices promoted by WE, and allowed Web application designers to address some of their most critical challenges, for example, the need to cope with the constantly growing complexity and with the new requirements on Web applications, in the face of a rapid evolution of the supporting technologies and platforms. Further architectural concerns, such as adaptation or distribution, need to be modeled and taken into account into new Web systems, beyond the traditional content-navigation-presentation aspects addressed by classical WE proposals. There is also an increasing trend towards the incorporation of emerging technologies like the Semantic Web, especially within the scope of the Web 2.0, its related technologies and richer applications. Furthermore, current Web applications need to interoperate with other external systems, which require their integration with third party Web-services, portals, portlets, and also with legacy systems. Finally, many of the existing WE proposals did not fully exploit all the potential benefits of MDE, such as complete platform independence, model transformation and analysis, and metamodeling.",
        "keywords": [],
        "authors": [
            "Geert-Jan Houben",
            "Nora Koch",
            "Gustavo Rossi",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-011-0196-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Using UML/MARTE to support performance tuning and stress testing in real-time systems",
        "submission-date": "2015/07",
        "publication-date": "2017/02",
        "abstract": "Abstract Real-time embedded systems (RTESs) operating\nin safety-critical domains have to satisfy strict performance\nrequirements in terms of task deadlines, response time, and\nCPU usage. Two of the main factors affecting the satisfaction\nof these requirements are the conﬁguration parameters reg-\nulating how the system interacts with hardware devices, and\ntheexternaleventstriggeringthesystemtasks.Inparticular,it\nisnecessarytocarefullytunetheparametersinordertoensure\na satisfactory trade-off between responsiveness and usage of\ncomputational resources, and also to stress test the system\nwith worst-case inputs likely to violate the requirements.\nPerformance tuning and stress testing are usually manual,\ntime-consuming, and error-prone processes, because the sys-\ntem parameters and input values range in a large domain, and\ntheir impact over performance is hard to predict without exe-\ncuting the system. In this paper, we provide an approach,\nbased on UML/MARTE, to support the generation of system\nconﬁgurations predicted to achieve a satisfactory trade-off\nbetween response time and CPU usage, and stress test cases\nthat push the system tasks to violate their deadlines. First,\nwe devise a conceptual model that speciﬁes the abstractions\nrequired for analyzing task deadlines, response time, and\nCPU usage, and provide a mapping between these abstrac-\ntions and UML/MARTE. Then, we prune the UML/MARTE\nmetamodel to only contain a purpose-speciﬁc subset of enti-",
        "keywords": [
            "UML/MARTE",
            "Real-time systems",
            "Safety-critical systems",
            "Performance tuning",
            "Stress testing",
            "Constrained optimization"
        ],
        "authors": [
            "Stefano Di Alesio",
            "Sagar Sen"
        ],
        "file_path": "data/sosym-all/s10270-017-0585-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Live modeling in the context of state machine models and code generation",
        "submission-date": "2019/09",
        "publication-date": "2020/11",
        "abstract": "Live modeling has been recognized as an important technique to edit behavioral models while being executed and helps in better understanding the impact of a design choice. In the context of model-driven development, models can be executed by interpretation or by the translation of models into existing programming languages, often by code generation. This work is concerned with the support of live modeling in the context of state machine models when they are executed by code generation. To this end, we propose an approach that is completely independent of any live programming support offered by the target language. This independence is achieved with the help of a model transformation which equips the model with support for features which are required for live modeling. A subsequent code generation then produces a self-reﬂective program that allows changes to the model elements at runtime (through synchronization of design and runtime models). We have applied the approach in the context of UML-RT and created a prototype (Live-UMLRT) that provides a full set of services for live modeling of UML-RT state machines such as re-execution, adding/removing states and transitions, and adding/removing action code. We have evaluated the prototype on several use cases. The evaluation shows that (1) generation of a self-reﬂective and model instrumentation can be carried out with reasonable performance, and (2) our approach can apply model changes to the running execution faster than the standard approach that depends on the live programming support of the target language.",
        "keywords": [
            "Model execution",
            "Live modeling",
            "Model-level debugging",
            "MDD",
            "UML-RT"
        ],
        "authors": [
            "Mojtaba Bagherzadeh",
            "Karim Jahed",
            "Benoit Combemale",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-020-00829-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reﬁnement-based Validation of Event-B Speciﬁcations",
        "submission-date": "2014/11",
        "publication-date": "2016/02",
        "abstract": "Abstract The validation of formal speciﬁcations is a challenging task. It is one of the factors that impede the penetration of formal methods into the common practices of software development. This paper discusses the issue of validating formal models by executing them in the context of Event-B. The most important problem lies in the non-determinism which often prevents purely automatic tools to execute models. In this paper, we ﬁrst present and discuss the techniques we have created to allow the execution of models at all levels of abstraction. These techniques rely on users to overcome the barriers resulting from non-deterministic features by either modifying the model or providing ad hoc implementations. Then, we present our main contribution, the formal deﬁnition of the notion of ﬁdelity, that guarantees that all the observable behaviors of the executable models are indeed speciﬁed by the original (non-deterministic) models. The notion of ﬁdelity can be expressed in terms of proof obligations.",
        "keywords": [
            "Formal methods",
            "Reﬁnement",
            "Model validation",
            "Event-B"
        ],
        "authors": [
            "Atif Mashkoor",
            "Faqing Yang",
            "Jean-Pierre Jacquot"
        ],
        "file_path": "data/sosym-all/s10270-016-0514-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "How fair are we? From conceptualization to automated assessment of fairness definitions",
        "submission-date": "2023/10",
        "publication-date": "2025/01",
        "abstract": "Fairness is a critical concept in ethics and social domains, but it is also a challenging property to engineer in software systems. With the increasing use of machine learning in software systems, researchers have been developing techniques to assess the fairness of software systems automatically. Nonetheless, many of these techniques rely upon pre-established fairness definitions, metrics, and criteria, which may fail to encompass the wide-ranging needs and preferences of users and stakeholders. To overcome this limitation, we propose a novel approach, called MODNESS, that enables users to customize and define their fairness concepts using a dedicated modeling environment. Our approach guides the user through the definition of new fairness concepts also in emerging domains, and the specification and composition of metrics for its evaluation through a dedicated domain-specific language. Ultimately, MODNESS generates the source code to implement fair assessment based on these custom definitions. In addition, we elucidate the process we followed to collect and analyze relevant literature on fairness assessment in software engineering (SE). We compare MODNESS with the selected approaches and evaluate how they support the distinguishing features identified by our study. Our findings reveal that i) most of the current approaches do not support user-defined fairness concepts; ii) our approach can cover additional application domains not addressed by currently available tools, e.g., mitigating bias in recommender systems for software engineering and Arduino software component recommendations; iii) MODNESS demonstrates the capability to overcome the limitations of the only two other model-driven engineering-based approaches for fairness assessment.",
        "keywords": [
            "Fairness assessment",
            "Model-driven engineering",
            "Bias and Fairness definition"
        ],
        "authors": [
            "Giordano d’ Aloisio",
            "Claudio Di Sipio",
            "Antinisca Di Marco",
            "Davide Di Ruscio"
        ],
        "file_path": "data/sosym-all/s10270-025-01277-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based engineering in the embedded systems domain: an industrial survey on the state-of-practice",
        "submission-date": "2015/05",
        "publication-date": "2016/03",
        "abstract": "Model-based engineering (MBE) aims at increasing the effectiveness of engineering by using models as important artifacts in the development process. While empirical studies on the use and the effects of MBE in industry exist, only few of them target the embedded systems domain. We contribute to the body of knowledge with an empirical study on the use and the assessment of MBE in that particular domain. The goal of this study is to assess the current state-of-practice and the challenges the embedded systems domain is facing due to shortcomings with MBE. We collected quantitative data from 113 subjects, mostly professionals working with MBE, using an online survey. The collected data spans different aspects of MBE, such as the used modeling languages, tools, notations, effects of MBE introduction, or shortcomings of MBE. Our main findings are that MBE is used by a majority of all participants in the embedded systems domain, mainly for simulation, code generation, and documentation. Reported positive effects of MBE are higher quality and improved reusability. Main shortcomings are interoperability difficulties between MBE tools, high training effort for developers and usability issues. Our study offers valuable insights into the current industrial practice and can guide future research in the fields of systems modeling and embedded systems.",
        "keywords": [
            "Model-based engineering",
            "Model-driven engineering",
            "Embedded systems",
            "Industry",
            "Modeling",
            "Empirical study",
            "State-of-practice"
        ],
        "authors": [
            "Grischa Liebel",
            "Nadja Marko",
            "Matthias Tichy",
            "Andrea Leitner",
            "Jörgen Hansson"
        ],
        "file_path": "data/sosym-all/s10270-016-0523-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Case model landscapes: toward an improved representation of knowledge-intensive processes using the fCM-language",
        "submission-date": "2019/12",
        "publication-date": "2021/04",
        "abstract": "Case Management is a paradigm to support knowledge-intensive processes. The different approaches developed for modeling these types of processes tend to result in scattered information due to the low abstraction level at which the inherently complex processes are represented. Thus, readability and understandability are more challenging than in imperative process models. This paper extends a case modeling language—the fragment-based Case Management (fCM) language—to a so-called fCM landscape (fCML) with the goal of modeling a single knowledge-intensive process from a higher abstraction level. Following the Design Science Research (DSR) methodology, we ﬁrst deﬁne requirements for an fCML, and then review how literature—in the ﬁelds of process overviews and case management—could support them. Design decisions are formalized by specifying a syntax for an fCML and the transformation rules from a given fCM model. The proposal is empirically evaluated via a laboratory experiment. Quantitative results imply that interpreting an fCML requires less effort in terms of time—and is thus more efﬁcient—than interpreting its equivalent fCM case model. Qualitative results provide indications on the factors affecting case model interpretation and guidelines for future work.",
        "keywords": [
            "Case management",
            "Knowledge-intensive process",
            "Process landscape",
            "Process map",
            "Process architecture"
        ],
        "authors": [
            "Fernanda Gonzalez-Lopez",
            "Luise Pufahl",
            "Jorge Munoz-Gama",
            "Valeria Herskovic",
            "Marcos Sepúlveda"
        ],
        "file_path": "data/sosym-all/s10270-021-00885-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Recommendations for visual feedback about problems within BPMN process models",
        "submission-date": "2020/10",
        "publication-date": "2022/01",
        "abstract": "Business process modeling is a key task in business process management because, besides representing processes, the process models are used, for example, for communication purposes among stakeholders. When not correctly modeled, process models may diminish businesses’ proﬁtability. In this work, we conducted a survey with 57 participants, where we gathered a list of modelers’ needs regarding the feedback they would like to get about problems in process models. For example, modelers would like to get feedback according to their level of experience and be able to activate/deactivate automatic validation. Then, we built a catalog of required features that represents a set of features that process modeling tools should address regarding feedback about problems in process models. Furthermore, we mapped the identiﬁed modelers’ needs to how a group of process modeling tools provides such kind of feedback and to the solutions found in the literature. Finally, based on the gaps found in the mapping, we provide a set of recommendations for visual feedback about problems in process models, which can guide the development of future process modeling tools. Our work focuses on the Business Process Model and Notation because it is an ISO standard, supported by several process modeling and execution tools.",
        "keywords": [
            "Process modeling",
            "Problems in process models",
            "Survey",
            "Recommendations",
            "Visual feedback"
        ],
        "authors": [
            "Vinicius Stein Dani",
            "Carla Maria Dal Sasso Freitas",
            "Lucinéia Heloisa Thom"
        ],
        "file_path": "data/sosym-all/s10270-021-00972-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Systematic approach for constructing an understandable state machine from a contract-based speciﬁcation: controlled experiments",
        "submission-date": "2013/03",
        "publication-date": "2014/11",
        "abstract": "Contract-basedspeciﬁcationsusingOCLorJML\nare employed widely to describe the behaviors of systems.\nHowever, complex behaviors might not be understood using\nthese speciﬁcations because they focus on each individual\nmethod instead of the relationships between them. State\nmachines (SMs) can be used to model the dynamic behavior\nincluding acceptable event sequences. However, the man-\nual construction of SMs is a time-consuming and error-\nprone task. Many studies have aimed to construct SMs from\ncontract-based speciﬁcations. However, existing SM con-\nstruction approaches are not concerned with certain qualities\nof the SMs, such as understandability. In this study, we aimed\nto develop a combined atomic condition-based approach for\nconstructing highly understandable SMs from formal speci-\nﬁcations. We conducted two controlled experiments to evalu-\natetheunderstandabilityoftheSMsconstructed:technology-\noriented and human-oriented evaluations. Two existing SM\nconstruction approaches, i.e., condition-partitioning-based\napproach and experience-based approach, were used as the\ncontrols in the two experiments, for comparison with the pro-\nposed approach. In the technology-oriented experiment, 36\nSMs were constructed from 12 speciﬁcations using the three\napproaches. A paired-samples Wilcoxon’s signed-rank test\nwas used to test the differences in the values of a SM under-\nstandability metric based on cohesion and coupling metrics.\nIn the human-oriented experiment, we used 15 of the 36 SMs\nand the differences between the understandability correct-\nness (the number of correct answers/the number of answered\nquestions) measured by 23 participants were tested using an\nindependent t test. The results of the two experiments showed\nthat the understandability of SMs constructed using the pro-\nposed approach was signiﬁcantly better than that of SMs\nconstructed using the two control approaches (p < 0.05).\nThe proposed approach does not support advanced features\nsuch as the containers of contract-based speciﬁcations and\nthe hierarchy/concurrency of SMs.",
        "keywords": [
            "Systematic construction",
            "Reverse engineering",
            "Design by contract",
            "State machine",
            "Understandability",
            "Technology-oriented controlled experiment",
            "Human-oriented controlled experiment"
        ],
        "authors": [
            "Jung Ho Bae",
            "Heung Seok Chae"
        ],
        "file_path": "data/sosym-all/s10270-014-0440-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Questionnaire-based variability modeling for system conﬁguration",
        "submission-date": "2007/06",
        "publication-date": "2008/06",
        "abstract": "Variability management is a recurrent issue in systems engineering. It arises for example in enterprise sys- tems, where modules are conﬁgured and composed to meet the requirements of individual customers based on modiﬁ- cations to a reference model. It also manifests itself in the context of software product families, where variants of a system are built from a common code base. This paper pro- poses an approach to capture system variability based on questionnaire models that include order dependencies and domain constraints. The paper presents analysis techniques to detect circular dependencies and contradictory constraints in questionnaire models, as well as techniques to incrementally prevent invalid conﬁgurations by restricting the space of allowed answers to a question based on previous answers. The approach has been implemented as a toolset and has been used in practice to capture conﬁgurable process models for ﬁlm post-production.",
        "keywords": [
            "Variability modeling",
            "System conﬁguration",
            "Questionnaire",
            "Software product family"
        ],
        "authors": [
            "Marcello La Rosa",
            "Wil M. P. van der Aalst",
            "Marlon Dumas",
            "Arthur H. M. ter Hofstede"
        ],
        "file_path": "data/sosym-all/s10270-008-0090-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Domain analysis of dynamic system reconﬁguration",
        "submission-date": "2006/01",
        "publication-date": "2007/01",
        "abstract": "A domain analysis of dynamic system reconﬁguration is presented in this paper. The intent is to provide a comprehensive conceptual framework within which to systematically and consistently address problems and solutions related to dynamically reconﬁgurable systems. The analysis identiﬁes and categorizes the various types of change that may be required, the relationship between those types, and the system integrity characteristics that need to be considered when such changes take place. A system model is employed to describe each change type using examples of global and local properties in the context of a ﬁnancial analysis system. A rigorous formal methodology, based on the Alloy language and tools, is employed to specify precisely and formally the detailed relationships between various parts of the model. Based upon these descriptions, the types of change of dynamic system reconﬁguration are presented as a series of UML class models.",
        "keywords": [
            "Alloy",
            "Component",
            "based systems",
            "Dynamic reconﬁguration",
            "Feature modeling",
            "Model-",
            "driven development",
            "Software evolution",
            "System integrity",
            "UML"
        ],
        "authors": [
            "James D’Arcy Walsh",
            "Francis Bordeleau",
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-006-0038-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial for the theme section on modeling language engineering",
        "submission-date": "2023/02",
        "publication-date": "2023/03",
        "abstract": "Software-intensive systems are becoming more complex, driven by the need to integrate across multiple aspects. Consequently, the development of such systems requires the integration of different concerns and skills. These concerns are usually covered by different domain-speciﬁc modeling languages, with speciﬁc concepts, technologies, and abstraction levels. This multiplication of languages eases the development related to one speciﬁc concern but raises language and technology integration problems at the different stages of the software life cycle. To support effective language integration, there is a pressing need to reify and classify these relationships, as well as the language interactions that the relationships enable. Similarly, the proliferation of domain-speciﬁc modeling languages increases the need for effective and efﬁcient techniques for engineering languages and their support infrastructures. Hence, software developers are faced both with the challenging task of engineering each separate modeling language and associated technologies and with the task of integrating the different languages from different concern spaces.",
        "keywords": [],
        "authors": [
            "Benoît Combemale",
            "Romina Eramo",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-023-01097-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Accidental complexity in multilevel modeling revisited",
        "submission-date": "2020/05",
        "publication-date": "2022/01",
        "abstract": "Multilevel modeling (MLM) conceptualizes software models as layered architectures of sub-models that are inter-related by the instance-of relation. Conceptually, MLM provides beneﬁts in terms of ontological classiﬁcation. Pragmatically, based on arguments in knowledge engineering, MLM meaningfully reduces accidental complexity. In this paper, the problem of accidental complexity in MLM is revisited. The paper focuses on the role of the context of type-instance structures on MLM architectures. We analyze factors of accidental complexity in multilevel models, suggest quantitative metrics for these factors, and show how they can be used for guiding MLM rearchitecture transformations. The relevance of the proposed factors and metrics is shown in an experimental study of type-instance contexts in multiple real-world models.",
        "keywords": [
            "Multilevel modeling",
            "Context",
            "Rearchitecture",
            "Accidental complexity",
            "Quantitative measures",
            "Evaluation criteria"
        ],
        "authors": [
            "Mira Balaban",
            "Igal Khitron",
            "Azzam Maraee"
        ],
        "file_path": "data/sosym-all/s10270-021-00938-2.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Formal methods in the scope of the Software and Systems Modeling journal",
        "submission-date": "2025/04",
        "publication-date": "2025/04",
        "abstract": "Software and Systems Modeling (SoSyM) is a journal dedicated to advancing the field of software and systems modeling by publishing high-quality research that contributes to the theory and practice of modeling in software and systems engineering, which also includes processes executed automatically or involving humans. The journal aims to bridge the gap between academia and industry by fostering discussions on modeling languages, methodologies, tools, and their applications to real-world challenges. SoSyM encourages submissions that present innovative modeling approaches, their precise semantic foundations, empirical evaluations, and applications that have tangible impacts on software and system development processes. Given this mission, the journal welcomes research on formal methods, provided that such work is framed within the context of software and systems modeling.",
        "keywords": [],
        "authors": [
            "Marsha Chechik",
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-025-01287-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The Impact of Model Simulation on Semantic Conceptual Model Quality for Novice Modelers",
        "submission-date": "2025/03",
        "publication-date": "Not found",
        "abstract": "Purpose: Conceptual modeling is crucial for information systems development, yet novice modelers often struggle with ensuring a high semantic quality, leading to ﬂawed system designs. This study investigates the impact of Feedback-enabled Interactive Rapid Prototyping (FIRP) simulation on novice modelers’ ability to identify and correct semantic errors in conceptual models.\nMethods: We analyzed data from 39 master-level students who used the MERODE prototyper in a FIRP simulation environment. We examined the relationship between testing effort, measured by event invocations and model coverage, and the likelihood of students identifying seeded semantic mistakes and providing correct solutions, using logistic regression.\nResults: Logistic regression analyses revealed a statistically signiﬁcant positive correlation between testing effort and the detection of speciﬁc behavioral constraint violations that resulted in deadlocks. However, the relationship was not consistent across all mistake types.\nConclusion: FIRP simulation is particularly effective in helping novice modelers identify behavioral constraint violations that disrupt system workﬂows, i.e., those leading to deadlocks. Active model exploration through FIRP enhances error detection, highlighting its potential as a valuable tool for conceptual modeling education. However, the effectiveness varies with the type of semantic error, suggesting the need for tailored simulation strategies.",
        "keywords": [
            "Conceptual Modeling",
            "Model Simulation",
            "Semantic Quality",
            "FIRP",
            "Deadlock Situations"
        ],
        "authors": [
            "Felix Cammaerts",
            "Beatriz Marín",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-025-01317-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Business process modeling language selection for research modelers",
        "submission-date": "2022/09",
        "publication-date": "2023/05",
        "abstract": "Business process modeling is a crucial aspect of domains such as Business Process Management and Software Engineering. The availability of various BPM languages in the market makes it challenging for process modelers to select the best-fit BPM language for a specific process modeling task. A decision model is necessary to systematically capture and make scattered knowledge on BPM languages available for reuse by process modelers and academics. This paper presents a decision model for the BPM language selection problem in research projects. The model contains mappings of 72 BPM features to 23 BPM languages. We validated and refined the decision model through 10 expert interviews with domain experts from various organizations. We evaluated the efficiency, validity, and generality of the decision model by conducting four case studies of academic research projects with their original researchers. The results confirmed that the decision model supports process modelers in the selection process by providing more insights into the decision process. Based on the empirical evidence from the case studies and domain expert feedback, we conclude that having the knowledge readily available in the decision model supports academics in making more informed decisions that align with their preferences and prioritized requirements. Furthermore, the captured knowledge provides a comprehensive overview of BPM languages, features, and quality characteristics that other researchers can employ to tackle future research challenges. Our observations indicate that BPMN is a commonly used modeling language for process modeling. Therefore, it is more sensible for academics to explain why they did not select BPMN than to discuss why they chose it for their research project(s).",
        "keywords": [
            "Business process modeling language selection",
            "Decision model",
            "Multi-criteria decision-making",
            "Decision support system",
            "Case study research"
        ],
        "authors": [
            "Siamak Farshidi",
            "Izaak Beer Kwantes",
            "Slinger Jansen"
        ],
        "file_path": "data/sosym-all/s10270-023-01110-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SYMBOLEOPC: checking properties of legal contracts",
        "submission-date": "2023/05",
        "publication-date": "2024/07",
        "abstract": "Legal contracts specify requirements for business transactions. Symboleo was recently proposed as a formal speciﬁcation language for legal contracts. It allows the speciﬁcation of the contractual requirements by specifying the obligations and powers of the parties, as well as specifying the events that can occur in a contract’s lifecycle. With appropriate tool support, Symboleo can allow monitoring the contract lifecycle. However, because of mistakes in contract interpretation or formal speciﬁcation, speciﬁed contracts may violate properties expected by contracting parties. This paper presents SymboleoPC, a tool for analyzing Symboleo contracts using the nuXmv model checker, where properties can be expressed in both Linear Temporal Logic and Computation Tree Logic. The presentation highlights the architecture, implementation, and testing of the tool, as well as a scalability evaluation, based on performance data. The performance of the tool was evaluated with respect to varying numbers of obligations and powers, with varying numbers of inter-dependencies among them, with parameters derived from the analysis of real contracts. These results suggest that SymboleoPC can be usefully applied to the analysis of formal speciﬁcations of contracts with real-life sizes and structures.",
        "keywords": [
            "Legal contracts",
            "Smart contracts",
            "Software requirements speciﬁcations",
            "Formal speciﬁcation languages",
            "Model checking",
            "Performance analysis",
            "SymboleoPC",
            "nuXmv"
        ],
        "authors": [
            "Alireza Parvizimosaed",
            "Marco Roveri",
            "Aidin Rasti",
            "Amal Ahmed Anda",
            "Sofana Alfuhaid",
            "Daniel Amyot",
            "Luigi Logrippo",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-024-01180-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correlating contexts and NFR conﬂicts from event logs",
        "submission-date": "2022/03",
        "publication-date": "2023/02",
        "abstract": "In the design of autonomous systems, it is important to consider the preferences of the interested parties to improve the user experience. These preferences are often associated with the contexts in which each system is likely to operate. The operational behavior of a system must also meet various non-functional requirements (NFRs), which can present different levels of conﬂict depending on the operational context. This work aims to model correlations between the individual contexts and the consequent conﬂicts between NFRs. The proposed approach is based on analyzing the system event logs, tracing them back to the leaf elements at the speciﬁcation level and providing a contextual explanation of the system’s behavior. The traced contexts and NFR conﬂicts are then mined to produce Context-Context and Context-NFR conﬂict sequential rules. The proposed Contextual Explainability (ConE) framework uses BERT-based pre-trained language models and sequential rule mining libraries for deriving the above correlations. Extensive evaluations are performed to compare the existing state-of-the-art approaches. The best-ﬁt solutions are chosen to integrate within the ConE framework. Based on experiments, an accuracy of 80%, a precision of 90%, a recall of 97%, and an F1-score of 88% are recorded for the ConE framework on the sequential rules that were mined.",
        "keywords": [
            "Sequential rule mining",
            "Context correlation",
            "Context-NFR conﬂict correlation",
            "Goal models"
        ],
        "authors": [
            "Mandira Roy\nSouvick Das\nNovarun Deb\nAgostino Cortesi\nRituparna Chaki\nNabendu Chaki"
        ],
        "file_path": "data/sosym-all/s10270-023-01087-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Effective product-line testing using similarity-based product prioritization",
        "submission-date": "2015/07",
        "publication-date": "2016/12",
        "abstract": "A software product line comprises a family of software products that share a common set of features. Testing an entire product-line product-by-product is infeasible due to the potentially exponential number of products in the number of features. Accordingly, several sampling approaches have been proposed to select a presumably minimal, yet sufﬁcient number of products to be tested. Since the time budget for testing is limited or even a priori unknown, the order in which products are tested is crucial for effective product-line testing. Prioritizing products is required to increase the probability of detecting faults faster. In this article, we propose similarity-based prioritization, which can be efﬁciently applied on product samples. In our approach, we incrementally select the most diverse product in terms of features to be tested next in order to increase feature interaction coverage as fast as possible during product-by-product testing. We evaluate the gain in the effectiveness of similarity-basedprioritizationonthreeproductlineswithreal faults. Furthermore, we compare similarity-based prioritization to random orders, an interaction-based approach, and the default orders produced by existing sampling algorithms considering feature models of various sizes. The results show that our approach potentially increases effectiveness in terms of fault detection ratio concerning faults within real-world product-line implementations as well as synthetically seeded faults. Moreover, we show that the default orders of recent sampling algorithms already show promising results, which, however, can still be improved in many cases using similarity-based prioritization.",
        "keywords": [
            "Software product lines",
            "Product-line testing",
            "Model-based testing",
            "Combinatorial interaction testing",
            "Test-case prioritization"
        ],
        "authors": [
            "Mustafa Al-Hajjaji",
            "Thomas Thüm",
            "Malte Lochau",
            "Jens Meinicke",
            "Gunter Saake"
        ],
        "file_path": "data/sosym-all/s10270-016-0569-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Realizing strategic ﬁt within the business architecture: the design of a Process-Goal Alignment modeling and analysis technique",
        "submission-date": "2015/04",
        "publication-date": "2017/01",
        "abstract": "The realization of strategic ﬁt within the business architecture is an important challenge for organizations. Research in the ﬁeld of enterprise modeling has resulted in the development of a wide range of modeling techniques that provide visual representations to improve the understanding and communication about the business architecture. As these techniques only provide partial solutions for the issue of realizing strategic ﬁt, the Process-Goal Alignment technique is presented in this paper. This technique combines the visual expressiveness of heat mapping techniques with the analytical capabilities of performance measurement and Strategic Management frameworks to provide a comprehensible and well-informed modeling language for the realization of strategic ﬁt within an organization’s business architecture. The paper reports on the design of the proposed technique by means of Action Design Research, which included iterative cycles of building, intervention, and evaluation through case studies. To support the application of the technique, a software tool was developed using the ADOxx meta-modeling platform.",
        "keywords": [
            "Strategic ﬁt",
            "Business architecture",
            "Enterprise modeling",
            "Process-Goal Alignment",
            "Heat map"
        ],
        "authors": [
            "Ben Roelens",
            "Wout Steenacker",
            "Geert Poels"
        ],
        "file_path": "data/sosym-all/s10270-016-0574-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Constraint-driven modeling through transformation",
        "submission-date": "2012/10",
        "publication-date": "2013/06",
        "abstract": "In model-driven software engineering, model transformation plays a key role for automatically generating and updating models. Transformation rules define how source model elements are to be transformed into target model elements. However, defining transformation rules is a complex task, especially in situations where semantic differences or incompleteness allow for alternative interpretations or where models change continuously before and after transformation. This paper proposes constraint-driven modeling where transformation is used to generate constraints on the target model rather than the target model itself. We evaluated the approach on three case studies that address the above difficulties and other common transformation issues. We also developed a proof-of-concept implementation that demonstrates its feasibility. The implementation suggests that constraint-driven transformation is an efficient and scalable alternative and/or complement to traditional transformation.",
        "keywords": [
            "Model-driven engineering (MDE)",
            "Model transformation",
            "Ambiguity",
            "Consistency checking",
            "Incremental constraint management"
        ],
        "authors": [
            "Andreas Demuth",
            "Roberto Erick Lopez-Herrejon",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-013-0363-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Exploring inconsistencies between modal transition systems",
        "submission-date": "2009/06",
        "publication-date": "2010/02",
        "abstract": "It is commonplace to have multiple behaviour\nmodels that describe the same system but have been pro-\nduced by different stakeholders or synthesized from different\nsources. Although in practice, such models frequently exhibit\ninconsistencies, there is a lack of tool support for analyz-\ning them. There are two key difﬁculties in explaining why\ntwo behavioural models are inconsistent: (1) explanations\noften require branching structures rather than linear traces,\nor scenarios; and (2) there can be multiple sources of incon-\nsistency and many different ways of explaining each one.\nIn this paper, we present an approach that supports explo-\nration of inconsistencies between modal transition systems,\nan extension to labelled transition systems. We show how\nto produce sound graphical explanations for inconsistencies,\nhow to compactly represent all possible explanations in a\ncomposition of the models being compared, and how mod-\nelers can use this composition to explore the explanations\nencoded therein.",
        "keywords": [
            "Labelled transition systems",
            "Inconsistency identiﬁcation and resolution",
            "μ-Calculus",
            "Distinguishing property",
            "Graphical feedback"
        ],
        "authors": [
            "Mathieu Sassolas",
            "Marsha Chechik",
            "Sebastian Uchitel"
        ],
        "file_path": "data/sosym-all/s10270-010-0148-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the use of models for high-performance scientiﬁc computing applications: an experience report",
        "submission-date": "2015/05",
        "publication-date": "2016/03",
        "abstract": "This paper reports on a four-year project that aims to raise the abstraction level through the use of model-driven engineering (MDE) techniques in the development of scientiﬁc applications relying on high-performance computing. The development and maintenance of high-performance scientiﬁc computing software is reputedly a complex task. This complexity results from the frequent evolutions of supercomputers and the tight coupling between software and hardware aspects. Moreover, current parallel programming approaches result in a mixing of concerns within the source code. Our approach relies on the use of MDE and consists in deﬁning domain-speciﬁc modeling languages targeting various domain experts involved in the development of HPC applications, allowing each of them to handle their dedicated modelinabothuser-friendlyandhardware-independentway. The different concerns are separated thanks to the use of several models as well as several modeling viewpoints on these models. Depending on the targeted execution platforms, these abstract models are translated into executable implementations by means of model transformations. To make all of these effective, we have developed a tool chain that is also presented in this paper. The approach is assessed through a multi-dimensional validation that focuses on its applicability, its expressiveness and its efﬁciency. To capitalize on the gained experience, we analyze some lessons learned during this project.",
        "keywords": [
            "HPC",
            "High-performance calculus",
            "MDE",
            "Model-driven engineering",
            "Architecture",
            "Fortran"
        ],
        "authors": [
            "Ileana Ober",
            "Marc Palyart",
            "Jean-Michel Bruel",
            "David Lugato"
        ],
        "file_path": "data/sosym-all/s10270-016-0518-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A systematic approach to generate B preconditions: application to the database domain",
        "submission-date": "2006/08",
        "publication-date": "2008/07",
        "abstract": "Maintaining integrity constraints in information systems is a real issue. In our previous work, we have defined a formal approach that derives B formal specifications from a UML description of the system. Basically, the generated B specification is composed of a set of variables modeling data and a set of operations representing transactions. The integrity constraints are directly specified as B invariant properties. So far, the operations we generate establish only a reduced class of constraints. In this paper, we describe a systematic approach to identify preconditions that take a larger class of invariants into account. The key idea is the definition of rewriting and simplification rules that we apply to the B invariants.",
        "keywords": [
            "Integrity constraints",
            "Formal specification",
            "B operations",
            "Invariant",
            "Precondition"
        ],
        "authors": [
            "Amel Mammar"
        ],
        "file_path": "data/sosym-all/s10270-008-0098-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated product line test case selection: industrial case study and controlled experiment",
        "submission-date": "2014/08",
        "publication-date": "2015/04",
        "abstract": "Automated test case selection for a new prod-uct in a product line is challenging due to several reasons. First, the variability within the product line needs to be cap-tured in a systematic way; second, the reusable test cases from the repository are required to be identiﬁed for testing a new product. The objective of such automated process is to reduce the overall effort for selection (e.g., selection time), while achieving an acceptable level of the coverage of test-ing functionalities. In this paper, we propose a systematic and automated methodology using a feature model for testing (FM_T) to capture commonalities and variabilities of a prod-uct line and a component family model for testing (CFM_T) to capture the overall structure of test cases in the reposi-tory. With our methodology, a test engineer does not need to manually go through the repository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM_T at a higher level of abstraction for a product and a set of rele-vant test cases will be selected automatically. We evaluated our methodology via three different ways: (1) We applied our methodology to a product line of video conferencing systems called Saturn developed by Cisco, and the results show that our methodology can reduce the selection effort signiﬁcantly; (2) we conducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM_T and CFM_T. The results show that test engineers are positive about adapting our methodology and models (FM_T and CFM_T) in their current practice; (3) we conducted a controlled experiment with 20 graduate students to assess the performance (i.e., cost, effectiveness and efﬁ-ciency) of our automated methodology as compared to the manual approach. The results showed that our methodology is cost-effective as compared to the manual approach, and at the same time, its efﬁciency is not affected by the increased complexity of products.",
        "keywords": [
            "Test case selection",
            "Product line",
            "Feature model",
            "Component family model"
        ],
        "authors": [
            "Shuai Wang",
            "Shaukat Ali",
            "Arnaud Gotlieb",
            "Marius Liaaen"
        ],
        "file_path": "data/sosym-all/s10270-015-0462-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Correction: Cost-sensitive precomputation of real-time-aware reconﬁguration strategies based on stochastic priced timed games",
        "submission-date": "2024/08",
        "publication-date": "2025/02",
        "abstract": "The article “Cost-sensitive precomputation of real-time-aware reconﬁguration strategies based on stochastic priced timed games”, written by Hendrik Göttmann, Birte Caesar, Lasse Beers, Malte Lochau, Andy Schürr and Alexander Fay, was originally published electronically on the publisher’s internet portal on 5 August 2024 without open access.",
        "keywords": [],
        "authors": [
            "Hendrik Göttmann",
            "Birte Caesar",
            "Lasse Beers",
            "Malte Lochau",
            "Andy Schürr",
            "Alexander Fay"
        ],
        "file_path": "data/sosym-all/s10270-025-01269-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Verifying B proof rules using deep embedding and automated theorem proving",
        "submission-date": "2012/03",
        "publication-date": "2013/06",
        "abstract": "We propose a formal and mechanized framework which consists in verifying proof rules of the B method, which cannot be automatically proved by the elementary prover of Atelier B and using an external automated theorem prover called Zenon. This framework contains in particular a set of tools, named BCARe and developed by Siemens IC-MOL, which relies on a deep embedding of the B theory withinthelogicofthe Coqproofassistant.Thistoolkitallows us to automatically generate the required properties to be checked for a given proof rule. Currently, this tool chain is able to automatically verify a part of the derived rules of the B-Book, as well as some added rules coming from Atelier B and the rule database maintained by Siemens IC-MOL.",
        "keywords": [
            "B Method",
            "Proof rules",
            "Veriﬁcation",
            "Deep embedding",
            "Automated theorem proving",
            "Coq",
            "Zenon"
        ],
        "authors": [
            "Mélanie Jacquel",
            "Karim Berkani",
            "David Delahaye",
            "Catherine Dubois"
        ],
        "file_path": "data/sosym-all/s10270-013-0322-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A component framework for system modeling based on high-level replacement systems",
        "submission-date": "2003/01",
        "publication-date": "2004/04",
        "abstract": "The aim of this paper is to present a generic component framework for system modeling that satis-ﬁes main requirements for component-based development in software engineering. In this sense, we have deﬁned a framework that can be used, by providing an adequate instantiation, in connection with a large class of semi-formal and formal modeling techniques. Moreover, the framework is also ﬂexible with respect to the connection of components, providing a compositional semantics of components. This means more precisely that the semantics of a system can be inferred from the semantics of its components. In contrast to other component concepts for data type speciﬁcation techniques, our component framework is based on a generic notion of transformations. In particular, reﬁnements and transformations are used to express intradependencies, between the export interface and the body of a component, and interdependencies, between the import and the export interfaces of diﬀerent components. The generic component framework gener-alizes module concepts for diﬀerent kinds of Petri nets and graph transformation systems proposed in the liter-ature, and seems to be also suitable for visual modeling techniques, including parts of the UML, if these tech-niques provide a suitable reﬁnement or transformation concept. In this paper the generic approach is instanti-ated in two steps. First to high-level replacement systems generalizing the transformation concept of graph trans-formations. In a second step it is further instantiated to low-level and high-level Petri nets. To show applica-bility we present sample components from a case study in the domain of production automation as proposed in a priority program of the German Research Coun-cil (DFG).",
        "keywords": [
            "Components",
            "Formal semantics",
            "Generic framework",
            "Petri nets"
        ],
        "authors": [
            "Hartmut Ehrig",
            "Fernando Orejas",
            "Benjamin Braatz",
            "Markus Klein",
            "Martti Piirainen"
        ],
        "file_path": "data/sosym-all/s10270-003-0043-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models for formal methods and tools: the case of railway systems",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "Formal methods and tools are successfully applied to the development of safety-critical systems for decades now, in particular in the transport domain, without a single technique or tool emerging as the dominant solution for system design. Formal methods are highly recommended by the existing safety standards in the railway industry, but railway engineers typically lack the knowledge to transform their semi-formal models into a formal model, with a precise semantics, that can serve as input to formal methods tools. We share the results of performing empirical studies in the ﬁeld, including usability analyses of formal methods tools involving railway practitioners. We discuss, in particular with respect to railway systems and their modelling, our experiences in applying formal methods and tools to a variety of case studies, for which we interacted with a number of companies from the railway domain. We report on lessons learned from these experiences and provide pointers to steer future research towards facilitating further synergies between researchers and developers of formal methods and tools on the one hand and practitioners from the railway industry on the other.",
        "keywords": [
            "Formal methods",
            "Models",
            "Railway systems"
        ],
        "authors": [
            "M. H. ter Beek"
        ],
        "file_path": "data/sosym-all/s10270-025-01276-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven generative development of measurement software",
        "submission-date": "2009/10",
        "publication-date": "2010/06",
        "abstract": "Metrics offer a practical approach to evaluate properties of domain-speciﬁc models. However, it is costly to develop and maintain measurement software for each domain-speciﬁc modeling language. In this paper, we present a model-driven and generative approach to measuring models. The approach is completely domain-independent and operationalized through a prototype that synthesizes a measurement infrastructure for a domain-speciﬁc modeling language. This model-driven measurement approach is model-driven from two viewpoints: (1) it measures models of a domain-speciﬁc modeling language; (2) it uses models as unique and consistent metric speciﬁcations, with respect to a metric speciﬁcation metamodel which captures all the necessary concepts for model-driven speciﬁcations of metrics. The beneﬁt from applying the approach is evaluated by four case studies. They indicate that this approach significantly eases the measurement activities of model-driven development processes.",
        "keywords": [],
        "authors": [
            "Martin Monperrus",
            "Jean-Marc Jézéquel",
            "Benoit Baudry",
            "Joël Champeau",
            "Brigitte Hoeltzener"
        ],
        "file_path": "data/sosym-all/s10270-010-0165-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Early timing analysis based on scenario requirements and platform models",
        "submission-date": "2020/11",
        "publication-date": "2022/04",
        "abstract": "Distributed, software-intensive systems (e.g., in the automotive sector) must fulﬁll communication requirements under hard real-time constraints. The requirements have to be documented and validated carefully using a systematic requirements engineering (RE) approach, for example, by applying scenario-based requirements notations. The resources of the execution platforms and their properties (e.g., CPU frequency or bus throughput) induce effects on the timing behavior, which may lead to violations of the real-time requirements. Nowadays, the platform properties and their induced timing effects are veriﬁed against the real-time requirements by means of timing analysis techniques mostly implemented in commercial-off-the-shelf tools. However, such timing analyses are conducted in late development phases since they rely on artifacts produced during these phases (e.g., the platform-speciﬁc code). In order to enable early timing analyses already during RE, we extend a scenario-based requirements notation with allocation means to platform models and deﬁne operational semantics for the purpose of simulation-based, platform-aware timing analyses. We illustrate and evaluate the approach with an automotive software-intensive system.",
        "keywords": [
            "Scenario-based requirements",
            "Platform modeling",
            "Real-time systems",
            "Timing analysis"
        ],
        "authors": [
            "Jörg Holtmann",
            "Julien Deantoni",
            "Markus Fockel"
        ],
        "file_path": "data/sosym-all/s10270-022-01002-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A semi-automated BPMN-based framework for detecting conflicts between security, data-minimization, and fairness requirements",
        "submission-date": "2019/02",
        "publication-date": "2020/02",
        "abstract": "Requirements are inherently prone to conflicts. Security, data-minimization, and fairness requirements are no exception. Importantly, undetected conflicts between such requirements can lead to severe effects, including privacy infringement and legal sanctions. Detecting conflicts between security, data-minimization, and fairness requirements is a challenging task, as such conflicts are context-specific and their detection requires a thorough understanding of the underlying business processes. For example, a process may require anonymous execution of a task that writes data into a secure data storage, where the identity of the writer is needed for the purpose of accountability. Moreover, conflicts not arise from trade-offs between requirements elicited from the stakeholders, but also from misinterpretation of elicited requirements while implementing them in business processes, leading to a non-alignment between the data subjects’ requirements and their specifications. Both types of conflicts are substantial challenges for conflict detection. To address these challenges, we propose a BPMN-based framework that supports: (i) the design of business processes considering security, data-minimization and fairness requirements, (ii) the encoding of such requirements as reusable, domain-specific patterns, (iii) the checking of alignment between the encoded requirements and annotated BPMN models based on these patterns, and (iv) the detection of conflicts between the specified requirements in the BPMN models based on a catalog of domain-independent anti-patterns. The security requirements were reused from SecBPMN2, a security-oriented BPMN 2.0 extension, while the fairness and data-minimization parts are new. For formulating our patterns and anti-patterns, we extended a graphical query language called SecBPMN2-Q. We report on the feasibility and the usability of our approach based on a case study featuring a healthcare management system, and an experimental user study.",
        "keywords": [
            "Conflicts",
            "Requirements engineering",
            "Security",
            "Data minimization",
            "Fairness",
            "BPMN"
        ],
        "authors": [
            "Qusai Ramadan",
            "Daniel Strüber",
            "Mattia Salnitri",
            "Jan Jürjens",
            "Volker Riediger",
            "Steffen Staab"
        ],
        "file_path": "data/sosym-all/s10270-020-00781-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Welcome to the second issue, and the ﬁrst special issue of the Software and System Modeling (SoSyM) journal",
        "submission-date": "2001",
        "publication-date": "2002/12",
        "abstract": "This special issue contains extended and improved versions of the best papers presented at the fourth International Conference of the Uniﬁed Modeling Language, << UML >> 2001. The extended forms of the papers published in this issue showcase some of the innovative and high quality work that researchers in the UML community are undertaking. This issue also contains an invited paper by Grady Booch about the future of software engineering and modeling techniques.",
        "keywords": [],
        "authors": [
            "Martin Gogolla",
            "Grady Booch"
        ],
        "file_path": "data/sosym-all/s10270-002-0014-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incremental model transformations with triple graph grammars for multi-version models and multi-version pattern matching",
        "submission-date": "2024/04",
        "publication-date": "Not found",
        "abstract": "Like conventional software projects, projects in model-driven software engineering require adequate management of multiple versions of development artifacts, importantly allowing living with temporary inconsistencies. In previous work, we have introduced multi-version models for model-driven software engineering, which allow checking well-formedness and finding merge conflicts for multiple versions of the same model at once. However, situations where different models are linked via automatic model transformations also have to be handled for multi-version models. In this paper, we propose a technique for jointly handling the transformation of multiple versions of a source model into corresponding versions of a target model. This enables the use of a more compact representation that may afford improved execution time of both the transformation and further analysis. Our approach is based on the well-known formalism of triple graph grammars and the aforementioned encoding of model version histories called multi-version models. In addition to batch transformation of an entire history, the technique covers incremental synchronization of changes in the framework of multi-version models. Our solution is complemented by a dedicated pattern matching technique for multi-version models. We show the correctness of our approach with respect to the standard semantics of triple graph grammars and conduct an empirical evaluation to investigate the performance of our technique regarding execution time and memory consumption. Our results indicate that the proposed solution affords lower memory consumption and may improve execution time for batch transformation of large version histories, but can also come with computational overhead in unfavorable cases.",
        "keywords": [
            "Multi-version models",
            "Triple graph grammars",
            "Incremental model transformation",
            "Graph pattern matching"
        ],
        "authors": [
            "Matthias Barkowsky",
            "Holger Giese"
        ],
        "file_path": "data/sosym-all/s10270-024-01238-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Multi-view approaches for software and system modelling: a systematic literature review",
        "submission-date": "2018/02",
        "publication-date": "2019/02",
        "abstract": "Over the years, a number of approaches have been proposed on the description of systems and software in terms of mul- tiple views represented by models. This modelling branch, so-called multi-view software and system modelling, praises a differentiated and complex scientiﬁc body of knowledge. With this study, we aimed at identifying, classifying, and evaluating existing solutions for multi-view modelling of software and systems. To this end, we conducted a systematic literature review of the existing state of the art related to the topic. More speciﬁcally, we selected and analysed 40 research studies among over 8600 entries. We deﬁned a taxonomy for characterising solutions for multi-view modelling and applied it to the selected studies. Lastly, we analysed and discussed the data extracted from the studies. From the analysed data, we made several observations, among which: (i) there is no uniformity nor agreement in the terminology when it comes to multi-view artefact types, (ii) multi-view approaches have not been evaluated in industrial settings and (iii) there is a lack of support for semantic consistency management and the community does not appear to consider this as a priority. The study results provide an exhaustive overview of the state of the art for multi-view software and systems modelling useful for both researchers and practitioners.",
        "keywords": [
            "Model-driven engineering",
            "Multi-view modelling",
            "Viewpoints",
            "Views",
            "Consistency"
        ],
        "authors": [
            "Antonio Cicchetti",
            "Federico Ciccozzi",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-018-00713-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Platform independent Web application modeling and development with Netsilon",
        "submission-date": "2004/05",
        "publication-date": "2005/06",
        "abstract": "This paper discusses platform independent Web application modeling and development in the context of model-driven engineering. A specific metamodel (and associated notation) is introduced and motivated for the modeling of dynamic Web specific concerns. Web applications are represented via three independent but related models (business, hypertext and presentation). A kind of action language (based on OCL and Java) is used all over these models to write methods and actions, specify constraints and express conditions. The concepts described in the paper have been implemented in the Netsilon tool and operational model-driven Web information systems have been successfully deployed by translation from abstract models to platform specific models.",
        "keywords": [
            "MDA",
            "PIMs",
            "PSMs",
            "Web application development"
        ],
        "authors": [
            "Pierre-Alain Muller",
            "Philippe Studer",
            "Fr´ed´eric Fondement",
            "Jean Bezivin"
        ],
        "file_path": "data/sosym-all/s10270-005-0091-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Validating UML and OCL models in USE by automatic snapshot generation",
        "submission-date": "2004/05",
        "publication-date": "2005/06",
        "abstract": "We study the testing and certiﬁcation of UML and OCL models as supported by the validation tool USE. We extend the available USE features by introducing a language for deﬁning properties of desired snapshots and by showing how such snapshots are generated. Within the approach, it is possible to treat test cases and validation cases. Test cases show that snapshots having desired properties can be constructed. Validation cases show that given properties are consequences of the original UML and OCL model.",
        "keywords": [
            "UML",
            "OCL",
            "Model validation",
            "Model testing",
            "Reasoning about models",
            "Class diagram",
            "Invariant",
            "Pre- and postcondition",
            "Test case",
            "Snapshot"
        ],
        "authors": [
            "Martin Gogolla",
            "Jörn Bohling",
            "Mark Richters"
        ],
        "file_path": "data/sosym-all/s10270-005-0089-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on model transformation",
        "submission-date": "2008/10",
        "publication-date": "2009/11",
        "abstract": "Models have become essential for dealing with the numerous aspects involved in developing and maintaining complex IT systems. Models assist in capturing the relevant aspects of a system from a given perspective and at a precise level of abstraction. Model transformation represents a key activity in model-driven engineering by supporting the definition and implementation of the operations on models, which can provide a chain that enables the automated development of a system from its corresponding models. Furthermore, a model transformation may also be considered a model in its own right, which presents opportunities for higher-order transformations, i.e., transformations that manipulate models representing other model transformations. The objective of this special section is to provide a representative sample of advanced research emerging from the field of model transformation. The selected papers provide an overview of current open issues and identify potential lines for further research.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Alfonso Pierantonio",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-009-0139-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating the appropriateness of the BPMN 2.0 standard for modeling service choreographies: using an extended quality framework",
        "submission-date": "2012/06",
        "publication-date": "2014/03",
        "abstract": "The concept of choreography has emerged over the past years as a foundational concept for capturing and managing collaborative business processes. The latest version of the Business Process Modeling Notation (BPMN 2.0) adopts choreography as a first-class citizen. However, it remains an open question whether BPMN 2.0 is actually appropriate for capturing this concept. In this paper, we extend an existing language evaluation framework in order to evaluate the support for choreographies in BPMN 2.0. The framework provides a means of identifying the strengths and weaknesses of BPMN 2.0 for choreographies. We also give potential solutions that may be taken into account in future extensions or improvements to BPMN 2.0.",
        "keywords": [
            "Service choreographies",
            "Language quality framework",
            "BPMN 2.0",
            "Evaluation"
        ],
        "authors": [
            "Mario Cortes-Cornax",
            "Sophie Dupuy-Chessa",
            "Dominique Rieu",
            "Nadine Mandran"
        ],
        "file_path": "data/sosym-all/s10270-014-0398-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven management of BPMN-based business process families",
        "submission-date": "2020/12",
        "publication-date": "2022/03",
        "abstract": "Business processes can have variants depending on speciﬁc business requirements, which lead to the deﬁnition of a so-called\nbusiness process family. Since conventional business process modeling languages, e.g., the Business Process Model and\nNotation (BPMN), do not explicitly support variants’ speciﬁcation, several proposals have emerged to deal with it. However,\nthey mainly focus on languages’ deﬁnition, while less emphasis is made on providing complete variability management. This\narticle presents a Model-Driven Engineering approach for managing BPMN-based business process families composed of a\nmetamodel for conceptualizing process families, a high-level process for managing them (involving model transformations\nfor the conﬁguration of variants), and tool support for the complete approach. We validated the proposal using a real-world\nexample from a university and an empirical study with real users. Users rated the support tool’s principal functional suitability\nand usability features as very good. Many improvement opportunities were detected, e.g., version control, collaborative work,\nand error reporting. We also provide a literature review and thorough evaluation of BPMN-based business process families’\nproposals using the VIVACE framework.",
        "keywords": [
            "Business process families",
            "Variability",
            "BPMN 2.0",
            "Model-Driven Engineering",
            "VIVACE"
        ],
        "authors": [
            "Andrea Delgado",
            "Daniel Calegari",
            "Félix García",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-022-00985-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Dynamic hierarchical mega models: comprehensive traceability and its efﬁcient maintenance",
        "submission-date": "2009/01",
        "publication-date": "2009/12",
        "abstract": "In the world of model-driven engineering (MDE) support for traceability and maintenance of traceability infor-mation is essential. On the one hand, classical traceability approaches for MDE address this need by supporting auto-mated creation of traceability information on the model ele-ment level. On the other hand, global model management approaches manually capture traceability information on the model level. However, there is currently no approach that supports comprehensive traceability, comprising traceabil-ity information on both levels, and efﬁcient maintenance of traceability information, which requires a high-degree of automation and scalability. In this article, we present a com-prehensive traceability approach that combines classical traceability approaches for MDE and global model man-agement in form of dynamic hierarchical mega models. We further integrate efﬁcient maintenance of traceability infor-mation based on top of dynamic hierarchical mega models. The proposed approach is further outlined by using an indus-trial case study and by presenting an implementation of the concepts in form of a prototype.",
        "keywords": [
            "Model-driven engineering",
            "Traceability maintenance",
            "Global model management",
            "Mega model"
        ],
        "authors": [
            "Andreas Seibel",
            "Stefan Neumann",
            "Holger Giese"
        ],
        "file_path": "data/sosym-all/s10270-009-0146-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based assurance evidence management for safety–critical systems",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Most safety–critical systems are subject to rigorous assurance processes to justify that the systems satisfy given requirements and are dependable. These processes are typically conducted in compliance with standards and require the provision of assurance evidence in the form of system artifacts, such as system speciﬁcations and testing results. The management of assurance evidence is usually a complex process because of the large number of artifacts to deal with, the amount of information to gather about the artifacts, and the need to guarantee evidence quality, among other issues. Our aim is to facilitate assurance evidence management by means of a model-based approach. The approach is based on a metamodel that deﬁnes the information to be collected about evidence artifacts during their lifecycle. A process for assurance evidence management and usage guidance are also presented. The approach has been developed in the scope of several industry-academia projects, implemented in the OpenCert tool, and validated by practitioners in 10 industrial case studies. Based on the results of the validation, we argue that the approach is an effective means for assurance evidence management and that it could improve the state of the practice.",
        "keywords": [
            "Assurance evidence",
            "Safety",
            "critical systems",
            "System assurance",
            "System certiﬁcation",
            "Model-Driven Engineering",
            "OpenCert"
        ],
        "authors": [
            "Jose Luis de la Vara",
            "Arturo S. García",
            "Jorge Valero",
            "Clara Ayora"
        ],
        "file_path": "data/sosym-all/s10270-021-00957-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Integration of DFDs into a UML-based model-driven engineering approach",
        "submission-date": "2004/12",
        "publication-date": "2006/06",
        "abstract": "The main aim of this article is to discuss how the functional and the object-oriented views can be inter-played to represent the various modeling perspectives of embedded systems. We discuss whether the object-oriented modeling paradigm, the predominant one to develop software at the present time, is also adequate for modeling embedded software and how it can be used with the functional paradigm. More specifically, we present how the main modeling tool of the traditional structured methods, data ﬂow diagrams, can be integrated in an object-oriented development strategy based on the uniﬁed modeling language. The rationale behind the approach is that both views are important for modeling purposes in embedded systems environments, and thus a combined and integrated model is not only useful, but also fundamental for developing complex systems. The approach was integrated in a model-driven engineering process, where tool support for the models used was provided. In addition, model transformations have been speciﬁed and implemented to automate the process. We exemplify the approach with an IPv6 router case study.",
        "keywords": [
            "MDA",
            "UML",
            "Data-ﬂow Diagram",
            "Model Transformation",
            "Embedded Systems Speciﬁcation",
            "Process Model",
            "Activity Diagram",
            "IPv6 router"
        ],
        "authors": [
            "João M. Fernandes",
            "Johan Lilius",
            "Dragos Truscan"
        ],
        "file_path": "data/sosym-all/s10270-006-0013-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Speciﬁcation and automated veriﬁcation of atomic concurrent real-time transactions",
        "submission-date": "2019/09",
        "publication-date": "2020/07",
        "abstract": "Many database management systems (DBMS) need to ensure atomicity and isolation of transactions for logical data consistency, as well as to guarantee temporal correctness of the executed transactions. Since the mechanisms for atomicity and isolation may lead to breaching temporal correctness, trade-offs between these properties are often required during the DBMS design. To be able to address this concern, we have previously proposed the pattern-based UPPCART framework, which models the transactions and the DBMS mechanisms as timed automata, and veriﬁes the trade-offs with provable guarantee. However, the manual construction of UPPCART models can require considerable effort and is prone to errors. In this paper, we advance the formal analysis of atomic concurrent real-time transactions with tool-automated construction of UPPCART models. The latter are generated automatically from our previously proposed UTRAN speciﬁcations, which are high-level UML-based speciﬁcations familiar to designers. To achieve this, we ﬁrst propose formal deﬁnitions for the modeling patterns in UPPCART, as well as for the pattern-based construction of DBMS models, respectively. Based on this, we establish a translational semantics from UTRAN speciﬁcations to UPPCART models, to provide the former with a formal semantics relying on timed automata, and develop a tool that implements the automated transformation. We also extend the expressiveness of UTRAN and UPPCART, to incorporate transaction sequences and their timing properties. We demonstrate the speciﬁcation in UTRAN, automated transformation to UPPCART, and veriﬁcation of the traded-off properties, via an industrial use case.",
        "keywords": [
            "Transaction",
            "Atomicity",
            "Isolation",
            "Temporal correctness",
            "Uniﬁed modeling language",
            "Model checking"
        ],
        "authors": [
            "Simin Cai",
            "Barbara Gallina",
            "Dag Nyström",
            "Cristina Seceleanu"
        ],
        "file_path": "data/sosym-all/s10270-020-00819-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Method engineering in information systems analysis and design: a balanced scorecard approach for method improvement",
        "submission-date": "2017/09",
        "publication-date": "2018/08",
        "abstract": "Modeling methods have been proven to provide beneﬁcial instrumental support for different modeling tasks during informa-tion system analysis and design. However, methods are a complex phenomenon that include constructs such as procedural guidelines, concepts to focus on, visual representations and cooperation principles. In general, method development is an expensive task that usually involves many stakeholders and results in various method iterations. Since methods and method development are complex in nature, there is a need for a well-structured and resource-efﬁcient approach for method improve-ment. This paper aims to contribute to the ﬁeld of method improvement by proposing a balanced scorecard-based approach and by reporting on experiences from developing and using it in the context of a method for information demand analysis. The main contributions of the paper are as follows: (1) It provides a description of the process for developing a scorecard for method improvement; (2) it shows how the scorecard as such can be used as a tool for improving a speciﬁc method; and (3) it discusses experiences from applying the scorecard in industrial settings.",
        "keywords": [
            "Method improvement",
            "Balanced scorecard",
            "Method engineering",
            "Information demand analysis method"
        ],
        "authors": [
            "Kurt Sandkuhl",
            "Ulf Seigerroth"
        ],
        "file_path": "data/sosym-all/s10270-018-0692-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Lessons learned from building model-driven development tools",
        "submission-date": "2011/11",
        "publication-date": "2012/07",
        "abstract": "Tools to support modelling in system and software engineering are widespread, and have reached a degree of maturity where their use and availability are accepted. Tools to support model-driven development (MDD)—where models are manipulated and managed throughout the system/software engineering lifecycle—have, over the last 10 years, seen much research and development attention. Over the last 10 years, we have had significant experience in the design, development and deployment of MDD tools in practical settings. In this paper, we distill some of the important lessons we have learned in developing and deploying two MDD tools: Epsilon and VIATRA. In doing so, we aim to identify some of the key principles of developing successful MDD tools, as well as some hints of the pitfalls and risks.",
        "keywords": [
            "Model-driven development",
            "Model management",
            "MDD tools",
            "Model tansformation"
        ],
        "authors": [
            "Richard F. Paige",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-012-0257-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Epsilon, VIATRA"
        }
    },
    {
        "title": "MDI: a rule-based multi-document and tool integration approach",
        "submission-date": "2004/11",
        "publication-date": "2006/06",
        "abstract": "Nowadays,typicalsoftwareandsystemengineer-ing projects in various industrial sectors (automotive, tele-communication, etc.) involve hundreds of developers using quite a number of different tools. Thus, the data of a pro-ject as a whole is distributed over these tools. Therefore, it is necessary to make the relationships of different tool data repositories visible and keep them consistent with each other. This still is a nightmare due to the lack of domain-speciﬁc adaptable tool and data integration solutions which support maintenance of traceability links, semi-automatic consistency checking as well as incremental update prop-agation. Currently used solutions are usually hand-coded one-way transformations between pairs of tools only. In this article we propose a new rule-based approach that allows for thedeclarativespeciﬁcationofdataintegrationrulesconcern-ing multiple data repositories. Hence, we call our approach “Multi Document Integration”. It generalizes the formalism of triple graph grammars and replaces the underlying data structureofdirectedgraphsbythemoregeneraldatastructure of MOF-compliant meta models. Our integration rule speci-ﬁcations are translated into JMI-compliant Java code which is used for various purposes by a tool integration framework. As a result we give an answer to OMG’s request for proposals for a MOF-compliant “queries, views, and transformation” approach from the “model driven application development” (MDA) ﬁeld.",
        "keywords": [],
        "authors": [
            "Alexander Königs",
            "Andy Schürr"
        ],
        "file_path": "data/sosym-all/s10270-006-0016-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Classification and trend analysis of UML books (1997–2009)",
        "submission-date": "2010/04",
        "publication-date": "2011/02",
        "abstract": "Technical books of each subject area denote the levelofmaturityandknowledgedemandinthatarea.Accord-ing to the Google Books database, about 208 Uniﬁed Mod-eling Language (UML) books have been published from itsinception in 1997 until 2009. While various book reviewsare frequently published in various sources (e.g., IEEE Soft-ware Bookshelf), there are no studies to classify UML booksinto meaningful categories. Such a classiﬁcation can helpresearchers in the area to identify trends and also reveal thelevel of activity in each sub-area of UML. The statistical sur-vey reported in this article intends to be a ﬁrst step in classiﬁ-cation and trend analysis of the UML books published from1997 to 2009. The study also sheds light on the quantity ofbooks published in different focus areas (e.g., UML’s coreconcepts, patterns, tool support, Object Constraint Languageand Model-Driven Architecture) and also on different appli-cation domains (e.g., database modeling, web applications,and real-time systems). The trends of book publications ineach sub-area of UML are also used to track the level ofmaturity, to identify possible Hype cycles and also to mea-sure knowledge demand in each area.",
        "keywords": [
            "Survey",
            "Statistical study",
            "Classification",
            "Trend analysis",
            "UML",
            "Books"
        ],
        "authors": [
            "Vahid Garousi"
        ],
        "file_path": "data/sosym-all/s10270-011-0189-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Implementation of a continuous delivery pipeline for enterprise architecture model evolution",
        "submission-date": "2019/09",
        "publication-date": "2020/09",
        "abstract": "The discipline of enterprise architecture (EA) is an established approach to model and manage the interaction of business processes and IT in an organization. Thereby, the EA model as a central artifact of EA is subject to a continuous evolution caused by multiple sources of changes. The continuous evolution requires a lot of effort in controlling and managing the evolution of the EA model. This is especially true when merging the induced changes from different sources in the EA model. Additionally, the lack of tool and automation support makes this a very time-consuming and error-prone task. The evolutionary character and the automated quality assessment of artifacts is a well-known challenge in the software development domain as well. To meet these challenges, the discipline of continuous delivery (CD) has emerged to be very useful. The evolution of EA model artifacts shows similarities to the evolution of software artifacts. Therefore, we leveraged practices of CD to practices of EA maintenance. Thus, we created a conceptual framework for automated EA model maintenance. The concepts were realized in a first prototype and were evaluated in a fictitious case study against equivalence classes based on EA model metrics and a set of several requirements for automated EA model maintenance from research. Overall, the concepts prove to be a promising basis for further refinement, implementation, and evaluation in research in an industrial context.",
        "keywords": [
            "Enterprise architecture model evolution",
            "Continuous delivery",
            "Enterprise architecture model maintenance"
        ],
        "authors": [
            "Alex R. Sabau",
            "Simon Hacks",
            "Andreas Steﬀens"
        ],
        "file_path": "data/sosym-all/s10270-020-00828-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A local and global tour on MOMoT",
        "submission-date": "2016/11",
        "publication-date": "2017/12",
        "abstract": "Many model transformation scenarios require ﬂexible execution strategies as they should produce models with the highest possible quality. At the same time, transformation problems often span a very large search space with respect to possible transformation results. Recently, different proposals for ﬁnding good transformation results without enumerating the complete search space have been proposed by using meta-heuristic search algorithms. However, determining the impact of the different kinds of search algorithms, such as local search or global search, on the transformation results is still an open research topic. In this paper, we present an extension to MOMoT, which is a search-based model transformation tool, for supporting not only global searchers for model transformation orchestrations, but also local ones. This leads to a model transformation framework that allows as the ﬁrst of its kind multi-objective local and global search. By this, the advantages and disadvantages of global and local search for model transformation orchestration can be evaluated. This is done in a case-study-based evaluation, which compares different performance aspects of the local- and global-search algorithms available in MOMoT. Several interesting conclusions have been drawn from the evaluation: (1) local-search algorithms perform reasonable well with respect to both the search exploration and the execution time for small input models, (2) for bigger input models, their execution time can be similar to those of global-search algorithms, but global-search algorithms tend to outperform local-search algorithms in terms of search exploration, (3) evolutionary algorithms show limitations in situations where single changes of the solution can have a signiﬁcant impact on the solution’s ﬁtness.",
        "keywords": [
            "Model-driven engineering",
            "Model transformation",
            "Search-based software engineering",
            "Local search",
            "Global search"
        ],
        "authors": [
            "Robert Bill",
            "Martin Fleck",
            "Javier Troya",
            "Tanja Mayerhofer",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-017-0644-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest Editorial to the Special Section on MODELS 2007",
        "submission-date": "2007/09",
        "publication-date": "2009/11",
        "abstract": "This section of “Software & Systems Modeling” contains four selected papers of the tenth MODELS conference. MODELS 2007 was held in Nashville, TN, USA, from September 30 to October 5, 2007. This conference on Model Driven Engineering Languages and Systems has established itself as the key international venue for the presentation of scientific results in the domain of model driven engineering and related topics such as software modeling and model transformation.",
        "keywords": [],
        "authors": [
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-009-0140-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Securing critical infrastructures with a cybersecurity digital twin",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "With the diffusion of integrated design environments and tools for visual threat modeling for critical infrastructures, the concept of Digital Twin (DT) is gaining momentum in the ﬁeld of cybersecurity. Its main use is for enabling attack simulations and evaluation of countermeasures, without causing outage of the physical system. However, the use of a DT is considered foremost as a facilitator of system operation rather than an integral part of its architecture design. In this work, we introduce a speciﬁc architecture view in the system representation, called Cybersecurity View. From it, we derive a cybersecurity Digital Twin as part of the security-by-design practice for Industrial Automation and Control Systems used in Critical Infrastructures. Not only this digital twin serves the purpose of simulating cyber-attacks and devising countermeasures, but its design and function are also directly tied to the architecture model of the system for which the cybersecurity requirements are posed. Moreover, this holds regardless of whether the model is generated as part of the development cycle or through an empirical observation of the system as-is. With this, we enable the identiﬁcation of adequate cybersecurity measures for the system, while improving the overall system design. To demonstrate the practical usefulness of the proposed methodology, its application is illustrated through two real-world use cases: the Cooperative Intelligent Transport System (C-ITS) and the Road tunnel scenario.",
        "keywords": [
            "Enterprise architecture",
            "Reference architecture",
            "Cybersecurity view",
            "Digital twin",
            "Threat modeling",
            "Critical infrastructure",
            "Transportation"
        ],
        "authors": [
            "Massimiliano Masi",
            "Giovanni Paolo Sellitto",
            "Helder Aranha",
            "Tanja Pavleska"
        ],
        "file_path": "data/sosym-all/s10270-022-01075-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Efﬁcient regression testing of distributed real-time reactive systems in the context of model-driven development",
        "submission-date": "2022/03",
        "publication-date": "2023/03",
        "abstract": "Regression testing is indispensable, especially for real-time distributed systems to ensure that existing functionalities are not affected by changes. Despite recent advances, regression testing for distributed systems remains challenging and extremely costly. Existing techniques often require running a failing system several times before detecting a regression. As a result, conventional approaches that use re-execution without considering the inherent non-determinism of distributed systems, and providing no (or low) control over execution are inadequate in many ways. In this paper, we present MRegTest, a replay-based regression testing framework in the context of model-driven development to facilitate deterministic replay of traces for detecting regressions while offering sufﬁcient control for the purpose of testing over the execution of the changed system. The experimental results show that compared to the traditional approaches that annotate traces with timestamps and variable values MRegTest detects almost all regressions while reducing the size of the trace signiﬁcantly and incurring similar runtime overhead.",
        "keywords": [
            "MDD",
            "Distributed systems",
            "Regression testing"
        ],
        "authors": [
            "Majid Babaei",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-023-01086-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "RaQuN: a generic and scalable n-way model matching algorithm",
        "submission-date": "2022/03",
        "publication-date": "2022/11",
        "abstract": "Model matching algorithms are used to identify common elements in input models, which is a fundamental precondition for\nmany software engineering tasks, such as merging software variants or views. If there are multiple input models, an n-way\nmatching algorithm that simultaneously processes all models typically produces better results than the sequential application\nof two-way matching algorithms. However, existing algorithms for n-way matching do not scale well, as the computational\neffort grows fast in the number of models and their size. We propose a scalable n-way model matching algorithm, which\nuses multi-dimensional search trees for efﬁciently ﬁnding suitable match candidates through range queries. We implemented\nour generic algorithm named RaQuN (Range Queries on N input models) in Java and empirically evaluate the matching\nquality and runtime performance on several datasets of different origins and model types. Compared to the state of the art,\nour experimental results show a performance improvement by an order of magnitude, while delivering matching results of\nbetter quality.",
        "keywords": [
            "Model-driven engineering",
            "n-Way model matching",
            "Clone-and-own development",
            "Software product lines"
        ],
        "authors": [
            "Alexander Schultheiß\nPaul Maximilian Bittner\nAlexander Boll\nLars Grunske\nThomas Thüm\nTimo Kehrer"
        ],
        "file_path": "data/sosym-all/s10270-022-01062-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "In memory of Bernhard Schätz, long- time friend and SoSyM editor",
        "submission-date": "2018/01",
        "publication-date": "2018/01",
        "abstract": "This paper is an editorial commemorating Bernhard Schätz, highlighting his contributions to science, research, and the Software and Systems Engineering (SSE) group. It discusses his dedication, skills, and impact on tool development, including AutoFocus, MontiCore, and MontiArc.",
        "keywords": [],
        "authors": [
            "Manfred Broy",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-018-0657-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "R-IO SUITE: integration of LLM-based AI into a knowledge management and model-driven based platform dedicated to crisis management",
        "submission-date": "2024/03",
        "publication-date": "Not found",
        "abstract": "This article presents how the R-IO SUITE software platform, a decision support system for crisis management entirely based on model-driven engineering principles, considerably beneﬁts from large language model (LLM)-based artiﬁcial intelligence (AI). The different components of the R-IO SUITE platform are used to climb the four abstraction layers: data, informa-tion, decision and action through interpretation (from data to information), exploitation (from information to decision) and implementation (from decision to action). These transitions between layers are supported by a knowledge base embedding knowledge instances structured according to a crisis management metamodel. From a functional perspective, this platform is fully operational, however, to be able to cover any type of crisis situation, the knowledge base should be enriched, ﬁrst, from a “resource perspective” (to embed the various available means to deal with any faced situation), and second, from an “issue perspective” (to understand all risks and damage that can appear on a crisis situation). It is not reasonable to consider creating and maintaining such an exhaustive knowledge base. However, the connection of the R-IO SUITE platform with LLM software such as ChatGPT© makes it possible, by generating appropriate prompts, to update on-the-ﬂy the knowledge base according to the faced context. This article shows how the LLM AI can provide complementary knowledge to formally fulﬁl the knowledge base to make it relevant to the faced crisis situation. This article presents the R-IO SUITE as a LLM-empowered model-driven platform to become an extended crisis management supporting system.",
        "keywords": [
            "Large language model",
            "Artiﬁcial intelligence",
            "Model-driven engineering",
            "Decision support system",
            "Knowledge base",
            "Ontologies",
            "Metamodel",
            "Complex event processing",
            "Business process management"
        ],
        "authors": [
            "Aurélie Congès",
            "Audrey Fertier",
            "Nicolas Salatgé",
            "Sébastien Rebière",
            "Frederick Benaben"
        ],
        "file_path": "data/sosym-all/s10270-024-01237-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-based resource analysis and synthesis of service-oriented automotive software architectures",
        "submission-date": "2020/03",
        "publication-date": "2021/09",
        "abstract": "Context Automotive software architectures describe distributed functionality by an interaction of software components. One drawback of today’s architectures is their strong integration into the onboard communication network based on predeﬁned dependencies at design time. The idea is to reduce this rigid integration and technological dependencies. To this end, service-oriented architecture offers a suitable methodology since network communication is dynamically established at run-time.\nAim We target to provide a methodology for analysing hardware resources and synthesising automotive service-oriented architectures based on platform-independent service models. Subsequently, we focus on transforming these models into a platform-speciﬁc architecture realisation process following AUTOSAR Adaptive.\nApproach For the platform-independent part, we apply the concepts of design space exploration and simulation to analyse and synthesise deployment conﬁgurations, i.e., mapping services to hardware resources at an early development stage. We reﬁne these conﬁgurations to AUTOSAR Adaptive software architecture models representing the necessary input for a subsequent implementation process for the platform-speciﬁc part.\nResult We present deployment conﬁgurations that are optimal for the usage of a given set of computing resources currently under consideration for our next generation of E/E architecture. We also provide simulation results that demonstrate the ability of these conﬁgurations to meet the run time requirements. Both results helped us to decide whether a particular conﬁguration can be implemented. As a possible software toolchain for this purpose, we ﬁnally provide a prototype.\nConclusion The use of models and their analysis are proper means to get there, but the quality and speed of development must also be considered.",
        "keywords": [
            "Service-oriented architecture",
            "Real-time behaviour",
            "Model-based design",
            "Automotive architectures"
        ],
        "authors": [
            "Stefan Kugele",
            "Philipp Obergfell",
            "Eric Sax"
        ],
        "file_path": "data/sosym-all/s10270-021-00896-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Some Basic Tenets of Description",
        "submission-date": "2002/06",
        "publication-date": "2002/09",
        "abstract": "Description – often referred to as modelling – is fundamental to software development. The developer should always be ready to say of each description: what subject it describes; what it says about its subject; and how it fits with other descriptions in the same development. Sometimes a very informal – even a casual – approach to these questions may be adopted. But often a more careful and explicit approach is needed. This short paper lays out some basic tenets of such an approach.",
        "keywords": [
            "Description",
            "Modelling",
            "Model domain",
            "Use cases"
        ],
        "authors": [
            "Michael Jackson"
        ],
        "file_path": "data/sosym-all/s10270-002-0005-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Consistent speciﬁcation of interface suites in UML",
        "submission-date": "2002/02",
        "publication-date": "2002/12",
        "abstract": "The paper motivates and describes a model oriented approach for consistent speciﬁcation of interface suites in UML. An interface suite is a coherent collection of interfaces deﬁning interactions that transcend component boundaries. The speciﬁcation of interface suites contains diagrammatic views and documentation, but it is extended with templates for structured speciﬁcations deriving from the ISpec approach. To guarantee that the speciﬁcation views, documentation and templates are consistent, a speciﬁcation model has been constructed. The model contains both structural and behavioural information, represented in the form of sequences of carefully designed tuples. The model provides the underlying structure for the tool supporting the design process. The tool directs the designer to specify all elements of the model in a consistent way. The speciﬁcation is collected both by customized speciﬁcation templates and by diagrams. The documentation and the diagram elements – both derived from the template information – are automatically generated. This prevents errors and provides speciﬁcation consistency.",
        "keywords": [
            "UML",
            "Component speciﬁcation",
            "Consistency of several views",
            "Speciﬁcation model"
        ],
        "authors": [
            "E.E. Roubtsova",
            "L.C.M. van Gool",
            "R. Kuiper",
            "H.B.M. Jonkers"
        ],
        "file_path": "data/sosym-all/s10270-002-0011-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated generation of consistent, diverse and structurally realistic graph models",
        "submission-date": "2020/01",
        "publication-date": "2021/05",
        "abstract": "In this paper, we present a novel technique to automatically synthesize consistent, diverse and structurally realistic domain-speciﬁc graph models. A graph model is (1) consistent if it is metamodel-compliant and it satisﬁes the well-formedness constraints of the domain; (2) it is diverse if local neighborhoods of nodes are highly different; and (1) it is structurally realistic if a synthetic graph is at a close distance to a representative real model according to various graph metrics used in network science, databases or software engineering. Our approach grows models by model extension operators using a hill-climbing strategy in a way that (A) ensures that there are no constraint violation in the models (for consistency reasons), while (B) more realistic candidates are selected to minimize a target metric value (wrt. the representative real model). We evaluate the effectiveness of the approach for generating realistic models using multiple metrics for guidance heuristics and compared to other model generators in the context of three case studies with a large set of real human models. We also highlight that our technique is able to generate a diverse set of models, which is a requirement in many testing scenarios.",
        "keywords": [
            "Model generation",
            "Domain-speciﬁc languages",
            "Test generation",
            "Graph metrics"
        ],
        "authors": [
            "Oszkár Semeráth",
            "Aren A. Babikian",
            "Boqi Chen",
            "Chuning Li",
            "Kristóf Marussy",
            "Gábor Szárnyas",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-021-00884-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On model-based analysis of organizational structures: an assessment of current modeling approaches and application of multi-level modeling in support of design and analysis of organizational structures",
        "submission-date": "2018/09",
        "publication-date": "2019/11",
        "abstract": "Conceptual modeling promises to support various analysis questions on organizational structures, such as allocation of tasks, responsibilities, and authority in an organization. In this paper, we ﬁrst synthesize requirements on an organizational structure analysis from the business scholar literature and assess to what extent and how current modeling languages fulﬁll these. In par-ticular, we ﬁnd limitations in the covered scope as well as the information processing capabilities of the reviewed approaches. Second, as a response to identiﬁed gaps, we propose multi-level modeling and integrated modeling and programming as a way to support design and analysis of organizational structure. We use the structure of universities as a case scenario. This paper is an extension of our earlier work. Firstly, we add an explicit set of requirements derived from business scholar literature. Sec-ondly, we draw a comparison to the abstraction mechanisms used in conventional meta-modeling, as prominently exempliﬁed by UML class diagrams, and we critically discuss multi-level modeling. Finally, we discuss a prototypical implementation of our multi-level model in the XModeler software tool.",
        "keywords": [
            "Organizational structure",
            "Multi-level modeling",
            "Conceptual modeling"
        ],
        "authors": [
            "Sybren de Kinderen",
            "Monika Kaczmarek-Heß"
        ],
        "file_path": "data/sosym-all/s10270-019-00767-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Impromptu: a framework for model-driven prompt engineering",
        "submission-date": "2024/04",
        "publication-date": "Not found",
        "abstract": "Generative artiﬁcial intelligence (AI) systems are capable of synthesizing complex artifacts such as text, source code or images according to the instructions provided in a natural language prompt. The quality of the input prompt, in terms of both content and structure, has a large impact on the quality of the output. This has given rise to prompt engineering, the process of designing natural language prompts to best take advantage of the capabilities of generative AI systems. This paper describes Impromptu, a model-driven engineering framework to support the creation, management and reuse of prompts for generative AI. Impromptu offers a domain-speciﬁc language (DSL) to deﬁne multimodal prompts in a modular and tool-independent way. The language offers additional features such as versioning, prompt chaining and multi-language support. Moreover, it provides tool support to adapt prompts for speciﬁc generative AI systems, execute those prompts on a generative AI system and validate the quality of the response that is generated. Impromptu is available as a Langium-based Visual Studio Code plugin.",
        "keywords": [
            "Prompt engineering",
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Generative AI",
            "Large language models"
        ],
        "authors": [
            "Sergio Morales",
            "Robert Clarisó",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-024-01235-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A framework for the operationalization of monitoring in business intelligence requirements engineering",
        "submission-date": "2013/09",
        "publication-date": "2014/06",
        "abstract": "Business intelligence (BI) is perceived as a critical activity for organizations and is increasingly discussed in requirements engineering (RE). RE can contribute to the successful implementation of BI systems by assisting the identification and analysis of such systems’ requirements and the production of the specification of the system to be. Within RE for BI systems, we focus in this paper on the following questions: (i) how the expectations of a BI system’s stakeholders can be translated into accurate BI requirements, and (ii) how do we operationalize specifically these requirements in a system specification? In response, we define elicitation axes for the documentation of BI-specific requirements, give a list of six BI entities that we argue should be accounted for to operationalize business monitoring, and provide notations for the modeling of these entities. We survey important contributions of BI to define elicitation axes, adapt existing BI notations issued from RE literature, and complement them with new BI-specific notations. Using the i* framework, we illustrate the application of our proposal using a real-world case study.",
        "keywords": [
            "Business intelligence",
            "Requirement",
            "Monitoring",
            "Indicator",
            "Analytic",
            "Field",
            "Schema",
            "Source"
        ],
        "authors": [
            "Corentin Burnay",
            "Ivan J. Jureta",
            "Isabelle Linden",
            "Stéphane Faulkner"
        ],
        "file_path": "data/sosym-all/s10270-014-0417-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating large language models on business process modeling: framework, benchmark, and self-improvement analysis",
        "submission-date": "2024/11",
        "publication-date": "2025/08",
        "abstract": "Large language models (LLMs) are rapidly transforming various ﬁelds, including the ﬁeld of business process management (BPM). LLMs provide new ways for analyzing and improving operational processes. This paper assesses the capabilities of LLMs on business process modeling using a framework for automating this task and a robust evaluation approach. We design a comprehensive benchmark, consisting of 20 diverse business processes, and we demonstrate our evaluation approach by assessing 16 current state-of-the-art LLMs from major AI vendors. Our analysis highlights signiﬁcant performance variations across LLMs and reveals a positive correlation between efﬁcient error handling and the quality of generated models. It also shows consistent performance trends within similar LLM groups. Furthermore, we use our evaluation approach to investigate LLM self-improvement techniques, encompassing self-evaluation, input optimization, and output optimization. Our ﬁndings indicate that output optimization, in particular, offers promising potential for enhancing quality, especially in models with initially lower performance. Our contributions provide insights for leveraging LLMs in BPM, paving the way for more advanced and automated process modeling techniques.",
        "keywords": [
            "Business process modeling",
            "Large language models",
            "Generative AI",
            "Benchmarking",
            "Process mining"
        ],
        "authors": [
            "Humam Kourani",
            "Alessandro Berti",
            "Daniel Schuster",
            "Wil M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-025-01318-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the complex nature of MDE evolution and its impact on changeability",
        "submission-date": "2014/08",
        "publication-date": "2015/04",
        "abstract": "In model-driven engineering (MDE), a particular MDE setting of employed languages and automated and manual activities has major impact on productivity. Furthermore, it has been observed that such MDE settings evolve over time. However, currently not much is known about this evolution and its impact on the MDE setting’s maturity, i.e., on changeability and other productivity dimensions. Research so far focuses on evolution of separate building blocks, such as (modeling-) languages, tools, or transformation, only. In this article, we address the lack of knowledge about evolution of MDE settings by investigating case studies from different companies. The ﬁrst results reveal (1) that there is evolution that affects the composition of an MDE setting (structural evolution) and has the potential to strongly impact aspects, such as changeability and (2) that this structural evolution actually occurs in practice. Based on these ﬁrst results, we investigated (3) whether there are cases in practice, where structural evolution already altered the risks of changeability given by the respective MDE setting. Therefore, we search and identify examples for such evolution steps on MDE set-tings from practice and collected six case studies on evolution histories in detail. As a result, we show in this paper that structural evolution (a) is not seldom in practice and (b) sometimes leads to the introduction of changeability risks.",
        "keywords": [
            "Model-driven engineering",
            "Evolution",
            "Empirical research"
        ],
        "authors": [
            "Regina Hebig",
            "Holger Giese"
        ],
        "file_path": "data/sosym-all/s10270-015-0464-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "UML formal semantics: lessons learned",
        "submission-date": "2011/05",
        "publication-date": "2011/06",
        "abstract": "The article below presents the insights gained during a number of years of research dedicated to the formalisation of the Uniﬁed Modeling Language.",
        "keywords": [
            "Formal semantics",
            "Compositional semantics",
            "Multiple system views",
            "All-encompassing UML semantics",
            "Formal model-driven system development"
        ],
        "authors": [
            "Manfred Broy",
            "María Victoria Cengarle"
        ],
        "file_path": "data/sosym-all/s10270-011-0207-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "CaRE: a reﬁnement calculus for requirements engineering based on argumentation theory",
        "submission-date": "2020/11",
        "publication-date": "2021/11",
        "abstract": "The Requirements Engineering (RE) process starts with initial requirements elicited from stakeholders—however conﬂicting, unattainable, incomplete and ambiguous—and successively reﬁnes them until a consistent, complete, valid, and unambiguous speciﬁcation is reached. This is achieved by balancing stakeholders’ viewpoints and preferences to reach compromises through negotiation. Several frameworks have been developed to support this process in a structured way, such as KAOS, i*, and RationalGLR. However, none provides the means to model the dialectic negotiation inherent to the RE process, so that the derivation of speciﬁcations from requirements is fully explicit and traceable. To address this gap, we propose CaRE, a reﬁnement calculus for requirements engineering based on argumentation theory. CaRE casts the RE reﬁnement problem as an iterative argument between all relevant stakeholders, who point out defects (ambiguity, incompleteness, etc.) of existing requirements, and then propose suitable reﬁnements to address them, thereby leading to the construction of a reﬁnement graph. This graph is then a conceptual model of the RE process. The semantics of reﬁnement graphs is provided using Argumentation Theory, enabling reasoning over the RE process and the automatic computation of software speciﬁcations. An alternate semantics is also presented based on abduction and using Horn Theory. The application of CaRE is showcased with an extensive example from the railway domain, and a prototype tool for identifying speciﬁcations in a reﬁnement graph is presented.",
        "keywords": [
            "Requirements engineering",
            "Requirements reﬁnement",
            "RE process",
            "RE calculus",
            "Argumentation theory",
            "Formal semantics"
        ],
        "authors": [
            "Yehia Elrakaiby",
            "Alexander Borgida",
            "Alessio Ferrari",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-021-00943-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Involving users in the development of a modeling language for customer journeys",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Although numerous methods for handling the technical aspects of developing domain-speciﬁc modeling languages (DSMLs) have been formalized, user needs and usability aspects are often addressed late in the development process and in an ad hoc manner. To this concern, this paper presents the development of the customer journey modeling language (CJML), a DSML for modeling service processes from the end-user’s perspective. Because CJML targets a wide and heterogeneous group of users, its usability can be challenging to plan and assess. This paper describes how an industry-relevant DSML was systematically improved by using a variety of user-centered design techniques in close collaboration with the target group, whose feedback was used to reﬁne and evolve the syntax and semantics of CJML. We also suggest how a service-providing organization may beneﬁt from adopting CJML as a unifying language for documentation purposes, compliance analysis, and service innovation. Finally, we distill what we learned into general lessons and methodological guidelines.",
        "keywords": [
            "Domain-speciﬁc modeling language (DSML)",
            "Customer journey",
            "Conceptual modeling",
            "User involvement"
        ],
        "authors": [
            "Ragnhild Halvorsrud",
            "Odnan Ref Sanchez",
            "Costas Boletsis",
            "Marita Skjuve"
        ],
        "file_path": "data/sosym-all/s10270-023-01081-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the automated translational execution of the action language for foundational UML",
        "submission-date": "2016/03",
        "publication-date": "2016/09",
        "abstract": "To manage the rapidly growing complexity of software development, abstraction and automation have been recognised as powerful means. Among the techniques pushing for them, model-driven engineering has gained increasing attention from industry for, among others, the possibility to automatically generate code from models. To generate fully executable code, models should describe complex behaviours. While pragmatically this is achieved by employing programming languages for deﬁning actions within models, the abstraction gap between modelling and programming languages can undermine consistency between models and code as well as analysability and reusability of models. In light of this, model-aware action languages should be preferred. This is the case of the Action Language for Foundational UML (ALF). In this paper, we provide a solution for the fully automated translational execution of ALF towards C++. Additionally, we give an insight on how to simplify the transition from the use of programming languages for modelling ﬁne-grained behaviours to model-aware action languages in industrial MDE. The solution presented in this paper has been assessed on industrial applications to verify its applicability to complex systems as well as its scalability.",
        "keywords": [
            "Model-driven engineering",
            "Translational execution",
            "Code generation",
            "UML",
            "ALF"
        ],
        "authors": [
            "Federico Ciccozzi"
        ],
        "file_path": "data/sosym-all/s10270-016-0556-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "ALF"
        }
    },
    {
        "title": "Guest editorial to the theme section on AI-enhanced model-driven engineering",
        "submission-date": "2022/02",
        "publication-date": "2022/03",
        "abstract": "This theme section brings together the latest research at the intersection of artiﬁcial intelligence (AI) and model-driven engineering (MDE). Over the past years, we have witnessed a substantial rise of AI successfully applied to different domains, including software development and MDE. Dedicated events at the intersection of AI and MDE have been created, too, such as the MDE Intelligence workshop series co-located with the MODELS conference. This theme section covers research contributions integrating AI components into MDE approaches—increasing the current beneﬁts of MDE processes and tools and pushing the limits of “classic” MDE with the goal to provide software and systems engineers with the right techniques to develop the next generation of highly complex model-based systems—and applications of MDE to the development of AI components. In total, nine submissions were accepted in the theme section after a thorough peer-reviewing process.",
        "keywords": [
            "Artiﬁcial intelligence",
            "Model-driven engineering",
            "Software engineering",
            "Systems engineering"
        ],
        "authors": [
            "Lola Burgueño",
            "Jordi Cabot",
            "Manuel Wimmer",
            "Steﬀen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-022-00988-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Evaluating data-centric process approaches: Does the human factor factor in?",
        "submission-date": "2014/09",
        "publication-date": "2016/02",
        "abstract": "The Business Process Management ﬁeld addresses design, improvement, management, support, and execution of business processes. In doing so, we argue that it focuses more on developing modeling notations and process design approaches than on the needs and preferences of the individual who is modeling (i.e., the user). New data-centric process modeling approaches are taken as a relevant and timely stream of process design approaches to test our argument. First, we provide a review of existing data-centric process approaches, culminating in a theoretical classification framework. Next, we empirically evaluate three specific approaches with regard to the claims they make. We had participants representative of actual users try out these approaches on realistic scenarios via a series of workshops. Participants assessed to what extent quality claims from the literature could be recognized within the workshop sessions. The results of this evaluation substantiate a number of claims behind the approaches, but also identify opportunities to further improve them. Most prominently, we found that the usability aspects of all considered approaches are a source of concern. This leads us to the insight that usability aspects of process design approaches are crucial and, in the perception of groups representative of actual users, leave much to be desired. In that sense, our research can be seen as a wake-up call for process modeling notation designers to consider the usability side—and as such, the interest of the human modeler—more than is currently the case.",
        "keywords": [
            "Process modeling",
            "User",
            "Usability",
            "Data-centric",
            "Evaluation",
            "Review"
        ],
        "authors": [
            "Hajo A. Reijers",
            "Irene Vanderfeesten",
            "Marijn G. A. Plomp",
            "Pieter Van Gorp",
            "Dirk Fahland",
            "Wim L. M. van der Crommert",
            "H. Daniel Diaz Garcia"
        ],
        "file_path": "data/sosym-all/s10270-015-0491-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model-driven approach to automate the propagation of changes among Architecture Description Languages",
        "submission-date": "2009/11",
        "publication-date": "2010/07",
        "abstract": "As it is widely recognized, a universal nota-tion accepted by any software architect cannot exist. This caused a proliferation of architecture description languages (ADLs) each focussing on a speciﬁc application domain, analysis type, or modelling environment, and with its own speciﬁc notations and tools. Therefore, the production of a software architecture description often requires the use of multiple ADLs, each satisfying some stakeholder’s concerns. When dealing with multiple notations, suitable techniques are required in order to keep models in a consistent state. Several solutions have been proposed so far but they lack in convergence and scalability. In this paper, we propose a convergent change propagation approach between multi-ple architectural languages. The approach is generic since it depends neither on the notations to synchronize nor on their corresponding models. It is implemented within the Eclipse modelling framework and we demonstrate its usability and scalability by experimenting it on well known architectural languages.",
        "keywords": [
            "Architectural languages interoperability",
            "Model transformation",
            "Model synchronization",
            "Automation",
            "Metamodelling"
        ],
        "authors": [
            "Romina Eramo",
            "Ivano Malavolta",
            "Henry Muccini",
            "Patrizio Pelliccione",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-010-0170-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Transforming XML schemas into OWL ontologies using formal concept analysis",
        "submission-date": "2016/11",
        "publication-date": "2018/01",
        "abstract": "Ontology Web Language (OWL) is considered as a data representation format exploited by the Extensible Markup Language (XML) format. OWL extends XML by providing properties to further express the semantics of data. To this effect, transforming XML data into OWL proves important and constitutes an added value for indexing XML documents and re-engineering ontologies. In this paper, we propose a formal method to transform XSD schemas into OWL schemas using transformation patterns. To achieve this end, we extend at the beginning, a set of existing transformation patterns to allow the maximum transformation of XSD schema constructions. In addition, a formal method is presented to transform an XSD schema using the extended patterns. This method named PIXCO comprises several processes. The ﬁrst process models both the transformation patterns and all the constructions of XSD schema to be transformed. The patterns are modeled using the context of Formal Concept Analysis. The XSD constructions are modeled using a proposed mathematical model. This modeling will be used in the design of the following process. The second process identiﬁes the most appropriate patterns to transform each construction set of XSD schema. The third process generates for each XSD construction set an OWL model according to the pattern that is identiﬁed. Finally, it creates the OWL ﬁle encompassing the generated OWL models.",
        "keywords": [
            "XML schema",
            "OWL ontology",
            "Formal transformation",
            "XSD formalization",
            "FCA",
            "Transformation patterns"
        ],
        "authors": [
            "Mokhtaria Hacherouf",
            "Saﬁa Nait-Bahloul",
            "Christophe Cruz"
        ],
        "file_path": "data/sosym-all/s10270-017-0651-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An example is worth a thousand words: Creating graphical modelling environments by example",
        "submission-date": "2016/11",
        "publication-date": "2017/11",
        "abstract": "Domain-speciﬁc languages (DSLs) are heavily used in model-driven and end-user development approaches. Compared to general-purpose languages, DSLs present numerous beneﬁts like powerful domain-speciﬁc primitives, an intuitive syntax for domain experts, and the possibility of advanced code generation for narrow domains. While a graphical syntax is sometimes desired for a DSL, constructing graphical modelling environments is a costly and highly technical task. This relegates domain experts to a rather passive role in their development and hinders a wider adoption of graphical DSLs. Our aim is achieving a simpler DSL construction process where domain experts can contribute actively. For this purpose, we propose an example-based technique for the automatic generation of modelling environments for graphical DSLs. This way, starting from examples of the DSL likely provided by domain experts using drawing tools like yED, our system synthesizes a graphical modelling environment that mimics the syntax of the provided examples. This includes a meta-model for the abstract syntax of the DSL and a graphical concrete syntax supporting spatial relationships like containment and adjacency. Our system, called metaBUP, is implemented as an Eclipse plug-in. In this paper, we demonstrate its usage on a running example in the home networking domain and evaluate its suitability for the construction of graphical modelling environments by means of a user study.",
        "keywords": [
            "Domain-speciﬁc modelling languages",
            "Graphical modelling environments",
            "Example-based meta-modelling",
            "Flexible modelling"
        ],
        "authors": [
            "Jesús J. López-Fernández",
            "Antonio Garmendia",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-017-0632-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modelling on mobile devices: A systematic mapping study",
        "submission-date": "2021/01",
        "publication-date": "2021/06",
        "abstract": "Modelling is central to many disciplines in engineering and the natural and social sciences. A wide variety of modelling\nlanguages and tools have been proposed along the years, traditionally for static environments such as desktops and laptops.\nHowever, the availability of increasingly powerful mobile devices makes it possible to proﬁt from their embedded sensors\nand components (e.g. camera, microphone, GPS, accelerometer, gyroscope) for modelling. This has promoted a new range\nof modelling tools specially designed for their use in mobility. Such tools open the door to modelling in dynamic scenarios\nthat go beyond the capabilities of traditional desktop tools. For example, modelling in mobility can be useful to design smart\nfactories on-site, or to create models of hiking routes while walking along the routes, among many other scenarios. In this\npaper, we report on a systematic mapping study to identify the state of the art and trends in modelling on mobile devices.\nThe study covers both research papers and modelling apps from the Android and iOS stores. From this analysis, we derive a\nclassiﬁcation for mobile modelling tools along three orthogonal dimensions, discuss current gaps, and propose avenues for\nfurther research.",
        "keywords": [
            "Model-driven engineering",
            "Modelling tools",
            "Mobile devices",
            "Systematic mapping study"
        ],
        "authors": [
            "Léa Brunschwig",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-021-00897-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Bridging value modelling to ArchiMate via transaction modelling",
        "submission-date": "2011/11",
        "publication-date": "2012/11",
        "abstract": "The ArchiMate modelling language provides a coherent and a holistic view of an enterprise in terms of its products, services, business processes, actors, business units, software applications and more. Yet, ArchiMate currently lacks (1) expressivity in modelling an enterprise from a value exchange perspective, and (2) rigour and guidelines in modelling business processes that realize the transactions relevant from a value perspective. To address these issues, we show how to connect e3value, a technique for value modelling, to ArchiMate via transaction patterns from the DEMO methodology. Using ontology alignment techniques, we show a transformation between the meta mod-els underlying e3value, DEMO and ArchiMate. Furthermore, we present a step-wise approach that shows how this model transformation is achieved and, in doing so, we also show the relevance of such a transformation. We exemplify the transformation of DEMO and e3value into ArchiMate by means of a case study in the insurance industry. As a proof of concept, we present a software tool supporting our trans-formation approach. Finally, we discuss the functionalities and limitations of our approach; thereby, we analyze its advantages and practical applicability.",
        "keywords": [
            "ArchiMate",
            "e3value",
            "DEMO",
            "Meta model",
            "Model transformation."
        ],
        "authors": [
            "Sybren de Kinderen",
            "Khaled Gaaloul",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-012-0299-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling languages in Industry 4.0: an extended systematic mapping study",
        "submission-date": "2018/07",
        "publication-date": "2019/09",
        "abstract": "Industry 4.0 integrates cyber-physical systems with the Internet of Things to optimize the complete value-added chain. Successfully applying Industry 4.0 requires the cooperation of various stakeholders from different domains. Domain-speciﬁc modeling languages promise to facilitate their involvement through leveraging (domain-speciﬁc) models to primary develop-ment artifacts. We aim to assess the use of modeling in Industry 4.0 through the lens of modeling languages in a broad sense. Based on an extensive literature review, we updated our systematic mapping study on modeling languages and modeling techniques used in Industry 4.0 (Wortmann et al., Conference on model-driven engineering languages and systems (MOD-ELS’17), IEEE, pp 281–291, 2017) to include publications until February 2018. Overall, the updated study considers 3344 candidate publications that were systematically investigated until 408 relevant publications were identiﬁed. Based on these, we developed an updated map of the research landscape on modeling languages and techniques for Industry 4.0. Research on modeling languages in Industry 4.0 focuses on contributing methods to solve the challenges of digital representation and integration. To this end, languages from systems engineering and knowledge representation are applied most often but rarely combined. There also is a gap between the communities researching and applying modeling languages for Industry 4.0 that originates from different perspectives on modeling and related standards. From the vantage point of modeling, Industry 4.0 is the combination of systems engineering, with cyber-physical systems, and knowledge engineering. Research currently is splintered along topics and communities and accelerating progress demands for multi-disciplinary, integrated research efforts.",
        "keywords": [
            "Industry 4.0",
            "Modeling languages",
            "Smart manufacturing"
        ],
        "authors": [
            "Andreas Wortmann",
            "Olivier Barais",
            "Benoit Combemale",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-019-00757-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Introducing probabilistic reasoning within Event-B",
        "submission-date": "2017/03",
        "publication-date": "2017/10",
        "abstract": "Event-B is a proof-based formal method used for discrete systems modelling. Several works have previously focused on the extension of Event-B for the description of probabilistic systems. In this paper, we propose an exten-sion of Event-B that allows designing fully probabilistic systems as well as systems containing both probabilistic and non-deterministic choices. Compared to existing approaches which only focus on probabilistic assignments, our approach allows expressing probabilistic choices in all places where non-deterministic choices originally appear in a standard Event-B model: in the choice between enabled events, event parameter values and in probabilistic assignments. Furthermore, we introduce novel and adapted proof obligations for the consistency of such systems and introduce two key aspects to incremental design: probabilisation of existing events and reﬁnement through the addition of new proba-bilistic events. In particular, we provide proof obligations for the almost-certain convergence of a set of new events, which is a required property in order to prove standard reﬁnement in this context. Finally, we propose a fully detailed case study, which we use throughout the paper to illustrate our new con-structions.",
        "keywords": [
            "Event-B",
            "Probabilistic systems",
            "Markov chains"
        ],
        "authors": [
            "Mohamed Amine Aouadhi",
            "Benoît Delahaye",
            "Arnaud Lanoix"
        ],
        "file_path": "data/sosym-all/s10270-017-0626-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Supporting multiple perspectives in feature-based conﬁguration",
        "submission-date": "2011/04",
        "publication-date": "2011/11",
        "abstract": "Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.",
        "keywords": [
            "Software product line engineering",
            "Feature diagram",
            "Separation of concerns",
            "Multi-view",
            "Feature-based conﬁguration"
        ],
        "authors": [
            "Arnaud Hubaux",
            "Patrick Heymans",
            "Pierre-Yves Schobbens",
            "Dirk Deridder",
            "Ebrahim Khalil Abbasi"
        ],
        "file_path": "data/sosym-all/s10270-011-0220-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated generation of smart contract code from legal contract speciﬁcations with SYMBOLEO2SC",
        "submission-date": "2023/05",
        "publication-date": "2024/06",
        "abstract": "Smart contracts (SCs) are software systems that monitor and partially control the execution of legal contracts to ensure compliance with the contracts’ terms and conditions, which essentially are sets of obligations and powers, triggered by events. Such systems often exploit Internet-of-Things technologies to support their monitoring functions and blockchain technology to ensure the integrity of their data. Enterprise-level blockchain platforms (such as Hyperledger Fabric) and public ones (such as Ethereum) are popular choices for SC development. However, usually, legal experts are not able to directly encode contract requirements into SCs. Symboleo is a formal speciﬁcation language for legal contracts that was introduced to address this issue. Symboleo uses an ontology that deﬁnes legal concepts such as parties, obligations, powers, and assets, with semantics expressed with state machines. This paper proposes a tool that automatically translates Symboleo speciﬁcations into smart contract code for Hyperledger Fabric. Towards this end, we have extended the current Symboleo IDE, implemented the ontology and semantics by using the modelling language Umple, and created a reusable library. The resulting Symboleo2SC tool generates Hyperledger Fabric code exploiting this library. This code is a complete translation and does not require further development. Symboleo2SC was evaluated with ﬁve sample contracts. These were converted to SCs for contract monitoring and control purposes. Symboleo2SC helps simplify the SC development process, saves development effort, and helps reduce risks of coding errors.",
        "keywords": [
            "Smart contracts",
            "Code generation",
            "Blockchain",
            "Domain-speciﬁc languages",
            "Legal ontology",
            "Symboleo"
        ],
        "authors": [
            "Aidin Rasti",
            "Amal Ahmed Anda",
            "Sofana Alfuhaid",
            "Alireza Parvizimosaed",
            "Daniel Amyot",
            "Marco Roveri",
            "Luigi Logrippo",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-024-01187-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Symboleo"
        }
    },
    {
        "title": "Matters of (meta-) modeling",
        "submission-date": "2004/11",
        "publication-date": "2006/07",
        "abstract": "With the recent trend to model driven engi-neering a common understanding of basic notions such as “model” and “metamodel” becomes a pivotal issue. Even though these notions have been in widespread use for quite a while, there is still little consensus about when exactly it is appropriate to use them. The aim of this article is to start establishing a consensus about generally acceptable terminology. Its main contributions are the distinction between two fundamentally different kinds of model roles, i.e. “token model” versus “type model” (The terms “type” and “token” have been introduced by C.S. Peirce, 1839–1914.), a formal notion of “meta-ness”, and the consideration of “generalization” as yet another basic relationship between models. In particular, the recognition of the fundamental difference between the above mentioned two kinds of model roles is crucial in order to enable communication among the model driven engineering community that is free of both unnoticed misunderstandings and unnecessary disagreement.",
        "keywords": [
            "Model driven engineering",
            "Modeling",
            "Metamodeling",
            "Token model",
            "Type model"
        ],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-006-0017-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Defining business model key performance indicators using intentional linguistic summaries",
        "submission-date": "2020/09",
        "publication-date": "2021/06",
        "abstract": "To sustain competitiveness in contemporary, fast-paced markets, organizations increasingly focus on innovating their business models to enhance current value propositions or to explore novel sources of value creation. However, business model innovation is a complex task, characterized by shifting characteristics in terms of uncertainty, data availability and its impact on decision making. To cope with such challenges, business model evaluation is advocated to make sense of novel business models and to support decision making. Key performance indicators (KPIs) are frequently used in business model evaluation to structure the performance assessment of these models and to evaluate their strategic implications, in turn aiding business model decision making. However, given the shifting characteristics of the innovation process, the application and effectiveness of KPIs depend significantly on how such KPIs are defined. The techniques proposed in the existing literature typically generate or use quantitatively oriented KPIs, which are not well-suited for the early phases of the business model innovation process. Therefore, following a design science research methodology, we have developed a novel method for defining business model KPIs, taking into account the characteristics of the innovation process, offering holistic support toward decision making. Building on theory on linguistic summarization, we use a set of structured templates to define qualitative KPIs that are suitable to support early-phase decision making. In addition, we show how these KPIs can be gradually quantified to support later phases of the innovation process. We have evaluated our method by applying it in two real-life business cases, interviewing 13 industry experts to assess its utility.",
        "keywords": [
            "Business model evaluation",
            "Key performance indicators",
            "Linguistic summarization",
            "Intentional linguistic summaries",
            "Business model innovation"
        ],
        "authors": [
            "Rick Gilsing",
            "Anna Wilbik",
            "Paul Grefen",
            "Oktay Turetken",
            "Baris Ozkan",
            "Onat Ege Adali",
            "Frank Berkers"
        ],
        "file_path": "data/sosym-all/s10270-021-00894-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Incremental execution of model-to-text transformations using property access traces",
        "submission-date": "2016/06",
        "publication-date": "2018/03",
        "abstract": "Automatic generation of textual artefacts (including code, documentation, conﬁguration ﬁles, build scripts, etc.) from models\nin a software development process through the application of model-to-text (M2T) transformation is a common MDE activity.\nDespite the importance of M2T transformation, contemporary M2T languages lack support for developing transformations\nthat scale with the size of the input model. As MDE is applied to systems of increasing size and complexity, a lack of scalability\nin M2T transformation languages hinders industrial adoption. In this paper, we propose a form of runtime analysis that can be\nused to identify the impact of source model changes on generated textual artefacts. The structures produced by this runtime\nanalysis, property access traces, can be used to perform efﬁcient source-incremental transformation: our experiments show\nan average reduction of 60% in transformation execution time compared to non-incremental (batch) transformation.",
        "keywords": [
            "Model-driven engineering",
            "Scalability",
            "Model-to-text transformation",
            "Incrementality"
        ],
        "authors": [
            "Babajide Ogunyomi",
            "Louis M. Rose",
            "Dimitrios S. Kolovos"
        ],
        "file_path": "data/sosym-all/s10270-018-0666-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The design of a language for model transformations",
        "submission-date": "2004/11",
        "publication-date": "2006/07",
        "abstract": "Model-driven development of software systems envisions transformations applied in various stages of the development process. Similarly, the use of domain-speciﬁc languages also necessitates transformations that map domain-speciﬁc constructs into the constructs of an underlying programming language. Thus, in these cases, the writing of transformation tools becomes a ﬁrst-class activity of the software engineer. This paper introduces a language that was designed to support implementing highly efﬁcient transformation programs that perform model-to-model or model-to-code translations. The language uses the concepts of graph transformations and metamodeling, and is supported by a suite of tools that allow the rapid prototyping and realization of transformation tools.",
        "keywords": [
            "Model transformation",
            "UML",
            "Graph transformation",
            "Graph rewriting",
            "Model driven architecture"
        ],
        "authors": [
            "Aditya Agrawal",
            "Gabor Karsai",
            "Sandeep Neema",
            "Feng Shi",
            "Attila Vizhanyo"
        ],
        "file_path": "data/sosym-all/s10270-006-0027-7.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "None"
        }
    },
    {
        "title": "OCL 1.4/5 vs. 2.0 Expressions",
        "submission-date": "2003/11",
        "publication-date": "2003/11",
        "abstract": "A type inference system and a big-step operational semantics for expressions of the “Object Constraint Language” (OCL), the declarative and navigational constraint language for the “Uniﬁed Modeling Language” (UML), are provided; the account is mainly based on OCL 1.4/5, but also includes the main features of OCL 2.0. The formal systems are parameterised in terms of UML static structures and UML object models, which are treated abstractly. It is proved that the operational semantics satisﬁes a subject reduction property with respect to the type inference system. Proceeding from the operational semantics and providing a denotational semantics, pure OCL 2.0 expressions are shown to exactly represent the primitive recursive functions, whereas pure OCL 1.4/5 expressions are Turing complete.",
        "keywords": [
            "OCL",
            "UML",
            "Formal semantics"
        ],
        "authors": [
            "Mar´ıa Victoria Cengarle",
            "Alexander Knapp"
        ],
        "file_path": "data/sosym-all/s10270-003-0035-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on model transformation",
        "submission-date": "2008/06",
        "publication-date": "2009/01",
        "abstract": "Models play a cornerstone role in Model-Driven Engineering (MDE). The use of models opens up new possibilities for creating, analyzing, and manipulating systems through various types of tools and languages. Each model usually addresses one set of related concerns, and transformations between models provide a chain that enables the automated development of a system from its corresponding models. Model transformation speciﬁcation, implementation, and execution are the major parts of this process. Furthermore, model transformations may also be realized using models, and are, therefore, an integral part of a model-driven approach. Model transformations need specialized support in several aspects in order to realize their full potential for system modelers, transformation developers, and tool vendors. The problem goes beyond having speciﬁc languages to represent model transformations; we also need to understand the key concepts and operators supporting those languages, their semantics and their structuring mechanisms and properties (e.g., modularity, composability, and parameterization). In addition, model transformations can be stored in repositories as reusable assets, where they can be discovered and reused. There is also a need to chain and combine model transformations in order to produce new and more powerful transformations. Moreover, they need to be fully integrated into software development methodologies supported by appropriate tools and environments. The objective of this special section is to provide a representative sample of advanced research emerging from the ﬁeld of model transformation. The selected papers provide an overview of current open issues and identify potential lines for further research.",
        "keywords": [],
        "authors": [
            "Jean Bézivin",
            "Alfonso Pierantonio",
            "Antonio Vallecillo",
            "Jeff Gray"
        ],
        "file_path": "data/sosym-all/s10270-008-0097-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An investigation of the relationship between joint visual attention and product quality in collaborative business process modeling: a dual eye-tracking study",
        "submission-date": "2020/11",
        "publication-date": "2022/02",
        "abstract": "Collaborative business process modeling is a collective activity where team members jointly discuss, design, and document business processes. During such activities, team members need to communicate with each other to coordinate the modeling activities, propose and justify changes, and negotiate common terms and deﬁnitions. Throughout this process, stakeholders should be aware of when and what kind of changes have been made by each team member on the shared space so that they can discuss design ideas and build on each other’s work. Joint visual attention has a fundamental role in establishing and maintaining common ground among interlocutors in such cooperative work settings. In addition to this, the co-constructed model’s quality is often considered a key evaluation outcome measure to assess the success of collaboration. However, process and outcome measures of collaboration have been prone to difﬁculties due to challenges in devising measures that can adequately capture the complex dynamics of collaborative work. This study explored the relationship between a popularly used outcome measure in the business process modeling literature and a process measure approximating the level of joint visual attention present among the participants based on the degree of gaze cross-recurrence among the team members over a shared task space. The results suggest that joint visual attention as operationalized in terms of gaze cross-recurrence was a strong predictor of the syntactic, semantic, and pragmatic qualities of collaboratively produced business process models. Moreover, the collaboration process was subjected to qualitative analysis to probe further into the interactional organization of the modeling activity, which identiﬁed communication, coordination, awareness, group decision making, and motivation dimensions as key factors contributing to the quality of collaboration among group members. The results indicated strong relationships between the distribution of quality factors and the degree of gaze cross-recurrence and the ﬁnal models’ syntactic and semantic quality scores. Given the increasing availability of affordable eye trackers and the low resolution, practical nature of the employed analysis methodology, the proposed approach can be fruitfully employed to evaluate team performance and test the effectiveness of software interfaces designed to support collaborative work.",
        "keywords": [
            "Computer-supported collaborative business process modeling",
            "Joint visual attention",
            "Business process model quality",
            "Dual eye tracking",
            "Gaze cross-recurrence"
        ],
        "authors": [
            "Duygu Fındık-Co¸skunçay",
            "Murat Perit Çakır"
        ],
        "file_path": "data/sosym-all/s10270-022-00974-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Towards an integrated formal method for veriﬁcation of liveness properties in distributed systems: with application to population protocols",
        "submission-date": "2013/11",
        "publication-date": "2015/12",
        "abstract": "Abstract State-based formal methods [e.g. Event-B/ RODIN (Abrial in Modeling in Event-B—system and soft-ware engineering. Cambridge University Press, Cambridge, 2010; Abrial et al. in Int J Softw Tools Technol Transf (STTT) 12(6):447–466, 2010)] for critical system devel-opment and veriﬁcation are now well established, with track records including tool support and industrial applica-tions. The focus of proof-based veriﬁcation, in particular, is on safety properties. Liveness properties, which guarantee eventual, or converging computations of some requirements, are less well dealt with. Inductive reasoning about liveness is not explicitly supported. Liveness proofs are often com-plex and expensive, requiring high-skill levels on the part of the veriﬁcation engineer. Fairness-based temporal logic approaches have been proposed to address this, e.g. TLA Lamport (ACM Trans Program Lang Syst 16(3):872–923, 1994) and that of Manna and Pnueli (Temporal veriﬁcation of reactive systems—safety. Springer, New York, 1995). We contribute to this technology need by proposing a fairness-based method integrating temporal and ﬁrst-order logic, proof and tools for modelling and veriﬁcation of safety and liveness properties. The method is based on an integration of Event-B and TLA. Building on our previous work (Méry and Poppleton in Integrated formal methods, 10th interna-Communicated by Prof. Einar Broch Johnsen and Luigia Petre.",
        "keywords": [
            "Reﬁnement",
            "Formal method",
            "Distributed sytems",
            "Veriﬁcation",
            "Liveness",
            "Fairness"
        ],
        "authors": [
            "Dominique Méry\nMichael Poppleton"
        ],
        "file_path": "data/sosym-all/s10270-015-0504-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Requirement-driven model-based development methodology applied to the design of a real-time MEG data processing unit",
        "submission-date": "2019/05",
        "publication-date": "2020/05",
        "abstract": "The paper describes a multidisciplinary work that uses a model-based systems engineering method for developing real-time magnetoencephalography (MEG) signal processing. We introduce a requirement-driven, model-based development methodology (RDD and MBD) to provide a high-level environment and efficiently handle the complexity of computation and control systems. The proposed development methodology focuses on the use of System Modeling Language to define high-level model-based design descriptions for later implementation in heterogeneous hardware/software systems. The proposed approach was applied to the implementation of a real-time artifact rejection unit in MEG signal processing and demonstrated high efficiency in designing complex high-performance embedded systems. In MEG signal processing, biological artifacts in particular have a signal strength that overtop the signal of interest by orders of magnitude and must be removed from the measurement to achieve high-quality source reconstructions with minimal error contributions. However, many existing brain–computer interface studies overlook real-time artifact removal because of the demanding computational process. In this work, an automated real-time artifact rejection method is introduced, which is based on the recently presented method “ocular and cardiac artifact rejection for real-time analysis in MEG” (OCARTA). The method has been implemented using the RDD and MBD approach and successfully verified on a Virtex-6 field-programmable gate array.",
        "keywords": [
            "MBSE",
            "SysML",
            "Real-time systems",
            "MEG",
            "Artifact rejection",
            "Neurofeedback"
        ],
        "authors": [
            "Tao Chen",
            "Michael Schiek",
            "Jürgen Dammers",
            "N. Jon Shah",
            "Stefan van Waasen"
        ],
        "file_path": "data/sosym-all/s10270-020-00797-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MUPPIT: a method for using proper patterns in model transformations",
        "submission-date": "2019/12",
        "publication-date": "2021/01",
        "abstract": "Model transformation plays an important role in developing software systems using the model-driven engineering paradigm. Examples of applications of model transformation include forward engineering, reverse engineering of code into models, and refactoring. Poor-quality model transformation code is costly and hard to maintain. There is a need to develop techniques and tools that can support transformation engineers in designing high-quality model transformations. The goal of this paper is to present a process, called MUPPIT (method for using proper patterns in model transformations), which can be used by transformation engineers to improve the quality of model transformations by detecting anti-patterns in the transformations and automatically applying pattern solutions. MUPPIT consists of four phases: (1) identifying a transformation anti-pattern, (2) proposing a pattern-solution, (3) applying the pattern-solution, and (4) evaluating the transformation model. MUPPIT takes a transformation design model (TDM), which is a representation of the given transformation, to search for the presence of an anti-pattern of interest. If found, MUPPIT proposes a pattern solution from a catalogue of patterns to the transformation engineer. The application of the pattern solution results in the restructuring of the TDM. While MUPPIT, as a process, is independent of any transformation language and transformation engineering framework, we have implemented an instance of it as a tool using transML and MeTAGeM, which support exogenous transformations using rule-based transformation and OCL-based languages such as ATL and ETL. We evaluate MUPPIT through a number of case studies in which we show how MUPPIT can detect four anti-patterns and propose the corresponding pattern solutions. We also evaluate MUPPIT by collecting a number of metrics to assess the quality of the resulting transformations. The results show that MUPPIT optimizes the transformations by improving reusability, modularity, simplicity, and maintainability, as well as decreasing the complexity. MUPPIT can help transformation engineers to produce high-quality transformations using a pattern-based approach. An immediate future direction would be to experiment with more anti-patterns and pattern solutions. Moreover, we need to implement MUPPIT using other transformation engineering frameworks.",
        "keywords": [
            "Transformation pattern",
            "Transformation anti-pattern",
            "Model-driven engineering",
            "Transformation engineering"
        ],
        "authors": [
            "Mahsa Panahandeh",
            "Mohammad Hamdaqa",
            "Bahman Zamani",
            "Abdelwahab Hamou-Lhadj"
        ],
        "file_path": "data/sosym-all/s10270-020-00853-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From scenarios to code: An air traﬃc control case study",
        "submission-date": "2003/06",
        "publication-date": "2004/11",
        "abstract": "There has been much recent interest in synthesis algorithms that generate ﬁnite state machines from scenarios of intended system behavior. One of the uses of such algorithms is in the transition from requirements scenarios to design. Despite much theoretical work on the nature of these algorithms, there has been very little work on applying the algorithms to practical applications. In this paper, we apply the Whittle & Schumann synthesis algorithm [32] to a component of an air traﬃc advisory system under development at NASA Ames Research Center. We not only apply the algorithm to generate state machine designs from scenarios but also show how to generate code from the generated state machines using existing commercial code generation tools. The results demonstrate the possibility of generating application code directly from scenarios of system behavior.",
        "keywords": [
            "Code generation",
            "Software modeling",
            "Scenario",
            "State machine",
            "Case study"
        ],
        "authors": [
            "Jon Whittle",
            "Richard Kwan",
            "Jyoti Saboo"
        ],
        "file_path": "data/sosym-all/s10270-004-0067-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "EDITORIAL",
        "submission-date": "2017/07",
        "publication-date": "2017/08",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a two-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of information systems theory. The goals, format, and history of BPMDS can be found on the Web site http://www.bpmds.org/. This special section follows the 16th edition of the BPMDS (business process modeling, development, and support) series, organized in conjunction with CAISE’15, which was held in Stockholm, Sweden, June 2015.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-017-0615-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Query-driven soft traceability links for models",
        "submission-date": "2013/06",
        "publication-date": "2014/10",
        "abstract": "Model repositories play a central role in the model driven development of complex software-intensive systems by offering means to persist and manipulate models obtained from heterogeneous languages and tools. Complex models can be assembled by interconnecting model fragments by hard links, i.e., regular references, where the target end points to external resources using storage-speciﬁc identiﬁers. This approach, in certain application scenarios, may prove to be a too rigid and error prone way of interlinking models. As a ﬂexible alternative, we propose to combine derived features with advanced incremental model queries as means for soft interlinking of model elements residing in different model resources. These soft links can be calculated on-demand with graceful handling for temporarily unresolved references. In the background, the links are maintained efﬁciently and ﬂexibly by using incremental model query evaluation. The approach is applicable to modeling environments or even property graphs for representing query results as ﬁrst-class relations, which also allows the chaining of soft links that is useful for modular applications. The approach is evaluated using the Eclipse Modeling Framework (EMF) and EMF- IncQuery in two complex industrial case studies. The ﬁrst case study is motivated by a knowledge management project from the ﬁnancial domain, involving a complex interlinked structure of concept and business process models. The second case study is set in the avionics domain with strict traceability requirements enforced by certiﬁcation standards (DO-178b). It consists of multiple domain models describing the allocation scenario of software functions to hardware components.",
        "keywords": [
            "Soft links",
            "Incremental model queries",
            "Derived features",
            "Traceability"
        ],
        "authors": [
            "Ábel Hegedüs",
            "Ákos Horváth",
            "István Ráth",
            "Rodrigo Rizzi Starr",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-014-0436-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling cultures of the embedded software industry: feedback from the field",
        "submission-date": "2019/01",
        "publication-date": "2020/06",
        "abstract": "Engineering of modern embedded systems requires complex technical, managerial and operational processes. To cope with the complexity, modeling is a commonly used approach in the embedded software industry. The modeling approaches in embedded software vary since the characteristics of modeling such as purpose, medium type and life cycle phase differ among systems and industrial sectors. The objective of this paper is to detail the use of a characterization model MAPforES (“Modeling Approach Patterns for Embedded Software”). This paper presents the results of applying MAPforES in multiple case studies. The applications are performed in three sectors of the embedded software industry: defense and aerospace, automotive and transportation, and consumer electronics. A series of both structured and semi-structured interviews with 35 embedded software professionals were conducted as part of the case studies. The characterization model was successfully applied to these cases. The results show that identifying individual patterns provides insight for improving both individual behavior and the behavior of projects and organizations.",
        "keywords": [
            "Software modeling",
            "Embedded software",
            "Modeling patterns and cultures",
            "Characterization model",
            "Case study"
        ],
        "authors": [
            "Deniz Akdur",
            "Bilge Say",
            "Onur Demirörs"
        ],
        "file_path": "data/sosym-all/s10270-020-00810-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "The 2015 “State of the Journal” report",
        "submission-date": "2016/01",
        "publication-date": "2016/01",
        "abstract": "With the inception of SoSyM in 2001, we will this year celebrate its 15th year anniversary! Over the past 14 volumes, SoSyM has published a total of 512 different articles and editorials. The journal is doing very well and recently received an Impact Factor of 1.408. In 2015, SoSyM published 79 articles and editorials. There were 198 submissions to SoSyM during the 2015 calendar year. The acceptance rate over the past 12months has been 21.8%.",
        "keywords": [],
        "authors": [
            "Geri Georg",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-016-0515-3.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A logical approach to systems engineering artifacts: semantic relationships and dependencies beyond traceability—from requirements to functional and architectural views",
        "submission-date": "2016/03",
        "publication-date": "2017/09",
        "abstract": "Not only system assurance drives a need for semantically richer relationships across various artifacts, work products, and items of information than are implied in the terms “trace and traceability” as used in current standards and textbooks. This paper deals with the task of work-ing out artifacts in software and system development, their representation, and the analysis and documentation of the relationships between their logical contents—herein referred to as tracing and traceability; this is a richer meaning of traceability than in standards like IEEE STD 830. Among others, key tasks in system development are as follows: capturing, analyzing, and documenting system-level requirements, the step to functional system speciﬁcations, the step to architectures given by the decomposition of systems into subsystems with their connections and behavioral interac-tions. Each of these steps produces artifacts for documenting the development, as a basis for a speciﬁcation and a design rationale, for documentation, for veriﬁcation, and impact analysis of change requests. Crucial questions are how to rep-resent and formalize the content of these artifacts and how to relate their content to support, in particular, system assur-ance. When designing multi-functional systems, key artifacts are system-level requirements, functional speciﬁcations, and architecturesintermsoftheirsubsystemspeciﬁcations.Links and traces between these artifacts are introduced to relate their contents. Traceability has the goal to relate artifacts. It is required for instance in standards for functional system safety such as the ISO 26262. An approach to specifying semantic relationships is shown, such that the activity of cre-ating and using (navigating through) these relationships can be supported with automation.",
        "keywords": [
            "Speciﬁcation",
            "Architecture",
            "Trace",
            "Traceability",
            "Reﬁnement",
            "Semantic dependencies"
        ],
        "authors": [
            "Manfred Broy"
        ],
        "file_path": "data/sosym-all/s10270-017-0619-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An approach for reverse engineering of design patterns",
        "submission-date": "2002/12",
        "publication-date": "2004/04",
        "abstract": "For the maintenance of software systems, developers have to completely understand the existing system. The usage of design patterns leads to beneﬁts for new and young developers by enabling them to reuse the knowledge of their experienced colleagues. Design patterns can support a faster and better understanding of software systems. There are diﬀerent approaches for supporting pattern recognition in existing systems by tools. They are evaluated by the Information Retrieval criteria precision and recall. An automated search based on structures has a highly positive inﬂuence on the manual validation of the results by developers. This validation of graphical structures is the most intuitive technique. In this paper a new approach for automated pattern search based on minimal key structures is presented. It is able to detect all patterns described by the GOF [15]. This approach is based on positive and negative search criteria for structures and is prototypically implemented using Rational Rose and Together.",
        "keywords": [
            "Design patterns",
            "Reverse engineering",
            "Pattern recognition"
        ],
        "authors": [
            "Ilka Philippow",
            "Detlef Streitferdt",
            "Matthias Riebisch",
            "Sebastian Naumann"
        ],
        "file_path": "data/sosym-all/s10270-004-0059-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model driven design and aspect weaving",
        "submission-date": "2007/12",
        "publication-date": "2008/02",
        "abstract": "Amodelisasimpliﬁedrepresentationofanaspect\nof the world for a speciﬁc purpose. In complex systems,\nmany aspects are to be handled, from architectural aspects to\ndynamic behavior, functionalities, user-interface, and extra-\nfunctional concerns (such as security, reliability, timeliness,\netc.). For software systems, the design process can then be\ncharacterized as the weaving of all these aspects into a detai-\nled design model. Model Driven Design aims at automating\nthis weaving process, that is automatically deriving software\nsystems from theirs models. This paper explores the rela-\ntionship between modeling and aspect weaving. It points out\nsomeofthechallengesrelatedtosuchautomaticmodelweav-\ning, illustrating them with the example of a weaving process\nfor behavioral models represented as scenarios.",
        "keywords": [],
        "authors": [
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-008-0080-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines",
        "submission-date": "2016/12",
        "publication-date": "2017/07",
        "abstract": "A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.",
        "keywords": [
            "Software product lines",
            "Search-based software engineering",
            "Multi-objective evolutionary algorithms",
            "Constraint solving",
            "Feature models"
        ],
        "authors": [
            "Jianmei Guo",
            "Jia Hui Liang",
            "Kai Shi",
            "Dingyu Yang",
            "Jingsong Zhang",
            "Krzysztof Czarnecki",
            "Vijay Ganesh",
            "Huiqun Yu"
        ],
        "file_path": "data/sosym-all/s10270-017-0610-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Effects of stability on model composition effort: an exploratory study",
        "submission-date": "2011/07",
        "publication-date": "2013/01",
        "abstract": "Model composition plays a central role in many software engineering activities, e.g., evolving design models to add new features. To support these activities, developers usually rely on model composition heuristics. The problem is that the models to-be-composed usually conﬂict with each other in several ways and such composition heuristics might be unable to properly deal with all emerging conﬂicts. Hence, the composed model may bear some syntactic and semantic inconsistencies that should be resolved. As a result, the production of the intended model is an error-prone and effort-consuming task. It is often the case that developers end up examining all parts of the output composed model instead of prioritizing the most critical ones, i.e., those that are likely to be inconsistent with the intended model. Unfortunately, little is known about indicators that help developers (1) to identify which model is more likely to exhibit inconsisten-cies, and (2) to understand which composed models require more effort to be invested. It is often claimed that software systems remaining stable over time tends to have a lower number of defects and require less effort to be ﬁxed than unstable systems. However, little is known about the effects of software stability in the context of model evolution when supported by composition heuristics. This paper, therefore, presents an exploratory study analyzing stability as an indi-cator of inconsistency rate and resolution effort on model composition activities. Our ﬁndings are derived from 180 compositions performed to evolve design models of three software product lines. Our initial results, supported by statistical tests, also indicate which types of changes led to lower inconsistency rate and lower resolution effort.",
        "keywords": [
            "Model composition",
            "Software development effort",
            "Design stability"
        ],
        "authors": [
            "Kleinner Farias",
            "Alessandro Garcia",
            "Carlos Lucena"
        ],
        "file_path": "data/sosym-all/s10270-012-0308-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven analysis and synthesis of textual concrete syntax",
        "submission-date": "2007/03",
        "publication-date": "2008/04",
        "abstract": "Meta-modeling is raising more and more interest in the ﬁeld of language engineering. While this approach is now well understood for deﬁning abstract syntaxes, formally deﬁning textual concrete syntaxes with meta-models is still a challenge. Textual concrete syntaxes are traditionally expressed with rules, conforming to EBNF-like grammars, which can be processed by compiler compilers to generate parsers. Unfortunately, these generated parsers produce concrete syntax trees, leaving a gap with the abstract syntax deﬁned by meta-models, and further ad hoc hand-coding is required. In this paper we propose a new kind of speciﬁcation for concrete syntaxes, which takes advantage of meta-models to generate fully operational tools (such as parsers or text generators). The principle is to map abstract syntaxes to textual concrete syntaxes via bidirectional mapping-models with support for both model-to-text, and text-to-model transformations.",
        "keywords": [
            "MDD",
            "MDE",
            "Language engineering",
            "Meta-modeling"
        ],
        "authors": [
            "Pierre-Alain Muller",
            "Frédéric Fondement",
            "Franck Fleurey",
            "Michel Hassenforder",
            "Rémi Schnekenburger",
            "Sébastien Gérard",
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-008-0088-x.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Comparing and classifying model transformation reuse approaches across metamodels",
        "submission-date": "2018/12",
        "publication-date": "2019/11",
        "abstract": "Model transformations are essential elements of model-driven engineering (MDE) solutions, as they enable the automatic manipulation of models. MDE promotes the creation of domain-speciﬁc metamodels, but without proper reuse mechanisms, model transformations need to be developed from scratch for each new metamodel. In this paper, our goal is to understand whether transformation reuse across metamodels is needed by the community, evaluate its current state, identify practical needs and propose promising lines for further research. For this purpose, we ﬁrst report on a survey to understand the reuse approaches used currently in practice and the needs of the community. Then, we propose a classiﬁcation of reuse techniques based on a feature model and compare a sample of speciﬁc approaches—model types, concepts, a-posteriori typing, multilevel modeling, typing requirement models, facet-oriented modeling, mapping operators, constraint-based model types, and design patterns for model transformations—based on this feature model and a common example. We discuss strengths and weaknesses of each approach, provide a reading grid used to compare their features, compare with community needs, identify gaps in current transformation reuse approaches in relation to these needs and propose future research directions.",
        "keywords": [
            "Model transformation",
            "Reuse",
            "Survey",
            "Classiﬁcation",
            "Feature model"
        ],
        "authors": [
            "Jean-Michel Bruel",
            "Benoit Combemale",
            "Esther Guerra",
            "Jean-Marc Jézéquel",
            "Jörg Kienzle",
            "Juan de Lara",
            "Gunter Mussbacher",
            "Eugene Syriani",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-019-00762-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model-driven development platform selection: four industry case studies",
        "submission-date": "2020/04",
        "publication-date": "2021/01",
        "abstract": "Model-driven development platforms shift the focus of software development activity from coding to modeling for enterprises. A signiﬁcant number of such platforms are available in the market. Selecting the best ﬁtting platform is challenging, as domain experts are not typically model-driven deployment platform experts and have limited time for acquiring the needed knowledge. We model the problem as a multi-criteria decision-making problem and capture knowledge systematically about the features and qualities of 30 alternative platforms. Through four industry case studies, we conﬁrm that the model supports decision-makers with the selection problem by reducing the time and cost of the decision-making process and by providing a richer list of options than the enterprises considered initially. We show that having decision knowledge readily available supports decision-makers in making more rational, efﬁcient, and effective decisions. The study’s theoretical contribution is the observation that the decision framework provides a reliable approach for creating decision models in software production.",
        "keywords": [
            "Model-driven development platform",
            "Decision model",
            "Multi-criteria decision-making",
            "Decision support system",
            "Industry case study"
        ],
        "authors": [
            "Siamak Farshidi",
            "Slinger Jansen",
            "Sven Fortuin"
        ],
        "file_path": "data/sosym-all/s10270-020-00855-w.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Querying process models by behavior inclusion",
        "submission-date": "2012/09",
        "publication-date": "2013/12",
        "abstract": "Business processes are vital to managing organizations as they sustain a company’s competitiveness. Consequently, these organizations maintain collections of hundreds or thousands of process models for streamlining working procedures and facilitating process implementation. Yet, the management of large process model collections requires effective searching capabilities. Recent research focused on similarity search of process models, but querying process models is still a largely open topic. This article presents an approach to querying process models that takes a process example as input and discovers all models that allow replaying the behavior of the query. To this end, we provide a notion of behavioral inclusion that is based on trace semantics and abstraction. Additional to deciding a match, a closeness score is provided that describes how well the behavior of the query is represented in the model and can be used for ranking. The article introduces the formal foundations of the approach and shows how they are applied to querying large process model collections. An experimental evaluation has been conducted that confirms the suitability of the solution as well as its applicability and scalability in practice.",
        "keywords": [
            "Process model search",
            "Behavioral querying",
            "Trace inclusion",
            "Process model repositories"
        ],
        "authors": [
            "Matthias Kunze",
            "Matthias Weidlich",
            "Mathias Weske"
        ],
        "file_path": "data/sosym-all/s10270-013-0389-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Managing time-awareness in modularized processes",
        "submission-date": "2016/12",
        "publication-date": "2018/02",
        "abstract": "Managing temporal process constraints in a suitable way is crucial for long-running business processes in many application domains. However, proper support of time-aware processes is still missing in contemporary information systems. This paper tackles a particular challenge existing in this context, namely the handling of temporal constraints for modularized processes (i.e., processes comprising subprocesses), which shall enable both the reuse of process knowledge and the modular design of complex processes. In detail, this paper focuses on the representation and support of time-aware modularized processes in process-aware information systems. To this end, we present a sound and complete method to derive the duration restrictions of a time-aware (sub-)process in such a way that its temporal properties are completely speciﬁed. We then show how this characterization of a process can be utilized when reusing it as a subprocess within a modularized process. As a motivating example, we consider a compound process from healthcare. Altogether the proper handling of temporal constraints for modularized processes is crucial for the enhancement of time- and process-aware information systems.",
        "keywords": [
            "Process-aware information system",
            "Temporal constraints",
            "Subprocess",
            "Process modularity",
            "Controllability"
        ],
        "authors": [
            "Roberto Posenato",
            "Andreas Lanz",
            "Carlo Combi",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-017-0643-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A knowledge-based approach for guided development of Infrastructure as Code",
        "submission-date": "2024/01",
        "publication-date": "2025/04",
        "abstract": "Infrastructure as Code (IaC) uses versionable software code to deﬁne, deploy, and conﬁgure physical computational resources, software execution platforms, and applications. As a result, IaC enables the scalable management of complex computing environments while preventing environment drift. IaC frameworks typically offer speciﬁc languages such as the industrial Terraform, Ansible, Chef, or TOSCA—standing for Topology and Orchestration Speciﬁcation for Cloud Applications—the OASIS (Organization for the Advancement of Structured Information Standards) open standard approach to IaC. Developing high-quality IaC for deploying and managing applications demands expertise and knowledge in speciﬁc IaC languages, infrastructure resources, resource providers, quality issues in IaC scripts, and so on. While several model-driven engineering (MDE) approaches have been proposed to simplify IaC development, they cannot capture and use expert knowledge to assist with modeling tasks and MDE processes by providing interactive recommendations. This paper presents a knowledge-based framework for guiding the model-driven development of IaC. We use TOSCA as the target IaC language as it is an open standard. We enable IaC and resource experts to share their IaC and resource-related knowledge with application operational experts to help simplify the development of application deployment models. We use an ontology to record the relevant deployment knowledge and ontology reasoning to implement modeling guidance capabilities such as TOSCA model auto-completion, code smell and error detection, and model element matchmaking. We show the ﬂexibility of our methodology by applying it to three industrial applications, covering cloud, edge, and HPC (High-Performance Computing) domains. Moreover, we also assess the use acceptance of our approach and framework by conducting controlled experiments with expert and non-expert IaC users. The results indicate that our method can simplify IaC development by providing appropriate recommendations.",
        "keywords": [
            "Model-driven engineering",
            "IaC",
            "Recommendation system",
            "TOSCA",
            "Ontology",
            "Knowledge graph",
            "Semantic web"
        ],
        "authors": [
            "Zoe Vasileiou",
            "Indika Kumara",
            "Georgios Meditskos",
            "Kamil Tokmakov",
            "Dragan Radolovi´c",
            "Jesús Gorroñogoitia Cruz",
            "Elisabetta Di Nitto",
            "Damian Andrew Tamburri",
            "Willem-Jan Van Den Heuvel",
            "Stefanos Vrochidis"
        ],
        "file_path": "data/sosym-all/s10270-025-01294-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Wodel-Test: a model-based framework for language-independent mutation testing",
        "submission-date": "2019/12",
        "publication-date": "2020/10",
        "abstract": "Mutation testing (MT) targets the assessment of test cases by measuring their efﬁciency to detect faults. This technique involves modifying the program under test to emulate programming faults, and assessing whether the existing test cases detect such mutations. MT has been extensively studied since the 70’s, and many tools have been proposed for widely used languages like C, Java, Fortran, Ada and SQL; and for notations like Petri-nets. However, building MT tools is costly and error-prone, which may prevent their development for new programming and domain-speciﬁc (modelling) languages. In this paper, we propose a framework called Wodel- Test to reduce the effort to create MT tools. For this purpose, it follows a model-driven approach by which MT tools are synthesized from a high-level description. This description makes use of the domain-speciﬁc language Wodel to deﬁne and execute model mutations. Wodel is language-independent, as it allows the creation of mutation operators for any language deﬁned by a meta-model. Starting from the deﬁnition of the mutation operators, Wodel- Test generates a MT environment which parses the program under test into a model, applies the mutation operators, and evaluates the test-suite against the generated mutants, offering a rich collection of MT metrics. We report on an evaluation of the approach based on the creation of MT tools for Java and the Atlas transformation language.",
        "keywords": [
            "Mutation testing",
            "Model mutation",
            "Model-driven engineering",
            "Domain-speciﬁc languages",
            "Java",
            "Model transformation"
        ],
        "authors": [
            "Pablo Gómez-Abajo\nEsther Guerra\nJuan de Lara\nMercedes G. Merayo"
        ],
        "file_path": "data/sosym-all/s10270-020-00827-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Wodel, Atlas transformation language"
        }
    },
    {
        "title": "Detection and quantiﬁcation of ﬂow consistency in business process models",
        "submission-date": "2015/10",
        "publication-date": "2017/01",
        "abstract": "Business process models abstract complex business processes by representing them as graphical models. Their layout, as determined by the modeler, may have an effect when these models are used. However, this effect is currently not fully understood. In order to systematically study this effect, a basic set of measurable key visual features is proposed, depicting the layout properties that are meaningful to the human user. The aim of this research is thus twofold: first, to empirically identify key visual features of business process models which are perceived as meaningful to the user and second, to show how such features can be quantiﬁed into computational metrics, which are applicable to business process models. We focus on one particular feature, consistency of ﬂow direction, and show the challenges that arise when transforming it into a precise metric. We propose three different metrics addressing these challenges, each following a different view of ﬂow consistency. We then report the results of an empirical evaluation, which indicates which metric is more effective in predicting the human perception of this feature. Moreover, two other automatic evaluations describing the performance and the computational capabilities of our metrics are reported as well.",
        "keywords": [
            "Business process modeling",
            "Metrics",
            "Visual layout",
            "Qualitative empirical study",
            "Consistency of ﬂow"
        ],
        "authors": [
            "Andrea Burattin",
            "Vered Bernstein",
            "Manuel Neurauter",
            "Pnina Soffer",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-017-0576-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Foundations of information technology based on Bunge’s systemist philosophy of reality",
        "submission-date": "2020/08",
        "publication-date": "2021/01",
        "abstract": "General ontology is a prominent theoretical foundation for information technology analysis, design, and development. Ontology is a branch of philosophy which studies what exists in reality. A widely used ontology in information systems, especially for conceptual modeling, is the BWW (Bunge–Wand–Weber), which is based on ideas of the philosopher and physicist Mario Bunge, as synthesized by Wand and Weber. The ontology was founded on an early subset of Bunge’s philosophy; however, many of Bunge’s ideas have evolved since then. An important question, therefore, is: do the more recent ideas expressed by Bunge call for a new ontology? In this paper, we conduct an analysis of Bunge’s earlier and more recent works to address this question. We present a new ontology based on Bunge’s later and broader works, which we refer to as Bunge’s Systemist Ontology (BSO). We then compare BSO to the constructs of BWW. The comparison reveals both considerable overlap between BSO and BWW, as well as substantial differences. From this comparison and the initial exposition of BSO, we provide suggestions for further ontology studies and identify research questions that could provide a fruitful agenda for future scholarship in conceptual modeling and other areas of information technology.",
        "keywords": [
            "Ontology",
            "Upper-level ontology",
            "General ontology",
            "Mario Bunge",
            "Bunge",
            "Wand",
            "Weber ontology",
            "Bunge’s Systemist Ontology",
            "Conceptual modeling",
            "Software engineering",
            "Database design",
            "IT development",
            "IT design",
            "Real-world domains",
            "Reality",
            "Philosophy"
        ],
        "authors": [
            "Roman Lukyanenko",
            "Veda C. Storey",
            "Oscar Pastor"
        ],
        "file_path": "data/sosym-all/s10270-021-00862-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A platform independent model for the electronic marketplace domain",
        "submission-date": "2005/07",
        "publication-date": "2007/04",
        "abstract": "An electronic marketplace supports interactions between multiple users for the exchange of information on products for sale or purchase. The signiﬁcance of electronic marketplaces is apparent from the huge number of websites that currently provide services in almost any area one can think of. However, the absence of clear documentation on the similarities that these sites share restricts the reutilization of software for the development of new electronic marketplaces. To improve this situation, we propose a platform independent model (PIM) for the e-marketplace domain that describes both the structural and behavioral properties of a generic electronic marketplace. Speciﬁc application PIMs aimed at generating different e-marketplaces can be obtained from our generic domain PIM by adapting it to the requirements of each particular application. In this way, reutilization of our domain PIM contributes to a reduction in the cost and time involved in the development of new electronic marketplaces.",
        "keywords": [
            "Platform independent model",
            "Model driven architecture",
            "Uniﬁed modeling language",
            "Domain analysis",
            "Domain model",
            "E-marketplaces",
            "Reference models"
        ],
        "authors": [
            "Anna Queralt",
            "Ernest Teniente"
        ],
        "file_path": "data/sosym-all/s10270-007-0047-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Expressing aspectual interactions in design: evaluating three AOM approaches in the slot machine domain",
        "submission-date": "2014/01",
        "publication-date": "2014/12",
        "abstract": "In the context of an industrial project, we evaluated the implementation of the software of a casino slot machine. This software has a significant amount of cross-cutting concerns that depend on and interact with each other as well as with the modular concerns. We therefore wished to express our design using an appropriate aspect-oriented modeling approach. We therefore evaluated three candidate methodologies: Theme/UML, WEAVR, and RAM to establish their suitability. Remarkably, only the last of the three has shown to allow an adequate expression of the interactions, albeit not fully explicit. The first two fall short because half of the interaction types cannot be expressed at all while the other half need to be expressed using a work-around that hides the intention of the design. Neither does RAM allow a fully explicit expression of interactions, but it would be the most adequate approach for the slot machine case.",
        "keywords": [
            "Aspect-oriented modeling",
            "Aspect interactions",
            "Case study"
        ],
        "authors": [
            "Johan Fabry",
            "Arturo Zambrano",
            "Silvia Gordillo"
        ],
        "file_path": "data/sosym-all/s10270-014-0442-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Extracting models from source code in software modernization",
        "submission-date": "2011/03",
        "publication-date": "2012/09",
        "abstract": "Model-driven software modernization is a discipline in which model-driven development (MDD) techniques are used in the modernization of legacy systems. When existing software artifacts are evolved, they must be transformed into models to apply MDD techniques such as model transformations. Since most modernization scenarios (e.g., application migration) involve dealing with code in general-purpose programming languages (GPL), the extraction of models from GPL code is an essential task in a model-based modernization process. This activity could be performed by tools to bridge grammarware and MDD technical spaces, which is normally carried out by dedicated parsers. Grammar-to-Model Transformation Language (Gra2MoL) is a domain-speciﬁc language (DSL) tailored to the extraction of models from GPL code. This DSL is actually a text-to-model transformation language which can be applied to any code conforming to a grammar. Gra2MoL aims to reduce the effort needed to implement grammarware-MDD bridges, since building dedicated parsers is a complex and time-consuming task. Like ATL and RubyTL languages, Gra2MoL incorporates the binding concept needed to write mappings between grammar elements and metamodel elements in a simple declarative style. The language also provides a powerful query language which eases the retrieval of scattered information in syntax trees. Moreover, it incorporates extensibility and grammar reuse mechanisms. This paper describes Gra2MoL in detail and includes a case study based on the application of the language in the extraction of models from Delphi code.",
        "keywords": [
            "Model-driven engineering",
            "Model-driven software development",
            "Domain-speciﬁc languages",
            "Software modernization",
            "Model-driven software modernization"
        ],
        "authors": [
            "Javier Luis Cánovas Izquierdo",
            "Jesús García Molina"
        ],
        "file_path": "data/sosym-all/s10270-012-0270-z.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "Gra2MoL"
        }
    },
    {
        "title": "Dinosaur meets Archaeopteryx? or: Is there an alternative for Rational’s Uniﬁed Process?",
        "submission-date": "2003/05",
        "publication-date": "2003/09",
        "abstract": "Since 1999, Rational’s Uniﬁed Process (RUP) is being oﬀered as a guideline for software projects using the Uniﬁed Modeling Language (UML). RUP has been advertised to be iterative, and incremental, use case-driven and architecture-centric. These claims are discussed while RUP core concepts like phase, iteration, discipline (formerly: workﬂow) and milestone are reviewed in more detail. It turns out that the RUP constitutes a considerable step towards a broad dissemination of software process modelling ideas but some of the RUP def-initions and structures lack clear structure and are too complex and overloaded for practical use. Among others, I see the following particular prob-lems: (1) phases do still dominate the process and iteration structure, (2) the term “software architecture” is not clearly deﬁned and its role is still underestimated, (3) RUP “disciplines” are a partly redundant concept complicating the process more than supporting it, (4) powerful and transparent structuring principles like recursion and orthogonality do not get the attention they deserve. As an alternative, our model for Evolutionary, Object-oriented Softwaredevelopment (EOS) is contrasted with the RUP.",
        "keywords": [
            "Software process modeling",
            "Rational Uniﬁed Process (RUP)",
            "Evolutionary software development (EOS model)",
            "Architecture-centric fractal software process",
            "Project management support"
        ],
        "authors": [
            "Wolfgang Hesse"
        ],
        "file_path": "data/sosym-all/s10270-003-0033-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Theoretical foundations and implementation of business process diagrams’ complexity management technique based on highlights",
        "submission-date": "2016/11",
        "publication-date": "2017/08",
        "abstract": "The main purpose of business process diagrams is to make the communication between process-related stakeholders more effective. To this end, they need to be simple to read, which is often challenging to achieve. In this manner, the complexity of business process diagrams can negatively affect their correctness and understandability. The goal of this paper was to investigate an approach that makes business process diagrams appear less complex, without changing the corresponding notation. This was done by manipulating one of the properties of the notation’s elements, namely opacity. Firstly, a literature overview was performed in order to obtain the theoretical foundations. Secondly, an exploratory case study was conducted and the results were applied in practice. Finally, the proposed solution was implemented in the form of a prototype software solution. Our analysis demonstrated that the structural complexity of the diagrams decreases when applying the proposed solution.",
        "keywords": [
            "Business process diagram",
            "Complexity",
            "Highlights",
            "Opacity",
            "BPMN"
        ],
        "authors": [
            "Gregor Jošt",
            "Marjan Heriˇcko",
            "Gregor Polanˇciˇc"
        ],
        "file_path": "data/sosym-all/s10270-017-0618-5.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Span(Graph): a canonical feedback algebra of open transition systems",
        "submission-date": "2022/03",
        "publication-date": "2023/03",
        "abstract": "We show that Span(Graph)∗, an algebra for open transition systems introduced by Katis, Sabadini and Walters, satisfies\na universal property. By itself, this is a justiﬁcation of the canonicity of this model of concurrency. However, the universal\nproperty is itself of interest, being a formal demonstration of the relationship between feedback and state. Indeed, feedback\ncategories, also originally proposed by Katis, Sabadini and Walters, are a weakening of traced monoidal categories, with\nvarious applications in computer science. A state bootstrapping technique, which has appeared in several different contexts,\nyields free such categories. We show that Span(Graph)∗arises in this way, being the free feedback category over Span(Set).\nGiven that the latter can be seen as an algebra of predicates, the algebra of open transition systems thus arises—roughly\nspeaking—as the result of bootstrapping state to that algebra. Finally, we generalize feedback categories endowing state\nspaces with extra structure: this extends the framework from mere transition systems to automata with initial and ﬁnal states.",
        "keywords": [],
        "authors": [
            "Elena Di Lavore",
            "Alessandro Gianola",
            "Mario Román",
            "Nicoletta Sabadini",
            "Paweł Soboci´nski"
        ],
        "file_path": "data/sosym-all/s10270-023-01092-7.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Automated testing of metamodels and code co-evolution",
        "submission-date": "2023/08",
        "publication-date": "2024/12",
        "abstract": "Metamodels are cornerstone in MDE. They deﬁne the different domain concepts and the relations between them. A metamodel\nis also used to generate concrete artifacts such as code. Developers then rely on the generated code to build their language\nservices and tooling, e.g., editors, checkers. To check the behavior of their client code, developers write or generate unit\ntests. As metamodels evolve between releases, the generated code is automatically updated. As a consequence, the additional\ndevelopers’ code is impacted and is co-evolved accordingly for each release. However, there is no guarantee that the co-\nevolution of the code is performed correctly. One way to do so is to rerun all the tests after each code co-evolution, which\nis expensive and time-consuming. This paper proposes an automatic solution for tracing impacted tests due to metamodel\nevolution. Thus, we end up matching metamodel changes with impacted code methods and their corresponding tests in both\nthe original and evolved versions of a given project. After that, we map the two versions of the impacted tests and compare\nthem to analyze the behavior of the code before and after its evolution due to the metamodel evolution. In particular, we\nimplemented an Eclipse plug-in that allows tracing, mapping, execution, and reporting back the results to the developers for\neasier in-depth analysis of the effect of metamodel evolutions rather than analyzing the whole test suite. We ﬁrst ran a user\nstudy experiment to gain evidence on the difﬁculty or not of the manual task of tracing impacted tests. We found that manually\ntracing the tests impacted by the evolution of the metamodel is a hard and error-prone task. Not only the participants could\nnot trace all tests, but they even wrongly traced non-impacted tests. We then evaluated our approach on 18 Eclipse projects\nfrom OCL, Modisco, Papyrus, and EMF over several evolved versions of metamodels. For the 14 projects without manual\ntests, we generated a test suite for each release with the state-of-the-art tool EvoSuite. The results show that we successfully\ntraced the impacted tests automatically by selecting 1608 out of 34,612 tests due to 473 metamodel changes. When running\nthe traced tests before and after co-evolution, we observed cases indicating possibly both behaviorally correct and incorrect\ncode co-evolution. Finally, we reached gains representing, on average, a reduction of 88% in the number of tests and 84% in\nthe execution time.",
        "keywords": [
            "Metamodel evolution",
            "Code co-evolution",
            "Unit tests",
            "Testing co-evolution"
        ],
        "authors": [
            "Zohra Kaouter Kebaili",
            "Djamel Eddine Khelladi",
            "Mathieu Acher",
            "Olivier Barais"
        ],
        "file_path": "data/sosym-all/s10270-024-01245-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Execution of UML models: a systematic review of research and practice",
        "submission-date": "2017/07",
        "publication-date": "2018/04",
        "abstract": "Several research efforts from different areas have focused on the execution of UML models, resulting in a diverse and complex scientiﬁc body of knowledge. With this work, we aim at identifying, classifying, and evaluating existing solutions for the execution of UML models. We conducted a systematic review in which we selected 63 research studies and 19 tools among over 5400 entries by applying a systematic search and selection process. We deﬁned a classiﬁcation framework for characterizing solutions for UML model execution, and we applied it to the 82 selected entries. Finally, we analyzed and discussed the obtained data. From the analyzed data, we drew the following conclusions: (i) There is a growing scientiﬁc interest on UML model execution; (ii) solutions providing translational execution clearly outnumber interpretive solutions; (iii) model-level debugging is supported in very few cases; (iv) only a few research studies provide evidence of industrial use, with very limited empirical evaluations; (v) the most common limitation deals with coverage of the UML language. Based on these observations, we discuss potential research challenges and implications for the future of UML model execution. Our results provide a concise overview of states of the art and practice for UML model execution intended for use by both researchers and practitioners.",
        "keywords": [
            "UML",
            "Model execution",
            "Code generation",
            "Model compilation",
            "Model interpretation",
            "Systematic review"
        ],
        "authors": [
            "Federico Ciccozzi",
            "Ivano Malavolta",
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-018-0675-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An efﬁcient and scalable search engine for models",
        "submission-date": "2021/02",
        "publication-date": "2021/12",
        "abstract": "Search engines extract data from relevant sources and make them available to users via queries. A search engine typically crawls the web to gather data, analyses and indexes it and provides some query mechanism to obtain ranked results. There exist search engines for websites, images, code, etc., but the speciﬁc properties required to build a search engine for models have not been explored much. In the previous work, we presented MAR, a search engine for models which has been designed to support a query-by-example mechanism with fast response times and improved precision over simple text search engines. The goal of MAR is to assist developers in the task of ﬁnding relevant models. In this paper, we report new developments of MAR which are aimed at making it a useful and stable resource for the community. We present the crawling and analysis architecture with which we have processed about 600,000 models. The indexing process is now incremental and a new index for keyword-based search has been added. We have also added a web user interface intended to facilitate writing queries and exploring the results. Finally, we have evaluated the indexing times, the response time and search precision using different conﬁgurations. MAR has currently indexed over 500,000 valid models of different kinds, including Ecore meta-models, BPMN diagrams, UML models and Petri nets. MAR is available at http://mar-search.org.",
        "keywords": [
            "Model repositories",
            "Search engines",
            "Model-driven engineering"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-021-00960-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Models for digitalization",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "To continue the tradition of SoSyM editorials by high-lighting speciﬁc topics in modeling research, we examine the emerging trend of “digitalization,” which represents the integration of multiple technologies into all aspects of daily life that can be digitized. A few examples of digitalization include smart homes (for entertainment, security, childcare, electrical, and heating), e-healthcare, smart mobility, and smart cities. The widespread impact of digitalization affects everything from personal relationships augmented by social media and their services, to other relationships such as how citizens interact with support services in e-government. Gartner deﬁnes digitalization with a more business-oriented focus: “Digitalization is the use of digital technologies to change a business model and provide new revenue and value-producing opportunities; it is the process of mov-ing to a digital business.” (gartner.com/it-glossary in August 2015). This deﬁnition spans relationships between different businesses, in addition to business and government, and the vital relationship to customers. A goal is to realize digitalizationsuchthatthereisaclearrelationshipbetweentheservices offered by businesses and the actual needs of customers.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-015-0494-9.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Clarifying matters of (meta-) modeling: an author’s reply",
        "submission-date": "2006/09",
        "publication-date": "2006/10",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-006-0034-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Property-based testing of web services by deriving properties from business-rule models",
        "submission-date": "2016/09",
        "publication-date": "2017/12",
        "abstract": "Property-based testing is well suited for web-service applications, which was already shown in various case studies. For example, it has been demonstrated that JSON schemas can be used to automatically derive test case generators for web forms. In this work, we present a test case generation approach for a rule engine-driven web-service application. Business-rule models serve us as input for property-based testing. We parse these models to automatically derive generators for sequences of web-service requests together with their required form data. Property-based testing is mostly applied in the context of functional programming. Here, we deﬁne our properties in an object-oriented style in C# and its tool FsCheck. We apply our method to the business-rule models of an industrial web-service application in the automotive domain.",
        "keywords": [
            "Model-based testing",
            "Test case generation",
            "Property-based testing",
            "QuickCheck",
            "FsCheck",
            "Web services",
            "Business-rule models"
        ],
        "authors": [
            "Bernhard K. Aichernig",
            "Richard Schumi"
        ],
        "file_path": "data/sosym-all/s10270-017-0647-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An architecture for coupled digital twins with semantic lifting",
        "submission-date": "2023/08",
        "publication-date": "2024/11",
        "abstract": "To enable the reuse of Digital Twins, in the form of simulation units or other forms of behavioral models, of single physical components, one must be able to connect and couple them. Current platform and architectures consider mostly monolithic digital twins and offer little support for coupling and checking the consistency of the coupling. The coupling must be internally consistent—satisfy constraints related to their co-simulation—and externally consistent—mirror the structure of the composed physical system. In this paper, we propose an extension to a behavior-extended Digital Twin architecture for individual Digital Twins to include co-simulation scenarios for coupled systems lifted from configuration files, which can be implemented along with a Digital-Twin-as-a-Service platform to make assets reusable in time. To monitor and query these connections, we introduce a semantic lifting service, which interprets the coupled Digital Twins as Knowledge Graphs and enables the use of queries to express internal and external consistency constraints. Two representative case studies for systems with coupled behavior are used for the demonstration of this approach and show that it indeed enables reusability of components and services between different Digital Twins.",
        "keywords": [
            "Digital twin",
            "Knowledge graph",
            "Behavioral model",
            "Co-simulation"
        ],
        "authors": [
            "Santiago Gil",
            "Eduard Kamburjan",
            "Prasad Talasila",
            "Peter Gorm Larsen"
        ],
        "file_path": "data/sosym-all/s10270-024-01221-w.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Reconciling software requirements and architectures with intermediate models",
        "submission-date": "2003/12",
        "publication-date": "2003/12",
        "abstract": "Little guidance and few methods are avail-\nable for the reﬁnement of software requirements into an\narchitecture satisfying those requirements. Part of the\nchallenge stems from the fact that requirements and ar-\nchitectures use diﬀerent terms and concepts to capture\nthe model elements relevant to each. In this paper we will\npresent CBSP, a lightweight approach intended to pro-\n vide a systematic way of reconciling requirements and\n architectures using intermediate models. CBSP lever-\n ages a simple set of architectural concepts (components,\n connectors, overall systems, and their properties) to re-\ncast and reﬁne the requirements into an intermediate\nmodel facilitating their mapping to architectures. Fur-\nthermore, the intermediate CBSP model eases captur-\ning and maintaining arbitrarily complex relationships be-\ntween requirements and architectural model elements, as\nwell as among CBSP model elements. We have applied\nCBSP within the context of diﬀerent requirements and\narchitecture deﬁnition techniques. We leverage that ex-\nperience in this paper to demonstrate the CBSP method\nand tool support using a large-scale example.",
        "keywords": [
            "Requirements elicitation and negotiation",
            "Architecture modeling",
            "Intermediate models",
            "Trace-\nability"
        ],
        "authors": [
            "Paul Gr¨unbacher",
            "Alexander Egyed",
            "Nenad Medvidovic"
        ],
        "file_path": "data/sosym-all/s10270-003-0038-6.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Service feature modeling: modeling and participatory ranking of service design alternatives",
        "submission-date": "2012/08",
        "publication-date": "2014/05",
        "abstract": "The design of software-intensive service systems involves and affects numerous stakeholders including software engineers, legal and business experts as well as a potentially large number of consumers. In consequence, the challenge arises to adequately represent the interests of these groups with respect to service design decisions. Specifically, shared service design artifacts and participatory methods for influencing their development in consensus are required, which are not yet state of the art in software service engineering. To this end, we present service feature modeling. Using a modeling notation based on feature-oriented analysis, our approach can represent and interrelate diverse service design concerns and capture their potential combinations as service design alternatives. We further present a method that allows stakeholders to rank service design alternatives based on their preferences. The ranking can support service engineers in selecting viable alternatives for implementation. To exploit this potential, we have implemented a toolkit to enable both modeling and participative ranking of service design alternatives. It has been used to apply service feature modeling in the context of public service design and evaluate the approach in this context.",
        "keywords": [
            "Software service engineering",
            "Participatory design",
            "Service variation modeling",
            "Multi-criteria feature configuration decisions"
        ],
        "authors": [
            "Erik Wittern",
            "Christian Zirpins"
        ],
        "file_path": "data/sosym-all/s10270-014-0414-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special section on ECMFA and ICMT at STAF 2016",
        "submission-date": "2018/01",
        "publication-date": "2018/02",
        "abstract": "Model-based engineering (MBE) is an approach to the design, analysis and development of software and systems that relies on exploiting high-level models and computer-based automation to achieve significant boosts in both productivity and quality. Model transformation (MT) is the field where engineers leverage different transformation paradigms to solve complex transformation problems. The 12th European Conference on Modelling Foundations and Applications (ECMFA) and the 9th International Conference on Theory and Practice of Model Transformations (ICMT) were held as part of STAF 2016 in Vienna, Austria, from July 4, 2016, to July 5, 2016.",
        "keywords": [],
        "authors": [
            "Pieter Van Gorp",
            "Andrzej Wąsowski"
        ],
        "file_path": "data/sosym-all/s10270-018-0659-4.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Active model learning of stochastic reactive systems (extended version)",
        "submission-date": "2022/07",
        "publication-date": "2024/03",
        "abstract": "Black-box systems are inherently hard to verify. Many veriﬁcation techniques, like model checking, require formal models as a basis. However, such models often do not exist, or they might be outdated. Active automata learning helps to address this issue by offering to automatically infer formal models from system interactions. Hence, automata learning has been receiving much attention in the veriﬁcation community in recent years. This led to various efﬁciency improvements, paving the way toward industrial applications. Most research, however, has been focusing on deterministic systems. In this article, we present an approach to efﬁciently learn models of stochastic reactive systems. Our approach adapts L∗-based learning for Markov decision processes, which we improve and extend to stochastic Mealy machines. When compared with previous work, our evaluation demonstrates that the proposed optimizations and adaptations to stochastic Mealy machines can reduce learning costs by an order of magnitude while improving the accuracy of learned models.",
        "keywords": [
            "Active automata learning",
            "Model mining",
            "Probabilistic veriﬁcation",
            "Stochastic mealy machines",
            "Markov decision processes"
        ],
        "authors": [
            "Edi Muškardin",
            "Martin Tappler",
            "Bernhard K. Aichernig",
            "Ingo Pill"
        ],
        "file_path": "data/sosym-all/s10270-024-01158-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "ExpRunA: a domain-speciﬁc approach for technology-oriented experiments",
        "submission-date": "2018/10",
        "publication-date": "2019/08",
        "abstract": "Conducting technology-oriented experiments (i.e., experiments in which treatments are applied to objects by a computer-based tool) without proper tool support is often a time-consuming and highly error-prone task. Although many techniques have been proposed to help conducting controlled experiments, none of them simultaneously addresses (1) the executable speciﬁcation of experiments at a high level of abstraction; (2) automated treatment execution and automated data analysis from the experiment speciﬁcation; and (3) formal guaranties of the correctness of results according to an experiment speciﬁcation for technology-oriented experiments. To address these issues, we provide a Domain-Speciﬁc Modeling approach to create a Web-based tool (ExpRunA) comprising a Domain-Speciﬁc Language named ToExpDSL, execution and analysis script generators, a supporting framework, and a running infrastructure. An experimenter uses ToExpDSL to specify an experiment using experimentation concepts. From this speciﬁcation, applications corresponding to the underlying treatments are executed, execution results are collected and analyzed, and, ﬁnally, the analysis results are presented to the experimenter. We establish the consistency of such results with respect to the experiment speciﬁcation by formalizing and proving key correctness properties of ExpRunA. We empirically evaluated ExpRunA with respect to automation by replicating three already published experiments; we evaluated the level of abstraction by a qualitative assessment. Our empirical evaluation shows that ToExpDSL is expressive enough to specify three technology-oriented experiments and that ExpRunA can be used to enable sound automation of execution and analysis from the speciﬁcation of technology-oriented experiments at a high level of abstraction.",
        "keywords": [
            "Controlled experiments",
            "Technology-oriented experiments",
            "Domain-speciﬁc modeling",
            "Domain-speciﬁc language"
        ],
        "authors": [
            "Eneias Silva",
            "Alessandro Leite",
            "Vander Alves",
            "Sven Apel"
        ],
        "file_path": "data/sosym-all/s10270-019-00749-6.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Machine learning for enterprise modeling assistance: an investigation of the potential and proof of concept",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Though modeling tools are developing fast, today, enterprise modeling is still a highly manual task that requires substantial\nhuman effort. Today, human modelers are not only assigned the creative component of the process, but they also need to\nperform routine work related to comparing the being developed model with existing ones. Larger amount of information\navailable today makes it possible for a modeler to analyze more information and existing models when developing own\nmodels. However, it also complicates the process since the modeler is often not able to analyze all of them. In this work, we\ndiscuss the potential of the novel idea of using machine learning methods for enterprise modeling assistance that would beneﬁt\nfrom their ability to discover tacit knowledge/regularities in the available data. Graph neural networks have been chosen as\nthe main technique. The contribution lies in the proposed modeling assistance scenarios as well as carried out evaluation of\nthe potential beneﬁts for the modeler. The presented illustrative case study scenario is aimed to demonstrate the feasibility of\nthe proposed approach. The viability and potential of the idea are proved via experiments.",
        "keywords": [
            "Enterprise modeling",
            "Assisted modeling",
            "Machine learning",
            "Graph neural networks"
        ],
        "authors": [
            "Nikolay Shilov",
            "Walaa Othman",
            "Michael Fellmann",
            "Kurt Sandkuhl"
        ],
        "file_path": "data/sosym-all/s10270-022-01077-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Special section of business process modeling, development and support (BPMDS) 2018: new perspectives for business process modeling, development and support",
        "submission-date": "2020/08",
        "publication-date": "2020/09",
        "abstract": "The Business Process Modeling, Development and Support (BPMDS) working conference series serves as a meeting place for researchers and practitioners in the areas of business development and business applications (software) development. By incorporating these multiple views, BPMDS offers a unique community venue that integrates different streams of research on business processes and business information systems, and enables to take in a view on the whole range of BPMDS research and interrelationships among different perspectives. This makes it attractive for authors to publish cutting edge research results at BPMDS. This special section contains a selection of the most inﬂuential contributions from the 2018 edition of the working conference.",
        "keywords": [],
        "authors": [
            "Jens Gulden",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-020-00826-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Quo Vadis modeling? Findings of a community survey, an ad-hoc bibliometric analysis, and expert interviews on data, process, and software modeling",
        "submission-date": "2023/07",
        "publication-date": "2023/10",
        "abstract": "Models are the key tools humans use to manage complexity in description, development, and analysis. This applies to all scientific and engineering disciplines and in particular to the development of software and data-intensive systems. However, different methods and terminologies have become established in the individual disciplines, even in the sub-fields of Informatics, which raises the need for a comprehensive and cross-sectional analysis of the past, present, and future of modeling research. This paper aims to shed some light on how different modeling disciplines emerged and what characterizes them with a discussion of the potential toward a common modeling future. It focuses on the areas of software, data, and process modeling and reports on an analysis of the research approaches, goals, and visions pursued in each, as well as the methods used. This analysis is based on the results of a survey conducted in the communities concerned, on a bibliometric study, and on interviews with a prominent representative of each of these communities. The paper discusses the different viewpoints of the communities, their commonalities and differences, and identifies possible starting points for further collaboration. It further discusses current challenges for the communities in general and modeling as a research topic in particular and highlights visions for the future.",
        "keywords": [
            "Research communities",
            "Software engineering",
            "Software modeling",
            "Data modeling",
            "Process modeling",
            "Information systems"
        ],
        "authors": [
            "Judith Michael",
            "Dominik Bork",
            "Manuel Wimmer",
            "Heinrich C. Mayr"
        ],
        "file_path": "data/sosym-all/s10270-023-01128-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An approach based on the domain perspective to develop WSAN applications",
        "submission-date": "2014/11",
        "publication-date": "2015/09",
        "abstract": "As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.",
        "keywords": [
            "WSAN applications",
            "Model-driven architecture",
            "Domain-speciﬁc language",
            "UML proﬁle",
            "Architecture",
            "Code generation",
            "Abstraction"
        ],
        "authors": [
            "Taniro Rodrigues",
            "Flávia C. Delicato",
            "Thais Batista",
            "Paulo F. Pires",
            "Luci Pirmez"
        ],
        "file_path": "data/sosym-all/s10270-015-0498-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "MBIPV: a model-based approach for identifying privacy violations from software requirements",
        "submission-date": "2022/01",
        "publication-date": "2022/12",
        "abstract": "Nowadays, large-scale software systems in many domains, such as smart cities, involve multiple parties whose privacy policies may conﬂict with each other, and thus, data privacy violations may arise even without users being aware of it. In this context, identifying data security requirements and detecting potential privacy violations are crucial. In the area of model-based security requirements analysis, numerous research efforts have been done. However, few existing studies support automatic privacy violation identiﬁcation from software requirements. To ﬁll this gap, this paper presents MBIPV, a Model-Based approach for Identifying Privacy Violations from software requirements. First, this paper identiﬁes six types of privacy violations in software requirements. Second, the MBIPV proﬁle is proposed to support modeling software requirements using UML. Third, the MBIPV prototype tool is developed to generate formal models and corresponding privacy properties automatically. Then, the privacy properties are automatically veriﬁed by model checking. We evaluated the MBIPV method through case studies of four representative software systems from different domains: smart health, smart transportation, smart home, and e-commerce. The results show that MBIPV has high accuracy and efﬁciency in identifying the privacy violations from the software requirements. To the best of our knowledge, MBIPV is the ﬁrst model-based approach that supports the automatic veriﬁcation of privacy properties of UML software requirement models. The source code of the MBIPV tool and the experimental data are available online at https://github.com/YETONG1219/MBIPV.",
        "keywords": [
            "UML",
            "Software requirement modeling",
            "Privacy violation",
            "Formal modeling",
            "Formal veriﬁcation"
        ],
        "authors": [
            "Tong Ye",
            "Yi Zhuang",
            "Gongzhe Qiao"
        ],
        "file_path": "data/sosym-all/s10270-022-01072-3.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Workﬂow patterns put into context",
        "submission-date": "2012/01",
        "publication-date": "2012/02",
        "abstract": "In his paper “Approaches to Modeling Business Processes. A Critical Analysis of BPMN, Workﬂow Pat-terns and YAWL”, Egon Börger criticizes the work of the Workﬂow Patterns Initiative in a rather provocative manner. Although the workﬂow patterns and YAWL are well estab-lished and frequently used, Börger seems to misunderstand the goals and contributions of the Workﬂow Patterns Initia-tive. Therefore, we put the workﬂow patterns and YAWL in their historic context. Moreover, we address some of the criticism of Börger by pointing out the real purpose of the workﬂow patterns and their relationship to formal languages (Petri nets) and real-life WFM/BPM systems.",
        "keywords": [
            "Workﬂow patterns",
            "YAWL",
            "Petri nets",
            "Business process management"
        ],
        "authors": [
            "W. M. P. van der Aalst",
            "A. H. M. ter Hofstede"
        ],
        "file_path": "data/sosym-all/s10270-012-0233-4.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Guest editorial to the special issue on model transformation",
        "submission-date": "2011/08",
        "publication-date": "2011/08",
        "abstract": "Models are widespread in systems engineering, and provide a mechanism for managing and controlling complexity when developing large-scale complex IT systems. Models help engineers capture essential information at a suitable level of abstraction, and reason about that information in precise ways. Models are at the heart of model-driven approaches to systems engineering. A key operation in model-driven engineering is model transformation, which is the practice of deﬁning and implementing operations on models. Model transformations reﬁne and/or evolve a model into a different artefact: a new model (e.g., expressed in a different language), an abstraction of the original model, text (e.g., source code), or some other representation needed for a speciﬁc domain context. Model transformation approaches are becoming mainstream in model-driven engineering: there are now numerous model transformation languages and tools that allow the speciﬁcation, implementation, orchestration and execution of transformations, applied to different languages and on different platforms. The research ﬁeld is extremely active, with substantial research efforts in understanding the advantages and disadvantages of different approaches to model transformation, foundational principles, semantics of transformation languages, and model transformation properties like modularity and composability. There is also substantial interest in treatment of model transformations as reusable assets—models in and of themselves—and on developing rigorous methodologies for the construction of transformations that are ﬁt-for-purpose. Many of these issues are the focus of this special issue. This special issue synthesises some of the advanced, state-of-the-art research in model transformation. The selected papers also provide an overview of current open issues and identify potential lines for further research.",
        "keywords": [],
        "authors": [
            "Richard F. Paige",
            "Jeff Gray"
        ],
        "file_path": "data/sosym-all/s10270-011-0209-9.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Modeling modeling modeling",
        "submission-date": "2010/03",
        "publication-date": "2010/08",
        "abstract": "Model-driven engineering and model-based approaches have permeated all branches of software engineering to the point that it seems that we are using models, as Molière’s Monsieur Jourdain was using prose, without knowing it. At the heart of modeling, there is a relation that we establish to represent something by something else. In this paper we review various definitions of models and relations between them. Then, we deﬁne a canonical set of relations that can be used to express various kinds of representation relations and we propose a graphical concrete syntax to represent these relations. We also define a structural definition for this language in the form of a metamodel and a formal interpretation using Prolog. Hence, this paper is a contribution towards a theory of modeling.",
        "keywords": [
            "Model",
            "Metamodel",
            "Notation",
            "Representation"
        ],
        "authors": [
            "Pierre-Alain Muller",
            "Frédéric Fondement",
            "Benoît Baudry",
            "Benoît Combemale"
        ],
        "file_path": "data/sosym-all/s10270-010-0172-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "From analytical purposes to data visualizations: a decision process guided by a conceptual framework and eye tracking",
        "submission-date": "2017/10",
        "publication-date": "2019/07",
        "abstract": "Data visualizations are versatile tools for gaining cognitive access to large amounts of data and for making complex relationships in data understandable. This paper proposes a method for assessing data visualizations according to the purposes they fulfill in domain-specific data analysis settings. We introduce a framework that gets configured for a given analysis domain and allows to choose data visualizations in a methodically justified way, based on analysis questions that address different aspects of data to be analyzed. Based on the concepts addressed by the analysis questions, the framework provides systematic guidance for determining which data visualizations are able to serve which conceptual analysis interests. In a second step of the method, we propose to follow a data-driven approach and to experimentally compare alternative data visualizations for a particular analytical purpose. More specifically, we propose to use eye tracking to support justified decisions about which of the data visualizations selected with the help of the framework are most suitable for assessing the analysis domain in a cognitively efficient way. We demonstrate our approach of how to come from analytical purposes to data visualizations using the example domain of Process Modeling Behavior Analysis. The analyses are performed on the background of representative analysis questions from this domain.",
        "keywords": [
            "Data visualization",
            "Process execution data",
            "Process Modeling Behavior Analysis",
            "Eye tracking",
            "Reading patterns",
            "Process mining"
        ],
        "authors": [
            "Jens Gulden",
            "Andrea Burattin",
            "Amine A. Andaloussi",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-019-00742-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A functional size measurement method for object-oriented conceptual schemas: design and evaluation issues",
        "submission-date": "2004/10",
        "publication-date": "2005/09",
        "abstract": "Functional Size Measurement (FSM) methods are intended to measure the size of software by quantifying the functional user requirements of the software. The capability to accurately quantify the size of software in an early stage of the development lifecycle is critical to software project managers for evaluating risks, developing project estimates and having early project indicators. In this paper, we present OO-Method Function Points (OOmFP), which is a new FSM method for object-oriented systems that is based on measuring conceptual schemas. OOmFP is presented following the steps of a process model for software measurement. Using this process model, we present the design of the measurement method, its application in a case study, and the analysis of different evaluation types that can be carried out to validate the method and to verify its application and results.",
        "keywords": [
            "Conceptual modeling",
            "Object orientation",
            "Functional size measurement",
            "Measure validation",
            "Measurement veriﬁcation"
        ],
        "authors": [
            "Silvia Abrahão",
            "Geert Poels",
            "Oscar Pastor"
        ],
        "file_path": "data/sosym-all/s10270-005-0098-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Paradigm shift in mechanical system design: toward automated and collaborative design with digital twin web",
        "submission-date": "2023/08",
        "publication-date": "2024/10",
        "abstract": "Analyzing multi-vendor mechanical system designs requires a signiﬁcant amount of manual work, resulting in a design paradigm where analysis is conducted only after the design is locked and components are selected. This leads to a suboptimal design with compatibility issues, over-dimensioned components, inferior performance, poor energy efﬁciency, and a lack of collaboration between OEMs (original equipment manufacturers) and system integrators. To overcome these issues, this paper proposes Co-Des (collaborative design) framework for automated and collaborative multi-vendor system design. The framework relies on standardized digital twin documents (DTD) of system designs, components, and analyses. The discov-erability and distribution of these DTDs are enabled with digital twin web (DTW). Co-Des framework allows for ﬁnding suitable components for the design task by automatically running selected analyses employing component digital twins. In addition, OEMs can provide customized components for system integrators using the initial system design deﬁned in the system design DTD. The use of the Co-Des framework was demonstrated with a windmill powertrain design use case, and the applicability of the automated assembly analysis for component selection was veriﬁed with performance measurements. The adoption of the proposed framework will lead to a paradigm shift from manual and siloed work relying on the exchange of PDFs to a more automated and collaborative design of mechanical systems. The adoption rate is deﬁned by the willingness of system integrators to publish their initial system designs and OEMs their components as public digital twins.",
        "keywords": [
            "Collaborative design",
            "Simulation",
            "Digital twin",
            "Digital twin web",
            "Industry 4.0",
            "Open-source"
        ],
        "authors": [
            "Riku Ala-Laurinaho",
            "Juuso Autiosalo",
            "Sampo Laine",
            "Urho Hakonen",
            "Raine Viitala"
        ],
        "file_path": "data/sosym-all/s10270-024-01215-8.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Ontology-based security modeling in ArchiMate",
        "submission-date": "2023/04",
        "publication-date": "2024/02",
        "abstract": "Enterprise Risk Management involves the process of identiﬁcation, evaluation, treatment, and communication regarding risks throughout the enterprise. To support the tasks associated with this process, several frameworks and modeling languages have been proposed, such as the Risk and Security Overlay (RSO) of ArchiMate. An ontological investigation of this artifact would reveal its adequacy, capabilities, and limitations w.r.t. the domain of risk and security. Based on that, a language redesign can be proposed as a reﬁnement. Such analysis and redesign have been executed for the risk elements of the RSO grounded in the Common Ontology of Value and Risk. The next step along this line of research is to address the following research problems: What would be the outcome of an ontological analysis of security-related elements of the RSO? That is, can we identify other semantic deﬁciencies in the RSO through an ontological analysis? Once such an analysis is provided, can we redesign the security elements of the RSO accordingly, in order to produce an improved artifact? Here, with the aid of the Reference Ontology for Security Engineering (ROSE) and the ontological theory of prevention behind it, we address the remaining gap by proceeding with an ontological analysis of the security-related constructs of the RSO. The outcome of this assessment is an ontology-based redesign of the ArchiMate language regarding security modeling. In a nutshell, we report the following contributions: (1) an ontological analysis of the RSO that identiﬁes six limitations concerning security modeling; (2) because of the key role of the notion of prevention in security modeling, the introduction of the ontological theory of prevention in ArchiMate; (3) a well-founded redesign of security elements of ArchiMate; and (4) ontology-based security modeling patterns that are logical consequences of our proposal of redesign due to its underlying ontology of security. As a form of evaluation, we show that our proposal can describe risk treatment options, according to ISO 31000. Finally, besides presenting multiple examples, we proceed with a real-world illustrative application taken from the cybersecurity domain.",
        "keywords": [
            "Security modeling",
            "Ontological analysis",
            "Ontological patterns",
            "Enterprise architecture",
            "ArchiMate",
            "Reference ontology for security engineering",
            "Uniﬁed foundational ontology"
        ],
        "authors": [
            "Ítalo Oliveira",
            "Tiago Prince Sales",
            "João Paulo A. Almeida",
            "Riccardo Baratella",
            "Mattia Fumagalli",
            "Giancarlo Guizzardi"
        ],
        "file_path": "data/sosym-all/s10270-024-01149-1.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the realizability of collaborative services",
        "submission-date": "2010/04",
        "publication-date": "2011/10",
        "abstract": "This paper considers compositional speciﬁcations of services using UML 2 collaborations, activity and interaction diagrams, and addresses the realizability problem for such speciﬁcations: given a global speciﬁcation, can we construct a set of communicating system components whose joint behavior is precisely the speciﬁed global behavior? We approach the problem by looking at how the sequencing of collaborationsandlocalactionsmaybedescribedusingUML activity diagrams. We identify the realizability problems for each of the sequencing operators, such as strong and weak sequence, choice of alternatives, loops, and concurrency. The nature of these realizability problems and possible solutions are discussed. This brings a new look at already known prob-lems: we show that given some conditions, certain problems can already be detected at an abstract level, without looking at the detailed interactions of the collaborations, provided that we know the components that initiate and terminate the different collaborations.",
        "keywords": [
            "Service composition",
            "Compositional speciﬁcation of collaborations",
            "Realizability of distributed implementations",
            "Distributed system design",
            "Design guidelines",
            "Deriving component behavior from global speciﬁcations",
            "Workﬂow for collaborations",
            "UML activity diagrams",
            "Service oriented architecture"
        ],
        "authors": [
            "Humberto Nicolás Castejón",
            "Gregor von Bochmann",
            "Rolv Bræk"
        ],
        "file_path": "data/sosym-all/s10270-011-0216-x.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A model and workﬂow-driven approach for engineering domain-speciﬁc low-code platforms and applications",
        "submission-date": "2025/02",
        "publication-date": "Not found",
        "abstract": "The need to produce software quicker and in greater quantities continues to grow, while the market for professional program- mers struggles to meet the rising demand. Low-code development platforms (LCDPs) have been proposed as a solution that enables individuals without formal training to develop applications. However, current LCDPs typically rely on proprietary, ﬁxed code generation processes, which limit ﬂexibility, hinder interoperability, and risk vendor lock-in. To address these limitations, this paper introduces Dandelion+, a cloud-based low-code platform designed to create domain-speciﬁc LCDPs and applications within them. Our approach is highly model-driven, from deﬁning the scaffolds of low-code platforms (e.g., models, meta-models, users, and roles) to creating applications. In particular, applications are powered by PlatFlow, a platform-aware graphical workﬂow language. Comprising both model operations and low-code-focused nodes, PlatFlow offers ﬂexible control over the code generation process that drives LCDPs. We conduct two case studies to evaluate our approach. First, we confront Dandelion+ with other low-code platforms to design interactive applications and produce arti- facts. Second, we report on the beneﬁts of Dandelion+ in the industrial context of UGROUND, a Spanish software ﬁrm that uses low code in its projects.",
        "keywords": [
            "Low-code platforms",
            "Model-driven engineering",
            "Workﬂow languages"
        ],
        "authors": [
            "Francisco Martínez-Lasaca",
            "Pablo Díez",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-025-01308-y.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Model transformation by example using inductive logic programming",
        "submission-date": "2007/07",
        "publication-date": "2008/08",
        "abstract": "Model transformation by example is a novel approach in model-driven software engineering to derive model transformation rules from an initial prototypical set of interrelated source and target models, which describe critical cases of the model transformation problem in a purely declarative way. In the current paper, we automate this approach using inductive logic programming (Muggleton and Raedt in J Logic Program 19-20:629–679, 1994) which aims at the inductive construction of ﬁrst-order clausal theories from examples and background knowledge.",
        "keywords": [
            "Model transformation",
            "By-example synthesis",
            "Inductive logic programming"
        ],
        "authors": [
            "Zoltán Balogh",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-008-0092-1.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Nivel: a metamodelling language with a formal semantics",
        "submission-date": "2007/04",
        "publication-date": "2008/11",
        "abstract": "Much work has been done to clarify the notion of metamodelling and new ideas, such as strict metamodelling, distinction between ontological and linguistic instantiation, uniﬁed modelling elements and deep instantiation, have been introduced. However, many of these ideas have not yet been fully developed and integrated into modelling languages with (concrete) syntax, rigorous semantics and tool support. Consequently, applying these ideas in practice and reasoning about their meaning is difﬁcult, if not impossible. In this paper, we strive to add semantic rigour and conceptual claritytometamodellingthroughtheintroductionof Nivel,a novel metamodelling language capable of expressing models spanning an arbitrary number of levels. Nivel is based on a core set of conceptual modelling concepts: class, generali-sation, instantiation, attribute, value and association. Nivel adheres to a form of strict metamodelling and supports deep instantiation of classes, associations and attributes. A for-mal semantics is given for Nivel by translation to weight constraint rule language (WCRL), which enables decidable, automated reasoning about Nivel. The modelling facilities of Nivel and the utility of the formalisation are demonstrated in a case study on feature modelling.",
        "keywords": [
            "Conceptual modelling",
            "Metamodelling",
            "Nivel",
            "Weight constraint rules",
            "Formal semantics"
        ],
        "authors": [
            "Timo Asikainen",
            "Tomi Männistö"
        ],
        "file_path": "data/sosym-all/s10270-008-0103-2.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Deﬁnition and validation of model transformations",
        "submission-date": "2004/11",
        "publication-date": "2006/08",
        "abstract": "With model transformations becoming more widely used, there is an increasing need for approaches focussing on a systematic development of model trans-formations. Although a number of approaches for specifyingmodel transformations exist, noneof themfocusses on systematically validating model transformations with respect to termination and conﬂuence. Termination and conﬂuence ensure that a model transformation always produces a unique result. Also called functionality, these properties are important requirements for practical app-lications of model transformations. In this paper, we introduce our approach to model transformation. Using and extending results from the theory of graph trans-formation, we investigate termination and conﬂuence properties of model transformations speciﬁed in our ap-proach. We establish a set of criteria for termination and conﬂuence to be checked at design time by static analysis of the transformation rules and the underlying metamodels. Moreover, the criteria are formulated in such a way that they require less experience with the theory of graph transformation. Our concepts are illus-trated by a running example of a model tranformation from statecharts to the process algebra Communicating Sequential Processes.",
        "keywords": [],
        "authors": [
            "Jochen M. Küster"
        ],
        "file_path": "data/sosym-all/s10270-006-0018-8.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "PARMOREL: a framework for customizable model repair",
        "submission-date": "2021/03",
        "publication-date": "2022/05",
        "abstract": "In model-driven software engineering, models are used in all phases of the development process. These models must hold a high quality since the implementation of the systems they represent relies on them. Several existing tools reduce the burden of manually dealing with issues that affect models’ quality, such as syntax errors, model smells, and inadequate structures. However, these tools are often inﬂexible for customization and hard to extend. This paper presents a customizable and extensible model repair framework, PARMOREL, that enables users to deal with different issues in different types of models. The framework uses reinforcement learning to automatically ﬁnd the best sequence of actions for repairing a broken model according to user preferences. As proof of concept, we repair syntactic errors in class diagrams taking into account a model distance metric and quality characteristics. In addition, we restore inter-model consistency between UML class and sequence diagrams while improving the coupling qualities of the sequence diagrams. Furthermore, we evaluate the approach on a large publicly available dataset and a set of real-world inspired models to show that PARMOREL can decide and pick the best solution to solve the issues present in the models to satisfy user preferences.",
        "keywords": [
            "Model repair",
            "Reinforcement learning",
            "Customizable framework"
        ],
        "authors": [
            "Angela Barriga",
            "Rogardt Heldal",
            "Adrian Rutle",
            "Ludovico Iovino"
        ],
        "file_path": "data/sosym-all/s10270-022-01005-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Pattern reiﬁcation as the basis for description-driven systems",
        "submission-date": "2003/06",
        "publication-date": "2003/06",
        "abstract": "One of the main factors driving object-oriented software development for information systems is the requirement for systems to be tolerant to change. To address this issue in designing systems, this paper proposes a pattern-based, object-oriented, description-driven system (DDS) architecture as an extension to the standard UML four-layer meta-model. A DDS architecture is proposed in which aspects of both static and dynamic systems behavior can be captured via descriptive models and meta-models. The proposed architecture embodies four main elements – ﬁrstly, the adoption of a multi-layered meta-modeling architecture and reﬂective meta-level architecture, secondly the identiﬁcation of four data modeling relationships that can be made explicit such that they can be modiﬁed dynamically, thirdly the identiﬁcation of ﬁve design patterns which have emerged from practice and have proved essential in providing reusable building blocks for data management, and fourthly the encoding of the structural properties of the ﬁve design patterns by means of one fundamen-tal pattern, the Graph pattern. A practical example of this philosophy, the CRISTAL project, is used to demonstrate the use of description-driven data objects to handle system evolution.",
        "keywords": [
            "Meta-models",
            "System description",
            "UML",
            "Design patterns",
            "Reﬂection"
        ],
        "authors": [
            "Florida Estrella",
            "Zsolt Kovacs",
            "Jean-Marie Le Goﬀ",
            "Richard McClatchey",
            "Tony Solomonides",
            "Norbert Toth"
        ],
        "file_path": "data/sosym-all/s10270-003-0023-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "On the relationship between models and ontologies",
        "submission-date": "2022/07",
        "publication-date": "2022/07",
        "abstract": "The application of ontologies toward various engineering domains has gained much momentum recently, such that it is beneficial to understand the relationship between ontology building and modeling.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-01021-0.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Software and Systems Modeling (2025) 24:7–8",
        "submission-date": "2024/11",
        "publication-date": "2024/12",
        "abstract": "The Business Process Modeling, Development, and Support (BPMDS) working conference has been held for more than two decades, dealing with, and promoting research on BPMDS and has been a platform for a multitude of inﬂuential research papers. In keeping with its tradition, the working conference covers a broad range of theoretical and application-based research. BPMDS started in 1998 as a recurring workshop. During this period, business process analysis and design were recognized as central issues in the area of information systems engineering. The continued interest in these topics on behalf of the IS engineering community is reﬂected by the success of recent BPMDS events and the emergence of new conferences and workshops devoted to the theme. In 2011, BPMDS became a two-day working conference attached to CAiSE. The goals, format, and history of BPMDS can be found at www.bpmds.org.",
        "keywords": [],
        "authors": [
            "Han van der Aa",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-024-01248-z.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "Checking security compliance between models and code",
        "submission-date": "2020/09",
        "publication-date": "2022/03",
        "abstract": "It is challenging to verify that the planned security mechanisms are actually implemented in the software. In the context\nof model-based development, the implemented security mechanisms must capture all intended security properties that were\nconsidered in the design models. Assuring this compliance manually is labor intensive and can be error-prone. This work\nintroduces the ﬁrst semi-automatic technique for secure data ﬂow compliance checks between design models and code. We\ndevelop heuristic-based automated mappings between a design-level model (SecDFD, provided by humans) and a code-level\nrepresentation (Program Model, automatically extracted from the implementation) in order to guide users in discovering\ncompliance violations, and hence, potential security ﬂaws in the code. These mappings enable an automated, and project-\nspeciﬁc static analysis of the implementation with respect to the desired security properties of the design model. We developed\ntwo types of security compliance checks and evaluated the entire approach on open source Java projects.",
        "keywords": [
            "Security-by-design",
            "Security compliance",
            "Data ﬂow diagram (DFD)",
            "Static program analysis"
        ],
        "authors": [
            "Katja Tuma",
            "Sven Peldszus",
            "Daniel Strüber",
            "Riccardo Scandariato",
            "Jan Jürjens"
        ],
        "file_path": "data/sosym-all/s10270-022-00991-5.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "An empirical study of manual abstraction between class diagrams and code of open-source systems",
        "submission-date": "2024/07",
        "publication-date": "2025/03",
        "abstract": "Models play a crucial role in software design, analysis, and supporting new maintainers. However, over time, the beneﬁts of models can diminish as system implementations evolve without corresponding updates to the original models. Reverse engineering methods and tools can help maintain alignment between models and implementation code. Yet, automatically reverse-engineered models often lack abstraction and contain extensive details that hinder comprehension. Recent advance- ments in AI-based content generation suggest that we may soon see reverse engineering tools capable of human-grade abstraction. To guide the design and validation of such tools, we need a principled understanding of manual abstraction—a topic that has received limited attention in existing literature. In pursuit of this goal, our paper presents a multiple-case study of model-to-code differences, examining nine substantial open-source software projects obtained through repository mining. We manually matched source code from projects comprising 4983 classes, 26k attributes, and 54k operations to 523 model elements (including classes, attributes, operations, and relationships). These mappings precisely capture discrepan- cies between provided class diagram designs and actual implementation code. By analyzing these differences in detail, we derive a taxonomy of difference types and provide a well-organized list of cases corresponding to identiﬁed differences. Our ﬁndings have the potential to contribute to improved reverse engineering methods and tools, propose new mapping rules for model-to-code consistency checks, and offer guidelines to avoid over-abstraction and over-speciﬁcation during the design process.",
        "keywords": [
            "Software design",
            "Modeling",
            "Abstraction"
        ],
        "authors": [
            "Wenli Zhang",
            "Weixing Zhang",
            "Daniel Strüber",
            "Regina Hebig"
        ],
        "file_path": "data/sosym-all/s10270-025-01289-y.pdf",
        "classification": {
            "is_transformation_paper": "false",
            "is_transformation_language": "false",
            "language": "None"
        }
    },
    {
        "title": "A modular timed graph transformation language for simulation-based design",
        "submission-date": "2009/04",
        "publication-date": "2011/06",
        "abstract": "We introduce the MoTif (Modular Timed graph transformation) language, which allows one to elegantly model complex control structures for programmed graph transformation. These include modular construction, parallel composition, and a temporal dimension in addition to the usual transformation control structures. The ﬁrst part of this contribution formally introduces MoTif and its semantics is based on the Discrete EVent system Speciﬁcation (DEVS) formalism which allows for highly modular, hierarchical modelling of timed, reactive systems. In MoTif, graphs are embedded in events and individual transformation rules are embedded in atomic DEVS models. A side effect of the use of DEVS is the introduction of an explicit notion of time. This allows one to model a time-advance for every rule as well as to interrupt (pre-empt) rule execution. In the second part, we design a case study to show how the explicit notion of time allows for the simulation-based design of reactive systems such as modern computer games. We use the well-known game of PacMan as an example and model its dynamics in MoTif. This also allows the modelling of player behaviour, incorporatingdataabout humanplayers’ behaviour, andreac-tion times. Thus, a model of both player and game is obtained which can be used to evaluate, through simulation, the play-ability of a game design. We propose a playability perfor-mance measure and change the value of some parameters of thePacMangame.Foreachvariantofthegamethusobtained, simulation yields a value for the quality of the game. This allows us to choose an “optimal” (from a playability point of view) game conﬁguration. The user model is subsequently replaced by a visual interface to a real player, and the game model is executed using a real-time DEVS simulator.",
        "keywords": [
            "Simulation",
            "DEVS",
            "Graph transformation"
        ],
        "authors": [
            "Eugene Syriani",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-011-0205-0.pdf",
        "classification": {
            "is_transformation_paper": "true",
            "is_transformation_language": "true",
            "language": "MoTif"
        }
    }
]