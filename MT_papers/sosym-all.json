[
    {
        "title": "Model aesthetics",
        "submission-date": "2005/04",
        "publication-date": "2005/04",
        "abstract": "We thank the readers that commented on our editorial on model quality in issue 2004-3, and we strongly encourage readers to send comments to us on the editorials and the papers published in SoSyM (our email addresses are included at the end of this editorial). Some comments pointed out that the study of model quality addresses a wide-ranging set of questions and concerns. Model quality is not only concerned with how faithfully a model describes desired properties of the real world or system; it should also be concerned with innate attributes that affect qualities such as analyzability, understandability, and evolvability: Is the model readable? Is the model ambiguous? Is the model concise and complete? Is the model unnecessarily redundant? It may be possible to identify “model smells” (similar to “code smells”) that provide indicators of a model’s quality. We encourage researchers in this area to submit high quality papers on this topic to SoSyM. Papers that describe the results of empirical studies on model quality and aesthetics are especially encouraged.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-005-0081-6.pdf"
    },
    {
        "title": "A metamodel for the compact but lossless exchange of execution traces",
        "submission-date": "2009/11",
        "publication-date": "2010/10",
        "abstract": "Understanding the behavioural aspects of a software system can be made easier if efﬁcient tool support is provided. Lately, there has been an increase in the number of tools for analysing execution traces. These tools, however, have different formats for representing execution traces, which hinders interoperability and limits reuse and sharing of data. To allow for better synergies among trace analysis tools, it would be beneﬁcial to develop a standard format for exchanging traces. In this paper, we present a graph-based format, called compact trace format (CTF), which we hope will lead the way towards such a standard. CTF can model traces generated from a variety of programming languages, including both object-oriented and procedural ones. CTF is built with scalability in mind to overcome the vast size of most interesting traces. Indeed, the design of CTF is based on the idea that call trees can be transformed into more compact ordered acyclic directed graphs by representing similar subtrees only once. CTF is also supported by our trace anal-ysis tool SEAT (Software Exploration and Analysis Tool).",
        "keywords": [
            "Metamodelling",
            "Exchange format",
            "Execution traces",
            "Dynamic analysis"
        ],
        "authors": [
            "Abdelwahab Hamou-Lhadj",
            "Timothy C. Lethbridge"
        ],
        "file_path": "data/sosym-all/s10270-010-0180-x.pdf"
    },
    {
        "title": "Runtime veriﬁcation of component-based systems in the BIP framework with formally-proved sound and complete instrumentation",
        "submission-date": "2012/03",
        "publication-date": "2013/04",
        "abstract": "Veriﬁcation of component-based systems still suffers from limitations such as state space explosion since a large number of different components may interact in a heterogeneous environment. These limitations entail the need for complementary veriﬁcation methods such as run-time veriﬁcation. Runtime veriﬁcation is a dynamic analysis technique and is prone to scalability. In this paper, we integrate runtime veriﬁcation into the BIP (Behavior, Interaction and Priority) framework. BIP is a powerful and expressive component-based framework for the formal construction of heterogeneous systems. Our method augments BIP systems with monitorsto check speciﬁcations at runtime. This method has been implemented in RV-BIP, a prototype tool that we used to validate the whole approach on a robotic application.",
        "keywords": [
            "Runtime veriﬁcation",
            "Component-based systems",
            "Instrumentation",
            "Formal methods"
        ],
        "authors": [
            "Yliès Falcone",
            "Mohamad Jaber",
            "Thanh-Hung Nguyen",
            "Marius Bozga",
            "Saddek Bensalem"
        ],
        "file_path": "data/sosym-all/s10270-013-0323-y.pdf"
    },
    {
        "title": "A framework for qualitative assessment of domain-speciﬁc languages",
        "submission-date": "2013/03",
        "publication-date": "2013/11",
        "abstract": "Domain-speciﬁc languages (DSLs) are used for improving many facets of software development, but whether and to what extent this aim is achieved is an important issue that must be addressed. This paper presents a proposal for a Framework for Qualitative Assessment of DSLs (FQAD). FQAD is used for determining the perspective of the evaluator, understanding the goal of the assessment and selecting fundamental DSL quality characteristics to guide the evaluator in the process. This framework adapts and integrates the ISO/IEC 25010:2011 standard, CMMI maturity level evaluationapproachandthescalingapproachusedinDESMETinto a perspective-based assessment. A detailed list of domain-speciﬁc language quality characteristics is elaborated, and a novel assessment method is proposed. Two case studies through which FQAD is matured and evaluated are reported. The case studies have shown that stakeholders ﬁnd the FQAD process beneﬁcial.",
        "keywords": [
            "Domain-speciﬁclanguages",
            "Qualitymeasures",
            "Qualitative assessment",
            "ISO/IEC 25010",
            "CMMI"
        ],
        "authors": [
            "Gökhan Kahraman",
            "Semih Bilgen"
        ],
        "file_path": "data/sosym-all/s10270-013-0387-8.pdf"
    },
    {
        "title": "Adding higher-level semantics to Functional Mock-up Units for easier, faster, and more robust co-simulation connections",
        "submission-date": "2023/06",
        "publication-date": "2025/01",
        "abstract": "The task of interfacing sub-simulators in a co-simulation often remains difﬁcult, tedious, and prone to error. Here, we describe how this process, and the validation of the resulting interface connections, can be made simpler, faster, and more reliable. Especially when, as is often the case, several individuals or teams collaborate on modeling and simulating a full system. This is achieved by grouping functional mock-up interface (FMI) variables into types close to the relevant engineering domains using semantics formalized as an ontology. Several beneﬁts, we argue, are gained from this: clearer communication, increased validation potential, and reduced number of interface connections to deal with. The validity and the limitations of our approach are demonstrated with a detailed case study of a real maritime system: A dynamic positioning (DP) hardware controller connected to several independent co-simulation models via 156 variables in 78 connections. The proposed solution greatly reduces the complexities and the error potential related to interfacing such systems. All results including reference implementations are openly available through the open simulation platform.",
        "keywords": [
            "Functional Mock-up Interface",
            "Co-Simulation",
            "Simulator Interface",
            "Ontology"
        ],
        "authors": [
            "Martin Rindarøy",
            "Håvard Nordahl",
            "Severin Sadjina",
            "Stian Skjong",
            "Marianne Hagaseth"
        ],
        "file_path": "data/sosym-all/s10270-024-01244-3.pdf"
    },
    {
        "title": "Variability extraction and modeling for product variants",
        "submission-date": "2014/10",
        "publication-date": "2016/01",
        "abstract": "Fast-changing hardware and software technologies in addition to larger and more specialized customer bases demand software tailored to meet very diverse requirements. Software development approaches that aim at capturing this diversity on a single consolidated platform often require large upfront investments, e.g., time or budget. Alternatively, companies resort to developing one variant of a software product at a time by reusing as much as possible from already-existing product variants. However, identifying and extracting the parts to reuse is an error-prone and inefﬁcient task compounded by the typically large number of product variants. Hence, more disciplined and systematic approaches are needed to cope with the complexity of developing and maintainingsetsofproductvariants.Suchapproachesrequire detailed information about the product variants, the features they provide and their relations. In this paper, we present an approach to extract such variability information from product variants. It identiﬁes traces from features and feature interactions to their implementation artifacts, and computes their dependencies. This work can be useful in many scenarios ranging from ad hoc development approaches such as clone-and-own to systematic reuse approaches such as software product lines. We applied our variability extraction approach to six case studies and provide a detailed evaluation. The results show that the extracted variability information is consistent with the variability in our six case study systems given by their variability models and available product variants.",
        "keywords": [
            "Feature",
            "Trace",
            "Product variant",
            "Variability",
            "Dependency"
        ],
        "authors": [
            "Lukas Linsbauer",
            "Roberto Erick Lopez-Herrejon",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-015-0512-y.pdf"
    },
    {
        "title": "Improving the quality of use case models using antipatterns",
        "submission-date": "2008/05",
        "publication-date": "2009/02",
        "abstract": "Use case (UC) modeling is a popular requirements modeling technique. While these models are simple to create and read; this simplicity is often misconceived, leading practitioners to believe that creating high quality models is straightforward. Therefore, many low quality models that are inconsistent, incorrect, contain premature restrictive design decision and contain ambiguous information are produced. To combat this problem of creating low quality UC models, this paper presents a new technique that utilizes antipatterns as a mechanism for remedying quality problems in UC models. The technique, supported by the tool ARBIUM, provides a framework for developers to define antipatterns. The feasibility of the approach is demonstrated by applying it to a real-world system. The results indicate that applying the technique improves the overall quality and clarity of UC models.",
        "keywords": [
            "Use cases",
            "Antipatterns",
            "UML",
            "Use case modeling qualityattributes",
            "OCL"
        ],
        "authors": [
            "Mohamed El-Attar",
            "James Miller"
        ],
        "file_path": "data/sosym-all/s10270-009-0112-9.pdf"
    },
    {
        "title": "Guest editorial to the special section on SEFM’22",
        "submission-date": "2024/03",
        "publication-date": "2024/04",
        "abstract": "This special section contains revised and extended versions of selected papers from SEFM’22, the 20th International Conference on Software Engineering and Formal Methods, held in Berlin, Germany, on September 28–30, 2022. The SEFM conference series aims to bring together researchers and practitioners from academia, industry and government, to advance the state of the art in formal methods, to facilitate their uptake in the software industry, and to encourage their integration within practical software engineering methods and tools.",
        "keywords": [],
        "authors": [
            "Bernd-Holger Schlingloﬀ",
            "Ming Chai"
        ],
        "file_path": "data/sosym-all/s10270-024-01174-0.pdf"
    },
    {
        "title": "Generating process model collections",
        "submission-date": "2014/11",
        "publication-date": "2015/10",
        "abstract": "Business process management plays an important role in the management of organizations. More and more organizations describe their operations as business processes. It is common for organizations to have collections of thousands of business processes, but for reasons of confidentiality these collections are often not, or only partially, available to researchers. On the other hand, research on techniques for managing process model collections, such as techniques for process retrieval, requires large collections for evaluation purposes. Therefore, this paper proposes a technique to generate such collections of process models, based on the properties of real-world collections. Where existing techniques focus on the structure of the process models, the technique proposed in this paper also generates task labels that consists of words from real-life task labels and considers semantic information of node and edge types. We evaluate our technique by applying it to generate two synthetic collections of process models of over 60,000 and over 2,000 models, respectively. We show that the generated synthetic collections have similar properties to the original collections. To the best of our knowledge, this is the first technique that can generate synthetic BPMN models, thus enabling experimentation with process collections that have laboratory-set quantitative parameters and qualitative properties that are based on real-world process model collections.",
        "keywords": [
            "Synthetic process models",
            "Process model generation",
            "Process model similarity"
        ],
        "authors": [
            "Zhiqiang Yan",
            "Remco Dijkman",
            "Paul Grefen"
        ],
        "file_path": "data/sosym-all/s10270-015-0497-6.pdf"
    },
    {
        "title": "Design and evaluation of a collaborative UML modeling environment in virtual reality",
        "submission-date": "2022/03",
        "publication-date": "2022/11",
        "abstract": "Modeling is a key activity in conceptual design and system design. Through collaborative modeling, end-users, stakeholders, experts, and entrepreneurs are able to create a shared understanding of a system representation. While the Uniﬁed Modeling Language (UML) is one of the major conceptual modeling languages in object-oriented software engineering, more and more concerns arise from the modeling quality of UML and its tool-support. Among them, the limitation of the two-dimensional presentation of its notations and lack of natural collaborative modeling tools are reported to be signiﬁcant. In this paper, we explore the potential of using virtual reality (VR) technology for collaborative UML software design by comparing it with classical collaborative software design using conventional devices (desktop PC/laptop). For this purpose, we have developed a VR modeling environment that offers a natural collaborative modeling experience for UML Class Diagrams. Based on a user study with 24 participants, we have compared collaborative VR modeling with conventional modeling with regard to efﬁciency, effectiveness, and user satisfaction. Results show that the use of VR has some disadvantages concerning efﬁciency and effectiveness, but the user’s fun, the feeling of being in the same room with a remote collaborator, and the naturalness of collaboration were increased.",
        "keywords": [
            "Collaborative modeling",
            "Virtual reality",
            "UML"
        ],
        "authors": [
            "Enes Yigitbas",
            "Simon Gorissen",
            "Nils Weidmann",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-022-01065-2.pdf"
    },
    {
        "title": "aCHAT-WF: Generating conversational agents for teaching business process models",
        "submission-date": "2020/11",
        "publication-date": "2021/10",
        "abstract": "This paper proposes a general approach for using conversational interfaces such as chatbots to offer adaptive learning of\nbusiness processes in an environment involving different actors. Adaptivity concerns both the content being proposed, the\nsequence of learning items, and the way the conversation is conducted. The original approach allows the development\nof sustainable chatbots and empowers various non-technical actors (authors, teachers, publishers, and learners) to control\nthe chatbot features directly. The aCHAT-WF framework (adaptive CHATbot for WorkFlows), proposed in this paper for\nmanaging conversational interfaces, conceptually represents all the aspects related to a conversation about business processes,\nwith different facets for the user, the conversation ﬂow, and the conversation contents, combining them to obtain a ﬂexible\ninteraction with the user. The paper focuses on the different preparation phases for instructional material based on Business\nProcess Modeling Notation (BPMN) models, separating the different roles involved in the construction of a chatbot for teaching\nbusiness processes and with the possibility of deﬁning different styles for the interaction with the users. The proposed method\nis conﬁguration-driven, to facilitate the separation of the different aspects of the control of the interaction and the delivery of\ncontents.",
        "keywords": [
            "Chatbot",
            "Business process",
            "BPMN",
            "Digital transformation",
            "Conﬁguration driven",
            "Educational conversational agent"
        ],
        "authors": [
            "Donya Rooein",
            "Devis Bianchini",
            "Francesco Leotta",
            "Massimo Mecella",
            "Paolo Paolini",
            "Barbara Pernici"
        ],
        "file_path": "data/sosym-all/s10270-021-00925-7.pdf"
    },
    {
        "title": "A formal approach to finding inconsistencies in a metamodel",
        "submission-date": "2018/12",
        "publication-date": "2021/01",
        "abstract": "Checking the consistency of a metamodel involves finding a valid metamodel instance that provably meets the set of constraints that are defined over the metamodel. These constraints are often specified in Object Constraint Language. Often, a metamodel is inconsistent due to conflicts among the constraints. Existing approaches and tools are typically incapable of pinpointing the conflicting constraints, and this makes it difficult for users to debug and fix their metamodels. In this paper, we present a formal approach for locating conflicting constraints in inconsistent metamodels. Our approach has four distinct features: (1) users can rank individual metamodel features using their own domain-specific knowledge, (2) we transform these ranked features to a weighted maximum satisfiability modulo theories problem and solve it to compute the set of maximum achievable features, (3) we pinpoint the conflicting constraints by solving the set cover problem using a novel algorithm, and (4) we have implemented our approach into a fully automated tool called MaxUSE. Our evaluation results, using our assembled set of benchmarks, demonstrate the scalability of our work and that it is capable of efficiently finding conflicting constraints.",
        "keywords": [
            "Metamodel",
            "Conflicts",
            "SMT"
        ],
        "authors": [
            "Hao Wu",
            "Marie Farrell"
        ],
        "file_path": "data/sosym-all/s10270-020-00849-8.pdf"
    },
    {
        "title": "A situational method for semi-automated Enterprise Architecture Documentation",
        "submission-date": "2013/08",
        "publication-date": "2014/04",
        "abstract": "The business capabilities of modern enterprises crucially rely on the enterprises’ information systems and underlying IT infrastructure. Hence, optimization of the business-IT alignment is a key objective of Enterprise Architecture Management (EAM). To achieve this objective, EAM creates, maintains and analyzes a model of the current state of the Enterprise Architecture. This model covers different concepts reflecting both the business and the IT perspective and has to be constantly maintained in response to ongoing transformations of the enterprise. In practice, EA models grow large and are difficult to maintain, since many stakeholders from various backgrounds have to contribute architecture-relevant information. EAM literature and two practitioner surveys conducted by the authors indicate that EA model maintenance, in particular the manual documentation activities, poses one of the biggest challenges to EAM in practice. Current research approaches target the automation of the EA documentation based on specific data sources. These approaches, as our systematic literature review showed, do not consider enterprise specificity of the documentation context or the variability of the data sources from organization to organization. The approach presented in this article specifically accounts for these factors and presents a situational method for EA documentation. It builds on four process-supported documentation techniques which can be selected, composedandappliedtodesignanorganization-speciﬁcdoc-umentation process. The techniques build on a meta-model for EA documentation, which is implemented in an EA-repository prototype that supports the conﬁguration and execution of the documentation techniques. We applied our documentation method assembly process at a German insurance company and report the ﬁndings from this case study in particular regarding practical applicability and usability of our approach.",
        "keywords": [
            "Enterprise Architecture",
            "Documentation",
            "Maintenance",
            "Model",
            "Automation",
            "Situational method"
        ],
        "authors": [
            "Matthias Farwick",
            "Christian M. Schweda",
            "Ruth Breu",
            "Inge Hanschke"
        ],
        "file_path": "data/sosym-all/s10270-014-0407-3.pdf"
    },
    {
        "title": "Hybrid co-simulation: it’s about time",
        "submission-date": "2016/02",
        "publication-date": "2017/11",
        "abstract": "Model-based design methodologies are commonly used in industry for the development of complex cyber-physical systems (CPSs). There are many different languages, tools, and formalisms for model-based design, each with its strengths and weaknesses. Instead of accepting some weaknesses of a particular tool, an alternative is to embrace heterogeneity, and to develop tool integration platforms and protocols to leverage the strengths from different environments. A fairly recent attempt in this direction is the functional mock-up interface (FMI) standard that includes support for co-simulation. Although this standard has reached acceptance in industry, it provides only limited support for simulating systems that mix continuous and discretebehavior, whicharetypical of CPS. This paper identiﬁes the representation of time as a key problem, because the FMI representation does not support well the discrete events that typically occur at the cyber-physical boundary. We analyze alternatives for representing time in hybrid co-simulation and conclude that a superdense model of time using integers only solves many of these problems. We show how an execution engine can pick an adequate time resolution, and how disparities between time representations internal to co-simulated components and the resulting effects of time quantization can be managed. We propose a concrete extension to the FMI standard for supporting hybrid co-simulation that includes integer time, automatic choice of time resolution, and the use of absent signals. We explain how these extensions can be implemented modularly within the frameworks of existing simulation environments.",
        "keywords": [
            "Co-simulation",
            "Functional mock-up interface",
            "Time"
        ],
        "authors": [
            "Fabio Cremona",
            "Marten Lohstroh",
            "David Broman",
            "Edward A. Lee",
            "Michael Masin",
            "Stavros Tripakis"
        ],
        "file_path": "data/sosym-all/s10270-017-0633-6.pdf"
    },
    {
        "title": "Editorial to the theme issue on model-driven service engineering",
        "submission-date": "2013/07",
        "publication-date": "2013/07",
        "abstract": "During the last years, Model-Driven Engineering (MDE) has started to have a direct inﬂuence in other research ﬁelds. Under the light of the premise that everything is a model, coined by Jean Bézivin in 2004 and adopted by the MDE community since then, practitioners from other areas have discovered that they were able to express their problems in terms of models and then take advantage of MDE techniques to solve them—or at least simplify them, either by increasing the level of automation or by raising the abstraction level at which solutions are planned and developed. The scope varies widely, from generic domains, like Web Engineering (see the previous SoSyM Theme Issue on Model-Driven Web Engi- neering), to more speciﬁc ones, like DB schema matching or domotics. Out of doubt, one of the areas that has more decisively beneﬁted from MDE advances has been Service Engineer- ing. It aims at bringing together the beneﬁts of Service Orientation and Business Process Management, therefore making the most of Service Orientation to help organizations deliver sustainable business value with increased agility and cost effectiveness.",
        "keywords": [],
        "authors": [
            "Juan Manuel Vara",
            "Mike Papazoglou",
            "Il-Yeol Song"
        ],
        "file_path": "data/sosym-all/s10270-013-0368-y.pdf"
    },
    {
        "title": "Editorial for the SoSyM issue 2014/04",
        "submission-date": "2014/09",
        "publication-date": "2014/09",
        "abstract": "In recent years, SoSyM has had a pretty large pipeline of papers to be published. This resulted in long hard copy publication turnaround times for accepted SoSyM papers. In order to facilitate more timely publication of accepted papers, we requested Springer to increase the number of pages per issue. Springer agreed to signiﬁcantly increase the number of pages per issue for 2014. Speciﬁcally, they agreed to increase the number of pages in the ﬁrst two issues of 2014 to 448 pages, and the last two issues of 2014 to 304 pages. Springer is also looking at increasing the number of pages published in 2015 to further reduce the pipeline. We are very grateful to the Springer staff for their willingness to take concrete steps to reduce the publication pipeline.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-014-0434-0.pdf"
    },
    {
        "title": "Modeling and simulation of the IEEE 802.11e wireless protocol with hidden nodes using Colored Petri Nets",
        "submission-date": "2019/05",
        "publication-date": "2020/07",
        "abstract": "Wireless technologies are continuously evolving, including features such as the extension to mid- and long-range commu-nications and the support of an increasing number of devices. However, longer ranges increase the probability of suffering from hidden terminal issues. In the particular case of Wireless Local Area Networks (WLANs), the use of Quality of Service (QoS) mechanisms introduced in IEEE 802.11e compromises scalability, exacerbates the hidden node problem, and creates congestion as the number of users and the variety of services in the network grow. In this context, this paper presents a configurable Colored Petri Net (CPN) model for the IEEE 802.11e protocol with the aim of analyzing the QoS support in mid- and long-range WLANs The CPN model covers the behavior of the protocol in the presence of hidden nodes to examine the performance of the RTS/CTS exchange in scenarios where the QoS differentiation may involve massive collision chains and high delays. Our CPN model sets the basis for further exploring the performance of the various mechanisms defined by the IEEE 802.11 standard. We then use this CPN model to provide a comprehensive study of the effectiveness of this protocol by using the simulation and monitoring capabilities of CPN Tools.",
        "keywords": [
            "IEEE 802.11",
            "QoS",
            "Colored Petri Nets",
            "Simulation",
            "Performance",
            "Hidden terminal"
        ],
        "authors": [
            "Estefanía Coronado",
            "Valentín Valero",
            "Luis Orozco-Barbosa",
            "María-Emilia Cambronero",
            "Fernando L. Pelayo"
        ],
        "file_path": "data/sosym-all/s10270-020-00817-2.pdf"
    },
    {
        "title": "Multilevel modeling of geographic information systems based on international standards",
        "submission-date": "2020/06",
        "publication-date": "2021/07",
        "abstract": "Even though different applications based on Geographic Information Systems (GIS) provide different features and functions, they all share a set of common concepts (e.g., spatial data types, operations, services), a common architecture, and a common set of technologies. Furthermore, common structures appear repeatedly in different GIS, although they have to be specialized in speciﬁc application domains. Multilevel modeling is an approach to model-driven engineering (MDE) in which the number of metamodel levels is not ﬁxed. This approach aims at solving the limitations of a two-level metamodeling approach, which forces the designer to include all the metamodel elements at the same level. In this paper, we address the application of multilevel modeling to the domain of GIS, and we evaluate its potential beneﬁts. Although we do not present a complete set of models, we present four representative scenarios supported by example models. One of them is based on the standards deﬁned by ISO TC/211 and the Open Geospatial Consortium. The other three are based on the EU INSPIRE Directive (territory administration, spatial networks, and facility management). These scenarios show that multilevel modeling can provide more beneﬁts to GIS modeling than a two-level metamodeling approach.",
        "keywords": [
            "Model-driven engineering",
            "Multilevel software modeling",
            "Geographic information systems"
        ],
        "authors": [
            "Suilen H. Alvarado",
            "Alejandro Cortiñas",
            "Miguel R. Luaces",
            "Oscar Pedreira",
            "Angeles S. Places"
        ],
        "file_path": "data/sosym-all/s10270-021-00901-1.pdf"
    },
    {
        "title": "Modeling dynamic structures",
        "submission-date": "2020/04",
        "publication-date": "2020/04",
        "abstract": "David Harel once said during a talk, “Bridges are made to stand and software is there to do.” This is a very appropriate analogy, because it shows that software is about behavior. As a consequence, many software modeling techniques supported by languages also allow the description of behavior. However, behavior usually is embedded in some structure. In object-oriented systems, this is typically the object, where a system is composed of many object instances, with the class as the describing artifact that deﬁnes the blueprint. In many forms of complex or distributed systems, the notion of “component” or “assembly” is also used in various forms to describe structure.",
        "keywords": [],
        "authors": [
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-020-00793-7.pdf"
    },
    {
        "title": "Conﬁguring use case models in product families",
        "submission-date": "2016/01",
        "publication-date": "2016/06",
        "abstract": "In many domains such as automotive and avion- ics, the size and complexity of software systems is quickly increasing. At the same time, many stakeholders tend to be involved in the development of such systems, which typi- cally must also be conﬁgured for multiple customers with varying needs. Product Line Engineering (PLE) is therefore an inevitable practice for such systems. Furthermore, because in many areas requirements must be explicit and traceabil- ity to them is required by standards, use cases and domain models are common practice for requirements elicitation and analysis. In this paper, based on the above observations, we aim at supporting PLE in the context of use case-centric development. Therefore, we propose, apply, and assess a use case-driven conﬁguration approach which interactively receives conﬁguration decisions from the analysts to gen- erate product-speciﬁc (PS) use case and domain models. Our approach provides the following: (1) a use case-centric product line modeling method (PUM), (2) automated, inter- active conﬁguration support based on PUM, and (3) an automatic generation of PS use case and domain models from Product Line (PL) models and conﬁguration decisions. The approach is supported by a tool relying on Natural Language Processing (NLP) and integrated with an industrial requirements management tool, i.e., IBM DOORS. We successfully applied and evaluated our approach to an industrial case study in the automotive domain, thus showing evidence that the approach is practical and beneﬁcial to capture variability at the appropriate level of granularity and to conﬁgure PS use case and domain models in industrial settings.",
        "keywords": [
            "Product line engineering",
            "Use case-driven development",
            "Conﬁguration",
            "Natural language processing",
            "Consistency checking"
        ],
        "authors": [
            "Ines Hajri",
            "Arda Goknil",
            "Lionel C. Briand",
            "Thierry Stephany"
        ],
        "file_path": "data/sosym-all/s10270-016-0539-8.pdf"
    },
    {
        "title": "A theme section on the central role of modeling in designing and explaining data-driven systems and software",
        "submission-date": "2023/10",
        "publication-date": "2023/11",
        "abstract": "Following the stimulating 10th International Conference on Model and Data Engineering (MEDI 2021), we are excited to announce our proposal to edit a \"Theme Sec-tion\" for the International Journal on Software and Systems Modeling (SoSyM). This theme section will be dedicated to showcasing the recent results from the Data and Models communities, with a particular emphasis on the signiﬁcance of the 10th edition of MEDI.",
        "keywords": [],
        "authors": [
            "Christian Attiogbé",
            "Sadok Ben Yahia",
            "Ladjel Bellatreche"
        ],
        "file_path": "data/sosym-all/s10270-023-01133-1.pdf"
    },
    {
        "title": "Synthesis of veriﬁable concurrent Java components from formal models",
        "submission-date": "2016/03",
        "publication-date": "2017/02",
        "abstract": "Concurrent systems are hard to program, and ensuring quality by means of traditional testing techniques is often very hard as errors may not show up easily and reproducing them is hard. In previous work, we have advocated a model-driven approach to the analysis and design of concurrent, safety-critical systems. However, to take full advantage of these techniques, they must be supported by code generation schemes for concrete programming languages. Ideally, this translation should be traceable, automated and should supporttheveriﬁcationofthegeneratedcode.Inourwork,we consider the problem of generating a concurrent Java component from a high-level model of inter-process interaction (i.e., communication + synchronization). We call our formalism shared resources. From the model, which can be represented in mathematical notation or written as a Java interface annotated using an extension of JML, a Java component can be obtained by a semiautomatic translation. We describe how to obtain shared memory (using a priority monitors library) and message passing (using the JCSP library) implementations. Focusing on inter-process interaction for formal development is justiﬁed by several reasons, e.g., mathematical models are language-independent and allow to analyze certain concurrency issues, such as deadlocks or liveness properties prior to code generation. Also, the Java components produced from the shared resource model will contain all the concurrency-related language constructs, which are often responsible for many of the errors in concurrent software. We follow a realistic approach where the translation is semiautomatic (schemata for code generation) and the programmer still retains the power of coding or modifying parts of the code for the resource. The code thus obtained is JML-annotated Java with proof obligations that help with code traceability and veriﬁcation of safety and liveness properties. As the code thus obtained is not automatically correct, there is still the need to verify its conformance to the original specs. We illustrate the methodology by developing a concurrent control system and verifying the code obtained using the KeY program veriﬁcation tool. We also show how KeY can be used to ﬁnd errors resulting from a wrong use of the templates.",
        "keywords": [
            "CSP",
            "JCSP",
            "KeY",
            "Java",
            "JML",
            "Shared resources",
            "Veriﬁcation",
            "Model-driven",
            "Concurrency",
            "Message passing"
        ],
        "authors": [
            "Julio Mariño",
            "Raúl N. N. Alborodo",
            "Lars-Åke Fredlund",
            "Ángel Herranz"
        ],
        "file_path": "data/sosym-all/s10270-017-0581-1.pdf"
    },
    {
        "title": "Petri nets in systems biology",
        "submission-date": "2014/01",
        "publication-date": "2014/06",
        "abstract": "Petri nets are used in many areas. This article discusses the application of Petri nets in systems biology. Using an example from biochemistry, concepts for the automatic decomposition of biochemical systems are introduced. The article focuses on those concepts that fulﬁll steady-state conditions. Interestingly, all the concepts are based on minimal,semi-positivetransitioninvariants.Thearticledescribes, which new deﬁnitions for network decomposition can be derived and how they can be interpreted in the context of biology. This is illustrated with the example of the citric acid cycle, for which a new metabolic pathway could be predicted with the help of such an analysis.",
        "keywords": [
            "Petri net",
            "Systems biology",
            "T-invariant",
            "MCT-set",
            "T-cluster",
            "Citrate cycle"
        ],
        "authors": [
            "Ina Koch"
        ],
        "file_path": "data/sosym-all/s10270-014-0421-5.pdf"
    },
    {
        "title": "Developing BP-driven web applications through the use of MDE techniques",
        "submission-date": "2009/11",
        "publication-date": "2010/10",
        "abstract": "Model driven engineering (MDE) is a suitable approach for performing the construction of software systems (in particular in the Web application domain). There are different types of Web applications depending on their purpose (i.e., document-centric, interactive, transactional, workﬂow/business process-based, collaborative, etc). This work focusses on business process-based Web applications in order to be able to understand business processes in a broad sense, from the lightweight business processes already addressed by existing proposals to long-running asynchro- nous processes. This work presents a MDE method for the construction of systems of this type. The method has been designed in two steps following the MDE principles. In the ﬁrst step, the system is represented by means of models in a technology-independent manner. These models capture the different aspects of Web-based systems (these aspects refer to behaviour, structure, navigation, and presentation issues). In the second step, the model transformations (both model-to-model and model-to-text) are applied in order to obtain the ﬁnal system in terms of a speciﬁc technology. In addition, a setofEclipse-basedtoolshasbeendevelopedtoprovideauto- mation in the application of the proposed method in order to validate the proposal.",
        "keywords": [
            "Web engineering",
            "Model driven engineering",
            "Business processes"
        ],
        "authors": [
            "Victoria Torres",
            "Pau Giner",
            "Vicente Pelechano"
        ],
        "file_path": "data/sosym-all/s10270-010-0177-5.pdf"
    },
    {
        "title": "Model-based ﬂeet deployment in the IoT–edge–cloud continuum",
        "submission-date": "2021/02",
        "publication-date": "2022/05",
        "abstract": "With the increasing computing and networking capabilities, IoT devices and edge gateways have become part of a larger IoT–edge–cloud computing continuum, where processing and storage tasks are distributed across the whole network hierarchy, not concentrated only in the cloud. At the same time, this also introduced continuous delivery practices to the development of software components for network-connected gateways and sensing/actuating nodes. These devices are placed on end users’ premises and are characterized by continuously changing cyber-physical contexts, forcing software developers to maintain multiple application versions and frequently redeploy them on a distributed ﬂeet of devices with respect to their current contexts. Doing this correctly and efﬁciently goes beyond manual capabilities and requires an intelligent and reliable automated solution. This paper describes a model-based approach to automatically assigning multiple software deployment plans to hundreds of edge gateways and connected IoT devices implemented in collaboration with a smart healthcare application provider. From a platform-speciﬁc model of an existing edge computing platform, we extract a platform-independent model that describes a list of target devices and a pool of available deployment plans. Next, we use constraint solving to automatically assign deployment plans to devices at once with respect to their speciﬁc contexts. The result is transformed back into the platform-speciﬁc model and includes a suitable deployment plan for each device, which is then consumed by our engine to deploy software components not only on edge gateways but also on their downstream IoT devices with constrained resources and connectivity. We validate the approach with a ﬂeet deployment prototype integrated into a DevOps toolchain used by the partner application provider. Initial experiments demonstrate the viability of the approach and its usefulness in supporting DevOps for edge and IoT software development.",
        "keywords": [
            "Software deployment",
            "IoT",
            "Model-based software engineering",
            "Device ﬂeet",
            "DevOps",
            "Constraint solving"
        ],
        "authors": [
            "Hui Song",
            "Rustem Dautov",
            "Nicolas Ferry",
            "Arnor Solberg",
            "Franck Fleurey"
        ],
        "file_path": "data/sosym-all/s10270-022-01006-z.pdf"
    },
    {
        "title": "Introduction to the special issue on the 18th international conference on model driven engineering languages and systems (MODELS’15)",
        "submission-date": "2016/12",
        "publication-date": "2017/01",
        "abstract": "MODELS is the premier conference series for model-based software and systems engineering. It has been established in 1998 and has since been covering all aspects of modeling, from languages and methods to tools and applications. MODELS’15 was the 18th edition of the conference. It took place in Ottawa, Canada from September 27 to October 2, 2015. MODELS’15 challenged the modeling community to promote the magic of modeling by solidifying and extending the foundations and successful applications of modeling in areas such as business information and embedded systems, but also by exploring the use of modeling for new and emerging systems, paradigms, and challenges including cyber-physical systems, cloud computing, services, social media, big data, security, and open source. This challenge resulted in 216 abstract submissions that materialized in 172 papers, consisting of 132 technical papers (including 22 new ideas papers) and 40 in-practice papers. Of these, the Program Committee and Program Board accepted 35 foundations papers (26.5% acceptance rate) and 11 in-practice papers (28% acceptance rate). The program also included a diverse group of keynote talks, including presentations on climate models, automotive models, and software supply chains. Out of the accepted papers, we invited the best ones for this special issue. This invitation was based on a careful evaluation of all papers by the Program Board and Program Committee. The authors of these best papers were then asked to prepare a substantial improved and extended version for this special issue. Each article underwent a full journal review process and authors received anonymous feedback in two rounds of reviewing from three expert reviewers.",
        "keywords": [],
        "authors": [
            "Jordi Cabot",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-017-0577-x.pdf"
    },
    {
        "title": "WESSBAS: extraction of probabilistic workload speciﬁcations for load testing and performance prediction—a model-driven approach for session-based application systems",
        "submission-date": "2015/07",
        "publication-date": "2016/10",
        "abstract": "Thespeciﬁcationofworkloadsisrequiredinorder\nto evaluate performance characteristics of application sys-\ntemsusingloadtestingandmodel-basedperformancepredic-\ntion. Deﬁning workload speciﬁcations that represent the real\nworkload as accurately as possible is one of the biggest chal-\nlenges in both areas. To overcome this challenge, this paper\npresents an approach that aims to automate the extraction and\ntransformation of workload speciﬁcations for load testing\nand model-based performance prediction of session-based\napplication systems. The approach (WESSBAS) comprises\nthree main components. First, a system- and tool-agnostic\ndomain-speciﬁc language (DSL) allows the layered mod-\neling of workload speciﬁcations of session-based systems.\nSecond, instances of this DSL are automatically extracted\nfrom recorded session logs of production systems. Third,\nthese instances are transformed into executable workload\nspeciﬁcations of load generation tools and model-based per-\nformance evaluation tools. We present transformations to the\ncommon load testing tool Apache JMeter and to the Palla-\ndio Component Model. Our approach is evaluated using the\nindustry-standard benchmark SPECjEnterprise2010 and the\nWorld Cup 1998 access logs. Workload-speciﬁc characteris-\ntics (e.g., session lengths and arrival rates) and performance\ncharacteristics (e.g., response times and CPU utilizations)\nshow that the extracted workloads match the measured work-\nloads with high accuracy.",
        "keywords": [
            "Workload speciﬁcations",
            "Load testing",
            "Performance prediction",
            "Performance models"
        ],
        "authors": [
            "Christian Vögele",
            "André van Hoorn",
            "Eike Schulz",
            "Wilhelm Hasselbring",
            "Helmut Krcmar"
        ],
        "file_path": "data/sosym-all/s10270-016-0566-5.pdf"
    },
    {
        "title": "Speciﬁcation-driven model transformation testing",
        "submission-date": "2012/10",
        "publication-date": "2013/08",
        "abstract": "Testing model transformations poses several challenges, among them the automatic generation of appropriate input test models and the speciﬁcation of oracle functions. Most approaches for the generation of input models ensure a certain coverage of the source meta-model or the transformation implementation code, whereas oracle functions are frequently deﬁned using query or graph languages. However, these two tasks are usually performed independently regardless of their common purpose, and sometimes, there is a gap between the properties exhibited by the generated input models and those considered by the transformations. Recently, we proposed a formal speciﬁcation language for the declarative formulation of transformation properties (bymeansofinvariants,pre-,andpostconditions)fromwhich we generated partial oracle functions used for transformation testing. Here, we extend the usage of our speciﬁcation language for the automated generation of input test models by SAT solving. The testing process becomes more intentional because the generated models ensure a certain coverage of the transformation requirements. Moreover, we use the same speciﬁcation to consistently derive both the input test models and the oracle functions. A set of experiments is presented, aimed at measuring the efﬁcacy of our technique.",
        "keywords": [
            "Model transformation",
            "Model transformation speciﬁcation",
            "Model transformation testing",
            "Model ﬁnding",
            "Test oracle"
        ],
        "authors": [
            "Esther Guerra",
            "Mathias Soeken"
        ],
        "file_path": "data/sosym-all/s10270-013-0369-x.pdf"
    },
    {
        "title": "Modeling ecosystems of reference frameworks for assurance: a case on privacy impact assessment regulation and guidelines",
        "submission-date": "2022/02",
        "publication-date": "2022/11",
        "abstract": "To assure certain critical quality properties (e.g., safety, security, or privacy), supervisory authorities and industrial associations provide reference frameworks such as standards or guidelines that in some cases are enforced (e.g., regulations). Given the pace at which both technical advancements and risks appear, there is an increase in the number of reference frameworks. As several frameworks might apply for same systems, certain overlaps appear (e.g., regulations for different countries where the system will operate, or generic standards in conjunction with more concrete standards for a given industrial sector or system type). We propose the use of modelling for alleviating the complexity of these reference frameworks ecosystems, and we provide a tool-supported method to create them for the beneﬁt of different stakeholders. The case study is based on privacy data protection, and more concretely on privacy impact assessment processes. The European GDPR regulates the movement and processing of personal data, and, contrary to available software engineering privacy guidelines, articles in legal texts are usually difﬁcult to translate to the underlying processes, artefacts and roles that they refer to. To facilitate the mutual comprehension of legal experts and engineers, in this work we investigate how mappings can be created between these two domains of expertise. Notably, we rely on modelling as a central point. We modelled the legal requirements of the GDPR on data protection impact assessments, and then, we selected the ISO/IEC 29134, a mainstream engineering guideline for privacy impact assessment, and, taking a concrete sector as example, the EU Smart Grid Data Protection Impact Assessment template. The OpenCert tool was used for providing technical support to both the modelling and the creation of the mapping models in a systematic way. We provide a qualitative evaluation from legal experts and privacy engineering practitioners to report on the beneﬁts and limitations of this approach.",
        "keywords": [
            "Modelling",
            "OpenCert",
            "Reference frameworks",
            "Privacy",
            "GDPR",
            "ISO 29134",
            "Smart grid",
            "Privacy impact assessment"
        ],
        "authors": [
            "Alejandra Ruiz",
            "Yod-Samuel Martin",
            "Jabier Martinez",
            "Jacobo Quintans",
            "Guillaume Mockly",
            "Amelie Gyrard",
            "Tommaso Crepax"
        ],
        "file_path": "data/sosym-all/s10270-022-01061-6.pdf"
    },
    {
        "title": "Probabilistic modelling and veriﬁcation using RoboChart and PRISM",
        "submission-date": "2020/11",
        "publication-date": "2021/10",
        "abstract": "RoboChart is a timed domain-speciﬁc language for robotics, distinctive in its support for automated veriﬁcation by model checking and theorem proving. Since uncertainty is an essential part of robotic systems, we present here an extension to RoboChart to model uncertainty using probabilism. The extension enriches RoboChart state machines with probability through a new construct: probabilistic junctions as the source of transitions with a probability value. RoboChart has an accompanying tool, called RoboTool, for modelling and veriﬁcation of functional and real-time behaviour. We present here also an automatic technique, implemented in RoboTool, to transform a RoboChart model into a PRISM model for veriﬁcation. We have extended the property language of RoboTool so that probabilistic properties expressed in temporal logic can be written using controlled natural language.",
        "keywords": [
            "State machines",
            "Formal semantics",
            "Model transformation",
            "PRISM",
            "Probabilistic model checking",
            "Domain-speciﬁc language for robotics"
        ],
        "authors": [
            "Kangfeng Ye",
            "Ana Cavalcanti",
            "Simon Foster",
            "Alvaro Miyazawa",
            "Jim Woodcock"
        ],
        "file_path": "data/sosym-all/s10270-021-00916-8.pdf"
    },
    {
        "title": "Simplifying autonomic enterprise Java Bean applications via model-driven engineering and simulation",
        "submission-date": "2006/02",
        "publication-date": "2007/05",
        "abstract": "The goal of autonomic computing is to reduce the conﬁguration, operational, and maintenance costs of distributed applications by enabling them to self-manage, self-heal, and self-optimize. This paper provides two contributions to the Model-Driven Engineering (MDE) of autonomic computing systems using Enterprise Java Beans (EJBs). First, we describe the structure and functionality of an MDE tool that visually captures the design of EJB applications, their quality of service (QoS) requirements, and the adaptations applied to their EJBs. Second, the paper describes how MDE tools can be used to generate code to simulate adaptive systems for veriﬁcation and plug EJBs into a Java component framework that provides runtime adaptation capabilities.",
        "keywords": [
            "Autonomic Computing",
            "Model-Driven Engineering",
            "Enterprise Java Beans"
        ],
        "authors": [
            "Jules White",
            "Douglas C. Schmidt",
            "Aniruddha Gokhale"
        ],
        "file_path": "data/sosym-all/s10270-007-0057-9.pdf"
    },
    {
        "title": "Model modularity for reuse, libraries and composition: symbol management is key",
        "submission-date": "2024/06",
        "publication-date": "2024/06",
        "abstract": "It is insightful to observe the similarities between the ways that programs are decomposed into sets of individual and reusable artifacts and the corresponding ways that models are (or should be?) deﬁned. Modularity and encapsulation were introduced in the earlier work on Algebraic Datatypes and then Parnas pointed out the important mechanism of mod-ularity with his famous paper about “On the Criteria to be Used in Decomposing Systems into Modules.” Dijkstra also discussed modularity by popularizing the term “Separation of Concerns.” Most modern programming languages provide mechanisms for modularity and developers use the concepts of class, module, and assembly in their general programming repertoire. It is a core principle that the internal realization of implementation detail is secretly encapsulated and can only be accessed by an explicitly deﬁned and exported interface. Thisinterfaceisdeﬁnedbydeﬁnitiveprogrammingelements, which are given a human-readable name, such as class name “Person” or method name “getAge()”. Modularity has several beneﬁts, according to Parnas, including: • When the underlying secret is changed, an implementation can be exchanged and evolved without the need to modify the dependent code because the deﬁned interface remains untouched. This supports software changeability while minimizing global impacts across the code base. • Code can be developed independently within the same project, which supports parallel development by a team of software engineers. This also supports the integration and reuse of external and open-source libraries. Since McIlroy’s paper on reusability in the late 1960s, the general reusability of library code and frameworks has greatly increased programmer efﬁciency. • Modularity also helps with the comprehensibility of the program code by allowing each module to be reasoned about locally without a deep need to understand all of the inner details of other parts of a project. On the contrary, modern modeling languages and their tools do not natively provide a deep collection of reusable libraries. One key problem is the lack of well-understood, encapsulating interfaces of the deﬁned models. So far models have often been used to describe the interface of an underlying component, but the concept that a model itself has an interface and an encapsulated “body” is still unfamiliar. Only a few works from the community promote this idea, such as the concept of symbol tables for models, potentially aggregated to a model type or requirement model. This idea also has been further explored with different levels of interfaces, such as the interfaces for variability, customization, and use (VCU). Moreover, the availability of interfaces on models means that there must exist an efﬁcient management of symbols. Regarding symbols, we mean named elements that modelers deﬁne inside a model that are allowed to be referenced from outside, using the deﬁned name and signature (as providedbysymboltables,modeltypes,orrequirementmodels). This includes classes, states, activities, pins, ports, methods, attributes, variables, and many more potential kinds of symbols. Symbols are a well elaborated concept in programming languages (e.g., symbol tables explicitly store symbols in separate or joint artifacts). But symbol management is often absent in modeling languages and thus not very well accommodated in most tools. Most modern UML and SysML tools do not manage symbols explicitly, but store an integrated large model, where elements are directly connected, even though models provide explicitly deﬁned names for their",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01190-0.pdf"
    },
    {
        "title": "The technological landscape of collaborative model-driven software engineering",
        "submission-date": "2023/10",
        "publication-date": "2025/02",
        "abstract": "Collaborative technologies are continuously evolving to address existing problems and introduce innovative features for enhancing collaboration in the landscape of model-driven software engineering (MDSE). Different collaborative MDSE technologies (CMTs) provide different solutions to facilitate collaboration, making it hard for practitioners to choose the technology that best suits their needs. This study aims to investigate the landscape of CMTs and to provide a list of recommended technologies tailored to speciﬁc use case scenarios in the context of MDSE. We compiled a comprehensive list of CMTs using a systematic search complemented with snowballing, investigating both academic and grey literature. The technologies were selected through a set of inclusion and exclusion criteria and eventually analyzed through an in-depth analysis focusing on model management, collaboration, and communication. The ﬁndings of our study reveal that the current landscape of CMTs is characterized by a relatively narrow range of capabilities offered by different technologies. Consequently, practitioners often have to become proﬁcient in combining several different technologies in order to meet their needs. While various CMTs offer distinct collaboration approaches, the current landscape could be richer in terms of capabilities. Our research provides a comprehensive description of recommended CMTs, enabling practitioners to make informed decisions and improve collaboration in their MDSE processes.",
        "keywords": [
            "Collaborative modeling",
            "Model-driven software engineering",
            "Collaborative modeling technologies"
        ],
        "authors": [
            "Abhishek Choudhury",
            "Ivano Malavolta",
            "Federico Ciccozzi",
            "Kousar Aslam",
            "Patricia Lago"
        ],
        "file_path": "data/sosym-all/s10270-025-01274-5.pdf"
    },
    {
        "title": "Mashup of metalanguages and its implementation in the Kermeta language workbench",
        "submission-date": "2012/01",
        "publication-date": "2013/06",
        "abstract": "With the growing use of domain-speciﬁc languages (DSL) in industry, DSL design and implementa-tion goes far beyond an activity for a few experts only and becomes a challenging task for thousands of software engi-neers. DSL implementation indeed requires engineers to care for various concerns, from abstract syntax, static semantics, behavioral semantics, to extra-functional issues such as run-time performance. This paper presents an approach that uses one metalanguage per language implementation concern. We show that the usage and combination of those metalanguages is simple and intuitive enough to deserve the term mashup. We evaluate the approach by completely implementing the non-trivial fUML modeling language, a semantically sound and executable subset of the Uniﬁed Modeling Language (UML).",
        "keywords": [
            "DSL design and Implementation",
            "Model-driven engineering",
            "Software language engineering"
        ],
        "authors": [
            "Jean-Marc Jézéquel",
            "Benoit Combemale",
            "Olivier Barais",
            "Martin Monperrus",
            "François Fouquet"
        ],
        "file_path": "data/sosym-all/s10270-013-0354-4.pdf"
    },
    {
        "title": "The KeY tool\nIntegrating object oriented design and formal veriﬁcation",
        "submission-date": "2002/12",
        "publication-date": "2004/04",
        "abstract": "KeY is a tool that provides facilities for formal\nspeciﬁcation and veriﬁcation of programs within a com-\nmercial platform for UML based software development.\nUsing the KeY tool, formal methods and object-oriented\ndevelopment techniques are applied in an integrated man-\nner. Formal speciﬁcation is performed using the Object\nConstraint Language (OCL), which is part of the UML\nstandard. KeY provides support for the authoring and for-\nmal analysis of OCL constraints. The target language of\nKeY based development is Java Card DL, a proper sub-\nset of Java for smart card applications and embedded\nsystems. KeY uses a dynamic logic for Java Card DL to\nexpress proof obligations, and provides a state-of-the-art\ntheorem prover for interactive and automated veriﬁca-\ntion. Apart from its integration into UML based software\ndevelopment, a characteristic feature of KeY is that for-\nmal speciﬁcation and veriﬁcation can be introduced in-\ncrementally.",
        "keywords": [
            "Object-oriented design",
            "Formal speciﬁcation",
            "Formal veriﬁcation",
            "UML",
            "OCL",
            "Design patterns",
            "Java"
        ],
        "authors": [
            "Wolfgang Ahrendt",
            "Thomas Baar",
            "Bernhard Beckert",
            "Richard Bubel",
            "Martin Giese",
            "Reiner H¨ahnle",
            "Wolfram Menzel",
            "Wojciech Mostowski",
            "Andreas Roth",
            "Steﬀen Schlager",
            "Peter H. Schmitt"
        ],
        "file_path": "data/sosym-all/s10270-004-0058-x.pdf"
    },
    {
        "title": "Design notations for secure software: a systematic literature review",
        "submission-date": "2014/07",
        "publication-date": "2015/08",
        "abstract": "In the past 10years, the research community has produced a signiﬁcant number of design notations to represent security properties and concepts in a design artifact. These notations are aimed at documenting and analyzing security in a software design model. The fragmentation of the research space, however, has resulted in a complex tangle of different techniques. Hence, practitioners are confronted with the challenging task of scouting the right approach from a multitude of proposals. Similarly, it is hard for researchers to keep track of the synergies among the existing notations, in order to identify the existing opportunities for original contributions. This paper presents a systematic literature review that inventorizes the existing notations and provides an in-depth, comparative analysis for each.",
        "keywords": [
            "Security",
            "Notation",
            "Software design",
            "Empirical study"
        ],
        "authors": [
            "Alexander van den Berghe",
            "Riccardo Scandariato",
            "Koen Yskout",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-015-0486-9.pdf"
    },
    {
        "title": "ModelXGlue: a benchmarking framework for ML tools in MDE",
        "submission-date": "2023/05",
        "publication-date": "2024/06",
        "abstract": "The integration of machine learning (ML) into model-driven engineering (MDE) holds the potential to enhance the efﬁciency of modelers and elevate the quality of modeling tools. However, a consensus is yet to be reached on which MDE tasks can derive substantial beneﬁts from ML and how progress in these tasks should be measured. This paper introduces ModelXGlue, a dedicated benchmarking framework to empower researchers when constructing benchmarks for evaluating the application of ML to address MDE tasks. A benchmark is built by referencing datasets and ML models provided by other researchers, and by selecting an evaluation strategy and a set of metrics. ModelXGlue is designed with automation in mind and each component operates in an isolated execution environment (via Docker containers or Python environments), which allows the execution of approaches implemented with diverse technologies like Java, Python, R, etc. We used ModelXGlue to build reference benchmarks for three distinct MDE tasks: model classiﬁcation, clustering, and feature name recommendation. To build the benchmarks we integrated existing third-party approaches in ModelXGlue. This shows that ModelXGlue is able to accommodate heterogeneous ML models, MDE tasks and different technological requirements. Moreover, we have obtained, for the ﬁrst time, comparable results for these tasks. Altogether, it emerges that ModelXGlue is a valuable tool for advancing the understanding and evaluation of ML tools within the context of MDE.",
        "keywords": [
            "Benchmarking",
            "Machine Learning",
            "Model-Driven Engineering"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Jesús Sánchez Cuadrado",
            "Riccardo Rubei",
            "Davide Di Ruscio"
        ],
        "file_path": "data/sosym-all/s10270-024-01183-z.pdf"
    },
    {
        "title": "Reproducible execution of POSIX programs with DiOS",
        "submission-date": "2020/03",
        "publication-date": "2020/10",
        "abstract": "In this paper, we describe DiOS, a lightweight model operating system, which can be used to execute programs that make use of POSIX APIs. Such executions are fully reproducible: running the same program with the same inputs twice will result in two exactly identical instruction traces, even if the program uses threads for parallelism. DiOS is implemented almost entirely in portable C and C++: although its primary platform is DiVM, a veriﬁcation-oriented virtual machine, it can be conﬁgured to also run in KLEE, a symbolic executor. Finally, it can be compiled into machine code to serve as a user-mode kernel. Additionally, DiOS is modular and extensible. Its various components can be combined to match both the capabilities of the underlying platform and to provide services required by a particular program. Components can be added to cover additional system calls or APIs or removed to reduce overhead. The experimental evaluation has three parts. DiOS is ﬁrst evaluated as a component of a program veriﬁcation platform based on DiVM. In the second part, we consider its portability and modularity by combining it with the symbolic executor KLEE. Finally, we consider its use as a standalone user-mode kernel.",
        "keywords": [
            "Software veriﬁcation",
            "Operating systems",
            "POSIX",
            "Reproducibility",
            "C/C++"
        ],
        "authors": [
            "Petr Roˇckai",
            "Zuzana Baranová",
            "Jan Mrázek",
            "Katarína Kejstová",
            "Jiˇríí Barnat"
        ],
        "file_path": "data/sosym-all/s10270-020-00837-y.pdf"
    },
    {
        "title": "Basic problems in multi-view modeling",
        "submission-date": "2016/02",
        "publication-date": "2017/12",
        "abstract": "Modeling all aspects of a complex system within a single model is a difﬁcult, if not impossible, task. Multi-view modeling is a methodology where different aspects of the system are captured by different models, or views. A key question then is consistency: if different views of a system have some degree of overlap, how can we guarantee that they are consistent, i.e., that they do not contradict each other? In this paper we formulate this and other basic problems in multi-view modeling within an abstract formal framework. We then instantiate this framework onto several discrete system settings: languages and automata over ﬁnite and inﬁnite words, and symbolic transition systems; and study how checking view consistency and other problems can be solved in these settings.",
        "keywords": [
            "Formal methods",
            "System modeling",
            "Views",
            "Veriﬁcation",
            "Synthesis",
            "Consistency",
            "Automata",
            "Symbolic transition systems",
            "Projection",
            "Inverse projection"
        ],
        "authors": [
            "Jan Reineke",
            "Christos Stergiou",
            "Stavros Tripakis"
        ],
        "file_path": "data/sosym-all/s10270-017-0638-1.pdf"
    },
    {
        "title": "Model-based simulation of legal policies: framework, tool support, and validation",
        "submission-date": "2016/01",
        "publication-date": "2016/07",
        "abstract": "Simulation of legal policies is an important decision-support tool in domains such as taxation. The primary goal of legal policy simulation is predicting how changes in the law affect measures of interest, e.g., revenue. Legal policy simulation is currently implemented using a combination of spreadsheets and software code. Such a direct implementation poses a validation challenge. In particular, legal experts often lack the necessary software background to review complex spreadsheets and code. Consequently, these experts currently have no reliable means to check the correctness of simulations against the requirements envisaged by the law. A further challenge is that representative data for simulation may be unavailable, thus necessitating a data generator. A hard-coded generator is difﬁcult to build and validate. We develop a framework for legal policy simulation that is aimed at addressing the challenges above. The framework uses models for specifying both legal policies and the probabilistic characteristics of the underlying population. We devise an automated algorithm for simulation data generation. We evaluate our framework through a case study on Luxembourg’s Tax Law.",
        "keywords": [
            "Legal policies",
            "Simulation",
            "UML profiles",
            "Model-driven code generation",
            "Probabilistic data generation"
        ],
        "authors": [
            "Ghanem Soltana",
            "Nicolas Sannier",
            "Mehrdad Sabetzadeh",
            "Lionel C. Briand"
        ],
        "file_path": "data/sosym-all/s10270-016-0542-0.pdf"
    },
    {
        "title": "FlexiSketch: a lightweight sketching and metamodeling approach for end-users",
        "submission-date": "2016/09",
        "publication-date": "2017/09",
        "abstract": "Engineers commonly use paper and whiteboards to sketch and discuss ideas in early phases of requirements elicitation and software modeling. These physical media foster creativity because they are quick to use and do not restrict in any way the form in which content can be drawn. If the sketched information needs to be reused later on, however, engineers have to spend extra effort for preserving the information in a form that can be processed by a software modeling tool. While saving information in a machine-readable way comes for free with formal software modeling tools, they typically anticipate the use of speciﬁc, predeﬁned modeling languages and therefore hamper creativity. To combine the advantages of informal and formal tools, we have developed a ﬂexible tool-supported modeling approach that augments a sketching environment with lightweight metamodeling capabilities. Users can create their own modeling languages by deﬁning sketched constructs on demand and export model sketches as semiformal models. In this article, we ﬁrst give an overview of FlexiSketch and then focus on an evaluation of our approach with two studies conducted with both novice modelers and experienced practitioners. Our goal was to ﬁnd out how well modelers manage to use our lightweight metamodeling mechanisms, and how they build notations collaboratively. Results show that experienced modelers adopt our approach quickly, while novices have difﬁculties to distinguish between the model and meta-model levels and would beneﬁt from additional guidance and user awareness features. The lessons learned from our studies can serve as advice for similar ﬂexible modeling approaches.",
        "keywords": [
            "Requirements engineering",
            "Tool",
            "Sketching",
            "Ad hoc modeling",
            "Notation deﬁnition",
            "End-user metamodeling",
            "Lightweight metamodeling",
            "Collaborative metamodeling",
            "Evaluation"
        ],
        "authors": [
            "Dustin Wüest",
            "Norbert Seyff",
            "Martin Glinz"
        ],
        "file_path": "data/sosym-all/s10270-017-0623-8.pdf"
    },
    {
        "title": "Diagrammatic physical robot models",
        "submission-date": "2022/11",
        "publication-date": "2025/03",
        "abstract": "Simulation is a favoured technique in robotics. It is, however, costly, in terms of development time, and its usability is limited by the lack of standardisation and portability of simulators. We present RoboSim, a diagrammatic tool-independent domain-specific language to model robotic platforms and their controllers. It can be regarded as a profile of UML/SysML enriched with time primitives, differential equations, and a mathematical semantics. Our previous work on RoboSim described a notation to specify control software. In this paper, we present a novel notation to describe physical models: block diagrams that can be linked to the platform-independent software model to characterise how services required by the software are realised by actuators and sensors. Behaviours are specified by differential equations, and simulations and mathematical models of the whole system can be generated automatically. Our main contributions are a modular and extensible diagrammatic notation that supports the explicit specification of physical behaviours; a set of validation rules that identify well-formed models; a model-to-model transformation from RoboSim to an input format accepted by several simulators; and a formal semantics for mathematical reasoning.",
        "keywords": [
            "Simulation",
            "Veriﬁcation",
            "SDF",
            "Hybrid models",
            "Diagrammatic models"
        ],
        "authors": [
            "Alvaro Miyazawa",
            "Sharar Ahmadi",
            "Ana Cavalcanti",
            "James Baxter",
            "Mark Post",
            "Pedro Ribeiro",
            "Jon Timmis",
            "Thomas Wright"
        ],
        "file_path": "data/sosym-all/s10270-025-01270-9.pdf"
    },
    {
        "title": "Practitioners’ experiences with model-driven engineering: a meta-review",
        "submission-date": "2021/10",
        "publication-date": "2022/07",
        "abstract": "The Object Management Group introduced the Model-Driven Architecture in 2001. Since then, the research community has embraced model-driven engineering (MDE), but to a lesser extent than practitioners had hoped. A good awareness of practitioners’ challenges, particularly with modeling, is required to ensure the relevance of a research agenda. Therefore, this study conducts a meta-review on the state of practice in using modeling languages for software engineering over the last ﬁve years using Kitchenham’s guidelines. This study serves as an orientation within the research ﬁeld and a basis for further research. It contributes to the literature by focusing on publications discussing the practical use of modeling languages and the beneﬁts and problems perceived by practitioners. The main ﬁnding of this review is that practitioners beneﬁt from MDE in the following ways: it is beneﬁcial for several stakeholders; it saves cost; it is easy to use; it improves productivity, quality, and understanding of the system; and it provides support for software development activities. However, practitioners continue to face several serious challenges. The most frequently reported issues are the missing tool functionalities. Many studies have found that adhering to the Physics of Notation principles would improve modeling languages. Other ﬁndings include that modeling is mostly used for documentation and requirements elicitation, and UML is the most often used.",
        "keywords": [
            "Model-driven engineering",
            "Modeling in practice",
            "Systematic literature review",
            "Meta-review",
            "UML",
            "BPMN",
            "Conceptual modeling"
        ],
        "authors": [
            "Charlotte Verbruggen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-022-01020-1.pdf"
    },
    {
        "title": "On Modeling Object-Oriented Information Systems",
        "submission-date": "2001/08",
        "publication-date": "2004/04",
        "abstract": "Object-Oriented Information System (OOIS) is an information system that employs object-oriented technologies in system design and implementation. Recent research advances and industrial innovations in distributed system modeling and Internet applications have enabled OOIS design and implementation to be carried out on the basis of new technologies and platforms. This special section on Modeling Object-Oriented Information Systems presents readers with a set of best papers selected from the 7th International Conference on OOIS. Reviews of theories and applications of OOIS’s are also provided for predicating trends in OOIS modeling.",
        "keywords": [
            "System modeling",
            "Information systems",
            "Object orientation",
            "Foundations",
            "Architectures",
            "Distributed objects",
            "Patterns",
            "Trends"
        ],
        "authors": [
            "Yingxu Wang",
            "Shushma Patel"
        ],
        "file_path": "data/sosym-all/s10270-004-0053-2.pdf"
    },
    {
        "title": "Modeling and enforcing invariants of dynamic software architectures",
        "submission-date": "2008/10",
        "publication-date": "2010/04",
        "abstract": "In this paper, we propose an “end-to-end” approach that supports dynamic reconﬁguration of software architectures taking advantage of graphical modeling, formal methods and aspect-oriented programming. There are three ingredients of the proposal. The speciﬁcation end of the solution is covered by a new UML proﬁle enabling to specify the desired architectural style (model), its invariants and the intended reconﬁguration operations. In order to verify the consistency of the model and the preservation of the invariants aftereveryreconﬁguration,weautomaticallygenerateformal speciﬁcations in Z notation from the deﬁned model. At the runtime enforcing end of the solution, we propose to encode the enforcement logic as aspect in the AspectJ language. The third important ingredient that makes our approach end-to-end is the automatic translation of formal speciﬁcations into aspect-based enforcement code.",
        "keywords": [
            "Software architecture",
            "UML proﬁle",
            "Formal speciﬁcation and veriﬁcation",
            "Aspect-oriented programming",
            "Runtime enforcement"
        ],
        "authors": [
            "Slim Kallel",
            "Mohamed Hadj Kacem",
            "Mohamed Jmaiel"
        ],
        "file_path": "data/sosym-all/s10270-010-0162-z.pdf"
    },
    {
        "title": "Mutation testing with hyperproperties",
        "submission-date": "2020/02",
        "publication-date": "2021/04",
        "abstract": "We present a new method for model-based mutation-driven test case generation. Mutants are generated by making small syntactical modiﬁcations to the model or source code of the system under test. A test case kills a mutant if the behavior of the mutant deviates from the original system when running the test. In this work, we use hyperproperties—which allow to express relations between multiple executions—to formalize different notions of killing for both deterministic as well as non-deterministic models. The resulting hyperproperties are universal in the sense that they apply to arbitrary reactive models and mutants. Moreover, an off-the-shelf model checking tool for hyperproperties can be used to generate test cases. Furthermore, we propose solutions to overcome the limitations of current model checking tools via a model transformation and a bounded SMT encoding. We evaluate our approach on a number of models expressed in two different modeling languages by generating tests using a state-of-the-art mutation testing tool.",
        "keywords": [],
        "authors": [
            "Andreas Fellner",
            "Mitra Tabaei Befrouei",
            "Georg Weissenbacher"
        ],
        "file_path": "data/sosym-all/s10270-020-00850-1.pdf"
    },
    {
        "title": "Editorial for the Speccial Issue UML 2001 Conference",
        "submission-date": "2002/12",
        "publication-date": "2002/12",
        "abstract": "This paper is an editorial for a special issue of SoSyM focusing on the UML 2001 conference. It discusses the aims and scope of SoSyM, the importance of UML in the software development community, and the context of the UML 2001 conference within the evolution of the UML standard.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-002-0007-5.pdf"
    },
    {
        "title": "SoSyM reﬂections: the 2021 \"state of the journal\" report",
        "submission-date": "2022/02",
        "publication-date": "2022/02",
        "abstract": "The inaugural issue of each SoSyM volume-year is usually written during a time of personal and professional reﬂection. Over the past 12 months, researchers from all disciplines continued to experience the need for ﬂexibility as the COVID-19 pandemic extended its impact across the globe through many variants. This has forced us all to adapt in many ways such that the idea of “virtual everything” permeates our daily discussions. The software and systems modeling community continued to thrive and exhibited a great degree of ﬂexibil-ity and increased production in the overall research output, despite the limitations for in-person collaborations. In fact, for many years, the ﬁrst issue of each SoSyM volume-year has reported growth in submissions and other important met-rics. The same is true for the 2021 publication year of SoSyM.",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-022-00979-1.pdf"
    },
    {
        "title": "Tradeoffs in modeling performance of highly conﬁgurable software systems",
        "submission-date": "2016/07",
        "publication-date": "2018/02",
        "abstract": "Modeling the performance of a highly conﬁgurable software system requires capturing the inﬂuences of its conﬁguration optionsandtheirinteractionsonthesystem’sperformance.Performance-inﬂuencemodelsquantifytheseinﬂuences,explaining this way the performance behavior of a conﬁgurable system as a whole. To be useful in practice, a performance-inﬂuence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively inﬂuence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large conﬁguration space can be described accurately only with large models and signiﬁcant resource investment. By means of 10 real-world highly conﬁgurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-inﬂuence model can ﬁt different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more conﬁguration options have only a minor inﬂuence on the prediction error and that ignoring them when learning a performance-inﬂuence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on speciﬁc regions of the conﬁguration space and ﬁnd a sweet spot between accuracy and effort. We further analyzed the causes for the conﬁguration options and their interactions having the observed inﬂuences on the systems’ performance. We were able to identify several patterns across subject systems, such as dominant conﬁguration options and data pipelines, that explain the inﬂuences of highly inﬂuential conﬁguration options and interactions, and give further insights into the domain of highly conﬁgurable systems.",
        "keywords": [
            "Performance-inﬂuence models",
            "Highly conﬁgurable software systems",
            "Performance prediction",
            "Feature interactions",
            "Variability",
            "Software product lines",
            "Machine learning"
        ],
        "authors": [
            "Sergiy Kolesnikov",
            "Norbert Siegmund",
            "Christian Kästner",
            "Alexander Grebhahn",
            "Sven Apel"
        ],
        "file_path": "data/sosym-all/s10270-018-0662-9.pdf"
    },
    {
        "title": "On applying residual reasoning within neural network veriﬁcation",
        "submission-date": "2023/02",
        "publication-date": "2023/11",
        "abstract": "As neural networks are increasingly being integrated into mission-critical systems, it is becoming crucial to ensure that they meet various safety and liveness requirements. Toward, that end, numerous complete and sound veriﬁcation techniques have been proposed in recent years, but these often suffer from severe scalability issues. One recently proposed approach for improving the scalability of veriﬁcation techniques is to enhance them with abstraction/reﬁnement capabilities: instead of verifying a complex and large network, abstraction allows the veriﬁer to construct and then verify a much smaller network, and the correctness of the smaller network immediately implies the correctness of the original, larger network. One shortcoming of this scheme is that whenever the smaller network cannot be veriﬁed, the veriﬁer must perform a reﬁnement step, in which the size of the network being veriﬁed is increased. The veriﬁer then starts verifying the new network from scratch—effectively “forgetting” its earlier work, in which the smaller network was veriﬁed. Here, we present an enhancement to abstraction-based neural network veriﬁcation, which uses residual reasoning: a process where information acquired when verifying an abstract network is utilized in order to facilitate the veriﬁcation of reﬁned networks. At its core, the method enables the veriﬁer to retain information about parts of the search space in which it was determined that the reﬁned network behaves correctly, allowing the veriﬁer to focus on areas of the search space where bugs might yet be discovered. For evaluation, we implemented our approach as an extension to the Marabou veriﬁer and obtained highly promising results.",
        "keywords": [
            "Neural networks",
            "Veriﬁcation",
            "Abstraction reﬁnement",
            "Residual reasoning",
            "Incremental reasoning"
        ],
        "authors": [
            "Yizhak Yisrael Elboher",
            "Elazar Cohen",
            "Guy Katz"
        ],
        "file_path": "data/sosym-all/s10270-023-01138-w.pdf"
    },
    {
        "title": "An ontology-based approach to engineering ethicality requirements",
        "submission-date": "2022/11",
        "publication-date": "2023/07",
        "abstract": "In a world where Artiﬁcial Intelligence (AI) is pervasive, humans may feel threatened or at risk by giving up control to machines. In this context, ethicality becomes a major concern to prevent AI systems from being biased, making mistakes, or going rogue. Requirements Engineering (RE) is the research area that can exert a great impact in the development of ethical systems by design. However, proposing concepts, tools and techniques that support the incorporation of ethicality into the software development processes as explicit requirements remains a great challenge in the RE ﬁeld. In this paper, we rely on Ontology-based Requirements Engineering (ObRE) as a method to elicit and analyze ethicality requirements (‘Ethicality requirements’ is adopted as a name for the class of requirements studied in this paper by analogy to other quality requirements studied in software engineering, such as usability, reliability, and portability, etc. The use of this term (as opposed to ‘ethical requirements’) highlights that they represent requirements for ethical systems, analogous to how ‘trustworthiness requirements’ represent requirements for trustworthy systems. To put simply: the predicates ‘ethical’ or ‘trustworthy’ are not meant to be predicated over the requirements themselves). ObRE applies ontological analysis to ontologically unpack terms and notions that are referred to in requirements elicitation. Moreover, this method instantiates the adopted ontology and uses it to guide the requirements analysis activity. In a previous paper, we presented a solution concerning two ethical principles, namely Beneﬁcence and Non-maleﬁcence. The present paper extends the previous work by targeting two other important ethicality principles, those of Explicability and Autonomy. For each of these new principles, we do ontological unpacking of the relevant concepts, and we present requirements elicitation and analysis guidelines, as well as examples in the context of a driverless car case. Furthermore, we validate our approach by analysing the requirements elicitation made for the driverless car case in contrast with a similar case, and by assessing our method’s coverage w.r.t European Union guidelines for Trustworthy AI.",
        "keywords": [
            "Requirements elicitation and analysis",
            "Ontological analysis",
            "Foundational ontologies",
            "Ethicality requirements",
            "Ethical systems"
        ],
        "authors": [
            "Renata Guizzardi",
            "Glenda Amaral",
            "Giancarlo Guizzardi",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-023-01115-3.pdf"
    },
    {
        "title": "Event-driven grammars: relating abstract and concrete levels of visual languages",
        "submission-date": "2005/02",
        "publication-date": "2007/03",
        "abstract": "In this work we introduce event-driven grammars, a kind of graph grammars that are especially suited for visual modelling environments generated by meta-modelling. Rules in these grammars may be triggered by user actions (such as creating, editing or connecting elements) and in their turn may trigger other user-interface events. Their combination with triple graph transformation systems allows constructing and checking the consistency of the abstract syntax graph while the user is building the concrete syntax model, as well as managing the layout of the concrete syntax representation. As an example of these concepts, we show the deﬁnition of a modelling environment for UML sequence diagrams. A discussion is also presented of methodological aspects for the generation of environments for visual languages with multiple views, its connection with triple graph grammars, the formalization of the latter in the double pushout approach and its extension with an inheritance concept.",
        "keywords": [
            "Graph Grammars",
            "Triple Graph Transformation",
            "Meta-Modelling",
            "Visual Languages",
            "Consistency",
            "UML"
        ],
        "authors": [
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-007-0051-2.pdf"
    },
    {
        "title": "Merging of EMF models",
        "submission-date": "2011/03",
        "publication-date": "2012/10",
        "abstract": "Inadequate version control for models significantly impedes the application of model-driven software development. In particular, sophisticated support for merging model versions is urgently needed. In this paper, we present a formal approach to both two- and three-way merging of models in the EMF framework. The approach may be applied to instances of arbitrary Ecore models. We specify context-free as well as context-sensitive rules for model merging which both detect and resolve merge conflicts. Based on these rules, a merge algorithm is developed which produces a consistent model from consistent input models. The merge algorithm does neither assume unique object identifiers, nor does it require change logs. In contrast, it relies on matchings among the input models which identify common elements (state-based approach). The requirements imposed on these matchings are reduced to a minimum, e.g., there are no restrictions on the relative positions of matched elements. Altogether, the merge algorithm is widely applicable, preserves consistency, and offers advanced features, such as merging of ordered collections in the presence of arbitrary moves and handling of context-sensitive conflicts which are hard to detect and to resolve.",
        "keywords": [
            "EMF models",
            "Three-way merging",
            "Version control"
        ],
        "authors": [
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-012-0279-3.pdf"
    },
    {
        "title": "DEPS: a model- and property-based language for system synthesis problems",
        "submission-date": "2022/11",
        "publication-date": "2023/10",
        "abstract": "DEPS (design problem speciﬁcation) is a new modeling language designed to pose and solve system design problems. DEPS addresses problems of sizing, conﬁguration, resource allocation and of architecture generation for systems. Unlike system modeling languages, which are dedicated to the representation of a deﬁned system for evaluation or analysis, we propose a problem modeling language for representing the design problem with a view to its automatic resolution. Compared with other declarative problem modeling languages, DEPS is a declarative structured and property-based language that combinesstructuralmodelingfeaturesspeciﬁctoobject-orientedlanguageswithproblemspeciﬁcationfeaturesfromconstraint programming. The mathematical nature of the problems is described by formal properties encapsulated in models organized according to the architecture of the studied system. The main features of the language are presented in details and are illustrated with examples in different domains. An integrated modeling and solving environment called DEPS Studio allows the designer to express its models in DEPS, to compile the models and to compute automatically the solutions. The validation of the approach is done through two case studies. Finally, we will conclude with the studies and developments in progress which will be integrated into the next version of DEPS Studio.",
        "keywords": [
            "Model-based system synthesis",
            "Speciﬁcation",
            "Design problem modeling language",
            "Problem solving",
            "Constraint programming"
        ],
        "authors": [
            "Pierre-Alain Yvars",
            "Laurent Zimmer"
        ],
        "file_path": "data/sosym-all/s10270-023-01129-x.pdf"
    },
    {
        "title": "Reusable model transformations",
        "submission-date": "2009/11",
        "publication-date": "2010/11",
        "abstract": "Model transformations written for an input metamodel may often apply to other metamodels that share similar concepts. For example, a transformation written to refactor Java models can be applicable to refactoring UML class diagrams as both languages share concepts such as classes,methods,attributes,andinheritance.Derivingmotivation from this example, we present an approach to make model transformations reusable such that they function correctly across several similar metamodels. Our approach relies on these principal steps: (1) We analyze a transformation to obtain an effective subset of used concepts. We prune the input metamodel of the transformation to obtain an effec- tive input metamodel containing the effective subset. The effective input metamodel represents the true input domain of transformation. (2) We adapt a target input metamodel by weaving it with aspects such as properties derived from the effective input metamodel. This adaptation makes the tar- get metamodel a subtype of the effective input metamodel. The subtype property ensures that the transformation can process models conforming to the target input metamodel without any change in the transformation itself. We validate our approach by adapting well known refactoring transfor- mations (Encapsulate Field, Move Method, and Pull Up Method) written for an in-house domain-speciﬁc modeling language (DSML) to three different industry standard meta- models (Java, MOF, and UML).",
        "keywords": [
            "Adaptation",
            "Aspect weaving",
            "Genericity",
            "Metamodel pruning",
            "Model typing",
            "Model transformation",
            "Refactoring"
        ],
        "authors": [
            "Sagar Sen",
            "Naouel Moha",
            "Vincent Mahé",
            "Olivier Barais",
            "Benoit Baudry",
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-010-0181-9.pdf"
    },
    {
        "title": "Automated generation of consistent models using qualitative abstractions and exploration strategies",
        "submission-date": "2021/02",
        "publication-date": "2021/09",
        "abstract": "Automatically synthesizing consistent models is a key prerequisite for many testing scenarios in autonomous driving to ensure a designated coverage of critical corner cases. An inconsistent model is irrelevant as a test case (e.g., false positive); thus, each synthetic model needs to simultaneously satisfy various structural and attribute constraints, which includes complex geometric constraints for trafﬁc scenarios. While different logic solvers or dedicated graph solvers have recently been developed, they fail to handle either structural or attribute constraints in a scalable way. In the current paper, we combine a structural graph solver that uses partial models with an SMT-solver and a quadratic solver to automatically derive models which simultaneously fulﬁll structural and numeric constraints, while key theoretical properties of model generation like completeness or diversity are still ensured. This necessitates a sophisticated bidirectional interaction between different solvers which carry out consistency checks, decision, unit propagation, concretization steps. Additionally, we introduce custom exploration strategies to speed up model generation. We evaluate the scalability and diversity of our approach, as well as the inﬂuence of customizations, in the context of four complex case studies.",
        "keywords": [
            "Model generation",
            "Partial model",
            "Graph solver",
            "SMT-solver",
            "Numeric solver",
            "Exploration strategy",
            "Test scenario synthesis"
        ],
        "authors": [
            "Aren A. Babikian",
            "Oszkár Semeráth",
            "Anqi Li",
            "Kristóf Marussy",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-021-00918-6.pdf"
    },
    {
        "title": "Conceptual distance of models and languages",
        "submission-date": "2019/05",
        "publication-date": "2019/05",
        "abstract": "Conceptual distance (i.e., measurement of the distance between two sets of concepts) had its roots in linguistics as the semantic distance problem. In the linguistics context, conceptual distance provides a metric of the difficulty in understanding a topic across different disciplines or subject areas. However, it seems that there is no commonly agreed and well-founded theory that allows measurement of the distance between two sets of concepts. We believe that it is worthwhile to measure the conceptual distance between models, and also between languages.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-019-00734-z.pdf"
    },
    {
        "title": "A graph grammar-based formal validation of object-process diagrams",
        "submission-date": "2010/02",
        "publication-date": "2011/04",
        "abstract": "Two basic requirements from a system’s conceptual model are correctness and comprehensibility. Most modeling methodologies satisfy only one of these apparently contradicting requirements, usually comprehensibility, leaving aside problems of correctness and ambiguousness that are associated with expressiveness. Some formal modeling languages do exist, but in these languages a complete model of a complex system is fairly complicated to understand. Object-process methodology (OPM) is a holistic systems modeling methodology that combines the two major aspects of a system—structure and behavior—in one model, providing mechanisms to manage the complexity of the model using refinement-abstraction operations, which divide a complex system into many interconnected diagrams. Although the basic syntax and semantics of an OPM model are defined, they are incomplete and leave room for incorrect or ambiguous models. This work advances the formal definition of OPM by providing a graph grammar for creating and checking OPM diagrams. The grammar provides a validation methodology of the semantic and syntactic correctness of a single object-process diagram.",
        "keywords": [
            "Formal system model",
            "Object-process modeling",
            "Graph transformation",
            "Model veriﬁcation"
        ],
        "authors": [
            "Arieh Bibliowicz",
            "Dov Dori"
        ],
        "file_path": "data/sosym-all/s10270-011-0201-4.pdf"
    },
    {
        "title": "Corpus-based analysis of domain-speciﬁc languages",
        "submission-date": "2012/03",
        "publication-date": "2013/06",
        "abstract": "As more domain-speciﬁc languages (DSLs) are\ndesigned and developed, the need to evaluate these languages\nbecomes an essential part of the overall DSL life cycle.\nCorpus-based analysis can serve as an evaluation mecha-\nnism to identify characteristics of the language after it has\nbeen deployed by looking at how end-users employ it in\npractice. This analysis that is based on actual usage of the\nlanguage brings a new perspective which can be considered\nby a language engineer when working toward improving\nthe language. In this paper, we describe our utilization of\ncorpus-based analysis techniques and exemplify them on the\nevaluation of the Puppet and ATL DSLs. We also outline an\nEclipse plug-in, which is a generic corpus-based DSL analy-\nsis tool that can accommodate the evaluation of different\nDSLs.",
        "keywords": [
            "Domain-speciﬁc languages",
            "DSL",
            "Corpus",
            "Analysis",
            "ATL",
            "Puppet"
        ],
        "authors": [
            "Robert Tairas",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-013-0352-6.pdf"
    },
    {
        "title": "Model hybridization: towards a unifying theory for inductive and deductive reasoning",
        "submission-date": "2024/12",
        "publication-date": "2024/12",
        "abstract": "Whilesoftwareisrevolutionizingthemodernworld,software engineering practices must keep pace accordingly. Modern software-based systems operate under rapidly changing conditions and face ever-increasing uncertainty. These dynamics demand accelerated adaptability, or more precisely, temporal adaptability—the ability to adapt not only to a ﬁxed set of requirements but also to an evolving sequence of variable requirements, which are often increasingly driven by newly incoming usage and context data. This adaptability corresponds to how humans handle uncertainty in dynamic environments by continuously updating their mental models to accommodate new information. As a result, the traditional boundary between development-time and operations-time is blurring. \nTo address such a pressing context, current development processes often implement agile methodologies that increase the release frequency to leverage the available runtime and telemetry data, eventually driving the overall roadmap. These continuous engineering processes promise tremendous potential for gaining insights, optimizing operations, and improving decision-making. This is true in the software industry, but also in the broader scope of cyber-physical systems accompanied by so-called digital twins, across various industries, including manufacturing, health-care, transportation, and more. Recent developments show that MDE can—or maybe even must—play a central role in systematically leveraging the runtime and telemetry data to cope with this new temporal adaptability, and many researchers from the MDE community have investigated MDE technology to provide a smooth continuum between development and operations.\nIn the future, predictive capabilities in the context of continuous engineering will be leveraged for dynamically evolving ecosystems to address challenges such as sustainability at a much more complex scale. To this aim, techniques for the coordinated use of heterogeneous descriptive, predictive, and prescriptive models need to be elaborated, and the propagation of uncertainty investigated, leading towards the deﬁnition of a unifying theory for inductive and deductive reasoning.\nAt the heart and soul of the combination of inductive and deductive reasoning is the need for model hybridization. With this term, we mean that there is an urgent need and opportunity for a systematic approach to combine heterogeneous models, such as architectural models, continuous and discrete-event behavioral models, and statistical models (e.g., machine learning models).",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01249-y.pdf"
    },
    {
        "title": "Flexible and configurable verification policies with Omnibus",
        "submission-date": "2006/04",
        "publication-date": "2007/06",
        "abstract": "The three main assertion-based verification approaches are: run-time assertion checking (RAC), extended static checking (ESC) and full formal verification (FFV). Each approach offers a different balance between rigour and ease of use, making them appropriate in different situations. Our goal is to explore the use of these approaches together in a flexible way, enabling an application to be broken down into parts with different reliability requirements and different verification approaches used in each part. We explain the benefits of using the approaches together, present a set of guidelines to avoid potential conflicts and give an overview of how the Omnibus IDE provides support for the full range of assertion-based verification approaches within a single tool.",
        "keywords": [],
        "authors": [
            "Thomas Wilson",
            "Savi Maharaj",
            "Robert G. Clark"
        ],
        "file_path": "data/sosym-all/s10270-007-0060-1.pdf"
    },
    {
        "title": "State-based versus event-based speciﬁcations for information systems: a comparison of B and EB3",
        "submission-date": "2005/05",
        "publication-date": "2005/05",
        "abstract": "This paper compares two formal methods, B and eb3, for specifying information systems. These two methods are chosen as examples of the state-based paradigm and the event-based paradigm, respectively. The paper considers four viewpoints: functional behav-ior expression, validation, veriﬁcation, and evolution. Issues in expressing event ordering constraints, data in-tegrity constraints, and modularity are thereby consid-ered. A simple case study is used to illustrate the compar-ison, namely, a library management system. Two equiva-lent speciﬁcations are presented using each method. The paper concludes that B and eb3 are complementary. The former is better at expressing complex ordering and static data integrity constraints, whereas the latter provides a simpler, modular, explicit representation of dynamic constraints that are closer to the user’s point of view, while providing loosely coupled deﬁnitions of data attributes. The generality of these results from the state-based paradigm and the event-based paradigm per-spective are discussed.",
        "keywords": [
            "State-based paradigm",
            "Event-based paradigm",
            "eb3",
            "B",
            "Process algebra",
            "Information system",
            "Formal speciﬁcation"
        ],
        "authors": [
            "Benoˆıt Fraikin",
            "Marc Frappier",
            "R´egine Laleau"
        ],
        "file_path": "data/sosym-all/s10270-005-0083-4.pdf"
    },
    {
        "title": "A feature-based survey of model view approaches",
        "submission-date": "2017/01",
        "publication-date": "2017/09",
        "abstract": "When dealing with complex systems, information is very often fragmented across many different models expressed within a variety of (modeling) languages. To provide the relevant information in an appropriate way to different kinds of stakeholders, (parts of) such models have to be combined and potentially revamped by focusing on concerns of particular interest for them. Thus, mechanisms to deﬁne and compute views over models are highly needed. Several approaches have already been proposed to provide (semi)automated support for dealing with such model views. This paper provides a detailed overview of the current state of the art in this area. To achieve this, we relied on our own experiences of designingandapplyingsuchsolutions inorder to conduct a literature review on this topic. As a result, we discuss the main capabilities of existing approaches and pro- pose a corresponding research agenda. We notably contribute a feature model describing what we believe to be the most important characteristics of the support for views on models. We expect this work to be helpful to both current and poten- tial future users and developers of model view techniques, as well as to any person generally interested in model-based software and systems engineering.",
        "keywords": [
            "Modeling",
            "Viewpoint",
            "View",
            "Model",
            "Survey"
        ],
        "authors": [
            "Hugo Bruneliere",
            "Erik Burger",
            "Jordi Cabot",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-017-0622-9.pdf"
    },
    {
        "title": "Testing models and model transformations using classifying terms",
        "submission-date": "2016/01",
        "publication-date": "2016/12",
        "abstract": "This paper proposes the use of equivalence partitioning techniques for testing models and model transformations. In particular, we introduce the concept of classifying terms, which are general OCL terms on a class model enriched with OCL constraints. Classifying terms permit deﬁning equivalence classes, in particular for partitioning the source and target model spaces of the transformation, deﬁning for each class a set of equivalent models with regard to the transformation. Using these classes, a model validator tool is able to automatically construct object models for each class, which constitute relevant test cases for the transformation. We show how this approach of guiding the construction of test cases in an orderly, systematic and efﬁcient manner can be effectively used in combination with Tracts for testing both directional and bidirectional model transformations and for analyzing their behavior.",
        "keywords": [
            "Model transformations",
            "Contract-based speciﬁcations",
            "Equivalence partitioning"
        ],
        "authors": [
            "Frank Hilken",
            "Martin Gogolla",
            "Loli Burgueño",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-016-0568-3.pdf"
    },
    {
        "title": "Reusable speciﬁcation templates for deﬁning dynamic semantics of DSLs",
        "submission-date": "2016/10",
        "publication-date": "2017/03",
        "abstract": "In the context of model-driven engineering, the dynamic (execution) semantics of domain-speciﬁc languages (DSLs) is usually not speciﬁed explicitly and stays (hard) coded in model transformations and code generation. This poses challenges such as learning, debugging, understanding, maintaining, and updating a DSL. Facing the lack of supporting tools for specifying the dynamic semantics of DSLs (or programming languages in general), we propose to specify the architecture and the detailed design of the software that implements the DSL, rather than requirements for the behavior expected from DSL programs. To compose such a speciﬁcation, we use speciﬁcation templates that capture softwaredesignsolutionstypicalforthe(application)domain of the DSL. As a result, on the one hand, our approach allows for an explicit and clear deﬁnition of the dynamic semantics of a DSL, supports separation of concern and reuse of typical design solutions. On the other hand, we do not introduce (yet another) speciﬁcation formalism, but we base our approach on an existing formalism and apply its extensive tool support for veriﬁcation and validation to the dynamic semantics of a DSL.",
        "keywords": [
            "Domain-speciﬁclanguage",
            "Dynamicsemantics",
            "Speciﬁcation template",
            "Generic programming",
            "Aspect-oriented programming"
        ],
        "authors": [
            "Ulyana Tikhonova"
        ],
        "file_path": "data/sosym-all/s10270-017-0590-0.pdf"
    },
    {
        "title": "Constructing and verifying a robust Mix Net using CSP",
        "submission-date": "2013/11",
        "publication-date": "2015/06",
        "abstract": "A Mix Net is a cryptographic protocol that unlinksthecorrespondencebetweenitsinputsanditsoutputs. Inthispaper,weformallyanalyseaMixNetusingtheprocess algebra CSP and its associated model checker FDR. The protocol that we verify removes the reliance on a Web Bul- letin Board: rather than communicating via a Web Bulletin Board, the protocol allows the mix servers to communicate directly, exchanging signed messages and maintaining their own records of the messages they have received. Mix Net analyses in the literature are invariably focused on safety properties; important liveness properties, such as deadlock freedom, are wholly neglected. This is an unhappy omission, however, since a Mix Net that produces no results is of little use. In contrast, we verify here that the Mix Net is guaranteed to terminate, with each honest mix server outputting the decrypted vector of plaintexts alongside a chain proving that each re-encryption/permutation and partial decryption operation was performed correctly, under the assumption that there is an honest majority of them acting according to the protocol.",
        "keywords": [
            "Mix Nets",
            "Formal methods",
            "Model checking",
            "CSP",
            "FDR"
        ],
        "authors": [
            "Efstathios Stathakidis",
            "David M. Williams",
            "James Heather"
        ],
        "file_path": "data/sosym-all/s10270-015-0474-0.pdf"
    },
    {
        "title": "The complexities of the satisﬁability checking problems of feature diagram sublanguages",
        "submission-date": "2022/02",
        "publication-date": "2022/10",
        "abstract": "It is well-known that the satisﬁability problem of feature diagrams (FDs) is computationally hard. This paper examines the complexities of the satisﬁability problems of sixteen FD sublanguages, i.e., languages that only permit a subset of the syntactic modeling elements of general FDs. Previous work neglected a detailed examination of the complexities of the satisﬁability problems of FD sublanguages, although results in this area could lead to the development of efﬁcient satisﬁability checking procedures for the FDs of speciﬁc sublanguages. This paper contributes to ﬁll this gap by analyzing the complexities of the satisﬁability problems of sixteen FD sublanguages that differ in whether they allow or-groups, xor-groups, excludes constraints, and requires constraints. The results of this paper show that the satisﬁability problem is NP-complete for eight of these sublanguages, is solvable in linear time for two of these sublanguages, and is trivial for the remaining six sublanguages in the sense that every FD of these sublanguages is satisﬁable. The results are extended to feature model sublanguages with complex cross-tree constraints by extending FD sublanguages that have satisﬁability problems, which are solvable in polynomial time. The feature model sublanguages also have satisﬁability problems that are solvable in polynomial time.",
        "keywords": [
            "Feature diagram",
            "Feature model",
            "Satisﬁability",
            "Analysis",
            "Semantics",
            "Complexity",
            "NP-complete"
        ],
        "authors": [
            "Oliver Kautz"
        ],
        "file_path": "data/sosym-all/s10270-022-01048-3.pdf"
    },
    {
        "title": "Variability testing in the wild: the Drupal case study",
        "submission-date": "2014/10",
        "publication-date": "2015/04",
        "abstract": "Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in singlefeaturesand160faultstriggeredbytheinteractionofupto four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.",
        "keywords": [
            "Test case prioritization",
            "Test case selection",
            "Variability-intensive systems",
            "Automated testing",
            "Non-functional properties",
            "Variability testing"
        ],
        "authors": [
            "Ana B. Sánchez",
            "Sergio Segura",
            "José A. Parejo",
            "Antonio Ruiz-Cortés"
        ],
        "file_path": "data/sosym-all/s10270-015-0459-z.pdf"
    },
    {
        "title": "Decision-making under uncertainty: be aware of your priorities",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Self-adaptive systems (SASs) are increasingly leveraging autonomy in their decision-making to manage uncertainty in their operating environments. A key problem with SASs is ensuring their requirements remain satisﬁed as they adapt. The trade-off analysis of the non-functional requirements (NFRs) is key to establish balance among them. Further, when performing the trade-offs it is necessary to know the importance of each NFR to be able to resolve conﬂicts among them. Such trade-off analyses are often built upon optimisation methods, including decision analysis and utility theory. A problem with these techniques is that they use a single-scalar utility value to represent the overall combined priority for all the NFRs. However, this combined scalar priority value may hide information about the impacts of the environmental contexts on the individual NFRs’ priorities, which may change over time. Hence, there is a need for support for runtime, autonomous reasoning about the separate priority values for each NFR, while using the knowledge acquired based on evidence collected. In this paper, we propose Pri-AwaRE, a self-adaptive architecture that makes use of Multi-Reward Partially Observable Markov Decision Process (MR-POMDP) to perform decision-making for SASs while offering awareness of NFRs’ priorities. MR-POMDP is used as a priority-aware runtime speciﬁcation model to support runtime reasoning and autonomous tuning of the distinct priority values of NFRs using a vector-valued reward function. We also evaluate the usefulness of our Pri-AwaRE approach by applying it to two substantial example applications from the networking and IoT domains.",
        "keywords": [
            "Self-Adaptive systems",
            "Priorities",
            "Non-functional requirements",
            "Decision-making"
        ],
        "authors": [
            "Huma Samin",
            "Nelly Bencomo",
            "Peter Sawyer"
        ],
        "file_path": "data/sosym-all/s10270-021-00956-0.pdf"
    },
    {
        "title": "An improved approach on the model checking for an agent-based simulation system",
        "submission-date": "2019/06",
        "publication-date": "2020/06",
        "abstract": "Model checking is an effective way to verify behaviours of an agent-based simulation system. Three behaviours are analysed: operational, control, and global behaviours. Global behaviours of a system emerge from operational behaviours of local components regulated by control behaviours of the system. The previous works principally focus on verifying the system from the operational point of view (operational behaviour). The satisfaction of the global behaviour of the system conforming to the control behaviour has not been investigated. Thus, in this paper, we propose a more complete approach for verifying global and operational behaviours of systems. To do so, these three behaviours are ﬁrstly formalized by automata-based techniques. The meta-transformation between automata theories and Kripke structure is then provided, in order to illustrate the feasibility for the model transformation between the agent-based simulation model and Kripke structure-based model. Then, a mapping between the models is proposed. Subsequently, the global behaviour of the system is veriﬁed by the properties extracted from the control behaviour and the operational behaviour is checked by general system performance properties (e.g. safety, deadlock freedom). Finally, a case study on the simulation system for aircraft maintenance has been carried out. A counterexample of signals sending between Flight agent and Plane agent has been produced by NuSMV model checker. Modiﬁcations for the NuSMV model and agent-based simulation model have been performed. The experiment results show that 9% out of 19% of ﬂights have been changed to be serviceable.",
        "keywords": [
            "Model checking",
            "Agent-based simulation system",
            "Global behaviours and operational behaviours",
            "Model transformation"
        ],
        "authors": [
            "Yinling Liu",
            "Tao Wang",
            "Haiqing Zhang",
            "Vincent Cheutet"
        ],
        "file_path": "data/sosym-all/s10270-020-00807-4.pdf"
    },
    {
        "title": "Automatic veriﬁcation of behavior preservation at the transformation level for relational model transformation",
        "submission-date": "2017/07",
        "publication-date": "2018/12",
        "abstract": "The correctness of model transformations is a crucial element for model-driven engineering of high-quality software. In particular, behavior preservation is an important correctness property avoiding the introduction of semantic errors during the model-driven engineering process. Behavior preservation veriﬁcation techniques show some kind of behavioral equivalence or reﬁnement between source and target model of the transformation. Automatic tool support is available for verifying behavior preservation at the instance level, i.e., for a given source and target model speciﬁed by the model transformation. However, until now there is no sound and automatic veriﬁcation approach available at the transformation level, i.e., for all source and target models. In this article, we extend our results presented in earlier work (Giese and Lambers, in: Ehrig et al (eds) Graph transformations, Springer, Berlin, 2012) and outline a new transformation-level approach for the sound and automatic veriﬁcation of behavior preservation captured by bisimulation resp. simulation for outplace model transformations speciﬁed by triple graph grammars and semantic deﬁnitions given by graph transformation rules. In particular, we ﬁrst show how behavior preservation can be modeled in a symbolic manner at the transformation level and then describe that transformation-level veriﬁcation of behavior preservation can be reduced to invariant checking of suitable conditions for graph transformations. We demonstrate that the resulting checking problem can be addressed by our own invariant checker for an example of a transformation between sequence charts and communicating automata.",
        "keywords": [
            "Relational model transformation",
            "Formal veriﬁcation of behavior preservation",
            "Behavioral equivalence and reﬁnement",
            "Bisimulation and simulation",
            "Graph transformation",
            "Triple graph grammars",
            "Invariant checking"
        ],
        "authors": [
            "Johannes Dyck",
            "Holger Giese",
            "Leen Lambers"
        ],
        "file_path": "data/sosym-all/s10270-018-00706-9.pdf"
    },
    {
        "title": "Can explainable artiﬁcial intelligence support software modelers in model comprehension?",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "As software systems become increasingly complex, the application of artiﬁcial intelligence (AI) in software engineering is gaining relevance. However, a critical gap exists in the understanding and interpretation of AI-driven decision-making processes, especially in areas intrinsically linked to human expertise, such as software modeling. This paper proposes an exploratory study on the feasibility, efﬁcacy, and relevance of eXplainable Artiﬁcial Intelligence (XAI) techniques within this context. The application of machine learning (ML) to software models is relatively recent, so efforts such as the ModelSet dataset are crucial for deriving effective training data for ML models. In addition, predictive software modeling tasks present some unusual requirements, such as the need for multi-class and multi-label approaches that are not as commonly investigated from an XAI perspective. In fact, the adoption of XAI in software modeling has been barely explored and possibly requires adapted methodologies. Our approach encompasses an in-depth examination of explanations generated by ﬁve XAI techniques that evaluate feature contributions globally for ML models and locally for speciﬁc predictions. This could help software modelers understand, for example, why a model is classiﬁed in a particular domain. Additionally, our study includes a survey conducted among software modelers to capture how explanations support their decision-making, to evaluate the perceived level of agreement between different XAI techniques, and to identify current limitations. We argue that XAI can improve the transparency and trustworthiness of the decision-making process for software modelers, thereby fostering a deeper understanding of intricate modeling tasks.",
        "keywords": [
            "Software modeling",
            "Machine learning",
            "Explainable artiﬁcial intelligence"
        ],
        "authors": [
            "Francisco Javier Alcaide",
            "José Raúl Romero",
            "Aurora Ramírez"
        ],
        "file_path": "data/sosym-all/s10270-024-01251-4.pdf"
    },
    {
        "title": "Epsilon Flock: a model migration language",
        "submission-date": "2011/03",
        "publication-date": "2012/11",
        "abstract": "Model-driven engineering introduces additional challenges for controlling and managing software evolution. Today, tools exist for generating model editors and for managing models with transformation, validation, merging and weaving. There is limited support, however, for model migration—a development activity in which instance models areupdatedinresponsetometamodelevolution.Inthispaper, we propose conservative copy—a style of model transforma-tion that we believe is well-suited to model migration—and Epsilon Flock—a compact model-to-model transformation language tailored for model migration. The proposed struc-tures are evaluated by comparing the conciseness of model migration strategies written in different styles of transfor-mation language, using several examples of evolution taken from UML and the graphical modelling framework.",
        "keywords": [
            "Model migration",
            "Metamodel evolution",
            "Model transformation"
        ],
        "authors": [
            "Louis M. Rose",
            "Dimitrios S. Kolovos",
            "Richard F. Paige",
            "Fiona A. C. Polack",
            "Simon Poulding"
        ],
        "file_path": "data/sosym-all/s10270-012-0296-2.pdf"
    },
    {
        "title": "A descriptive study of assumptions in STRIDE security threat modeling",
        "submission-date": "2020/10",
        "publication-date": "2021/11",
        "abstract": "Security threat modeling involves the systematic elicitation of plausible threat scenarios, and leads to the identiﬁcation and articulation of the security requirements in the early stages of software development. Although they are an important source of architectural knowledge, assumptions made in this context are in practice left implicit or at best, documented informally in an unstructured textual format. As guidelines and best practices are lacking, the nature, purpose and impact of assumptions made in this context is generally not well understood. We present a descriptive study of in total 640 textual assumptions made in 96 STRIDE threat models of the same system. The study mainly focuses on the diversity in how assumptions are used in practice, in terms of (i) the role or function of these assumptions in the threat modeling process, (ii) the degree of coupling between the assumptions and the system under analysis, and (iii) the extent to which these assumptions are exclusively speciﬁc to security. We observe large differences on all three investigated aspects: practitioners use the mechanism of assumption-making for diverse purposes, but predominantly to exclude certain threats from further analysis, i.e. to scope the analysis effort by steering it away from threat scenarios that are considered less relevant up front. Based on our ﬁndings, we argue against the exclusive use of Data Flow Diagrams as the main basis for threat analysis, and in favor of integrating more expressive attacker and trust models which can co-evolve with the threat model and the system.",
        "keywords": [
            "Threat modeling",
            "Security architecture",
            "Secure development life-cycle (SDLC)",
            "STRIDE",
            "Security assumptions",
            "Architecture knowledge management"
        ],
        "authors": [
            "Dimitri Van Landuyt",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-021-00941-7.pdf"
    },
    {
        "title": "Jidoka: automation with a human touch",
        "submission-date": "2024/01",
        "publication-date": "2024/12",
        "abstract": "Organisations often ﬁnd themselves trapped in a costly cycle of replacing legacy systems. Jidoka is a software development methodology that aims to break this cycle and for organisations to mature into a mode of continuous modernisation. Jidoka, which loosely translates to automation with a human touch, offers several strategies for developers to modernise legacy systems onto their preferred technology stack, ensuring that quality, security, and other compliance is met. The paper presents state-of-practice information on how the process emerged through the use of Model-Driven Engineering (MDE), DevOps, and lessons learned using other software development methodologies across 100+ projects.",
        "keywords": [
            "Software development methodologies",
            "Model-driven engineering",
            "DevOps",
            "Software evolution",
            "Legacy system modernisation",
            "Empirical"
        ],
        "authors": [
            "Eban Escott"
        ],
        "file_path": "data/sosym-all/s10270-024-01256-z.pdf"
    },
    {
        "title": "Verifying consistency of software product line architectures with product architectures",
        "submission-date": "2022/05",
        "publication-date": "2023/07",
        "abstract": "There has been increasing interest in modeling software product lines (SPLs) using architecture description languages (ADLs). However, sometimes it is required to reverse engineer an SPL architecture from a set of product architectures. This procedure needs to be performed manually as currently does not exist tool support to automate this task. In this case, verifying consistency between the product architectures and the reverse engineered SPL architecture is still a challenge; particularly, verifying component interconnection aspects of product architectures with respect to the commonality and variability of an SPL architecture represented in an ADL. Current approaches are unable to detect whether the component interconnections in a product architecture have inconsistencies with the component interconnections deﬁned by the SPL architecture. To tackle these shortcomings, we developed the Ontology-based Product Architecture Veriﬁcation (OntoPAV) framework. OntoPAV relies on the ontology formalism to capture the commonality and variability of SPLs architectures. Reasoning engines are employed to automatically identify component interconnection inconsistencies among SPL and product architectures. Our evaluation results show that our veriﬁer has a high accuracy for detecting consistency errors and that it scales linearly for architectures from 1000 to 5000 architecture elements.",
        "keywords": [
            "Software product lines",
            "Software architecture",
            "SPL Veriﬁcation",
            "Architecture veriﬁcation",
            "Ontologies",
            "Model-driven engineering"
        ],
        "authors": [
            "Hector A. Duran-Limon",
            "Perla Velasco-Elizondo",
            "Manuel Mora",
            "Maria E. Meda-Campana",
            "Karina Aguilar",
            "Martha Hernandez-Ochoa",
            "Leonardo Soto Sumuano"
        ],
        "file_path": "data/sosym-all/s10270-023-01114-4.pdf"
    },
    {
        "title": "Automatic generation of basic behavior schemas from UML class diagrams",
        "submission-date": "2008/03",
        "publication-date": "2008/12",
        "abstract": "The speciﬁcation of a software system must include all relevant static and dynamic aspects of the domain. Dynamic aspects are usually speciﬁed by means of a behavioral schema consisting of a set of system operations that the user may execute to update the system state. To be useful, such a set must be complete (i.e. through these operations, users should be able to modify the population of all elements in the class diagram) and executable (i.e. for each opera-tion, there must exist a system state over which the oper-ation can be successfully applied). A manual speciﬁcation of these operations is an error-prone and time-consuming activity. Therefore, the aim of this paper is to present a strat-egy for the automatic generation of a basic behavior schema. Operations in the schema are drawn from the static aspects of the domain as deﬁned in the UML class diagram and take into account possible dependencies among them to ensure the completeness and executability of the operations. We believe our approach is especially useful in a Model-Driven Devel-opment setting, where the full implementation of the system is derived from its speciﬁcation. In this context, our approach facilitates the definition of the behavioral speciﬁcation and ensures its quality obtaining, as a result, an improved code generation phase.",
        "keywords": [
            "Behavior schema",
            "Operation",
            "Structural event",
            "Class diagram",
            "UML",
            "OCL"
        ],
        "authors": [
            "Manoli Albert",
            "Jordi Cabot",
            "Cristina Gómez",
            "Vicente Pelechano"
        ],
        "file_path": "data/sosym-all/s10270-008-0108-x.pdf"
    },
    {
        "title": "The design space of multi-language development environments",
        "submission-date": "2012/11",
        "publication-date": "2013/09",
        "abstract": "Non-trivial software systems integrate many artifacts expressed in multiple modeling and programming languages. However, even though these artifacts heavily depend on each other, existing development environments do not sufficiently support handling relations between artifacts in different languages. By means of a literature survey, tool prototyping, and experiments, we study the design space of multi-language development environments (MLDEs)—tools that consider cross-language relations as first artifacts. We ask: What is the state of the art in the MLDE space? What are the design choices and challenges faced by tool builders? To what extent are MLDEs desired by users, and what aspects of MLDEs are particularly helpful? Our main conclusions are that (a) cross-language relations are ubiquitous and troublesome in multi-language systems, (b) users highly appreciate cross-language support mechanisms of MLDEs, and (c) generic MLDEs clearly advance the state of the art in tooling for language integration. The technical artifacts resulting from this study include a feature model of the MLDE design space, a data set of harvested cross-language relations in a case study system (JTrac) and two MLDE prototypes, TexMo and Coral, that implement two radically different choices in the design space.",
        "keywords": [
            "Multi-language development environment",
            "Multi-modeling",
            "Cross-language relations"
        ],
        "authors": [
            "Rolf-Helge Pfeiffer",
            "Andrzej Wa˛sowski"
        ],
        "file_path": "data/sosym-all/s10270-013-0376-y.pdf"
    },
    {
        "title": "Modeling learning technology systems as business systems",
        "submission-date": "2002/09",
        "publication-date": "2003/06",
        "abstract": "The design of Learning Technology Systems, and the Software Systems that support them, is largely conducted on an intuitive, ad hoc basis, thus resulting in inefficient systems that defectively support the learning process. There is now justifiable, increasing effort in formalizing the engineering of Learning Technology Systems in order to achieve better learning effectiveness as well as development efficiency. This paper presents such an approach for designing Learning Technology Systems and their most popular specialization, the Web-based Learning Systems, by modeling them as business systems, using business-modeling methods. The aim is to provide an in-depth analysis and comprehension of the Learning Technology Systems and Web-based Learning Systems’ domain, that can be used for improving the systems themselves, as well as for building the supporting software systems. Our work is based upon the Learning Technology Systems Architecture standard of IEEE LTSC, on the empirical results of designing Web-based Learning Systems for university courses and on the practices of the Rational Unified Process and the Unified Modeling Language.",
        "keywords": [
            "Business model",
            "Learning technology system",
            "Unified modeling language",
            "Rational unified process",
            "Web-based learning systems",
            "Open and distance learning",
            "e-learning",
            "Learning technology systems architecture"
        ],
        "authors": [
            "Paris Avgeriou",
            "Symeon Retalis",
            "Nikolaos Papaspyrou"
        ],
        "file_path": "data/sosym-all/s10270-003-0022-1.pdf"
    },
    {
        "title": "Context-driven process discovery: enhancing process flow interpretability with contextualized activity hierarchies",
        "submission-date": "2024/11",
        "publication-date": "Not found",
        "abstract": "Analyzing business processes is important for organizations aiming to optimize operations and identify inefficiencies. Traditional discovered process models often lack sufficient contextual depth, limiting the interpretability and actionability of the revealed activity process flows. This paper addresses the challenge of balancing interpretability with complexity in discovered process models by introducing a new context-driven method, namely Contextualized Activity hierarchies for Process discovery (CARPI). CARPI consists of a detailed five-step process to identify, extract, and integrate meaningful contextual variables into core activities flows in the process to enhance model clarity and decision-making support. We implement and validate this method using a real-world case study in manufacturing and the BPI Challenge 2017 dataset, demonstrating how the integration of relevant contextual variables refines process models to make activity flows more interpretable and actionable. This contribution advances the field of process mining by offering a clear and structured method to enrich process models with important context variables, laying the foundation for more insightful and effective business process management and improvement.",
        "keywords": [
            "Process mining",
            "Contextualization",
            "Activity specialization",
            "Context aggregation",
            "Sensor data",
            "Activity labeling",
            "Industrial case study"
        ],
        "authors": [
            "Zahra Ahmadi",
            "Jochen De Weerdt",
            "Estefanía Serral Asensio"
        ],
        "file_path": "data/sosym-all/s10270-025-01313-1.pdf"
    },
    {
        "title": "Softw Syst Model (2003) 2: 3–4",
        "submission-date": "2003/02",
        "publication-date": "2003/02",
        "abstract": "In nearly every ﬁeld of computer science, models and modeling play an important role. Many researchers concentrate their research on modeling issues in their respective ﬁelds, for example in requirements, databases, workﬂow management or Petri Nets. However, these researchers are typically rooted in the research communities of their speciﬁc sub-domains. Modeling, being a cross-cutting issue, has no research community of its own. Consequently, researchers are frequently not aware of modeling research and results outside their own sub-domain. In 1997, a couple of German-speaking modeling researchers became aware of this problem and developed the idea of bringing together researchers from various ﬁelds of computer science who share a common interest in modeling problems.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-003-0016-z.pdf"
    },
    {
        "title": "Ontological Evaluation of the UML Using the Bunge–Wand–Weber Model",
        "submission-date": "2002/03",
        "publication-date": "2002/09",
        "abstract": "An ontological model of information systems, the Bunge–Wand–Weber (BWW) model, is used to analyse and evaluate the Uniﬁed Modeling Language (UML) as a language for representing concrete problem domains. As a result, each relevant and major UML construct becomes more precisely deﬁned in terms of the phenomena in and aspects of the problem domain it represents. The analysis and evaluation shows that many of the central UML constructs are well matched with the BWW-model, but also suggests several concrete improvements to the UML-metamodel.Newmetaclassesareproposedtodistin-guish between (physically) impossible and (humanly) disallowed events, based on UML-exceptions. New abstract metaclasses are proposed for static and behavioural constraints, behaviours and static behaviours, as well as bindingrelationshipsandcoupledevents.Newmeta-subclasses ofUML-objects,-classes,-typesand-relationshipsarepro-posed to make the UML more orthogonal,and a new deﬁ-nition is proposed for UML-responsibilities. The analysis also shows that the constructs in the UML must play sev-eral roles simultaneously, supporting representation both of the problem domain, of the development artifacts and of the proposed software or information system, while ﬁtting together as a tightly integrated, well-deﬁned language.",
        "keywords": [
            "Object-oriented analysis",
            "Problem domain representation",
            "Ontological analysis and evaluation",
            "Uniﬁed Modeling Language (UML)",
            "The Bunge",
            "Wand",
            "Weber model (BWW)"
        ],
        "authors": [
            "Andreas L. Opdahl",
            "Brian Henderson-Sellers"
        ],
        "file_path": "data/sosym-all/s10270-002-0003-9.pdf"
    },
    {
        "title": "Implementing associations: UML 2.0 to Java 5",
        "submission-date": "2005/05",
        "publication-date": "2006/07",
        "abstract": "A signiﬁcant current software engineering problem is the conceptual mismatch between the abstract concept of an association as found in modelling languages such as UML and the lower level expressive facilities available in object-oriented languages such as Java. This paper introduces some code generation patterns that aid the production of Java based implementations from UML models. The work is motivated by a project to construct model driven development tools in support of the construction of embedded systems. This involves the speciﬁcation and implementation of a number of meta-models (or models of languages). Many current UML oriented tools provide code generation facilities, in particular the generation of object-oriented code from class diagrams. However, many of the more complex aspects of class diagrams, such as qualiﬁed associations are not supported. In addition, several concepts introduced in UML version 2.0 are also not supported.The aim of the work presented in this paper is to develop a number of code generation patterns that allow us to support the automatic generation of Java code from UML class diagrams that support these new and complex association concepts. These patterns signiﬁcantly improve the code generation abilities of UML tools, providing a useful automation facility that bridges the gap between the concept of an association and lower level object-oriented programming languages.",
        "keywords": [
            "UML",
            "Java",
            "Association",
            "Property",
            "Code Generation"
        ],
        "authors": [
            "D. Akehurst",
            "G. Howells",
            "K. McDonald-Maier"
        ],
        "file_path": "data/sosym-all/s10270-006-0020-1.pdf"
    },
    {
        "title": "Visual notations in container orchestrations: an empirical study with Docker Compose",
        "submission-date": "2021/07",
        "publication-date": "2022/09",
        "abstract": "Container orchestration tools supporting infrastructure-as-code allow new forms of collaboration between developers and operatives. Still, their text-based nature permits naive mistakes and is more difﬁcult to read as complexity increases. We can ﬁnd few examples of low-code approaches for deﬁning the orchestration of containers, and there seems to be a lack of empirical studies showing the beneﬁts and limitations of such approaches. We hypothesize that a complete visual notation for Docker-based orchestrations could reduce the effort, the error rate, and the development time. Therefore, we developed a tool featuring such a visual notation for Docker Compose conﬁgurations, and we empirically evaluated it in a controlled experiment with novice developers. The results show a signiﬁcant reduction in development time and error-proneness when deﬁning Docker Compose ﬁles, supporting our hypothesis. The participants also thought the prototype easier to use and useful, and wanted to use it in the future.",
        "keywords": [
            "Container orchestrations",
            "Infrastructure as code",
            "Empirical study",
            "Visual programming",
            "Docker",
            "Docker Compose"
        ],
        "authors": [
            "Bruno Piedade",
            "João Pedro Dias",
            "Filipe F. Correia"
        ],
        "file_path": "data/sosym-all/s10270-022-01027-8.pdf"
    },
    {
        "title": "On the reuse and recommendation of model refactoring specifications",
        "submission-date": "2011/03",
        "publication-date": "2012/04",
        "abstract": "Refactorings can be used to improve the structure of software artefacts while preserving the semantics of the encapsulated information. Various types of refactorings have been proposed and implemented for programming languages (e.g., Java or C#). With the advent of Model-Driven Software Development (MDSD), a wealth of modelling languages rises and the need for restructuring models similar to programs has emerged. Since parts of these modelling languages are often very similar, we consider it beneficial to reuse the core transformation steps of refactorings across languages. In this sense, reusing the abstract transformation steps and the abstract participating elements suggests itself. Previous work in this field indicates that refactorings can be specified generically to foster their reuse. However, existing approaches can handle certain types of modelling languages only and solely reuse refactorings once per language. In this paper, a novel approach based on role models to specify generic refactorings is presented. Role models are suitable for this problem since they support declaration of roles which have to be played in a certain context. Assigned to generic refactoring, contexts are different refactorings and roles are the participating elements. We discuss how this resolves the limitations of previous works, as well as how specific refactorings can be defined as extensions to generic ones. The approach was implemented in our tool Refactory based on the Eclipse Modeling Framework (EMF) and evaluated using multiple modelling languages and refactorings. In addition, this paper investigates on the recommendation of refactoring specifications. This is motivated by the fact that language designers have many possibilities to enable refactorings in their modelling languages with regard to the language structures. To overcome this problem and to support language designers in deciding which refactorings to enable, we propose a solution and a prototypical implementation.",
        "keywords": [
            "Generic model refactoring",
            "Role-based refactoring",
            "Refactoring reuse",
            "Refactoring recommendation",
            "Role modelling"
        ],
        "authors": [
            "Jan Reimann",
            "Mirko Seifert",
            "Uwe Aßmann"
        ],
        "file_path": "data/sosym-all/s10270-012-0243-2.pdf"
    },
    {
        "title": "Dynamic Meta Modeling with time: Specifying the semantics of multimedia sequence diagrams",
        "submission-date": "2003/02",
        "publication-date": "2004/04",
        "abstract": "The Uniﬁed Modeling Langugage (UML) of-fers diﬀerent diagram types to model the behavior of software systems. In some domains like embedded real-time systems or multimedia systems, it is necessary to in-clude speciﬁcations of time in behavioral models since the correctness of these applications depends on the fulﬁll-ment of temporal requirements in addition to functional requirements. UML thus already incorporates language features to model time and temporal constraints. Such model elements must have an equivalent in the semantic domain.\nWe have proposed Dynamic Meta Modeling (DMM), an approach based on graph transformation, as a means for specifying operational semantics of dynamic UML di-agrams. In this article, we extend this approach to also account for time by extending the semantic domain to timed graph transformation. This enables us to deﬁne the operational semantics of UML diagrams with time speci-ﬁcations. As an example, we provide semantics for special sequence diagrams from the domain of multimedia application modeling.",
        "keywords": [
            "Formal semantics",
            "Meta modeling",
            "UML extensions",
            "Graph transformation",
            "Time",
            "Multimedia",
            "Sequence diagram"
        ],
        "authors": [
            "Jan Hendrik Hausmann",
            "Reiko Heckel",
            "Stefan Sauer"
        ],
        "file_path": "data/sosym-all/s10270-003-0045-7.pdf"
    },
    {
        "title": "Meet OCL♯, a relational object constraint language",
        "submission-date": "2024/04",
        "publication-date": "2025/03",
        "abstract": "At its core, OCL as currently deﬁned is a ﬁrst-order functional language: its expressions evaluate to single values, with collections accounting for multitudes of values, and special values null and invalid for partiality. By contrast, the data model providing the context of OCL expressions is inherently relational: the associations of UML class diagrams are essentially relations, with uniqueness and order designators extending expressiveness to ordered multirelations. As a result, OCL suffers from a functional/relational impedance mismatch, which is only superﬁcially addressed by its navigation shorthands. At the same time, OCL is inherently unsafe: expressions containing subexpressions evaluating to null may be invalid, translating to a runtime error in programming languages. We address this situation by turning OCL into a relational language that retains most of OCL’s original syntax and semantics, yet revises its fundamental design decisions that lead up to the noted problems. In particular, our version of OCL, which we call OCL♯, is type-safe.",
        "keywords": [
            "Speciﬁcation language",
            "Relational language",
            "Collections",
            "Type safety"
        ],
        "authors": [
            "Friedrich Steimann",
            "Robert Clarisó",
            "Martin Gogolla"
        ],
        "file_path": "data/sosym-all/s10270-025-01286-1.pdf"
    },
    {
        "title": "SoSyM reﬂections: the 2018 “State of the Journal” report",
        "submission-date": "2019/01",
        "publication-date": "2019/01",
        "abstract": "We are delighted to kick off another volume of SoSyM, with this first issue of 2019! The “health” of SoSyM continues to be in good shape, with evidence of its growing popularity and impact. The statistics reported in the next section indicate that we have a steady rise in the number of incoming and also accepted papers. Therefore, we are proud to announce that beginning with this issue, we will have an increase from four to six issues per year. This will allow us to grow the overall number of pages published each year.",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-019-00715-2.pdf"
    },
    {
        "title": "Synchrony and asynchrony in conformance testing",
        "submission-date": "2012/03",
        "publication-date": "2013/01",
        "abstract": "We present and compare different notions of\nconformance testing based on labeled transition systems.\nWe formulate and prove several theorems which enable\nusing synchronous conformance testing techniques such as\ninput–output conformance testing (ioco) in order to test\nimplementationsonlyaccessiblethroughasynchronouscom-\nmunication channels. These theorems deﬁne when the syn-\nchronous test cases are sufﬁcient for checking all aspects of\nconformance that are observable by asynchronous interac-\ntion with the implementation under test.",
        "keywords": [
            "Conformance testing",
            "ioco",
            "Asynchronous conformance testing",
            "Queue context",
            "Internal choice implementation"
        ],
        "authors": [
            "Neda Noroozi",
            "Ramtin Khosravi",
            "Mohammad Reza Mousavi",
            "Tim A. C. Willemse"
        ],
        "file_path": "data/sosym-all/s10270-012-0302-8.pdf"
    },
    {
        "title": "Assessing the testing skills transfer of model-based testing on testing skill acquisition",
        "submission-date": "2023/04",
        "publication-date": "2024/01",
        "abstract": "When creating a software model, it is necessary that it accurately captures the desired behaviour, while at the same time ensuring that any undesired behaviour is excluded. On the one hand, formal veriﬁcation tools can be used to check the internal consistency of a software system, ensuring that the behaviour of one software component does not contradict another. On the other hand, software testing is essential to check the external validity of the model more comprehensively. Unfortunately, software testing is often overlooked in curricula, resulting in graduates with inadequate software testing skills for industry. Software testing tools such as TesCaV can be used to help teachers teach software testing topics in a non-intrusive and less time-consuming way. Previous research has shown that TesCaV is easy to use and that novice users produce better quality software tests when using TesCaV. However, it has remained unclear whether learners retain the skills they gain from using TesCaV even when the tool is not offered for help. In order to understand the positive effect of TesCaV on learners’ software testing skills, this study conducted an experiment with 45 participants. The experiment used a pretest-treatment-posttest design. The results show that participants feel equally conﬁdent about the completeness of their test coverage, even though they identify more test cases. It is concluded that for course design, a capsule such as TesCaV can help students to understand the full complexity of software testing and help them to be more systematic in their approach.",
        "keywords": [
            "Model-based testing",
            "Model-driven engineering",
            "TesCaV",
            "MERODE"
        ],
        "authors": [
            "Felix Cammaerts",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-023-01141-1.pdf"
    },
    {
        "title": "Correct-by-construction requirement decomposition",
        "submission-date": "2024/05",
        "publication-date": "2025/03",
        "abstract": "In systems engineering, accurately decomposing requirements is crucial for creating well-deﬁned and manageable system\ncomponents, particularly in safety-critical domains. Despite the critical need, rigorous, top-down methodologies for effectively\nbreaking down complex requirements into precise, actionable sub-requirements are scarce, especially compared to the wealth\nof bottom-up veriﬁcation techniques. Addressing this gap, we introduce a formal decomposition for contract-based design that\nguarantees the correctness of decomposed requirements if speciﬁc conditions are met. Our (semi-)automated methodology\naugments contract-based design with reachability analysis and constraint programming to systematically identify, verify, and\nvalidate sub-requirements representable by continuous bounded sets—continuous relations between real-valued inputs and\noutputs. We demonstrate the efﬁcacy and practicality of a correct-by-construction approach through a comprehensive case\nstudy on a cruise control system, highlighting how our methodology improves the interpretability, tractability, and veriﬁability\nof system requirements.",
        "keywords": [
            "Contract-based design",
            "Requirement decomposition",
            "Constraint programming"
        ],
        "authors": [
            "Minghui Sun",
            "Georgios Bakirtzis",
            "Hassan Jafarzadeh",
            "Cody Fleming"
        ],
        "file_path": "data/sosym-all/s10270-025-01291-4.pdf"
    },
    {
        "title": "Code generation by model transformation: a case study in transformation modularity",
        "submission-date": "2008/11",
        "publication-date": "2009/11",
        "abstract": "The realization of model-driven software development requires effective techniques for implementing code generators for domain-speciﬁc languages. This paper identifies techniques for improving separation of concerns in the implementation of generators. The core technique is code generation by model transformation, that is, the generation of a structured representation (model) of the target program instead of plain text. This approach enables the transformation of code after generation, which in turn enables the extension of the target language with features that allow better modularity in code generation rules. The technique can also be applied to ‘internal code generation’ for the translation of high-level extensions of a DSL to lower-level constructs within the same DSL using model-to-model transformations. This paper refines our earlier description of code generation by model transformation with an improved architecture for the composition of model-to-model normalization rules, solving the problem of combining type analysis and transformation. Instead of coarse-grained stages that alternate between normalization and type analysis, we have developed a new style of type analysis that can be integrated with normalizing transformations in a fine-grained manner. The normalization strategy has a simple extension interface and integrates non-local, context-sensitive transformation rules. We have applied the techniques in a realistic case study of domain-speciﬁc language engineering, i.e. the code generator for WebDSL, using Stratego, a high-level transformation language that integrates model-to-model, model-to-code, and code-to-code transformations.",
        "keywords": [
            "Transformation",
            "Transformation engineering",
            "Term rewriting",
            "Webapplication DSL",
            "Combination of analysis and transformation"
        ],
        "authors": [
            "Zef Hemel",
            "Lennart C. L. Kats",
            "Danny M. Groenewegen",
            "Eelco Visser"
        ],
        "file_path": "data/sosym-all/s10270-009-0136-1.pdf"
    },
    {
        "title": "Using DevOps toolchains in Agile model-driven engineering",
        "submission-date": "2021/11",
        "publication-date": "2022/05",
        "abstract": "For model-driven engineering (MDE) to become more Agile, the community needs to embrace development and operations (DevOps) practices. One of the core practices of DevOps is the use of pipelines to enable CI/CD to make teams more Agile and break down the barriers between development and operations with faster deployments. Current MDE tooling is not designed at its core to participate in DevOps pipelines. Consequently this makes the adoption of MDE in industry more difﬁcult. In this article, we cover an industrial experience report describing how we enabled our pipelines using DevOps and MDE.",
        "keywords": [
            "DevOps",
            "CI/CD",
            "Ant",
            "EMF",
            "Eclipse",
            "Agile",
            "Model-driven engineering",
            "MDE"
        ],
        "authors": [
            "Jörn Guy Süß",
            "Samantha Swift",
            "Eban Escott"
        ],
        "file_path": "data/sosym-all/s10270-022-01003-2.pdf"
    },
    {
        "title": "On the beneﬁts of ﬁle-level modularity for EMF models",
        "submission-date": "2018/12",
        "publication-date": "2020/06",
        "abstract": "Model-driven development (MDD) tools based on the Eclipse Modeling Framework (EMF) typically store all elements\nin a model in a single ﬁle which arguably is one of the main reasons why these tools do not scale well and cannot take\nadvantage of existing code versioning systems and other related facilities such as Git and Make. In this work, we describe an\napproach for storing models in multiple ﬁles. We argue that EMF-based MDD tools can beneﬁt signiﬁcantly from this ﬁle-level\nmodularity not only by improving the performance and scalability of basic model operations, but also by simplifying many\nmodel management activities through the use of existing code versioning systems and build automation tools. We introduce\na domain-speciﬁc language that allows deﬁning, at the metamodel level: (1) the mapping between models’ elements and\nthe ﬁle structure for model storage and (2) the dependencies between model elements that affect the code generation and\ncompilation (if the integration with code-based tools is required). Our suite then generates an API and scripts to provide\nsupport for ﬁle-level modularity and facilitate using code-based versioning and build tools. We have used our DSL in the\ncontext of Papyrus-RT, an MDD tool for real-time and embedded software, and show how ﬁle-level modularity can (1)\nsubstantially improve performance and scalability of load and save operations, (2) enable collaborative model development,\nand (3) facilitate MDD-speciﬁc activities such as model comparison and incremental code generation. Our implementation\nand the models used for evaluation are publicly available.",
        "keywords": [
            "Model management",
            "Model versioning",
            "Model comparison",
            "Incremental code generation",
            "Build automation",
            "UML-RT"
        ],
        "authors": [
            "Karim Jahed\nMojtaba Bagherzadeh\nJuergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-020-00804-7.pdf"
    },
    {
        "title": "ChronoSphere: a graph-based EMF model repository for IT landscape models",
        "submission-date": "2018/04",
        "publication-date": "2019/03",
        "abstract": "IT Landscape models are representing the real-world IT infrastructure of a company. They include hardware assets such as physical servers and storage media, as well as virtual components like clusters, virtual machines and applications. These models are a critical source of information in numerous tasks, including planning, error detection and impact analysis. The responsible stakeholders often struggle to keep such a large and densely connected model up-to-date due to its inherent size and complexity, as well as due to the lack of proper tool support. Even though modeling techniques are very suitable for this domain, existing tools do not offer the required features, scalability or ﬂexibility. In order to solve these challenges and meet the requirements that arise from this application domain, we combine domain-driven modeling concepts with scalable graph-based repository technology and a custom language for model-level queries. We analyze in detail how we synthesized these requirements from the application domain and how they relate to the features of our repository. We discuss the architecture of our solution which comprises the entire data management stack, including transactions, queries, versioned persistence and metamodel evolution. Finally, we evaluate our approach in a case study where our open-source repository implementation is employed in a production environment in an industrial context, as well as in a comparative benchmark with an existing state-of-the-art solution.",
        "keywords": [
            "Model-driven engineering",
            "Model repositories",
            "Versioning",
            "Graph database",
            "IT landscape"
        ],
        "authors": [
            "Martin Haeusler",
            "Thomas Trojer",
            "Johannes Kessler",
            "Matthias Farwick",
            "Emmanuel Nowakowski",
            "Ruth Breu"
        ],
        "file_path": "data/sosym-all/s10270-019-00725-0.pdf"
    },
    {
        "title": "Speciﬁcation and analysis of legal contracts with Symboleo",
        "submission-date": "2020/11",
        "publication-date": "2022/10",
        "abstract": "Legal contracts specify the terms and conditions—in essence, requirements—that apply to business transactions. This paper proposes a formal speciﬁcation language for legal contracts, called Symboleo, where contracts consist of collections of obligations and powers that deﬁne a legal contract’s compliant executions. Symboleo offers execution time operations such as subcontracting, assignment, and substitution. Its formal semantics is deﬁned in terms of logical axioms on statecharts that describe the lifetimes of contracts, obligations, and powers. We have implemented two tools to support the analysis of contract speciﬁcations. One is a conformance validation tool that enables checking that a speciﬁcation is consistent with the expectations of contracting parties. The other tool enables model-checking of desired contract properties, expressed in temporal logic. We envision Symboleo with its associated tools as enablers for the formal veriﬁcation of contracts to detect requirements-level issues. Our proposal includes an evaluation through the speciﬁcation of two real life-inspired contracts.",
        "keywords": [
            "Legal contracts",
            "Software requirements speciﬁcations",
            "Formal speciﬁcation languages",
            "Model checking",
            "nuXmv",
            "Smart contracts"
        ],
        "authors": [
            "Alireza Parvizimosaed",
            "Sepehr Shariﬁ",
            "Daniel Amyot",
            "Luigi Logrippo",
            "Marco Roveri",
            "Aidin Rasti",
            "Ali Roudak",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-022-01053-6.pdf"
    },
    {
        "title": "On the use of large language models in model-driven engineering",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Model-driven engineering (MDE) has seen signiﬁcant advancements with the integration of machine learning (ML) and deep learning techniques. Building upon the groundwork of previous investigations, our study provides a concise overview of current large language models (LLMs) applications in MDE, emphasizing their role in automating tasks like model repository classiﬁcation and developing advanced recommender systems. The paper also outlines the technical considerations for seamlessly integrating LLMs in MDE, offering a practical guide for researchers and practitioners. Looking forward, the paper proposes a focused research agenda for the future interplay of LLMs and MDE, identifying key challenges and opportunities. This concise roadmap envisions the deployment of LLM techniques to enhance the management, exploration, and evolution of modeling ecosystems. Moreover, we also discuss the adoption of LLMs in various domains by means of model-driven techniques and tools, i.e., MDE for supporting LLMs. By offering a compact exploration of LLMs in MDE, this paper contributes to the ongoing evolution of MDE practices, providing a forward-looking perspective on the transformative role of large language models in software engineering and model-driven practices.",
        "keywords": [
            "LLMs",
            "Generative AI",
            "Model-Driven Engineering"
        ],
        "authors": [
            "Juri Di Rocco\nDavide Di Ruscio\nClaudio Di Sipio\nPhuong T. Nguyen\nRiccardo Rubei"
        ],
        "file_path": "data/sosym-all/s10270-025-01263-8.pdf"
    },
    {
        "title": "Detection and resolution of conﬂicting change operations in version management of process models",
        "submission-date": "2011/03",
        "publication-date": "2011/12",
        "abstract": "Version management of process models requires\nthat different versions of process models are integrated by\napplying change operations. Conﬂict detection between indi-\nvidually applied change operations and conﬂict resolution\nsupport are integral parts of version management. For con-\nﬂict detection it is utterly important to compute a precise set\nof conﬂicts, since the minimization of the number of detected\nconﬂicts also reduces the overhead for merging different pro-\ncess model versions. As not every syntactic conﬂict leads to\na conﬂict when taking into account model semantics, a com-\nputation of conﬂicts solely on the syntax leads to an unnec-\nessary high number of conﬂicts. Moreover, even the set of\nprecisely computed conﬂicts can be extensive and their res-\nolution means a significant workload for a user. As a con-\nsequence, adequate support is required that guides a user\nthrough the resolution process and suggests possible reso-\nlution strategies for individual conﬂicts. In this paper, we\nintroduce the notion of syntactic and semantic conﬂicts for\nchange operations of process models. We provide a method\nhow to efﬁciently compute conﬂicts precisely, using a term\nformalization of process models and consider the subsequent\nresolution of the detected conﬂicts based on different strat-",
        "keywords": [
            "Business process model",
            "Versionmanagement",
            "Conﬂict detection",
            "Conﬂict resolution"
        ],
        "authors": [
            "Christian Gerth",
            "Jochen M. Küster",
            "Markus Luckey",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-011-0226-8.pdf"
    },
    {
        "title": "Automating the development of API-based generators using code idioms mining",
        "submission-date": "2024/03",
        "publication-date": "2025/04",
        "abstract": "API-Based Generators (ABGs) allow practitioners to achieve the advantages of Model-Driven Engineering (MDE) without\nmaking signiﬁcant changes to their current workﬂow or the architecture of their solution. However, using ABGs that are deﬁned\nin terms of the syntax rules of a target language can be cumbersome and impractical. Additionally, manually developing an\nABG based on higher-level concepts already used in an existing solution is labor-intensive and time-consuming. This paper\nintroduces a new approach called Semi-automatic API-based Generators Development (SAGED) to expedite the creation\nof ABGs customized for MDE development of existing solutions. SAGED accomplishes this by (i) providing insight into\ncode idioms that could be considered good candidates for code generation, as they frequently appear in the codebase, and\n(ii) automating the creation of a code generation API for an ABG, deﬁned in terms of the identiﬁed code idioms. The\nSAGED approach relies on mining code idioms from existing, unlabeled source code based on a machine learning technique\ncalled the nonparametric Bayesian Probabilistic Tree Substitution Grammar (PTSG). The main contributions of this paper\ninclude the introduction of the SAGED approach, an explanation and optimization of the Type-Based MCMC as a method for\napproximating the nonparametric Bayesian PTSG, and the development of an open-source inference core for implementing the\ninference method in different programming languages. Furthermore, the paper presents a solution for implementing SAGED\nin the C# programming language, along with case studies that demonstrate its effectiveness.",
        "keywords": [
            "API-Based Generators",
            "Machine Learning",
            "Code Idioms Mining"
        ],
        "authors": [
            "Nenad Todorovi´c",
            "Aleksandar Luki´c",
            "Nikola Todorovi´c",
            "Bojana Dragaš",
            "Gordana Milosavljevi´c"
        ],
        "file_path": "data/sosym-all/s10270-025-01296-z.pdf"
    },
    {
        "title": "Design for service compatibility\nBehavioural compatibility checking and diagnosis",
        "submission-date": "2011/03",
        "publication-date": "2012/01",
        "abstract": "Service composition is a recent ﬁeld that has seen\na ﬂurry of different approaches proposed towards the goal\nof ﬂexible distributed heterogeneous interoperation of soft-\nware systems, usually based on the expectation that such\nsystems must be derived from higher-level models rather\nthan be coded at low level. In practice, achieving service\ninteroperability nonetheless continues to require significant\nmodelling approach at multiple abstraction levels, and exist-\ning formal approaches typically require the analysis of the\nglobal space of joint executions of interacting services. Based\non our earlier work on providing locally checkable consis-\ntency rules for guaranteeing the behavioural consistency of\ninheritance hierarchies, a model-driven approach for creating\nconsistent service orchestrations is proposed. Service execu-\ntion and interaction is represented with a high-level model in\nterms of extended Petri net notation; formal criteria are pro-\nvided for service consistency that can be checked in terms of\nlocal model properties, and give a multi-step design approach\nfor developing services that are guaranteed to be interoper-\nable. Finally, it is outlined how the presented results can be\ncarried over and applied to modelling processes using the\nBusiness Process Modelling Notation (BPMN).",
        "keywords": [
            "Service compatibility",
            "Petri net",
            "BPMN",
            "Business process modelling",
            "Behavior diagrams",
            "Consistency rules"
        ],
        "authors": [
            "Georg Grossmann",
            "Michael Schreﬂ",
            "Markus Stumptner"
        ],
        "file_path": "data/sosym-all/s10270-012-0229-0.pdf"
    },
    {
        "title": "An empirical approach toward the resolution of conﬂicts in goal-oriented models",
        "submission-date": "2014/08",
        "publication-date": "2015/04",
        "abstract": "One signiﬁcant problem requirements engineers have to cope with is the management of unclear requirements, ambiguities, and conﬂicts that may arise between stakeholders. Such issues may be desirable since they may allow for further elicitation of requirements that would have been missed otherwise. Goal models capture the objectives and other intentions of different stakeholders, together with theirrelationships.Theycanbeusedtoreﬁneunclearrequire-ments and to detect conﬂicts and ambiguities early during model validation. However, resolving such ambiguities and conﬂicts is key for the successful implementation of the goal models. In this paper, we propose a novel approach to validate models in the Goal-oriented Requirement Language and resolve conﬂicts between the perspectives of intervening stakeholders (and especially between stakeholders of a given group). Our approach is based on a statistical analy-sis of empirical data that we collect from surveys designed for each group of stakeholders. We apply concept analysis in order to ﬁx goal-model artifacts that are subject to con-ﬂict. We illustrate our approach using a case study of a goal model describing the involvement of undergraduate students in university research activities.",
        "keywords": [
            "Goal model",
            "Conﬂict resolution",
            "GRL",
            "Statistical analysis",
            "Empirical analysis",
            "Concept analysis"
        ],
        "authors": [
            "Jameleddine Hassine",
            "Daniel Amyot"
        ],
        "file_path": "data/sosym-all/s10270-015-0460-6.pdf"
    },
    {
        "title": "A simple game-theoretic approach to checkonly QVT Relations",
        "submission-date": "2009/12",
        "publication-date": "2011/03",
        "abstract": "The QVT Relations (QVT-R) transformation language allows the definition of bidirectional model transformations, which are required in cases where two (or more) models must be kept consistent in the face of changes to either or both. A QVT-R transformation can be used either in check-only mode, to determine whether a target model is consistent with a given source model, or in enforce mode, to change the target model. A precise understanding of checkonly mode transformations is prerequisite to a precise understanding of enforce mode transformations, and this is the focus of this paper. In order to give semantics to checkonly QVT-R transformations, we need to consider the overall structure of the transformation as given by when and where clauses, and the role of trace classes. In the standard, the semantics of QVT-R are given both directly, and by means of a translation to QVT Core, a language which is intended to be simpler. In this paper, we argue that there are irreconcilable differences between the intended semantics of QVT-R and those of QVT Core, so that no translation from QVT-R to QVT Core can be semantics-preserving, and hence no such translation can be helpful in defining the semantics of QVT-R. Treating QVT-R directly, we propose a simple game-theoretic semantics. We demonstrate its behaviour on examples and show how it can be used to prove an example result comparing two QVT-R transformations. We demonstrate that consistent models may not possess a single trace model whose objects can be read as traceability links in either direction. We briefly discuss the effect of variations in the rules of the game, to elucidate some design choices available to the designers of the QVT-R language.",
        "keywords": [
            "Bidirectional model transformation",
            "QVT Relations",
            "QVT Core",
            "Games",
            "Semantics",
            "Consistency checking"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-011-0198-8.pdf"
    },
    {
        "title": "Modeling safety and airworthiness (RTCA DO-178B) information: conceptual model and UML proﬁle",
        "submission-date": "2009/01",
        "publication-date": "2010/06",
        "abstract": "Several safety-related standards exist for developing and certifying safety-critical systems. System safety assessments are common practice and system certiﬁcation according to a standard requires submitting relevant system safety information to appropriate authorities. The RTCA DO-178B standard is a software quality assurance, safety-related standard for the development of software aspects of aerospace systems. This research introduces an approach to improve communication and collaboration among safety engineers, software engineers, and certiﬁcation authorities in the context of RTCA DO-178B. This is achieved by utilizing a Uniﬁed Modeling Language (UML) proﬁle that allows software engineers to model safety-related concepts and properties in UML, the de facto software modeling standard. A conceptual meta-model is deﬁned based on RTCA DO-178B, and then a corresponding UML proﬁle, which we call Safe-UML, is designed to enable its precise modeling. We show how SafeUML improves communication by, for example, allowing monitoring implementation of safety requirements during the development process, and supporting system certiﬁcation per RTCA DO-178B. This is enabled through automatic generation of safety and certiﬁcation-related information from UML models. We validate this approach through a case study on developing an aircraft’s navigation controller subsystem.",
        "keywords": [
            "UML",
            "UML proﬁle",
            "Conceptual model",
            "Meta-model",
            "Airworthiness",
            "RTCA DO-178B",
            "Safety",
            "Safety-critical",
            "Safety assessment",
            "Certiﬁcation"
        ],
        "authors": [
            "Gregory Zoughbi",
            "Lionel Briand",
            "Yvan Labiche"
        ],
        "file_path": "data/sosym-all/s10270-010-0164-x.pdf"
    },
    {
        "title": "Using structural decomposition and reﬁnements for deep modeling of software architectures",
        "submission-date": "2017/11",
        "publication-date": "2018/10",
        "abstract": "Traditional metamodeling in two levels gets to its limits when model elements of a domain should be described as instances of other model elements. For example in architecture description languages, components may be instances of their component types. Although workarounds to model such instance relations between model elements exist, they require many validation constraintsandimplyacumbersomeinterface.Toobtainmoreelegantmetamodels,deepmodelingseekswaystorepresentnon-transitive instantiation chains directly. However, existing concepts cannot be applied in some situations we refer as composite instantiation patterns. Further, these concepts make existing technologies for model transformation and analysis obsolete as these languages have to be adapted. In this paper, we present an approach to realize deep modeling through structural decomposition and reﬁnements that can be implemented as a noninvasive extension to EMOF-like meta-metamodels. As a consequence, existing tools need not be adapted and composite instantiation patterns are fully supported. We validate our concept by creating a deep modeling architecture description language based on the Palladio Component Model and demonstrate its advantages by modeling a synthetic web application. We show that existing tools for incremental model analysis can be reused and manifest several orders of speedup for a synthetic example analysis.",
        "keywords": [
            "Deep modeling",
            "Structural decomposition",
            "Reﬁnements",
            "NMeta",
            "Modeling language"
        ],
        "authors": [
            "Georg Hinkel"
        ],
        "file_path": "data/sosym-all/s10270-018-0701-6.pdf"
    },
    {
        "title": "Correct development of real time and embedded systems",
        "submission-date": "2008/03",
        "publication-date": "2008/03",
        "abstract": "This issue is special in that for the ﬁrst time in the 7-year history of SoSyM, a special section is dedicated to the outcome of a major research project. The OMEGA project was funded by the European Union (EU) and brought together experts in the ﬁeld of embedded and real-time systems. A major result of the project is a UML proﬁle called “OMEGA UML proﬁle”. This proﬁle provides concepts and modeling constructs for specifying real-time constraints.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-008-0087-y.pdf"
    },
    {
        "title": "The RALph miner for automated discovery and veriﬁcation of resource-aware process models",
        "submission-date": "2018/12",
        "publication-date": "2020/08",
        "abstract": "Automated process discovery is a technique that extracts models of executed processes from event logs. Logs typically include information about the activities performed, their timestamps and the resources that were involved in their execution. Recent approaches to process discovery put a special emphasis on (human) resources, aiming at constructing resource-aware process models that contain the inferred resource assignment constraints. Such constraints can be complex and process discovery approaches so far have missed the opportunity to represent expressive resource assignments graphically together with process models. A subsequent veriﬁcation of the extracted resource-aware process models is required in order to check the proper utilisation of resources according to the resource assignments. So far, research on discovering resource-aware process models has assumed that models can be put into operation without modiﬁcation and checking. Integrating resource mining and resource-aware process model veriﬁcation faces the challenge that different types of resource assignment languages are used for each task. In this paper, we present an integrated solution that comprises (i) a resource mining technique that builds upon a highly expressive graphical notation for deﬁning resource assignments; and (ii) automated model-checking support to validate the discovered resource-aware process models. All the concepts reported in this paper have been implemented and evaluated in terms of feasibility and performance.",
        "keywords": [
            "Model checking",
            "Organisational mining",
            "Process mining",
            "Process veriﬁcation",
            "RALph",
            "Resource assignment",
            "Resource mining"
        ],
        "authors": [
            "Cristina Cabanillas",
            "Lars Ackermann",
            "Stefan Schönig",
            "Christian Sturm",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-020-00820-7.pdf"
    },
    {
        "title": "Stateful component-based performance models",
        "submission-date": "2012/02",
        "publication-date": "2013/04",
        "abstract": "The accuracy of performance-prediction models\nis crucial for widespread adoption of performance prediction\nin industry. One of the essential accuracy-inﬂuencing aspects\nofsoftwaresystemsisthedependenceofsystembehaviouron\na conﬁguration, context or history related state of the system,\ntypically reﬂected with a (persistent) system attribute. Even\nin the domain of component-based software engineering, the\npresence of state-reﬂecting attributes (the so-called inter-\nnal states) is a natural ingredient of the systems, implying\nthe existence of stateful services, stateful components and\nstateful systems as such. Currently, there is no consensus on\nthe deﬁnition or method to include state-related information\nin component-based prediction models. Besides the task to\nidentify and localise different types of stateful information\nacross component-based software architecture, the issue is\nto balance the expressiveness and complexity of prediction\nmodels via an effective abstraction of state modelling. In\nthis paper, we identify and classify stateful information in\ncomponent-based software systems, study the performance\nimpact of the individual state categories, and discuss the costs\nof their modelling in terms of the increased model size. The\nobservations are formulated into a set of heuristics-guiding\nsoftwareengineersinstatemodelling.Finally,practicaleffect\nof state modelling on software performance is evaluated on a\nreal-world case study, the SPECjms2007 Benchmark. The\nobserved deviation of measurements and predictions was\nsigniﬁcantly decreased by more precise models of stateful\ndependencies.",
        "keywords": [
            "Stateful components",
            "Performance prediction",
            "Prediction accuracy"
        ],
        "authors": [
            "Lucia Happe",
            "Barbora Buhnova",
            "Ralf Reussner"
        ],
        "file_path": "data/sosym-all/s10270-013-0336-6.pdf"
    },
    {
        "title": "The 2012 “State of the Journal” Report",
        "submission-date": "2013/01",
        "publication-date": "2013/01",
        "abstract": "SoSyM continues to experience an increasing number of submissions. In 2012, 308 new manuscripts were submitted, of which 49 % were reviewed for regular issues, while the other 51 % were submitted for special or theme sections. The average number of days from submission to a ﬁnal decision (that is, a ﬁnal accept or reject) was 136 days in 2012. This is 12 days less than the average in 2011. The number of months to online publication of an accepted paper has continued to drop as well, and is now between 3 and 4 weeks.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Geri Georg",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-012-0310-8.pdf"
    },
    {
        "title": "Rigorous identiﬁcation and encoding of trace-links in model-driven engineering",
        "submission-date": "2009/01",
        "publication-date": "2010/03",
        "abstract": "Model-driven engineering (MDE) involves the construction and manipulation of many models of different kinds in an engineering process. In principle, models can be used in the product engineering lifecycle in an end-to-end manner for representing requirements, designs and implementations, and assisting in deployment and maintenance. The manipulations applied to models may be manual, but they can also be automated—for example, using model transformations, code generation, and validation. To enhance automated analysis, consistency and coherence of models used in an MDE process, it is useful to identify, establish and maintain trace-links between models. However, the breadth and scope of trace-links that can be used in MDE is substantial, and managing trace-link information can be very complex. In this paper, we contribute to managing the complexity of traceability information in MDE in two ways: ﬁrstly, we demonstrate how to identify the different kinds of trace-links that may appear in an end-to-end MDE process; secondly, we describe a rigorous approach to deﬁning semantically rich trace-links between models, where the models themselves may be constructed using diverse modelling languages. The definition of rich trace-links allows us to use tools to maintain and analyse traceability relationships.",
        "keywords": [
            "Traceability",
            "Semantics",
            "Classiﬁcations",
            "Identiﬁcation"
        ],
        "authors": [
            "Richard F. Paige",
            "Nikolaos Drivalos",
            "Dimitrios S. Kolovos",
            "Kiran J. Fernandes",
            "Christopher Power",
            "Goran K. Olsen",
            "Steffen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-010-0158-8.pdf"
    },
    {
        "title": "Feature Nets: behavioural modelling of software product lines",
        "submission-date": "2013/12",
        "publication-date": "2015/06",
        "abstract": "Software product lines (SPLs) are diverse systems that are developed using a dual engineering process: (a) family engineering defines the commonality and variability among all members of the SPL, and (b) application engineering derives specific products based on the common foundation combined with a variable selection of features. The number of derivable products in an SPL can thus be exponential in the number of features. This inherent complexity poses two main challenges when it comes to modelling: firstly, the formalism used for modelling SPLs needs to be modular and scalable. Secondly, it should ensure that all products behave correctly by providing the ability to analyse and verify complex models efficiently. In this paper, we propose to integrate an established modelling formalism (Petri nets) with the domain of software product line engineering. To this end, we extend Petri nets to Feature Nets. While Petri nets provide a framework for formally modelling and verifying single software systems, Feature Nets offer the same sort of benefits for software product lines. We show how SPLs can be modelled in an incremental, modular fashion using Feature Nets, provide a Feature Nets variant that supports modelling dynamic SPLs, and propose an analysis method for SPL modelled as Feature Nets. By facilitating the construction of a single model that includes the various behaviours exhibited by the products in an SPL, we make a significant step towards efficient and practical quality assurance methods for software product lines.",
        "keywords": [
            "Behavioural modelling",
            "Software product lines",
            "Petri nets",
            "Variability"
        ],
        "authors": [
            "Radu Muschevici",
            "José Proença",
            "Dave Clarke"
        ],
        "file_path": "data/sosym-all/s10270-015-0475-z.pdf"
    },
    {
        "title": "Model-driven assessment of system dependability",
        "submission-date": "2007/03",
        "publication-date": "2008/03",
        "abstract": "Designers of complex real-time systems need to address dependability requirements early on in the development process. This paper presents a model-based approach that allows developers to analyse the dependability of use cases and to discover more reliable and safe ways of designing the interactions of the system with the environment. The hardware design and the dependability of the hardware to be used also needs to be considered. We use a probabilistic extension of statecharts to formally model the interaction requirements deﬁned in the use cases. The model is then evaluated analytically based on the success and failure probabilities of events. The analysis may lead to further reﬁnement of the use cases by introducing detection and recovery measures to ensure dependable system interaction. A visual modelling environment for our extended statecharts formalism support-ing automatic probability analysis has been implemented in AToM3, A Tool for Multi-formalism and Meta-Modelling. Our approach is illustrated with an elevator control system case study.",
        "keywords": [
            "Dependability",
            "Use cases",
            "Reliability",
            "Safety",
            "Requirements"
        ],
        "authors": [
            "Sadaf Mustaﬁz",
            "Ximeng Sun",
            "Jörg Kienzle",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-008-0084-1.pdf"
    },
    {
        "title": "BPMN-E2: a BPMN extension for an enhanced workﬂow description",
        "submission-date": "2016/08",
        "publication-date": "2018/03",
        "abstract": "This paper discusses a business process model and notation (BPMN) extension that includes new elements designed to improve its expressiveness. In previous works, different shortcomings concerning the BPMN language were detected. As a result, a set of requirements to overcome these issues was collected and used to guide this work. The proposed extension supports the representation of information commonly used by experts in the hazard analysis and critical control points domain, usually expressed in natural language, in a machine-understandable fashion. To take full advantage of the features introduced in this BPMN extension, tools such as ProM can be easily upgraded with appropriate plugins to support the new elements. In this line, an advanced conformance checking plugin was developed for process mining on BPMN models. A real-world example of use showing the beneﬁts of applying the new elements is also discussed. This proposal paves the way for novel advanced analysis mechanisms for traceability systems.",
        "keywords": [
            "BPMN",
            "HACCP",
            "Quality controls",
            "Conformance checking",
            "Process monitoring",
            "Process mining",
            "Enhanced expressiveness",
            "Time constraints",
            "Activity effect",
            "Parenteral nutrition"
        ],
        "authors": [
            "Mateo Ramos-Merino",
            "Juan M. Santos-Gago",
            "Luis M. Álvarez-Sabucedo",
            "Victor M. Alonso-Roris",
            "Javier Sanz-Valero"
        ],
        "file_path": "data/sosym-all/s10270-018-0669-2.pdf"
    },
    {
        "title": "Special Section of BPMDS 2022—reﬂections on interactions and responsibility in a digitized business processes ecosystem",
        "submission-date": "2024/10",
        "publication-date": "2024/11",
        "abstract": "The Business Process Modeling, Development and Support (BPMDS) working conference series has served as a meeting place for researchers and practitioners in business process modeling, development, and support since 1998. This special section follows the 23rd edition of the BPMDS series, organized in conjunction with CAISE 2022, which focused on the theme of ‘Reﬂections on human-human interaction and responsibility in a virtual environment.’",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-024-01240-7.pdf"
    },
    {
        "title": "Empirical assessment of two approaches for specifying software product line use case scenarios",
        "submission-date": "2014/10",
        "publication-date": "2015/05",
        "abstract": "Modularity beneﬁts, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those beneﬁts have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the beneﬁts of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The ﬁrst uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario speciﬁcations. After evaluating these approaches through the speciﬁcations of several systems, we ﬁnd out that MSVCM reduces feature scatteringandimprovesscenariocohesion.Theseresultssug- gest that evolving a product line speciﬁcation using MSVCM requiresonlylocalizedchanges.Ontheotherhand,theresults of six experiments reveal that MSVCM requires more time to derive the product line speciﬁcations and, contrasting with the modularity results, reduces the time to evolve a prod- uct line speciﬁcation only when the subjects have been well trained and are used to the task of evolving product line spec- iﬁcations.",
        "keywords": [
            "Usage scenarios",
            "Requirements engineering",
            "Software modularity",
            "Software product lines",
            "Experimentation in software engineering"
        ],
        "authors": [
            "Rodrigo Bonifácio",
            "Paulo Borba",
            "Cristiano Ferraz",
            "Paola Accioly"
        ],
        "file_path": "data/sosym-all/s10270-015-0471-3.pdf"
    },
    {
        "title": "Synthesis of test scenarios using UML activity diagrams",
        "submission-date": "2008/04",
        "publication-date": "2009/10",
        "abstract": "Often system developers follow Uniﬁed Modeling Language (UML) activity diagrams to depict all possible ﬂows of controls commonly known as scenarios of use cases. Hence, an activity diagram is treated as a useful design artifact to identify all possible scenarios and then check faults in scenarios of a use case. However, identiﬁcation of all possible scenarios and then testing with activity diagrams is a challenging task because several control ﬂow constructs and their nested combinations make path identiﬁcation difﬁcult. In this paper, we address this problem and propose an approach to identify all scenarios from activity diagrams and use them to test use cases. The proposed approach is based on the classiﬁcation of control constructs followed by a transformation approach which takes into account any combination of nested structures and transforms an activity diagram into a model called Intermediate Testable Model (ITM). We use ITM to generate test scenarios. With our approach it is possible to generate more scenarios than the existing work. Further, the proposed approach can be directly carried out using design models without any addition of testability information unlike the existing approaches.",
        "keywords": [
            "UML",
            "Software testing",
            "Model-based testing",
            "Activity diagram",
            "Test case generation"
        ],
        "authors": [
            "Ashalatha Nayak",
            "Debasis Samanta"
        ],
        "file_path": "data/sosym-all/s10270-009-0133-4.pdf"
    },
    {
        "title": "A framework for evaluating tool support for co-evolution of modeling languages, tools and models",
        "submission-date": "2024/02",
        "publication-date": "2024/10",
        "abstract": "We present a framework for evaluating language workbenches’ capabilities for co-evolution of graphical modeling languages, modeling tools and models. As with programming, maintenance tasks such as language reﬁnement and enhancement typically account for more work than the initial development phase. Modeling languages have the added challenge of keeping tools and existing models in step with the evolving language. As domain-speciﬁc modeling languages and tools have started to be used widely, thanks to reports of signiﬁcant productivity improvements, some language workbench users have indeed reported problems with co-evolution of tools and models. Our tool-agnostic evaluation framework aims to cover changes across the whole language deﬁnition: the abstract syntax, concrete syntax, and constraints. Change impact is assessed for knock-on effects within the language deﬁnition, the modeling tools, semantics via generators, and existing models. We demonstrate the viability of the framework by evaluating MetaEdit+, EMF/Sirius and Jjodel, providing a detailed evaluation process for others to repeat with their tools. The results of the evaluation show differences among the tools: from editors not opening correctly or at all, through highlighting of items requiring manual intervention, to fully automatic updates of languages, models and editors. We call for industry to evaluate their tool choices with the framework, tool developers to extend their tool support for co-evolution, and researchers to reﬁne the evaluation framework and evaluations presented.",
        "keywords": [
            "Domain-speciﬁc modeling",
            "Domain-speciﬁc language",
            "Evolution",
            "Maintenance",
            "Metamodel evolution",
            "Model evolution"
        ],
        "authors": [
            "Juha-Pekka Tolvanen",
            "Steven Kelly",
            "Juri Di Rocco",
            "Alfonso Pierantonio",
            "Giordano Tinella"
        ],
        "file_path": "data/sosym-all/s10270-024-01218-5.pdf"
    },
    {
        "title": "Analyzing the impact of human errors on interactive service robotic scenarios via formal veriﬁcation",
        "submission-date": "2022/06",
        "publication-date": "2023/08",
        "abstract": "Developing robotic applications with human–robot interaction for the service sector raises a plethora of challenges. In these settings, human behavior is essentially unconstrained as they can stray from the plan in numerous ways, constituting a critical source of uncertainty for the outcome of the robotic mission. Application designers require accessible and reliable frameworks to address this issue at an early development stage. We present a model-driven framework for developing interactive service robotic scenarios, allowing designers to model the interactive scenario, estimate its outcome, deploy the application, and smoothly reconﬁgure it. This article extends the framework compared to previous works by introducing an analysis of the impact of human errors on the mission’s outcome. The core of the framework is a formal model of the agents at play—the humans and the robots—and the robotic mission under analysis, which is subject to statistical model checking to estimate the mission’s outcome. The formal model incorporates a formalization of different human erroneous behaviors’ phenotypes, whose likelihood can be tuned while conﬁguring the scenario. Through scenarios inspired by the healthcare setting, the evaluation highlights how different conﬁgurations of erroneous behavior impact the veriﬁcation results and guide the designer toward the mission design that best suits their needs.",
        "keywords": [
            "Human",
            "robot interaction",
            "Human errors",
            "Service robotics",
            "Formal veriﬁcation",
            "Formal modeling",
            "Stochastic Hybrid Automata",
            "Statistical model checking"
        ],
        "authors": [
            "Livia Lestingi",
            "Andrea Manglaviti",
            "Davide Marinaro",
            "Luca Marinello",
            "Mehrnoosh Askarpour",
            "Marcello M. Bersani",
            "Matteo Rossi"
        ],
        "file_path": "data/sosym-all/s10270-023-01125-1.pdf"
    },
    {
        "title": "The concepts of Petri nets",
        "submission-date": "2014/01",
        "publication-date": "2014/08",
        "abstract": "“Everything is connected to everything else.” An attempt to\nfind the origins of this piece of lore with a search engine\nwill produce sources from esoterism, Buddhism, quantum\nmechanics and many more completely different areas. For\ncomputer scientists and engineers, this sentence is a chal-\nlenge. With only a few components, such a system is man-\nageable. However, if the system grows and each new com-\nponent is connected to each existing one, the number of con-\nnections grows quadratically. Such systems will soon be too\nlarge to manage. In fact, large technical systems are generally\nconstructed differently: They consist of subsystems (compo-\nnents), each of which is directly connected to only a few other\nsubsystems. In this way, too, everything can be connected\nto everything else, but many connections are only indirect,\nmediated by other subsystems. The currently most striking\nexample of such a connected system is probably the Internet.\nTo design a system, analyze it or present it to other people,\nit is modeled. Since the 1950s, primarily automata models\nhave been used to model the behavior of technical systems. A\nclassic automaton models a system as a monolithic block. A\nglobal state indicates the current condition of each element at\na given point in time. A step transforms one global state into\nanother global state. Such a global perception is unsuitable\nfor large systems with many components because the number\nof global states often grows exponentially with the number\nof elements. No one, for example, would even attempt to\nconceive of the Internet as an automaton, which proceeds in\na sequence of steps from one global state to the next. This is\nwhy modeling is often actually limited to individual system\ncomponents, while the interaction with other components is\ndiscussed elsewhere, or is even neglected.\nAlready in the early 1960s, these practices led C. A. Petri\nto look for an alternative to automata that would put the\nlocal interaction of components at the center of modeling\n[10]. It was a bold idea, in a time when informatics sys-\ntems were not yet that large. Today, the idea is not that sur-\nprising anymore because the complex connection structures\nbetween components have a much greater inﬂuence on the",
        "keywords": [
            "Modeling language",
            "Petri nets",
            "Carl Adam\nPetri",
            "Modularity",
            "Locality"
        ],
        "authors": [
            "Jörg Desel",
            "Wolfgang Reisig"
        ],
        "file_path": "data/sosym-all/s10270-014-0423-3.pdf"
    },
    {
        "title": "Modeling for sustainability: Sustainable Development Goals (SDG) of the United Nations",
        "submission-date": "2024/07",
        "publication-date": "2024/07",
        "abstract": "Our world has pressing problems. We are glad that the United Nations has undertaken the effort to develop a framework for deﬁning the most pressing Sustainable Development Goals (SDGs) and how they should be addressed. Their website (https://sdgs.un.org/goals) summarizes the 17 goals for sustainable development as follows: This list of SDGs was adopted by all United Nations Member States in 2015. It also embodies an urgent call for action by all countries to help end poverty and other deprivations, improve health and education, reduce inequality, spur economic growth, tackle climate change, and preserve our oceans and forests.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01196-8.pdf"
    },
    {
        "title": "Embedding domain-speciﬁc modelling languages in Maude speciﬁcations",
        "submission-date": "2011/01",
        "publication-date": "2012/02",
        "abstract": "We propose a formal approach for the defini-tion and analysis of domain-speciﬁc modelling languages (dsml). The approach uses standard model-driven engineering artifacts for deﬁning a language’s syntax (using metamodels) and its operational semantics (using model transformations). We give formal meanings to these artifacts by translating them to the Maude language: metamodels and models are mapped to equational speciﬁcations, and model transformations are mapped to rewrite rules between such speciﬁcations, which are also expressible in Maude due to Maude’s reﬂective capabilities. These mappings provide us, on the one hand, with abstract definitions of the mde concepts used for deﬁning dsml, which naturally capture their intended meanings; and, on the other hand, with equivalent executable definitions, which can be directly used by Maude for formal veriﬁcation. We also study a notion of operational semantics-preserving model transformations, which are model transformations between two dsml that ensure that each execution of a transformed instance is matched by an execution of the original instance. We propose a semi-decision procedure, implemented in Maude, for checking the semantics-preserving property. We also show how the procedure can be adapted for tracing ﬁnite executions of the transformed instance back to matching executions of the original one. The approach is illustrated on xspem, a language for describing the execution of activities constrained by time, precedence, and resource availability.",
        "keywords": [
            "Domain-speciﬁc languages",
            "Algebraic speciﬁcations",
            "Formal veriﬁcation",
            "Maude"
        ],
        "authors": [
            "Vlad Rusu"
        ],
        "file_path": "data/sosym-all/s10270-012-0232-5.pdf"
    },
    {
        "title": "Connecting software build with maintaining consistency between models: towards sound, optimal, and ﬂexible building from megamodels",
        "submission-date": "2019/07",
        "publication-date": "2020/03",
        "abstract": "Software build systems tackle the problem of building software from sources in a way which is sound (when a build completes successfully, the relations between the generated and source ﬁles are as speciﬁed) and optimal (only genuinely required rebuilding steps are done). In this paper, we explain and exploit the connection between software build and the megamodel consistency problem. The model-driven development of systems involves multiple models, metamodels and transformations. Transformations—which may be bidirectional—specify, and provide means to enforce, desired “consistency” relationships between models. We can describe the whole conﬁguration using a megamodel. As development proceeds, and various models are modiﬁed, we need to be able to restore consistency in the megamodel, so that the consequences of decisions ﬁrst recorded in one model are appropriately reﬂected in the others. At the same time, we need to minimise the amount of recomputation needed; in particular, we would like to avoid reapplying a transformation when no relevant changes have occurred in the models it relates. The megamodel consistency problem requires ﬂexibility beyond what is found in conventional software build, because different results are obtained depending on which models are allowed to be modiﬁed and on the order and directionoftransformationapplication.Inthispaper,weproposeusinganorientationmodeltomakeimportantchoicesexplicit. We show how to extend the formalised build system pluto to provide a means of restoring consistency in a megamodel, that is, in appropriate senses, ﬂexible, sound and optimal.",
        "keywords": [
            "Megamodel",
            "Build system",
            "Model transformation",
            "Bidirectionality",
            "Orientation model"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-020-00788-4.pdf"
    },
    {
        "title": "On enterprise coherence governance with GEA: a 15-year co-evolution of practice and theory",
        "submission-date": "2022/02",
        "publication-date": "2022/10",
        "abstract": "General Enterprise Architecting (GEA) is an enterprise architecture method which has emerged out of a need in practice, and has been developed and matured over the past 15 years. The GEA method differs from other enterprise architecture approaches in that it has a strong focus on enterprise coherence and the explicit governance thereof. This focus followed from the observed need to move beyond the Business-IT alignment and ‘Business-to-IT’ stack thinking that is embodied in most of the existing enterprise architecture approaches. The main objective of this paper is to report, and reﬂect on, the development of the GEA method (so-far), which involved a co-evolution between theory and practice. In doing so, we also present core elements of (the current version of) GEA, and illustrate these in terms of a real-world (social housing) case. We will, furthermore, also discuss some of the lessons learned in applying GEA across different organizations.",
        "keywords": [
            "Enterprise architecture"
        ],
        "authors": [
            "Henderik A. Proper",
            "Roel Wagter",
            "Joost Bekel"
        ],
        "file_path": "data/sosym-all/s10270-022-01059-0.pdf"
    },
    {
        "title": "An overview of model checking practices on veriﬁcation of PLC software",
        "submission-date": "2013/11",
        "publication-date": "2014/12",
        "abstract": "Programmablelogiccontrollers(PLCs)areheav-ily used in industrial control systems, because of their high capacity of simultaneous input/output processing capabili-ties. Characteristically, PLC systems are used in mission crit-ical systems, and PLC software needs to conform real-time constraints in order to work properly. Since PLC program-ming requires mastering low-level instructions or assembly like languages, an important step in PLC software produc-tion is modelling using a formal approach like Petri nets or automata. Afterward, PLC software is produced semiau-tomatically from the model and reﬁned iteratively. Model checking, on the other hand, is a well-known software veriﬁ-cation approach, where typically a set of timed properties are veriﬁed by exploring the transition system produced from the software model at hand. Naturally, model checking is applied in a variety of ways to verify the correctness of PLC-based software. In this paper, we provide a broad view about the difﬁculties that are encountered during the model checking process applied at the veriﬁcation phase of PLC software pro-duction. We classify the approaches from two different per-spectives: ﬁrst, the model checking approach/tool used in the veriﬁcation process, and second, the software model/source code and its transformation to model checker’s speciﬁcation language. In a nutshell, we have mainly examined SPIN, SMV, and UPPAAL-based model checking activities and model construction using Instruction Lists (and alike), Func-tion Block Diagrams, and Petri nets/automata-based model construction activities. As a result of our studies, we provide a comparison among the studies in the literature regarding various aspects like their application areas, performance con-siderations, and model checking processes. Our survey can be used to provide guidance for the scholars and practitioners planning to integrate model checking to PLC-based software veriﬁcation activities.",
        "keywords": [
            "Model checking",
            "Programmable logic controllers",
            "Program veriﬁcation"
        ],
        "authors": [
            "Tolga Ovatman",
            "Atakan Aral",
            "Davut Polat",
            "Ali Osman Ünver"
        ],
        "file_path": "data/sosym-all/s10270-014-0448-7.pdf"
    },
    {
        "title": "Modeling with Gentleman: a web-based projectional editor",
        "submission-date": "2023/11",
        "publication-date": "2024/10",
        "abstract": "Domain-speciﬁc modeling (DSM) makes software more accessible and inclusive, leveraging the expertise and knowledge of\nvarious experts instead of relying solely on technical experts. However, the widespread adoption of DSM is hampered due to poor tool support and modeling languages. In recent years, projectional editing has proven to be a promising approach to\ncreating and manipulating domain-speciﬁc languages. This editing approach contrasts with traditional parser-based editing by allowing user edits to directly modify the abstract syntax tree. It supports various notations, enabling more intuitive lan-\nguages and facilitating language extension and composition. However, current projectional editing solutions are heavyweight,\nplatform-speciﬁc, and hard to integrate with. We aim to provide better support for this paradigm and make modeling more accessible to domain experts. Thus, we present Gentleman, a lightweight web-based projectional editor. With Gentleman,\nusers can create models with simple structures and manipulate them with user-friendly projections. We evaluate Gentleman through a user study, demonstrating its ability to create and manipulate models effectively.",
        "keywords": [
            "Projectional editing",
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Language workbench"
        ],
        "authors": [
            "Louis-Edouard Lafontant\nEugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-024-01219-4.pdf"
    },
    {
        "title": "Modeling in advanced systems engineering",
        "submission-date": "2022/03",
        "publication-date": "2022/03",
        "abstract": "Previous editorials highlighted the use of models and the activity of modeling as an integral part of almost all development activities for complex systems. Complexity is an omnipresent challenge, for software, for mechanical systems with intelligence (e.g., autonomous cars, robots, healthcare gadgets or airplanes), for production plants, and also for biological systems (e.g., advanced medicine, human cells, organs, and even full organisms), that will be engineered with increased frequency in the future. “Systems of systems” are typically confederated and collaborating ensembles of systems that were originally designed individually, but through new opportunities that were not originally envisioned, are now required to tightly cooperate. Examples of such systems include telecommunications services, multiple integrated web services across the internet, energy networks and also the city networks of collaborating smart buildings and transportation infrastructure. \nAll of the examples listed previously need models. During development of these initially independent systems, models are often deﬁned before the system exists, requiring explicit modeling activities to explicate requirements on the system to document multiple design decisions and alternatives. This is an entirely different process from how models come to life when using data science techniques to extract models (e.g., of behavior or typical dynamic conﬁgurations) from observations of existing systems. These other kinds of extracted models are helpful when optimizing existing systems and processes, but are not intended for the original design (which may have occurred by nature). However, it is our belief that even though the process for model creation and use is differ- ent across various domains, the same underlying modeling paradigms and concepts can often be incorporated. As a consequence, these models could also be adopted in simi- lar modeling languages.",
        "keywords": [],
        "authors": [
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-00999-x.pdf"
    },
    {
        "title": "A method for transforming knowledge discovery metamodel to ArchiMate models",
        "submission-date": "2020/08",
        "publication-date": "2021/08",
        "abstract": "Enterprise architecture has become an important driver to facilitate digital transformation in companies, since it allows to manage IT and business in a holistic and integrated manner by establishing connections among technology concerns and strategical/motivational ones. Enterprise architecture modelling is critical to accurately represent business and their IT assets in combination. This modelling is important when companies start to manage their enterprise architecture, but also when it is remodelled so that the enterprise architecture is realigned in a changing world. Enterprise architecture is commonly modelled by few experts in a manual way, which is error-prone and time-consuming and makes continuous realignment difficult. In contrast, other enterprise architecture modelling proposal automatically analyses some artefacts like source code, databases, services, etc. Previous automated modelling proposals focus on the analysis of individual artefacts with isolated transformations toward ArchiMate or other enterprise architecture notations and/or frameworks. We propose the usage of Knowledge Discovery Metamodel (KDM) to represent all the intermediate information retrieved from information systems’ artefacts, which is then transformed into ArchiMate models. Thus, the core contribution of this paper is the model transformation between KDM and ArchiMate metamodels. The main implication of this proposal is that ArchiMate models are automatically generated from a common knowledge repository. Thereby, the relationships between different-nature artefacts can be exploited to get more complete and accurate enterprise architecture representations.",
        "keywords": [
            "Enterprise architecture",
            "ArchiMate",
            "Knowledge discovery metamodel",
            "Model transformation",
            "MDE",
            "ATL"
        ],
        "authors": [
            "Ricardo Pérez‑Castillo",
            "Andrea Delgado",
            "Francisco Ruiz",
            "Virginia Bacigalupe",
            "Mario Piattini"
        ],
        "file_path": "data/sosym-all/s10270-021-00912-y.pdf"
    },
    {
        "title": "On the adoption of blockchain for business process monitoring",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Being the blockchain and distributed ledger technologies particularly suitable to create trusted environments where participants do not trust each other, business process management represents a proper setting in which these technologies can be adopted. In this direction, current research work primarily focuses on blockchain-oriented business process design, or on execution engines able to enact processes through smart contracts. Conversely, less attention has been paid to study if and how blockchains can be beneﬁcial to business process monitoring. This work aims to ﬁll this gap by (1) providing a reference architecture for enabling the adoption of blockchain technologies in business process monitoring solutions, (2) deﬁning a set of relevant research challenges derived from this adoption, and (3) discussing the current approaches to address the aforementioned challenges.",
        "keywords": [
            "Blockchain",
            "Distributed ledger technology",
            "Business process management",
            "Software architectures",
            "Business process monitoring"
        ],
        "authors": [
            "Claudio Di Ciccio",
            "Giovanni Meroni",
            "Pierluigi Plebani"
        ],
        "file_path": "data/sosym-all/s10270-021-00959-x.pdf"
    },
    {
        "title": "Models as the subject of research",
        "submission-date": "2019/08",
        "publication-date": "2019/08",
        "abstract": "It is said that in the times before Plato, mathematics was only a vehicle to solve practical problems. Plato himself, however, identified mathematics as a pure subject of research, with the observation that the beauty of mathematical theories was of deep intrinsic value to be developed and refined independent of practical application. This spawned mathematics as a field of research, which also led to many more practical applications as a result.\nComputer science has a much younger history. Some early approaches to general-purpose programming (e.g., Charles Babbage’s mechanical general-purpose computer in the 1800s, the Analytical Engine, and Ada Lovelace’s first algorithms) are the most well-known examples. However, computer science (or better: informatics) really emerged in the twentieth century. Informatics started as a tool to help with other science and mathematics investigations. When the increasing complexity became clear, informatics became the target of its own research area with beautiful results in various subdomains.\nModeling, however, is much older. Conceptual models are omnipresent in philosophy, physics, chemistry, and many other inquiry-based disciplines. Physical manifestations of models were used frequently in building construction. The term “model” had an early historical connection to the construction of churches, where a model was a 1:10 reduced wooden version of the church to be built. From the great inventor Da Vinci, we still have many drawings that model surprising and smart machines—even though many of them have never been built. The authors of this editorial, however, are not aware of the topic of modeling as its own subject or research area in these earlier centuries.\nThe concept of a model needed more formalization when a specific semantics was required, such as modeling of systems and software. The most widely used definition of “model” was coined by Stachowiak only in 1973. Programming theory and semantics definitions needed a precisely defined notion of a well-formed piece of code. This notion also carried over to other forms of digitally communicated models. This includes Petri Nets in their various forms, automata and statecharts, class diagrams, action languages, and other forms of modeling languages. The formal methods domain (including logic) and the programming language domain (in particular, compiler construction) in informatics were the earliest to put some focus on the value of models and explicitly defined modeling languages. Software engineers and database administrators also relied upon models in various representations, focusing on the practical use of restricted forms of models. Interestingly, modeling tools have been a core product offered by commercial vendors since the beginning of the software industry. In fact, the first software product sold independently of a hardware package was Autoflow, which was a flowchart modeling tool developed in 1964 by Martin Goetz of Applied Data Research.\nThe use of models was widely discussed in the software engineering domain during the 1980s and 1990s, where a variety of different modeling languages (e.g., Booch’s clouds and Rumbaugh’s object models) were adopted. Around 1994, the overlap of common concepts across the various modeling languages pointed to the need for a unification of the different languages. The “methods wars”, which were intensely discussed at the OOPSLA conferences during the 1990s, were finally resolved into a standardization effort, first called the “Unified Method” and later leading to the “Unified Modeling Language” (UML). During this crucial period of unification, it became clear that defining such a standard would not be an easy task. A research community emerged that became interested in studying models and the UML as a core research subject area of its own.\nIn 1998, Jean Bézivin and Pierre-Alain Muller organized the first UML workshop, “The Unified Modeling Language. «UML»’98: Beyond the Notation” in Mulhouse, France,",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-019-00751-y.pdf"
    },
    {
        "title": "Multi-dimensional multi-level modeling",
        "submission-date": "2020/05",
        "publication-date": "2022/01",
        "abstract": "The growth of multi-level modeling has resulted in an increase of level-organization alternatives which signiﬁcantly differ from each other with respect to their underlying foundations and the well-formedness rules they enforce. Alternatives substantially divergewithrespect tohowlevel boundaries shouldgovern instance-of relationships, what modelingmechanisms theyemploy, and what modeling principles they establish. In this article, I analyze how a number of multi-level modeling approaches deal with certain advanced modeling scenarios. In particular, I identify linear domain metamodeling, i.e., the requirement that all domain-induced instance-of relationships align with a single global level-hierarchy, as a source of accidental complexity. I propose a novel multi-dimensional multi-level modeling approach based on the notion of orthogonal ontological classiﬁcation that supports modeling of domain scenarios with minimal complexity while supporting separation of concerns and sanity-checking to avoid inconsistent modeling choices.",
        "keywords": [
            "Multi-level modeling",
            "Level",
            "well-formedness",
            "Sanity-checking",
            "Ontological classiﬁcation",
            "Multi-dimensional"
        ],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-021-00951-5.pdf"
    },
    {
        "title": "Model-driven engineering with domain-speciﬁc meta-modelling languages",
        "submission-date": "2012/10",
        "publication-date": "2013/07",
        "abstract": "Domain-speciﬁc modelling languages are normally deﬁned through general-purpose meta-modelling languages like the MOF. While this is satisfactory for many model-driven engineering (MDE) projects, several researchers have identiﬁed the need for domain-speciﬁc meta-modelling (DSMM) languages. These provide customised domain-speciﬁc meta-modelling primitives aimed at the deﬁnition of modelling languages for a speciﬁc domain, as well as the construction of meta-model families. Unfortunately, current approaches to DSMM rely on ad hoc methods which add unnecessary complexity to the realization of DSMM in practice. Hence, the goal of this paper is to simplify the deﬁnition and usage of DSMM languages. For this purpose, we apply multi-level meta-modelling for the systematic engineering of DSMM architectures. Our method integrates techniques to control the meta-modelling primitives offered to the users of the DSMM languages, provides a ﬂexible approach to deﬁne textual concrete syntaxes for DSMM languages, and extends existing model management languages (for model-to-model transformation, in-place transformation and code generation) to work in a multi-level setting, thus enabling the practical use of DSMM in MDE. As a proof of concept, we report on a working implementation of these ideas in the MetaDepth tool.",
        "keywords": [
            "Model-driven engineering",
            "Multi-level meta-modelling",
            "Domain-speciﬁc meta-modelling",
            "Textual concrete syntax",
            "MetaDepth"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-013-0367-z.pdf"
    },
    {
        "title": "Broadened support for software and system model interchange",
        "submission-date": "2017/06",
        "publication-date": "2019/04",
        "abstract": "Although sound performance analysis theories and techniques exist, they are not widely used because they require extensive expertise in performance modeling and measurement. The overall goal of our work is to make performance modeling more accessible by automating much of the modeling effort. We have proposed a model interoperability framework that enables performance models to be automatically exchanged among modeling (and other) tools. The core of the framework is a set of model interchange formats (MIF): a common representation for data required by performance modeling tools. Our previous research developed a representation for system performance models (PMIF) and another for software performance models (S-PMIF), both based on the Queueing Network Modeling (QNM) paradigm. In order to manage the research scope and focus on model interoperability issues, the initial MIFs were limited to QNMs that can be solved by efﬁcient, exact solution algorithms. The overall model interoperability approach has now been demonstrated to be viable. This paper broadens the scope of PMIF and S-PMIF to represent models that can be solved with additional methods such as analytical approximations or simulation solutions. It presents the extensions considered, describes the extended meta-models, and provides veriﬁcation with examples and a case study.",
        "keywords": [
            "Performance modeling",
            "Interoperability",
            "Model interchange formats",
            "Software performance engineering (SPE)",
            "Queueing networks",
            "Software performance models"
        ],
        "authors": [
            "Catalina M. Lladó",
            "Connie U. Smith"
        ],
        "file_path": "data/sosym-all/s10270-019-00728-x.pdf"
    },
    {
        "title": "An executable object-oriented semantics and its application to ﬁrewall veriﬁcation",
        "submission-date": "2008/07",
        "publication-date": "2010/04",
        "abstract": "This paper presents a formal executable semantics of object-oriented models. We made it possible to conduct both simulation and theorem proving on the semantics by implementing it within the expressive intersection of the functional programming language ML and the theorem prover HOL. In this paper, we present the definition and implementation of the semantics. We also present a prototype veriﬁcation tool ObjectLogic which supports simulation and theorem proving on the semantics. As a case study, we show the veriﬁcation of a practical ﬁrewall system.",
        "keywords": [
            "Object-Oriented",
            "Theorem proving",
            "Simulation",
            "HOL",
            "ML"
        ],
        "authors": [
            "Kenro Yatake",
            "Takuya Katayama"
        ],
        "file_path": "data/sosym-all/s10270-010-0160-1.pdf"
    },
    {
        "title": "Evaluating the comprehension of means-ends maps",
        "submission-date": "2017/11",
        "publication-date": "2018/09",
        "abstract": "Information and software systems development is rapidly changing due to exponential technology development. This acceleration is also impacting other technology or engineering domains. Thus, there is a need to identify problems and their solutions, and to reason about new options so as to better arrive at the right decision of which technology or solution should be adopted among various alternatives. In this paper, we argue that such know-how information can be mapped, to ease such tasks. In particular, we examine the hypothesis that know-how mapping, using an approach we call Means-Ends Map (ME-MAP), facilitates analysis in technological domains. We design a controlled experiment to assess the comprehension of ME maps with that of textual summaries in two different domains. We find that subjects exploring a domain using ME maps were able to better identify solutions and better understand the tradeoffs among alternative solutions. Furthermore, these subjects gained that understanding faster compared to those using textual summaries.",
        "keywords": [
            "Knowledge mapping",
            "Literature review",
            "ME-MAP",
            "Evaluation",
            "Controlled experiment"
        ],
        "authors": [
            "Jumana Nassour",
            "Michael Elhadad",
            "Arnon Sturm",
            "Eric Yu"
        ],
        "file_path": "data/sosym-all/s10270-018-0691-4.pdf"
    },
    {
        "title": "Applying CSP ∥B to information systems",
        "submission-date": "2005/02",
        "publication-date": "2007/04",
        "abstract": "CSP ∥B is a formal approach which combines state and event-based descriptions of a system. It enables the automatic veriﬁcation of dynamic properties using model checking techniques. In this paper we identify a variation on the standard CSP ∥B architecture so that it is more appli-cable to support the speciﬁcation of information systems. We specify a library system using this new architecture. We examine several safety and liveness requirements and dem-onstrate that we can compositionally verify them using FDR. If a property fails to model check we identify an abstrac-tion technique which enables us to pinpoint the cause of the failure.",
        "keywords": [
            "CSP",
            "B",
            "Information systems",
            "Combining formalisms",
            "Compositional veriﬁcation"
        ],
        "authors": [
            "Neil Evans",
            "Helen Treharne",
            "Régine Laleau",
            "Marc Frappier"
        ],
        "file_path": "data/sosym-all/s10270-007-0048-x.pdf"
    },
    {
        "title": "Enhancing remaining time prediction in business processes by considering system-level and resource-level inter-case features",
        "submission-date": "2023/04",
        "publication-date": "2025/01",
        "abstract": "Accurately predicting the remaining time of running cases is crucial for effective scheduling in business processes. To achieve accurate predictions, it is essential to consider the conditions and behavior of organizational resources, which signiﬁcantly impact process completion times. Interactions between different cases can change resource conditions, so inter-case features created by these interactions should be considered in remaining time prediction. These inter-case features can be considered at both the system and resource levels. While past studies have largely focused on system-level features, they have often neglected the impact of resource-level features. This research investigates the effect of inter-case features on various prediction models by developing a conceptual framework that extracts open cases and resource multitasking as indicators of system and resource workloads, along with resource experience features, from event logs. Using four regression algorithms, four bucketing methods, and ﬁve encoding techniques, the study applies predictive process monitoring models to eight real-world datasets. The ﬁndings indicate that incorporating inter-case features generally improves prediction accuracy, particularly in processes with high resource and system workloads. However, the optimal model conﬁguration for the highest accuracy varies across datasets. Although inter-case features enhance prediction accuracy, they also increase both ofﬂine and online execution times. This study underscores the importance of considering both system and resource workloads for accurate remaining time prediction and provides a framework to guide the integration of inter-case features.",
        "keywords": [
            "Predictive process monitoring",
            "Remaining time prediction",
            "Inter-case features",
            "Open cases",
            "Resource multitasking"
        ],
        "authors": [
            "Reza Aalikhani",
            "Mohammad Fathian",
            "Mohammad Reza Rasouli"
        ],
        "file_path": "data/sosym-all/s10270-025-01267-4.pdf"
    },
    {
        "title": "Are models better read on paper or on screen? A comparative study",
        "submission-date": "2021/04",
        "publication-date": "2022/01",
        "abstract": "Is it really better to print everything, including software models, or is it better to view them on screen? With the ever\nincreasing complexity of software systems, software modeling is integral to software development. Software models facilitate\nand automate many activities during development, such as code and test case generation. However, a core goal of software\nmodeling is to communicate and collaborate. Software models are presented to team members on many mediums and two of\nthe most common mediums are paper and computer screens. Reading from paper or screen is ostensibly considered to have the\nsame effect on model comprehension. However, the literature on text reading has indicated that the reading experiences can\nbe very different which in turn effects various metrics related to reader performance. This paper reports on an experiment that\nwas conducted to investigate the effect of reading software models on paper in comparison with reading them on a computer\nscreen with respect to cognitive effectiveness. Cognitive effectiveness here refers to the ease by which a model reader can read\na model. The experiment used a total of 74 software engineering students as subjects. The experiment results provide strong\nevidence that displaying diagrams on a screen allows subjects to read them quicker. There is also evidence that indicates that\non screen viewing induces fewer reading errors.",
        "keywords": [
            "Paper-based reading",
            "Screen-based reading use case diagrams",
            "Feature diagrams",
            "Student-based experiments",
            "Controlled experiment",
            "Model comprehension",
            "Model representation"
        ],
        "authors": [
            "Mohamed El-Attar"
        ],
        "file_path": "data/sosym-all/s10270-021-00966-y.pdf"
    },
    {
        "title": "Linear parallel algorithms to compute strong and branching bisimilarity",
        "submission-date": "2022/04",
        "publication-date": "2022/12",
        "abstract": "We present the ﬁrst parallel algorithms that decide strong and branching bisimilarity in linear time. More precisely, if a transition system has n states, m transitions and |Act| action labels, we introduce an algorithm that decides strong bisimilarity in O(n + |Act|) time on max(n, m) processors and an algorithm that decides branching bisimilarity in O(n + |Act|) time using up to max(n2, m, |Act|n) processors.",
        "keywords": [
            "Strong bisimulation",
            "Branching bisimulation",
            "RCPP",
            "Parallel algorithms",
            "PRAM"
        ],
        "authors": [
            "Jan Martens",
            "Jan Friso Groote",
            "Lars B. van den Haak",
            "Pieter Hijma",
            "Anton Wijs"
        ],
        "file_path": "data/sosym-all/s10270-022-01060-7.pdf"
    },
    {
        "title": "Proﬁling the publish/subscribe paradigm for automated analysis using colored Petri nets",
        "submission-date": "2018/04",
        "publication-date": "2019/01",
        "abstract": "UML sequence diagrams are used to graphically describe the message interactions between the objects participating in a certain scenario. Combined fragments extend the basic functionality of UML sequence diagrams with control structures, such as sequences, alternatives, iterations, or parallels. In this paper, we present a UML proﬁle to annotate sequence diagrams with combined fragments to model timed Web services with distributed resources under the publish/subscribe paradigm. This proﬁle is exploited to automatically obtain a representation of the system based on Colored Petri nets using a novel model-to-model (M2M) transformation. This M2M transformation has been speciﬁed using QVT and has been integrated in a new add-on extending a state-of-the-art UML modeling tool. Generated Petri nets can be immediately used in well-known Petri net software, such as CPN Tools, to analyze the system behavior. Hence, our model-to-model transformation tool allows for simulating the system and ﬁnding design errors in early stages of system development, which enables us to ﬁx them at these early phases and thus potentially saving development costs.",
        "keywords": [
            "UML 2.5",
            "Distributed resources",
            "Publish/Subscribe",
            "Automated analysis",
            "WSRF",
            "WSN",
            "Colored Petri nets",
            "CPN tools"
        ],
        "authors": [
            "Abel Gómez",
            "Ricardo J. Rodríguez",
            "María-Emilia Cambronero",
            "Valentín Valero"
        ],
        "file_path": "data/sosym-all/s10270-019-00716-1.pdf"
    },
    {
        "title": "Remarks on Egon Börger: “Approaches to model business processes: a critical analysis of BPMN, workﬂow patterns and YAWL, SOSYM 11:305–318”",
        "submission-date": "2012/10",
        "publication-date": "2013/01",
        "abstract": "Egon Börger (SOSYM, 11, pp. 305–318, 2012) challenges the concepts of BPMN, workﬂow patterns and YAWL as useful contributions to the modeling of business processes.IshowthathemisjudgestheroleofBPMN,YAWL and similar techniques in the modeling of business processes. In particular he mistakes YAWL’s formal basis, i.e. Petri nets. Börger furthermore suggests evaluation criteria for business process modeling tools. I argue that his criteria overemphasize some less important aspects, while ignoring some decisive ones.",
        "keywords": [
            "Business process modelling",
            "BPMN",
            "YAWL",
            "Petri nets",
            "Evaluation criteria for tools"
        ],
        "authors": [
            "Wolfgang Reisig"
        ],
        "file_path": "data/sosym-all/s10270-012-0306-4.pdf"
    },
    {
        "title": "Specifying business rules in object-oriented analysis",
        "submission-date": "2003/12",
        "publication-date": "2004/08",
        "abstract": "A major purpose of analysis is to represent precisely all relevant facts, as they are observed in the external world. A substantial problem in object-oriented analysis is that most modelling languages are more suit- able to build computational models than to develop con- ceptual models. It is a rather blind assumption that con- cepts that are convenient for design can also be applied during analysis. Preconditions, postconditions and in- variants are typical examples of concepts with blurred se- mantics. At the level of analysis they are most often used to specify business rules. This paper introduces proper concepts for modelling business rules and speciﬁes their semantics.",
        "keywords": [
            "Object-oriented analysis",
            "Constraints",
            "Business rules",
            "Uniﬁed modeling language",
            "Object Constraint Language"
        ],
        "authors": [
            "Frank Devos",
            "Eric Steegmans"
        ],
        "file_path": "data/sosym-all/s10270-004-0064-z.pdf"
    },
    {
        "title": "A model-driven development approach for context-aware systems",
        "submission-date": "2015/04",
        "publication-date": "2016/10",
        "abstract": "The widespread usage of various types of computer devices with different platform characteristics created a need for new methods and tools to support the development of context-aware applications capable of dynamically adapting themselves to context changes. In this paper, we present a new model-based approach that addresses the development of context-aware applications from both the theoretical and practical perspectives and that supports all development phases of context-aware systems. On the one hand, we describe how our approach is applied to dynamically capture, observe the change of the context and notify the system at runtime. On the other hand, we show how our approach is used by programmers to develop a context- aware application.",
        "keywords": [
            "Context modeling",
            "Application adaptation",
            "Context-aware application development",
            "Model-driven development"
        ],
        "authors": [
            "Imen Jaouadi",
            "Raoudha Ben Djemaa",
            "Hanêne Ben-Abdallah"
        ],
        "file_path": "data/sosym-all/s10270-016-0550-0.pdf"
    },
    {
        "title": "Using DAG transformations to verify Euler/Venn homogeneous and Euler/Venn FOL heterogeneous rules of inference",
        "submission-date": "2003/01",
        "publication-date": "2004/04",
        "abstract": "In this paper we will present a graph-transformation based method for the veriﬁcation of heterogeneous ﬁrst order logic (FOL) and Euler/Venn proofs. In previous work, it has been shown that a special collection of directed acyclic graphs (DAGs) can be used interchangeably with Euler/Venn diagrams in reasoning processes. Thus, proofs which include Euler/Venn diagrams can be thought of as proofs with DAGs where steps involving only Euler/Venn diagrams can be treated as particular DAG transformations. Here we will show how the characterization of these manipulations can be used to verify Euler/Venn proofs. Also, a method for verifying the use of heterogeneous Euler/Venn and FOL reasoning rules will be presented that is also based upon DAG transformations.",
        "keywords": [
            "Euler and Venn diagrams",
            "Diagrammatic reasoning",
            "Graph transformation",
            "Proof veriﬁcation"
        ],
        "authors": [
            "Nik Swoboda",
            "Gerard Allwein"
        ],
        "file_path": "data/sosym-all/s10270-003-0044-8.pdf"
    },
    {
        "title": "In search of eﬀective design abstractions",
        "submission-date": "2004/02",
        "publication-date": "2004/02",
        "abstract": "The perceived popularity of the modeling languages such as the UML may lead some to believe that there is wide-spread appreciation of the value of modeling in the software development industry. Informal polls in North American trade journals seem to indicate otherwise. Use of the UML seems to be limited to the use of use cases for requirements and class diagrams for graphically representing programs. The polls also seem to indicate that awareness of the Object Management Group’s Model Driven Architecture (MDA) is not widespread. As MDA is currently the most widespread model-based development approach this strongly indicates that much needs to be done to convince practicing developers of the value of model-driven development approaches in general. But, we expect to see in 2004 a surge in modeling tools in particular those that claim to be “MDA-Compliant”. Tool support is essential to realizing the vision of model-driven development, but use of tools without a solid understanding of the principles and methods they support can lead to failed projects and faulty perceptions of model-driven development. In this editorial we reﬂect on the value of the abstraction principle in software development and on how modeling approaches can support this principle.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-004-0052-3.pdf"
    },
    {
        "title": "Ceaml: A novel modeling language for enabling cloud and edge continuum orchestration",
        "submission-date": "2023/10",
        "publication-date": "2024/11",
        "abstract": "Cloud modeling languages (CMLs) are utilized to depict and describe Cloud infrastructures and services in a manner that is easily comprehensible and manipulable by Cloud orchestration systems. These languages play a pivotal role in the domain of Cloud and Edge computing, as they enable the deployment of applications and services that can be dynamically scaled and adapted to evolving workloads. In this paper, a novel CML is presented that encompasses the ability to represent runtime adaptation through an event-driven syntax that relies on Quality of Experience conditions. In addition to introducing the modeling language, this paper also explains how it was applied in a system to deploy and adapt applications within Kubernetes at runtime.",
        "keywords": [
            "Cloud modeling language",
            "Application model",
            "Cloud",
            "Edge",
            "Orchestration",
            "Runtime adaptation",
            "Workflows"
        ],
        "authors": [
            "Ioannis Korontanis",
            "Antonios Makris",
            "Konstantinos Tserpes"
        ],
        "file_path": "data/sosym-all/s10270-024-01222-9.pdf"
    },
    {
        "title": "The uncertainty interaction problem in self-adaptive systems",
        "submission-date": "2022/04",
        "publication-date": "2022/08",
        "abstract": "The problem of mitigating uncertainty in self-adaptation has driven much of the research proposed in the area of software engineering for self-adaptive systems in the last decade. Although many solutions have already been proposed, most of them tend to tackle speciﬁc types, sources, and dimensions of uncertainty (e.g., in goals, resources, adaptation functions) in isolation. A special concern are the aspects associated with uncertainty modeling in an integrated fashion. Different uncertainties are rarely independent and often compound, affecting the satisfaction of goals and other system properties in subtle and often unpredictable ways. Hence, there is still limited understanding about the speciﬁc ways in which uncertainties from various sources interact and ultimately affect the properties of self-adaptive, software-intensive systems. In this SoSym expert voice, we introduce the Uncertainty Interaction Problem as a way to better qualify the scope of the challenges with respect to representing different types of uncertainty while capturing their interaction in models employed to reason about self-adaptation. We contribute a characterization of the problem and discuss its relevance in the context of case studies taken from two representative application domains. We posit that the Uncertainty Interaction Problem should drive future research in software engineering for autonomous and self-adaptive systems, and therefore, contribute to evolving uncertainty modeling towards holistic approaches that would enable the construction of more resilient self-adaptive systems.",
        "keywords": [
            "Uncertainty",
            "Modeling",
            "Self-adaptation",
            "Assurances"
        ],
        "authors": [
            "Javier Cámara",
            "Javier Troya",
            "Antonio Vallecillo",
            "Nelly Bencomo",
            "Radu Calinescu",
            "Betty H. C. Cheng",
            "David Garlan",
            "Bradley Schmerl"
        ],
        "file_path": "data/sosym-all/s10270-022-01037-6.pdf"
    },
    {
        "title": "Modelling in low-code development: a multi-vocal systematic review",
        "submission-date": "2021/07",
        "publication-date": "2022/01",
        "abstract": "In 2014, a new software development approach started to get a foothold: low-code development. Already from its early\ndays, practitioners in software engineering have been showing a rapidly growing interest in low-code development. In 2021\nonly, the revenue of low-code development technologies reached 13.8 billion USD. Moreover, the business success of low-\ncode development has been sided by a growing interest from the software engineering research community. The model-driven\nengineering community has shown a particular interest in low-code development due to certain similarities between the two. In\nthis article, we report on the planning, execution, and results of a multi-vocal systematic review on low-code development, with\nspecial focus to its relation to model-driven engineering. The review is intended to provide a structured and comprehensive\nsnapshot of low-code development in its peak of inﬂated expectations technology adoption phase. From an initial set of\npotentially relevant 720 peer-reviewed publications and 199 grey literature sources, we selected 58 primary studies, which\nwe analysed according to a meticulous data extraction, analysis, and synthesis process. Based on our results, we tend to frame\nlow-code development as a set of methods and/or tools in the context of a broader methodology, often being identiﬁed as\nmodel-driven engineering.",
        "keywords": [
            "Low-code development",
            "Modelling",
            "Model-driven engineering",
            "Systematic review"
        ],
        "authors": [
            "Alessio Bucaioni",
            "Antonio Cicchetti",
            "Federico Ciccozzi"
        ],
        "file_path": "data/sosym-all/s10270-021-00964-0.pdf"
    },
    {
        "title": "Correctly defined concrete syntax",
        "submission-date": "2007/03",
        "publication-date": "2008/07",
        "abstract": "Due to their complexity, the syntax of modern modeling languages is preferably defined in two steps. The abstract syntax identifies all modeling concepts whereas the concrete syntax should clarify how these concepts are rendered by graphical and/or textual elements. While the abstract syntax is often defined in form of a metamodel, there does not exist such standard format yet for concrete syntax definitions. The diversity of definition formats—ranging from EBNF grammars to informal text—is becoming a major obstacle for advances in modeling language engineering, including the automatic generation of editors. In this paper, we propose a uniform format for concrete syntax definitions. Our approach captures both textual and graphical model representations and even allows to assign more than one rendering to the same modeling concept. Consequently, following our approach, a model can have multiple, fully equivalent representations, but—in order to avoid ambiguities when reading a model representation—two different models should always have distinguishable representations. We call a syntax definition correct, if all well-formed models are represented in a non-ambiguous way. As the main contribution of this paper, we present a rigorous analysis technique to check the correctness of concrete syntax definitions.",
        "keywords": [
            "Visual languages",
            "Concrete syntax",
            "Metamodeling",
            "OCL",
            "Triple-Graph-Grammars (TGGs)"
        ],
        "authors": [
            "Thomas Baar"
        ],
        "file_path": "data/sosym-all/s10270-008-0086-z.pdf"
    },
    {
        "title": "Run-time threat models for systematic and continuous risk assessment",
        "submission-date": "2023/09",
        "publication-date": "2024/12",
        "abstract": "Threat modeling involves systematically assessing the likelihood and potential impact of diverse security threat scenarios. Existing threat modeling approaches and tools act at the level of a software architecture or design (e.g., a data ﬂow diagram), at the level of abstract system elements. These approaches, however, do not allow more in-depth analysis that takes into account concrete instances and conﬁgurations of these elements. This lack of expressiveness—as threats that require articulation at the level of instances cannot be expressed nor managed properly—hinders systematic risk calculation—as risks cannot be expressed and estimated in terms of instance-level properties. In this paper, we present a novel threat modeling approach that supports modeling complex systems at two distinct levels: (i) the design model deﬁnes the classes and entity types in the system, and (ii) the instance model speciﬁes concrete instances and their properties. This innovation allows systematically calculating broader risk estimates at the design level, yet also performing more reﬁned analysis in terms of more precise risk values at the instance level. Moreover, the ability to assess instance-level risks serves as an enabler for run-time continuous threat and risk (re-)assessment, and risk-adaptive security in general. We evaluate this approach in a prototype and through simulation of the dynamics of a realistic IoT-based system, a smart trafﬁc application that involves vehicles and other infrastructural elements such as smart trafﬁc lights. In these efforts, we demonstrate the practical feasibility of the approach, and we quantify the performance cost of maintaining a threat model at run-time, taking into account the time to perform risk assessment.",
        "keywords": [
            "Threat modeling",
            "risk assessment",
            "digital twin",
            "security-by-design"
        ],
        "authors": [
            "Stef Verreydt",
            "Dimitri Van Landuyt",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-024-01242-5.pdf"
    },
    {
        "title": "Formal detection of feature interactions with logic programming and LOTOS",
        "submission-date": "2005/12",
        "publication-date": "2005/12",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Nicolas Gorse",
            "Luigi Logrippo",
            "Jacques Sincennes"
        ],
        "file_path": "data/sosym-all/s10270-005-0104-3.pdf"
    },
    {
        "title": "MOOGLE: a metamodel-based model search engine",
        "submission-date": "2009/06",
        "publication-date": "2010/07",
        "abstract": "Models are becoming increasingly important in the software development process. As a consequence, the number of models being used is increasing, and so is the need for efﬁcient mechanisms to search them. Various existing search engines could be used for this purpose, but they lack features to properly search models, mainly because they are strongly focused on text-based search. This paper presents Moogle,amodelsearchenginethatusesmetamodelinginfor- mation to create richer search indexes and to allow more complex queries to be performed. The paper also presents the results of an evaluation of Moogle, which showed that the metamodel information improves the accuracy of the search.",
        "keywords": [
            "Model-driven development",
            "Model search",
            "Model reuse"
        ],
        "authors": [
            "Daniel Lucrédio",
            "Renata P. de M. Fortes",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-010-0167-7.pdf"
    },
    {
        "title": "Multi-objective exploration of architectural designs by composition of model transformations",
        "submission-date": "2016/03",
        "publication-date": "2017/01",
        "abstract": "Designing software architectures and optimizing them based on extra-functional properties (EFPs) require to identify appropriate design decisions and to apply them on valid architectural elements. Software designers have to check whether the resulting architecture fulﬁlls the requirements and how it positively improves (possibly con-ﬂicting) EFPs. In practice, they apply well-known solu-tions such as design patterns manually. This is time-consuming, error-prone, and possibly sub-optimal. Well-established approaches automate the search of the design space for an optimal solution. They are based model-driven engineering techniques that formalized design decisions as model transformations and architectural elements as compo-nents. Using multi-objective optimizations techniques, they explore the design space by randomly selecting a set of com-ponents and applying to them variation operators that include a ﬁxed set of predeﬁned design decisions. In this work, we claim that the design space exploration requires to reason on both architectural components as well as model transforma-tions. More speciﬁcally, we focus on possible instantiations of model transformations materialized as the application of model transformation alternatives on a set of architectural components. This approach was prototyped in RAMSES, a model transformation and code generation framework. Experimental results show the capability of our approach (i) to combine evolutionary algorithms and model transforma-tion techniques to explore efﬁciently a set of architectural alternatives with conﬂicting EFPs, (ii) to instantiate, and select transformation instances that generate architectures satisfying stringent structural constraints, and (iii) to explore design spaces by chaining more than one transformation. In particular, we evaluated our approach on EFPs, architectures, and design alternatives inspired from the railway industry by chaining model transformations dedicated to implement safety design patterns and software components allocation on a multi-processor hardware platform.",
        "keywords": [
            "Component-based software engineering",
            "Model transformations composition",
            "Design space exploration",
            "Rule-based transformation languages",
            "AADL models",
            "Extra-functional properties",
            "Multiple objectives evolutionary algorithms",
            "NSGA-II",
            "SAT solvers",
            "Linear programming"
        ],
        "authors": [
            "Smail Rahmoun",
            "Asma Mehiaoui-Hamitou",
            "Etienne Borde",
            "Laurent Pautet",
            "Elie Soubiran"
        ],
        "file_path": "data/sosym-all/s10270-017-0580-2.pdf"
    },
    {
        "title": "A process mining-based analysis of business process work-arounds",
        "submission-date": "2013/10",
        "publication-date": "2014/06",
        "abstract": "Business process work-arounds are speciﬁc\nforms of incompliant behavior, where employees inten-\ntionally decide to deviate from the required procedures\nalthough they are aware of them. Detecting and understand-\ning the work-arounds performed can guide organizations in\nredesigning and improving their processes and support sys-\ntems. Existing process mining techniques for compliance\nchecking and diagnosis of incompliant behavior rely on the\navailable information in event logs and emphasize techno-\nlogical capabilities for analyzing this information. They do\nnot distinguish intentional incompliance and do not address\nthe sources of this behavior. In contrast, the paper builds on\na list of generic types of work-arounds found in practice and\nexplores whether and how they can be detected by process\nmining techniques. Results obtained for four work-around\ntypes in ﬁve real-life processes are reported. The remaining\ntwo types are not reﬂected in events logs and cannot be cur-\nrently detected by process mining. The detected work-around\ndata are further analyzed for identifying correlations between\nthe frequency of speciﬁc work-around types and properties of\nthe processes and of speciﬁc activities. The analysis results\npromote the understanding of work-around situations and\nsources.",
        "keywords": [
            "Business process work-arounds",
            "Process mining",
            "Compliance checking"
        ],
        "authors": [
            "Nesi Outmazgin",
            "Pnina Soffer"
        ],
        "file_path": "data/sosym-all/s10270-014-0420-6.pdf"
    },
    {
        "title": "Automatic security-ﬂaw detection - towards a fair evaluation and comparison",
        "submission-date": "2024/05",
        "publication-date": "2025/05",
        "abstract": "Threat Modeling is an essential step in secure software system development. It is a (so far) manual, attacker-centric approach for identifying architecture-level security ﬂaws during the planning phase of software systems. In recent years, academia has presented ideas to automate threat detection that do not focus on a particular class of security ﬂaws but offer means of pattern-based security ﬂaw descriptions. However, comparing presented ideas (tools) for automated threat detection contains the potential for unwilling bias or restricted information content. In this work, we investigate the process of comparing automatic security ﬂaw detection tools, clarify common pitfalls during this process, and propose a fair, reproducible, and informative comparison approach to be used as a community standard. We additionally discuss the necessary steps for the community to effectively implement this approach and support improved comparisons and evaluations in the future. We use a previously published case study to determine problems with current comparison techniques and classify different levels of comparison to be used for future reference as our main contribution. As a consequence, we propose using a model-based approach for specifying security ﬂaws and apply an existing natural language-based catalogue to this model-based approach. Furthermore, we introduce an inspection process model (for providing a standard to specify ﬁndings of a threat detection process) to streamline the evaluation and comparisons of automatic security ﬂaw detection tools. We provide an exemplary evaluation of this detection guideline and inspection process model along the lines of both automatic approaches from the original case study. All artefacts of the work are publicly available to support the research community and to create a common baseline for future tool comparisons.",
        "keywords": [
            "Threat modeling",
            "Dataﬂow diagrams",
            "Security ﬂaw detection",
            "Automation",
            "Interoperability",
            "Comparison"
        ],
        "authors": [
            "Bernhard J. Berger",
            "Christina Plump"
        ],
        "file_path": "data/sosym-all/s10270-025-01300-6.pdf"
    },
    {
        "title": "Six years of modeling in SoSyM",
        "submission-date": "2007/11",
        "publication-date": "2007/11",
        "abstract": "Already 6 years? Wow. Time is passing quickly. Since we began, the International Journal on Software and Systems Modeling (SoSyM) is now a well established journal with many quality papers communicating research and experience related to building and using models in the development of software-based systems. Time to rethink the form of publication and what can be improved—Feedback on the Journal as well as our considerations is welcome.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-007-0070-z.pdf"
    },
    {
        "title": "An efﬁcient line-based approach for resolving merge conﬂicts in XMI-based models",
        "submission-date": "2021/07",
        "publication-date": "2022/03",
        "abstract": "Conﬂicts in software artefacts can appear during collaborative development through version control systems. When these\nconﬂicts happen in XMI models, the conﬂict sections generated by diff programs break the XMI serialisation and compromise\nthe ability to use model editors that assume well-formedness of this serialisation. While these conﬂict sections already mark\nthe conﬂicting lines of the model, current tools for conﬂict resolution in models ignore them and instead load the different\nversions of a model from the repository, over which they perform a full and costly comparison that re-identiﬁes the conﬂicts.\nWe present a novel approach that prevents this repetition of work by directly parsing XMI-based models with conﬂict sections,\nwhich allows for a targeted analysis of only the lines of the model that have been detected to be in conﬂict by the version control\nsystem. We have implemented this approach in the Peacemaker tool, which can load XMI models with conﬂict sections,\ncompute and display conﬂicts at the model level, and provide appropriate resolution actions. Compared with state-of-the-art\nmodel comparison tools with support for conﬂict resolution, Peacemaker is able to identify the vast majority of conﬂicts\nin models while reducing the required time by up to 60%. The small subset of non-identiﬁed conﬂicts does not introduce\nissues into the models, e.g. there is no loss of model information, and the resulting models after line-merging these conﬂicts\nconform to their metamodels.",
        "keywords": [
            "Model-driven engineering",
            "Version control systems",
            "Conﬂict resolution"
        ],
        "authors": [
            "Alfonso de la Vega",
            "Dimitris Kolovos"
        ],
        "file_path": "data/sosym-all/s10270-022-00976-4.pdf"
    },
    {
        "title": "A UML-Based Approach to System Testing",
        "submission-date": "2002/02",
        "publication-date": "2002/09",
        "abstract": "System testing is concerned with testing an entire system based on its speciﬁcations. In the context of object-oriented, UML development, this means that system test requirements are derived from UML analysis artifacts such as use cases, their corresponding sequence and collaboration diagrams, class diagrams, and possibly Object Constraint Language (OCL) expressions across all these artifacts. Our goal here is to support the derivation of functional system test requirements, which will be transformed into test cases, test oracles, and test drivers once we have detailed design information. In this paper, we describe a methodology in a practical way and il- lustrate it with an example. In this context, we address testability and automation issues, as the ultimate goal is to fully support system testing activities with high- capability tools.",
        "keywords": [
            "Testing of object-oriented systems",
            "System testing",
            "UML",
            "Use Cases",
            "Sequence Diagrams",
            "Testability"
        ],
        "authors": [
            "Lionel Briand",
            "Yvan Labiche"
        ],
        "file_path": "data/sosym-all/s10270-002-0004-8.pdf"
    },
    {
        "title": "Automatic maintenance of association invariants",
        "submission-date": "2006/04",
        "publication-date": "2008/03",
        "abstract": "Many approaches to software speciﬁcation and design make use of invariants: constraints whose truth is preserved under operations on a system or component. Object modelling involves the definition of association invariants: constraints upon the sets of links corresponding to particular associations, most often concerning type, multiplicity, or symmetry. This paper shows how the definitions of operations may be extended to take account of association invariants, so that they may be properly considered when the operations are implemented. It introduces a formal, object-based modelling notation in which the process of extension and implementation, and thus the maintenance of association invariants, can be automated, making it easier to produce correct implementations of an object-oriented design.",
        "keywords": [
            "Integrity",
            "Maintenance",
            "Association",
            "Invariants",
            "Model",
            "Completion"
        ],
        "authors": [
            "James Welch",
            "David Faitelson",
            "Jim Davies"
        ],
        "file_path": "data/sosym-all/s10270-008-0085-0.pdf"
    },
    {
        "title": "Fair treatment of evaluations in reviews",
        "submission-date": "2008/06",
        "publication-date": "2008/06",
        "abstract": "As scientists, we understand and appreciate the value of evaluating the results of our research effort. As software engineers, we are painfully aware of the difﬁcult challenges we must address when attempting to rigorously evaluate the methods, techniques, tools, languages, and other artifacts that we produce. The pressing problems that we tackle in the software and system modeling research domain can be classiﬁed as “wicked problems”: we learn more about the nature of the problems we tackle through experimentation with proposed solutions. Rigorous evaluation of these solutions invariably entails costly and lengthy experimentation in industrial contexts. Experiments that seek to evaluate solutions based on novel or radically different ideas are particularly difﬁcult to sell to potential industrial partners because the risks are not well-understood by all involved. Even with committed industrial partners, the wide variations in industrial development environments makes it difﬁcult (if not foolhardy) to extrapolate the results beyond the speciﬁc industries. Despite the difficulties, there is no getting away from the reality that evaluation is key to developing progressively better solutions to wicked problems. As researchers, we must evaluate the products of our research. The responsibilities of manuscript authors with respect to the evaluation content are not the focus of this editorial; there are many published high quality articles on this topic. Rather, this editorial focuses on the responsibilities of reviewers when it comes to commenting on the evaluation content of submitted journal papers.",
        "keywords": [],
        "authors": [
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-008-0096-x.pdf"
    },
    {
        "title": "Lightweight string reasoning in model ﬁnding",
        "submission-date": "2012/11",
        "publication-date": "2013/04",
        "abstract": "Models play a key role in assuring software quality in the model-driven approach. Precise models usually require the deﬁnition of well-formedness rules to specify constraints that cannot be expressed graphically. The Object Constraint Language (OCL) is a de-facto standard to deﬁne such rules. Techniques that check the satisﬁability of such models and ﬁnd corresponding instances of them are important in various activities, such as model-based testing and validation. Several tools for these activities have been developed, but to our knowledge, none of them supports OCL string operations on scale that is sufficient for, e.g., model-based testing. As, in contrast, many industrial models do contain such operations, there is evidently a gap. We present a lightweight solver that is specifically tailored to generate large solutions for tractable string constraints in model ﬁnding, and that is suited to directly express the main operations of the OCL datatype String. It is based on constraint logic programming (CLP) and constraint handling rules, and can be seamlessly combined with other constraint solvers in CLP. We have integrated our solver into the EMFtoCSP model ﬁnder, and we show that our implementation efficiently solves several common string constraints on large instances.",
        "keywords": [
            "Model instantiation",
            "OCL",
            "String constraints",
            "Constraint logic programming",
            "Constraint handling rules"
        ],
        "authors": [
            "Fabian Büttner",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-013-0332-x.pdf"
    },
    {
        "title": "Introduction to the theme issue on performance modeling",
        "submission-date": "2012/08",
        "publication-date": "Not found",
        "abstract": "The increasing complexity of modern computer systems demands new and efficient performance modeling techniques to aid developers in producing the most efficient system for a desired task. Performance evaluation seeks to understand and explain system performance. It should be an integral component of the development process from the beginning to reduce the costs required to fix problems late in the development cycle. Indeed, decisions made early in the process will directly impact the quality of the final product. The model-based development paradigm, in contrast, advocates developing systems starting from models—at multiple levels of abstraction—that express domain-specific concepts precisely and intuitively while supporting automated manipulation and transformation. The goal of this theme issue is to illuminate the deep relationship between computer systems performance evaluation and system modeling. ",
        "keywords": [],
        "authors": [
            "David J. Lilja",
            "Raffaela Mirandola"
        ],
        "file_path": "data/sosym-all/s10270-012-0269-5.pdf"
    },
    {
        "title": "A formal approach to model refactoring and model refinement",
        "submission-date": "2005/02",
        "publication-date": "2006/08",
        "abstract": "Model-driven engineering is an emerging software engineering approach that relies on model transformation. Typical kinds of model transformations are model refinement and model refactoring. Whenever such a transformation is applied to a consistent model, we would like to know whether the consistency is preserved by the transformation. Therefore, in this article, we formally define and explore the relation between behaviour inheritance consistency of a refined model with respect to the original model, and behaviour preservation of a refactored model with respect to the original model. As it turns out, there is a strong similarity between these notions of behaviour consistency and behaviour preservation. To illustrate this claim, we formalised the behaviour specified by UML 2.0 sequence and protocol state machine diagrams. We show how the reasoning capabilities of description logics, a decidable fragment of first-order logic, can be used in a natural way to detect behaviour inconsistencies. These reasoning capabilities can be used in exactly the same way to detect behaviour preservation violations during model refactoring. A prototype plug-in in a UML CASE tool has been developed to validate our claims.",
        "keywords": [
            "Model-driven engineering",
            "UML 2.0",
            "Description logics",
            "Model refinement",
            "Model refactoring",
            "Behaviour preservation"
        ],
        "authors": [
            "Ragnhild Van Der Straeten",
            "Viviane Jonckers",
            "Tom Mens"
        ],
        "file_path": "data/sosym-all/s10270-006-0025-9.pdf"
    },
    {
        "title": "Industry 4.0 as a Cyber-Physical System study",
        "submission-date": "2015/01",
        "publication-date": "2015/10",
        "abstract": "Advances in computation and communication are taking shape in the form of the Internet of Things, Machine-to-Machine technology, Industry 4.0, and Cyber-Physical Systems (CPS). The impact on engineering such systems is a new technical systems paradigm based on ensembles of collaborating embedded software systems. To successfully facilitate this paradigm, multiple needs can be identified along three axes: (i) online configuring an ensemble of systems, (ii) achieving a concerted function of collaborating systems, and (iii) providing the enabling infrastructure. This work focuses on the collaborative function dimension and presents a set of concrete examples of CPS challenges. The examples are illustrated based on a pick and place machine that solves a distributed version of the Towers of Hanoi puzzle. The system includes a physical environment, a wireless network, concurrent computing resources, and computational functionality such as, service arbitration, various forms of control, and processing of streaming video. The pick and place machine is of medium-size complexity. It is representative of issues occurring in industrial systems that are coming online. The entire study is provided at a computational model level, with the intent to contribute to the model-based research agenda in terms of design methods and implementation technologies necessary to make the next generation systems a reality.",
        "keywords": [
            "Cyber-Physical Systems",
            "Industry 4.0",
            "Modeling and simulation",
            "Industrial practice"
        ],
        "authors": [
            "Pieter J. Mosterman",
            "Justyna Zander"
        ],
        "file_path": "data/sosym-all/s10270-015-0493-x.pdf"
    },
    {
        "title": "Growing the UML",
        "submission-date": "2002/12",
        "publication-date": "2002/12",
        "abstract": "The entire history of software engineering can be characterized as an increase in levels of abstraction. This paper discusses the growing complexity of software and its impact on cost, and explores technologies that are on the watch list for the next 3-5 years.",
        "keywords": [],
        "authors": [
            "Grady Booch"
        ],
        "file_path": "data/sosym-all/s10270-002-0013-7.pdf"
    },
    {
        "title": "Genericity for model management operations",
        "submission-date": "2010/11",
        "publication-date": "2011/06",
        "abstract": "Models are the core assets in model-driven engineering, and are therefore subject to all kind of manipulations, such as refactorings, animations, transformations into other languages, comparisons and merging. This set of model-related activities is known as model management. Even though many languages and approaches have been proposed for model management, most of them are type-centric, specific to concrete meta-models, and hence leading to specifications with a low level of abstraction and difficult to be reused in practice. In this paper, we introduce ideas from generic programming into model management to raise the level of abstraction of the specifications of model manipulations and facilitate their reuse. In particular we adopt generic meta-model concepts as an intermediate, abstract meta-model over which model management specifications are defined. Such meta-model concepts are mapped to concrete meta-models, so that specifications can be applied to families of meta-models satisfying the concept requirements. As a proof of concept, we show the implementation of these ideas using the Eclipse Modeling Framework and the Epsilon family of languages for model management.",
        "keywords": [
            "Model management",
            "Genericity",
            "Reusability",
            "Epsilon",
            "Eclipse Modelling Framework"
        ],
        "authors": [
            "Louis Rose",
            "Esther Guerra",
            "Juan de Lara",
            "Anne Etien",
            "Dimitris Kolovos",
            "Richard Paige"
        ],
        "file_path": "data/sosym-all/s10270-011-0203-2.pdf"
    },
    {
        "title": "Formalizing and verifying stochastic system architectures using Monterey Phoenix",
        "submission-date": "2013/10",
        "publication-date": "2014/04",
        "abstract": "The analysis of software architecture plays an important role in understanding the system structures and facilitate proper implementation of user requirements. Despite its importance in the software engineering practice, the lack of formal description and veriﬁcation support in this domain hinders the development of quality architectural models. To tackle this problem, in this work, we develop an approach for modeling and verifying software architectures speciﬁed using Monterey Phoenix (MP) architecture description language. MP is capable of modeling system and environment behaviors based on event traces, as well as supporting different architecture composition operations and views. First, we formalize the syntax and operational semantics for MP; therefore, formal veriﬁcation of MP models is feasible. Second, we extend MP to support shared variables and stochastic characteristics, which not only increases the expressiveness of MP, but also widens the properties MP can check, such as quantitative requirements. Third, a dedicated model checker for MP has been implemented, so that automatic veriﬁcation of MP models is supported. Finally, several experiments are conducted to evaluate the applicability and efﬁciency of our approach",
        "keywords": [
            "Model checking",
            "Stochastic system architecture",
            "Monterey Phoenix"
        ],
        "authors": [
            "Songzheng Song",
            "Jiexin Zhang",
            "Yang Liu",
            "Mikhail Auguston",
            "Jun Sun",
            "Jin Song Dong",
            "Tieming Chen"
        ],
        "file_path": "data/sosym-all/s10270-014-0411-7.pdf"
    },
    {
        "title": "An enterprise architecture framework for multi-attribute information systems analysis",
        "submission-date": "2011/09",
        "publication-date": "2012/11",
        "abstract": "Enterprise architecture is a model-based IT and business management discipline. Enterprise architecture analysis concerns using enterprise architecture models for analysis of selected properties to provide decision support. This paper presents a framework based on the ArchiMate metamodel for the assessment of four properties, viz., application usage, system availability, service response time and data accuracy. The framework integrates four existing meta-models into one and implements these in a tool for enterprise architecture analysis. The paper presents the overall metamodel and four viewpoints, one for each property. The underlying theory and formalization of the four viewpoints is presented. In addition to the tool implementation, a running example as well as guidelines for usage makes the viewpoints easily applicable.",
        "keywords": [
            "Enterprise architecture",
            "Enterprise architecture analysis",
            "Enterprise architecture tool",
            "Data accuracy",
            "Technology usage",
            "Service availability",
            "Service response time"
        ],
        "authors": [
            "Per Närman",
            "Markus Buschle",
            "Mathias Ekstedt"
        ],
        "file_path": "data/sosym-all/s10270-012-0288-2.pdf"
    },
    {
        "title": "Measuring and achieving test coverage of attack simulations extended version",
        "submission-date": "2021/10",
        "publication-date": "2022/09",
        "abstract": "Designing secure and reliable systems is a difﬁcult task. Threat modeling is a process that supports the secure design of systems by easing the understanding of the system’s complexity, as well as identifying and modeling potential threats. These threat models can serve as input for attack simulations, which are used to analyze the behavior of attackers within the system. To ensure the correct functionality of these attack simulations, automated tests are designed that check if an attacker can reach a certain point in the threat model. Currently, there is no way for developers to estimate the degree to which their tests cover the attack simulations and, thus, they cannot determine the quality of their tests. To resolve this shortcoming, we analyze structural testing methods from the software engineering domain and transfer them to the threat modeling domain by following an action design research approach. Further, we develop a ﬁrst prototype, which is able to assess the test coverage in an automated way and provide a first approach to achieve full coverage. This will enable threat modeler to determine the quality of their tests and, simultaneously, increase the quality of the threat models.",
        "keywords": [
            "Threat modeling",
            "Attack simulations",
            "Testing",
            "Test coverage"
        ],
        "authors": [
            "Simon Hacks",
            "Linus Persson",
            "Nicklas Hersén"
        ],
        "file_path": "data/sosym-all/s10270-022-01042-9.pdf"
    },
    {
        "title": "UML 2002",
        "submission-date": "2002/00",
        "publication-date": "2003/09",
        "abstract": "This paper provides an overview of the UML 2002 conference, its submissions, accepted papers, and the topics covered. It highlights the conference as a leading forum for researchers and practitioners in UML and model-driven engineering, and discusses the evolution of UML towards better integration with other OMG technologies.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-003-0030-1.pdf"
    },
    {
        "title": "Use cases – Yesterday, today, and tomorrow",
        "submission-date": "2004/07",
        "publication-date": "2004/07",
        "abstract": "To my knowledge, no other software engineering language construct as signiﬁcant as use cases has been adopted so quickly and so widely among practitioners. I believe this is because use cases play a role in so many diﬀerent aspects of software engineering. Although I ﬁrst used the term in 1986, I had actually been working on and evolving the concept of use cases since 1967. So many people have asked me how I came up with this concept that I decided to write this article to explain the origins and evolution of use cases. I’ll also summarize what they have helped us achieve so far, and then suggest a few improvements for the future.",
        "keywords": [
            "Use case",
            "Use case driven development",
            "History of use cases",
            "Extension use cases",
            "Inclusion use cases",
            "Roles of use cases",
            "Use cases are early aspects",
            "Use case fragments"
        ],
        "authors": [
            "Ivar Jacobson"
        ],
        "file_path": "data/sosym-all/s10270-004-0060-3.pdf"
    },
    {
        "title": "Formal reconﬁguration model for cloud resources",
        "submission-date": "2021/01",
        "publication-date": "2022/03",
        "abstract": "The execution context of the cloud composite services is dynamically and rapidly changing. In the cloud environment, the service demands can increase/decrease in a restrained time interval. Due to this fact, cloud composite services have to evolve continuously by scaling up/down their capacity to handle new demands. Scaling up consists in making a component larger or faster to handle a greater load. Scaling down is the reverse of Scaling up and it is the situation of reducing component capacity when the load decreases. Dynamic adaptation mechanisms must be in place to take into account the evolution of the execution context and environment. In this paper, we propose a new Event-B formal model to manage the dynamic reconﬁguration of composite services in the cloud context. The proposed approach sets up the required reconﬁguration mechanisms and takes into account the coordination between the different cloud computing levels: Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). The proposed model contains four abstraction levels and implements the scaling mechanisms at each abstraction level. The model consistency has been proved thanks to the Event-B dedicated tools.",
        "keywords": [
            "Cloud environment",
            "Dynamic reconﬁguration",
            "Elasticity coordination",
            "Formal approach",
            "Event-B"
        ],
        "authors": [
            "Aida Lahouij",
            "Lazhar Hamel",
            "Mohamed Graiet"
        ],
        "file_path": "data/sosym-all/s10270-022-00990-6.pdf"
    },
    {
        "title": "Case-based exploration of bidirectional transformations in QVT Relations",
        "submission-date": "2015/07",
        "publication-date": "2016/05",
        "abstract": "QVT Relations (QVT-R), a standard issued by the Object Management Group, is a language for the declarative speciﬁcation of model transformations. This paper focuses on a particularly interesting feature of QVT-R: the declarative speciﬁcation of bidirectional transformations. Rather than writing two unidirectional transformations separately, a transformation developer may provide a single relational speciﬁcation which may be executed in both directions. This approach saves speciﬁcation effort and ensures the consistency of forward and backward transformations. This paper explores QVT-R’s support for bidirectional model transformations through a spectrum of transformation cases. The transformation cases vary with respect to several factors such as the size of the transformation deﬁnition or the relationships between the metamodels for source and target models. The cases are solved in QVT-R, but may be applied to other bidirectional transformation languages, as well; thus, they may be used as a benchmark for comparing bidirectional transformation languages. In our work, we focus on the following research questions: functionality of bidirectional transformations in terms of relations between source and target models, solvability (which problems may be solved by a single relational speciﬁcation of a bidirectional transformation), variability (does a bidirectional transformation contain varying elements, i.e., elements being speciﬁc to one direction), comprehensibility (referring to the ease of understanding and constructing QVT-R transformations), and the semantic soundness of bidirectional transformations written in QVT-R.",
        "keywords": [
            "Model-driven software engineering",
            "Bidirectional model transformations",
            "QVT Relations"
        ],
        "authors": [
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-016-0527-z.pdf"
    },
    {
        "title": "The many meanings of UML 2 Sequence Diagrams: a survey",
        "submission-date": "2009/07",
        "publication-date": "2010/04",
        "abstract": "Scenario languages are widely used in software development. Typical usage scenarios, forbidden behaviors, test cases, and many more aspects can be depicted with graphical scenarios. Scenario languages were introduced into the Uniﬁed Modeling Language (UML) under the name of Sequence Diagrams. The 2.0 version of UML changed Sequence Diagrams significantly and the expressiveness of the language was highly increased. However, the complexity of the language (and the diversity of the goals Sequence Diagrams are used for) yields several possible choices in its semantics. This paper collects and categorizes the semantic choices in the language, surveys the formal semantics proposed for Sequence Diagrams, and presents how these approaches handle the various semantic choices.",
        "keywords": [
            "UML",
            "Sequence diagrams",
            "Semantics"
        ],
        "authors": [
            "Zoltán Micskei",
            "Hélène Waeselynck"
        ],
        "file_path": "data/sosym-all/s10270-010-0157-9.pdf"
    },
    {
        "title": "Least-change bidirectional model transformation with QVT-R and ATL",
        "submission-date": "2013/09",
        "publication-date": "2014/11",
        "abstract": "QVT Relations (QVT-R) is the standard lan-guage proposed by the OMG to specify bidirectional model transformations. Unfortunately, in part due to ambigui-ties and omissions in the original semantics, acceptance and development of effective tool support have been slow. Recently, the checking semantics of QVT-R has been clar-iﬁed and formalized. In this article, we propose a QVT-R tool that complies to such semantics. Unlike any other exist-ing tool, it also supports meta-models enriched with OCL constraints (thus avoiding returning ill-formed models) and proposes an alternative enforcement semantics that works according to the simple and predictable “principle of least change.” The implementation is based on an embedding of bothQVT-RtransformationsandUMLclassdiagrams(annotated with OCL) in Alloy, a lightweight formal speciﬁcation language with support for automatic model ﬁnding via SAT solving. We also show how this technique can be applied to bidirectionalize ATL, a popular (but unidirectional) model transformation language.",
        "keywords": [
            "Model transformation",
            "Bidirectional transformation",
            "Least-change principle",
            "QVT-R",
            "ATL",
            "Alloy"
        ],
        "authors": [
            "Nuno Macedo",
            "Alcino Cunha"
        ],
        "file_path": "data/sosym-all/s10270-014-0437-x.pdf"
    },
    {
        "title": "Event-B patterns and their tool support",
        "submission-date": "2010/06",
        "publication-date": "2011/01",
        "abstract": "Event-B has given developers the opportunity\nto construct models of complex systems that are correct-\nby-construction. However, there is no systematic approach,\nespecially in terms of reuse, which could help with the con-\nstruction of these models. We introduce the notion of design\npatterns within the framework of Event-B to shorten this\ngap. Our approach preserves the correctness of the models,\nwhich is critical in formal methods and also reduces the prov-\ning effort. Within our approach, an Event-B design pattern is\njust another model devoted to the formalisation of a typical\nsub-problem. As a result, we can use patterns to construct a\nmodel which can subsequently be used as a pattern to con-\nstruct a larger model. We also present the interaction between\ndevelopers and the tool support within the associated RODIN\nPlatform of Event-B. The approach has been applied success-\nfully to some medium-size industrial case studies.",
        "keywords": [
            "Event-B",
            "Formal methods",
            "Design patterns",
            "Formal modelling",
            "Model reuse"
        ],
        "authors": [
            "Thai Son Hoang",
            "Andreas Fürst",
            "Jean-Raymond Abrial"
        ],
        "file_path": "data/sosym-all/s10270-010-0183-7.pdf"
    },
    {
        "title": "EMF-Syncer: scalable maintenance of view models over heterogeneous data-centric software systems at run time",
        "submission-date": "2022/05",
        "publication-date": "2023/06",
        "abstract": "Withtheincreasingpresenceofcyber-physicalsystems(CPSs),likeautonomousvehiclesystemsanddigitaltwins,thefutureof\nsoftware engineering is predicated on the importance of designing and developing data-centric software systems that can adapt\nintelligently at run time. CPSs consist of complex heterogeneous software components. Model-driven engineering advocates\nusing software models to tame such complexity, capturing the relevant design concerns of such systems at different levels of\nabstraction. Yet most of the existing CPSs are engineered without considering MDE practices and tools, facing fundamental\nchallenges when working with data: monitoring the program data at run time, syncing updates between program and model,\ndealing with heterogeneous data sources, and representing such observed data at run time to facilitate automated analysis. In\nthis work, we introduce the notion of view models to explicitly represent parts of the domain knowledge implicitly embedded\nin the source code of a software system. This notion is equipped with a scalable bidirectional syncing mechanism that extracts\nview model instances from program snapshots at run time. The syncing mechanism is proposed from a conceptual point of\nview, independently of speciﬁc implementations and supports incremental view model update and view model maintenance.\nWe show how this syncing mechanism is ﬂexible enough to facilitate the non-intrusive adoption of MDE technology over\nexisting MDE-agnostic heterogeneous data-centric systems. We study the run-time cost implied by the EMF- Syncer ,\nthe tool implementing this syncing mechanism for Java applications and view models atop the eclipse modeling framework\n(EMF) when executing data analytic and transformation tasks over large volumes of data in the presence of data updates at run\ntime. An empirical evaluation of the EMF- Syncer has been conducted with an industry-targeted benchmark for decision\nsupport systems, analyzing performance and scalability. The novel syncing mechanism enables new opportunities to adopt\nMDE technology in heterogeneous data-centric systems.",
        "keywords": [
            "Model-driven engineering",
            "Models@runtime",
            "Roundtrip synchronization",
            "View update problem"
        ],
        "authors": [
            "Artur Boronat"
        ],
        "file_path": "data/sosym-all/s10270-023-01111-7.pdf"
    },
    {
        "title": "Lazy model checking for recursive state machines",
        "submission-date": "2022/06",
        "publication-date": "2024/03",
        "abstract": "Recursive state machines (RSMs) are state-based models for procedural programs with wide-ranging applications in program veriﬁcation and interprocedural analysis. Model-checking algorithms for RSMs and related formalisms have been intensively studied in the literature. In this article, we devise a new model-checking algorithm for RSMs and requirements in computation tree logic (CTL) that exploits the compositional structure of RSMs by ternary model checking in combination with a lazy evaluation scheme. Speciﬁcally, a procedural component is only analyzed in those cases in which it might inﬂuence the satisfaction of the CTL requirement. We implemented our model-checking algorithms and evaluate them on randomized scalability benchmarks and on an interprocedural data-ﬂow analysis of Java programs, showing both practical applicability and signiﬁcant speedups in comparison to state-of-the-art model-checking tools for procedural programs.",
        "keywords": [
            "Model checking",
            "Lazy veriﬁcation",
            "Interprocedural static analysis",
            "Recursive state machines",
            "Computation tree logic"
        ],
        "authors": [
            "Clemens Dubslaff",
            "Patrick Wienhöft",
            "Ansgar Fehnker"
        ],
        "file_path": "data/sosym-all/s10270-024-01159-z.pdf"
    },
    {
        "title": "Report on the State of the SoSyM Journal end of 2022",
        "submission-date": "2023/02",
        "publication-date": "2023/02",
        "abstract": "SoSyM continued to expand in many ways over the past\nyear. Modeling remains a challenging topic, in terms of both\nresearch investigations and practical applications. While a\ngrowing number of industrial projects are collecting impor-\ntant experiences about the application of models in various\nforms, they are also identifying new and challenging ques-\ntions that need to be addressed and solved. This holds for\nmodels applied to traditional software development, models\nthat assist in the design of cyber physical systems, and models\nthat offer support for systems modeling, in general. A com-\npletely new chapter has been opened with the use of models\nnot only for system development, but also for the investiga-\ntion of their accompanying digital twins, as addressed in a\nrecent editorial.",
        "keywords": [],
        "authors": [
            "Stéphanie Challita",
            "Benoit Combemale",
            "Huseyin Ergin",
            "JeﬀGray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-023-01085-6.pdf"
    },
    {
        "title": "Improving the accuracy of UML metamodel extensions by introducing induced associations",
        "submission-date": "2006/07",
        "publication-date": "2007/07",
        "abstract": "In the process of extending the UML metamodel for a speciﬁc domain, the metamodel speciﬁer introduces frequently some metaassociations at MOF level M2 with the aim that they induce some speciﬁc associations at MOF level M1. For instance, if a metamodel for software process modelling states that a “Role” is responsible for an “Arti-fact”, we can interpret that its speciﬁer intended to model two aspects: (1) the implications of this metaassociation at level M1 (e.g., the speciﬁc instance of Role “TestEngineer” is responsible for the speciﬁc instance of Artifact “TestPlans”); and (2) the implications of this metaassociation at level M0 (e.g., “John Doe” is the responsible test engineer for elaborating the test plans for the package “Foo”). Unfortunately, the second aspect is often not enforced by the metamodel and, as a result, the models which are deﬁned as its instances may not incorporate it. This problem, consequence of the so-called “shallow instantiation” in Atkinson and Kühne (Procs.UML’01,LNCS2185,Springer,2001),preventsthese models from being accurate enough in the sense that they do not express all the information intended by the metamodel speciﬁer and consequently do not distinguish metaassocia-tions that induce associations at M1 from those that do not. In this article we introduce the concept of induced association that may come up when an extension of the UML metamodel is developed. The implications that this concept has both in the extended metamodel and in its instances are discussed. We also present a methodology to enforce that M1 models incorporate the associations induced by the metamodel which they are instances from. Next, as an example of application we present a quality metamodel for software artifacts which makes intensive use of induced associations. Finally, we introduce a software tool to assist the development of quality models as correct instantiations of the metamodel, assuring the proper application of the induced associations as required by the metamodel.",
        "keywords": [
            "Metamodelling",
            "MOF",
            "Shallow instantiation",
            "UML extension",
            "Software quality",
            "Metaassociations"
        ],
        "authors": [
            "Xavier Burgués",
            "Xavier Franch",
            "Josep M. Ribó"
        ],
        "file_path": "data/sosym-all/s10270-007-0062-z.pdf"
    },
    {
        "title": "Transactional execution of hierarchical reconﬁgurations in cyber-physical systems",
        "submission-date": "2016/03",
        "publication-date": "2017/02",
        "abstract": "Cyber-physical systems reconﬁgure the structure of their software architecture, e.g., to avoid hazardous situations and to optimize operational conditions like their energy consumption. These reconﬁgurations have to be safe so that the systems protect their users or environment against harm-ful conditions or events while changing their structure. As software architectures are typically built on components, reconﬁguration actions need to take into account the component structure. This structure should support vertical composition to enable hierarchically encapsulated components. While many reconﬁguration approaches for cyber-physical and embedded real-time systems allow the use of hierarchi-cally embedded components, i.e., vertical composition, none of them offers a modeling and veriﬁcation solution to take hierarchical composition, i.e., encapsulation, into account thus limiting reuse and compositional veriﬁcation. In this paper, we present an extension to our existing modeling language, MechatronicUML, to enable safe hierarchical reconﬁgurations. The three extensions are (a) an adapted variant of the 2-phase-commit protocol to initiate recon-ﬁgurations that maintain component encapsulation, (b) the integration of feedback controllers during reconﬁguration, and(c)averiﬁcationapproachbasedon(timed)modelcheck-ing for instances of our model. We illustrate our approach on a case study in the area of smart railway systems by show-ing two different use cases of our approach. We show that using our approach the systems can be easily designed to reconﬁgure safely.",
        "keywords": [
            "CPS",
            "Safe reconﬁguration",
            "Correctness-by-construction",
            "Runtime reconﬁguration",
            "Component model",
            "Reconﬁguration behavior",
            "Feedback controller exchange",
            "Transactions",
            "Atomicity",
            "Consistency",
            "Isolation",
            "Timed model checking"
        ],
        "authors": [
            "Christian Heinzemann",
            "Steffen Becker",
            "Andreas Volk"
        ],
        "file_path": "data/sosym-all/s10270-017-0583-z.pdf"
    },
    {
        "title": "A data-driven approach for constructing multilayer network-based service ecosystem models",
        "submission-date": "2021/09",
        "publication-date": "2022/08",
        "abstract": "Services are ﬂourishing drastically both on the Internet and in the real world. In addition, services have become much more interconnected to facilitate transboundary business collaboration to create and deliver distinct new values to customers. Various service ecosystems come into being and are increasingly becoming a focus in both research and practice. However, due to the lack of widely recognized service ecosystem models and sufﬁcient real data for constructing such models, existing studies on service ecosystems are limited to a very narrow scope and cannot effectively guide the design, optimization, and evolution of service ecosystems. In this paper, we ﬁrst propose a multilayer network-based service ecosystem model (MSEM), which covers a variety of service-related elements, including stakeholders, channels, functional and nonfunctional features, and domains, and more importantly, structural and evolutionary relations between them. “Events” are introduced to describe the triggers of service ecosystem evolution. Then, we propose a data-driven approach for constructing MSEM from public media news and external data sources. Experiments conducted on real news corpora show that compared with other approaches, our approach can construct large-scale models for real-world service ecosystems with lower cost and higher efﬁciency.",
        "keywords": [
            "Service ecosystem",
            "Multilayer knowledge graph",
            "Service-related event",
            "Event mining",
            "Model construction",
            "Evolution"
        ],
        "authors": [
            "Mingyi Liu",
            "Zhiying Tu",
            "Xiaofei Xu",
            "Zhongjie Wang",
            "Yan Wang"
        ],
        "file_path": "data/sosym-all/s10270-022-01029-6.pdf"
    },
    {
        "title": "An intermediate metamodel with scenarios and resources for generating performance models from UML designs",
        "submission-date": "2005/02",
        "publication-date": "2006/08",
        "abstract": "Performance analysis of a software speciﬁ- cation in a language such as UML can assist a design team in evaluating performance-sensitive design deci- sions and in making design trade-offs that involve per- formance. Annotations to the design based on the UML Proﬁle for Schedulability, Performance and Time pro- vide necessary information such as workload parame- ters for a performance model, and many different kinds of performance techniques can be applied. The Core Scenario Model (CSM) described here provides a meta- model for an intermediate form which correlates mul- tiple UML diagrams, extracts the behaviour elements with the performance annotations, attaches important resource information that is obtained from the UML, and supports the creation of many different kinds of per- formance models. Models can be made using queueing networks, layered queues, timed Petri nets, and it is proposed to develop the CSM as an intermediate language for all performance formalisms. This paper deﬁnes the CSM and describes how it resolves questions that arise in performance model-building.",
        "keywords": [
            "Uniﬁed modeling language",
            "Performance evaluation",
            "Scenarios",
            "Model transformations"
        ],
        "authors": [
            "Dorin B. Petriu",
            "Murray Woodside"
        ],
        "file_path": "data/sosym-all/s10270-006-0026-8.pdf"
    },
    {
        "title": "Designing secure business processes with SecBPMN",
        "submission-date": "2014/10",
        "publication-date": "2015/10",
        "abstract": "Modern information systems are increasingly large and consist of an interplay of technical components and social actors (humans and organizations). Such interplay threatens the security of the overall system and calls for veriﬁcation techniques that enable determining compliance with security policies. Existing veriﬁcation frameworks either have a limited expressiveness that inhibits the speciﬁcationofreal-worldrequirementsorrelyonformallanguages that are difﬁcult to use for most analysts. In this paper, we overcome the limitations of existing approaches by presenting the SecBPMN framework. Our proposal includes: (1) the SecBPMN-ml modeling language, a security-oriented extension of BPMN for specifying composite information systems; (2) the SecBPMN-Q query language for representing security policies; and (3) a query engine that enables checking SecBPMN-Q policies against SecBPMN-ml speciﬁcations. We evaluate our approach by studying its understandability and perceived complexity with experts, running scalability analysis of the query engine, and through an application to a large case study concerning air trafﬁc management.",
        "keywords": [
            "Information systems",
            "Security policies",
            "BPMN",
            "Compliance"
        ],
        "authors": [
            "Mattia Salnitri",
            "Fabiano Dalpiaz",
            "Paolo Giorgini"
        ],
        "file_path": "data/sosym-all/s10270-015-0499-4.pdf"
    },
    {
        "title": "A survey of traceability in requirements engineering and model-driven development",
        "submission-date": "2009/01",
        "publication-date": "2009/12",
        "abstract": "Traceability—the ability to follow the life of software artifacts—is a topic of great interest to software developers in general, and to requirements engineers and model-driven developers in particular. This article aims to bring those stakeholders together by providing an overview of the current state of traceability research and practice in both areas. As part of an extensive literature survey, we identify commonalities and differences in these areas and uncover several unresolved challenges which affect both domains. A good common foundation for further advances regarding these challenges appears to be a combination of the formal basis and the automated recording opportunities of MDD on the one hand, and the more holistic view of traceability in the requirements engineering domain on the other hand.",
        "keywords": [
            "Requirements engineering",
            "Model-driven engineering",
            "Model-driven development",
            "Traceability"
        ],
        "authors": [
            "Stefan Winkler",
            "Jens von Pilgrim"
        ],
        "file_path": "data/sosym-all/s10270-009-0145-0.pdf"
    },
    {
        "title": "Typing artifacts in megamodeling",
        "submission-date": "2009/11",
        "publication-date": "2011/02",
        "abstract": "Model management is essential for coping with the complexity introduced by the increasing number and varied nature of artifacts involved in model-driven engineering-based projects. Global model management (GMM) addresses this issue by enabling the representation of artifacts, particularly transformation composition and execution, within a model called a megamodel. Type information about artifacts can be used for preventing type errors during execution. Built on our previous work, in this paper we present the core elements of a type system for GMM that improves its original typing approach and enables both typechecking and type inference on artifacts within a megamodel. This type system is able to deal with non-trivial situations such as the use of higher order transformations. We also present a prototypical implementation of such a type system.",
        "keywords": [
            "Model transformation",
            "Type system",
            "Megamodeling"
        ],
        "authors": [
            "Andrés Vignaga",
            "Frédéric Jouault",
            "María Cecilia Bastarrica",
            "Hugo Brunelière"
        ],
        "file_path": "data/sosym-all/s10270-011-0191-2.pdf"
    },
    {
        "title": "Assessing event correlation in non-process-aware information systems",
        "submission-date": "2011/10",
        "publication-date": "2012/09",
        "abstract": "Many present-day companies carry out a huge amount of daily operations through the use of their information systems without ever having done their own enterprise modeling. Business process mining is a well-proven solution which is used to discover the underlying business process models that are supported by existing information systems. Business process discovery techniques employ event logs as input, which are recorded by process-aware information systems. However, a wide variety of traditional information systems do not have any in-built mechanisms with which to collect events (representing the execution of business activities). Various mechanisms with which to collect events from non-process-aware information systems have been proposed in order to enable the application of process mining techniques to traditional information systems. Unfortunately, since business processes supported by traditional information systems are implicitly defined, correlating events into the appropriate process instance is not trivial. This challenge is known as the event correlation problem. This paper presents an adaptation of an existing event correlation algorithm and incorporates it into a technique in order to collect event logs from the execution of traditional information systems. The technique first instruments the source code to collect events together with some candidate correlation attributes. Based on several well-known design patterns, the technique provides a set of guidelines to support experts when instrumenting the source code. The event correlation algorithm is subsequently applied to the data set of events to discover the best correlation conditions, which are then used to create event logs. The technique has been semi-automated to facilitate its validation through an industrial case study involving a writer management system and a healthcare evaluation system. The study demonstrates that the technique is able to discover an appropriate correlation set and obtain well-formed event logs, thus enabling business process mining techniques to be applied to traditional information systems.",
        "keywords": [
            "Business process mining",
            "Event correlation",
            "Event model",
            "Case study"
        ],
        "authors": [
            "Ricardo Pérez-Castillo",
            "Barbara Weber",
            "Ignacio García-Rodríguez de Guzmán",
            "Mario Piattini",
            "Jakob Pinggera"
        ],
        "file_path": "data/sosym-all/s10270-012-0285-5.pdf"
    },
    {
        "title": "The evolution of modeling research challenges",
        "submission-date": "2013/05",
        "publication-date": "2013/05",
        "abstract": "In 2007 ICSE hosted a track called “Future of Software Development” (FOSD). We were invited to write and present a paper on the future of modeling for the track. The result-ing paper [1] described the state of modeling research, iden-tiﬁed some major challenges and proposed a research road map. Parts of this road map are currently being explored, and progress has been made in addressing some of the challenges we identiﬁed. However, there is still signiﬁcant research “to be done” with respect to the challenges outlined in that paper. It is not our intent to discuss the progress the community has made with respect to the road map in this editorial (our apolo-gies for deﬂating expectations in this regard; an editorial is simply not the place for such discussions). Rather, we would like to use this editorial to stimulate discussions around some of the challenges that have arisen since we wrote that paper.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-013-0346-4.pdf"
    },
    {
        "title": "Artefacts in software engineering: a fundamental positioning",
        "submission-date": "2018/05",
        "publication-date": "2019/01",
        "abstract": "Artefacts play a vital role in software and systems development processes. Other terms like documents, deliverables, or work products are widely used in software development communities instead of the term artefact. In the following, we use the term ‘artefact’ including all these other terms. Despite its relevance, the exact denotation of the term ‘artefact’ is still not clear due to a variety of different understandings of the term and to a careless negligent usage. This often leads to approaches being grounded in a fuzzy, unclear understanding of the essential concepts involved. In fact, there does not exist a common terminology. Therefore, it is our goal that the term artefact be standardised so that researchers and practitioners have a common understanding for discussions and contributions. In this position paper, we provide a positioning and critical reﬂection upon the notion of artefacts in software engineering at different levels of perception and how these relate to each other. We further contribute a metamodel that provides a description of an artefact that is independent from any underlying process model. This metamodel deﬁnes artefacts at three levels. Abstraction and reﬁnement relations between these levels allow correlating artefacts to each other and deﬁning the notion of related, reﬁned, and equivalent artefacts. Our contribution shall foster the long overdue and too often underestimated terminological discussion on what artefacts are to provide a common ground with clearer concepts and principles for future software engineering contributions, such as the design of artefact-oriented development processes and tools.",
        "keywords": [
            "Software engineering artefacts",
            "Metamodelling",
            "Propaedeutics",
            "Syntax of artefacts",
            "Semantics of artefacts",
            "Equivalence of artefacts"
        ],
        "authors": [
            "Daniel Méndez Fernández",
            "Wolfgang Böhm",
            "Andreas Vogelsang",
            "Jakob Mund",
            "Manfred Broy",
            "Marco Kuhrmann",
            "Thorsten Weyer"
        ],
        "file_path": "data/sosym-all/s10270-019-00714-3.pdf"
    },
    {
        "title": "From well-formedness to meaning preservation: model refactoring for almost free",
        "submission-date": "2012/04",
        "publication-date": "2013/04",
        "abstract": "Modelling languages such as the UML specify well-formedness as constraints on models. For the refactoring of a model to be correct, it must take these constraints into account and check that they are still satisﬁed after the refactoring has been performed—if not, execution of the refactoring must be refused. By replacing constraint checking with constraint solving, we show how the role of constraints can be lifted from permitting or denying a tentative refactoring to computing additional model changes required for the refactoring to be executable. Thus, to the degree that the semantics of a modelling language is speciﬁed using constraints, refactorings based on these constraints are guaranteed to be meaning preserving. To be able to exploit constraints available in the form of a language’s well-formedness rules for refactoring, we present a mapping from these rules to the constraint rules required by constraint-based refactoring. Where there are no gaps between well-formedness and (static) semantics of a modelling language, these mappings enable structural refactorings of models at no extra cost; where there are, we identify ways of detecting and ﬁlling the gaps.",
        "keywords": [
            "Model refactoring",
            "Constraints"
        ],
        "authors": [
            "Friedrich Steimann"
        ],
        "file_path": "data/sosym-all/s10270-013-0314-z.pdf"
    },
    {
        "title": "On the assessment of generative AI in modeling tasks: an experience report with ChatGPT and UML",
        "submission-date": "2023/03",
        "publication-date": "2023/05",
        "abstract": "Most experts agree that large language models (LLMs), such as those used by Copilot and ChatGPT, are expected to revo-lutionize the way in which software is developed. Many papers are currently devoted to analyzing the potential advantages and limitations of these generative AI models for writing code. However, the analysis of the current state of LLMs with respect to software modeling has received little attention. In this paper, we investigate the current capabilities of ChatGPT to perform modeling tasks and to assist modelers, while also trying to identify its main shortcomings. Our ﬁndings show that, in contrast to code generation, the performance of the current version of ChatGPT for software modeling is limited, with various syntactic and semantic deﬁciencies, lack of consistency in responses and scalability issues. We also outline our views on how we perceive the role that LLMs can play in the software modeling discipline in the short term, and how the modeling community can help to improve the current capabilities of ChatGPT and the coming LLMs for software modeling.",
        "keywords": [
            "Large language models",
            "ChatGPT",
            "Software models",
            "Modeling languages",
            "UML"
        ],
        "authors": [
            "Javier Cámara",
            "Javier Troya",
            "Lola Burgueño",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-023-01105-5.pdf"
    },
    {
        "title": "Example-driven meta-model development",
        "submission-date": "2013/06",
        "publication-date": "2013/12",
        "abstract": "The intensive use of models in model-driven engineering (MDE) raises the need to develop meta-models with different aims, such as the construction of textual and visual modelling languages and the speciﬁcation of source and target ends of model-to-model transformations. While domain experts have the knowledge about the concepts of the domain, they usually lack the skills to build meta-models. Moreover, meta-models typically need to be tailored according to their future usage and speciﬁc implementation platform, which demands knowledge available only to engineers with great expertise in speciﬁc MDE platforms. These issues hinder a wider adoption of MDE both by domain experts and software engineers. In order to alleviate this situation, we propose an interactive, iterative approach to meta-model construction, enabling the speciﬁcation of example model fragments by domain experts, with the possibility of using informal drawing tools like Dia or yED. These fragments can be annotated with hints about the intention or needs for certain elements. A meta-model is then automatically induced, which can be refactored in an interactive way, and then compiled into an implementation meta-model using proﬁles and patterns for different platforms and purposes. Our approach includes the use of a virtual assistant, which provides suggestions for improving the meta-model based on well-known refactorings, and a validation mode, enabling the validation of the meta-model by means of examples.",
        "keywords": [
            "Meta-modelling",
            "Domain-speciﬁc modelling languages",
            "Interactive meta-modelling",
            "Meta-model induction",
            "Example-driven modelling",
            "Meta-model design exploration",
            "Meta-model validation"
        ],
        "authors": [
            "Jesús J. López-Fernández",
            "Jesús Sánchez Cuadrado",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-013-0392-y.pdf"
    },
    {
        "title": "Playground for multi-level modeling constructs",
        "submission-date": "2020/05",
        "publication-date": "2021/08",
        "abstract": "In recent years, multi-level modeling has become more and more popular. It is mainly due to the fact that multi-level modeling aims to reduce or even totally eliminate any accidental complexity inadvertently created as by-product in traditional model design. Moreover, besides reducing model complexity, multi-level modeling also improves on general comprehension of models. The key enablers of multi-level modeling are the concepts of clabjects and deep instantiation. The latter is often governed by the potency notion, of which many different interpretations and variations emerged over the years. However, there exist also some approaches that disregard the potency notion. Thus, multi-level modeling approaches tend to take advantage of different theoretical and practical backgrounds. In this paper, we propose a unifying framework, the Multi-Level Modeling Playground (MLMP), which is a validating modeling environment for multi-level modeling research. The MLMP environment is based on our multi-layer modeling framework (the Dynamic Multi-Layer Algebra), which provides useful mechanisms to validate different multi-level modeling constructs. Since beyond the structure also the well-formedness rules of the modeling constructs can be speciﬁed, our proposed MLMP environment delivers several practical beneﬁts: i) well-formedness is always veriﬁed, ii) multi-level constructs can be experimented with independently of any concrete tool chains, and iii) relationships (i.e., correlations or exclusions) between different multi-level constructs can be easily investigated in practice. Also, the capability of the environment is demonstrated via complete examples inspired by state-of-the-art research literature.",
        "keywords": [
            "Multi-level modeling",
            "Potency notion",
            "Clabject",
            "Level-blind",
            "Scientiﬁc experimentation",
            "Modular playground"
        ],
        "authors": [
            "Ferenc A. Somogyi",
            "Gergely Mezei",
            "Zoltán Theisz",
            "Sándor Bácsi",
            "Dániel Palatinszky"
        ],
        "file_path": "data/sosym-all/s10270-021-00900-2.pdf"
    },
    {
        "title": "Enhancing automated network function onboarding through language extension and code refactoring",
        "submission-date": "2024/09",
        "publication-date": "2025/06",
        "abstract": "The management of modern communication networks requires sophisticated, specialized tooling that, akin to an operating system, provides an appropriately abstracted view of the network and its resources together with a broad and extensible range of functionality to observe, analyze, and change different aspects of the network such as software-deﬁned networking. This research explores the process of extending network management platforms, focusing speciﬁcally on a management platform in use at TELUS, a leading Canadian communication and IT company. The TELUS Intelligent Network Automation and Analytics (TINAA) is a model-driven control and management ecosystem. TINAA uses MDE techniques and YANG, a data modeling language widely used in the networking domain, to facilitate the implementation of new network functions. More speciﬁcally, to add a network function to TINAA, the following process is used: (1) The function is described using YANG. (2) Skeleton code is generated from these models. (3) The skeleton code is modiﬁed and completed as appropriate by a developer. In particular, we demonstrate how YANG’s language extension mechanism, combined with code clone detection and removal, can capture essential information at the model level and generate cleaner, more readable skeleton code, signiﬁcantly simplifying the manual code completion process. We evaluate our techniques using the L3VPN Network Model (L3NM), a widely adopted model for managing and conﬁguring Layer 3 VPN services. We identify general lessons learned of interest to both the networking and the MDE community.",
        "keywords": [
            "Model-based engineering",
            "Model-driven network automation",
            "NetOps",
            "Project scaffolding",
            "Network function virtualization",
            "RESTful API speciﬁcation",
            "Software-deﬁned networking (SDN)",
            "YANG",
            "Business Process Modeling Notation (BPMN)",
            "Code clones",
            "Refactoring"
        ],
        "authors": [
            "Hesham Elabd",
            "Juergen Dingel",
            "Tung Fai Lau",
            "Ali Tizghadam"
        ],
        "file_path": "data/sosym-all/s10270-025-01311-3.pdf"
    },
    {
        "title": "Formal foundation of consistent EMF model transformations by algebraic graph transformation",
        "submission-date": "2009/06",
        "publication-date": "2011/03",
        "abstract": "Model transformation is one of the key activities in model-driven software development. An increasingly popular technology to deﬁne modeling languages is provided by the Eclipse Modeling Framework (EMF). Several EMF model transformation approaches have been developed, focusing on different transformation aspects. To validate model transformations with respect to functional behavior and correctness, a formal foundation is needed. In this paper, we deﬁne consistent EMF model transformations as a restricted class of typed graph transformations usingnodetypeinheritance.ContainmentconstraintsofEMF model transformations are translated to a special kind of graph transformation rules such that their application leads to consistent transformation results only. Thus, consistent EMF model transformations behave like algebraic graph transformations and the rich theory of algebraic graph transformation can be applied to these EMF model transformations to show functional behavior and correctness. Furthermore, we propose parallel graph transformation as a suitable framework for modeling EMF model transformations with multi-object structures. Rules extended by multi-object structures can specify a ﬂexible number of recurring structures. The actual number of recurring structures is dependent on the application context of such a rule. We illustrate our approach by selected refactorings of simpliﬁed statechart models. Finally, we discuss the implementation of our concepts in a tool environment for EMF model transformations.",
        "keywords": [
            "Consistent EMF models",
            "Model transformation",
            "Graph transformation",
            "Rule amalgamation"
        ],
        "authors": [
            "Enrico Biermann",
            "Claudia Ermel",
            "Gabriele Taentzer"
        ],
        "file_path": "data/sosym-all/s10270-011-0199-7.pdf"
    },
    {
        "title": "In memory of Robert B. France, Co-Founder and Editor-in-Chief of SoSyM from 1999 to 2015",
        "submission-date": "2015/04",
        "publication-date": "2015/04",
        "abstract": "The SoSyM team has been devastated to learn about the passing of Prof. Robert B. France, on the evening of Sunday, February 15th, 2015. His passing was painless, after a battle against cancer. He was 54 years old.",
        "keywords": [],
        "authors": [
            "Marsha Chechik",
            "Geri Georg",
            "Martin Gogolla",
            "Jean-Marc Jezequel",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-015-0461-5.pdf"
    },
    {
        "title": "SQL-PL4OCL: an automatic code generator from OCL to SQL procedural language",
        "submission-date": "2016/11",
        "publication-date": "2017/05",
        "abstract": "In this paper, we introduce a SQL-PL code generator for OCL expressions that, in contrast to other proposals, is able to map OCL iterate and iterator expressions thanks to our use of stored procedures. We explain how the mapping presented here introduces key differences to the previous version of our mapping in order to (i) simplify its deﬁnition, (ii) improve the evaluation time of the resulting code, and (iii) consider OCL three-valued evaluation semantics. Moreover, we have implemented our mapping to target several relational database management systems, i.e., MySQL, MariaDB, PostgreSQL, and SQL server, which allows us to widen its usability and to benchmark the evaluation time of the SQL-PL code produced.",
        "keywords": [
            "OCL",
            "UML",
            "SQL",
            "Stored procedures",
            "Code generator"
        ],
        "authors": [
            "Marina Egea",
            "Carolina Dania"
        ],
        "file_path": "data/sosym-all/s10270-017-0597-6.pdf"
    },
    {
        "title": "Performance models of storage contention in cloud environments",
        "submission-date": "2011/07",
        "publication-date": "2012/03",
        "abstract": "We propose simple models to predict the performance degradation of disk requests due to storage device contention in consolidated virtualized environments. Model parameters can be deduced from measurements obtained inside Virtual Machines (VMs) from a system where a single VM accesses a remote storage server. The parameterized model can then be used to predict the effect of storage contention when multiple VMs are consolidated on the same server. We first propose a trace-driven approach that evaluates a queueing network with fair share scheduling using simulation. The model parameters consider Virtual Machine Monitor level disk access optimizations and rely on a calibration technique. We further present a measurement-based approach that allows a distinct characterization of read/write performance attributes. In particular, we define simple linear prediction models for I/O request mean response times, throughputs and read/write mixes, as well as a simulation model for predicting response time distributions. We found our models to be effective in predicting such quantities across a range of synthetic and emulated application workloads.",
        "keywords": [
            "Performance modeling",
            "Virtualization",
            "Storage"
        ],
        "authors": [
            "Stephan Kraft",
            "Giuliano Casale",
            "Diwakar Krishnamurthy",
            "Des Greer",
            "Peter Kilpatrick"
        ],
        "file_path": "data/sosym-all/s10270-012-0227-2.pdf"
    },
    {
        "title": "Formal synthesis of application and platform behaviors of embedded software systems",
        "submission-date": "2011/01",
        "publication-date": "2013/05",
        "abstract": "Two main embedded software components, application software and platform software, i.e., the real-time operating system (RTOS), interact with each other in order to achieve the functionality of the system. However, they are so different in behaviors that one behavior modeling language is not sufﬁcient to model both styles of behaviors and to reason about the characteristics of their individual behaviors as well as their parallel behavior and interaction properties. In this paper, we present a formal approach to the synthesis of the application software and the RTOS behavior models. In this approach, each of them is modeled with its adequate model-ing language and then is composed into a system model for analysis. Moreover, this paper also presents a consistent way of analyzing the application software with respect to both functional requirements and timing requirements. To show the effectiveness of the approach, a case study is conducted, where ARINC 653 and its application are modeled and veriﬁed against timing requirements. Using our approach, application software can be constructed as a behavioral model independently from a speciﬁc platform and can be veriﬁed against various platforms and timing constraints in a formal way.",
        "keywords": [
            "Embedded software systems",
            "Real-time operating systems",
            "Model-driven development",
            "Statecharts",
            "TRoS",
            "Formal methods and engineering"
        ],
        "authors": [
            "Jinhyun Kim",
            "Inhye Kang",
            "Jin-Young Choi",
            "Insup Lee",
            "Sungwon Kang"
        ],
        "file_path": "data/sosym-all/s10270-013-0342-8.pdf"
    },
    {
        "title": "Aspects of abstraction in software development",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "Abstraction is a fundamental tool of human thought in every context. This essay briefly reviews some manifestations of abstraction in everyday life, in engineering and mathematics, and in software and system development. Vertical and horizontal abstraction are distinguished and characterised. The use of vertical abstraction in top-down and bottom-up program development is discussed, and also, the use of horizontal abstraction in one very different approach to program design. The ubiquitous use of analogical models in software is explained in terms of analytical abstractions. Some aspects of the practical use of abstraction in the development of computer-based systems are explored. The necessity of multiple abstractions is argued from the essential nature of abstraction, which by definition focuses on some concerns at the expense of discarding others. Finally, some general recommendations are offered for a consciously thoughtful use of abstraction in software development.",
        "keywords": [
            "Abstraction",
            "Analogic model",
            "Bottom-up design",
            "Grounded abstraction",
            "Free abstraction",
            "Horizontal abstraction",
            "Monsters",
            "Refinement",
            "Theory",
            "Top-down design",
            "Vertical abstraction"
        ],
        "authors": [
            "Michael Jackson"
        ],
        "file_path": "data/sosym-all/s10270-012-0259-7.pdf"
    },
    {
        "title": "Strategic business modeling: representation and reasoning",
        "submission-date": "2011/11",
        "publication-date": "2012/10",
        "abstract": "Business intelligence (BI) offers tremendous potential for business organizations to gain insights into their day-to-day operations, as well as longer term opportunities and threats. However, most of today’s BI tools are based on models that are too much data-oriented from the point of view of business decision makers. We propose an enterprise modeling approach to bridge the business-level understand-ing of the enterprise with its representations in databases and data warehouses. The business intelligence model (BIM) offers concepts familiar to business decision making—such as goals, strategies, processes, situations, inﬂuences, and indicators. Unlike many enterprise models which are meant to be used to derive, manage, or align with IT system imple-mentations, BIM aims to help business users organize and make sense of the vast amounts of data about the enterprise and its external environment. In this paper, we present core BIM concepts, focusing especially on reasoning about situ-ations, inﬂuences, and indicators. Such reasoning supports strategic analysis of business objectives in light of current enterprise data, allowing analysts to explore scenarios and ﬁnd alternative strategies. We describe how goal reasoning techniquesfromconceptualmodelingandrequirementsengi-neering have been applied to BIM. Techniques are also pro-vided to support reasoning with indicators linked to business metrics, including cases where speciﬁcations of indicators are incomplete. Evaluation of the proposed modeling and reasoning framework includes an on-going prototype imple-mentation, as well as case studies.",
        "keywords": [
            "Business intelligence",
            "Business model",
            "Conceptual modeling languages",
            "Inﬂuence diagrams",
            "Goal",
            "Situation",
            "Key performance indicators",
            "Strategic planning"
        ],
        "authors": [
            "Jennifer Horkoff",
            "Daniele Barone",
            "Lei Jiang",
            "Eric Yu",
            "Daniel Amyot",
            "Alex Borgida",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-012-0290-8.pdf"
    },
    {
        "title": "Generating instance models from meta models",
        "submission-date": "2007/04",
        "publication-date": "2008/07",
        "abstract": "Meta modeling is a wide-spread technique to deﬁne visual languages, with the UML being the most pro-minent one. Despite several advantages of meta modeling such as ease of use, the meta modeling approach has one disadvantage: it is not constructive, i.e., it does not offer a direct means of generating instances of the language. This disadvantage poses a severe limitation for certain applica-tions. For example, when developing model transformations, it is desirable to have enough valid instance models available for large-scale testing. Producing such a large set by hand is tedious. In the related problem of compiler testing, a string grammar together with a simple generation algorithm is typically used to produce words of the language automatically. In this paper, we introduce instance-generating graph grammars for creating instances of meta models, thereby overcoming the main deﬁcit of the meta modeling approach for deﬁning languages.",
        "keywords": [
            "Meta model",
            "UML",
            "Graph grammar",
            "Instance generation"
        ],
        "authors": [
            "Karsten Ehrig",
            "Jochen Malte Küster",
            "Gabriele Taentzer"
        ],
        "file_path": "data/sosym-all/s10270-008-0095-y.pdf"
    },
    {
        "title": "What is a process model composed of? A systematic literature review of meta-models in BPM",
        "submission-date": "2019/09",
        "publication-date": "2021/01",
        "abstract": "Business process modelling languages typically enable the representation of business process models by employing (graphical) symbols. These symbols can vary depending upon the verbosity of the language, the modelling paradigm, the focus of the language and so on. To make explicit different constructs and rules employed by a speciﬁc language, as well as bridge the gap across different languages, meta-models have been proposed in the literature. These meta-models are a crucial source of knowledge on what state-of-the-art literature considers relevant to describe business processes. The goal of this work is to provide the ﬁrst extensive systematic literature review (SLR) of business process meta-models. This SLR aims to answer research questions concerning: (1) the kind of meta-models proposed in the literature, (2) the recurring constructs they contain, (3) their purposes and (4) their evaluations. The SRL was performed manually considering papers automatically retrieved from reference paper repositories as well as proceedings of the main conferences in the Business Process Management research area. Sixty-ﬁve papers were selected and evaluated against four research questions. The results indicate the existence of a reasonable body of work conducted in this speciﬁc area, but not a full maturity. In particular, in answering the research questions several challenges have (re-)emerged for the Business Process Community, concerning: (1) the type of elements that constitute a Business Process and their meaning, (2) the absence of a (or several) reference meta-model(s) for the community, (3) the purpose for which meta-models are introduced in the literature and (4) a framework for the evaluation of the meta-models themselves. Moreover, the classiﬁcation framework devised to answer the four research questions can provide a reference structure for future descriptive categorizations.",
        "keywords": [
            "Business Process Modelling",
            "Conceptual Modelling",
            "Meta-Models"
        ],
        "authors": [
            "Greta Adamo",
            "Chiara Ghidini",
            "Chiara Di Francescomarino"
        ],
        "file_path": "data/sosym-all/s10270-020-00847-w.pdf"
    },
    {
        "title": "Handling nonconforming individuals in search-based model-driven engineering: nine generic strategies for feature location in the modeling space of the meta-object facility",
        "submission-date": "2019/02",
        "publication-date": "2021/03",
        "abstract": "Lately, the model-driven engineering community has been paying more attention to the techniques offered by the search-based software engineering community. However, even though the conformance of models and metamodels is a topic of great interest for the modeling community, the works that address model-related problems through the use of search metaheuristics are not taking full advantage of the strategies for handling nonconforming individuals. The search space can be huge when searching in model artifacts (magnitudes of around 10150 for models of 500 elements). By handling the nonconforming individuals, the search space can be drastically reduced. In this work, we present a set of nine generic strategies for handling nonconforming individuals that are ready to be applied to model artifacts. The strategies are independent from the application domain and only include constraints derived from the meta-object facility. In addition, we evaluate the strategies with two industrial case studies using an evolutionary algorithm to locate features in models. The results show that the use of the strategies presented can reduce the number of generations needed to reach the solution by 90% of the original value. Generic strategies such as the ones presented in this work could lead to the emergence of more complex ﬁtness functions for searches in models or even new applications for the search metaheuristics in model-related problems.",
        "keywords": [
            "Model-driven engineering (MDE)",
            "Search-based software engineering (SBSE)",
            "Feature location (FL)",
            "Evolutionary algorithm (EA)"
        ],
        "authors": [
            "Jaime Font",
            "Lorena Arcega",
            "Øystein Haugen",
            "Carlos Cetina"
        ],
        "file_path": "data/sosym-all/s10270-021-00870-5.pdf"
    },
    {
        "title": "Formal veriﬁcation of QVT transformations for code generation",
        "submission-date": "2012/03",
        "publication-date": "2013/06",
        "abstract": "We present a formal calculus for operational QVT. The calculus is implemented in the interactive theorem prover KIV and allows to prove properties of QVT transformations for arbitrary meta models. Additionally, we present a framework for provably correct Java code generation. The framework uses a meta model for a Java abstract syntax tree as the target of QVT transformations. This meta model is mapped to a formal Java semantics in KIV. This makes it possible to formally prove (interactively) with the QVT calculus that a transformation always generates a Java model (i.e. a program) that is type correct and has certain semanti-cal properties. The Java model can be used to generate source code by a model-to-text transformation or byte code directly.",
        "keywords": [
            "Correctness of model transformations",
            "QVT",
            "Formal veriﬁcation",
            "Interactive theorem proving"
        ],
        "authors": [
            "Kurt Stenzel",
            "Nina Moebius",
            "Wolfgang Reif"
        ],
        "file_path": "data/sosym-all/s10270-013-0351-7.pdf"
    },
    {
        "title": "Polymorphic scenario-based speciﬁcation models: semantics and applications",
        "submission-date": "2010/02",
        "publication-date": "2010/07",
        "abstract": "We present polymorphic scenarios, a generalization of a UML2-compliant variant of Damm and Harel’s live sequence charts (LSC) in the context of object-orientation. Polymorphic scenarios are visualized using (modal) sequence diagrams where lifelines may represent classes and interfaces rather than concrete objects. Their semantics takes advantage of inheritance and interface realization to allow the speciﬁcation of most expressive, succinct, and reusable universal and existential inter-object scenarios for object-oriented system models. We motivate the use of polymorphic scenarios, formally deﬁne their trace-based semantics, and present their application for scenario-based testing and execution, as implemented in the S2A compiler developed at the Weizmann Institute of Science. We further discuss advanced semantic issues arising from the use of scenarios in a polymorphic setting, suggest possible extensions, present a UML proﬁle to support polymorphic scenarios, consider the application of the polymorphic semantics to other variants of scenario-based speciﬁcation languages, and position our work in the broader context of behavioral subtyping.",
        "keywords": [
            "Live sequence charts",
            "UML interactions",
            "Sequence diagrams",
            "Polymorphism",
            "Scenario-based modeling",
            "Behavioral subtyping"
        ],
        "authors": [
            "Shahar Maoz"
        ],
        "file_path": "data/sosym-all/s10270-010-0168-6.pdf"
    },
    {
        "title": "Guest editorial to the special issue on MODELS 2011",
        "submission-date": "2013/03",
        "publication-date": "2013/03",
        "abstract": "For the past 14 years, the MODELS conference has been the premier venue for the exchange of innovative ideas and experiences of model-based approaches in the development of complex systems. The conference series covers all aspects of model-based development for software and systems engineering, including modeling languages, methods, tools, and their applications. MODELS is universally recognized as one of the top conferences in software engineering research with an acceptance rate averaging 20% in recent years. In 16–21 October 2011, the MODELS conference was held in Wellington, New Zealand. The conference received 167 papers of which 34 were accepted for presentation by the programme committee, resulting in an acceptance rate of roughly 20%. Research in software and system modeling is now a relatively mature ﬁeld.",
        "keywords": [],
        "authors": [
            "Jon Whittle",
            "Tony Clark"
        ],
        "file_path": "data/sosym-all/s10270-013-0331-y.pdf"
    },
    {
        "title": "Model-driven engineering for digital twins: a systematic mapping study",
        "submission-date": "2023/09",
        "publication-date": "2025/03",
        "abstract": "Digital twins (DTs) are proliferating in a multitude of domains, including agriculture, automotive, avionics, logistics, manu-facturing, medicine, smart homes, etc. As domain experts and software experts both have to contribute to the engineering of effective DTs, several model-driven engineering (MDE) approaches have been recently proposed to ease the design, develop-ment, and operation of DTs. However, the diversity of domains in which MDE is currently applied to DTs, as well as the diverse landscape of DTs and MDE applications to DTs, makes it challenging for researchers and practitioners to get an overview of what techniques and artifacts are already applied in this context. In this paper, we shed light on the aforementioned aspects by performing a systematic mapping study on the application of MDE automation techniques, i.e., model-to-model transfor-mation, code generation, and model interpretation, in the context of DTs as well as on the characteristics of DTs including the twinned systems to which these techniques are applied in different domains. We systematically retrieved a set of 189 unique publications, of which 66 were selected for further investigation in this paper. Our results indicate that the distribution of employed MDE techniques (136 applications of automation techniques) is balanced between the different techniques, but there are signiﬁcant variations for different DT types. With respect to the different domains, we found that even though applications are available in many domains, a small number of domains currently dominate applications of MDE to DTs, i.e., more than half of included papers are in the manufacturing and transportation domains.",
        "keywords": [
            "Model-driven engineering",
            "Digital twin",
            "Digital twin engineering",
            "Digital twin modeling languages",
            "Literature review"
        ],
        "authors": [
            "Daniel Lehner",
            "Jingxi Zhang",
            "Jérôme Pfeiffer",
            "Sabine Sint",
            "Ann-Kathrin Splettstößer",
            "Manuel Wimmer",
            "Andreas Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-025-01264-7.pdf"
    },
    {
        "title": "Introductory paper",
        "submission-date": "2004/11",
        "publication-date": "2004/11",
        "abstract": "A lot of problems which occur when developing (embed-ded) systems are currently caused by highly decoupled development activities. In the major disciplines such as mechanical engineering or function development, moreover, complex activities, e.g. geometric modelling, requirement management or system architecture develop-ment are performed. Specialised tools are in place for each of the various activities, most of them commercial oﬀthe shelf (COTS) tools. The variety of activities carried out by COTS tools as well as their lack of interoperability, discourage co-operation within development teams. This separation causes high development costs due to long feedback cycles and investments bound to tool-related development results. Quality is reduced by gaps within the development processes used and low interaction between development disciplines. As part of the solution, system development must be supported by a new, sophisticated means of integrating existing tools.",
        "keywords": [],
        "authors": [
            "Andy Schürr",
            "Heiko Dörr"
        ],
        "file_path": "data/sosym-all/s10270-004-0069-7.pdf"
    },
    {
        "title": "Holistic data-driven requirements elicitation in the big data era",
        "submission-date": "2021/04",
        "publication-date": "2021/10",
        "abstract": "Digital transformation stimulates continuous generation of large amounts of digital data, both in organizations and in society\nat large. As a consequence, there have been growing efforts in the Requirements Engineering community to consider digital\ndata as sources for requirements acquisition, in addition to human stakeholders. The volume, velocity and variety of the data\nmake requirements discovery increasingly dynamic, but also unstructured and complex, which current elicitation methods\nare unable to consider and manage in a systematic and efﬁcient manner. We propose a framework, in the form of a conceptual\nmetamodel and a method, for continuous and automated acquisition, analysis and aggregation of heterogeneous digital sources\nthat aims to support data-driven requirements elicitation and management. The usability of the framework is partially validated\nby an in-depth case study from the business sector of video game development.",
        "keywords": [
            "Data-driven requirements engineering",
            "Big data",
            "Requirements modeling",
            "Machine learning",
            "Natural language processing"
        ],
        "authors": [
            "Aron Henriksson",
            "Jelena Zdravkovic"
        ],
        "file_path": "data/sosym-all/s10270-021-00926-6.pdf"
    },
    {
        "title": "Advanced testing and debugging support for reactive executable DSLs",
        "submission-date": "2021/08",
        "publication-date": "2022/09",
        "abstract": "ExecutableDomain-SpeciﬁcLanguages (xDSLs) allowthedeﬁnitionandtheexecutionof behavioral models. Somebehavioral models arereactive, meaningthat duringtheir execution, theyaccept external events andreact byexposingevents totheexternal environment. Since complex interaction may occur between the reactive model and the external environment, they should be tested as early as possible to ensure the correctness of their behavior. In this paper, we propose a set of generic testing facilities for reactive xDSLs using the standardized Test Description Language (TDL). Given a reactive xDSL, we generate a TDL library enabling the domain experts to write and run event-driven TDL test cases for conforming reactive models. To further support the domain expert, the approach integrates interactive debugging to help in localizing defects, and mutation analysis to measure the quality of test cases. We evaluate the level of genericity of the approach by successfully writing, executing, and analyzing 247 event-driven TDL test cases for 70 models conforming to two different reactive xDSLs.",
        "keywords": [
            "Reactive executable DSL",
            "Testing",
            "Test description language",
            "Debugging",
            "Mutation analysis"
        ],
        "authors": [
            "Faezeh Khorram",
            "Erwan Bousse",
            "Jean-Marie Mottu",
            "Gerson Sunyé"
        ],
        "file_path": "data/sosym-all/s10270-022-01025-w.pdf"
    },
    {
        "title": "Domain object hierarchies inducing multi-level models",
        "submission-date": "2020/05",
        "publication-date": "2022/03",
        "abstract": "Conceptual modeling of domain object hierarchies, such as product hierarchies or organization hierarchies, is difﬁcult due to the intricate nature of nonphysical domain objects organized in such hierarchies. Modeling domain object hierarchies as part-whole hierarchies covers their hierarchical structure, yet to capture their meaning, part-whole hierarchies have to be combined with specialization and multi-level instantiation. To this end we introduce the deep domain object (DDO) multi-level modeling pattern and approach. With the DDO approach, subclasses and metaclasses are induced by and integrated with the part-whole hierarchy. The approach is aligned with the multi-level theory (MLT) and formalized by a metamodel and a set of deductive rules implemented in F-Logic. The proof-of-concept prototype is used for automated application of the pattern and for querying induced multi-level models.",
        "keywords": [
            "Conceptual modeling",
            "Multi-level modeling",
            "Metamodeling",
            "Part-whole",
            "Generalization",
            "Abstraction",
            "Concretization"
        ],
        "authors": [
            "Bernd Neumayr\nMichael Schreﬂ"
        ],
        "file_path": "data/sosym-all/s10270-022-00973-7.pdf"
    },
    {
        "title": "Evaluating the accessibility of a PoN-enabled misuse case notation by the red–green colorblind community",
        "submission-date": "2021/06",
        "publication-date": "2022/03",
        "abstract": "In 2015, an improved version of the misuse case modeling notation designed using the Physics of Notations (PoN) framework was proposed. Empirical data support that the new notation is more cognitively effective than the original notation. The new notation makes use of color, in particular, red and green, meaning that red–green colorblind community will not be able to view the notation as designed and intended. The cognitive effectiveness of the red–green deﬁcient (RGD) version of the new notation in comparison to the original notation is unknown. The PoN outlines a number of principles that can be satisﬁed with or without the use of color, but would the deﬁciency of color may be so inhibiting that the cognitive effectiveness superiority of the PoN-enabled misuse case notation becomes diminished? Perhaps the original use case notation would be more cognitively effective for red–green colorblind users. An empirical study using 84 IT (Information Technology) professionals is conducted to assess the cognitive effectiveness of the RGD version of the PoN-enabled misuse case notation in comparison to the original misuse case notation. The experiment data are analyzed for any statistically signiﬁcant ﬁndings. The quantitative and qualitative results of the experiment indicate that the RGD version of PoN-enabled misuse case notation maintains its cognitive effectiveness superiority over the original notation. The results also show that the subjects are divided in their opinions with respect to the aesthetic appeal of the two notations. Adhering to the complete set of principles outlined in the PoN has allowed the new notation to maintain its cognitive effectiveness superiority over the original notation despite a curtailed color perspective.",
        "keywords": [
            "Misuse case notation",
            "Visual syntax",
            "Cognitive effectiveness",
            "UML",
            "Red",
            "green colorblindness"
        ],
        "authors": [
            "Mohamed El-Attar"
        ],
        "file_path": "data/sosym-all/s10270-022-00992-4.pdf"
    },
    {
        "title": "Guest editorial to the special section on model transformation",
        "submission-date": "2013/10",
        "publication-date": "2015/01",
        "abstract": "Modeling is a key element in reducing the complexity of the development and maintenance of software systems. Software engineering paradigms like model-driven engineering (MDE) consider models as the primary elements in the software construction process. In this setting, model transformations are essential for elevating models from documentation elements to first-class artifacts of the development. Model transformation includes model-to-text transformation to generate code from models, text-to-model transformations to parse textual representations to model representations, model extraction to derive higher-level models from legacy code, and model-to-model transformations to normalize, weave, optimize, simulate, and refactor models, as well as to translate between modeling languages.",
        "keywords": [],
        "authors": [
            "Zhenjiang Hu",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-013-0388-7.pdf"
    },
    {
        "title": "Least privilege analysis in software architectures",
        "submission-date": "2010/11",
        "publication-date": "2011/11",
        "abstract": "Due to the lack of both precise definitions and effective software engineering methodologies, security design principles are often neglected by software architects, resulting in potentially high-risk threats to systems. This work lays the formal foundations for understanding the security design principle of least privilege in software architectures and provides a technique to identify violations against this principle. The technique can also be leveraged to analyze violations against the security design principle of separation of duties. The proposed approach is supported by tools and has been validated in four case studies, two of which are presented in detail in this paper.",
        "keywords": [
            "Security analysis",
            "Least privilege",
            "Software architecture"
        ],
        "authors": [
            "Koen Buyens",
            "Riccardo Scandariato",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-011-0218-8.pdf"
    },
    {
        "title": "MoDALAS: addressing assurance for learning-enabled autonomous systems in the face of uncertainty",
        "submission-date": "2022/03",
        "publication-date": "2023/03",
        "abstract": "Increasingly, safety-critical systems include artiﬁcial intelligence and machine learning components (i.e., learning-enabled components (LECs)). However, when behavior is learned in a training environment that fails to fully capture real-world phenomena, the response of an LEC to untrained phenomena is uncertain and therefore cannot be assured as safe. Automated methods are needed for self-assessment and adaptation to decide when learned behavior can be trusted. This work introduces a model-driven approach to manage self-adaptation of a learning-enabled system (LES) to account for run-time contexts for which the learned behavior of LECs cannot be trusted. The resulting framework enables an LES to monitor and evaluate goal models at run time to determine whether or not LECs can be expected to meet functional objectives and enables system adaptation accordingly. Using this framework enables stakeholders to have more conﬁdence that LECs are used only in contexts comparable to those validated at design time.",
        "keywords": [
            "Goal-based modeling",
            "Self-adaptive systems",
            "Artiﬁcial intelligence",
            "Machine learning",
            "Models at run time",
            "Cyber physical systems",
            "Behavior oracles",
            "Autonomous vehicles"
        ],
        "authors": [
            "Michael Austin Langford",
            "Kenneth H. Chan",
            "Jonathon Emil Fleck",
            "Philip K. McKinley",
            "Betty H.C. Cheng"
        ],
        "file_path": "data/sosym-all/s10270-023-01090-9.pdf"
    },
    {
        "title": "Theme section of BPMDS’2014: the human perspective in business processes",
        "submission-date": "2016/11",
        "publication-date": "2017/01",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a two-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of Information Systems theory. This theme section follows the 15th edition of the BPMDS (Business Process Modeling, Development, and Support) series, organized in conjunction with CAISE’14, which was held in Thessaloniki, Greece, June 2014. BPMDS’2014 received 48 submissions from 23 countries, and 20 papers were selected and published in Springer LNBIP 175 volume.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-016-0570-9.pdf"
    },
    {
        "title": "Holistic security requirements analysis for socio-technical systems",
        "submission-date": "2015/10",
        "publication-date": "2016/09",
        "abstract": "Security has been a growing concern for large organizations, especially ﬁnancial and governmental institutions, as security breaches in the systems they depend on have repeatedlyresultedinbillionsofdollarsinlossesperyear,and this cost is on the rise. A primary reason for these breaches is that the systems in question are “socio-technical” a mix of people, processes, technology, and infrastructure. However, suchsystemsaredesignedinapiecemealratherthanaholistic fashion, leaving parts of the system vulnerable. To tackle this problem, we propose a three-layer security analysis framework consisting of a social layer (business processes, social actors), a software layer (software applications that support the social layer), and an infrastructure layer (physical and technological infrastructure). In our proposal, global security requirements lead to local security requirements, cutting across conceptual layers, and upper-layer security analysis inﬂuences analysis at lower layers. Moreover, we propose a set of analytical methods and a systematic process that together drive security requirements analysis across the three layers. To support analysis, we have deﬁned corresponding inference rules that (semi-)automate the analysis, helping to deal with system complexity. A prototype tool has been implemented to support analysts throughout the analysis process. Moreover, we have performed a case study on a real-world smart grid scenario to validate our approach.",
        "keywords": [
            "Security requirements",
            "Goal model",
            "Enterprise architecture",
            "Socio-technical system",
            "Security pattern"
        ],
        "authors": [
            "Tong Li",
            "Jennifer Horkoff",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-016-0560-y.pdf"
    },
    {
        "title": "Road to a reactive and incremental model transformation platform: three generations of the VIATRA framework",
        "submission-date": "2016/03",
        "publication-date": "2016/05",
        "abstract": "The current release of VIATRA provides open-\nsource tool support for an event-driven, reactive model\ntransformation engine built on top of highly scalable incre-\nments graph queries for models with millions of elements\nand advanced features such as rule-based design space\nexploration complex event processing or model obfuscation.\nHowever, the history of the VIATRA model transformation\nframework dates back to over 16years. Starting as an early\nacademic research prototype as part of the M.Sc project of\nthe the ﬁrst author it ﬁrst evolved into a Prolog-based engine\nfollowed by a family of open-source projects which by now\nmatured into a component integrated into various industrial\nand open-source tools and deployed over multiple technolo-\ngies. This invited paper brieﬂy overviews the evolution of\nthe VIATRA/IncQuery family by highlighting key features\nand illustrating main transformation concepts along an open\ncase study inﬂuenced by an industrial project.",
        "keywords": [
            "Model transformations",
            "Incremental evaluation",
            "Reactive transformations",
            "Graph queries"
        ],
        "authors": [
            "Dániel Varró",
            "Gábor Bergmann",
            "Ábel Hegedüs",
            "Ákos Horváth",
            "István Ráth",
            "Zoltán Ujhelyi"
        ],
        "file_path": "data/sosym-all/s10270-016-0530-4.pdf"
    },
    {
        "title": "A novel model-based testing approach for software product lines",
        "submission-date": "2013/11",
        "publication-date": "2016/02",
        "abstract": "Model-based testing relies on a model of the sys-tem under test. FineFit is a framework for model-based testing of Java programs. In the FineFit approach, the model is expressed by a set of tables based on Parnas tables. A software product line is a family of programs (the products) with well-deﬁned commonalities and variabilities that are developed by (re)using common artifacts. In this paper, we address the issue of using the FineFit approach to support the development of correct software product lines. We specify a software product line as a speciﬁcation product line where each product is a FineFit speciﬁcation of the corresponding software product. The main challenge is to concisely specify the software product line while retaining the readability of the speciﬁcation of a single system. To address this, we used delta-oriented programming, a recently proposed ﬂexible approach for implementing software product lines, and developed: (1) delta tables as a means to apply the delta-oriented programming idea to the speciﬁcation of software product lines; and (2) DeltaFineFit as a novel model-based testing approach for software product lines.",
        "keywords": [
            "Java",
            "Alloy",
            "Software product line",
            "Delta-oriented programming",
            "Model-based testing",
            "Reﬁnement"
        ],
        "authors": [
            "Ferruccio Damiani",
            "David Faitelson",
            "Christoph Gladisch",
            "Shmuel Tyszberowicz"
        ],
        "file_path": "data/sosym-all/s10270-016-0516-2.pdf"
    },
    {
        "title": "Automatic generation of built-in contract test drivers",
        "submission-date": "2011/06",
        "publication-date": "2012/09",
        "abstract": "Automatic generation of platform-independent and -dependent built-in contract test drivers that check pair-wise interactions between client and server components is presented, focusing on the built-in contract testing (BIT) method and the model-driven testing approach. Components are speciﬁed by UML diagrams that deﬁne the contract between client and server, independent of a speciﬁc platform. MDA approaches are applied to formalize and perform automatic transformations from a platform-independent model to a platform-independent test architecture according to a BIT proﬁle. The test architecture is mapped to Java platform models and then to test code. All these transformations are speciﬁed by a set of transformation rules written in the Atlas Transformation Language (ATL) that are automatically performed by the ATL engine. The solution named the MoBIT tool is applied to case studies in order to investigate the expected beneﬁts and challenges to be faced.",
        "keywords": [
            "Model-driven testing",
            "Built-in contract testing",
            "MDA"
        ],
        "authors": [
            "Everton L. G. Alves",
            "Patricia D. L. Machado",
            "Franklin Ramalho"
        ],
        "file_path": "data/sosym-all/s10270-012-0282-8.pdf"
    },
    {
        "title": "Improving the SAT modulo ODE approach to hybrid systems analysis by combining different enclosure methods",
        "submission-date": "2012/03",
        "publication-date": "2012/11",
        "abstract": "Aiming at automatic veriﬁcation and analy- sis techniques for hybrid discrete-continuous systems, we present a novel combination of enclosure methods for ordi- nary differential equations (ODEs) with the iSAT solver for large Boolean combinations of arithmetic constraints. Improving on our previous work, the contribution of this paper lies in combining iSAT with VNODE-LP, as a state-of-the-art interval solver for ODEs, and with bracketing systems, which exploit monotonicity properties allowing to ﬁnd enclosures for problems that VNODE-LP alone cannot enclose tightly. We apply the combined iSAT-ODE solver to the analysis of a variety of non-linear hybrid systems by solving predicative encodings of reachability properties and of an inductive stability argument, and evaluate the impact of the different enclosure methods, decision heuristics and their combination. Our experiments include classic benchmarks A preliminary version of this paper appeared in [6]. This work has been supported by the German Research Council DFG within SFB/TR 14 “http://www.avacs.org”, by the French National Research Agency under contract ANR 2011 INS 006 04 “http://projects.laas.fr/ ANR-MAGIC-SPS”, and by the Natural Sciences and Engineering Research Council of Canada. from the literature, as well as a newly-designed conveyor belt system that combines hybrid behavior of parallel com- ponents, a slip-stick friction model with non-linear dynamics and ﬂow invariants and several dimensions of parameteriza- tion. In the paper, we also present and evaluate an extension of VNODE-LP tailored to its use as a deduction mechanism within iSAT-ODE, to allow fast re-evaluations of enclosures over arbitrary subranges of the analyzed time span.",
        "keywords": [
            "Analysis of hybrid discrete-continuous systems",
            "Satisﬁability modulo theories",
            "Enclosure methods for ODEs",
            "Bracketing systems"
        ],
        "authors": [
            "Andreas Eggers",
            "Nacim Ramdani",
            "Nedialko S. Nedialkov",
            "Martin Fränzle"
        ],
        "file_path": "data/sosym-all/s10270-012-0295-3.pdf"
    },
    {
        "title": "Safety behavior abstraction and model evolution in autonomous driving",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "In the autonomous driving systems (ADSs) literature, most existing approaches primarily focus on identifying driving scenar-ios, which is challenged by the reality that real-world driving scenarios are countless and unpredictable, and it is impossible to have a comprehensive set of driving scenarios to demonstrate the test efﬁciency in covering all possible situations ADSs might face. To address these challenges, one fundamental step is to abstract complex ADS behaviors, e.g., (semi-)automatically derive a holistic view of how an ADS behaves under its driving environment with high-level representations, such as prior-knowledge-based models. Therefore, in this paper, we propose a novel Risk-basEd Model comprehension and Evolution approach for autonomous Driving sYstems, named REMEDY, which facilitates the development of such models and enables automated model evolution (i.e., discovering and extracting ADS behaviors and their interactions with the environment) via model execution and simulation with the autonomous driving simulator CARLA. To enable efﬁcient model evolution, we also equipped REMEDY with a risk-based strategy using Q-Learning, which is empirically evaluated by comparing it with three baselines (i.e., a random strategy, a coverage-based greedy strategy, and DeepCollision—a state-of-the-art approach). Results show that REMEDY is capable of discovering new and diverse behaviors, and the risk-based strategy is efﬁcient in discovering risky ADS behaviors.",
        "keywords": [
            "Autonomous driving",
            "Model evolution",
            "Model execution"
        ],
        "authors": [
            "Chao Tan",
            "Tiexin Wang",
            "Man Zhang",
            "Tao Yue"
        ],
        "file_path": "data/sosym-all/s10270-024-01261-2.pdf"
    },
    {
        "title": "Communicating the variability of a software-product family to customers",
        "submission-date": "2002/10",
        "publication-date": "2003/02",
        "abstract": "Variability is a central concept in software product family development. Variability empowers constructive reuse and facilitates the derivation of different, customer specific products from the product family. If many customer specific requirements can be realised by exploiting the product family variability, the reuse achieved is obviously high. If not, the reuse is low. It is thus important that the variability of the product family is adequately considered when eliciting requirements from the customer.\n\nIn this paper we sketch the challenges for requirements engineering for product family applications. More precisely we elaborate on the need to communicate the variability of the product family to the customer. We differentiate between variability aspects which are essential for the customer and aspects which are more related to the technical realisation and need thus not be communicated to the customer. Motivated by the successful usage of use cases in single product development we propose use cases as communication medium for the product family variability. We discuss and illustrate which customer relevant variability aspects can be represented with use cases, and for which aspects use cases are not suitable. Moreover we propose extensions to use case diagrams to support an intuitive representation of customer relevant variability aspects.",
        "keywords": [
            "Variability",
            "Product family",
            "UML",
            "Use cases",
            "Requirements engineering"
        ],
        "authors": [
            "G¨unter Halmans",
            "Klaus Pohl"
        ],
        "file_path": "data/sosym-all/s10270-003-0019-9.pdf"
    },
    {
        "title": "Meta3: a code generator framework for domain-speciﬁc languages",
        "submission-date": "2017/04",
        "publication-date": "2018/03",
        "abstract": "In software development, domain-speciﬁc languages (DSLs) are often applied for speciﬁc or repetitive tasks. For executable DSLs, a model interpreter can be developed to run DSL programs. Nevertheless, it is more widespread to generate code in a general-purpose programming language. A properly chosen DSL expresses the original problem more naturally for both domain and information technology experts, and thus, this approach makes the whole development process, especially requirements engineering and requirements analysis, more efﬁcient and less prone to human errors. There are code generator frameworks and so-called language workbenches available that make the development of code generators for DSLs easier. In this paper, we report on our own code generator framework, called Meta3. Meta3 is based on our code generator development experience. We believe that this experience report will be useful for developers of code generators and language workbenches interested in building more ﬂexible and robust code generators as well as better tools that support the construction of the latter.",
        "keywords": [
            "Domain-speciﬁc modeling",
            "Code generation",
            "Architecture"
        ],
        "authors": [
            "Gábor Kövesdán",
            "László Lengyel"
        ],
        "file_path": "data/sosym-all/s10270-018-0673-6.pdf"
    },
    {
        "title": "Ontologies for ﬁnding journalistic angles",
        "submission-date": "2019/10",
        "publication-date": "2020/06",
        "abstract": "Journalism relies more and more on information and communication technology (ICT). ICT-based journalistic knowledge platforms continuously harvest potentially news-relevant information from the Internet and make it useful for journalists. Because information about the same event is available from different sources and formats vary widely, knowledge graphs are emerging as a preferred technology for integrating, enriching, and preparing information for journalistic use. The paper explores how journalistic knowledge graphs can be augmented with support for news angles, which can help journalists to detect newsworthy events and make them interesting for the intended audience. We argue that ﬁnding newsworthy angles on news-related information is an important example of a topical problem in information science: that of detecting interesting events and situations in big data sets and presenting those events and situations in interesting ways.",
        "keywords": [
            "Computational journalism",
            "Data journalism",
            "Journalistic knowledge platforms",
            "News angles",
            "Knowledge graphs",
            "Ontology"
        ],
        "authors": [
            "Andreas L. Opdahl",
            "Bjørnar Tessem"
        ],
        "file_path": "data/sosym-all/s10270-020-00801-w.pdf"
    },
    {
        "title": "A UML-based quantitative framework for early prediction of resource usage and load in distributed real-time systems",
        "submission-date": "2006/06",
        "publication-date": "2008/08",
        "abstract": "This paper presents a quantitative framework for early prediction of resource usage and load in distributed real-time systems (DRTS). The prediction is based on an analysis of UML 2.0 sequence diagrams, augmented with timing information, to extract timed-control ﬂow information. It is aimed at improving the early predictability of a DRTS by offering a systematic approach to predict, at the design phase, system behavior in each time instant during its execution. Since behavioral models such as sequence diagrams are available in early design phases of the software life cycle, the framework enables resource analysis at a stage when design decisions are still easy to change. Though we provide a general framework, we use network trafﬁc as an example resource type to illustrate how the approach is applied. We also indicate how usage and load analysis of other types of resources (e.g., CPU and memory) can be performed in a similar fashion. A case study illustrates the feasibility of the approach.",
        "keywords": [
            "Resource usage prediction",
            "Load analysis",
            "Load forecasting",
            "Resource overuse detection",
            "Real-time systems",
            "Distributed systems",
            "UML"
        ],
        "authors": [
            "Vahid Garousi",
            "Lionel C. Briand",
            "Yvan Labiche"
        ],
        "file_path": "data/sosym-all/s10270-008-0099-7.pdf"
    },
    {
        "title": "Synthesizing object life cycles from business process models",
        "submission-date": "2013/11",
        "publication-date": "2014/05",
        "abstract": "Uniﬁed modeling language (UML) activity diagrams can model the ﬂow of stateful business objects among activities, implicitly specifying the life cycles of those objects. The actual object life cycles are typically expressed in UML state machines. The implicit life cycles in UML activity diagrams need to be discovered in order to derive the actual object life cycles or to check the consistency with an existing life cycle. This paper presents an automated approach for synthesizing a UML state machine modeling the life cycle of an object that occurs in different states in a UML activity diagram. The generated state machines can contain parallelism, loops, and cross-synchronization. The approach makes life cycles that have been modeled implicitly in activity diagrams explicit. The synthesis approach has been implemented using a graph transformation tool and has been applied in several case studies.",
        "keywords": [
            "Process models",
            "State machines",
            "UML",
            "Model transformation"
        ],
        "authors": [
            "Rik Eshuis",
            "Pieter Van Gorp"
        ],
        "file_path": "data/sosym-all/s10270-014-0406-4.pdf"
    },
    {
        "title": "Software engineering and formal methods: SEFM 2019 special section",
        "submission-date": "2021/02",
        "publication-date": "2021/03",
        "abstract": "This special section of Software and Systems Modeling contains extended versions of selected papers from the 17th International Conference on Software Engineering and Formal Methods (SEFM), which was held in Oslo, Norway, in September 2019. SEFM 2019 was the seventeenth edition of an annual series of conferences that aims at bringing together leading researchers and practitioners from academia and industry to advance the state of the art in formal methods, to facilitate their uptake in the software industry, and to encourage their integration within practical software engineering methods and tools.",
        "keywords": [],
        "authors": [
            "Peter Csaba Ölveczky",
            "Gwen Salaün"
        ],
        "file_path": "data/sosym-all/s10270-021-00874-1.pdf"
    },
    {
        "title": "Efﬁcient parallel reasoning on fuzzy goal models for run time requirements veriﬁcation",
        "submission-date": "2015/07",
        "publication-date": "2016/09",
        "abstract": "As software applications become highly inter-connected in dynamically provisioned platforms, they form the so-called systems-of-systems. Therefore, a key issue that arises in such environments is whether speciﬁc requirements are violated, when these applications interact in new unforeseen ways as new resources or system components are dynamically provisioned. Such environments require the continuous use of frameworks for assessing compliance against speciﬁc mission critical system requirements. Such frameworks should be able to (a) handle large requirements models, (b) assess system compliance repeatedly and frequently using events from possibly high velocity and high frequency data streams, and (c) use models that can reﬂect the vagueness that inherently exists in big data event collection and in modeling dependencies between components of complex and dynamically re-conﬁgured systems. In this paper, we introduce a framework for run time reasoning over medium and large-scale fuzzy goal models, and we propose a process which allows for the parallel evaluation of such models. The approach has been evaluated for time and space performance on large goal models, exhibiting that in a simulation environment, the parallel reasoning process offers signiﬁcant performance improvement over a sequential one.",
        "keywords": [
            "Goal models reasoning",
            "Fuzzy reasoning",
            "Parallel reasoning",
            "Software systems",
            "Veriﬁcation"
        ],
        "authors": [
            "George Chatzikonstantinou",
            "Kostas Kontogiannis"
        ],
        "file_path": "data/sosym-all/s10270-016-0562-9.pdf"
    },
    {
        "title": "Enhancing classic transformation languages to support multi-level modeling",
        "submission-date": "2012/10",
        "publication-date": "2013/10",
        "abstract": "As practical tools for disciplined multi-level modeling have begun to mature, the problem of supporting simple and efﬁcient transformations to-and-from multi-level models to facilitate interoperability has assumed grow-ing importance. The challenge is not only to support efﬁ-cient transformations between multi-level models, but also between multi-level and two-level model content represented in traditional modeling infrastructures such as the UML and programming languages. Multi-level model content can already be accessed by traditional transformation languages such as ATL and QVT, but in a way that is blind to the onto-logical classiﬁcation information they contain. In this paper, we present an approach for making rule-based transforma-tion languages “multi-level aware” so that the semantics of ontological classiﬁcation as well as linguistic classiﬁcation can be exploited when writing transformations.",
        "keywords": [
            "Multi-level transformation",
            "Orthogonal classiﬁcation architecture",
            "Ontological classiﬁcation",
            "Linguistic classiﬁcation"
        ],
        "authors": [
            "Colin Atkinson",
            "Ralph Gerbig",
            "Christian Vjekoslav Tunjic"
        ],
        "file_path": "data/sosym-all/s10270-013-0384-y.pdf"
    },
    {
        "title": "A framework for conceptual characterization of ontologies and its application in the cybersecurity domain",
        "submission-date": "2021/03",
        "publication-date": "2022/07",
        "abstract": "Organizations are actively seeking efﬁcient solutions for the management and protection of their assets. However, Cyber-security is a vast and complex domain, especially for large enterprises because it requires an interdisciplinary approach. Knowledge Graphs are one of the mechanisms that organizations use to explore security among assets and possible attacks. The grounding of concepts is fundamental to implementing Knowledge Graphs, and it is one of the most relevant ontology applications. Therefore, Cybersecurity Ontologies have emerged as an important research subject. The ﬁrst contribution of this paper is a search for previously existing works that have deﬁned Cybersecurity Ontologies. We found twenty-eight ontologies in this search. Based on this result, we propose a Cybersecurity Terminological Validation and a Framework for Classifying Ontologies. Then, we provide a cross-analysis of these two proposals and present a proposal of best practices for improving the ontological approach in the cybersecurity domain. We also discuss the impact of this proposal with regard to the Ontology Engineering process. Our goal is to provide a solution that meets the organization’s needs in terms of Cybersecurity and to contribute to Ontology Engineering research.",
        "keywords": [
            "Conceptual modeling",
            "Ontology classiﬁcation",
            "Cybersecurity ontology",
            "Ontology"
        ],
        "authors": [
            "Beatriz Franco Martins",
            "Lenin Javier Serrano Gil",
            "José Fabián Reyes Román",
            "José Ignacio Panach",
            "Oscar Pastor",
            "Moshe Hadad",
            "Benny Rochwerger"
        ],
        "file_path": "data/sosym-all/s10270-022-01013-0.pdf"
    },
    {
        "title": "Pragmatic reuse for DSML development\nComposing a DSL for hybrid CPS modeling",
        "submission-date": "2019/10",
        "publication-date": "2020/10",
        "abstract": "By bridging the semantic gap, domain-speciﬁc language (DSLs) serve an important role in the conquest to allow domain\nexperts to model their systems themselves. In this publication we present a case study of the development of the Continuous\nREactive SysTems language (CREST), a DSL for hybrid systems modeling. The language focuses on the representation\nof continuous resource ﬂows such as water, electricity, light or heat. Our methodology follows a very pragmatic approach,\ncombining the syntactic and semantic principles of well-known modeling means such as hybrid automata, data-ﬂow languages\nand architecture description languages into a coherent language. The borrowed aspects have been carefully combined and\nformalised in a well-deﬁned operational semantics. The DSL provides two concrete syntaxes: CREST diagrams, a graphical\nlanguage that is easily understandable and serves as a model basis, and crestdsl, an internal DSL implementation that\nsupports rapid prototyping—both are geared towards usability and clarity. We present the DSL’s semantics, which thoroughly\nconnect the various language concerns into an executable formalism that enables sound simulation and formal veriﬁcation in\ncrestdsl, and discuss the lessons learned throughout the project.",
        "keywords": [
            "Cyber-physical systems",
            "Domain-speciﬁc language",
            "Modeling",
            "Simulation",
            "Veriﬁcation"
        ],
        "authors": [
            "Stefan Klikovits",
            "Didier Buchs"
        ],
        "file_path": "data/sosym-all/s10270-020-00831-4.pdf"
    },
    {
        "title": "Object-oriented design: A goal-driven and pattern-based approach",
        "submission-date": "2006/09",
        "publication-date": "2007/07",
        "abstract": "In recent years, the inﬂuences of design patterns on software quality have attracted increasing attention in the area of software engineering, as design patterns encapsulate valuable knowledge to resolve design problems, and more importantly to improve the design quality. One of the key challenges in object-oriented design is how to apply appropriate design patterns during the system development. In this paper, design pattern is analyzed from different perspectives to see how it can facilitate design activities, handle non-functional requirement, solve design problems and resolve design conﬂicts. Based on the analysis, various kinds of applicability of design patterns are explored and integrated with a goal-driven approach to guiding developers to construct the object-oriented design model in a systematic manner. There are three beneﬁts to the proposed approach: making it easy to meet requirements, helping resolve design conﬂicts, and facilitating improvement of the design quality.",
        "keywords": [
            "Design pattern",
            "Object-oriented design",
            "Software quality",
            "Goal-driven requirement engineering"
        ],
        "authors": [
            "Nien-Lin Hsueh",
            "Jong-Yih Kuo",
            "Ching-Chiuan Lin"
        ],
        "file_path": "data/sosym-all/s10270-007-0063-y.pdf"
    },
    {
        "title": "Software engineering and formal methods",
        "submission-date": "2008/06",
        "publication-date": "2008/06",
        "abstract": "Formal methods, i.e., technologies for the formal description, construction, analysis, and validation of software—mostly based on logics and formal reasoning—have matured and can be expected to complement and partly replace traditional software engineering methods in the future. The field has outgrown the area of academic case studies, and industry is showing serious interest. The challenge now is to scale up the application of formal methods in software industry and to encourage their integration with practical engineering methods. This challenge is met by the conference series Software Engineering and Formal Methods (SEFM) (http://sefm.iist.unu.edu), which aims to bring together practitioners and researchers from academia, industry and government to advance the state of the art in formal methods and their integration in the software development process.",
        "keywords": [],
        "authors": [
            "Bernhard Aichernig",
            "Bernhard Beckert"
        ],
        "file_path": "data/sosym-all/s10270-008-0091-2.pdf"
    },
    {
        "title": "Broadcast psi-calculi with an application to wireless protocols",
        "submission-date": "2012/03",
        "publication-date": "2013/11",
        "abstract": "Psi-calculi is a parametric framework for the extensions of pi-calculus, with arbitrary data structures and logical assertions for facts about data. In this paper we add primitives for broadcast communication in order to model wireless protocols. The additions preserve the purity of the psi-calculi semantics, and we formally prove the standard congruence and structural properties of bisimilarity. We demonstrate the expressive power of broadcast psi-calculi by modelling the wireless ad hoc routing protocol LUNAR and verifying a basic reachability property.",
        "keywords": [
            "Psi-calculus",
            "Broadcast communication",
            "Bisimulation",
            "Ad hoc routing protocol"
        ],
        "authors": [
            "Johannes Borgström",
            "Shuqin Huang",
            "Magnus Johansson",
            "Palle Raabjerg",
            "Björn Victor",
            "Johannes Åman Pohjola",
            "Joachim Parrow"
        ],
        "file_path": "data/sosym-all/s10270-013-0375-z.pdf"
    },
    {
        "title": "Guest editorial to the theme section on Trends in Enterprise Architecture Research",
        "submission-date": "2024/03",
        "publication-date": "2024/04",
        "abstract": "This theme section on Trends in Enterprise Architecture Research originated at the 2022 TEAR workshop. Together with a general call for papers, an invitation to submit papers to this theme section was extended to the authors of the TEAR 2022 workshop papers. The international TEAR series brings together Enterprise Architecture (EA) researchers and provides a forum to present EA research results and to discuss future EA research directions. We received a total of 11 intentions to submit, from which six submissions materialized and went into review. After a thorough reviewing process, two papers were finally selected for inclusion into the theme section.",
        "keywords": [],
        "authors": [
            "Sybren de Kinderen",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-024-01165-1.pdf"
    },
    {
        "title": "A Model-based approach for the synthesis of software to firmware adapters for use with automatically generated components",
        "submission-date": "2015/05",
        "publication-date": "2016/05",
        "abstract": "This paper presents the MDE process in use at Elettronica SpA (ELT) for the development of complex embedded systems integrating software and firmware. The process is based on the adoption of SysML as the system-level modeling language and the use of Simulink for the refinement of selected subsystems. Implementations are generated automatically for both the software (C++ code) and firmware parts, and communication adapters are automatically generated from SysML using a dedicated profile and open-source tools for modeling and code generation. The process starts from a SysML system model, developed according to the platform-based design paradigm, in which a functional model of the system is paired to a model of the executionplatform.SubsystemsarereﬁnedasSimulinkmod- els or hand-coded in C++. An implementation for Simulink models is generated as software code or firmware on FPGA. Based on the SysML system architecture specification, our framework drives the generation of Simulink models with consistent interfaces, allows the automatic generation of the communication code among all subsystems (including the HW–FW interface code). In addition, it provides for the automatic generation of connectors for system-level simulation and of test harnesses and mockups to ease the integration and verification stage. We provide early results on the time savings obtained by using these technologies in the development process.",
        "keywords": [
            "Systemengineering",
            "Model-drivenarchitecture",
            "Model-based design",
            "Platform-based design",
            "Automatic code generation"
        ],
        "authors": [
            "Marco Di Natale",
            "David Perillo",
            "Francesco Chirico",
            "Andrea Sindico",
            "Alberto Sangiovanni-Vincentelli"
        ],
        "file_path": "data/sosym-all/s10270-016-0534-0.pdf"
    },
    {
        "title": "Systematic stereotype usage",
        "submission-date": "2003/08",
        "publication-date": "2003/08",
        "abstract": "As one of the UML’s main extension mechan-isms, stereotypes play a crucial role in the UML’s ability to serve a wide and growing base of users. However, the precise meaning of stereotypes and their intended mode of use has never been entirely clear and has even gener-ated signiﬁcant debate among experts. Two basic ways of using UML stereotypes have been observed in practice: One to support the classiﬁcation of classes as a means of emulating metamodel extensions, the other to support the classiﬁcation of objects as a means of assigning them certain properties. In this paper we analyze these two recognized stereotype usage scenarios and explain the rationale for explicitly identifying a third form of usage sce-nario. We propose some notational concepts which could be used to explicitly distinguish the three usage scenar-ios and provide heuristics as to when each should be used. Finally, we conclude by proposing enhancements to the UML which could support all three forms cleanly and concisely.",
        "keywords": [
            "UML extension mechanism",
            "Stereotypes",
            "Classiﬁcation",
            "Transitive properties"
        ],
        "authors": [
            "Colin Atkinson",
            "Thomas K¨uhne",
            "Brian Henderson-Sellers"
        ],
        "file_path": "data/sosym-all/s10270-003-0027-9.pdf"
    },
    {
        "title": "Toward a methodology for case modeling",
        "submission-date": "2018/11",
        "publication-date": "2019/11",
        "abstract": "Case management is increasingly used to capture and enact ﬂexible, knowledge-intensive processes in organizations. None of\ntheexistingcasemanagement approaches provides amethodologyfor casemodel elicitationandmodeling. Inthis contribution,\nthree modeling methods for fragment-based case management are presented: one which focuses on the control-ﬂow view, \nthe process-ﬁrst method, one which has a data-centric view, the object lifecycle-ﬁrst method, and one which focuses on \nthe goals of a case, the goals-ﬁrst method. Following the design science process, each of the three methods was evaluated in\ntwo case modeling workshops with two different stakeholder groups (PhD students and secretaries), resulting in a total of\nsix workshops. All participants were novices in case management and most of them as well in process modeling. The results\nindicate that the process-ﬁrst method can be quickly learned by novices and it might be useful for scenarios where the focus\nis on the main process with some degree of ﬂexibility. The object lifecycle-ﬁrst method yields more ﬂexible and consistent\ncase models, but requires a higher initial modeling effort, as the lifecycle of the main case object has to be designed ﬁrst. The\ngoals-ﬁrst method leads to a detailed and consistent case model and additionally provides, by means of the deﬁned goals, a\nchecklist what needs to be done for a case. This method requires in addition to the process modeling notation another model\ntype, the goal hierarchy, and therefore is less suited for novice modelers, as found by the workshop results.",
        "keywords": [
            "Case management",
            "Goal modeling",
            "Object lifecycle",
            "Process elicitation",
            "Process modeling",
            "t.BPM"
        ],
        "authors": [
            "Marcin Hewelt",
            "Luise Pufahl",
            "Sankalita Mandal",
            "Felix Wolﬀ",
            "Mathias Weske"
        ],
        "file_path": "data/sosym-all/s10270-019-00766-5.pdf"
    },
    {
        "title": "Domain speciﬁc modeling",
        "submission-date": "2005/01",
        "publication-date": "2005/01",
        "abstract": "Looking at other engineering disciplines, it is evident that modeling is a vital and important part of the development of complex artifacts. Good modeling techniques provide support for the separation of concerns principle, rigorous analysis of designs, and for structuring construction ac-tivities. Models can play a similar role in the development of software-based systems. Furthermore, the software de-velopment activity has a characteristic not present in physical engineering: deliverable products (software sys-tems) can be generated from models (an observation made by Bran Selic, IBM/Rational at an MDA summer school). This characteristic can and should be exploited in our quest to make software development an engineering activity.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-005-0078-1.pdf"
    },
    {
        "title": "The 9th annual state of SoSyM report",
        "submission-date": "2010/11",
        "publication-date": "2010/11",
        "abstract": "Another year has gone by and we are happy to report that the International Journal on Software and Systems Modeling (SoSyM) continues to do well in terms of a steady stream of submissions, quality of published papers, and average review turnaround time. Of particular significance is the first release of SoSyM’s impact factor, which, notably, is 1.533. As we have done on previous anniversaries, we take this opportunity to give a “state of the journal” report and to acknowledge the reviewers, editors, and publication staff that have contributed to the journal’s very good performance in the past year.",
        "keywords": [],
        "authors": [
            "Marco Aiello",
            "Ian Alexander",
            "Daniel Amyot",
            "Sven Apel",
            "Joao Araujo",
            "Colin Atkinson",
            "Joanne Atlee",
            "Orlando Avila",
            "Franck Barbier",
            "Luciano Baresi",
            "Balbir Barn",
            "Benoit Baudry",
            "Jorn Bettin",
            "Brian Blake",
            "Mireille Blay-Fornarino",
            "Christoph Bockisch",
            "Peter Bollen",
            "Behzad Bordbar",
            "Artur Boronat",
            "Jonathan Bowen",
            "Gyrd Brandeland",
            "Christoph Brandt",
            "Lionel Briand",
            "Phil Brooke",
            "Achim D. Brucker",
            "Jordi Cabot",
            "Artur Caetano",
            "Daniela Cancila",
            "Sven Castelyn",
            "Walter Cazzola",
            "María Victoria Cengarle",
            "Joanna Chimiak-Opoka",
            "Wei-Ngan Chin",
            "Tony Clark",
            "Manuel Clavel",
            "Sholom Cohen",
            "Philippe Collet",
            "Sara Comai",
            "Vittorio Cortellessa",
            "Fabio Costa",
            "Jorge R. Cuellar",
            "Guglielmo De Angelis",
            "Juan de Lara",
            "Olga De Troyer",
            "Serge Demeyer",
            "Ewen Denney",
            "Dirk Deridder",
            "Davide Di Ruscio",
            "Ada Diaconescu",
            "Oscar Díaz",
            "Juergen"
        ],
        "file_path": "data/sosym-all/s10270-010-0182-8.pdf"
    },
    {
        "title": "Special Section of BPMDS’2016: Business Processes in a Connected World",
        "submission-date": "2018/04",
        "publication-date": "2018/05",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a 2-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of Information Systems theory. The goals, format, and history of BPMDS can be found on the web site http://www.bpmds.org/. This special section follows the 17th edition of the BPMDS (Business Process Modeling, Development, and Support) series, organized in conjunction with CAISE’16, which was held in Ljubljana, Slovenia, June 2016. BPMDS’2016 received 48 submissions from 31 countries, and 19 papers were selected and published in Springer LNBIP 248 volume. The focus theme of BPMDS 2016 was “Business Processes in a Connected World.” It embraced three areas of research, covering different aspects.",
        "keywords": [],
        "authors": [
            "Rainer Schmidt",
            "Ilia Bider"
        ],
        "file_path": "data/sosym-all/s10270-018-0677-2.pdf"
    },
    {
        "title": "Report on the state of the SoSyM journal (2023 summary)",
        "submission-date": "2024/02",
        "publication-date": "2024/02",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Stéphanie Challita",
            "Benoit Combemale",
            "Huseyin Ergin",
            "JeﬀGray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-024-01152-6.pdf"
    },
    {
        "title": "Model-driven development of mobile applications for Android and iOS supporting role-based app variability",
        "submission-date": "2015/04",
        "publication-date": "2016/09",
        "abstract": "Rapidly increasing numbers of applications and users make the development of mobile applications to one of the most promising ﬁelds in software engineering. Due to short time to market, differing platforms, and fast emerging technologies, mobile application development faces typical challenges where model-driven development (MDD) can help. We present a modeling language and an infrastructure for the MDD of native apps in Android and iOS. Our approach allows a ﬂexible app development on differ-ent abstraction levels: compact modeling of standard app elements such as standard data management and increas-ingly detailed modeling of individual elements to cover, for example, speciﬁc behavior. Moreover, a kind of variability modeling is supported such that mobile apps with variants can be developed. We demonstrate our MDD approach with several apps including a conference app, a museum guide with augmented reality functionality, and a SmartPlug.",
        "keywords": [
            "Model-driven development",
            "Mobile applica-tion",
            "Android",
            "iOS"
        ],
        "authors": [
            "Steffen Vaupel",
            "Gabriele Taentzer",
            "René Gerlach",
            "Michael Guckert"
        ],
        "file_path": "data/sosym-all/s10270-016-0559-4.pdf"
    },
    {
        "title": "Developing conﬁgurations and solutions for logical puzzles with UML and OCL",
        "submission-date": "2024/03",
        "publication-date": "2025/02",
        "abstract": "Logical puzzles can be important factors for the development of rational analysis and application capabilities for pupils and students. Therefore, logical puzzles can also take a prominent supporting role in computer science education. This contribution proposes a UML class model with accompanying OCL constraints for developing logical puzzles. The class model acts as a metamodel for the description of the basic puzzle organization and the logical clues presented to the learners. The constraints express, for example, statements about uniqueness of solutions, and the degree of puzzle complexity may be tuned by appropriate model elements. Given a puzzle speciﬁcation which simply comprises the domain elements of the puzzle plus constraints, our implementation uses a UML and OCL solver to construct a puzzle instance (i.e., a set of clues and solutions) automatically. The puzzle is made playable by a graphical user interface. We have validated this approach for developing puzzles by building several puzzles from the literature of increasing complexity and performed a student survey.",
        "keywords": [
            "Logical puzzle",
            "Formal method",
            "UML",
            "OCL",
            "Metamodel"
        ],
        "authors": [
            "Martin Gogolla",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-025-01272-7.pdf"
    },
    {
        "title": "Unifying nominal and structural typing",
        "submission-date": "2017/03",
        "publication-date": "2018/02",
        "abstract": "In this article, I argue for a typing scheme for modeling that uniﬁes the hitherto separated approaches of nominal and structural typing. Both these approaches have their respective advantages and disadvantages, and I suggest a unifying approach that provides one with the best of both worlds on demand. The ultimate goal is to make a contribution toward removing the gulf currently running through the modeling community that is created by the differences between explanatory and constructive modeling with their dependence on structural and nominal typing, respectively. To this end, I ﬁrst characterize the typing disciplines underlying these different schools of thought, then identify their respective trade-offs, subsequently observe what aspects of these rather different typing approaches are compatible with each other and which are inherently incompatible, and ﬁnally propose a scheme that supports ﬂuid transitioning between the approaches.",
        "keywords": [
            "Explanatory modeling",
            "Constructive modeling",
            "Uniﬁcation",
            "Ontologies",
            "Conceptual models",
            "Structural typing",
            "Nominal typing"
        ],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-018-0660-y.pdf"
    },
    {
        "title": "Towards a model-driven approach for multiexperience AI-based user interfaces",
        "submission-date": "2021/06",
        "publication-date": "Not found",
        "abstract": "Software systems start to include other types of interfaces beyond the “traditional” Graphical-User Interfaces (GUIs). In particular, Conversational User Interfaces (CUIs) such as chat and voice are becoming more and more popular. These new types of interfaces embed smart natural language processing components to understand user requests and respond to them. To provide an integrated user experience all the user interfaces in the system should be aware of each other and be able to collaborate. This is what is known as a multiexperience User Interface. Despite their many beneﬁts, multiexperience UIs are challenging to build. So far CUIs are created as standalone components using a platform-dependent set of libraries and technologies. This raises signiﬁcant integration, evolution and maintenance issues. This paper explores the application of model-driven techniques to the development of software applications embedding a multiexperience User Interface. We will discuss how raising the abstraction level at which these interfaces are deﬁned enables a faster development and a better deployment and integration of each interface with the rest of the software system and the other interfaces with whom it may need to collaborate. In particular, we propose a new Domain Speciﬁc Language (DSL) for specifying several types of CUIs and show how this DSL can be part of an integrated modeling environment able to describe the interactions between the modeled CUIs and the other models of the system (including the models of the GUI). We will use the standard Interaction Flow Modeling Language (IFML) as an example “host” language.",
        "keywords": [
            "Multiexperience development platform (MXDP)",
            "Model-driven development (MDD)",
            "bots",
            "Conversational user interface (CUI)"
        ],
        "authors": [
            "Elena Planas",
            "Gwendal Daniel",
            "Marco Brambilla",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-021-00904-y.pdf"
    },
    {
        "title": "Systematizing modeler experience (MX) in model-driven engineering success stories",
        "submission-date": "2024/03",
        "publication-date": "2024/07",
        "abstract": "Modeling is often associated with complex and heavy tooling, leading to a negative perception among practitioners. However,\nalternative paradigms, such as everything-as-code or low-code, are gaining acceptance due to their perceived ease of use.\nThis paper explores the dichotomy between these perceptions through the lens of “modeler experience” (MX). MX includes\nfactors such as user experience, motivation, integration, collaboration and versioning, and language complexity. We examine\nthe relationships between these factors and their impact on different modeling usage scenarios. Our ﬁndings highlight the\nimportance of considering MX when understanding how developers interact with modeling tools and the complexities of\nmodeling and associated tooling.",
        "keywords": [
            "MDE",
            "User experience",
            "UX"
        ],
        "authors": [
            "Reyhaneh Kalantari",
            "Julian Oertel",
            "Joeri Exelmans",
            "Satrio Adi Rukmono",
            "Vasco Amaral",
            "Matthias Tichy",
            "Katharina Juhnke",
            "Jan-Philipp Steghöfer",
            "Silvia Abrahão"
        ],
        "file_path": "data/sosym-all/s10270-024-01194-w.pdf"
    },
    {
        "title": "SimIMA: a virtual Simulink intelligent modeling assistant",
        "submission-date": "2022/06",
        "publication-date": "2023/03",
        "abstract": "Intelligent virtual model assistance is a key challenge in cultivating model-driven engineering proliferation and growth. Such assistance will help improve the quality of software models, support education for students learning modeling, and lower the entry barriers to new modelers. We present SimIMA, an intelligent modeling assistant for Simulink, which is an extremely popular modeling language in both industry and academia. SimIMA provides modelers with two different forms of data-driven guidance using a knowledge base of conﬁgurable repositories and sources. The ﬁrst form of guidance, SimGESTION, suggests to modelers single-step operations they can perform on their models as they edit them in their modeling environment. These suggestions are based on the machine learning technique of ensemble learning through association rule mining and frequency classiﬁcation. The second form of guidance, SimXAMPLE, presents modelers with similar/related Simulink systems for modelers to either insert directly into their environments or to view for inspiration. SimXAMPLE accomplishes this through model clone detection. To validate SimIMA, we conduct experiments using an established, open, and curated large set of Simulink models coming from a variety of application domains. Our results show that both of SimIMA’s forms of guidance are inferring the appropriate model and element suggestions given SimIMA’s knowledge base and that SimIMA is both scalable and efﬁcient. Through our evaluation, SimIMA demonstrates a prediction accuracy of 78.86% for block-level suggestions and 82.04% for full system suggestions.",
        "keywords": [],
        "authors": [
            "Bhisma Adhikari",
            "Eric J. Rapos",
            "Matthew Stephan"
        ],
        "file_path": "data/sosym-all/s10270-023-01093-6.pdf"
    },
    {
        "title": "Systematic review of matching techniques used in model-driven methodologies",
        "submission-date": "2019/03",
        "publication-date": "2019/11",
        "abstract": "In model-driven methodologies, model matching is the process of ﬁnding a matching pair for every model element between two or more software models. Model matching is an important task as it is often used while differencing and merging models, which are key processes in version control systems. There are a number of different approaches to model matching, with most of them focusing on different goals, i.e., the accuracy of the matching process, or the generality of the algorithm. Moreover, there exist algorithms that use the textual representations of the models during the matching process. We present a systematic literature review that was carried out to obtain the state-of-the-art of model matching techniques. The search process was conducted based on a well-deﬁned methodology. We have identiﬁed a total of 3274 non-duplicate studies, out of which 119 have been included as primary studies for this survey. We present the state-of-the-art of model matching, highlighting the differences between different matching techniques, mainly focusing on text-based and graph-based algorithms. Finally, the main open questions, challenges, and possible future directions in the ﬁeld of model matching are discussed, also including topics like benchmarking, performance and scalability, and conﬂict handling.",
        "keywords": [
            "Model matching",
            "Model comparison",
            "Model differencing",
            "Version control",
            "Text-based modeling",
            "Systematic literature review"
        ],
        "authors": [
            "Ferenc Attila Somogyi",
            "Mark Asztalos"
        ],
        "file_path": "data/sosym-all/s10270-019-00760-x.pdf"
    },
    {
        "title": "Taming uncertainty with MDE: an historical perspective",
        "submission-date": "2024/03",
        "publication-date": "2024/10",
        "abstract": "Uncertainty in Informatics can stem from various sources, whether ontological (inherent unpredictability, such as aleatory\nfactors) or epistemic (due to insufﬁcient knowledge). Effectively handling uncertainty, encompassing both ontological and\nepistemic aspects, to create predictable systems is a key objective for a signiﬁcant portion of the software engineering\ncommunity, particularly within the model-driven engineering (MDE) realm. Numerous techniques have been proposed over\nthe years, leading to evolving trends in model-based software development paradigms. This paper revisits the history of MDE,\naiming to pinpoint the primary aspects of uncertainty that these paradigms aimed to tackle upon their introduction. Our claim\nis that MDE progressively came after more and more aspects of uncertainty, up to the point that it could now help fully\nembrace it.",
        "keywords": [
            "MDE",
            "Uncertainty",
            "MDA",
            "SPL",
            "Aleatory"
        ],
        "authors": [
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-024-01227-4.pdf"
    },
    {
        "title": "Introduction to the special section of the 20th International Conference on Model-Driven Engineering Languages and Systems (MODELS’17)",
        "submission-date": "2019/12",
        "publication-date": "2020/01",
        "abstract": "Since its inception in 1998, MODELS has been the premier conference series for model-based software and systems engineering, covering all aspects of modeling, from languages and methods to tools and applications. MODELS’17 represented the 20th edition of the conference and was held in Austin, Texas, USA, during the week of September 17–22, 2017.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Vinay Kulkarni"
        ],
        "file_path": "data/sosym-all/s10270-020-00774-w.pdf"
    },
    {
        "title": "Guest editorial to the special section on MODELS 2012",
        "submission-date": "2014/06",
        "publication-date": "2014/06",
        "abstract": "In this special section, we are happy to present three papers that resulted from invitations we made to the authors of some of the best papers presented at MODELS 2012. MODELS 2012 was the ACM/IEEE 15th International Conference on Model Driven Engineering Languages and Systems and took place in Innsbruck, Austria, from September 30 to October 5, 2012. The conference has established itself as one of the key international venues for the presentation of scientiﬁc results in the domain of model-driven engineering and related top- ics such as software modeling and model transformation.",
        "keywords": [],
        "authors": [
            "Jürgen Kazmeier",
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-014-0418-0.pdf"
    },
    {
        "title": "Current trends in digital twin development, maintenance, and operation: an interview study",
        "submission-date": "2023/05",
        "publication-date": "2024/04",
        "abstract": "Digital twins (DTs) are often deﬁned as a pairing of a physical entity and a corresponding virtual entity (VE), mimicking certain aspects of the former depending on the use-case. In recent years, this concept has facilitated numerous use-cases ranging from design to validation and predictive maintenance of large and small high-tech systems. Various heterogeneous cross-domain models are essential for such systems, and model-driven engineering plays a pivotal role in the design, development, and maintenance of these models. We believe models and model-driven engineering play a similarly crucial role in the context of a VE of a DT. Due to the rapidly growing popularity of DTs and their use in diverse domains and use-cases, the methodologies, tools, and practices for designing, developing, and maintaining the corresponding VEs differ vastly. To better understand these differences and similarities, we performed a semi-structured interview research with 19 professionals from industry and academia who are closely associated with different lifecycle stages of digital twins. In this paper, we present our analysis and ﬁndings from this study, which is based on seven research questions. In general, we identiﬁed an overall lack of uniformity in terms of the understanding of digital twins and used tools, techniques, and methodologies for the development and maintenance of the corresponding VEs. Furthermore, considering that digital twins are software intensive systems, we recognize a signiﬁcant growth potential for adopting more software engineering practices, processes, and expertise in various stages of a digital twin’s lifecycle.",
        "keywords": [
            "Digital twin",
            "Semi-structured interview",
            "Digital twin trends"
        ],
        "authors": [
            "Hossain Muhammad Muctadir",
            "David A. Manrique Negrin",
            "Raghavendran Gunasekaran",
            "Loek Cleophas",
            "Mark van den Brand",
            "Boudewijn R. Haverkort"
        ],
        "file_path": "data/sosym-all/s10270-024-01167-z.pdf"
    },
    {
        "title": "Searching textual and model-based process descriptions based on a uniﬁed data format",
        "submission-date": "2016/12",
        "publication-date": "2017/12",
        "abstract": "Documenting business processes using process models is common practice in many organizations. However, not all process information is best captured in process models. Hence, many organizations complement these models with textual descriptions that specify additional details. The problem with this supplementary use of textual descriptions is that existing techniques for automatically searching process repositories are limited to process models. They are not capable of taking the information from textual descriptions into account and, therefore, provide incomplete search results. In this paper, we address this problem and propose a technique that is capable of searching textual as well as model-based process descriptions. It automatically extracts activity-related and behavioral information from both descriptions types and stores it in a uniﬁed data format. An evaluation with a large Austrian bank demonstrates that the additional consideration of textual descriptions allows us to identify more relevant processes from a repository.",
        "keywords": [
            "Integrated Process Search",
            "Uniﬁed Data Format",
            "Textual Process Descriptions"
        ],
        "authors": [
            "Henrik Leopold",
            "Han van der Aa",
            "Fabian Pittke",
            "Manuel Raﬀel",
            "Jan Mendling",
            "Hajo A. Reijers"
        ],
        "file_path": "data/sosym-all/s10270-017-0649-y.pdf"
    },
    {
        "title": "Managing design-time uncertainty",
        "submission-date": "2016/10",
        "publication-date": "2017/03",
        "abstract": "Managing design-time uncertainty, i.e., uncertainty that developers have about making design decisions, requires creation of “uncertainty-aware” software engineering methodologies. In this paper, we propose a methodological approach for managing uncertainty using partial models. To this end, we identify the stages in the lifecycle of uncertainty-related design decisions and characterize the tasks needed to manage it. We encode this information in the Design-Time Uncertainty Management (DeTUM) model. We then use the DeTUM model to create a coherent, tool-supported methodology centred around partial model management. We demonstrate the effectiveness and feasibility of our methodology through case studies.",
        "keywords": [
            "Software methodology",
            "Software modelling",
            "Software design",
            "Design space management",
            "Uncertainty"
        ],
        "authors": [
            "Michalis Famelis",
            "Marsha Chechik"
        ],
        "file_path": "data/sosym-all/s10270-017-0594-9.pdf"
    },
    {
        "title": "A comparative study of students and professionals in syntactical \nmodel comprehension experiments",
        "submission-date": "2018/03",
        "publication-date": "2019/04",
        "abstract": "Empirical evaluations can be conducted with students or professionals as subjects. Students are much more accessible than \nprofessionals and they are inexpensive, allowing a greater number of empirical studies to be conducted. Professionals are pre-\nferred over students due to concerns regarding the external validity of student-based experiments. Professionals are believed \nto perform differently, most likely better than students. But with respect to evaluating the cognitive effectiveness of software \nengineering notations, are professionals really better? The literature has suggested that the presentation of information is just \nas critical as the content it conveys, hence necessitating this type of empirical studies. If professionals are not much better \nthan students, then such important finding can be a springboard to many much-needed empirical evaluations in this field. In \nthis paper, we report on an experiment that compare the performances of professionals and students with respect to syntac-\ntical model comprehension, which is a core factor for evaluating the cognitive effectiveness of notations. The experiment \ninvolved two groups of professionals and two student groups, totaling 74 professionals and 75 students. The results of the \nexperiment indicate that students can be used as an adequate replacement to professionals in such type of empirical studies.",
        "keywords": [
            "Statechart modeling",
            "Use case modeling",
            "Student-based experiments",
            "Professional-based experiments"
        ],
        "authors": [
            "Mohamed El‑Attar"
        ],
        "file_path": "data/sosym-all/s10270-019-00720-5.pdf"
    },
    {
        "title": "Guaranteed master for interval-based cosimulation",
        "submission-date": "2020/02",
        "publication-date": "2021/01",
        "abstract": "In this paper, we tackle the problem of guaranteed simulation of cyber-physical systems, an important model for current engineering systems. Their is always increasing complexity which leads to models of higher and higher dimensions, yet typically involving multiple subsystems or even multiple physics. Given this modularity, we more precisely explore cosimulation of such dynamical systems, with the aim of reaching higher dimensions of the simulated systems. In this paper, we present a guaranteed interval-based approach for cosimulation of continuous time systems. We propose an algorithm which first proves the existence and returns an enclosure of global solutions, using only local computations. This mitigates the curse of dimensionality faced by global (guaranteed) integration methods. Local computations are then realized with a safe estimate of the other sub-systems until the next macro-step. We increase the accuracy of the approach by using an interval extrapolation of the state of the other sub-systems. We finally propose some possible further improvements including adaptive macro-step size. Our method is fully guaranteed, taking into account all possible sources of error. It is implemented in a C++ prototype relying on the DynIbex library, and we illustrate our approach on multiple examples of the literature.",
        "keywords": [
            "Cosimulation",
            "Guaranteed simulation",
            "Integration methods"
        ],
        "authors": [
            "Adrien Le Coënt",
            "Julien Alexandre dit Sandretto",
            "Alexandre Chapoutot"
        ],
        "file_path": "data/sosym-all/s10270-020-00858-7.pdf"
    },
    {
        "title": "SAMM: an architecture modeling methodology for ship command and control systems",
        "submission-date": "2012/12",
        "publication-date": "2014/01",
        "abstract": "Ship command and control systems (SCCSs) are composed of large-scale, complex, real-time and software-intensive systems that complete tasks collaboratively. Open architecture has been introduced to design the architecture of SCCSs and has been reﬁned into functional architecture (FA) and technical architecture (TA) to meet architectural requirements such as adapting fast-speed functional and technical changes. Thereby, specifying the architecture of SCCSs, based on FA and TA, becomes a key issue for stakeholders of the domain. In this paper, we propose an architecture modeling methodology (named as SAMM) for describing the architecture of SCCSs. SAMM is derived by following a systematic and generic framework—modeling Goal, domain-speciﬁc Conceptual model, architecture View-point, and architecture description Language (GCVL), which guides domain experts to devise domain-speciﬁc architecture modeling methodologies of large-scale software-intensive systems. SAMM contains three viewpoints and 22 models, and a UML/SysML-based architecture description language. An industrial application of SAMM, along with the subsequent application of the derived SAMM architecture model (i.e., a deployed SCCS prototype) was conducted to evaluate SAMM. A questionnaire-based survey was also conducted to subjectively evaluate whether SAMM meets the modeling goals and its applicability. Results show that SAMM meets all modeling goals and is easy to apply.",
        "keywords": [
            "Architecture modeling",
            "Viewpoint",
            "UML",
            "SysML",
            "Ship command and control systems"
        ],
        "authors": [
            "Zhiqiang Fan",
            "Tao Yue",
            "Li Zhang"
        ],
        "file_path": "data/sosym-all/s10270-013-0393-x.pdf"
    },
    {
        "title": "Formal validation of domain-speciﬁc languages with derived features and well-formedness constraints",
        "submission-date": "2014/09",
        "publication-date": "2015/07",
        "abstract": "Despite the wide range of existing tool support, constructing a design environment for a complex domain-speciﬁc language (DSL) is still a tedious task as the large number of derived features and well-formedness constraints complementing the domain metamodel necessitate special handling. Such derived features and constraints are frequently deﬁned by declarative techniques (such graph patterns or OCL invariants). However, for complex domains, derived features and constraints can easily be formalized incorrectly resulting in inconsistent, incomplete or ambigu-ous DSL speciﬁcations. To detect such issues, we propose an automated mapping of EMF metamodels enriched with derived features and well-formedness constraints captured as graph queries in EMF- IncQuery or (a subset of) OCL invariants into an effectively propositional fragment of ﬁrst-order logic which can be efﬁciently analyzed by back-end reasoners. On the conceptual level, the main added value of our encoding is (1) to transform graph patterns of the EMF- IncQuery framework into FOL and (2) to introduce approximations for complex language features (e.g., transi-tive closure or multiplicities) which are not expressible in FOL. On the practical level, we identify and address relevant challenges and scenarios for systematically validating DSL speciﬁcations. Our approach is supported by a tool, and it will be illustrated on analyzing a DSL in the avionics domain. We also present initial performance experiments for the validation using Z3 and Alloy as back-end reasoners.",
        "keywords": [
            "Language validation",
            "Derived features",
            "Partial snapshots",
            "Model queries",
            "SMT solvers"
        ],
        "authors": [
            "Oszkár Semeráth",
            "Ágnes Barta",
            "Ákos Horváth",
            "Zoltán Szatmári",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-015-0485-x.pdf"
    },
    {
        "title": "A framework to specify system requirements using natural interpretation of UML/MARTE diagrams",
        "submission-date": "2016/03",
        "publication-date": "2017/03",
        "abstract": "The ever-increasing design complexity of embedded systems is constantly pressing the demand for more abstract design levels and possible methods for automatic veriﬁcation and synthesis. Transforming a text-based user requirements document into semantically sound models is always difﬁcult and error-prone as mostly these requirements are vague and improperly documented. This paper presents a framework to specify textual requirements graphically in standard modeling formalisms like uml and marte in the form of temporal and logical patterns. The underlying formal semantics of these graphical models allow to eliminate ambiguity in speciﬁcations and automatic design veriﬁcation at different abstraction levels using these patterns. The semantics of these operators/patterns are presented formally as state automatons and a comparison is made to the existing ccsl relational operators. To reap the beneﬁts of mde, a software plugin TemLoPAC is presented as part of the framework to transform the graphical patterns into ccsl and Verilog-based observers.",
        "keywords": [
            "FSL",
            "Graphical properties",
            "UML",
            "MARTE",
            "CCSL",
            "Modeling",
            "Embedded systems"
        ],
        "authors": [
            "Aamir M. Khan",
            "Frédéric Mallet",
            "Muhammad Rashid"
        ],
        "file_path": "data/sosym-all/s10270-017-0588-7.pdf"
    },
    {
        "title": "EDITORIAL",
        "submission-date": "2025/05",
        "publication-date": "2025/05",
        "abstract": "As we present this issue of the Journal of Software and Systems Modeling (SoSyM), we take a moment to acknowledge and celebrate the remarkable contributions of Professor Antonio Vallecillo. After decades of dedication to the field, Antonio is now retiring, leaving behind an impressive amount of results in software and systems modeling. He has been a cornerstone of our journal, serving as an editor almost from the very beginning of SoSyM’s 25 year journey. He was always helpful as editor, advisor, and friend. His expertise, vision, and continuous commitment have significantly shaped not only the special section presented in this issue, but also the broader research community.",
        "keywords": [],
        "authors": [
            "Antonio Vallecillo",
            "Stéphanie Challita",
            "Marsha Chechik",
            "Benoit Combemale",
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-025-01290-5.pdf"
    },
    {
        "title": "VbTrace: using view-based and model-driven development to support traceability in process-driven SOAs",
        "submission-date": "2009/01",
        "publication-date": "2009/11",
        "abstract": "In process-driven, service-oriented architectures, there are a number of important factors that hinder the trace-ability between design and implementation artifacts. First of all, there are no explicit links between process design and implementation languages not only due to the differences of syntax and semantics but also the differences of granularity. The second factor is the complexity caused by tangled process concerns that multiplies the difﬁculty of analyzing and understanding the trace dependencies. Finally, there is a lack of adequate tool support for establishing and maintaining the trace dependencies between process designs and implementations. We present in this article a view-based, model-driven traceability approach that tackles these chal-lenges. Our approach supports (semi-)automatically eliciting and (semi-)formalizing trace dependencies among process development artifacts at different levels of granularity and abstraction. A proof-of-concept tool support has been real-ized, and its functionality is illustrated via an industrial case study.",
        "keywords": [
            "Software traceability",
            "View-based",
            "Model-driven",
            "Process-driven SOA",
            "Tool support"
        ],
        "authors": [
            "Huy Tran",
            "Uwe Zdun",
            "Schahram Dustdar"
        ],
        "file_path": "data/sosym-all/s10270-009-0137-0.pdf"
    },
    {
        "title": "Event-driven temporal models for explanations - ETeMoX: explaining reinforcement learning",
        "submission-date": "2021/04",
        "publication-date": "2021/12",
        "abstract": "Modern software systems are increasingly expected to show higher degrees of autonomy and self-management to cope with uncertain and diverse situations. As a consequence, autonomous systems can exhibit unexpected and surprising behaviours. This is exacerbated due to the ubiquity and complexity of Artiﬁcial Intelligence (AI)-based systems. This is the case of Reinforcement Learning (RL), where autonomous agents learn through trial-and-error how to ﬁnd good solutions to a problem. Thus, the underlying decision-making criteria may become opaque to users that interact with the system and who may require explanations about the system’s reasoning. Available work for eXplainable Reinforcement Learning (XRL) offers different trade-offs: e.g. for runtime explanations, the approaches are model-speciﬁc or can only analyse results after-the-fact. Different from these approaches, this paper aims to provide an online model-agnostic approach for XRL towards trustworthy and understandable AI. We present ETeMoX, an architecture based on temporal models to keep track of the decision-making processes of RL systems. In cases where the resources are limited (e.g. storage capacity or time to response), the architecture also integrates complex event processing, an event-driven approach, for detecting matches to event patterns that need to be stored, instead of keeping the entire history. The approach is applied to a mobile communications case study that uses RL for its decision-making. In order to test the generalisability of our approach, three variants of the underlying RL algorithms are used: Q-Learning, SARSA and DQN. The encouraging results show that using the proposed conﬁgurable architecture, RL developers are able to obtain explanations about the evolution of a metric, relationships between metrics, and were able to track situations of interest happening over time windows.",
        "keywords": [
            "Temporal models",
            "Complex event processing",
            "Artiﬁcial intelligence",
            "Explainable reinforcement learning",
            "Event-driven monitoring"
        ],
        "authors": [
            "Juan Marcelo Parra-Ullauri",
            "Antonio García-Domínguez",
            "Nelly Bencomo",
            "Changgang Zheng",
            "Chen Zhen",
            "Juan Boubeta-Puig",
            "Guadalupe Ortiz",
            "Shufan Yang"
        ],
        "file_path": "data/sosym-all/s10270-021-00952-4.pdf"
    },
    {
        "title": "Extract, model, reﬁne: improved modelling of program veriﬁcation tools through data enrichment",
        "submission-date": "2023/05",
        "publication-date": "2025/01",
        "abstract": "In software engineering, models are used for many different things. In this paper, we focus on program veriﬁcation, where we use models to reason about the correctness of systems. There are many different types of program veriﬁcation techniques which provide different correctness guarantees. We investigate the domain of program veriﬁcation tools and present a concise megamodel to distinguish these tools. We also present a data set of 400+ program veriﬁcation tools. This data set includes the category of veriﬁcation tool according to our megamodel, practical information such as input/output format, repository links and more. The practical information, such as last commit date, is kept up to date through the use of APIs. Moreover, part of the data extraction has been automated to make it easier to expand the data set. The categorisation enables software engineers to ﬁnd suitable tools, investigate alternatives and compare tools. We also identify trends for each level in our megamodel. Our data set, publicly available at https://doi.org/10.4121/20347950, can be used by software engineers to enter the world of program veriﬁcation and ﬁnd a veriﬁcation tool based on their requirements. This paper is an extended version of https://doi.org/10.1145/3550355.3552426.",
        "keywords": [
            "Program veriﬁcation",
            "Megamodelling",
            "Data enrichment",
            "Data extraction"
        ],
        "authors": [
            "Sophie Lathouwers",
            "Yujie Liu",
            "Vadim Zaytsev"
        ],
        "file_path": "data/sosym-all/s10270-024-01232-7.pdf"
    },
    {
        "title": "Stress-testing remote model querying APIs for relational and graph-based stores",
        "submission-date": "2016/11",
        "publication-date": "2017/06",
        "abstract": "Recent research in scalable model-driven engineering now allows very large models to be stored and queried. Due to their size, rather than transferring such models over the network in their entirety, it is typically more efficient to access them remotely using networked services (e.g. model repositories, model indexes). Little attention has been paid so far to the nature of these services, and whether they remain responsive with an increasing number of concurrent clients. This paper extends a previous empirical study on the impact of certain key decisions on the scalability of concurrent model queries on two domains, using an Eclipse Connected Data Objects model repository, four configurations of the Hawk model index and a Neo4j-based configuration of the NeoEMF model store. The study evaluates the impact of the network protocol, the API design, the caching layer, the query language and the type of database and analyses the reasons for their varying levels of performance. The design of the API was shown to make a bigger difference compared to the network protocol (HTTP/TCP) used. Where available, the query-specific indexed and derived attributes in Hawk outperformed the comprehensive generic caching in CDO. Finally, the results illustrate the still ongoing evolution of graph databases: two tools using different versions of the same backend had very different performance, with one slower than CDO and the other faster than it.",
        "keywords": [
            "Model persistence",
            "Remote model querying",
            "NoSQL storage",
            "Relational databases",
            "API design",
            "Stress testing",
            "Collaborative modelling"
        ],
        "authors": [
            "Antonio Garcia-Dominguez",
            "Konstantinos Barmpis",
            "Dimitrios S. Kolovos",
            "Ran Wei",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-017-0606-9.pdf"
    },
    {
        "title": "Simulation and analysis of MultEcore multilevel models based on rewriting logic",
        "submission-date": "2020/05",
        "publication-date": "2021/11",
        "abstract": "Multilevel modelling (MLM) approaches make it possible for designers and modellers to work with an unlimited number of abstraction levels when specifying domain-speciﬁc modelling languages (DSMLs). In this paper, we present a functional infrastructure that allows modellers to deﬁne the structure and the operational semantics of multilevel modelling hierarchies, enabling simulation and analysis. Using the MultEcore tool, one can design and distribute the models that compose the language family in a multilevel hierarchy, and specify their behaviour by means of multilevel transformation, so-called multilevel coupled model transformations. We give a rewrite logic semantics to MultEcore’s MLM, on which we have based our automated transformation from MultEcore to the rewriting logic language Maude. Then, we rely on Maude to execute MultEcore models and to support analysis techniques, like reachability analysis, bounded and unbounded model checking of invariants and LTL formulas on systems with both ﬁnite and inﬁnite reachable state spaces using equational abstraction. We illustrate our developed techniques on a DSML family for Petri nets.",
        "keywords": [
            "Multilevel modelling",
            "Domain-speciﬁc modelling languages",
            "Model transformations",
            "Veriﬁcation",
            "Rewriting logic",
            "Maude"
        ],
        "authors": [
            "Alejandro Rodríguez",
            "Francisco Durán",
            "Lars Michael Kristensen"
        ],
        "file_path": "data/sosym-all/s10270-021-00947-1.pdf"
    },
    {
        "title": "Büchi automata for modeling component connectors",
        "submission-date": "2009/03",
        "publication-date": "2010/03",
        "abstract": "Reo is an exogenous coordination language for component connectors extending data ﬂow networks with synchronization and context-dependent behavior. The ﬁrst proposed formalism to capture the operational semantics of Reo is called constraint automaton. In this paper, we propose another operational model of Reo based on Büchi automata in which port synchronization is modeled by records labeling the transitions, whereas context dependencies are stored in the states. It is shown that constraint automata can be recast into our proposed Büchi automata of records. Also, we provide a composition operator which models the joining of two connectors and show that it can be obtained by using two standard operators: alphabet extension and automata product. Our semantics has the advantage over previous models in that it is based on standard automata theory, so that existing theories and tools can be easily reused. Moreover, it is the ﬁrst formal model addressing all of Reo’s features: synchronization, mutual exclusion, hiding, and context-dependency.",
        "keywords": [
            "Büchi automata",
            "Component connector",
            "Reo",
            "Constraint automata"
        ],
        "authors": [
            "Mohammad Izadi",
            "Marcello Bonsangue",
            "Dave Clarke"
        ],
        "file_path": "data/sosym-all/s10270-010-0152-1.pdf"
    },
    {
        "title": "Correction: A framework for embedded software portability and veriﬁcation: from formal models to low-level code",
        "submission-date": "2024/04",
        "publication-date": "2024/04",
        "abstract": "In the original publication the paper mentions \"BlindReviewOS\" in several locations. The real name of the software is \"SmartOS\".\nThis has been corrected in the original publication.",
        "keywords": [],
        "authors": [
            "Renata Martins Gomes",
            "Bernhard Aichernig",
            "Marcel Baunach"
        ],
        "file_path": "data/sosym-all/s10270-024-01166-0.pdf"
    },
    {
        "title": "Theme section on modeling and sustainability",
        "submission-date": "2025/02",
        "publication-date": "2025/03",
        "abstract": "Sustainability is becoming a key property of modern systems. Widespread digitalization and the accelerating pathways of digital technology have opened unprecedented opportunities for humankind to build more efﬁcient, robust, and performant systems. Unfortunately, the beneﬁts of elaborate digital technology come at the cost of an increased environmental footprint, unclear social impact, and increased maintenance efforts. Current systems’ engineering practices fall short of adequately addressing these sustainability challenges due to the highly multi-systemic and stratiﬁed nature of sustainability and need to be revised appropriately. Modeling offers numerous beneﬁts in understanding and assessing the sustainability properties of engineered systems. Modeling languages and tools support subject matter experts in expressing their views—process models allow for reasoning about trade-offs across the end-to-end systems’ engineering process, while runtime models allow for controlling engineering endeavors for sustainability. These are just a few of the many ways to support sustainability ambitions by modeling. This theme section invited researchers and practitioners in various areas of modeling and simulation to spotlight their recent innovative research results. We hope that this theme section will serve as an informative piece for anyone interested in modeling and sustainability, as well as motivate fellow researchers to devote their valuable time to the growing problem of unsustainable practices in technology and engineering.",
        "keywords": [],
        "authors": [
            "Istvan David",
            "Ankica Bariši´c",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-025-01279-0.pdf"
    },
    {
        "title": "UML 3.0 and the future of modeling",
        "submission-date": "2003/05",
        "publication-date": "2004/02",
        "abstract": "The major revision work for UML 2.0 is complete, and it is now an OMG Final Adopted Speciﬁcation. This is a good time to reﬂect on UML’s future, and the future of model-driven development.",
        "keywords": [],
        "authors": [
            "Cris Kobryn"
        ],
        "file_path": "data/sosym-all/s10270-004-0051-4.pdf"
    },
    {
        "title": "Heap-abstraction for an object-oriented calculus with thread classes",
        "submission-date": "2006/02",
        "publication-date": "2007/08",
        "abstract": "This paper formalizes an open semantics for a calculus featuring thread classes, where the environment, consisting in particular of an overapproximation of the heap topology, is abstractly represented. From an observational point of view, considering classes as part of a component makes instantiation a possible interaction between component and environment or observer. For thread classes it means that a component may create external activity, which influences what can be observed. The fact that cross-border instantiation is possible requires that the connectivity of the objects needs to be incorporated into the semantics. We extend our prior work not only by adding thread classes, but also in that thread names may be communicated, which means that the semantics needs to account explicitly for the possible acquaintance of objects with threads. We show soundness of the abstraction.",
        "keywords": [
            "Class-based OO languages",
            "Thread-based concurrency",
            "Open systems",
            "Formal semantics",
            "Heap abstraction",
            "Observable behavior"
        ],
        "authors": [
            "Erika Ábrahám",
            "Andreas Grüner",
            "Martin Steffen"
        ],
        "file_path": "data/sosym-all/s10270-007-0065-9.pdf"
    },
    {
        "title": "Five years of modeling in SoSyM",
        "submission-date": "2006/11",
        "publication-date": "2006/11",
        "abstract": "Since its inception in 2002 the International Journal on Software and Systems Modeling (SoSyM) has become a major source of quality papers describing research and experience related to building and using models in the development of software-based systems. Papers published in SoSyM cover many aspects of software development; from early business requirements modeling and analysis through system architecting to quality management, maintenance and evolution of software. However, in practice, models are primarily used in two ways: as informal descriptions of concepts to facilitate discussion (e.g., using the UML as a sketching notation), and as bases for generating code (e.g., generating code using model frameworks). Research on model-driven development (MDD) indicates that models can be better leveraged during development. However, there is a need to perform more foundational research and empirical studies to fully understand how the MDD vision of software development can be realized. The journal will continue to play a vital role in nurturing and advancing high quality research in MDD through the publication of results from high quality empirical studies, and of successful or highly promising approaches that address various aspects of MDD.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-006-0035-7.pdf"
    },
    {
        "title": "Agile model-based system development",
        "submission-date": "2018/08",
        "publication-date": "2018/08",
        "abstract": "Our journal is called “Software and Systems Modeling,” but when we look at most of the papers that have been published in SoSyM recently, the focus is much more on software than on other kinds of systems. It is worth a fresh look at how other engineering disciplines use models. In fact, modeling is at the heart of almost any engineering discipline. Thus, it is not surprising that our engineering colleagues have developed a detailed portfolio of modeling techniques to describe their systems in various perspectives, viewpoints, and abstractions. Furthermore, a plethora of tools has been developed to assist practitioners with each of the individual modeling languages. Colleagues from traditional engineering disciplines model many more aspects of their systems than software developers. However, the state-of-the-art in systems modeling has several challenges, where each modeling aspect and view is often assisted by an individual modeling and analysis tool. Data exchange between the tools is complicated, even though mostly automated, but suffers from concerns about robustness, completeness of the mappings between the models, as well as regular version upgrades of tools. A second problem is that these mappings between models are not easy to standardize, because different projects use the same modeling languages in different forms (semantics), which enforces configurable mappings or individually developed translations per project. We have discussed these aspects on modeling partially in previous editorials (please see http://www.sosym.org/editorials/). An important aspect of a modeling toolchain concerns the “length” of the toolchain—the longer the toolchain, the more information that may be added along the toolchain, potentially reducing the agility of the development process. Agility is a well-known software development technique that dominates smaller- and medium-sized software projects. Many projects have demonstrated that an agile, lean development process has merits in terms of cost reduction, quality, and time to market. Driven by the large software companies in Silicon Valley that have a development and innovation pace far beyond traditional engineering, there are currently several attempts to adapt agility to systems development, with examples including smartphones, autonomous and electrified cars, and also medical devices and home automation. Therefore, it is worthwhile to revisit the most important ingredients required by an agile process, which are: Lean processes with as little documentation overhead as possible, Immediate feedback as and early as possible, Feedback on all activities, Automation of many development tasks, Many, small iterations, Self-responsibility for developers to do whatever seems best at the current situation. Traditional engineers use system models to design and understand the product through analysis and automation of engineering tasks. Therefore, models should be more than just documentation artifacts. Each model that captures a perspective of the project should either become a part of the product construction or should be used for automated validation and test quality management. Although synthesizing software products from models is a common practice in the software engineering domain, physical products can also be produced (e.g., with appropriate 3D printers). In systems modeling, simulations of the physical product in various abstractions and for various validation and testing purposes can be a core benefit of modeling. Simulations help to understand product sustainability (e.g., deterioration), usability of a product in its context (e.g., autonomous driving), and many additional aspects that are more difficult to explore as “what-if” analyses without modeling support. The benefit of models and simulation occurs when validation steps are available on very early versions of a model and not only on a complete",
        "keywords": [],
        "authors": [
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-018-0694-1.pdf"
    },
    {
        "title": "Automated resolution of connector architectures using constraint solving (ARCAS method)",
        "submission-date": "2011/08",
        "publication-date": "2012/09",
        "abstract": "In current software systems, connectors play an important role by encapsulating the communication and coordination logic. Since they share common patterns (ele-ments) depending on characteristics of the connections, the elements can be predeﬁned and reused. A method of connector implementation based on a composition of predeﬁned elementsnaturallycomprisestwosteps:resolutionofthecon-nector architecture, and creation of the actual connector code based on the architecture. However, manual resolution of a connector architecture is very difﬁcult due to the number of factors to be considered. Thus, the challenge is to come up with an automated method, able to address all the important factors. In this paper, we present a method for automated res-olutionofconnectorarchitecturesbasedonconstraintsolving techniques. We exploit a propositional logic with relational calculus for deﬁning a connector theory, a constraint speciﬁ-cation reﬂecting both the predeﬁned parts and the important resolution factors, and employ a constraint solver to ﬁnd a suitable connector architecture as a model of the theory. As a proof of the concept, we show how the theory can be captured in the Alloy language and resolved via the Alloy Analyzer.",
        "keywords": [
            "Software architecture",
            "Software connectors",
            "Constraint solving",
            "Middleware-based connectors",
            "Connector theory",
            "Alloy"
        ],
        "authors": [
            "Jaroslav Keznikl",
            "Tomáš Bureš",
            "František Plášil",
            "Petr Hnˇetynka"
        ],
        "file_path": "data/sosym-all/s10270-012-0274-8.pdf"
    },
    {
        "title": "Accelerating similarity-based model matching using dual hashing",
        "submission-date": "2023/04",
        "publication-date": "2024/04",
        "abstract": "Similarity-based model matching is the cornerstone of model versioning. It pairs model elements based on a distance metric (e.g., edit distance). However, calculating the distances between elements is computationally expensive. Consequently, a similarity-based matcher typically suffers from performance issues when the model size increases. Based on observation, there are two main causes of the high computation cost: (1) when matching an element p, the matcher calculates the distance between p and every candidate element q, despite the obvious dissimilarity between p and q; (2) the matcher always calculates the distance between p and q′, even though q and q′ are very similar and the distance between p and q is already known. This paper proposes a dual-hash-based approach, which employs two entirely different hashing techniques—similarity-preserving hashing and integrity-based hashing—to accelerate similarity-based model matching. With similarity-preserving hashing, our approach can quickly ﬁlter out the dissimilar candidate elements according to their similarity hashes computed using our similarity-preserving hash function, which maps an element to a 64-bit binary hash. With integrity-based hashing, our approach can cache and reuse computed distance values by associating them with the checksums of model elements. We also propose an index structure to facilitate hash-based model matching. Our approach has been implemented and integrated into EMF Compare. We evaluate our approach using open-source Ecore and UML models. The results show that our hash function is effective in preserving the similarity between model elements and our matching approach reduces time costs by 20–88% while assuring the matching results consistent with EMF Compare.",
        "keywords": [
            "Model matching",
            "Similarity-preserving hashing",
            "Integrity-based hashing",
            "Edit distance"
        ],
        "authors": [
            "Xiao He",
            "Yi Liu",
            "Huihong He"
        ],
        "file_path": "data/sosym-all/s10270-024-01173-1.pdf"
    },
    {
        "title": "The 8th Annual State of SoSyM Report",
        "submission-date": "2009/11",
        "publication-date": "2009/11",
        "abstract": "Another year has gone by and we are happy to report that the International Journal on Software and Systems Modeling (SoSyM) is doing well. As we have done on previous anniversaries, we take this opportunity to give a “state of the journal” report and to acknowledge the reviewers, editors, and publication staff who have contributed to the journal’s very good performance in the past year.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-009-0143-2.pdf"
    },
    {
        "title": "Introduction",
        "submission-date": "2003/09",
        "publication-date": "2005/05",
        "abstract": "Speciﬁcations of complex systems are usually based on either states or actions (events). Some people believe that the ﬁrst step in system development should be the identiﬁcation of the right global state structure. Other people start characterizing the system by describing its interactions with the environment, that is, by identifying event patterns (e.g. use cases), without worrying, at least initially, about the shape of the state. Formal speciﬁcation approaches such as Abstract State Machines, B, CSP, LOTOS, Predicate/Transition Nets, Statecharts, TLA, Z, imply a considerable bias towards one of these two ways of conceiving system behaviour.",
        "keywords": [],
        "authors": [
            "Tommaso Bolognesi",
            "John Derrick"
        ],
        "file_path": "data/sosym-all/s10270-005-0082-5.pdf"
    },
    {
        "title": "Model-driven design space exploration for multi-robot systems in simulation",
        "submission-date": "2022/04",
        "publication-date": "2022/10",
        "abstract": "Multi-robot systems are increasingly deployed to provide services and accomplish missions whose complexity or cost is too high for a single robot to achieve on its own. Although multi-robot systems offer increased reliability via redundancy and enable the execution of more challenging missions, engineering these systems is very complex. This complexity affects not only the architecture modelling of the robotic team but also the modelling and analysis of the collaborative intelligence enabling the team to complete its mission. Existing approaches for the development of multi-robot applications do not provide a systematic mechanism for capturing these aspects and assessing the robustness of multi-robot systems. We address this gap by introducing ATLAS, a novel model-driven approach supporting the systematic design space exploration and robustness analysis of multi-robot systems in simulation. The ATLAS domain-speciﬁc language enables modelling the architecture of the robotic team and its mission and facilitates the speciﬁcation of the team’s intelligence. We evaluate ATLAS and demonstrate its effectiveness in three simulated case studies: a healthcare Turtlebot-based mission and two unmanned underwater vehicle missions developed using the Gazebo/ROS and MOOS-IvP robotic platforms, respectively.",
        "keywords": [
            "MRS",
            "Multi-robot systems",
            "Model-driven engineering",
            "MDE",
            "Simulation",
            "Design-space exploration"
        ],
        "authors": [
            "James Harbin",
            "Simos Gerasimou",
            "Nicholas Matragkas",
            "Thanos Zolotas",
            "Radu Calinescu",
            "Misael Alpizar Santana"
        ],
        "file_path": "data/sosym-all/s10270-022-01041-w.pdf"
    },
    {
        "title": "Automated conceptual model clustering: a relator-centric approach",
        "submission-date": "2021/03",
        "publication-date": "2021/09",
        "abstract": "In recent years, there has been a growing interest in the use of reference conceptual models to capture information about complex and sensitive business domains (e.g., ﬁnance, healthcare, space). These models play a fundamental role in different types of critical semantic interoperability tasks. Therefore, domain experts must be able to understand and reason with their content. In other words, these models need to be cognitively tractable. This paper contributes to this goal by proposing a model clustering technique that leverages on the rich semantics of ontology-driven conceptual models (ODCM). In particular, we propose a formal notion of Relational Context to guide the automated clusterization (or modular breakdown) of conceptual models. Such Relational Contexts capture all the information needed for understanding entities “qua players of roles” in the scope of an objectiﬁed (reiﬁed) relationship (relator). The paper also presents computational support for automating the identiﬁcation of Relational Contexts and this modular breakdown procedure. Finally, we report the results of an empirical study assessing the cognitive effectiveness of this approach.",
        "keywords": [
            "Ontology-driven conceptual modeling",
            "Complexity management in conceptual modeling",
            "Conceptual model clustering",
            "OntoUML"
        ],
        "authors": [
            "Giancarlo Guizzardi",
            "Tiago Prince Sales",
            "João Paulo A. Almeida",
            "Geert Poels"
        ],
        "file_path": "data/sosym-all/s10270-021-00919-5.pdf"
    },
    {
        "title": "Synthesis and exploration of multi-level, multi-perspective architectures of automotive embedded systems",
        "submission-date": "2016/08",
        "publication-date": "2017/04",
        "abstract": "In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today’s engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multi-level, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of question templates, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates relevant to their task at hand. Finally, using the modeling language Clafer, which is supported by automated backend reasoners, we show that it is possible to synthesize and explore optimal candidate architectures for two highly configurable automotive sub-systems.",
        "keywords": [
            "Architecture synthesis",
            "Multi-level architectures",
            "Multi-perspective architectures",
            "E/E architecture",
            "Architecture optimization",
            "Candidate architectures",
            "Early design"
        ],
        "authors": [
            "Jordan A. Ross",
            "Alexandr Murashkin",
            "Jia Hui Liang",
            "Michał Antkiewicz",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-017-0592-y.pdf"
    },
    {
        "title": "Model-driven performance prediction of systems of systems",
        "submission-date": "2015/07",
        "publication-date": "2016/07",
        "abstract": "Systems of systems exhibit characteristics that pose difficulty in modelling and predicting their overall performance capabilities, including the presence of operational independence, emergent behaviour, and evolutionary development. When considering systems of systems within the autonomous defence systems context, these aspects become increasingly critical, as constraints on the performance of the final system are typically driven by hard constraints on space, weight and power. System execution modelling languages and tools permit early prediction of the performance of model-driven systems; however, the focus to date has been on understanding the performance of a model rather than determining whether it meets performance requirements, and only subsequently carrying out analysis to reveal the causes of any requirement violations. Moreover, such an analysis is even more difficult when applied to several systems cooperating to achieve a common goal—a system of systems. In this article, we propose an integrated approach to performance prediction of model-driven real-time embedded defence systems and systems of systems. Our architectural prototyping system supports a scenario-driven experimental platform for evaluating model suitability within a set of deployment and real-timeperformanceconstraints.Wepresentanoverviewof our performance prediction system, demonstrating the integration of modelling, execution and performance analysis, and discuss a case study to illustrate our approach.",
        "keywords": [
            "Performance prediction",
            "Systems of systems",
            "Model-driven engineering",
            "System execution modelling"
        ],
        "authors": [
            "Katrina Falkner",
            "Claudia Szabo",
            "Vanea Chiprianov",
            "Gavin Puddy",
            "Marianne Rieckmann",
            "Dan Fraser",
            "Cathlyn Aston"
        ],
        "file_path": "data/sosym-all/s10270-016-0547-8.pdf"
    },
    {
        "title": "Use, potential, and showstoppers of models in automotive requirements engineering",
        "submission-date": "2017/09",
        "publication-date": "2018/05",
        "abstract": "Several studies report that the use of model-centric methods in the automotive domain is widespread and offers several beneﬁts. However, existing work indicates that few modelling frameworks explicitly include requirements engineering (RE), and that natural language descriptions are still the status quo in RE. Therefore, we aim to increase the understanding of current and potential future use of models in RE, with respect to the automotive domain. In this paper, we report our ﬁndings from a multiple-case study with two automotive companies, collecting interview data from 14 practitioners. Our results show that models are used for a variety of different purposes during RE in the automotive domain, e.g. to improve communication and to handle complexity. However, these models are often used in an unsystematic fashion and restricted to few experts. A more widespread use of models is prevented by various challenges, most of which align with existing work on model use in a general sense. Furthermore, our results indicate that there are many potential beneﬁts associated with future use of models during RE. Interestingly, existing research does not align well with several of the proposed use cases, e.g. restricting the use of models to informal notations for communication purposes. Based on our ﬁndings, we recommend a stronger focus on informal modelling and on using models for multi-disciplinary environments. Additionally, we see the need for future work in the area of model use, i.e. information extraction from models by non-expert modellers.",
        "keywords": [
            "Modelling",
            "MDE",
            "MBE",
            "Requirements engineering",
            "Empirical research",
            "Case study",
            "Automotive"
        ],
        "authors": [
            "Grischa Liebel",
            "Matthias Tichy",
            "Eric Knauss"
        ],
        "file_path": "data/sosym-all/s10270-018-0683-4.pdf"
    },
    {
        "title": "UML vs. classical vs. rhapsody statecharts: not all models are created equal",
        "submission-date": "2006/01",
        "publication-date": "2007/01",
        "abstract": "State machines, represented by statecharts or state machine diagrams, are an important formalism for behavioural modelling. According to the research literature, the most popular statechart formalisms appear to be Classical, UML, and that implemented by Rhapsody. These three formalisms seem to be very similar; however, there are several key syntactic and semantic differences. These differences are enough that a model written in one formalism could be ill-formed in another formalism. Worse, a model from one formalism might actually be well-formed in another, but be interpreted differently due to the semantic differences. This paper summarizes the results of an informal comparative study of these three formalisms with the help of several illustrativeexamples. Wepresent aclassiﬁcationof thediffer-ences according to the nature of potential problems caused by each difference. In addition, for each differ-ence we discuss how translation between formalisms can be achieved, if at all.",
        "keywords": [],
        "authors": [
            "Michelle L. Crane",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-006-0042-8.pdf"
    },
    {
        "title": "VPM: A visual, precise and multilevel metamodeling framework for describing mathematical domains and UML",
        "submission-date": "2003/02",
        "publication-date": "2003/08",
        "abstract": "As UML 2.0 is evolving into a family of languages with individually speciﬁed semantics, there is an increasing need for automated and provenly correct model transformations that (i) assure the integration of local views (diﬀerent diagrams) of the system into a consistent global view, and, (ii) provide a well-founded mapping from UML models to diﬀerent semantic domains (Petri nets, Kripke automaton, process algebras, etc.) for formal analysis purposes as foreseen, for instance, in submissions for the OMG RFP for Schedulability, Per-formance and Time. However, such transformations into diﬀerent semantic domains typically require the deep understanding of the underlying mathematics, which hinders the use of formal speciﬁcation techniques in industrial applications. In the paper, we propose a multilevel metamodeling technique with precise static and dynamic semantics (based on a reﬁnement calculus and graph transformation) where the structure and operational semantics of mathematical models can be deﬁned in a UML notation without cumbersome mathematical formulae.",
        "keywords": [
            "Metamodeling",
            "Formal semantics",
            "Reﬁnement",
            "Model transformation",
            "Graph transformation"
        ],
        "authors": [
            "D´aniel Varr´o",
            "Andr´as Pataricza"
        ],
        "file_path": "data/sosym-all/s10270-003-0028-8.pdf"
    },
    {
        "title": "Universal conceptual modeling: principles, beneﬁts, and an agenda for conceptual modeling research",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "The paper proposes universal conceptual modeling, conceptual modeling that strives to be as general-purpose as possible and accessible to anyone, professionals and non-experts alike. The idea of universal conceptual modeling is meant to catalyze new thinking in conceptual modeling and be used to evaluate and develop conceptual modeling solutions, such as modeling languages, approaches for requirements elicitation, or modeling tools. These modeling solutions should be usable by as many people and design agents as possible and for as many purposes as possible, aspiring to the ideals of universal conceptual model-ing. We propose foundations of universal conceptual modeling in the form of six principles: ﬂexibility, accessibility, ubiquity, minimalism, primitivism, and modularity. We then demonstrate the utility of these principles to evaluate existing conceptual modeling languages and understand conceptual modeling practices. Finally, we propose future research opportunities meant to realize the ideals of universal conceptual modeling.",
        "keywords": [
            "Universal conceptual modeling",
            "Conceptual modeling",
            "Conceptual modeling foundations",
            "Universal modeling language",
            "Datish",
            "RDF",
            "Inclusive modeling",
            "Universal design"
        ],
        "authors": [
            "Roman Lukyanenko",
            "Binny M. Samuel",
            "Jeﬀrey Parsons",
            "Veda C. Storey",
            "Oscar Pastor",
            "Araz Jabbari"
        ],
        "file_path": "data/sosym-all/s10270-024-01207-8.pdf"
    },
    {
        "title": "Evolution styles: foundations and models for software architecture evolution",
        "submission-date": "2011/03",
        "publication-date": "2012/11",
        "abstract": "As new market opportunities, technologies, platforms, and frameworks become available, systems require large-scale and systematic architectural restructuring to accommodate them. Today’s architects have few techniques to help them plan this architecture evolution. In particular, they have little assistance in planning alternative evolution paths, trading off various aspects of the different paths, or knowing best practices for particular domains. In this paper, we describe an approach for planning and reasoning about architecture evolution. Our approach focuses on providing architects with the means to model prospective evolution paths and supporting analysis to select among these candidate paths. To demonstrate the usefulness of our approach, we show how it can be applied to an actual architecture evolution. In addition, we present some theoretical results about our evolution path constraint specification language.",
        "keywords": [
            "Software architecture"
        ],
        "authors": [
            "Jeffrey M. Barnes",
            "David Garlan",
            "Bradley Schmerl"
        ],
        "file_path": "data/sosym-all/s10270-012-0301-9.pdf"
    },
    {
        "title": "A reference architecture for the development of GLSP-based web modeling tools",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "Web-based modeling tools provide unprecedented opportunities for the realization of modern, powerful, and usable diagram editors running in the cloud. The development of such tools, however, still poses signiﬁcant challenges for developers. The graphical language server platform (GLSP) aims to reduce some of these challenges by providing the necessary frameworks to efﬁciently create web modeling tools. However, realizing modeling tools with GLSP remains challenging and not much support for interested tool developers is provided yet. This paper discusses these challenges and lessons learned after working with GLSP and realizing several GLSP-based modeling tools. We present experiences, concepts, and a reusable reference architecture to develop and operate GLSP-based web modeling tools. As a proof of concept, we report on the realization of a GLSP-based UML editor called bigUML. Through bigUML, we show that our procedure and the reference architecture we developed resulted in a scalable and ﬂexible GLSP-based web modeling tool for the UML. The lessons learned, the procedural approach, the reference architecture, and the critical reﬂection on the challenges and opportunities of using GLSP provide valuable insights to the community and shall ease the decision of whether or not to use GLSP for future tool development projects. With this paper, we publicly release a reference implementation of our architecture.",
        "keywords": [
            "UML",
            "Software modeling",
            "GLSP",
            "Modeling tool",
            "Web modeling",
            "LSP"
        ],
        "authors": [
            "Haydar Metin",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-024-01257-y.pdf"
    },
    {
        "title": "Software engineering methods in other engineering disciplines",
        "submission-date": "2018/04",
        "publication-date": "2018/04",
        "abstract": "Software engineers are often told from experts in systems development that they “should develop their software in the same systematic and predictable way as other engineers (e.g., mechanical engineers)”. If we trace Software Engineering back to the 1968 NATO Science Committee [1] conference in Garmisch, Germany, organized by F. L. Bauer, then the discipline of Software Engineering is approaching 50 years old. Yet, Software Engineering is not a discipline that always develops products using processes similar to traditional mechanical engineering methods. Software developers have their own portfolio of methods, ranging from heavyweight documentation-oriented approaches to the much more beloved agile and light-weight methods. Even though there is always potential for optimization, agile techniques have reached a level where software developers can produce reliable products using cost efficient, relatively predictable and controllable processes. What seems to be even more important is that the way of developing software is tightly connected to innovative processes that not only lead to novel ideas about how to implement software, but also to new ideas about potential features and services that can be integrated into software solutions. The tight coupling of software development and innovation is closely related to the strong connection between requirements elicitation and direct implementation in agile processes, where the same stakeholders are responsible for the innovations of development. Furthermore, the invention of new control and data structures in object-oriented development can drive the innovation of new features. Companies in Silicon Valley have used this mood of innovation effectively. They have integrated software services into their core business model, which has led to innovative thinking and effective, agile development of products in other domains (e.g., medical services, autonomous driving, electric cars, finance). Traditional companies based on fundamental engineering processes may feel threatened by the radical change and fast rate of innovation that the new technologies offer. There are challenges in training classical engineers to adopt a new form of project organization, where the responsibilities are given to the developers much more than to the management hierarchies. However, this mindset is necessary to improve innovation. Traditional engineering-based companies, as well as other increasingly software-intensive companies, are now trying to catch up. Many good examples in these areas give us hope that change is possible. In systems that require expertise from multiple areas of engineering,traditional engineers are becoming more open to software engineering practices. Furthermore, software development methodologies are being adopted more frequently into traditional engineering practice. In a recent and well-known German TV Show [2], a highly esteemed expert in mechanical and production engineering, Günther Schuh, was asked why he and his team were able to produce the “StreetScooter” electric car so quickly. Schuh said, “Because they have used computer science methods”. In particular, Schuh mentioned agility as a key catalyst to the increased speed of development. The portfolio of methods that software engineering has developed over the last two decades is effective and efficient in multiple engineering domains, allowing innovative products to be created at a quick pace. After 50 years, Software Engineering has found its toolset of methods, languages, concepts and techniques that allow software developers to create various forms of software products, services and embedded software, which in turn enables various new business concepts. One of our main problems now is to teach these practices in appropriate forms to many more computer scientists and other engineers, such that there are enough people available to deliver new innovative projects in the future.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-018-0674-5.pdf"
    },
    {
        "title": "Improving manual reviews in function‑centered engineering of embedded systems using a dedicated review model",
        "submission-date": "2017/08",
        "publication-date": "2019/02",
        "abstract": "In model-based engineering of embedded systems, manual validation activities such as reviews and inspections are needed to ensure that the system under development satisfies the stakeholder intentions. During the engineering process, changes in the stakeholder intentions typically trigger revisions of already developed and documented engineering artifacts including requirements and design specifications. In practice, changes in stakeholder intentions are often not immediately perceived and not properly documented. Moreover, they are quite often not consistently incorporated into all relevant engineering artifacts. In industry, typically manual reviews are executed to ensure that the relevant stakeholder intentions are adequately considered in the engineering artifacts. In this article, we introduce a dedicated review model to aid the reviewer in conducting manual reviews of behavioral requirements and functional design specification—two core artifacts in function-centered engineering of embedded software. To investigate whether the proposed solution is beneficial we conducted controlled experiments showing that the use of the dedicated review model can significantly increase the effectiveness and efficiency of manual reviews. Additionally, the use of the dedicated review model leads to significantly more confident decisions of the reviewers and is perceived by the reviewers as significantly more supportive compared with reviews without the dedicated review model.",
        "keywords": [
            "Behavioral requirements",
            "Embedded software",
            "Functional design",
            "Review model",
            "Requirements engineering",
            "Perspective-based review",
            "Model transformations"
        ],
        "authors": [
            "Marian Daun",
            "Thorsten Weyer",
            "Klaus Pohl"
        ],
        "file_path": "data/sosym-all/s10270-019-00723-2.pdf"
    },
    {
        "title": "Contents for a Model-Based Software Engineering Body of Knowledge",
        "submission-date": "2019/05",
        "publication-date": "2019/07",
        "abstract": "Although Model-Based Software Engineering (MBE) is a widely accepted Software Engineering (SE) discipline, no agreed-upon core set of concepts and practices (i.e., a Body of Knowledge) has been deﬁned for it yet. With the goals of characterizing the contents of the MBE discipline, promoting a global consistent view of it, clarifying its scope with regard to other SE disciplines, and deﬁning a foundation for the development of educational curricula on MBE, this paper proposes the contents for a Body of Knowledge for MBE. We also describe the methodology that we have used to come up with the proposed list of contents, as well as the results of a survey study that we conducted to sound out the opinion of the community on the importance of the proposed topics and their level of coverage in the existing SE curricula.",
        "keywords": [
            "Model-Based Software Engineering",
            "Body of Knowledge",
            "Core concepts",
            "Education"
        ],
        "authors": [
            "Loli Burgueño",
            "Federico Ciccozzi",
            "Michalis Famelis",
            "Gerti Kappel",
            "Leen Lambers",
            "Sebastien Mosser",
            "Richard F. Paige",
            "Alfonso Pierantonio",
            "Arend Rensink",
            "Rick Salay",
            "Gabriele Taentzer",
            "Antonio Vallecillo",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-019-00746-9.pdf"
    },
    {
        "title": "Modeling NASA swarm-based systems: using agent-oriented software engineering and formal methods",
        "submission-date": "2009/09",
        "publication-date": "2009/10",
        "abstract": "The need to collect new data and perform new science is causing the complexity of NASA missions to continually increase. This complexity needs to be controlled via new technological advancements and balanced with a reduction in mission and operation costs. Planned and hypothesized missions involve self-management, biological-inspiration based on swarms, and autonomous operation as a means of achieving these goals. We consider a tailored software engineering approach to developing such systems based on agent-oriented software engineering and formal methods. We report on advances in modeling, implementing, and testing NASA swarm-based concept missions.",
        "keywords": [
            "Swarms",
            "Emergent behavior",
            "Agent-oriented software engineering",
            "Formal methods"
        ],
        "authors": [
            "Joaquin Peña",
            "Christopher A. Rouff",
            "Mike Hinchey",
            "Antonio Ruiz-Cortés"
        ],
        "file_path": "data/sosym-all/s10270-009-0135-2.pdf"
    },
    {
        "title": "A Multi-Paradigm Modelling approach to live modelling",
        "submission-date": "2018/02",
        "publication-date": "2018/10",
        "abstract": "To develop complex systems and tackle their inherent complexity, (executable) modelling takes a prominent role in the development cycle. But whereas good tool support exists for programming, tools for executable modelling have not yet reached the same level of functionality and maturity. In particular, live programming is seeing increasing support in programming tools, allowing users to dynamically change the source code of a running application. This signiﬁcantly reduces the edit–compile–debug cycle and grants the ability to gauge the effect of code changes instantly, aiding in debugging and code comprehension in general. In the modelling domain, however, live modelling only has limited support for a few formalisms. In this paper, we propose a Multi-Paradigm Modelling approach to add liveness to modelling languages in a generic way, which is reusable across multiple formalisms. Live programming concepts and techniques are transposed to (domain-speciﬁc) executable modelling languages, clearly distinguishing between generic and language-speciﬁc concepts. To evaluate our approach, live modelling is implemented for three modelling languages, for which the implementation of liveness substantially differs. For all three cases, the exact same structured process was used to enable live modelling, which only required a “sanitization” operation to be deﬁned.",
        "keywords": [
            "Live programming",
            "Live modelling",
            "Debugging",
            "Multi-Paradigm Modelling"
        ],
        "authors": [
            "Yentl Van Tendeloo",
            "Simon Van Mierlo",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-018-0700-7.pdf"
    },
    {
        "title": "Consolidation of database check constraints",
        "submission-date": "2016/07",
        "publication-date": "2017/11",
        "abstract": "Independent modeling of various modules of an information system (IS), and consequently database subschemas, may result in formal or semantic conﬂicts between the modules being modeled. Such conﬂicts may cause collisions between the integrated database schema of a whole IS and the modeled subschemas. In our previous work, we have proposed criteria and algorithms for identifying and resolving such conﬂicts so as to provide a consolidation of database subschemas with the integrated database schema with respect to various database concepts, such as domains, relation schemes, primary key constraints and referential integrity constraints. In this paper, we propose a new approach and algorithms for identifying conﬂicts and testing consolidation of subschemas with the integrated database schema against check constraints. The proposed approach is based on satisﬁability modulo theory (SMT) solvers. Hereby, we propose the integration of SMT solvers into our MDSD tool, aimed at supporting a database schema integration process.",
        "keywords": [
            "Database subschema consolidation",
            "Check constraint collision",
            "Implication problem",
            "SMT solver"
        ],
        "authors": [
            "Nikola Obrenovi´c",
            "Ivan Lukovi´c",
            "Sonja Risti´c"
        ],
        "file_path": "data/sosym-all/s10270-017-0637-2.pdf"
    },
    {
        "title": "Scalable modeling technologies in the wild: an experience report on wind turbines control applications development",
        "submission-date": "2019/02",
        "publication-date": "2020/01",
        "abstract": "Scalability in modeling has many facets, including the ability to build larger models and domain-speciﬁc languages (DSLs) efﬁciently. With the aim of tackling some of the most prominent scalability challenges in model-based engineering (MBE), the MONDO EU project developed the theoretical foundations and open-source implementation of a platform for scalable modeling and model management. The platform includes facilities for building large graphical DSLs, for splitting large models into sets of smaller interrelated fragments, to index large collections of models to speed-up their querying, and to enable the collaborative construction and reﬁnement of complex models, among other features. This paper reports on the tools provided by MONDO that Ikerlan, a medium-sized technology center which in the last decade has embraced the MBE paradigm, adopted in order to improve their processes. This experience produced as a result a set of model editors and related technologies that fostered collaboration and scalability in the development of wind turbine control applications. In order to evaluate the beneﬁts obtained, an on-site evaluation of the tools was performed. This evaluation shows that scalable MBE technologies give new growth opportunities to small- and medium-sized organizations.",
        "keywords": [
            "Model-based engineering (MBE)",
            "Scalability",
            "Domain-speciﬁc graphical modeling languages",
            "Collaborative modeling",
            "Model indexing",
            "Experience report"
        ],
        "authors": [
            "Abel Gómez",
            "Xabier Mendialdua",
            "Konstantinos Barmpis",
            "Gábor Bergmann",
            "Jordi Cabot",
            "Xabier de Carlos",
            "Csaba Debreceni",
            "Antonio Garmendia",
            "Dimitrios S. Kolovos",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-020-00776-8.pdf"
    },
    {
        "title": "Formalizing the structural semantics of domain-speciﬁc modeling languages",
        "submission-date": "2007/04",
        "publication-date": "2008/12",
        "abstract": "Model-based approaches to system design are now widespread and successful. These approaches make extensive use of model structure to describe systems using domain-speciﬁc abstractions, to specify and implement model transformations, and to analyze structural properties of models. In spite of its general importance the structural semantics of modeling languages are not well-understood. In this paper we develop the formal foundations for the structural semantics of domain-speciﬁc modeling languages (DSML), including the mechanisms by which metamodels specify the structural semantics of DSMLs. Additionally, we show how our formalization can complement existing tools, and how it yields algorithms for the analysis of DSMLs and model transformations.",
        "keywords": [
            "Model-based Design",
            "Domain-speciﬁc modeling languages",
            "Structural semantics",
            "Metamodeling",
            "Formal logic",
            "Horn logic"
        ],
        "authors": [
            "Ethan Jackson",
            "Janos Sztipanovits"
        ],
        "file_path": "data/sosym-all/s10270-008-0105-0.pdf"
    },
    {
        "title": "A systematic literature review of cross-domain model consistency checking by model management tools",
        "submission-date": "2019/09",
        "publication-date": "2020/10",
        "abstract": "Objective The goal of this study is to identify gaps and challenges related to cross-domain model management focusing on consistency checking. Method We conducted a systematic literature review. We used the keyword-based search on Google Scholar, and we identiﬁed 618 potentially relevant studies; after applying inclusion and exclusion criteria, 96 papers were selected for further analysis. Results The main ﬁndings/contributions are: (i) a list of available tools used to support model management; (ii) 40% of the tools can provide consistency checking on models of different domains and 25% on models of the same domain, and 35% do not provide any consistency checking; (iii) available strategies to keep the consistency between models of different domains are not mature enough; (iv) most of the tools that provide consistency checking on models of different domains can only capture up to two inconsistency types; (v) the main challenges associated with tools that manage models on different domains are related to interoperability between tools and the consistency maintenance. Conclusion The results presented in this study can be used to guide new research on maintaining the consistency between models of different domains. Example of further research is to investigate how to capture the Behavioral and Reﬁnement inconsistency types. This study also indicates that the tools should be improved in order to address, for example, more kinds of consistency check.",
        "keywords": [
            "Model management",
            "Systems engineering",
            "Model-based systems engineering"
        ],
        "authors": [
            "Weslley Torres",
            "Mark G. J. van den Brand",
            "Alexander Serebrenik"
        ],
        "file_path": "data/sosym-all/s10270-020-00834-1.pdf"
    },
    {
        "title": "An approach to clone detection in sequence diagrams and its application to security analysis",
        "submission-date": "2015/08",
        "publication-date": "2016/09",
        "abstract": "Duplication in software systems is an important issue in software quality assurance. While many methods for software clone detection in source code and structural models have been described in the literature, little has been done on similarity in the dynamic behaviour of interactive systems. In this paper, we present an approach to identifying near-miss interaction clones in reverse-engineered UML sequence diagrams. Our goal is to identify patterns of interaction (“conversations”) that can be used to characterize and abstract the run-time behaviour of web applications and other interactive systems. In order to leverage existing robust near-miss code clone technology, our approach is text-based, working on the level of XMI, the standard interchange serialization for UML. Clone detection in UML behavioural models, such as sequence diagrams, presents a number of challenges— first, it is not clear how to break a continuous stream of interaction between lifelines (representing the objects or actors in the system) into meaningful conversational units. Second, unlike programming languages, the XMI text representation for UML is highly non-local, using attributes to reference-related elements in the model file remotely. In this work, we use a set of contextualizing source transformations on the XMI text representation to localize related elements, exposing the hidden hierarchical structure of the model and allowing us to granularize behavioural interactions into conversational units. Then we adapt NICAD, a robust near-miss code clone detection tool, to help us identify conversational clones in reverse-engineered behavioural models. These conversational clones are then analysed to find worrisome interactions that may indicate security access violations.",
        "keywords": [
            "Model clone detection",
            "Model based security analysis"
        ],
        "authors": [
            "Manar H. Alalﬁ",
            "Elizabeth P. Antony",
            "James R. Cordy"
        ],
        "file_path": "data/sosym-all/s10270-016-0557-6.pdf"
    },
    {
        "title": "Contract-based veriﬁcation of discrete-time multi-rate Simulink models",
        "submission-date": "2013/11",
        "publication-date": "2015/06",
        "abstract": "This paper presents an approach to modular contract-based veriﬁcation of discrete-time multi-rate Simulink models. The veriﬁcation approach uses a translation of Simulink models to sequential programs that can then be veriﬁed using traditional software veriﬁcation techniques. Automatic generation of the proof obligations needed for veriﬁcation of correctness with respect to contracts, and automatic proofs are also discussed. Furthermore, the paper provides detailed discussions about the correctness of each step in the veriﬁcation process. The veriﬁcation approach is demonstrated on a case study involving control software for prevention of pressure peaks in hydraulics systems.",
        "keywords": [
            "Automated veriﬁcation",
            "Reﬁnement",
            "Control systems",
            "SMT solving",
            "Synchronous data ﬂow"
        ],
        "authors": [
            "Pontus Boström",
            "Jonatan Wiik"
        ],
        "file_path": "data/sosym-all/s10270-015-0477-x.pdf"
    },
    {
        "title": "Bridging the chasm between MDE and the world of compilation",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "Modeling and transforming have always been the cornerstones of software system development, albeit often investigated by different research communities. Modeling addresses how information is represented and processed, while transformation cares about what the results of processing this information are. To address the growing complexity of software systems, model-driven engineering (MDE) leverages domain-speciﬁc languages to deﬁne abstract models of systems and automated methods to process them. Meanwhile, compiler technology mostly concentrates on advanced techniques and tools for program transformation. For this, it has developed complex analyses and transformations (from lexical and syntactic to semantic analyses, down to platform-speciﬁcoptimizations).Thesetwocommunitiesappeartoday quite complementary and are starting to meet again in the software language engineering (SLE) ﬁeld. SLE addresses all the stages of a software language lifecycle, from its deﬁnition to its tooling. In this article, we show how SLE can lean on the expertise of both MDE and compiler research communities and how each community can bring its solutions to the other one. We then draw a picture of the current state of SLE and of the challenges it has still to face.",
        "keywords": [
            "Model-driven engineering (MDE)",
            "Domain-speciﬁc language (DSL)",
            "Compilation",
            "Intermediate representation (IR)",
            "Software language engineering (SLE)"
        ],
        "authors": [
            "Jean-Marc Jézéquel",
            "Benoit Combemale",
            "Steven Derrien",
            "Clément Guy",
            "Sanjay Rajopadhye"
        ],
        "file_path": "data/sosym-all/s10270-012-0266-8.pdf"
    },
    {
        "title": "A model for dynamic reconﬁguration in service-oriented architectures",
        "submission-date": "2010/11",
        "publication-date": "2012/02",
        "abstract": "The importance of modelling the dynamic characteristics of the architecture of software systems has long been recognised. However, the nature of the dynamics of service-oriented applications goes beyond what is currently addressed by architecture description languages (ADLs). At the heart of the service-oriented approach is the logical separation between the service need and the need-fulﬁllment mechanism, i.e., the provision of the service: the binding between the requester and the provider is deferred to run time and established at the instance level, i.e., each time the need for the service arises. As a consequence, computation in the context of service-oriented architectures transforms not only the states of the components that implement applications but also the conﬁgurations of those applications. In this paper, we present a model for dynamic reconﬁguration that is general enough to support the definition of ADLs that are able to address the full dynamics of service-oriented applications. As an instance of the model, we present a simple service-oriented ADL derived from the modelling language srmlthat we developed in the Sensoria project.",
        "keywords": [
            "Software architecture",
            "Service-oriented computing",
            "Dynamic formal modelling"
        ],
        "authors": [
            "José Luiz Fiadeiro",
            "Antónia Lopes"
        ],
        "file_path": "data/sosym-all/s10270-012-0236-1.pdf"
    },
    {
        "title": "Application and evaluation of interlinked approaches for modeling changing capabilities",
        "submission-date": "2023/03",
        "publication-date": "2024/05",
        "abstract": "The nature of modern organizations needs to be increasingly adaptive, since they are dealing with a constant demand to respond to stimuli derived from the dynamic environments they operate in. Changing their capabilities is a common response, and this makes capability management a vital aspect of organizational survivability. To date, there are no approaches specifically designed to address this specific situation. KYKLOS and Compass are two interlinked approaches of different complexity, a DSMLandacanvas,developedtosupportcapabilitychange.Asrecentlydevelopedmethods,theylackedformaldemonstration and evaluation; therefore, the goal of this article is to present the demonstration and evaluation of the two approaches by their stakeholders, in particular, business and modeling experts. A case study in a Swedish company in the ERP system consulting domain that is undergoing changes in its sales and consulting capabilities related to evolving customer requirements has been used to demonstrate and evaluate the two approaches. The process consisted of two evaluation cycles. The first cycle concerned KYKLOS and used two categories of evaluators, the business experts and the modeling experts. While the modeling experts evaluated positively the method, the business experts had difficulties associated with its ease of use and adoption. This resulted in the development of Compass, which was evaluated by business experts during the second evaluation cycle. Compass was evaluated more positively in terms of the difficult aspects, but the challenge is ongoing and motivates further future research.",
        "keywords": [
            "Capability management",
            "Enterprise modeling",
            "DSML",
            "Method evaluation",
            "Change management"
        ],
        "authors": [
            "Georgios Koutsopoulos",
            "Anna Andersson",
            "Janis Stirna",
            "Martin Henkel"
        ],
        "file_path": "data/sosym-all/s10270-024-01181-1.pdf"
    },
    {
        "title": "Module superimposition: a composition technique for rule-based model transformation languages",
        "submission-date": "2008/11",
        "publication-date": "2009/10",
        "abstract": "As the application of model transformation becomes increasingly commonplace, the focus is shifting from model transformation languages to the model transformations themselves. The properties of model transformations, such as scalability, maintainability and reusability, have become important. Composition of model transformations allows for the creation of smaller, maintainable and reusable transformation definitions that together perform a larger transformation. This paper focuses on composition for two rule-based model transformation languages: the ATLAS Transformation Language (ATL) and the QVT Relations language. We propose a composition technique called module superimposition that allows for extending and overriding rules in transformation modules. We provide executable semantics as well as a concise and scalable implementation of module superimposition based on ATL.",
        "keywords": [
            "Software engineering",
            "Model driven engineering",
            "Model transformation"
        ],
        "authors": [
            "Dennis Wagelaar",
            "Ragnhild Van Der Straeten",
            "Dirk Deridder"
        ],
        "file_path": "data/sosym-all/s10270-009-0134-3.pdf"
    },
    {
        "title": "SoSyM reflections: the 2020 \"State of the Journal\" report",
        "submission-date": "2021/02",
        "publication-date": "2021/02",
        "abstract": "When writing the “2019 State of the Journal Report” at this same time last year, we could not have predicted the global changes that would beset us with so many challenges from the emergence of the COVID-19 pandemic. Some of us lost loved ones, friends, and colleagues, while the entire world adapted to working from home, participating in remote classrooms, and adopting safety precautions as a new means of daily life. The research community was also affected, with the cessation of travel leading to virtual conferences. Thanks to the heroic efforts of many conference chairs1 adapting to the quick pace of change, scientific discussions contin-ued, but sometimes in a less personable form. Journals also experienced changes with submissions on the rise, but fewer reviewers available to assist with the evaluation because of personal challenges faced by many.\nYet, in the presence of a global pandemic, scientific con-tributions continued in the software and systems modeling community. The number of SoSyM submissions over the past year saw an increase, while the general health of the journal remains strong. Measures put in place recently, such as the second year of moving to six issues per year, have helped to reduce the time to publication significantly. The Open Access movement is also progressing, with Springer announcing new initiatives to make SoSyM publications more accessible to a broader community of researchers. The rest of this editorial summarizes the progress made by the journal during this unprecedented situation.",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-021-00871-4.pdf"
    },
    {
        "title": "An integrated multi-level modeling approach for industrial-scale data interoperability",
        "submission-date": "2015/02",
        "publication-date": "2016/04",
        "abstract": "Multi-level modeling is currently regaining attention in the database and software engineering community with different emerging proposals and implementations. One driver behind this trend is the need to reduce model complexity, a crucial aspect in a time of analytics in Big Data that deal with complex heterogeneous data structures. So far no standard exists for multi-level modeling. Therefore, different formalization approaches have been proposed to address multi-level modeling and veriﬁcation in different frameworks and tools. In this article, we present an approach that integrates the formalization, implementation, querying, andveriﬁcationofmulti-levelmodels.Theapproachhasbeen evaluated in an open-source F-Logic implementation and applied in a large-scale data interoperability project in the oil and gas industry. The outcomes show that the framework is adaptable to industry standards, reduces the complexity of speciﬁcations, and supports the veriﬁcation of standards from a software engineering point of view.",
        "keywords": [
            "Multi-level modeling",
            "Interoperability",
            "Multi-level model reasoning",
            "F-Logic",
            "Multi-level model querying",
            "Multi-level model veriﬁcation"
        ],
        "authors": [
            "Muzaffar Igamberdiev",
            "Georg Grossmann",
            "Matt Selway",
            "Markus Stumptner"
        ],
        "file_path": "data/sosym-all/s10270-016-0520-6.pdf"
    },
    {
        "title": "gLTSdiff: a generalized framework for structural comparison of software behavior",
        "submission-date": "2024/04",
        "publication-date": "Not found",
        "abstract": "Structural comparison of state machine models – such as labeled transition systems and (extended) ﬁnite automata – is used for numerous applications, such as ﬁnding potential behavioral regressions in new software versions, evaluating the accuracy of different model learning algorithms, and ﬁngerprinting software for security applications. The state-of-the-art LTSDiff structural comparison algorithm has limited assumptions, making it broadly applicable. However, representation-speciﬁc information is not taken into account, requiring adaptations to prevent sub-optimal or even invalid results. We introduce gLTSdiff, which generalizes and extends LTSDiff, allowing a wide range of state machine models to be compared, by recursively comparing over the structure of state and transition labels. Additional challenges that we faced while applying LTSDiff in industrial practice are also addressed by gLTSdiff, as it rewrites undesired difference patterns, supports comparison of any number of input models, and allows for an effort/quality trade-off. We formally deﬁne gLTSdiff, and make it available as an extensible open source library for structural model comparison. Using multiple large-scale industrial and open source case studies, we evaluate both its practical value and its various improvements.",
        "keywords": [
            "Software behavior",
            "State machines",
            "Structural comparison",
            "Industrial application"
        ],
        "authors": [
            "Dennis Hendriks",
            "Wytse Oortwijn"
        ],
        "file_path": "data/sosym-all/s10270-024-01239-0.pdf"
    },
    {
        "title": "Testing concurrent user behavior of synchronous web applications with Petri nets",
        "submission-date": "2016/09",
        "publication-date": "2018/02",
        "abstract": "Web applications are now used in every aspect of our lives to manage work, provide products and services, read email, and provide entertainment. The software technologies used to build web applications provide features that help designers provide flexible functionality, but that are challenging to model and test. In particular, the network-based request-response model of programming means that web applications are inherently “stateless” and implicitly concurrent. They are stateless because a new network connection is made for each request (for example, when a user clicks a submit button). Thus, the server does not, by default, recognize multiple requests from the same user. Web applications are also concurrent because multiple users can use the same web application at the same time, creating contention for the same resources. Unfortunately, most web application testing does not adequately evaluate these aspects of web applications, leaving many software faults in deployed web applications. Part of this problem is because most traditional software modeling tools (such as UML) do not have built-in support for the stateless and concurrent aspects of web applications. This research project uses a novel model that is based on Petri nets to describe certain aspects of the behavior of web applications. This paper makes several contributions. We present a novel technique to design tests from this model that explicitly tests concurrency in web applications. We present novel coverage criteria that are defined on the Petri net model. We present results from an empirical study of 18 web applications with 343 components and 30,186 lines of code, followed by a case study on a large industrial web application. The tests found significantly more faults than traditional requirements-based tests, with fewer tests.",
        "keywords": [
            "Web applications",
            "Model-based testing",
            "Test criteria",
            "Petri nets"
        ],
        "authors": [
            "Jeff Offutt",
            "Sunitha Thummala"
        ],
        "file_path": "data/sosym-all/s10270-018-0655-8.pdf"
    },
    {
        "title": "A dependability profile within MARTE",
        "submission-date": "2009/01",
        "publication-date": "2009/08",
        "abstract": "The importance of assessing software non-functional properties (NFP) beside the functional ones is well accepted in the software engineering community. In particular, dependability is a NFP that should be assessed early in the software life-cycle by evaluating the system behaviour under different fault assumptions. Dependability-speciﬁc modeling and analysis techniques include for example Failure Mode and Effect Analysis for qualitative evaluation, stochastic Petri nets for quantitative evaluation, and fault trees for both forms of evaluation. Uniﬁed Modeling Language (UML) may be specialized for different domains by using the proﬁle mechanism. For example, the MARTE proﬁle extends UML with concepts for modeling and quantitative analysis of real-time and embedded systems (more speciﬁcally, for schedulability and performance analysis). This paper proposes to add to MARTE a proﬁle for dependability analysis and modeling (DAM). A case study of an intrusion-tolerant message service will offer insight on how the MAR-TE-DAM proﬁle can be used to derive a stochastic Petri net model for performance and dependability assessment.",
        "keywords": [],
        "authors": [
            "Simona Bernardi",
            "José Merseguer",
            "Dorina C. Petriu"
        ],
        "file_path": "data/sosym-all/s10270-009-0128-1.pdf"
    },
    {
        "title": "Special section of BPMDS’2020 business process management meets data",
        "submission-date": "2022/03",
        "publication-date": "2022/04",
        "abstract": "The business process modeling, development and support (BPMDS) working conference series serves as a meeting place for researchers and practitioners in the areas of Business Process Modeling, Development and Support. This special section follows the 21st edition of the BPMDS series, organized in conjunction with CAiSE’20, which was held online in Grenoble, France, June 2020. BPMDS’2020 received 30 submissions from 19 countries, and 13 papers were selected and published in Springer LNBIP 387 volume. The theme of BPMDS’2020: ‘BPM meets Data’ follows the emergence of data science as a prominent area, and is thus investigating various aspects of the relations between processes and data.",
        "keywords": [],
        "authors": [
            "Pnina Soffer",
            "Selmin Nurcan"
        ],
        "file_path": "data/sosym-all/s10270-022-00997-z.pdf"
    },
    {
        "title": "Live process modeling with the BPMN Sketch Miner",
        "submission-date": "2021/02",
        "publication-date": "2022/06",
        "abstract": "BPMN Sketch Miner is a modeling environment for generating visual business process models starting from constrained\nnatural language textual input. Its purpose is to support business process modelers who need to rapidly sketch visual BPMN\nmodels during interviews and design workshops, where participants should not only provide input but also give feedback\non whether the sketched visual model represents accurately what has been described during the discussion. In this article,\nwe present a detailed description of the BPMN Sketch Miner design decisions and list the different control ﬂow patterns\nsupported by the current version of its textual DSL. We also summarize the user study and survey results originally published\nin MODELS 2020 concerning the tool usability and learnability and present a new performance evaluation regarding the\nvisual model generation pipeline under actual usage conditions. The goal is to determine whether it can support a rapid\nmodel editing cycle, with live synchronization between the textual description and the visual model. This study is based on\na benchmark including a large number of models (1350 models) exported by users of the tool during the year 2020. The\nmain results indicate that the performance is sufﬁcient for a smooth live modeling user experience and that the end-to-end\nexecution time of the text-to-model-to-visual pipeline grows linearly with the model size, up to the largest models (with 195\nlines of textual description) found in the benchmark workload.",
        "keywords": [
            "Business Process Model and Notation (BPMN)",
            "Process mining",
            "Domain-speciﬁc languages",
            "Performance\nevaluation"
        ],
        "authors": [
            "Ana Ivanchikj",
            "Souhaila Serbout",
            "Cesare Pautasso"
        ],
        "file_path": "data/sosym-all/s10270-022-01009-w.pdf"
    },
    {
        "title": "GReTL: an extensible, operational, graph-based transformation language",
        "submission-date": "2011/04",
        "publication-date": "2012/05",
        "abstract": "This article introduces the graph-based transformation language GReTL. GReTL is operational, and transformations are either speciﬁed in plain Java using the GReTL API or in a simple domain-speciﬁc language. GReTL follows the conception of incrementally constructing the target meta-model together with the target graph. When creating a new metamodel element, a set-based semantic expression is speciﬁed that describes the set of instances that have to be created in the target graph. This expression is deﬁned as a query on the source graph. GReTL is a kernel language consisting of a minimal set of operations, but it is designed for being extensible. Custom higher-level operations can be built on top of the kernel operations easily. After a description of the foundations of GReTL, its most important elements are introduced along with a transformation example in the ﬁeld of metamodel integration. Insights into the design of the GReTL API are given, and a convenience copy operation is implemented to demonstrate GReTL’s extensibility.",
        "keywords": [
            "Model transformation",
            "Graph transformation",
            "Metamodel merging"
        ],
        "authors": [
            "Jürgen Ebert",
            "Tassilo Horn"
        ],
        "file_path": "data/sosym-all/s10270-012-0250-3.pdf"
    },
    {
        "title": "Supporting the reconciliation of models of object behaviour",
        "submission-date": "2002/09",
        "publication-date": "2004/04",
        "abstract": "This paper presents Reconciliation+, a method which identifies overlaps between models of software systems behaviour expressed as UML object interaction diagrams (i.e., sequence and/or collaboration diagrams), checks whether the overlapping elements of these models satisfy specific consistency rules and, in cases where they violate these rules, guides software designers in handling the detected inconsistencies. The method detects overlaps between object interaction diagrams by using a probabilistic message matching algorithm that has been developed for this purpose. The guidance to software designers on when to check for inconsistencies and how to deal with them is delivered by enacting a built-in process model that specifies the consistency rules that can be checked against overlapping models and different ways of handling violations of these rules. Reconciliation+ is supported by a toolkit. It has also been evaluated in a case study. This case study has produced positive results which are discussed in the paper.",
        "keywords": [
            "Consistency management",
            "Software design models",
            "Object interaction diagrams"
        ],
        "authors": [
            "George Spanoudakis",
            "Hyoseob Kim"
        ],
        "file_path": "data/sosym-all/s10270-004-0054-1.pdf"
    },
    {
        "title": "The KlaperSuite framework for model-driven reliability analysis of component-based systems",
        "submission-date": "2012/02",
        "publication-date": "2013/03",
        "abstract": "Automatic prediction tools play a key role in enabling the application of non-functional requirements analysis,tosimplifytheselectionandtheassemblyofcomponentsforcomponent-basedsoftwaresystems,andinreducing the need for strong mathematical skills for software designers. By exploiting the paradigm of Model-Driven Engineering (MDE), it is possible to automatically transform design models into analytical models, thus enabling formal property veriﬁcation. MDE is the core paradigm of the Klaper-Suite framework presented in this paper, which exploits the KLAPER pivot language to ﬁll the gap between design and analysis of component-based systems for reliability properties. KlaperSuite is a family of tools empowering designers with the ability to capture and analyze quality of service viewsoftheirsystems,bybuildingaone-clickbridgetowards a number of established veriﬁcation instruments. In this arti-cle, we concentrate on the reliability-prediction capabilities of KlaperSuite and we evaluate them with respect to several case studies from literature and industry.",
        "keywords": [
            "Model-driven engineering",
            "Reliability analysis",
            "Component-based systems"
        ],
        "authors": [
            "Andrea Ciancone",
            "Mauro Luigi Drago",
            "Antonio Filieri",
            "Vincenzo Grassi",
            "Heiko Koziolek",
            "Raffaela Mirandola"
        ],
        "file_path": "data/sosym-all/s10270-013-0334-8.pdf"
    },
    {
        "title": "Repository mining for changes in Simulink and Stateﬂow models",
        "submission-date": "2022/04",
        "publication-date": "2023/06",
        "abstract": "Model-Based Development (MBD) is widely used for embedded controls development, with MATLAB/Simulink/Stateﬂow being one of the most used development environments in the automotive industry. Simulink/Stateﬂow models are the primary design artifacts in automotive controls MBD development, and these models must be maintained over their lifetime. We achieve this in traditional software designs through the use of information hiding. It is thus beneﬁcial to develop these models so that they facilitate likely changes that do not adversely impact the quality of the design. In order to do so, the types of frequently performed changes must be understood and appropriate language mechanisms must be available to support these changes. While some work has been done to analyze changes in Simulink/Stateﬂow models, a much deeper understanding is needed. We leveraged an extraordinary opportunity of having access to a comprehensive industrial software repository and its associated version control system to gain insight into likely changes for Simulink/Stateﬂow in automotive controls development. This analysis provides accurate feedback on actual changes made over many years to Simulink/Stateﬂow models, and classiﬁes changes to suggest how particular model changes can impact system evolution.",
        "keywords": [
            "Simulink",
            "Stateﬂow",
            "Model-Based Development",
            "Model change",
            "Repository mining",
            "Software evolution",
            "Version control system"
        ],
        "authors": [
            "Monika Jaskolka",
            "Vera Pantelic",
            "Alan Wassyng",
            "Richard F. Paige",
            "Mark Lawford"
        ],
        "file_path": "data/sosym-all/s10270-023-01113-5.pdf"
    },
    {
        "title": "Interface protocol inference to aid understanding legacy software components",
        "submission-date": "2019/06",
        "publication-date": "2020/06",
        "abstract": "High-tech companies are struggling today with the maintenance of legacy software. Legacy software is vital to many organizations as it contains the important business logic. To facilitate maintenance of legacy software, a comprehensive understanding of the software’s behavior is essential. In terms of component-based software engineering, it is necessary to completely understand the behavior of components in relation to their interfaces, i.e., their interface protocols, and to preserve this behavior during the maintenance activities of the components. For this purpose, we present an approach to infer the interface protocols of software components from the behavioral models of those components, learned by a blackbox technique called active (automata) learning. To validate the learned results, we applied our approach to the software components developed with model-based engineering so that equivalence can be checked between the learned models and the reference models, ensuring the behavioral relations are preserved. Experimenting with components having reference models and performing equivalence checking builds confidence that applying active learning technique to reverse engineer legacy software components, for which no reference models are available, will also yield correct results. To apply our approach in practice, we present an automated framework for conducting active learning on a large set of components and deriving their interface protocols. Using the framework, we validated our methodology by applying active learning on 202 industrial software components, out of which, interface protocols could be successfully derived for 156 components within our given time bound of 1h for each component.",
        "keywords": [
            "Active automata learning",
            "Interface protocols",
            "Learning framework",
            "Equivalence oracles"
        ],
        "authors": [
            "Kousar Aslam",
            "Loek Cleophas",
            "Ramon Schiﬀelers",
            "Mark van den Brand"
        ],
        "file_path": "data/sosym-all/s10270-020-00809-2.pdf"
    },
    {
        "title": "Guest Editorial to the Special Issue on MoDELS 2006",
        "submission-date": "2006/10",
        "publication-date": "2008/08",
        "abstract": "This issue of “Software and Systems Modeling” and the following one are devoted to selected papers of the ninth MODELS Conference held in Genoa, Italy, from October 1–6, 2006. The conference has established itself as one of the key international venues for the presentation of scientific results in the domain of model-driven engineering and related topics such as software modeling and model transformation.",
        "keywords": [],
        "authors": [
            "Oscar Nierstrasz",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-008-0100-5.pdf"
    },
    {
        "title": "Guest editorial for EMMSAD’2017 special section",
        "submission-date": "2018/08",
        "publication-date": "2018/08",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) series has produced 22 events, associated with CAiSE (Conference on Advanced Information Systems Engineering), from 1996 to 2017. From 2009, EMMSAD has become a 2-day working conference. The topics addressed by the EMMSAD series focus on modeling methods for software systems, enterprises, and business processes, as well as their evaluation through a variety of empirical and non-empirical approaches. The aims, topics, and history of EMMSAD can be found on the Web site at http://www.emmsad.org/. This special section follows the 22nd edition of the EMM-SAD series, organized in conjunction with CAiSE’17 at Essen, Germany, June 2017. A total of 25 submissions from 18 countries were received, and 9 full papers and 2 short papers were selected and published in Springer LNBIP 287 volume. These papers referred to: (1) modeling approaches that support decision making, business process or behavior speciﬁcation, and evolving contexts of enterprise modeling and cloud computing; and (2) evaluation and comparison of modeling languages and methods.",
        "keywords": [],
        "authors": [
            "Iris Reinhartz-Berger",
            "Wided Guédria",
            "Palash Bera"
        ],
        "file_path": "data/sosym-all/s10270-018-0693-2.pdf"
    },
    {
        "title": "Conﬂuence of aspects for sequence diagrams",
        "submission-date": "2009/07",
        "publication-date": "2011/09",
        "abstract": "The last decade has seen several aspect language proposals for UML 2 sequence diagrams. Aspects allow the modeler to deﬁne crosscutting concerns of sequence diagrams and to have these woven with the sequence diagrams of a so-called base model, in order to create a woven model. In a real-world scenario, there may be multiple aspects applicable to the same base model. This raises the need to analyse the set of aspects to identify possible aspect interactions (dependencies and conﬂicts) between applications of aspects. We call a set of aspects terminating if they may not be applied inﬁnitely many times for any given base model. Furthermore, we call a set of terminating aspects conﬂuent, if they, for any given base model, always yield the same ﬁnal result regardless of the order in which they are applied. Since conﬂuence must hold for any base model, this is a much stronger result than many of the current approaches that have addressed detection of aspect interactions limited to a speciﬁc base model. Our aspects are speciﬁed using standard sequence diagrams with some extensions. In this paper, we present a conﬂuence theory specialized for our highly expressive aspect language. For the most expressive aspects, we prove that conﬂuence is undecidable. For another class of aspects with considerable expressiveness, we prescribe an algorithm to check conﬂuence. This algorithm is based on what we call an extended critical pair analysis. These results are useful both for modelers and researchers working with sequence diagram aspects and for researchers wanting to establish a conﬂuence theory for other aspect-oriented modelling or model transformation approaches.",
        "keywords": [
            "Aspect",
            "Weave",
            "Conﬂuence",
            "Aspect interaction",
            "Aspect interference",
            "Graph transformation",
            "Critical pair",
            "UML",
            "Sequence diagram"
        ],
        "authors": [
            "Roy Grønmo",
            "Ragnhild Kobro Runde",
            "Birger Møller-Pedersen"
        ],
        "file_path": "data/sosym-all/s10270-011-0212-1.pdf"
    },
    {
        "title": "Understanding MDE projects: megamodels to the rescue for architecture recovery",
        "submission-date": "2018/12",
        "publication-date": "2019/07",
        "abstract": "Conventional wisdom on Model-Driven Engineering (MDE) suggests that this software discipline is the key to achieve superior automation, whether it be refactoring, simulation, or code generation. However, the diversity of employed languages and technologies blurs the picture making it difﬁcult to analyze existing MDE-based projects in order to retrieve architectural information to foster a better understanding about the rationale behind them. Thus, the ability of carefully analyzing projects to identify their components and their interrelationships is key to obtain representations at a higher level of abstraction that can support reuse processes. In this paper, a megamodel-based approach to the reverse engineering of model-driven projects is proposed in order to leverage the representation of the involved technologies and assets. An automated recovery technique implemented by the MDEprofiler infrastructure is presented and illustrated by analyzing community projects in terms of basic MDE artifacts (such as models and metamodels) and the usage of common technologies such as model transformations and code generators.",
        "keywords": [
            "Megamodeling",
            "Reverse engineering",
            "Architecture recovery",
            "MDE",
            "Code generator",
            "Model transformation"
        ],
        "authors": [
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Johannes Härtel",
            "Ludovico Iovino",
            "Ralf Lämmel",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-019-00748-7.pdf"
    },
    {
        "title": "Hardware architecture exploration: automatic exploration of distributed automotive hardware architectures",
        "submission-date": "2019/08",
        "publication-date": "2020/03",
        "abstract": "As the engineering of distributed embedded systems is getting more and more complex, due to increasingly sophisticated functionalities demanding more and more powerful hardware, model-based development of software-intensive embedded systems has become a de facto standard in recent years. Among other advantages, it enables design space exploration methods allowing for frontloading techniques which support a system architect already at early stages of development. In this paper, we want to present an approach which is capable of automatically generating automotive E/E architectures (electric/electronic architecture; in-car network of processing units and buses). Based on the concept of viewpoints, we will introducededicatedtechnicalmeta-models,alanguagetoformallydescribeahardwarearchitectureexplorationproblemandan automatic exploration approach using satisﬁability modulo theories. We will furthermore introduce a dedicated methodology and show how an exploration integrates into a system development process. In the end, we will evaluate our approach by applying it to an industrial use case provided by Continental.",
        "keywords": [
            "E/E architecture",
            "Design space exploration",
            "Automotive"
        ],
        "authors": [
            "Johannes Eder",
            "Sebastian Voss",
            "Andreas Bayha",
            "Alexandru Ipatiov",
            "Maged Khalil"
        ],
        "file_path": "data/sosym-all/s10270-020-00786-6.pdf"
    },
    {
        "title": "Using DSLs to manage consistency in long-lived enterprise language speciﬁcations",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Modern enterprise systems are likely to have a very long life. Their speciﬁcations therefore need to employ mechanisms that allow them to evolve during their lifetime; where they exploit generic components, these must be adaptable for use in novel situations. The paper looks at some of the issues that arise from this requirement, and how the exploitation of domain-speciﬁc language technologies in the tool-chain can assist in maintaining consistency of the speciﬁcation as a whole. First, it reviews the ﬁnal state of the family of standards supporting the ODP Enterprise Language, which is intended to handle this kind of application. In particular, it looks at the way the framework for deﬁning policies can be used to accommodate changing requirements during the lifetime of an evolving system. It also looks at the way the idea of deontic tokens enables factoring out of the management of obligations from the basic behaviour of interacting system components. It then proposes a roadmap for building tools that can be used to unify the constraints from different areas of concern into a single speciﬁcation. The approach taken is to exploit the power of domain-speciﬁc languages (DSLs) to allow designers in the various areas of concern to provide their input in terms natural to them. Finally, it looks at the way this approach promotes the establishment of a robust tool-chain capable of handling the evolution and scalability of enterprise systems. The paper uses a running example from the e-health domain to show how speciﬁc areas identiﬁed in the e-health standards can lead to language deﬁnitions, and so to tooling, that can be used to manage uniﬁed, system-wide speciﬁcations.",
        "keywords": [
            "Open Distributed Processing",
            "Domain Speciﬁc Languages",
            "Policies",
            "Deontic Tokens"
        ],
        "authors": [
            "Peter Linington",
            "Zoran Milosevic",
            "Akira Tanaka",
            "Igor Dejanovi´c"
        ],
        "file_path": "data/sosym-all/s10270-024-01243-4.pdf"
    },
    {
        "title": "Nesting in Euler Diagrams: syntax, semantics and construction",
        "submission-date": "2003/10",
        "publication-date": "2003/10",
        "abstract": "This paper considers the notion of nesting in Euler diagrams, and how nesting affects the interpretation and construction of such diagrams. After setting up the necessary definitions for concrete Euler diagrams (drawn in the plane) and abstract diagrams (having just formal structure), the notion of nestedness is defined at both concrete and abstract levels. The concept of a dual graph is used to give an alternative condition for a drawable abstract Euler diagram to be nested. The natural progression to the diagram semantics is explored and we present a “nested form” for diagram semantics. We describe how this work supports tool-building for diagrams, and how effective we might expect this support to be in terms of the proportion of nested diagrams.",
        "keywords": [
            "Visual formalisms",
            "Formal methods",
            "Euler diagrams",
            "Nested Euler diagrams",
            "Diagrammatic reasoning"
        ],
        "authors": [
            "Jean Flower",
            "John Howse",
            "John Taylor"
        ],
        "file_path": "data/sosym-all/s10270-003-0036-8.pdf"
    },
    {
        "title": "Weaving variability into domain metamodels",
        "submission-date": "2010/02",
        "publication-date": "2010/12",
        "abstract": "Domain-speciﬁc modeling languages (DSMLs) are the essence of MDE. A DSML describes the concepts of a particular domain in a metamodel, as well as their rela- tionships. Using a DSML, it is possible to describe a wide range of different models that often share a common base and vary on some parts. On the one hand, some current approaches tend to distinguish the variability language from the DSMLs themselves, implying greater learning curve for DSMLs stakeholders and a significant overhead in product line engineering. On the other hand, approaches integrat- ing variability in DSMLs lack generality and tool support. Communicated by Andy Schuerr and Bran Selic. We argue that aspect-oriented modeling techniques enabling ﬂexible metamodel composition and results obtained by the software product line community to manage and resolve variability form the pillars for a solution for integrating variability into DSMLs. In this article, we consider vari- ability as an independent and generic aspect to be woven into the DSML. In particular, we detail how variability is woven and how to perform product line derivation. We vali- date our approach through the weaving of variability into two different metamodels: Ecore—widely used for DSML def- inition—and SmartAdapters, our aspect model weaver. These results emphasize how new abilities of the language can be provided by this means.",
        "keywords": [
            "Domain speciﬁc languages",
            "Model weaving",
            "Variability and software product lines"
        ],
        "authors": [
            "Gilles Perrouin",
            "Gilles Vanwormhoudt",
            "Brice Morin",
            "Philippe Lahire",
            "Olivier Barais",
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-010-0186-4.pdf"
    },
    {
        "title": "The 2014 “State of the Journal” report",
        "submission-date": "2015/01",
        "publication-date": "2015/01",
        "abstract": "SoSyM continues to experience a high number of submissions. In 2014, 295 manuscripts were submitted, of which 69% were reviewed for regular issues, while the other 31% were submitted for special or theme sections or the industry voice column. The average number of days from submission to a final decision (that is, a final accept or reject) was 221days.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Geri Georg",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-014-0452-y.pdf"
    },
    {
        "title": "Modeling and enforcing access control policies in conversational user interfaces",
        "submission-date": "2022/11",
        "publication-date": "2023/11",
        "abstract": "Conversational user interfaces (CUIs), such as chatbots, are becoming a common component of many software systems. Although they are evolving in many directions (such as advanced language processing features, thanks to new AI-based developments), less attention has been paid to access control and other security concerns associated with CUIs, which may pose a clear risk to the systems they interface with. In this paper, we apply model-driven techniques to model and enforce access-control policies in CUIs. In particular, we present a fully ﬂedged framework to integrate the role-based access-control (RBAC) protocol into CUIs by: (1) modeling a set of access-control rules to specify permissions over the bot resources using a domain-speciﬁc language that tailors core RBAC concepts to the CUI domain; and (2) describing a mechanism to show the feasibility of automatically generating the infrastructure to evaluate and enforce the modeled access control policies at runtime.",
        "keywords": [
            "Model-driven engineering",
            "Conversational user interfaces",
            "CUIs",
            "Access-control",
            "RBAC"
        ],
        "authors": [
            "Elena Planas",
            "Salvador Martínez",
            "Marco Brambilla",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-023-01131-3.pdf"
    },
    {
        "title": "Experience with model-based performance, reliability, and adaptability assessment of a complex industrial architecture",
        "submission-date": "2011/07",
        "publication-date": "2012/09",
        "abstract": "In this paper, we report on our experience with the application of validated models to assess performance, reliability, and adaptability of a complex mission critical system that is being developed to dynamically monitor and control the position of an oil-drilling platform. We present real-time modeling results that show that all tasks are schedulable. We performed stochastic analysis of the distribution of task execution time as a function of the number of system interfaces. We report on the variability of task execution times for the expected system conﬁgurations. In addition, we have executed a system library for an important task inside the performance model simulator. We report on the measured algorithm convergence as a function of the number of vessel thrusters. We have also studied the system architecture adaptability by comparing the documented system architecture and the implemented source code. We report on the adaptability ﬁndings and the recommendations we were able to provide to the system’s architect. Finally, we have developed models of hardware and software reliability. We report on hardware and software reliability results based on the evaluation of the system architecture.",
        "keywords": [
            "Performance",
            "Reliability",
            "Adaptability"
        ],
        "authors": [
            "Daniel Dominguez Gouvêa",
            "Cyro de A. Assis D. Muniz",
            "Gilson A. Pinto",
            "Alberto Avritzer",
            "Rosa Maria Meri Leão",
            "Edmundo de Souza e Silva",
            "Morganna Carmem Diniz",
            "Vittorio Cortellessa",
            "Luca Berardinelli",
            "Julius C. B. Leite",
            "Daniel Mossé",
            "Yuanfang Cai",
            "Michael Dalton",
            "Lucia Happe",
            "Anne Koziolek"
        ],
        "file_path": "data/sosym-all/s10270-012-0264-x.pdf"
    },
    {
        "title": "A taxonomy of tool-related issues affecting the adoption of model-driven engineering",
        "submission-date": "2014/09",
        "publication-date": "2015/08",
        "abstract": "Although poor tool support is often blamed for the low uptake of model-driven engineering (MDE), recent studies have shown that adoption problems are as likely to be down to social and organizational factors as with tooling issues. This article discusses the impact of tools on MDE adoption and practice and does so while placing tooling within a broader organizational context. The article revisits previous data on MDE use in industry (19 in-depth interviews with MDE practitioners) and reanalyzes that data through the speciﬁc lens of MDE tools in an attempt to identify and categorize the issues that users had with the tools they adopted. In addition, the article presents new data: 20 new interviews in two speciﬁc companies—and analyzes it through the same lens. A key contribution of the paper is a loose taxonomy of tool-related considerations, based on empirical industry data, which can be used to reﬂect on the tooling landscape as well as inform future research on MDE tools.",
        "keywords": [
            "Model-driven engineering",
            "Modeling tools",
            "Organizational change"
        ],
        "authors": [
            "Jon Whittle",
            "John Hutchinson",
            "Mark Rounceﬁeld",
            "Håkan Burden",
            "Rogardt Heldal"
        ],
        "file_path": "data/sosym-all/s10270-015-0487-8.pdf"
    },
    {
        "title": "A cross-technology benchmark for incremental graph queries",
        "submission-date": "2020/12",
        "publication-date": "2021/12",
        "abstract": "To cope with the increased complexity of systems, models are used to capture what is considered the essence of a system. Such models are typically represented as a graph, which is queried to gain insight into the modelled system. Often, the results of these queries need to be adjusted according to updated requirements and are therefore a subject of maintenance activities. It is thus necessary to support writing model queries with adequate languages. However, in order to stay meaningful, the analysis results need to be refreshed as soon as the underlying models change. Therefore, a good execution speed is mandatory in order to cope with frequent model changes. In this paper, we propose a benchmark to assess model query technologies in the presence of model change sequences in the domain of social media. We present solutions to this benchmark in a variety of 11 different tools and compare them with respect to explicitness of incrementalization, asymptotic complexity and performance.",
        "keywords": [
            "Graph queries",
            "Graph analytics",
            "Model-driven engineering",
            "Performance benchmark",
            "Graph databases",
            "relational databases",
            "Incremental queries",
            "Incremental computing"
        ],
        "authors": [
            "Georg Hinkel",
            "Antonio Garcia-Dominguez",
            "René Schöne",
            "Artur Boronat",
            "Massimo Tisi",
            "Théo Le Calvar",
            "Frederic Jouault",
            "József Marton",
            "Tamás Nyíri",
            "János Benjamin Antal",
            "Márton Elekes",
            "Gábor Szárnyas"
        ],
        "file_path": "data/sosym-all/s10270-021-00927-5.pdf"
    },
    {
        "title": "On Enterprise-Grade Tool Support for DEMO",
        "submission-date": "2021/03",
        "publication-date": "2021/07",
        "abstract": "The Design and Engineering Methodology for Organisations (DEMO) is a core method within the discipline of enterprise engineering. It enables the creation of so-called essential models of enterprises. Such models are enterprise models that aim to focus on the organisational essence of an enterprise by leaving out (as much as possible) details of the socio-technical implementation. The organisational essence is then expressed primarily in terms of the actor roles involved, and the business transactions between these roles. The DEMO method has a firm theoretical foundation. At the same time, there is an increasing uptake of DEMO in practice. This also results in a need for enterprise-grade tool support for the use of the method. In this paper, we report on a study concerning the selection, configuration, and extension, of an enterprise-grade tool platform to support the use of DEMO in practice. The configuration of the selected tool framework to support DEMO modelling, provided general insights regarding the development of enterprise-grade tool support for (model-driven) methods such as DEMO, while also providing feedback on the consistency and completeness of the DEMO specification language; the specification language that accompanies the DEMO method.",
        "keywords": [
            "Enterprise engineering",
            "DEMO",
            "Modelling tools"
        ],
        "authors": [
            "Mark A. T. Mulder",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-021-00911-z.pdf"
    },
    {
        "title": "OMEGA: correct development of real time and embedded systems",
        "submission-date": "2007/11",
        "publication-date": "2008/01",
        "abstract": "Building embedded real time software systems of guaranteed quality, in a cost-effective manner, is an important technological challenge. Component-based design leads to a need for a new architecture-based software engineering practice in which complex systems are built by composing avai-lable components with known properties and evaluating the impact of local design choices on their global behaviour. This requires new tool support for their development. The aim of the Omega project1 was to contribute to this new development paradigm by providing a framework for model-based development of real-time and embedded sys-tems. It should enable a consistent use of models by advanced analysis tools on one hand and commercial code generators on the other. The project has obtained a number of interesting results concerning the following three main work directions.",
        "keywords": [
            "Support for object oriented",
            "Model-based",
            "Component-based system design",
            "UML",
            "Type-checking",
            "Model checking",
            "Theorem proving",
            "Real time",
            "Embedded systems"
        ],
        "authors": [
            "Susanne Graf"
        ],
        "file_path": "data/sosym-all/s10270-007-0077-5.pdf"
    },
    {
        "title": "Erratum to: Introduction to the theme issue on variability modeling of software-intensive systems",
        "submission-date": "2015/11",
        "publication-date": "2015/11",
        "abstract": "Unfortunately there is a mistake in the author information for references [1, 5] in the text. The correct information is (as in the references itself): “Bonifácio, Borba, Ferraz, and Accioly (‘Empirical assessment of two approaches for specifying software prod-uct lines use case scenarios’ [5]) report the results of an empirical investigation of two different speciﬁcation approaches for software product line scenarios, . . .” and “In ‘The shape of feature code: an investigation of twenty c-preprocesor-based system’ [1] Queiroz, Passos, Valente, Hunsen, Apel, and Czarnecki investigate twenty projects using C preprocessor directives for variability management.”",
        "keywords": [],
        "authors": [
            "Andrzej Wa˛sowski",
            "Thorsten Weyer"
        ],
        "file_path": "data/sosym-all/s10270-015-0507-8.pdf"
    },
    {
        "title": "Reﬂection on the differences between modeling and programming",
        "submission-date": "2022/11",
        "publication-date": "2022/11",
        "abstract": "For a 2012 editorial [1], the relationships between modeling and programming languages were discussed. A decade later, it seems appropriate to revisit this issue because there is still not a widely accepted consensus on the main purpose of modeling languages and their models in software development. In this editorial, we highlight the most important aspects when identifying commonalities and differences between modeling and programming languages. We are fully aware that a detailed examination of all these relationships, properties, and their interdependencies is far beyond this short reﬂection.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-01057-2.pdf"
    },
    {
        "title": "Context-aware modeling for knowledge-intensive medicinal product development processes",
        "submission-date": "2022/02",
        "publication-date": "2022/12",
        "abstract": "Due to their unique characteristics, knowledge-intensive processes (KiPs) are difﬁcult to capture with conventional modeling and management approaches. One such KiP is the advanced therapy medicinal product (ATMP) development process. ATMPs are highly innovative medicinal products that are based on biomedical technology. ATMP development processes need to comply with complex regulatory frameworks. Currently, biomedical scientists that develop ATMPs manage the regulatory aspects of the ATMP development processes in an ad hoc fashion, resulting in inefﬁciencies such as reworks or even withdrawal of ATMPs from the market. This paper presents an explorative case study in which we use Enterprise Modeling and Context-aware Business Processes to support ATMP scientists in managing the regulatory aspects of ATMP development processes more efﬁciently and effectively. In our explorative case study, we use enterprise models to describe the important concepts and views in ATMP development processes. By introducing context-awareness to the models, we support ATMP scientists in performing relevant tasks to address the regulatory requirements efﬁciently and effectively under different contexts. We introduce the novel concept of execution-dependent dynamic context to properly deﬁne the context in ATMP development processes. Additionally, this paper takes a broader perspective on the case study by discussing the relevance of the solutions derived for the case study for other KiPs. Thereby this paper aims to present an exemplary approach for context-aware modeling of KiPs. The practical contribution of this paper are the models realized in a real-life ATMP development project. The scientiﬁc contribution of this paper is providing an exemplary approach for supporting knowledge workers who perform ﬂexible, KiPs under dynamic contexts and introducing the notion of execution-dependent dynamic context.",
        "keywords": [
            "Context-awareness",
            "Enterprise modeling",
            "Business process management",
            "Conceptual modeling",
            "Knowledge-intensive process"
        ],
        "authors": [
            "Zeynep Ozturk Yurt",
            "Rik Eshuis",
            "Anna Wilbik",
            "Irene Vanderfeesten"
        ],
        "file_path": "data/sosym-all/s10270-022-01070-5.pdf"
    },
    {
        "title": "Scalable model views over heterogeneous modeling technologies and resources",
        "submission-date": "2019/07",
        "publication-date": "2020/04",
        "abstract": "When engineering complex systems, models are typically used to represent various systems aspects. These models are often heterogeneous in terms of modeling languages, provenance, number or scale. As a result, the information actually relevant to engineers is usually split into different kinds of interrelated models. To be useful in practice, these models need to be properly integrated to provide global views over the system. This has to be made possible even when those models are serialized or stored in different formats adapted to their respective nature and scalability needs. Model view approaches have been proposed to tackle this issue. They provide uniﬁcation mechanisms to combine and query various different models in a transparent way. These views usually target speciﬁc engineering tasks such as system design, monitoring and evolution. In an industrial context, there can be many large-scale use cases where model views can be beneﬁcial, in order to trace runtime and design-time data, for example. However, existing model view solutions are generally designed to work on top of one single modeling technology (even though model import/export capabilities are sometimes provided). Moreover, they mostly rely on in-memory constructs and low-level modeling APIs that have not been designed to scale in the context of large models stored in different kinds of data sources. This paper presents a general solution to efﬁciently support scalable model views over heterogeneous modeling resources possibly handled via different modeling technologies. To this intent, it describes our integration approach between a model view framework and various modeling technologies providing access to multiple types of modeling resources (e.g., in XML/XMI, CSV, databases). It also presents how queries on such model views can be executed efﬁciently by beneﬁting from the optimization of the different model technologies and underlying persistence backends. Our solution has been evaluated on a practical large-scale use case provided by the industry-driven European MegaM@Rt2 project that aims at implementing a runtime ↔design time feedback loop. The corresponding EMF-based tooling support, modeling artifacts and reproducible benchmarks are all available online.",
        "keywords": [
            "Modeling",
            "Views",
            "Heterogeneity",
            "Scalability",
            "Persistence",
            "Database",
            "Design time",
            "Runtime"
        ],
        "authors": [
            "Hugo Bruneliere",
            "Florent Marchand de Kerchove",
            "Gwendal Daniel",
            "Sina Madani",
            "Dimitris Kolovos",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-020-00794-6.pdf"
    },
    {
        "title": "Incremental execution of temporal graph queries over runtime models with history and its applications",
        "submission-date": "2021/03",
        "publication-date": "2021/12",
        "abstract": "Modern software systems are intricate and operate in highly dynamic environments for which few assumptions can be made at design-time. This setting has sparked an interest in solutions that use a runtime model which reﬂects the system state and operational context to monitor and adapt the system in reaction to changes during its runtime. Few solutions focus on the evolution of the model over time, i.e., its history, although history is required for monitoring temporal behaviors and may enable more informed decision-making. One reason is that handling the history of a runtime model poses an important technical challenge, as it requires tracing a part of the model over multiple model snapshots in a timely manner. Additionally, the runtime setting calls for memory-efﬁcient measures to store and check these snapshots. Following the common practice of representing a runtime model as a typed attributed graph, we introduce a language which supports the formulation of temporal graph queries, i.e., queries on the ordering and timing in which structural changes in the history of a runtime model occurred. We present a querying scheme for the execution of temporal graph queries over history-aware runtime models. Features such as temporal logic operators in queries, the incremental execution, the option to discard history that is no longer relevant to queries, and the in-memory storage of the model, distinguish our scheme from relevant solutions. By incorporating temporal operators, temporal graph queries can be used for runtime monitoring of temporal logic formulas. Building on this capability, we present an implementation of the scheme that is evaluated for runtime querying, monitoring, and adaptation scenarios from two application domains.",
        "keywords": [
            "Runtime models",
            "History-awareness",
            "Historic data",
            "Temporal graph queries",
            "Incremental pattern matching",
            "Runtime monitoring",
            "Self-adaptive systems"
        ],
        "authors": [
            "Lucas Sakizloglou",
            "Sona Ghahremani",
            "Matthias Barkowsky",
            "Holger Giese"
        ],
        "file_path": "data/sosym-all/s10270-021-00950-6.pdf"
    },
    {
        "title": "Empirical evaluation of CMMN models: a collaborative process case study",
        "submission-date": "2018/11",
        "publication-date": "2020/06",
        "abstract": "Case Management Model and Notation (CMMN) was introduced by the Object Management Group as an alternative modeling language, targeting human-centric processes characterized by lack of structure and agility. However, although it is adequately supported by well-known process management tools, CMMN applicability as a modeling language is being questioned in practice. In this work, an empirical evaluation of CMMN models is presented, through a real-world case study where CMMN has been used for the analysis and implementation of a collaborative process by independent groups of process engineers. Their experience is being discussed, based on their modeling perspective. The produced models in the analysis and implementation phase are evaluated, using pre-existing metrics customized for CMMN. Based on the experience of engineers, CMMN applicability is evaluated, highlighting aspects in which its application might be limited, that should be addressed.",
        "keywords": [
            "Process modeling",
            "Collaborative processes",
            "Modeling perspectives",
            "Case Management Model and Notation",
            "Evaluation metrics",
            "Empirical evaluation"
        ],
        "authors": [
            "Ioannis Routis",
            "Mara Nikolaidou",
            "Dimosthenis Anagnostopoulos"
        ],
        "file_path": "data/sosym-all/s10270-020-00802-9.pdf"
    },
    {
        "title": "End-to-end model-transformation comprehension through ﬁne-grained traceability information",
        "submission-date": "2016/10",
        "publication-date": "2017/06",
        "abstract": "Abstract The construction and maintenance of model-to-\nmodel and model-to-text transformations pose numerous\nchallenges to novice and expert developers. A key chal-\nlenge involves tracing dependency relationships between\nartifacts of a transformation ecosystem. This is required\nto assess the impact of metamodel evolution, to determine\nmetamodel coverage, and to debug complex transformation\nexpressions. This paper presents an empirical study that\ninvestigates the performance of developers reﬂecting on the\nexecution semantics of model-to-model and model-to-text\ntransformations. We measured the accuracy and efﬁciency\nof 25 developers completing a variety of traceability-driven\ntasks in two model-based code generators. We compared the\nperformance of developers using ChainTracker, a traceabil-\nity analysis environment developed by our team, and that of\ndevelopers using Eclipse Modeling. We present statistically\nsigniﬁcant evidence that ChainTracker improves the perfor-\nmance of developers reﬂecting on the execution semantics of\ntransformation ecosystems. We discuss how developers sup-\nported by off-the-shelf development environments are unable\nto effectively identify dependency relationships in nontrivial\nmodel-transformation chains.",
        "keywords": [
            "Model-transformation comprehension",
            "Transformation comprehension",
            "Traceability analysis",
            "Software maintenance",
            "Development environments"
        ],
        "authors": [
            "Victor Guana",
            "Eleni Stroulia"
        ],
        "file_path": "data/sosym-all/s10270-017-0602-0.pdf"
    },
    {
        "title": "RBPMN: the value of roles for business process modeling",
        "submission-date": "2023/01",
        "publication-date": "2024/08",
        "abstract": "Business process modeling is essential for organizations to comprehend, analyze, and enhance their business operations. The\nbusiness process model and notation (BPMN) is a standard widely adopted for illustrating business processes. However, it falls\nshort when modeling roles, interactions, and responsibilities within complex modern processes that involve digital, human,\nand non-human entities, typically found in cyber-physical systems (CPS). In this paper, we introduce Role-based BPMN\n(RBPMN), a standard-compliant extension of BPMN 2.0 that distinctly depicts roles and their interactions within business\nprocesses. We underscore the value of RBPMN and a role-based context modeling approach through a modeling example in\nCPS that facilitates the representation of role-based variations in the process ﬂow, namely a production process in a smart\nfactory. Our ﬁndings suggest that RBPMN is a valuable BPMN extension that enhances the expressiveness, variability, and\ncomprehensiveness of business process models, especially in complex and context-sensitive processes.",
        "keywords": [
            "Business process modeling",
            "Role modeling",
            "Cyber-physical systems"
        ],
        "authors": [
            "Tarek Skouti\nRonny Seiger\nFrank J. Furrer\nSusanne Strahringer"
        ],
        "file_path": "data/sosym-all/s10270-024-01202-z.pdf"
    },
    {
        "title": "Reflections on the standardization of SysML 2",
        "submission-date": "2021/03",
        "publication-date": "2021/03",
        "abstract": "In 2018, we wrote an editorial entitled “Agile Model-Based System Development” [GR18c], in which we made several observations: 1. “In fact, modeling is at the heart of almost any engineering discipline. Thus, it is not surprising that our engineering colleagues have developed a detailed portfolio of modeling techniques to describe their systems in various perspectives, viewpoints, and abstractions.” 2. “However, the state-of-the-art in systems modeling has several challenges, where each modeling aspect and view is often assisted by an individual modeling and analysis tool. Data exchange between the tools is complicated, even though mostly automated, but suffers from concerns about robustness, completeness of the mappings between the models, as well as regular version upgrades of tools. A second problem is that these mappings between models are not easy to standardize, because different projects use the same modeling languages in different forms (semantics), which enforces configurable mappings or individually developed translations per project.” 3. “Traditional engineers use system models to design and understand the product through analysis and automation of engineering tasks. Therefore, models should be more than just documentation artifacts. Each model that captures a perspective of the project should either become a part of the product construction or should be used for automated validation and test quality management.” We also discussed that traditional systems development has not yet reached the maturity of agile software development for a number of reasons, including the far too long, complex, and dependent tool chain. Currently, the development of the updated standard SysML 2.0 is a larger effort, where the stakeholders are trying to accommodate a number of these problems:",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-021-00881-2.pdf"
    },
    {
        "title": "Improving timing analysis effectiveness for scenario-based specifications by combining SAT and LP techniques",
        "submission-date": "2021/04",
        "publication-date": "2022/02",
        "abstract": "Open environmental software systems are often time-sensitive, as they need to respond to other entities within the systems and/or in the environments promptly. The timing requirements are therefore an essential part of the system correctness. Scenario-based specifications (SBS) such as message sequence charts and UML interaction models play an important role in specifying open environmental software systems since they intuitively model interactions between different entities. While modelling these systems, the timing requirements can be specified as timing constraints. In this paper, we study the problem of checking the timing consistency of SBS with timing constraints. Although this problem can be transformed into a reachability analysis problem, checking its reachability can still be time-consuming. Therefore, we propose a novel SAT and linear programming (LP) collaborative timing analysis approach named Tassat for the bounded timing analysis of SBS. Instead of using depth-ﬁrst traversal algorithms, Tassat encodes the structures of the SBS into propositional formulas and adopts SAT solvers to find candidate paths. The timing analysis of candidate paths is then transformed to LP problems, where the irreducible infeasible set of the infeasible paths can be utilized to filter out candidate paths for checking. In addition, we propose an enhanced version of the approach that extends the bounded analysis results to the entire models if the infeasible path segments do not contain intermediate loops. The enhanced algorithm can prove that the given SBS satisfy the required properties on any bound condition. The experimental results show that Tassat is effective and has better performance than existing tools in terms of both time consumption and memory footprint.",
        "keywords": [
            "Scenario-based specifications",
            "Message sequence charts",
            "Model checking",
            "Reachability analysis"
        ],
        "authors": [
            "Longlong Lu",
            "Minxue Pan",
            "Tian Zhang",
            "Xuandong Li"
        ],
        "file_path": "data/sosym-all/s10270-022-00980-8.pdf"
    },
    {
        "title": "Model-driven system-level validation and veriﬁcation on the space software domain",
        "submission-date": "2020/11",
        "publication-date": "2021/11",
        "abstract": "The development process of on-board software applications can beneﬁt from model-driven engineering techniques. Model validation and model transformations can be applied to drive the activities of speciﬁcation, requirements deﬁnition, and system-level validation and veriﬁcation according to the space software engineering standards ECSS-E-ST-40 and ECSS-Q-ST-80. This paper presents a model-driven approach to completing these activities by avoiding inconsistencies between the documents that support them and providing the ability to automatically generate the system-level validation tests that are run on the Ground Support Equipment and the matrices required to complete the software veriﬁcation. A demonstrator of the approach has been built using as a proof of concept a subset of the functionality of the software of the control unit of the Energetic Particle Detector instrument on-board Solar Orbiter.",
        "keywords": [
            "MDE",
            "Validation",
            "Veriﬁcation",
            "Space",
            "Software",
            "ECSS"
        ],
        "authors": [
            "Aarón Montalvo",
            "Pablo Parra",
            "Óscar Rodríguez Polo",
            "Alberto Carrasco",
            "Antonio Da Silva",
            "Agustín Martínez",
            "Sebastián Sánchez"
        ],
        "file_path": "data/sosym-all/s10270-021-00940-8.pdf"
    },
    {
        "title": "SoSyM reflections: the 2019 “state of the journal” report",
        "submission-date": "2020/01",
        "publication-date": "2020/01",
        "abstract": "We are delighted to bring you this first issue of SoSyM for 2020! Over the past year, SoSyM has achieved many new milestones. As reported in the next section, the SoSyM Impact Factor has increased by almost a full point, to its highest historical level and the number of downloads continues to rise. This suggests that the journal remains very healthy and that the software systems and modeling research community continues to thrive!",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-020-00778-6.pdf"
    },
    {
        "title": "The Train Benchmark: cross-technology performance evaluation of continuous model queries",
        "submission-date": "2015/10",
        "publication-date": "2017/01",
        "abstract": "In model-driven development of safety-critical systems (like automotive, avionics or railways), well-formedness of models is repeatedly validated in order to detect design ﬂaws as early as possible. In many industrial tools, validation rules are still often implemented by a large amount of imperative model traversal code which makes those rule implementations complicated and hard to maintain. Additionally, as models are rapidly increasing in size and complexity, efﬁcient execution of validation rules is challenging for the currently available tools. Checking well-formedness constraints can be captured by declarative queries over graph models, while model update operations can be speciﬁed as model transformations. This paper presents a benchmark for systematically assessing the scalability of validating and revalidating well-formedness constraints over large graph models. The benchmark deﬁnes well-formedness validation scenarios in the railway domain: a metamodel, an instance model generator and a set of well-formedness constraints captured by queries, fault injection and repair operations (imitating the work of systems engineers by model transformations). The benchmark focuses on the performance of query evaluation, i.e. its execution time and memory consumption, with a particular emphasis on reevaluation. We demonstrate that the benchmark can be adopted to various technologies and query engines, including modeling tools; relational, graph and semantic databases. The Train Benchmark is available as an open-source project with continuous builds from https://github.com/FTSRG/trainbenchmark.",
        "keywords": [
            "Well-formedness validation",
            "Query evaluation",
            "Performance benchmark",
            "Graph databases",
            "Semantic databases",
            "Relational databases"
        ],
        "authors": [
            "Gábor Szárnyas",
            "Benedek Izsó",
            "István Ráth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-016-0571-8.pdf"
    },
    {
        "title": "Adaptive caching for operation-based versioning of models",
        "submission-date": "2024/01",
        "publication-date": "2025/01",
        "abstract": "In a collaborative multi-user model-driven engineering context, it becomes important to track who changed what model part how and why. Operation-based versioning addresses this need by persisting a meaningful edit history which enables a single user to navigate through a model’s evolution over time, to analyze arbitrary previous model versions, or to trace the impact of an operation. However, to load a distinct prior version, it must be restored by reapplying all previous operations, which is time-consuming and, thus, interrupts a user’s workﬂow. Caching with a ﬁxed distance between caches helps to overcome this problem to the cost of increasing memory requirements. Further, there is no caching approach supporting branches, merges, and possibly resolved conﬂicts. We propose two advanced caching strategies for operation-based versioning capable of the previously mentioned features: zonal and adaptive caching. Both strategies reduce the memory in use by not applying the same static distance between two caches across the whole edit history. Instead, the distance increases depending on a version’s age and its distance to a branch’s head. Both strategies aim to reduce the restoration time of arbitrary prior versions below a threshold to not interrupt a user’s ﬂow of thought. Zonal caching employs predeﬁned distances compatible with a broad range of model sizes. In contrast, adaptive caching derives the distances individually depending on the initial time to load the model on a user’s computer and the model’s size.We conducted controlled experiments with models of varying sizes and compared the time to restore model versions and the memory in use for no caching, caching with static distances, zonal, and adaptive strategies on different computers. The developed strategies decrease the time to restore a version remarkably while using less memory than static caching. Our results show that for all considered systems and models individual adaptive caching reduces memory usage even further compared to zone-based caching while still satisfying application responsiveness requirements.",
        "keywords": [
            "Operation-based versioning",
            "Modeling",
            "Caching",
            "Event-sourcing"
        ],
        "authors": [
            "Jakob Pietron\nHeiko Raab\nMatthias Tichy"
        ],
        "file_path": "data/sosym-all/s10270-024-01214-9.pdf"
    },
    {
        "title": "Guest editorial to the special issue on MODELS 2009",
        "submission-date": "2011/04",
        "publication-date": "2011/04",
        "abstract": "The pioneering organizers of the ﬁrst “UML” ‘98 workshop in Mulhouse, France in the summer of 1998 could hardly have anticipated that, in little over a decade, their initiative would blossom into today’s highly successful MODELS conference series. MODELS is unquestionably the premier annual gathering of researchers and practitioners working on what is starting to emerge as a bona ﬁde new technical discipline: model-based engineering of systems and software—a discipline distinguished by the fact that it fundamentally relies on the development and exploitation of models and supporting technologies.",
        "keywords": [],
        "authors": [
            "Andy Schürr",
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-011-0200-5.pdf"
    },
    {
        "title": "Modeling of, for, and with digital twins",
        "submission-date": "2022/09",
        "publication-date": "2022/09",
        "abstract": "There exists a plethora of definitions for Digital Twins (DTs). Although there is a rough convergence toward a common definition, there is no consensus about what a digital twin actually comprises. We have found differences in the definition given in publications, such as issues of narrowness, by focusing on specific use cases, domains, or technologies in the definition.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-01046-5.pdf"
    },
    {
        "title": "A technique for evaluating and improving the semantic transparency of modeling language notations",
        "submission-date": "2020/09",
        "publication-date": "2021/06",
        "abstract": "The notation of a modeling language is of paramount importance for its efﬁcient use and the correct comprehension of created models. A graphical notation, especially for domain-speciﬁc modeling languages, should therefore be aligned to the knowledge, beliefs, and expectations of the targeted model users. One quality attributed to notations is their semantic transparency, indicating the extent to which a notation intuitively suggests its meaning to untrained users. Method engineers should thus aim at semantic transparency for realizing intuitively understandable notations. However, notation design is often treated poorly—if at all—in method engineering methodologies. This paper proposes a technique that, based on iterative evaluation and improvement tasks, steers the notation toward semantic transparency. The approach can be efﬁciently applied to arbitrary modeling languages and allows easy integration into existing modeling language engineering methodologies. We show the feasibility of the technique by reporting on two cycles of Action Design Research including the evaluation and improvement of the semantic transparency of the Process-Goal Alignment modeling language notation. An empirical evaluation comparing the new notation against the initial one shows the effectiveness of the technique.",
        "keywords": [
            "Modeling language",
            "Notation",
            "Concrete syntax",
            "Semantic transparency",
            "Empirical evaluation"
        ],
        "authors": [
            "Dominik Bork",
            "Ben Roelens"
        ],
        "file_path": "data/sosym-all/s10270-021-00895-w.pdf"
    },
    {
        "title": "Formal veriﬁcation of software source code through semi-automatic modeling",
        "submission-date": "2002/11",
        "publication-date": "2004/01",
        "abstract": "We describe the experience of modeling and formally verifying a software cache algorithm using the model checker RuleBase. Contrary to prevailing wisdom, we used a highly detailed model created directly from the C code itself, rather than a high-level abstract model.",
        "keywords": [
            "Software model checking",
            "Software veriﬁcation",
            "Program veriﬁcation",
            "Functional veriﬁcation"
        ],
        "authors": [
            "Cindy Eisner"
        ],
        "file_path": "data/sosym-all/s10270-003-0042-x.pdf"
    },
    {
        "title": "A language-parametric test ampliﬁcation framework for executable domain-speciﬁc languages",
        "submission-date": "2023/05",
        "publication-date": "2025/05",
        "abstract": "Behavioral models are important assets that must be thoroughly veriﬁed early in the design process. This can be achieved with manually-written test cases that embed carefully hand-picked domain-speciﬁc input data. However, such test cases may not always reach the desired level of quality, such as high coverage or being able to localize faults efﬁciently. Test ampliﬁcation is an interesting emergent approach to improve a test suite by automatically generating new test cases out of existing manually-written ones. Yet, while ad-hoc test ampliﬁcation solutions have been proposed for a few programming languages, no solution currently exists for amplifying the test suites of behavioral models. In order to ﬁll this gap, we propose an automated and generic test ampliﬁcation approach for executable domain-speciﬁc languages (DSLs). Hence, given an executable DSL, a conforming behavioral model, and an existing test suite, our approach synthesizes new regression test cases in three steps: (i) generating new test inputs by applying a set of generic modiﬁers on the existing test inputs; (ii) running the model under test with new inputs and generating assertions from the execution traces; and (iii) selecting the new test cases that increase the initial test quality. We provide a textual DSL to control and conﬁgure the ampliﬁcation process, along with tool support for the whole approach atop the Eclipse GEMOC Studio. For assessment, we report on empirical evaluations over two different executable DSLs, which show improved test quality in terms of both coverage and mutation score.",
        "keywords": [
            "Test ampliﬁcation",
            "Regression testing",
            "Mutation testing",
            "Executable model",
            "Executable DSL"
        ],
        "authors": [
            "Faezeh Khorram",
            "Erwan Bousse",
            "Jean-Marie Mottu",
            "Gerson Sunyé",
            "Djamel Eddine Khelladi",
            "Pablo Gómez-Abajo",
            "Pablo C. Cañizares",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-025-01283-4.pdf"
    },
    {
        "title": "Teaching model-driven engineering from a relational database perspective",
        "submission-date": "2014/09",
        "publication-date": "2015/08",
        "abstract": "We reinterpret MDE from the viewpoint of relational databases to provide an alternative way to understand, demonstrate, and teach MDE using concepts and technologies that should be familiar to undergraduates. We use (1) relational database schemas to express metamodels, (2) relational databases to express models, (3) Prolog to express constraints and M2M transformations, (4) Java tools to implement M2T and T2M transformations, and (5) Java to execute transformations. Application case studies and a user study illuminate the viability and beneﬁts of our approach.",
        "keywords": [
            "MDE",
            "Teaching",
            "Tools"
        ],
        "authors": [
            "Don Batory",
            "Maider Azanza"
        ],
        "file_path": "data/sosym-all/s10270-015-0488-7.pdf"
    },
    {
        "title": "Compositional model analysis",
        "submission-date": "2020/03",
        "publication-date": "2020/03",
        "abstract": "Recently, one of the editors attended a Dagstuhl seminar about “Composing Model-Based Analysis Tools” organized by Francisco Durán, Robert Heinrich, Diego Pérez-Palacín, Carolyn L. Talcott, and Steffen Zschaler [1]. Never heard about Dagstuhl? Schloss Dagstuhl is a wonderful venue that offers invitation-only Computer Science seminars every week of the year (except Christmas). It regularly manages to bring together experienced scientists and young aspiring researchers with interest in a specific research theme to foster the discussion of exciting topics. Dagstuhl is located in a rural area of Germany, which helps to remove distractions from other concerns. Attendees are compelled to spend time together while focused on scientific and social discussions that are detailed, interesting, and fruitful. The local organizers and staff do everything to help participants focus on collaborative scientific activities. The food is exceptional, and the organization is always professional and attentive. An invitation to Dagstuhl is an honor that should be deeply considered when offered!\nSoSyM regularly benefits from Dagstuhl seminars through papers that originate from these discussions. The “Composing Model-Based Analysis Tools” seminar had approximately 50 attendees who explored various topics related to model analysis; in particular, how model analysis techniques can be modularized in such a way that they can be composed and built upon each other. There are various examples where simple analysis techniques (e.g., typing of expressions, completeness and reachability algorithms) are the foundation of analysis, with other desired characteristics (e.g., reliability, efficiency, and security metrics) built from the foundational concepts.\nThe participants at this seminar represented diverse backgrounds. They introduced many different ways models are used to describe views on systems and their (potentially hierarchically decomposed) components. One model typically describes a particular view on a part of the system (e.g., subsystem, component). Thus, many models are often used to describe a larger system. The multiple-models systems view can be contrasted with the Big-Blob-Gozilla-Model that covers everything, with limited potential for abstraction and thus reducing the benefits of modeling. A single large model also prevents reuse of any independently developed model and has some other drawbacks (e.g., less potential for collaborative design). The idea of compositional model analysis can benefit from much of the earlier research that intersected programming languages and software engineering to promote a notion of composition together with the idea of encapsulation (e.g., the works of Parnas, such as “On the criteria to be used in decomposing systems into modules” and “Modular structure of complex systems”).\nHowever, as models are being composed, it is an interesting question to ask whether the model composition structure and system composition structure are orthogonal, or exactly the same. Several examples have been discussed that support both variants (e.g., several models can be used to describe different aspects of the same system component, such as interface, internal data structure, behavioral constraints, or implementation; but, e.g., typically one architecture diagram describes many system components). The situation becomes more wicked as various viewpoints of the system are modeled using different modeling techniques, which enforces that not only models, but also the modeling languages need to be composed (e.g., usually quality analyses depend on very specific quality attributes evenly spread over many models, but defined within one sub-language; on the contrary, interaction analysis can be spread over many forms of interactions, imports, includes, and other forms of uses spear across many sub-languages).\nModeling is not a simple and easy task. Deep skills and expertise are needed to create models that are useful, well-structured, understandable, and correct. Models offer many benefits, from the sketching of new ideas to the full formal specification of a dynamic system. Many industry applica-",
        "keywords": [],
        "authors": [
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-020-00784-8.pdf"
    },
    {
        "title": "Guest editorial to the special section on MODELS 2018",
        "submission-date": "2020/04",
        "publication-date": "2020/05",
        "abstract": "The MODELS conference series is the premier event for model-based software and systems engineering. This special section presents the five articles that resulted from an invitation to authors of the best papers at MODELS 2018 to submit revised and extended versions of their papers.",
        "keywords": [],
        "authors": [
            "Andrzej Wa˛sowski",
            "Richard F. Paige",
            "Øystein Haugen"
        ],
        "file_path": "data/sosym-all/s10270-020-00800-x.pdf"
    },
    {
        "title": "Process mining using BPMN: relating event logs and process models",
        "submission-date": "2015/03",
        "publication-date": "2015/10",
        "abstract": "Process-aware information systems (PAIS) are systems relying on processes, which involve human and software resources to achieve concrete goals. There is a need to developapproachesformodeling,analysis,improvementand monitoring processes within PAIS. These approaches include process mining techniques used to discover process models from event logs, ﬁnd log and model deviations, and analyze performance characteristics of processes. The representational bias (a way to model processes) plays an important role in process mining. The BPMN 2.0 (Business Process Model and Notation) standard is widely used and allows to build conventional and understandable process models. In addition to the ﬂat control ﬂow perspective, subprocesses, data ﬂows, resources can be integrated within one BPMN diagram. This makes BPMN very attractive for both process miners and business users, since the control ﬂow perspec-tive can be integrated with data and resource perspectives discovered from event logs. In this paper, we describe and justify robust control ﬂow conversion algorithms, which provide the basis for more advanced BPMN-based discovery and conformance checking algorithms. Thus, on the basis of these conversion algorithms low-level models (such as Petri nets, causal nets and process trees) discovered from event logs using existing approaches can be represented in terms of BPMN. Moreover, we establish behavioral relations between Petri nets and BPMN models and use them to adopt existing conformance checking and performance analysis techniques in order to visualize conformance and performance infor-mation within a BPMN diagram. We believe that the results presented in this paper can be used for a wide variety of BPMN mining and conformance checking algorithms. We also provide metrics for the processes discovered before and after the conversion to BPMN structures. Cases for which conversion algorithms produce more compact or more complicated BPMN models in comparison with the initial models are identiﬁed.",
        "keywords": [
            "Process mining",
            "Process discovery",
            "Conformance checking",
            "BPMN (Business Process Model and Notation)",
            "Petri nets",
            "Bisimulation"
        ],
        "authors": [
            "Anna A. Kalenkova",
            "Wil M. P. van der Aalst",
            "Irina A. Lomazova",
            "Vladimir A. Rubin"
        ],
        "file_path": "data/sosym-all/s10270-015-0502-0.pdf"
    },
    {
        "title": "Modeling more software performance antipatterns in cyber-physical systems",
        "submission-date": "2022/11",
        "publication-date": "2023/12",
        "abstract": "The design of cyber-physical systems (CPS) is challenging due to the heterogeneity of software and hardware components that operate in uncertain environments (e.g., ﬂuctuating workloads), hence they are prone to performance issues. Software performance antipatterns could be a key means to tackle this challenge since they recognize design problems that may lead to unacceptable system performance. This manuscript focuses on modeling and analyzing a variegate set of software performance antipatterns with the goal of quantifying their performance impact on CPS. Starting from the speciﬁcation of eight software performance antipatterns, we build a baseline queuing network performance model that is properly extended to account for the corresponding bad practices. The approach is applied to a CPS consisting of a network of sensors and experimental results show that performance degradation can be traced back to software performance antipatterns. Sensitivity analysis investigates the peculiar characteristics of antipatterns, such as the frequency of checking the status of resources, that provides quantitative information to software designers to help them identify potential performance problems and their root causes. Quantifying the performance impact of antipatterns on CPS paves the way for future work enabling the automated refactoring of systems to remove these bad practices.",
        "keywords": [
            "Software modeling",
            "Software performance antipatterns",
            "Model-based performance analysis",
            "Cyber-physical systems"
        ],
        "authors": [
            "Riccardo Pinciroli",
            "Connie U. Smith",
            "Catia Trubiani"
        ],
        "file_path": "data/sosym-all/s10270-023-01137-x.pdf"
    },
    {
        "title": "Concern coverage in base station development: an empirical investigation",
        "submission-date": "2010/02",
        "publication-date": "2011/01",
        "abstract": "Contemporary model driven development tools only partially support the abstractions occurring in complex embedded systems development. This article presents an interpretive case study in which architectural concerns important to seven engineers in a large product developing organization were compared to the views actually provided by the organization’s models. The paper’s main ﬁnding is an empirically grounded catalogue of architectural concerns for a large, complex embedded systems project, and an assessment of the degree to which the studied organization has managed to realize support for these concerns within economical and organizational constraints. In the studied case, 114 different architectural concerns were found to be important to the interviewed engineers. Of this sample, 75% were documented in models, structured text, or informal documentation, whereas 47% of all documented concerns were modeled. The paper’s conclusion is that current modeling languages and methods inadequately address the full set of concerns that are important to engineers in base station development.",
        "keywords": [
            "Model driven development (MDD)",
            "Software architecture",
            "Embedded systems",
            "Telecommunication systems"
        ],
        "authors": [
            "Lars Pareto",
            "Peter Eriksson",
            "Staffan Ehnebom"
        ],
        "file_path": "data/sosym-all/s10270-010-0188-2.pdf"
    },
    {
        "title": "Supporting ﬁne-grained generative model-driven evolution",
        "submission-date": "2007/08",
        "publication-date": "2010/01",
        "abstract": "In the standard generative Model-driven Architecture (MDA), adapting the models of an existing system requires re-generation and restarting of that system. This is due to a strong separation between the modeling environment and the runtime environment. Certain current approaches remove this separation, allowing a system to be changed smoothly when the model changes. These approaches are, however, based on interpretation of modeling information rather than on generation, as in MDA. This paper describes an architecture that supports ﬁne-grained evolution combined with generative model-driven development. Fine-grained changes are applied in a generative model-driven way to a system that has itself been developed in this way. To achieve this, model changes must be propagated correctly toward impacted elements. The impact of a model change ﬂows along three dimensions: implementation, data (instances), and modeled dependencies. These three dimensions are explicitly represented in an integrated modeling-runtime environment to enable traceability. This implies a fundamental rethinking of MDA.",
        "keywords": [
            "Evolution",
            "Model-driven development",
            "Generative development",
            "Interpretive development"
        ],
        "authors": [
            "Theo Dirk Meijler",
            "Jan Pettersen Nytun",
            "Andreas Prinz",
            "Hans Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-009-0144-1.pdf"
    },
    {
        "title": "An asynchronous communication model for distributed concurrent objects",
        "submission-date": "2005/02",
        "publication-date": "2006/08",
        "abstract": "Distributed systems are often modeled by objects that run concurrently, each with its own processor, and communicate by synchronous remote method calls. This may be satisfactory for tightly coupled systems, but in the distributed setting synchronous external calls lead to much waiting; at best resulting in inefficient use of processor capacity, at worst resulting in deadlock. Furthermore, it is difficult to combine active and passive behavior in concurrent objects. This paper proposes an object-oriented solution to these problems by means of asynchronous method calls and conditional processor release points. Although at the cost of additional internal nondeterminism in the objects, this approach seems attractive in asynchronous or unreliable environments. The concepts are integrated in a small object-oriented language with an operational semantics defined in rewriting logic, and illustrated by examples.",
        "keywords": [
            "Asynchronous method calls",
            "Concurrent objects",
            "Distributed systems",
            "Rewriting logic"
        ],
        "authors": [
            "Einar Broch Johnsen",
            "Olaf Owe"
        ],
        "file_path": "data/sosym-all/s10270-006-0011-2.pdf"
    },
    {
        "title": "Conceptualization, measurement, and application of semantic transparency in visual notations\nA systematic literature review",
        "submission-date": "2020/09",
        "publication-date": "2021/05",
        "abstract": "Numerous visual notations are present in technical and business domains. Notations have to be cognitively effective to ease the planning, documentation, and communication of the domains’ concepts. Semantic transparency (ST) is one of the elementary principles that inﬂuence notations’ cognitive effectiveness. However, the principle is criticized for not being well deﬁned and challenges arise in the evaluations and applications of ST. Accordingly, this research’s objectives were to answer how the ST principle is deﬁned, operationalized, and evaluated in present notations as well as applied in the design of new notations in ICT and related areas. To meet these objectives, a systematic literature review was conducted with 94 studies passing the selection process criteria. The results reject one of the three aspects, which deﬁne semantic transparency, namely “ST is achieved with the use of icons.” Besides, taxonomies of related concepts and research methods, evaluation metrics, and other ﬁndings from this study can help to conduct veriﬁable ST-related experiments and applications, consequently improving the visual vocabularies of notations and effectiveness of the resulting diagrams.",
        "keywords": [
            "Semantic transparency",
            "Visual notations",
            "Cognitive effectiveness",
            "Systematic literature review"
        ],
        "authors": [
            "Saša Kuhar\nGregor Polanˇciˇc"
        ],
        "file_path": "data/sosym-all/s10270-021-00888-9.pdf"
    },
    {
        "title": "Evaluating the effort of composing design models: a controlled experiment",
        "submission-date": "2013/06",
        "publication-date": "2014/05",
        "abstract": "Model composition plays a key role in many tasks in model-centric software development, e.g., evolving UML diagrams to add new features or reconciling models developed in parallel by different software development teams. However, based on our experience in previous empirical studies, one of the main impairments for the widespread adoption of composition techniques is the lack of empirical knowledge about their effects on developers’ effort. This problem applies to both existing categories of model composition techniques, i.e., speciﬁcation-based (e.g., Epsilon) and heuristic-based techniques (e.g., IBM RSA). This paper, therefore, reports on a controlled experiment that investigates the effort of (1) applying both categories of model composi-tion techniques and (2) detecting and resolving inconsistencies in the output composed models. We evaluate the techniques in 144 evolution scenarios, where 2,304 compositions of elements of UML class diagrams were produced. The main results suggest that (1) the employed heuristic-based techniques require less effort to produce the intended model than the chosen speciﬁcation-based technique, (2) there is no signiﬁcant difference in the correctness of the output composed models generated by these techniques, and (3) the use of manual heuristics for model composition outperforms their automated counterparts.",
        "keywords": [
            "Model composition effort",
            "Empirical studies",
            "Effort measurement"
        ],
        "authors": [
            "Kleinner Farias",
            "Alessandro Garcia",
            "Jon Whittle",
            "Christina von Flach Garcia Chavez",
            "Carlos Lucena"
        ],
        "file_path": "data/sosym-all/s10270-014-0408-2.pdf"
    },
    {
        "title": "Toward live domain-speciﬁc languages\nFrom text differencing to adapting models at run time",
        "submission-date": "2016/06",
        "publication-date": "2017/08",
        "abstract": "Live programming is a style of development characterized by incremental change and immediate feedback. Instead of long edit-compile cycles, developers modify a running program by changing its source code, receiving immediate feedback as it instantly adapts in response. In this paper, we propose an approach to bridge the gap between running programs and textual domain-speciﬁc languages (DSLs). The ﬁrst step of our approach consists of applying a novel model differencing algorithm, tmdiff, to the textual DSL code. By leveraging ordinary text differencing and origin tracking, tmdiff produces deltas deﬁned in terms of the metamodel of a language. In the second step of our approach, the model deltas are applied at run time to update a running system, without having to restart it. Since the model deltas are derived from the static source code of the program, they are unaware of any run-time state maintained during model execution. We therefore propose a generic, dynamic patch architecture, rmpatch, which can be customized to cater for domain-speciﬁc state migration. We illustrate rmpatch in a case study of a live programming environment for a simple DSL implemented in Rascal for simultaneously deﬁning and executing state machines.",
        "keywords": [
            "Live programming",
            "Domain-speciﬁc languages",
            "Text differencing",
            "Model patching",
            "Adapting models",
            "Models at run time"
        ],
        "authors": [
            "Riemer van Rozen\nTijs van der Storm"
        ],
        "file_path": "data/sosym-all/s10270-017-0608-7.pdf"
    },
    {
        "title": "P-stable abstractions of hybrid systems",
        "submission-date": "2022/07",
        "publication-date": "2024/01",
        "abstract": "Stability is a fundamental requirement of dynamical systems. Most of the works concentrate on verifying stability for a given stability region. In this paper, we tackle the problem of synthesizing P-stable abstractions. Intuitively, the P-stable abstraction of a dynamical system characterizes the transitions between stability regions in response to external inputs. The stability regions are not given—rather, they are synthesized as their most precise representation with respect to a given set of predicates P. A P-stable abstraction is enriched by timing information derived from the duration of stabilization. We implement a synthesis algorithm in the framework of Abstract Interpretation that allows different degrees of approximation. We show the representational power of P-stable abstractions that provide a high-level account of the behavior of the system with respect to stability, and we experimentally evaluate the effectiveness of the algorithm in synthesizing P-stable abstractions for signiﬁcant systems.",
        "keywords": [
            "P-stable abstraction",
            "Hybrid systems",
            "Reverse engineering Abstract Interpretation",
            "Predicate abstraction",
            "Run-to-completion"
        ],
        "authors": [
            "Anna Becchi",
            "Alessandro Cimatti",
            "Enea Zaffanella"
        ],
        "file_path": "data/sosym-all/s10270-023-01145-x.pdf"
    },
    {
        "title": "Engineering model transformations with transML",
        "submission-date": "2011/03",
        "publication-date": "2011/09",
        "abstract": "Model transformation is one of the pillars of model-driven engineering (MDE). The increasing complexity of systems and modelling languages has dramatically raised the complexity and size of model transformations as well. Even though many transformation languages and tools have been proposed in the last few years, most of them are directed to the implementation phase of transformation development. In this way, even though transformations should be built using sound engineering principles—just like any other kind of software—there is currently a lack of cohesive support for the other phases of the transformation development, like requirements, analysis, design and testing. In this paper, we propose a uniﬁed family of languages to cover the life cycle of transformation development enabling the engineering of transformations. Moreover, following an MDE approach, we provide tools to partially automate the progressive reﬁnement of models between the different phases and the generation of code for several transformation implementation languages.",
        "keywords": [
            "Model-driven engineering",
            "Model transformation",
            "Domain-speciﬁc languages"
        ],
        "authors": [
            "Esther Guerra",
            "Juan de Lara",
            "Dimitrios S. Kolovos",
            "Richard F. Paige",
            "Osmar Marchi dos Santos"
        ],
        "file_path": "data/sosym-all/s10270-011-0211-2.pdf"
    },
    {
        "title": "Speciﬁcation-driven predictive business process monitoring",
        "submission-date": "2018/11",
        "publication-date": "2019/11",
        "abstract": "Predictive analysis in business process monitoring aims at forecasting the future information of a running business process. The prediction is typically made based on the model extracted from historical process execution logs (event logs). In practice, different business domains might require different kinds of predictions. Hence, it is important to have a means for properly specifying the desired prediction tasks, and a mechanism to deal with these various prediction tasks. Although there have been many studies in this area, they mostly focus on a speciﬁc prediction task. This work introduces a language for specifying the desired prediction tasks, and this language allows us to express various kinds of prediction tasks. This work also presents a mechanism for automatically creating the corresponding prediction model based on the given speciﬁcation. Differently from previous studies, instead of focusing on a particular prediction task, we present an approach to deal with various prediction tasks based on the given speciﬁcation of the desired prediction tasks. We also provide an implementation of the approach which is used to conduct experiments using real-life event logs.",
        "keywords": [
            "Predictive business process monitoring",
            "Prediction task speciﬁcation language",
            "Automatic prediction model creation",
            "Machine learning-based prediction"
        ],
        "authors": [
            "Ario Santoso",
            "Michael Felderer"
        ],
        "file_path": "data/sosym-all/s10270-019-00761-w.pdf"
    },
    {
        "title": "Circulise4LCE, a model-driven sustainability development framework for local circular economy engineering and simulation",
        "submission-date": "2025/01",
        "publication-date": "Not found",
        "abstract": "A local circular economy (LCE) is a closed-loop system operating at a community or regional scale where resources are efficiently recycled, repurposed, and managed to achieve sustainability. This paper introduces Circulise4LCE, a model-driven framework designed to support the development of LCEs through conceptual modeling. The framework consists of an i* LCE pattern, a circular BPMN workflow, a derived class diagram and a supporting simulation environment. It is designed to be adaptable, scalable, and replicable across various local contexts. The framework is applied to the Fanyatu case study (a non-profit organization supporting reforestation in the Congo Basin) allowing to show its ability to synchronize social, technological, and environmental stakeholders in a minimal system. The application leads to a simulation environment offering dynamic analysis of circularity and allowing for optimization and evaluation of local sustainability solutions. Results show how the framework helps address sustainability challenges by aligning resource management, community engagement, and technology integration.",
        "keywords": [
            "Local circular economy",
            "Circular economy",
            "Twin transition",
            "Digital twin",
            "iStar",
            "Sustainability",
            "Structure-in-5"
        ],
        "authors": [
            "Yves Wautelet",
            "Xavier Rouget"
        ],
        "file_path": "data/sosym-all/s10270-025-01314-0.pdf"
    },
    {
        "title": "The Petri net twist in explicit model checking",
        "submission-date": "2014/01",
        "publication-date": "2014/06",
        "abstract": "The invention of Petri nets was based on a critical analysis of then dominating automata models of systems. Explicit model checking explores the reachable states of a Petri net one by one. Essentially, it transforms a Petri net back to a transition system, that is, an automata-like model. At ﬁrst glance, this transformation appears to give up on all the speciﬁcs of Petri nets. Surveying the most dominant techniques of explicit state space veriﬁcation, we will, however, work out that even in explicit model checking, the deﬁning features of Petri nets are beneﬁcial and lead to more efﬁcient exploration routines. The ﬁndings in this paper are based on practical experience with a Petri net-based explicit model checking tool.",
        "keywords": [
            "Petri net",
            "Model checking",
            "Symmetry",
            "Sweep-line method",
            "Partial order reduction"
        ],
        "authors": [
            "Karsten Wolf"
        ],
        "file_path": "data/sosym-all/s10270-014-0422-4.pdf"
    },
    {
        "title": "Guest editorial to the special section of models 2019",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "The IEEE/ACM International Conference on Model Driven Engineering Languages and Systems (MODELS) serves as a venue for researchers and practitioners to share recent advances and practices in the area of model-based software and system engineering (MBE including MBSE). The 22nd edition of the conference, i.e., MODELS 2019, took place in Munich, Germany, with the overall aim to provide a platform for communicating advances in the foundations of MBE, and its innovative applications in engineering complex software systems, for sharing experiences of developing solutions particularly for handling properties such as uncertainty and robustness, by integrating machine learning and AI techniques when needed. A total of 114 papers were submitted, with 81 papers submitted to the Foundations Track (of which 18 were accepted) and 33 to the Practice and Innovation Track (of which 12 were accepted). Together, both tracks had an acceptance rate of 26%. Following the tradition of previous years, we selected and invited papers with the highest scores and reviews for this special section. These papers were extended according to the relevant SoSyM requirements and went through the SoSyM review process for the journal publication. Authors received anonymous feedback in two or three rounds of reviewing from three expert reviewers per paper. All in all, eleven submissions form the special section, as we discuss below.",
        "keywords": [],
        "authors": [
            "Tao Yue",
            "Silvia Abrahão",
            "Man Zhang"
        ],
        "file_path": "data/sosym-all/s10270-021-00939-1.pdf"
    },
    {
        "title": "Utilizing multi-level concepts for multi-phase modeling Context-awareness and process-based constraints to enable model evolution",
        "submission-date": "2020/09",
        "publication-date": "2022/01",
        "abstract": "In model-based systems engineering projects, engineers from multiple domains collaborate by establishing a common system model. Multi-level modeling is a technique that can be used to model the development from abstract ideas to concrete implementations. However, current multi-level modeling approaches are not adequate for processes with multiple modeling phases that might have to be rearranged later. In this paper, we introduce multi-phase modeling that utilizes concepts of multi-level modeling by considering a description of the expected phase ordering per domain. Constraints aware of this context can express that certain elements are only valid in speciﬁc phases without having to determine a concrete phase ordering for a particular model. This enables using multi-phase modeling in ﬂexible workﬂows, adapting to changing requirements and the deﬁnition of access rules in domain notation. We show feasibility of this multi-phase modeling by applying it to multiple real-life systems engineering projects of the aerospace domain.",
        "keywords": [
            "Model-based systems engineering",
            "Multi-level modeling",
            "Domain-speciﬁc languages",
            "Systems engineering"
        ],
        "authors": [
            "Tobias Franz",
            "Christoph Seidl",
            "Philipp M. Fischer",
            "Andreas Gerndt"
        ],
        "file_path": "data/sosym-all/s10270-021-00963-1.pdf"
    },
    {
        "title": "A classiﬁcation and rationalization of model-based software development",
        "submission-date": "2013/01",
        "publication-date": "2013/06",
        "abstract": "The use of model-based software development is increasingly popular due to recent advancements in modeling technology. Numerous approaches exist; this paper seeks to organize and characterize them. In particular, important terminological confusion, challenges, and recurring techniques of model-based software development are identiﬁed and rationalized. New perspectives are provided on some fundamental issues, such as the distinctions between model-driven development and architecture-centric development, code generation, and metamodeling. On the basis of this discussion, we opine that architecture-centric development and domain-speciﬁc model-driven development are the two most promising branches of model-based software development. Achieving a positive future will require, however, speciﬁc advances in software modeling, code generation, and model-code consistency management.",
        "keywords": [
            "Model-based software development",
            "Model-driven development",
            "Architecture-centric development"
        ],
        "authors": [
            "Yongjie Zheng",
            "Richard N. Taylor"
        ],
        "file_path": "data/sosym-all/s10270-013-0355-3.pdf"
    },
    {
        "title": "A study on the impact of the level of participation in enterprise modeling",
        "submission-date": "2024/02",
        "publication-date": "2025/01",
        "abstract": "Participatory enterprise modeling (PEM) is presumed to have a positive impact on commitment, ownership feelings and further appraisals by domain experts with respect to the model. However, there has not been a lot of research into whether PEM actually has the desired effects. In this paper we report on an investigation of the effects of three settings with different levels of involving domain experts: an overall model was created (1) from four individual interviews, (2) from four individual models, or (3) in a joint meeting of domain and modeling consultants. The results show that the non-participatory interview setting led to less favorable appraisals, e.g., the possibility to participate and the value of the model were perceived as lower and the contribution of the modeling consultants was perceived as higher. Our ﬁndings should help practitioners in weighing possible beneﬁts of participatory enterprise modeling against the organizational effort and monetary cost it involves.",
        "keywords": [
            "Participatory enterprise modeling",
            "Experiment",
            "Participation",
            "Conceptual modeling"
        ],
        "authors": [
            "Anne Gutschmidt",
            "Charlotte Verbruggen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-025-01275-4.pdf"
    },
    {
        "title": "Proﬁling users via their reviews: an extended systematic mapping study",
        "submission-date": "2019/09",
        "publication-date": "2020/03",
        "abstract": "Withtheextensivedevelopmentofbigdataandsocialnetworks,theuserproﬁleﬁeldhasreceivedmuchattention.Userproﬁling\nis essential for understanding the characteristics of various users, contributing to better understanding of their requirements\nin speciﬁc scenarios. User-generated contents which directly reﬂect people’s thoughts and intention are a valuable source\nfor proﬁling users, among which user reviews by nature are invaluable sources for acquiring user requirements and have\ndrawn increasing attention from both academia and industry. However, review-based user proﬁling (RBUP), as an emerging\nresearch direction, has not been systematically reviewed, hindering researchers from further investigation. In this work, we\ncarry out a systematic mapping study on review-based user proﬁling, with an emphasis on investigating the generic analysis\nprocess of RBUP and identifying potential research directions. Speciﬁcally, 51 out of 2478 papers were carefully selected\nfor investigation under a standardized and systematic procedure. By carrying out in-depth analysis over such papers, we\nhave identiﬁed a generic process that should be followed to perform review-based user proﬁling. In addition, we perform\nmulti-dimensional analysis on each step of the process in order to review current research progress and identify challenges\nand potential research directions. The results show that although traditional methods have been continuously improved,\nthey are not sufﬁcient to unleash the full potential of large-scale user reviews, especially the use of heterogeneous data for\nmulti-dimensional user proﬁling.",
        "keywords": [
            "User-generated reviews",
            "User proﬁling",
            "Systematic mapping study",
            "Software requirements"
        ],
        "authors": [
            "Xin Dong",
            "Tong Li",
            "Rui Song",
            "Zhiming Ding"
        ],
        "file_path": "data/sosym-all/s10270-020-00790-w.pdf"
    },
    {
        "title": "Semantics of trace relations in requirements models for consistency checking and inferencing",
        "submission-date": "2009/01",
        "publication-date": "2009/12",
        "abstract": "Requirements traceability is the ability to relate requirements back to stakeholders and forward to corresponding design artifacts, code, and test cases. Although considerable research has been devoted to relating requirements in both forward and backward directions, less attention has been paid to relating requirements with other requirements. Relations between requirements influence a number of activities during software development such as consistency checking and change management. In most approaches and tools, there is a lack of precise definition of requirements relations. In this respect, deficient results may be produced. In this paper, we aim at formal definitions of the relation types in order to enable reasoning about requirements relations. We give a requirements metamodel with commonly used relation types. The semantics of the relations is provided with a formalization in first-order logic. We use the formalization for consistency checking of relations and for inferring new relations. A tool has been built to support both reasoning activities. We illustrate our approach in an example which shows that the formal semantics of relation types enables new relations to be inferred and contradicting relations in requirements documents to be determined. The application of requirements reasoning based on formal semantics resolves many of the deficiencies observed in other approaches. Our tool supports better understanding of dependencies between requirements.",
        "keywords": [
            "Requirements metamodel",
            "Requirements traceability",
            "Inferencing",
            "Consistency checking",
            "Reasoning"
        ],
        "authors": [
            "Arda Goknil",
            "Ivan Kurtev",
            "Klaas van den Berg",
            "Jan-Willem Veldhuis"
        ],
        "file_path": "data/sosym-all/s10270-009-0142-3.pdf"
    },
    {
        "title": "A metamodel for modeling system features and their reﬁnement, constraint and interaction relationships",
        "submission-date": "2005/04",
        "publication-date": "2006/02",
        "abstract": "This paper presents a metamodel for modeling system features and relationships between features. The underlying idea of this metamodel is to employ features as ﬁrst-class entities in the problem space of software and to improve the customization of software by explicitly specifying both static and dynamic dependencies between system features. In this metamodel, features are organized as hierarchy structures by the reﬁnement relationships, static dependencies between features are speciﬁed by the constraint relationships, and dynamic dependencies between features are captured by the interaction relationships. A ﬁrst-order logic based method is proposed to formalize constraints and to verify constraints and customization. This paper also presents a framework for interaction classiﬁcation, and an informal mapping between interactions and constraints through constraint semantics.",
        "keywords": [
            "Feature model",
            "Relationships between features",
            "Reﬁnement",
            "Constraint",
            "Interaction",
            "Customization"
        ],
        "authors": [
            "Hong Mei",
            "Wei Zhang",
            "Haiyan Zhao"
        ],
        "file_path": "data/sosym-all/s10270-006-0004-1.pdf"
    },
    {
        "title": "ParDSL: a domain-speciﬁc language framework for supporting deployment of parallel algorithms",
        "submission-date": "2017/10",
        "publication-date": "2018/12",
        "abstract": "An important challenge in parallel computing is the mapping of parallel algorithms to parallel computing platforms. This requires several activities such as the analysis of the parallel algorithm, the deﬁnition of the logical conﬁguration of the platform and the implementation and deployment of the algorithm to the computing platform. However, in current parallel computing approaches very often only conceptual and idiosyncratic models are used which fall short in supporting the communication and analysis of the design decisions. In this article, we present ParDSL, a domain-speciﬁc language framework for providing explicit models to support the activities for mapping parallel algorithms to parallel computing platforms. The language framework includes four coherent set of domain-speciﬁc languages each of which focuses on an activity of the mapping process. We use the domain-speciﬁc languages for modeling the design as well as for generating the required platform-speciﬁc models and the code of the selected parallel algorithm. In addition to the languages, a library is deﬁned to support systematic reuse. We discuss the overall architecture of the language framework, the separate DSLs, the corresponding model transformations and the toolset. The framework is illustrated for four different parallel computing algorithms.",
        "keywords": [
            "Model-driven software development",
            "Parallel programming",
            "High-performance computing",
            "Domain-speciﬁc language",
            "Architecture framework"
        ],
        "authors": [
            "Bedir Tekinerdogan",
            "Ethem Arkin"
        ],
        "file_path": "data/sosym-all/s10270-018-00705-w.pdf"
    },
    {
        "title": "Personal programming and the object computer",
        "submission-date": "2019/10",
        "publication-date": "2019/12",
        "abstract": "My objective is to create an intuitive computer for laypeople who want to go beyond ready-made apps and create programs to control their electronic environment. I submit Loke, a new kind of computer that is a universe of objects and nothing but objects. I call it an object computer. Loke is implemented in Squeak, a variant of Smalltalk, and is an extensible, conceptual model for execution, inspection, and exploration. It was first used to demonstrate how Ellen, a novice, programs a smart alarm clock through a GUI adapted to her competence, needs, and preferences. Informal demonstrations indicated that laypeople immediately grasp the idea of communicating objects that represent real things in their environment. They also wanted to use it for their own purposes. They were creative in identifying personal opportunities for Loke and in sketching out their implementations. Interestingly, expert programmers who attended the demonstration did not see the point of Loke. I have completed the programming of Loke qua conceptual model. The model underpins its potential security and privacy and sustains its object and message models. The Loke qua programming environment is still in its infancy, and its inherent security and privacy properties are still not realized in practice. A future Loke device will be accessible from anywhere and embedded in its own hardware to achieve them. The Loke IDE rests on Data–Context–Interaction (DCI), a new program-ming paradigm that leads to readable code with a clear architecture. I submit Loke for the pleasure of personal programming.",
        "keywords": [
            "Personal programming",
            "Laypeople programming",
            "IoT",
            "Smart home",
            "Industry 5.0",
            "Loke",
            "Object computer",
            "BabyIDE",
            "MVC",
            "DCI",
            "Smalltalk",
            "Squeak"
        ],
        "authors": [
            "Trygve M. H. Reenskaug"
        ],
        "file_path": "data/sosym-all/s10270-019-00768-3.pdf"
    },
    {
        "title": "Trustworthy agent-based simulation: the case for domain-speciﬁc modelling languages",
        "submission-date": "2022/12",
        "publication-date": "2023/02",
        "abstract": "Simulation is a key tool for researching complex system behaviour. Agent-based simulation has been applied across domains, such as biology, health, economics and urban sciences. However, engineering robust, efﬁcient, maintainable, and reliable agent-based simulations is challenging. We present a vision for engineering agent simulations comprising a family of domain-speciﬁc modelling languages (DSMLs) that integrates core software engineering, validation and simulation experimentation. We relate the vision to examples of principled simulation, to show how the DSMLs would improve robustness, efﬁciency, and maintainability of simulations. Focusing on how to demonstrate the ﬁtness for purpose of a simulator, the envisaged approach supports bi-directional transparency and traceability between the original domain understanding to the implementation, interpretation of results and evaluation of hypotheses.",
        "keywords": [],
        "authors": [
            "Steffen Zschaler",
            "Fiona A. C. Polack"
        ],
        "file_path": "data/sosym-all/s10270-023-01082-9.pdf"
    },
    {
        "title": "Petri and how he saw the world",
        "submission-date": "2014/01",
        "publication-date": "2014/09",
        "abstract": "This article is a short summary and explanation of the scientiﬁc work of Carl Adam Petri. The very basics of net theory are sufﬁcient to understand it.",
        "keywords": [
            "Petri Nets",
            "Concurrency",
            "Asynchronous systems",
            "Discrete density",
            "Measuring theory"
        ],
        "authors": [
            "Einar Smith"
        ],
        "file_path": "data/sosym-all/s10270-014-0427-z.pdf"
    },
    {
        "title": "Exploring how users engage with hybrid process artifacts based on declarative process models: a behavioral analysis based on eye-tracking and think-aloud",
        "submission-date": "2019/11",
        "publication-date": "2020/07",
        "abstract": "Process design artifacts have been increasingly used to guide the modeling of business processes. To support users in designing and understanding process models, different process artifacts have been combined in several ways leading to the emergence of the so-called “hybrid process artifacts”. While many hybrid artifacts have been proposed in the literature, little is known about how they can actually support users in practice. To address this gap, this work investigates the way users engage with hybrid process artifacts during comprehension tasks. In particular, we focus on a hybrid representation of DCR Graphs (DCR-HR) combining a process model, textual annotations and an interactive simulation. Following a qualitative approach, we conduct a multi-granular analysis exploiting process mining, eye-tracking techniques, and verbal data analysis to scrutinize the reading patterns and the strategies adopted by users when being confronted with DCR-HR. The ﬁndings of the coarse-grained analysis provide important insights about the behavior of domain experts and IT specialists and show how user’s background and task type change the use of hybrid process artifacts. As for the ﬁne-grained analysis, user’s behavior was classiﬁed into goal-directed and exploratory and different strategies of using the interactive simulation were identiﬁed. In addition, a progressive switch from an exploratory behavior to a goal-directed behavior was observed. These insights pave the way for an improved development of hybrid process artifacts and delineate several directions for future work.",
        "keywords": [
            "Process models",
            "Hybrid process artifacts",
            "DCR graphs",
            "Eye-tracking",
            "Think-aloud",
            "Behavioral analysis"
        ],
        "authors": [
            "Amine Abbad Andaloussi",
            "Francesca Zerbato",
            "Andrea Burattin",
            "Tijs Slaats",
            "Thomas T. Hildebrandt",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-020-00811-8.pdf"
    },
    {
        "title": "Guidelines to derive an e3value business model from a BPMN process model: an experiment on real-world scenarios",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Process models, e.g., BPMN models, may represent how companies in an ecosystem interact with each other. However, the business model of the same ecosystem, e.g., expressed by an e3value model, is often left implicit. This hinders the proper analysis of the ecosystem at the business level, and more speciﬁcally ﬁnancial assessment, for which process models are less appropriate. Therefore, the question is if we can somehow derive e3value models from BPMN models. This would not only allow for proper business model analysis but would also facilitate business model mining, similar to the success of process mining. However, although an e3value model and BPMN model represent the same ecosystem, their perspectives differ signiﬁcantly. Therefore an automated derivation of an e3value model from a BPMN seems not to be feasible, but we can assist the e3value model designer with practical guidelines. We explore and test our guidelines in two real-world settings, we then analyze and evaluate its application to better understand their limitations and how to improve them.",
        "keywords": [
            "Ecosystems",
            "Business model",
            "Process model",
            "e3value",
            "BPMN"
        ],
        "authors": [
            "Isaac da Silva Torres",
            "Marcelo Fantinato",
            "Gabriela Musse Branco",
            "Jaap Gordijn"
        ],
        "file_path": "data/sosym-all/s10270-022-01074-1.pdf"
    },
    {
        "title": "Uniﬁed veriﬁcation and monitoring of executable UML speciﬁcations\nA transformation-free approach",
        "submission-date": "2020/05",
        "publication-date": "2021/11",
        "abstract": "The increasing complexity of embedded systems renders software veriﬁcation more complex, requiring monitoring and\nformal techniques, like model-checking. However, to use such techniques, system engineers usually need formal expertise to\nexpress the software requirements in a formal language. To facilitate the use of model-checking tools by system engineers,\nour approach uses a UML model interpreter through which the software requirements can directly be expressed in UML as\nwell. Formal requirements are encoded as UML state machines with the transition guards written in a speciﬁc observation\nlanguage, which expresses predicates on the execution of the system model. Each such executable UML speciﬁcation can\nmodel either a Büchi automaton or an observer automaton, and is synchronously composed with the system, to follow its\nexecution during model-checking. Formal veriﬁcation can continue at runtime for all deterministic observer automata used\nduring ofﬂine veriﬁcation by deploying them on real embedded systems. Our approach has been evaluated on multiple case\nstudies and is illustrated, in this paper, through the user interface model of a cruise-control system. The automata-based\nveriﬁcation results are in line with the veriﬁcation of the equivalent LTL properties. The runtime overhead during monitoring\nis proportional to the number of monitors.",
        "keywords": [
            "Model-checking",
            "Monitoring",
            "Model interpretation",
            "Embedded systems",
            "Observation Language",
            "Synchronous\nComposition"
        ],
        "authors": [
            "Valentin Besnard\nCiprian Teodorov\nFrédéric Jouault\nMatthias Brun\nPhilippe Dhaussy"
        ],
        "file_path": "data/sosym-all/s10270-021-00923-9.pdf"
    },
    {
        "title": "Towards the efﬁcient development of model transformations using model weaving and matching transformations",
        "submission-date": "2007/07",
        "publication-date": "2008/07",
        "abstract": "Model transformations can be used in many\ndifferent application scenarios, for instance, to provide inter-\noperability between models of different size and complexity.\nAs a consequence, they are becoming more and more\ncomplex. However, model transformations are typically\ndeveloped manually. Several code patterns are implemen-\nted repetitively, thus increasing the probability of program-\nming errors and reducing code reusability. There is not yet a\ncomplete solution that automates the development of model\ntransformations. In this paper, we present a novel approach\nthat uses matching transformations and weaving models to\nsemi-automate the development of transformations. Weaving\nmodels are models that contain different kinds of relation-\nships between model elements. These relationships capture\ndifferent transformation patterns. Matching transformations\nare a special kind of transformations that implement methods\nthat create weaving models. We present a practical\nsolution that enables the creation and the customization of\ndifferent creation methods in an efﬁcient way. We combine\ndifferent methods, and present a metamodel-based method\nthat exploits metamodel data to automatically produce wea-\nving models. The weaving models are derived into model\nintegration transformations. To validate our approach, we\npresent an experiment using metamodels with distinct size\nand complexity, which show the feasibility and scalability of\nour solution.",
        "keywords": [
            "Model engineering",
            "Matching transforma-\ntions",
            "Model weaving"
        ],
        "authors": [
            "Marcos Didonet Del Fabro",
            "Patrick Valduriez"
        ],
        "file_path": "data/sosym-all/s10270-008-0094-z.pdf"
    },
    {
        "title": "Continuous situation-speciﬁc development of business models: knowledge provision, method composition, and method enactment",
        "submission-date": "2021/10",
        "publication-date": "2022/07",
        "abstract": "The development of new business models is essential for startups to become successful, as well as for established companies to explore new business opportunities. However, developing such business models is a continuous challenging activity where different tasks need to be performed, and business decisions need to be made. Both have to ﬁt the constantly changeable situation in which the business model is developed to reduce the risk of developing ineffective business models with low market penetration. Therefore, a method for developing situation-speciﬁc business models is needed. As a solution, we reﬁne the concept of situational method engineering (SME) to business model development. SME, in turn, provides means to construct situation-speciﬁc development methods out of fragments from a method repository. We develop a concept for the continuous situation-speciﬁc development of business models based on design science. The approach uses the roles of a domain expert, a method engineer, and a business developer together with a repository with method fragments for developing business models and a repository with modeling artifacts for supporting the development. Both repositories are ﬁlled by utilizing the experience of domain experts. Out of these repositories, situation-speciﬁc development methods for developing business models can be continuously composed based on the changeable situation by the method engineer and enacted by the business developer. We implement it as an open-source tool and evaluate its applicability in an industrial case study of developing a business model for a local event platform. Our results show that situation awareness supports the continuous development of business models.",
        "keywords": [
            "Business model development",
            "Situational method engineering",
            "Situation-speciﬁc",
            "Business model canvas",
            "Continuous development"
        ],
        "authors": [
            "Sebastian Gottschalk",
            "Enes Yigitbas",
            "Alexander Nowosad",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-022-01018-9.pdf"
    },
    {
        "title": "Extending the UML use case metamodel with behavioral information to facilitate model analysis and interchange",
        "submission-date": "2011/12",
        "publication-date": "2013/03",
        "abstract": "Use case diagrams are primary artifacts used for modeling functional requirements. Use case diagrams are part of the Uniﬁed Modeling Language (UML) suite of models that has become a de facto standard for modeling object oriented languages. The use case diagram is considered the most controversial diagram in UML. Practitioners claim that the use case diagram cannot be used as a valuable artifact for requirement analysis. The main reason behind this concern is the lack of behavioral description of a use case depicted within the model. Quite a few extensions to the use case metamodel have been proposed in literature to incorporate behavioral aspect of a use case within the metamodel. All these extensions omit a few important features like generalization and most of them can only be used for model representation and cannot be used for model analysis and evaluation. In this paper, we propose an extension to the UML use case metamodel with use case behavior speciﬁcation elements. The main objective of the proposed extension is to provide a complete metamodel for use case diagrams which includes representation for all its elements and relationships in a conﬂict-free manner and one that includes information for model analysis, evaluation, and interchange among modeling tools. In order to include all valuable information related to a use case, a number of use case representation templates were considered for the proposed extension. Simultaneously, to enable the use case models generated based on the proposed metamodel to be used for analysis, pertinent information related to model usage in analysis such as effort estimation, use case scheduling, and use case metrics evaluation were considered from published studies, tools, and paradigms and included within the proposed metamodel.",
        "keywords": [
            "UML",
            "Use case diagram",
            "Metamodel",
            "Behavior speciﬁcation"
        ],
        "authors": [
            "Mohammed Misbhauddin",
            "Mohammad Alshayeb"
        ],
        "file_path": "data/sosym-all/s10270-013-0333-9.pdf"
    },
    {
        "title": "A fractal enterprise model and its application for business development",
        "submission-date": "2014/10",
        "publication-date": "2016/08",
        "abstract": "This paper suggests a new type of enterprise models called fractal enterprise models (FEM), with accompanying methodological support for their design. FEM shows interconnections between the business processes in an enterprise by connecting them to the assets they use and manage. Assets considered in the model could be tangible (buildings, heavy machinery, etc.) and intangible (employees, business process definitions, etc.). A FEM model is built by using two types of patterns called archetypes: a process-assets archetype that connects a process with assets used in it, and an asset-processes archetype that connects an asset with processes aimed to manage this asset (e.g., hiring people, or servicing machinery). Alternating these patterns creates a fractal structure that makes relationships between various parts of the enterprise explicit. FEM can be used for different purposes, including finding a majority of the processes in an enterprise and planning business change or radical transformation. Besides discussing FEM and areas of its usage, the paper presents results from a completed project in order to test the practical usefulness of FEM and its related methodological support.",
        "keywords": [
            "Business process",
            "Enterprise modeling",
            "Fractal enterprise",
            "Asset"
        ],
        "authors": [
            "Ilia Bider",
            "Erik Perjons",
            "Mturi Elias",
            "Paul Johannesson"
        ],
        "file_path": "data/sosym-all/s10270-016-0554-9.pdf"
    },
    {
        "title": "Design rationale capture for process improvement in the globalised enterprise: an industrial study",
        "submission-date": "2011/03",
        "publication-date": "2011/12",
        "abstract": "Design rationale ﬁlls in the gaps between the original requirements of a system and the ﬁnished product encompassing decisions, constraints and other information that inﬂuenced the outcome. Existing research in Software Engineering corroborates the importance of design rationale to capture knowledge assets, particularly in the context of the global enterprise, with its increased reliance on IT systems, and risk of knowledge loss through staff movement and attri- tion. Despite this, the practice of design rationale capture is not as extensive as could be expected due to reasons which include time and budget constraints, the lack of standards and tools, and some uncertainty as to its actual added value. In this paper, we address the viability and beneﬁts of capturing design rationale as a by-product of design in the context of a real-world global organisational setting. This was achieved through a study in which an emerging design approach—Problem Oriented Engineering—was applied in the context of a global ﬁnancial institution to address a critical IT prob- lem as part of its software supplier’s client resolution process. The study provides some positive evidence that the approach-guided knowledge capture of key design rationale elements and that it combined well with existing practices within the organisation and even led to improvement to one of their key processes.",
        "keywords": [
            "Design rationale",
            "Process improvement",
            "Problem oriented engineering",
            "Assurance-driven design"
        ],
        "authors": [
            "A. Nkwocha",
            "J. G. Hall",
            "L. Rapanotti"
        ],
        "file_path": "data/sosym-all/s10270-011-0223-y.pdf"
    },
    {
        "title": "Extending enterprise architecture modeling languages for domain specificity and collaboration: application to telecommunication service design",
        "submission-date": "2011/11",
        "publication-date": "2012/11",
        "abstract": "The competitive market forces organizations to be agile and flexible so as to react robustly to complex events. Modeling helps managing this complexity. However, in order to model an enterprise, many stakeholders, with different expertise, must work together and take decisions. These decisions and their rationale are not always captured explicitly, in a standard, formal manner. The main problem is to persuade stakeholders to capture them. This article synthesizes an approach for capturing and using the rationale behind enterprise modeling decisions. The approach is implemented through a domain-specific modeling language, defined as an extension of a standard enterprise architecture modeling language. It promotes coordination, enables presenting different stakeholders’ points of view, facilities participation and collaboration in modeling activities—activities focused here on enterprise architecture viewpoints. To present its benefits, such as rapid prototyping, the approach is applied to large organizations in the context of telecommunication service design. It is exemplified on modeling and capturing decisions on a conference service.",
        "keywords": [
            "Enterprise architecture framework",
            "Enterprise architecture modeling language",
            "Domain-specific modeling language",
            "Language extension",
            "Meta-model",
            "Design rationale",
            "Telecommunication service"
        ],
        "authors": [
            "Vanea Chiprianov",
            "Yvon Kermarrec",
            "Siegfried Rouvrais",
            "Jacques Simonin"
        ],
        "file_path": "data/sosym-all/s10270-012-0298-0.pdf"
    },
    {
        "title": "Implementing QVT-R via semantic interpretation in UML-RSDS",
        "submission-date": "2020/01",
        "publication-date": "2020/09",
        "abstract": "The QVT-Relations (QVT-R) model transformation language is an OMG standard notation for model transformation spec-\niﬁcation. It is highly declarative and supports (in principle) bidirectional (bx) transformation speciﬁcation. However, there\nare many unclear or unsatisfactory aspects to its semantics, which is not precisely deﬁned in the standard. UML-RSDS is an\nexecutable subset of UML and OCL. It has a precise mathematical semantics and criteria for ensuring correctness of applica-\ntions (including model transformations) by construction. There is extensive tool support for veriﬁcation and for production\nof 3GL code in multiple languages (Java, C#, C++, C, Swift and Python). In this paper, we deﬁne a translation from QVT-R\ninto UML-RSDS, which provides a logically oriented semantics for QVT-R, aligned with the RelToCore mapping semantics\nin the QVT standard. The translation includes variation points to enable specialised semantics to be selected in particular\ntransformation cases. The translation provides a basis for veriﬁcation and static analysis of QVT-R speciﬁcations and also\nenables the production of efﬁcient code implementations of QVT-R speciﬁcations. We evaluate the approach by applying it\nto solve benchmark examples of bx.",
        "keywords": [
            "Model transformations",
            "QVT-Relations",
            "UML-RSDS",
            "Model transformation semantics",
            "Model transformation tools"
        ],
        "authors": [
            "K. Lano",
            "S. Kolahdouz-Rahimi"
        ],
        "file_path": "data/sosym-all/s10270-020-00824-3.pdf"
    },
    {
        "title": "Analysing factors impacting BPMS performance: a case of a challenged technology adoption",
        "submission-date": "2020/10",
        "publication-date": "2021/09",
        "abstract": "Business Process Management Suites (BPMSs) have been adopted in organisations to model, improve and automate busi-ness processes as they aim to increase the quality, efficiency and agility of their business processes. Yet, many organisations struggle to achieve the benefits they expected from a BPMS. This interpretive case study in a large South African financial services organisation explains factors found to negatively impact successful BPMS adoption. The paper describes how an IT team struggled to increase process agility with a BPMS in a large legacy application landscape. The dominant factors caus-ing the struggle were the difficulty of integrating with other applications and a lack of governance around BPM. Interesting findings on the difficulties in resourcing BPM IT teams are presented. The impact of BPM strategy, culture and governance on BPM methods, resourcing, data and technology is explained. The BPM literature lacks empirical qualitative case studies and theoretical models. This paper aimed to contribute to both needs. The theoretical contribution of this paper is two mod-els. The first inductively derived explanatory contextual model should be useful for practitioners wanting to adopt a BPMS. Using this study’s findings and models from the literature, a second, more generic explanatory model of information system performance is derived for a BPMS.",
        "keywords": [
            "Business Process Management",
            "Business Process Management Suites",
            "BPM adoption",
            "Information systems performance"
        ],
        "authors": [
            "Lisa F. Seymour",
            "Ashley Koopman"
        ],
        "file_path": "data/sosym-all/s10270-021-00922-w.pdf"
    },
    {
        "title": "Adherence preserving reﬁnement of trace-set properties in STAIRS: exempliﬁed for information ﬂow properties and policies",
        "submission-date": "2008/07",
        "publication-date": "2008/09",
        "abstract": "STAIRS is a formal approach to system development with UML 2.1 sequence diagrams that supports an incremental and modular development process. STAIRS is underpinned by denotational and operational semantics that have been proved to be equivalent. STAIRS is more expressive than most approaches with a formal notion of reﬁnement. STAIRS supports a stepwise reﬁnement process under which trace properties as well as trace-set properties are preserved. This paper demonstrates the potential of STAIRS in this respect, in particular that reﬁnement in STAIRS preserves adherence to information ﬂow properties as well as policies.",
        "keywords": [],
        "authors": [
            "Fredrik Seehusen",
            "Bjørnar Solhaug",
            "Ketil Stølen"
        ],
        "file_path": "data/sosym-all/s10270-008-0102-3.pdf"
    },
    {
        "title": "Efﬁcient model similarity estimation with robust hashing",
        "submission-date": "2020/04",
        "publication-date": "2021/08",
        "abstract": "As model-driven engineering (MDE) is increasingly adopted in complex industrial scenarios, modeling artefacts become a key and strategic asset for companies. As such, any MDE ecosystem must provide mechanisms to protect and exploit them. Current approaches depend on the calculation of the relative similarity among pairs of models. Unfortunately, model similarity calculation mechanisms are computationally expensive which prevents their use in large repositories or very large models. In this sense, this paper explores the adaptation of the robust hashing technique to the MDE domain as an efﬁcient estimation method for model similarity. Indeed, robust hashing algorithms (i.e., hashing algorithms that generate similar outputs from similar input data) have proved useful as a key building block in intellectual property protection, authenticity assessment and fast comparison and retrieval solutions for different application domains. We present a detailed method for the generation of robust hashes for different types of models. Our approach is based on the translation to the MDE domain of diverse techniques such as summary extraction, minhash generation and locality-sensitive hash function families, originally developed for the comparison and classiﬁcation of large datasets. We validate our approach with a prototype implementation and show that: (1) our approach can deal with any graph-based model representation; (2) a strong correlation exists between the similarity calculated directly on the robust hashes and a distance metric calculated over the original models; and (3) our approach scales well on large models and greatly reduces the time required to ﬁnd similar models in large repositories.",
        "keywords": [
            "Model-driven engineering",
            "Locality-sensitive hashing",
            "Near-similar search",
            "Robust hashing Comparison"
        ],
        "authors": [
            "Salvador Martínez\nSébastien Gérard\nJordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-021-00915-9.pdf"
    },
    {
        "title": "Plug-and-play composition of features and feature interactions with statechart diagrams",
        "submission-date": "2003/11",
        "publication-date": "2003/11",
        "abstract": "This paper presents a new approach for modular design of highly-entangled software components by statechart diagrams. We structure the components into features, which represent reusable, self-contained services. These are modeled individually by statechart diagrams. For composition of components from features, we need to consider the interactions between the features. These feature interactions, well known in the telecommunications area, typically describe special cases or cooperations which only occur when two features are combined. We describe these interactions graphically by reﬁnement relations between statecharts. The main novelty is that full component descriptions are created in a plug-and-play fashion by combining the statecharts for the required features and interactions. Furthermore, we develop different classes of statecharts and show the interactions on a case-by-case basis. For composition, we use semantic refinement concepts for statecharts which preserve the original behavior.",
        "keywords": [
            "Graphic modeling techniques",
            "Plug-and-play composition",
            "Feature interaction",
            "Statechart diagrams",
            "Semantic refinement",
            "UML"
        ],
        "authors": [
            "Christian Prehofer"
        ],
        "file_path": "data/sosym-all/s10270-003-0040-z.pdf"
    },
    {
        "title": "Cyber security threat modeling based on the MITRE Enterprise ATT&CK Matrix",
        "submission-date": "2020/07",
        "publication-date": "2021/06",
        "abstract": "Enterprise systems are growing in complexity, and the adoption of cloud and mobile services has greatly increased the attack surface. To proactively address these security issues in enterprise systems, this paper proposes a threat modeling language for enterprise security based on the MITRE Enterprise ATT&CK Matrix. It is designed using the Meta Attack Language framework and focuses on describing system assets, attack steps, defenses, and asset associations. The attack steps in the language represent adversary techniques as listed and described by MITRE. This entity-relationship model describes enterprise ITsystemsasawhole;byusingavailabletools,theproposedlanguageenablesattacksimulationsonitssystemmodelinstances. These simulations can be used to investigate security settings and architectural changes that might be implemented to secure the system more effectively. Our proposed language is tested with a number of unit and integration tests. This is visualized in the paper with two real cyber attacks modeled and simulated.",
        "keywords": [
            "Threat modeling",
            "Domain-speciﬁc language",
            "Attack simulations",
            "Enterprise systems"
        ],
        "authors": [
            "Wenjun Xiong",
            "Emeline Legrand",
            "Oscar Åberg",
            "Robert Lagerström"
        ],
        "file_path": "data/sosym-all/s10270-021-00898-7.pdf"
    },
    {
        "title": "First Issue of the International Journal on Software and Systems Modeling",
        "submission-date": "2002/09",
        "publication-date": "2002/09",
        "abstract": "This paper introduces the first issue of the Software and Systems Modeling (SoSyM) journal. It outlines the journal's aims, scope, and objectives, focusing on theoretical and practical aspects of software and system modeling languages, methods, and techniques. The journal aims to publish high-quality research that benefits both practitioners and researchers in the field.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-002-0006-6.pdf"
    },
    {
        "title": "On leveraging the fruits of research efforts in the arena of business process modeling formalisms: a map-driven approach for decision making",
        "submission-date": "2017/10",
        "publication-date": "2018/08",
        "abstract": "The importance of business process (BP) modeling to Business Process Management can be justiﬁed by the serious problems, which may arise in the latter, if the former is not conducted correctly. This can take place, for instance, in the case of an inappropriate choice of a BP modeling formalism. Many frameworks have therefore been proposed in order to guide the selection of the most suitable formalism. Nevertheless, these research efforts still scattered throughout a wide range of publications, unlinked and often with a narrow scope. Hence, there is a need for both a combination of these research labors into a single reference source and an approach to support such combination. This paper is the ﬁrst contribution toward responding to this need. It proposes a methodological roadmap as a basis for developing a selection support system. The results presented in the current paper stand for a ﬁrst step toward a reference framework for helping researchers and practitioners in choosing the most appropriate BP modeling formalism given a set of contextual variables. The proposed methodological roadmap is speciﬁed using a modeling formalism (MAP) which considers the “decision” as a ﬁrst-class citizen.",
        "keywords": [
            "Business process modeling formalism",
            "Map-driven approach",
            "Methodological guidelines",
            "Systematic literature review",
            "Context model",
            "Roadmap"
        ],
        "authors": [
            "Afef Awadid",
            "Selmin Nurcan",
            "Sonia Ayachi Ghannouchi"
        ],
        "file_path": "data/sosym-all/s10270-018-0689-y.pdf"
    },
    {
        "title": "What can we learn from enterprise architecture models? An experiment comparing models and documents for capability development",
        "submission-date": "2015/08",
        "publication-date": "2016/05",
        "abstract": "Enterprise architecture (EA) has been established as a discipline to cope with the complex interactions of business operations and technology. Models, i.e., formal descriptionsintermsofdiagramsandviews,areattheheartof the approach. Though it is widely thought that such architecture models can contribute to improved understanding and decision making, this proposition has not rigorously been tested. This article describes an experiment conducted with a real EA model and corresponding real traditional documents, investigating whether the model or the documents lead to bet- ter and faster understanding. Understanding is interesting to study, as it is a prerequisite to other EA uses. The subjects (N = 98) were ofﬁcer cadets, and the experiment was carried out using a comprehensive description of military Close Air Support capability either (1) in the form of a MODAF model or (2) in the form of traditional documents. Based on the results, the model seems to lead to better, though not faster, understanding.",
        "keywords": [
            "Enterprise architecture",
            "MODAF",
            "Model-based capability development",
            "Experiment",
            "Models versus documents"
        ],
        "authors": [
            "Ulrik Franke",
            "Mika Cohen",
            "Johan Sigholm"
        ],
        "file_path": "data/sosym-all/s10270-016-0535-z.pdf"
    },
    {
        "title": "Editorial to the theme section on model-based design of cyber-physical systems",
        "submission-date": "2018/02",
        "publication-date": "2018/03",
        "abstract": "Cyber-physical systems (CPS) are engineered systems where functionalities are emerging from the networked interaction of physical and computational processes. CPS are a pervasive technology advancement, which is impacting today all industrial sectors and almost all aspects of society. While CPS design as a new research area started over a decade ago, the recent emergence of new industrial platforms for creating CPS such as the Internet of things (IoT), industrial Internet (II)andfogcomputingintheUSAandIndustrie4.0inEurope greatly accelerated the productization and deployment of the technology and created a “gold rush” toward new markets. Since model-based design plays pivotal role in all areas of engineered systems, it is an important issue to examine what are the new challenges for model-based design in CPS?",
        "keywords": [],
        "authors": [
            "Manfred Broy",
            "Heinrich Daembkes",
            "Janos Sztipanovits"
        ],
        "file_path": "data/sosym-all/s10270-018-0670-9.pdf"
    },
    {
        "title": "Modeling data protection and privacy: application and experience with GDPR",
        "submission-date": "2020/07",
        "publication-date": "2021/11",
        "abstract": "In Europe and indeed worldwide, the General Data Protection Regulation (GDPR) provides protection to individuals regarding their personal data in the face of new technological developments. GDPR is widely viewed as the benchmark for data protection and privacy regulations that harmonizes data privacy laws across Europe. Although the GDPR is highly beneﬁcial to individuals, it presents signiﬁcant challenges for organizations monitoring or storing personal information. Since there is currently no automated solution with broad industrial applicability, organizations have no choice but to carry out expensive manual audits to ensure GDPR compliance. In this paper, we present a complete GDPR UML model as a ﬁrst step toward designing automated methods for checking GDPR compliance. Given that the practical application of the GDPR is inﬂuenced by national laws of the EU Member States, we suggest a two-tiered description of the GDPR, generic and specialized. In this paper, we provide (1) the GDPR conceptual model we developed with complete traceability from its classes to the GDPR, (2) a glossary to help understand the model, (3) the plain-English description of 35 compliance rules derived from GDPR along with their encoding in OCL and (4) the set of 20 variations points derived from GDPR to specialize the generic model. We further present the challenges we faced in our modeling endeavor, the lessons we learned from it and future directions for research.",
        "keywords": [
            "General Data Protection Regulation (GDPR)",
            "Conceptual modeling",
            "Model variability",
            "Regulatory compliance",
            "Uniﬁed Modeling Language (UML)"
        ],
        "authors": [
            "Damiano Torre",
            "Mauricio Alferez",
            "Ghanem Soltana",
            "Mehrdad Sabetzadeh",
            "Lionel Briand"
        ],
        "file_path": "data/sosym-all/s10270-021-00935-5.pdf"
    },
    {
        "title": "Modeling to improve quality or efﬁciency? An automotive domain perspective",
        "submission-date": "2012/05",
        "publication-date": "2012/05",
        "abstract": "What added-value does modeling bring to the development process? An often stated value is the productivity improvement gained by using models to automatically generate code. Model-to-code generators provide automated support for bridging abstraction gaps, and thus relieve developers from routine, often tedious, decision-making and implementation activities. Developers using model-driven software development techniques should be able to deliver working software at a faster rate, and thus reduce time-to-market. Model-to-code generation is particularly valuable when it supports synchronized evolution of generated code and models, as opposed to “one-shot generation” in which generated code that has been modiﬁed cannot be re-generated from models because the models do not reﬂect the changes made to the generated code. Maintaining ﬁdelity between abstract models and the more detailed code as they evolve can lead to faster delivery of software extended with new or modiﬁed features. However, in the automotive and similar embedded software industries, process efﬁciency may be less of an issue. Consider the case of a particular European automotive company that had a total turnover of 60 Billion Euros and earned 6 Billion Euros. Car development, production, sales, after-sales,andothercostsforthiscompanythustotaledroughly54 Billion Euros. The company employs approximately 1,000 software developers and approximately another 1,200 software developers from component suppliers perform work for the company. If an engineer’s average salary is 120,000 Euros (this is the average salary reported in a recent study), then software development manpower cost for this company is approximately 264 Million Euros. This is 0.45 % of the overall costs. While 264 Million sounds like a lot, it is relatively insignificant compared to other costs. For example, a software error that results in a recall of over one million cars can be very costly. Assuming a cost of 400 Euros per recall, the result would be over 400 Million Euros in cost. In such a scenario the cost of a recall easily exceeds the total software development manpower costs. So what really counts in these cases? The answer should not be surprising: quality, quality, and quality. Time-to-market is second significant concern for these organizations. Efﬁciency probably only counts, when the availability of experienced developers to hire is low. Quality concerns can very nicely be addressed by model-based development methods. Modeling techniques can be used to automatically synthesize and compose sub-models that capture different features of a distributed system. Composed models can be used to generate high quality code that works on appropriate middleware, but was developed independent of any technology stack and can therefore be reused rather unchanged. The big advantage of reuse is not the efﬁ-ciency increase, but the reuse of quality that was already proven. We therefore need appropriate tooling infrastructure for developers in the automotive and similar domains for qual-ity and not that much for efﬁciency reasons. But we also need industries that have the courage to accept that the development process for software is significantly different from the traditional engineering process. Only then soft-ware will keep up with the desired quality and be there in time. Traditional engineering processes today significantly lack to address software speciﬁc characteristics, but appropriate combinations of software-aware development with",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-012-0251-2.pdf"
    },
    {
        "title": "Logic formulas in models",
        "submission-date": "2017/06",
        "publication-date": "2017/06",
        "abstract": "Many modeling languages, especially graphical ones, concentrate on the ease of expression in specifying certain aspects of a system that need to be developed. However, this often leads to a reduced ability to express complex relations between particular elements in the model. This is certainly obvious when using UML’s class diagrams, but also occurs in speciﬁcation-oriented versions of state machines, activity diagrams, business process models, and variants of architectural description languages. The restricted ability to describe additional constraints usually leads to the demand for an expressive logic that is used on top of the underlying modeling language. The Object Constraint Language (OCL), as part of the UML, is such an example.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-017-0605-x.pdf"
    },
    {
        "title": "An ontology-based framework for domain-speciﬁc modeling",
        "submission-date": "2010/11",
        "publication-date": "2012/04",
        "abstract": "Domain-speciﬁc languages (DSLs) provide abstractions and notations for better understanding and easier modeling of applications in a special domain. Current short-comings of DSLs include learning curve and formal semantics. This paper reports on a framework that allows the use of ontology technologies to describe and reason on DSLs. The formal semantics of OWL together with reasoning services allows for addressing constraint definition, progressive eval-uation, suggestions, and debugging. The approach integrates existing metamodels and concrete syntaxes in a new techni-cal space. A scenario in which domain models for network devices are created illustrates the framework.",
        "keywords": [
            "Domain-speciﬁc modeling",
            "Ontology technologies"
        ],
        "authors": [
            "Tobias Walter",
            "Fernando Silva Parreiras",
            "Steffen Staab"
        ],
        "file_path": "data/sosym-all/s10270-012-0249-9.pdf"
    },
    {
        "title": "Automaton-based comparison of Declare process models",
        "submission-date": "2022/03",
        "publication-date": "2022/12",
        "abstract": "The Declare process modeling language has been established within the research community for modeling so-called ﬂexible processes. Declare follows the declarative modeling paradigm and therefore guarantees ﬂexible process execution. For several reasons, declarative process models turned out to be hard to read and comprehend. Thus, it is also hard to decide whether two process models are equal with respect to their semantic meaning, whether one model is completely contained in another one or how far two models overlap. In this paper, we follow an automaton-based approach by transforming Declare process models into ﬁnite state automatons and applying automata theory for solving this issue.",
        "keywords": [
            "Business process modeling",
            "Declare",
            "Model comparison",
            "Declarative process management",
            "Automata theory"
        ],
        "authors": [
            "Nicolai Schützenmeier",
            "Martin Käppel",
            "Lars Ackermann",
            "Stefan Jablonski",
            "Sebastian Petter"
        ],
        "file_path": "data/sosym-all/s10270-022-01069-y.pdf"
    },
    {
        "title": "Lossless compaction of model execution traces",
        "submission-date": "2018/10",
        "publication-date": "2019/06",
        "abstract": "Dynamic veriﬁcation and validation (V&V) techniques are used to verify and validate the behavior of software systems early\nin the development process. In the context of model-driven engineering, such behaviors are usually deﬁned using executable\ndomain-speciﬁc modeling languages (xDSML). Many V&V techniques rely on execution traces to represent and analyze the\nbehavior of executable models. Traces, however, tend to be overwhelmingly large, hindering effective and efﬁcient analysis\nof their content. While there exist several trace metamodels to represent execution traces, most of them suffer from scalability\nproblems. In this paper, we present a generic compact trace representation format called generic compact trace metamodel\n(CTM) that enables the construction and manipulation of compact execution traces of executable models. CTM is generic\nin the sense that it supports a wide range of xDSMLs. We evaluate CTM on traces obtained from real-world fUML models.\nCompared to existing trace metamodels, the results show a signiﬁcant reduction in memory and disk consumption. Moreover,\nCTM offers a common structure with the aim to facilitate interoperability between existing trace analysis tools.",
        "keywords": [
            "Execution trace",
            "Compaction",
            "Model execution",
            "Trace metamodel"
        ],
        "authors": [
            "Fazilat Hojaji",
            "Bahman Zamani",
            "Abdelwahab Hamou-Lhadj",
            "Tanja Mayerhofer",
            "Erwan Bousse"
        ],
        "file_path": "data/sosym-all/s10270-019-00737-w.pdf"
    },
    {
        "title": "An algorithm for generating model-sensitive search plans for pattern matching on EMF models",
        "submission-date": "2012/10",
        "publication-date": "2013/09",
        "abstract": "In this paper, we propose a new model-sensitive search plan generation algorithm to speed up the process of graph pattern matching. This dynamic-programming-based algorithm, which is able to handle general n-ary constraints in an integrated manner, collects statistical data from the underlying EMF model and uses this information for optimization purposes. Additionally, the search plan generation algorithm itself and its runtime effects on the pattern matching engine have been evaluated by complexity analysis techniques and by quantitative performance measurements, respectively.",
        "keywords": [
            "Graph pattern matching",
            "Search plan generation algorithm",
            "Model-sensitive search plan"
        ],
        "authors": [
            "Gergely Varró",
            "Frederik Deckwerth",
            "Martin Wieber",
            "Andy Schürr"
        ],
        "file_path": "data/sosym-all/s10270-013-0372-2.pdf"
    },
    {
        "title": "Toward a well-founded theory for multi-level conceptual modeling",
        "submission-date": "2015/02",
        "publication-date": "2016/06",
        "abstract": "Multi-level conceptual modeling addresses the representation of subject domains dealing explicitly with multiple classiﬁcation levels. Despite the recent advances in multi-level modeling techniques, we believe that the literature in multi-level conceptual modeling would beneﬁt from a theory that: (1) formally characterizes the nature of classiﬁcation levels and (2) precisely deﬁnes the structural relations that may occur between elements of different classiﬁcation levels. This work aims to ﬁll this gap by proposing an axiomatic theory that can be considered a reference top-level ontology for types in multi-level conceptual modeling. The theory provides the modeler with basic concepts and patterns to articulate domains that require multiple levels of classification as well as to inform the development of well-founded languages for multi-level conceptual modeling. The whole theory is founded on a basic instantiation relation and characterizes the concepts of individuals and types, with types organized in levels related by instantiation. Further, it includes intra-level structural relations that are used to deﬁne expressive multi-level models and cross-level relations that allow us to account for and incorporate the different notions of power type in the literature.",
        "keywords": [
            "Multi-level modeling",
            "Conceptual modeling",
            "Power types",
            "Clabjects",
            "Ontology"
        ],
        "authors": [
            "Victorio A. Carvalho",
            "João Paulo A. Almeida"
        ],
        "file_path": "data/sosym-all/s10270-016-0538-9.pdf"
    },
    {
        "title": "Composing domain-speciﬁc physical models with general-purpose software modules in embedded control software",
        "submission-date": "2010/12",
        "publication-date": "2012/10",
        "abstract": "A considerable portion of software systems today are adopted in the embedded control domain. Embedded control software deals with controlling a physical sys-tem, and as such models of physical characteristics become part of the embedded control software. In current practices, usually general-purpose languages (GPL), such as C/C++ are used for embedded systems development. Although a GPL is suitable for expressing general-purpose computa-tion, it falls short in expressing the models of physical characteristics as desired. This reduces not only the read-ability of the code but also hampers reuse due to the lack of dedicated abstractions and composition operators. More-over, domain-speciﬁc static and dynamic checks may not be applied effectively. There exist domain-speciﬁc modeling languages (DSML) and tools to specify models of physical characteristics. Although they are commonly used for simu-lation and documentation of physical systems, they are often not used to implement embedded control software. This is due to the fact that these DSMLs are not suitable to express the general-purpose computation and they cannot be easily composed with other software modules that are implemented in GPL. This paper presents a novel approach to combine a DSML to model physical characteristics and a GPL to imple-ment general-purpose computation. The composition ﬁlters model is used to compose models speciﬁed in the DSML with modules speciﬁed in the GPL at the abstraction level of both languages. As such, this approach combines the bene-ﬁts of using a DSML to model physical characteristics with the freedom of a GPL to implement general-purpose compu-tation. The approach is illustrated using two industrial case studies from the printing systems domain.",
        "keywords": [
            "Domain speciﬁc languages",
            "Embedded systems",
            "Software composition",
            "Composition ﬁlters",
            "Aspect-oriented programming"
        ],
        "authors": [
            "Arjan de Roo",
            "Hasan Sözer",
            "Mehmet Ak¸sit"
        ],
        "file_path": "data/sosym-all/s10270-012-0283-7.pdf"
    },
    {
        "title": "Generating repairs for inconsistent models",
        "submission-date": "2021/04",
        "publication-date": "2022/04",
        "abstract": "There are many repair alternatives for resolving model inconsistencies, each involving one or more model changes. Enumer-\ning them all could overwhelm the developer because the number of possible repairs can grow exponentially. To address\nthis problem, this paper focuses on the immediate cause of an inconsistency. By focusing on the cause, we can generate a\nrepair tree with a subset of repair actions focusing on ﬁxing this cause. This strategy identiﬁes model elements that must be\nrepaired, as opposed to additional model elements that may or may not have to be repaired later. Furthermore, our approach\ncan provide an ownership-based ﬁlter for ﬁltering repairs that modify model elements not owned by a developer. This ﬁltering\ncan further reduce the repair possibilities, aiding the developer when choosing repairs to be performed. We evaluated our\napproach on 24 UML models and four Java systems, using 17 UML consistency rules and 14 Java consistency rules. The\nevaluation data contained 39,683 inconsistencies, showing our approach’s usability as the repair trees sizes ranged from ﬁve\nto nine on average per model. Also, these repair trees were generated in 0.3 seconds on average, showing our approach’s\nscalability. Based on the results, we discuss the correctness and minimalism with regard to the cause of the inconsistency.\nLastly, we evaluated the ﬁltering mechanism, showing that it is possible to further reduce the number of repairs generated by\nfocusing on ownership.",
        "keywords": [
            "Model-driven engineering",
            "Inconsistency repair",
            "Consistency checking",
            "Repair generation"
        ],
        "authors": [
            "Luciano Marchezan",
            "Roland Kretschmer",
            "Wesley K. G. Assunção",
            "Alexander Reder",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-022-00996-0.pdf"
    },
    {
        "title": "AI-driven streamlined modeling: experiences and lessons learned from multiple domains",
        "submission-date": "2021/03",
        "publication-date": "2022/02",
        "abstract": "Model-driven technologies (MD*), considered beneﬁcial through abstraction and automation, have not enjoyed widespread adoption in the industry. In keeping with the recent trends, using AI techniques might help the beneﬁts of MD* outweigh their costs. Although the modeling community has started using AI techniques, it is, in our opinion, quite limited and requires a change in perspective. We provide such a perspective through ﬁve industrial case studies where we use AI techniques in different modeling activities. We discuss our experiences and lessons learned, in some cases evolving purely modeling solutions with AI techniques, and in others considering the AI aids from the beginning. We believe that these case studies can help the researchers and practitioners make sense of various artifacts and data available to them and use applicable AI techniques to enhance suitable modeling activities.",
        "keywords": [
            "AI-driven",
            "Domain modeling",
            "Natural language processing",
            "Information extraction",
            "Knowledge graphs"
        ],
        "authors": [
            "Sagar Sunkle",
            "Krati Saxena",
            "Ashwini Patil",
            "Vinay Kulkarni"
        ],
        "file_path": "data/sosym-all/s10270-022-00982-6.pdf"
    },
    {
        "title": "Guest editorial to the theme section on multi-level modeling",
        "submission-date": "2022/02",
        "publication-date": "2022/03",
        "abstract": "Multi-level modeling (MLM) [5] represents a signiﬁcant extension to the traditional two-level object-oriented paradigm with the potential to improve upon the utility, reliability,andcomplexityofmodels.Differentfromconventional approaches, MLM approaches allow for an arbitrary number of classiﬁcation levels and introduce other concepts that foster expressiveness, reuse, and adaptability. A key aspect of the MLM paradigm is the use of entities (so-called clabjects) that are simultaneously types and instances [6], a feature which has consequences for conceptual modeling, for language engineering, and for the model-based development of software-intensive systems. MLM facilitates also deep instantiation [7], which, in contrast to shallow instantia-tion, allows model elements at a level to not only specify a scheme for elements at the next lower level but also to specify schemes for elements located at levels further down in the hierarchy. Different MLM approaches use different techniques to control and maintain this kind of instantiation. In Potency-based approaches [6,8], for instance, a natural number (potency) is assigned to each model element indicat-ing how many levels down in the hierarchy that element can be instantiated. Different variants of potency have been proposed to satisfy practical requirements, such as leap potency (facilitating jumps over levels) and depth (enforcing the last level at which an element may be instantiated). While MLM collects a group of modeling features under one umbrella term, it has also fertilized interesting discus-sions regarding some of its basic modeling constructs, the principles behind levels, and the mechanisms used for con-trolling deep instantiation. The MULTI workshop series is providing a forum for the MLM community to address the foundations of MLM approaches and support future developments through better modeling languages, tools, methods, and guidelines. The workshop calls have encouraged the presentation of case studies and tool demonstrations as well as novel concepts, implementation approaches, formalisms, controversial positions, and requirements for evaluation criteria. In the workshops, successful applications of MLM have been reported in domains such as software engineering, process modeling, enterprise modeling, industrial automation engineering, smart cities, and building information mod-eling. Furthermore, multiple common challenges, e.g., the bicycle challenge [2] and the process challenge [4], have been proposed as case studies in order to compare MLM approaches. Challenge participants were asked to employ an MLM approach to represent a domain that was initially described in natural language. These initiatives have brought together researchers and practitioners in the area of MLM, speciﬁcally interested in developing conceptual modeling, domain-speciﬁc lan-guages, database systems, and ontologies, to discuss syn-ergies, common problems, and solutions of the different engineering disciplines, tool building concerns and tech-niques, and importantly, the vision for the future of MLM. The Model-Driven Engineering (MDE) community has long embraced the need for MLM. Since the ﬁrst MULTI workshop on MLM in 2014, a series of 8 workshop edi-tions were co-located with the ﬂagship conference in MDE: the International Conference on Model Driven Engineering Languages and Systems (MODELS). Many articles relying on MLM principles have been published in the Software and Systems Modeling (SoSyM) journal, and in a special issue of the EMISA journal. Furthermore, a Dagstuhl Seminar [3] in 2017 was organized and an increasing number of tools and languages have been proposed over the last years [1] including DPF workbench, GModel, Melanee, MetaDepth, MultEcore, Nivel, OMME, ML2, and XModeller.",
        "keywords": [],
        "authors": [
            "Adrian Rutle",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-022-00987-1.pdf"
    },
    {
        "title": "Empirically evaluating modeling language ontologies: the Peira framework",
        "submission-date": "2023/06",
        "publication-date": "2024/04",
        "abstract": "Conceptual modeling plays a central role in planning, designing, developing and maintaining software-intensive systems. One of the goals of conceptual modeling is to enable clear communication among stakeholders involved in said activities. To achieve effective communication, conceptual models must be understood by different people in the same way. To support such shared understanding, conceptual modeling languages are deﬁned, which introduce rules and constraints on how individual models can be built and how they are to be understood. A key component of a modeling language is an ontology, i.e., a set of concepts that modelers must use to describe world phenomena. Once the concepts are chosen, a visual and/or textual vocabulary is adopted for representing the concepts. However, the choices both of the concepts and of the vocabulary used to represent them may affect the quality of the language under consideration: some choices may promote shared understanding better than other choices. To allow evaluation and comparison of alternative choices, we present Peira, a framework for empirically measuring the domain and comprehensibility appropriateness of conceptual modeling language ontologies. Given a language ontology to be evaluated, the framework is based on observing how prospective language users classify domain content under the concepts put forth by said ontology. A set of metrics is then used to analyze the observations and identify and characterize possible issues that the choice of concepts or the way they are represented may have. The metrics are abstract in that they can be operationalized into concrete implementations tailored to speciﬁc data collection instruments or study objectives. We evaluate the framework by applying it to compare an existing language against an artiﬁcial one that is manufactured to exhibit speciﬁc issues. We then test if the metrics indeed detect these issues. We ﬁnd that the framework does offer the expected indications, but that it also requires good understanding of the metrics prior to committing to interpretations of the observations.",
        "keywords": [
            "Conceptual modeling",
            "Modeling language quality",
            "Empirical methods",
            "Peira"
        ],
        "authors": [
            "Sotirios Liaskos",
            "Saba Zarbaf",
            "John Mylopoulos",
            "Shakil M. Khan"
        ],
        "file_path": "data/sosym-all/s10270-023-01147-9.pdf"
    },
    {
        "title": "A behavioral analysis and veriﬁcation approach to pattern-based design composition",
        "submission-date": "2002/09",
        "publication-date": "2004/04",
        "abstract": "Integrating software components to produce large-scale software systems is an eﬀective way to reuse experience and reduce cost. However, unexpected interactions among components when integrated into software systems are often the cause of failures. Discovering these composition errors early in the development process could lower the cost and eﬀort in ﬁxing them. This paper introduces a rigorous analysis approach to software design composition based on automated veriﬁcation techniques. We show how to represent, instantiate and integrate design components, and how to ﬁnd design composition errors using model checking techniques. We illustrate our approach with a Web-based hypermedia case study.",
        "keywords": [
            "Design patterns",
            "Software design",
            "Software components",
            "Software speciﬁcation",
            "Veriﬁcation",
            "Model checking",
            "Hypermedia systems"
        ],
        "authors": [
            "Jing Dong",
            "Paulo S.C. Alencar",
            "Donald D. Cowan"
        ],
        "file_path": "data/sosym-all/s10270-004-0056-z.pdf"
    },
    {
        "title": "FloWare: a model-driven approach fostering reuse and customisation in IoT applications modelling and development",
        "submission-date": "2021/10",
        "publication-date": "2022/08",
        "abstract": "The relevance of IoT-based solutions in everyday life is continuously increasing. The capability to sense the world, activate\ncomputation based on data gathered by sensors, and possibly produce reactions on the world itself results in an almost never-\nending identiﬁcation of novel IoT solutions and application scenarios. Nonetheless, IoT’s intrinsic nature, which includes a\nhigh degree of variability in used devices, data formats, resources, and communication protocols, complicates the design,\ndevelopment, reuse and customisation of IoT-based software systems. In addition, customers require personalised solutions\nstrongly based on their speciﬁc requirements. Reducing the complexity of building customised solutions and increasing the\nreusability of developed artefacts are among the topmost challenges for enterprises and IoT application developers. Upon\nthese challenges, we propose a model-driven approach organising the modelling and development of IoT applications in\ndifferent steps, handling the complexity in representing the IoT domain variability, and empowering the reusability of design\ndecisions and artefacts to simplify the derivation of customised IoT applications. Our proposal is named FloWare. It follows\nthe typical path of an MDE solution, providing modelling support through feature models to fully represent and handle the\npossible variability of devices in a speciﬁc IoT application domain. Once a speciﬁc conﬁguration has been selected, this\nwill be complemented with speciﬁc information about the deployment context to automatically derive fragments of the IoT\napplications, that will be successively combined by the developer within a low-code development environment. The approach\nis fully supported by a toolchain that has been released for public use.",
        "keywords": [
            "IoT application development",
            "Model-driven",
            "Variability modelling",
            "Low-code",
            "Customised applications",
            "Design artefact reusability"
        ],
        "authors": [
            "Flavio Corradini",
            "Arianna Fedeli",
            "Fabrizio Fornari",
            "Andrea Polini",
            "Barbara Re"
        ],
        "file_path": "data/sosym-all/s10270-022-01026-9.pdf"
    },
    {
        "title": "Web services-based tool-integration in the ETI platform",
        "submission-date": "2003/12",
        "publication-date": "2004/11",
        "abstract": "In this paper we present dETI, the next gen-eration of the Electronic Tool Integration (ETI) plat-form, an open platform for the interactive experimen-tation with and coordination of heterogeneous software tools via the internet. Our redesign, which is based on the experience gained while running the ETI platform since 1997, focusses on the tool integration process, which clearly marked the bottleneck for the wide acceptance of the ETI platform on the side of an important group of users: the tool providers. The new integration approach makes use of standard Web Services technology, which perfectly ﬁts in the overall ETI architecture.\nOur approach realizes a clear separation of concerns, which overcomes all the previously observed obstacles by (i) decoupling the integration tasks of the tool providers and the ETI team, (ii) pulling the ETI team out of the upgrading and maintenance loop and (iii) handing the upgrading and access control over to the tool providers. This guarantees the scalability in the number of tools available within ETI, and addresses the ﬂexibility con-cerns of the tool providers.",
        "keywords": [
            "Distributed coordination",
            "ETI",
            "Tool-experimentation platform",
            "Tool-integration",
            "Web services"
        ],
        "authors": [
            "Tiziana Margaria"
        ],
        "file_path": "data/sosym-all/s10270-004-0072-z.pdf"
    },
    {
        "title": "A generic approach to detect design patterns in model transformations using a string-matching algorithm",
        "submission-date": "2020/06",
        "publication-date": "2021/11",
        "abstract": "Maintaining software artifacts is a complex and time-consuming task. Like any other program, model transformations are subject to maintenance. In a maintenance process, much effort is dedicated to the comprehension of programs. To this end, several techniques are used, such as feature location and design pattern detection. In the particular case of model transformations, detecting design patterns contributes to a better comprehension as they carry valuable information on the transformation structure. In this paper, we propose a generic approach to detect, semi-automatically, design patterns and their variations in model transformations. Our approach encodes both design patterns and transformations as strings and use a string-matching algorithm for the detection. The approach is able to detect complete and partial implementations of design patterns in transformations, which is useful to refactoring and improving model transformations.",
        "keywords": [
            "Design pattern",
            "Model transformation",
            "Pattern detection",
            "String matching",
            "Bit-vector",
            "Model-driven engineering"
        ],
        "authors": [
            "Chihab eddine Mokaddem",
            "Houari Sahraoui",
            "Eugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-021-00936-4.pdf"
    },
    {
        "title": "The OsMoSys approach to multi-formalism modeling of systems",
        "submission-date": "2003/11",
        "publication-date": "2003/11",
        "abstract": "Analysis and simulation of complex systems are facilitated by the availability of appropriate modeling formalisms and tools. In many cases, no single analysis and modeling method can successfully cope with all aspects of a complex system: a multi-formalism multi-solution approach is very appealing, since it oﬀers the possibility of applying the most suitable formalisms and solution techniques to model and analyze diﬀerent components or aspects of a system. Another important feature that a successfull modeling approach should include is the possibility of reusing (sub)models: by composing parameterized submodels and then instantiating the parameters, complete models of diﬀerent scenarios can be obtained and analyzed.\nThis paper introduces an innovative approach to multi-formalism modeling of systems that is part of the OsMoSys (Object-based multi-formaliSm MOdeling of SYStems) framework. OsMoSys uses the proposed modeling approach to build multi-formalism models, and workﬂow management to achieve multi-solution. Our modeling approach is based on meta-modeling, allowing to easily deﬁne and integrate diﬀerent formalisms, and on some concepts from object orientation. Its main objectives are the interoperability of diﬀerent formalisms and the deﬁnition of mechanisms to guarantee the ﬂexibility and the scalability of the modeling framework.",
        "keywords": [
            "Multi-formalism modeling",
            "Meta-languages",
            "Object orientation",
            "Compositionality"
        ],
        "authors": [
            "V. Vittorini",
            "M. Iacono",
            "N. Mazzocca",
            "G. Franceschinis"
        ],
        "file_path": "data/sosym-all/s10270-003-0039-5.pdf"
    },
    {
        "title": "Enforcing ﬁne-grained access control for secure collaborative modelling using bidirectional transformations",
        "submission-date": "2017/03",
        "publication-date": "2017/11",
        "abstract": "Large-scale model-driven system engineering projects are carried out collaboratively. Engineering artefacts stored in model repositories are developed in either ofﬂine (checkout–modify–commit) or online (GoogleDoc-style) scenarios. Complex systems frequently integrate models and components developed by different teams, vendors and suppliers. Thus, conﬁdentiality and integrity of design artefacts need to be protected in accordance with access control policies. We propose a secure collaborative modelling approach where ﬁne-grained access control for models is strictly enforced by bidirectional model transformations. Collaborators obtain ﬁltered local copies of the model containing only those model elements which they are allowed to read; write access control policies are checked on the server upon submitting model changes. We present a formal collaboration schema which provenly guarantees certain correctness constraints, and its adaption to online scenarios with on-the-ﬂy change propagation and the integration into existing version control systems to support ofﬂine scenarios. The approach is illustrated, and its scalability is evaluated using a case study of the MONDO EU project.",
        "keywords": [
            "Collaborative modelling",
            "Secured views",
            "Access control",
            "Online collaboration",
            "Ofﬂine collaboration",
            "Bidirectional model transformation"
        ],
        "authors": [
            "Csaba Debreceni",
            "Gábor Bergmann",
            "István Ráth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-017-0631-8.pdf"
    },
    {
        "title": "Going beyond templates: composition and evolution in nested OSTRICH",
        "submission-date": "2023/05",
        "publication-date": "2024/05",
        "abstract": "Low-code frameworks strive to simplify and speed up application development. An essential mechanism to achieve these goals is to have native support for the safe reuse and usage of parameterized coarse-grain components, providing developers with strong guardrails and a rich software-building experience. OSTRICH—a rich template language for the OutSystems platform—was designed to simplify the use and creation of such components. Thus, the application developer can quickly reuse and assemble sophisticated and thoroughly tested application blocks. However, without a built-in composition and evolution mechanism, OSTRICH templates are still hard to create and maintain. This sometimes requires the repetition of code across different templates and creates a conflict between the customizations of the instantiated application models, and the update and reapplication of a template definition. This paper presents a principled mechanism for using abstraction in the creation of templates and simultaneously supporting the evolution of OSTRICH templates in applications after use. First, we introduce a template composition mechanism, its typing discipline, and its instantiation algorithm for model-driven low-code development environments. We start by extending OSTRICH to support nested templates and allow the instantiation (hatching) of templates in the definition of other templates. Nesting promotes a significant increase in code reuse potential, leading to a safer evolution of applications. We then introduce the support for customizable template instances, which allows one to evolve templates’ code and then update a template instance without losing customizations performed in the generated code. The present definition seamlessly extends the existing OutSystems metamodel with template constructs expressed by model annotations that maintain backward compatibility with the existing language toolchain. We present the metamodel, a set of annotations to support the extensions, and the corresponding validation and instantiation algorithms. In particular, we introduce a type-based validation procedure for abstractions that ensures that using templates always produces valid models. This work also extends prior developments on Nested OSTRICH with the support for safe customizations of instantiated code. We validate Nested OSTRICH using the OSTRICH benchmark by identifying the degree of reusability that can be reached in the existing sample of real templates and template uses. Our prototype is an extension of the OutSystems IDE that allows the annotation of models and their use to produce new models. We also analyze which existing OutSystems sample screen templates can be improved by using and sharing nested templates.",
        "keywords": [
            "Metaprogramming",
            "Low-code models",
            "Metamodel",
            "Templating",
            "Typechecking templates",
            "Model reuse"
        ],
        "authors": [
            "João Costa Seco",
            "Hugo Lourenço",
            "Joana Parreira",
            "Carla Ferreira"
        ],
        "file_path": "data/sosym-all/s10270-024-01178-w.pdf"
    },
    {
        "title": "Automated synthesis of local time requirement for service composition",
        "submission-date": "2018/02",
        "publication-date": "2020/03",
        "abstract": "Service composition aims at achieving a business goal by composing existing service-based applications or components. The response time of a service is crucial, especially in time-critical business environments, which is often stated as a clause in service-level agreements between service providers and service users. To meet the guaranteed response time requirement of a composite service, it is important to select a feasible set of component services such that their response time will collectively satisfy the response time requirement of the composite service. In this work, we use the BPEL modeling language that aims at specifying Web services. We extend it with timing parameters and equip it with a formal semantics. Then, we propose a fully automated approach to synthesize the response time requirement of component services modeled using BPEL, in the form of a constraint on the local response times. The synthesized requirement will guarantee the satisfaction of the global response time requirement, statically or dynamically. We implemented our work into a tool, Selamat and performed several experiments to evaluate the validity of our approach.",
        "keywords": [
            "Web service composition",
            "Parameter synthesis",
            "Modeling Web services",
            "Formal semantics",
            "BPEL",
            "Parametric model checking"
        ],
        "authors": [
            "Étienne André",
            "Tian Huat Tan",
            "Manman Chen",
            "Shuang Liu",
            "Jun Sun",
            "Yang Liu",
            "Jin Song Dong"
        ],
        "file_path": "data/sosym-all/s10270-020-00787-5.pdf"
    },
    {
        "title": "Validation and veriﬁcation in domain-speciﬁc modeling method engineering: an integrated life-cycle view",
        "submission-date": "2022/03",
        "publication-date": "2022/10",
        "abstract": "Enterprise models have the potential to constitute a valuable asset for organizations, e.g., in terms of enabling a variety of\nanalyses or by fostering cross-organizational communication. Therefore, while designing an enterprise modeling method one\nneeds to ensure that created enterprise models are of good quality in terms of: (1) syntactic validity, which entails that a\nmodel adheres to syntactic rules encoded in the underlying modeling language, (2) semantic validity, i.e., that the model\nshould make sense in its context of use, and (3) pragmatic validity, i.e., that the model should effectively and efﬁciently serve\nthe intended purpose. To ensure these three validity types, veriﬁcation and validation (V&V) techniques need to be exploited\nwhile designing the enterprise modeling method, e.g., to check created enterprise models against syntactic rules, or to ensure\nintra- and inter-model consistency. This paper targets the systematic embedding of V&V techniques into the engineering\nof (enterprise) domain-speciﬁc modeling methods (DSMMs). Speciﬁcally, after identifying and analyzing existing DSMM\nengineering approaches, we synthesize their elements (such as typical phases and steps) and enrich them with V&V techniques.\nThis paper is an extension of our previous work and additionally contributes (1) a systematic analysis of a wider set of existing\napproaches to DSMM engineering, (2) an extended background that covers information on models, modeling languages and\nmodeling methods, (3) additional details regarding selected validation and veriﬁcation techniques for each phase, and ﬁnally\n(4) a road-map encompassing desiderata for further advances in V&V in domain-speciﬁc modeling method engineering, from\nthe perspectives of practice, research and education.",
        "keywords": [
            "Domain-speciﬁc modeling method engineering",
            "Validation and veriﬁcation",
            "Enterprise modeling"
        ],
        "authors": [
            "Qin Ma",
            "Monika Kaczmarek-Heß",
            "Sybren de Kinderen"
        ],
        "file_path": "data/sosym-all/s10270-022-01056-3.pdf"
    },
    {
        "title": "Protocol modelling: A modelling approach that supports reusable behavioural abstractions",
        "submission-date": "2005/02",
        "publication-date": "2005/09",
        "abstract": "We describe a behavioural modelling approach based on the concept of a “Protocol Machine”, a machine whose behaviour is governed by rules that determine whether it accepts or refuses events that are presented to it. We show how these machines can be composed in the manner of mixins to model object behaviour and show how the approach provides a basis for deﬁning reusable ﬁne-grained behavioural abstractions. We suggest that this approach provides better encapsulation of object behaviour than traditional object modelling techniques when modelling transactional business systems. We relate the approach to work going on in model driven approaches, speciﬁcally the Model Driven Architecture initiative sponsored by the Object Management Group.",
        "keywords": [
            "Behavioural modelling",
            "Reuse",
            "Protocols",
            "State machines",
            "Mixins",
            "Executable modelling"
        ],
        "authors": [
            "Ashley McNeile",
            "Nicholas Simons"
        ],
        "file_path": "data/sosym-all/s10270-005-0100-7.pdf"
    },
    {
        "title": "Model-driven optimal resource scaling in cloud",
        "submission-date": "2015/07",
        "publication-date": "2017/02",
        "abstract": "Cloud computing offers the ﬂexibility to dynamically size the infrastructure in response to changes in workload demand. While both horizontal scaling and vertical scaling of infrastructure are supported by major cloud providers, these scaling options differ signiﬁcantly in terms of their cost, provisioning time, and their impact on workload performance. Importantly, the efﬁcacy of horizontal and vertical scaling critically depends on the workload characteristics, such as the workload’s parallelizability and its core scalability. In today’s cloud systems, the scaling decision is left to the users, requiring them to fully understand the trade-offs associated with the different scaling options. In thispaper,wepresentoursolutionforoptimizingtheresource scaling of cloud deployments via implementation in Open-Stack. The key component of our solution is the modeling engine that characterizes the workload and then quantitatively evaluates different scaling options for that workload. Our modeling engine leverages Amdahl’s Law to model service timescaling in scale-up environments and queueing-theoretic concepts to model performance scaling in scale-out environments.WefurtheremployKalmanﬁlteringtoaccount for inaccuracies in the model-based methodology and to dynamically track changes in the workload and cloud environment.",
        "keywords": [
            "Autoscaling",
            "Modeling",
            "Scale-up",
            "Scale-out",
            "Cost",
            "Optimal",
            "Experimentation",
            "Implementation"
        ],
        "authors": [
            "Anshul Gandhi",
            "Parijat Dube",
            "Alexei Karve",
            "Andrzej Kochut",
            "Li Zhang"
        ],
        "file_path": "data/sosym-all/s10270-017-0584-y.pdf"
    },
    {
        "title": "Contrasting dedicated model transformation languages versus general purpose languages: a historical perspective on ATL versus Java",
        "submission-date": "2021/06",
        "publication-date": "2021/11",
        "abstract": "Model transformations are among the key concepts of model-driven engineering (MDE), and dedicated model transformation languages (MTLs) emerged with the popularity of the MDE pssaradigm about 15 to 20 years ago. MTLs claim to increase the ease of development of model transformations by abstracting from recurring transformation aspects and hiding complex semantics behind a simple and intuitive syntax. Nonetheless, MTLs are rarely adopted in practice, there is still no empirical evidence for the claim of easier development, and the argument of abstraction deserves a fresh look in the light of modern general purpose languages (GPLs) which have undergone a signiﬁcant evolution in the last two decades. In this paper, we report about a study in which we compare the complexity and size of model transformations written in three different languages, namely (i) the Atlas Transformation Language (ATL), (ii) Java SE5 (2004–2009), and (iii) Java SE14 (2020); the Java transformations are derived from an ATL speciﬁcation using a translation schema we developed for our study. In a nutshell, we found that some of the new features in Java SE14 compared to Java SE5 help to signiﬁcantly reduce the complexity of transformations written in Java by as much as 45%. At the same time, however, the relative amount of complexity that stems from aspects that ATL can hide from the developer, which is about 40% of the total complexity, stays about the same. Furthermore we discovered that while transformation code in Java SE14 requires up to 25% less lines of code, the number of words written in both versions stays about the same. And while the written number of words stays about the same their distribution throughout the code changes signiﬁcantly. Based on these results, we discuss the concrete advancements in newer Java versions. We also discuss to which extent new language advancements justify writing transformations in a general purpose language rather than a dedicated transformation language. We further indicate potential avenues for future research on the comparison of MTLs and GPLs in a model transformation context.",
        "keywords": [
            "ATL",
            "Java",
            "Model transformations",
            "Model transformation language",
            "General purpose language",
            "Comparison",
            "MTL versus GPL",
            "Historical perspective",
            "Complexity measure",
            "Size measure"
        ],
        "authors": [
            "Stefan Höppner",
            "Timo Kehrer",
            "Matthias Tichy"
        ],
        "file_path": "data/sosym-all/s10270-021-00937-3.pdf"
    },
    {
        "title": "Evaluating user acceptance of knowledge-intensive business process modeling languages",
        "submission-date": "2022/02",
        "publication-date": "2023/08",
        "abstract": "CaseManagementhasbeenevolvingtosupportknowledge-intensivebusinessprocessmanagement,resultingindifferentmod-eling languages, e.g., Declare, Dynamic Condition Response (DCR), and Case Management Model and Notation (CMMN). A language will die if users do not accept and use it in practice—similar to extinct human languages. Thus, evaluating how users perceive languages is important to improve them. Although some studies have investigated how the process designers perceived Declare and DCR, there is a lack of research on how they perceive CMMN—especially in comparison with other languages. Therefore, this paper investigates and compares how process designers perceive these languages based on the Technology Acceptance Model. The paper includes two studies conducted in 2020 and 2022, both performed by educating participants through a course, with feedback on their assignments, to reduce biases. The perceptions are collected through questionnaires before and after feedback on the ﬁnal practice. Results show that the perceptions change is insigniﬁcant after feedback due to the participants being well-trained. The reliability of responses was tested using Cronbach’s alpha. The results of the ﬁrst study show that both DCR and CMMN were perceived as having acceptable usefulness and ease of use, but CMMN was perceived as signiﬁcantly better than DCR in terms of ease of use. The results of the second study show that only DCR was perceived signiﬁcantly better than Declare in terms of usefulness. The participants’ feedback shows potential areas for improvement in languages and tool support to enhance perceived usefulness and ease of use.",
        "keywords": [
            "Business process modeling",
            "Knowledge-intensive",
            "Case management",
            "CMMN",
            "DCR",
            "Declare"
        ],
        "authors": [
            "Amin Jalali"
        ],
        "file_path": "data/sosym-all/s10270-023-01120-6.pdf"
    },
    {
        "title": "TURTLE-P: a UML proﬁle for the formal validation of critical and distributed systems",
        "submission-date": "2004/01",
        "publication-date": "2006/07",
        "abstract": "The timed UML and RT-LOTOS environ-\nment, or TURTLE for short, extends UML class and\nactivity diagrams with composition and temporal oper-\nators. TURTLE is a real-time UML proﬁle with a for-\nmal semantics expressed in RT-LOTOS. Further, it is\nsupported by a formal validation toolkit. This paper\nintroduces TURTLE-P, an extended proﬁle no longer\nrestricted to the abstract modeling of distributed sys-\ntems. Indeed, TURTLE-P addresses the concrete\ndescriptions of communication architectures, including\nquality of service parameters (delay, jitter, etc.). This\nnew proﬁle enables co-design of hardware and soft-\nware components with extended UML component and\ndeployment diagrams. Properties of these diagrams can\nbe evaluated and/or validated thanks to the formal\nsemantics given in RT-LOTOS. The application of TUR-\nTLE-P is illustrated with a telecommunication satellite\nsystem.",
        "keywords": [],
        "authors": [
            "Ludovic Apvrille",
            "Pierre de Saqui-Sannes",
            "Ferhat Khendek"
        ],
        "file_path": "data/sosym-all/s10270-006-0029-5.pdf"
    },
    {
        "title": "Visual modeling of RESTful conversations with RESTalk",
        "submission-date": "2016/01",
        "publication-date": "2016/05",
        "abstract": "Abstract The cost savings introduced by Web services through code reuse and integration opportunities have motivated many businesses to develop Web APIs, with ever increasing numbers opting for the REST architectural style. RESTful Web APIs are decomposed in multiple resources, which the client can manipulate through HTTP interactions with well-deﬁned semantics. Getting the resource in the desired state might require multiple client–server interactions, what we deﬁne as a RESTful conversation. RESTful conversations are dynamically guided by hypermedia controls, such as links. Thus, when deciding whether and how to use a given RESTful service, the client might not be aware of all the interactions which are necessary to achieve its goal. This is because existing documentation of RESTful APIs describes the static structure of the interface, exposing low-level HTTP details, while little attention has been paid to conceptual, high-level, modeling of the dynamics of REST-ful conversations. Low-level HTTP details can be abstracted from during the design phase of the API, or when deciding which API to use. We argue that, in these situations, visual models of the required client–server interactions might increase developers’ efﬁciency and facilitate their understanding. Thus, to capture all possible interaction sequences in a given RESTful conversation, we propose RESTalk, an extension to the BPMN Choreography diagrams to render them more concise and yet sufficiently expressive in the specific REST domain. We also report on the results obtained from an exploratory survey we have conducted to assess the maturity of the field for a domain-specific language and to obtain feedback for future improvements of RESTalk.",
        "keywords": [
            "RESTful Web services",
            "Conversations",
            "BPMN Choreography",
            "Modeling notation extension",
            "Exploratory study",
            "Domain-specific language",
            "Questionnaire",
            "RESTalk"
        ],
        "authors": [
            "Ana Ivanchikj",
            "Cesare Pautasso",
            "Silvia Schreier"
        ],
        "file_path": "data/sosym-all/s10270-016-0532-2.pdf"
    },
    {
        "title": "Dash: declarative behavioural modelling in Alloy with control state hierarchy",
        "submission-date": "2021/06",
        "publication-date": "2022/08",
        "abstract": "We present Dash, an extension to the Alloy language to model dynamic behaviour using the labelled control state hierarchy of Statecharts. From Statecharts, Dash borrows the concepts to specify hierarchy, concurrency, and communication for describing behaviour in a compositional manner. From Alloy, Dash uses the expressiveness of relational logic and set theory to abstractly and declaratively describe structures, data, and operations. We justify our semantic design decisions for Dash, which carefully mix the usual semantic understanding of control state hierarchy with the declarative perspective. We describe and implement the semantics of a Dash model by translating it to Alloy, taking advantage of Alloy language features. We evaluate our Dash translation and perform model checking analysis, enabled by our translation, in the Alloy Analyzer using several case studies. Dash provides modellers with a language that seamlessly combines the semantics of control-modelling paradigms with Alloy’s existing strengths in modelling data and operations abstractly.",
        "keywords": [
            "Declarative modelling",
            "Transition systems",
            "Alloy",
            "Statecharts"
        ],
        "authors": [
            "Jose Serna",
            "Nancy A. Day",
            "Shahram Esmaeilsabzali"
        ],
        "file_path": "data/sosym-all/s10270-022-01012-1.pdf"
    },
    {
        "title": "Model interoperability in building information modelling",
        "submission-date": "2009/11",
        "publication-date": "2010/10",
        "abstract": "The exchange of design models in the design and construction industry is evolving away from 2-dimensional computer-aided design (CAD) and paper towards semantically-rich 3-dimensional digital models. This approach, known as Building Information Modelling (BIM), is anticipated to become the primary means of information exchange between the various parties involved in construction projects. From a technical perspective, the domain represents an interesting study in model-based interoperability, since the models are large and complex, and the industry is one in which collaboration is a vital part of business. In this paper, we present our experiences with issues of model-based interoperability in exchanging building information models between various tools, and in implementing tools which consume BIM models, particularly using the industry standard IFC data modelling format. We report on the successes and challenges in these endeavours, as the industry endeavours to move further towards fully digitised information exchange.",
        "keywords": [
            "Building Information Modelling",
            "Interoperability"
        ],
        "authors": [
            "Jim Steel",
            "Robin Drogemuller",
            "Bianca Toth"
        ],
        "file_path": "data/sosym-all/s10270-010-0178-4.pdf"
    },
    {
        "title": "MemoRec: a recommender system for assisting modelers in specifying metamodels",
        "submission-date": "2020/11",
        "publication-date": "2022/03",
        "abstract": "Model-driven engineering has been widely applied in software development, aiming to facilitate the coordination among various stakeholders. Such a methodology allows for a more efﬁcient and effective development process. Nevertheless, modeling is a strenuous activity that requires proper knowledge of components, attributes, and logic to reach the level of abstraction required by the application domain. In particular, metamodels play an important role in several paradigms, and specifying wrong entities or attributes in metamodels can negatively impact on the quality of the produced artifacts as well as other elements of the whole process. During the metamodeling phase, modelers can beneﬁt from assistance to avoid mistakes, e.g., getting recommendations like metaclasses and structural features relevant to the metamodel being deﬁned. However, suitable machinery is needed to mine data from repositories of existing modeling artifacts and compute recommendations. In this work, we propose MemoRec, a novel approach that makes use of a collaborative ﬁltering strategy to recommend valuable entities related to the metamodel under construction. Our approach can provide suggestions related to both metaclasses and structured features that should be added in the metamodel under deﬁnition. We assess the quality of the work with respect to different metrics, i.e., success rate, precision, and recall. The results demonstrate that MemoRec is capable of suggesting relevant items given a partial metamodel and supporting modelers in their task.",
        "keywords": [
            "Model-driven engineering",
            "Recommender systems",
            "Collaborative filtering techniques"
        ],
        "authors": [
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Claudio Di Sipio",
            "Phuong T. Nguyen",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-022-00994-2.pdf"
    },
    {
        "title": "The quest for runware: on compositional, executable and intuitive models",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "We believe that future models of complex soft-ware and systems will combine the crucial traits of intuitive-ness, compositionality, and executability. The importance of each of these to modeling is already well recognized, but our vision suggests a far more powerful synergy between them. First, models will be aligned with cognitive processes used by humans to think about system behavior and will be under-stood, and perhaps creatable, by almost anyone. Second, one will be able to build models incrementally, adding to, reﬁn-ing or sculpting away already-speciﬁed behaviors without changing most existing parts of the model. Third, there will be powerful ways to execute such intuitive and compositional models, in whole or in part, at any stage of the development. The presence of these three traits in a single artifact will blur the boundaries between natural-language requirements, for-mal models, and actual software, bringing in its wake a major advance in the way systems are built, and in their cost and quality. We propose the term runware1 to refer to this kind of higher level artifact.",
        "keywords": [
            "Executable speciﬁcations",
            "Model-driven engineering",
            "Behavioral programming",
            "Computational methods"
        ],
        "authors": [
            "David Harel",
            "Assaf Marron"
        ],
        "file_path": "data/sosym-all/s10270-012-0258-8.pdf"
    },
    {
        "title": "T-Core: a framework for custom-built model transformation engines",
        "submission-date": "2012/11",
        "publication-date": "2013/08",
        "abstract": "A large number of model transformation lan-guages and tools have emerged since the early 2000s. A transformation engineer is thus left with too many choices for the language he use to perform a speciﬁc transformation task. Furthermore, it is currently not possible to combine or reuse transformations implemented in different languages. We therefore propose T-Core, a framework where primitive transformation constructs can be combined to deﬁne and encapsulate reusable model transformation idioms. In this context, the transformation engineer is free to use existing transformation building blocks from an extensible library or deﬁne his own transformation units. The proposed primi-tive transformation operators are the result of deconstructing different existing transformation languages. Reconstructing these languages offers a common basis to compare their expressiveness, provides a framework for inter-operating them, and allows the transformation engineer to design trans-formations with the most appropriate constructs for the task at hand.",
        "keywords": [
            "Model transformation",
            "Domain-speciﬁc model transformation",
            "Transformation library",
            "Reengineering"
        ],
        "authors": [
            "Eugene Syriani",
            "Hans Vangheluwe",
            "Brian LaShomb"
        ],
        "file_path": "data/sosym-all/s10270-013-0370-4.pdf"
    },
    {
        "title": "Privacy-enhanced BPMN: enabling data privacy analysis in business process models",
        "submission-date": "2018/02",
        "publication-date": "2019/01",
        "abstract": "Privacy-enhancing technologies play an important role in preventing the disclosure of private data as information is transmitted and processed. Although business process model and notation (BPMN) is well suited for expressing stakeholder collaboration and business processes support by technical solutions, little is done to depict and analyze the ﬂow of private information and its technical safeguards as it is disclosed to process participants. This gap motivates the development of privacy-enhanced BPMN (PE-BPMN)—a BPMN language for capturing PET-related activities in order to study the ﬂow of private information and ease the communication of privacy concerns and requirements among stakeholders. We demonstrate its feasibility in a mobile app scenario and present techniques to analyze information disclosures identiﬁed by models enriched with PE-BPMN.",
        "keywords": [
            "Privacy",
            "Business process model and notation (BPMN)",
            "Privacy-enhancing technology (PET)",
            "Information disclosure"
        ],
        "authors": [
            "Pille Pullonen",
            "Jake Tom",
            "Raimundas Matuleviˇcius",
            "Aivo Toots"
        ],
        "file_path": "data/sosym-all/s10270-019-00718-z.pdf"
    },
    {
        "title": "A model-driven traceability framework for software product lines",
        "submission-date": "2009/01",
        "publication-date": "2009/06",
        "abstract": "Software product line (SPL) engineering is a\nrecent approach to software development where a set of soft-\nwareproducts arederivedfor awell deﬁnedtarget application\ndomain, from a common set of core assets using analogous\nmeans of production (for instance, through Model Driven\nEngineering). Therefore, such family of products are built\nfrom reuse, instead of developed individually from scratch.\nSPL promise to lower the costs of development, increase the\nquality of software, give clients more ﬂexibility and reduce\ntime to market. These beneﬁts come with a set of new prob-\nlems and turn some older problems possibly more complex.\nOne of these problems is traceability management. In the\nEuropean AMPLE project we are creating a common trace-\nability framework across the various activities of the SPL\ndevelopment. We identiﬁed four orthogonal traceability\ndimensions in SPL development, one of which is an exten-\nsion of what is often considered as “traceability of variabil-\nity”. This constitutes one of the two contributions of this\npaper. The second contribution is the speciﬁcation of a meta-\nmodel for a repository of traceability links in the context\nof SPL and the implementation of a respective traceabil-\nity framework. This framework enables fundamental trace-\nability management operations, such as trace import and\nCommunicated by Prof. Richard Paige.\nexport, modiﬁcation, query and visualization. The power of\nour framework is highlighted with an example scenario.",
        "keywords": [
            "Traceability",
            "Software product line",
            "Model driven engineering"
        ],
        "authors": [
            "Nicolas Anquetil",
            "Uirá Kulesza",
            "Ralf Mitschke",
            "Ana Moreira",
            "Jean-Claude Royer",
            "Andreas Rummler",
            "André Sousa"
        ],
        "file_path": "data/sosym-all/s10270-009-0120-9.pdf"
    },
    {
        "title": "Theme section on model-driven requirements engineering",
        "submission-date": "2022/10",
        "publication-date": "2022/10",
        "abstract": "To date, the Model-Driven Requirements Engineering (MoDRE) workshop series has produced 12 workshops from 2011 to 2022, all of which were co-located with the IEEE International Requirements Engineering (RE) Conference. The MoDRE workshop series provides a forum to discuss the challenges of Model-Driven Development (MDD) for RE. Building on the interest of MDD for design and implementation, RE may benefit from MDD techniques when properly balancing flexibility for capturing varied user needs with formal rigidity required for model analysis and transformations as well as high-level abstraction with information richness. MoDRE seeks to explore those areas of RE that have not yet been formalized sufficiently to be incorporated into an MDD environment as well as how RE models can benefit from emerging topics in the model-driven community. Over the years, this included topics such as models@runtime, reuse, flexible and collaborative modeling, modeling for DevOps and iterative development, sustainability, human values and ethics, equality and fairness, adaptive systems, and artificial intelligence-enabled modeling and applications.",
        "keywords": [],
        "authors": [
            "Ana Moreira",
            "Gunter Mussbacher",
            "João Araújo",
            "Pablo Sánchez"
        ],
        "file_path": "data/sosym-all/s10270-022-01055-4.pdf"
    },
    {
        "title": "Code generation for a family of executable modelling notations",
        "submission-date": "2009/06",
        "publication-date": "2010/10",
        "abstract": "We are investigating semantically configurable model-driven engineering (MDE). The goal of this work is a modelling environment that supports flexible, configurable modelling notations, in which specifiers can configure the semantics of notations to suit their needs and yet still have access to the types of analysis tools and code generators normally associated with MDE. In this paper, we describe semantically configurable code generation for a family of behavioural modelling notations. The family includes variants of statecharts, process algebras, Petri Nets, and SDL88. The semantics of this family is defined using template semantics, which is a parameterized structured operational semantics in which parameters represent semantic variation points. A specific notation is derived by instantiating the family’s template semantics with parameter values that specify semantic choices. We have developed a code-generator generator (CGG) that creates a suitable Java code generator for a subset of derivable modelling notations. Our prototype CGG supports 26 semantics parameters, 89 parameter values, and 7 composition operators. As a result, we are able to produce code generators for a sizable family of modelling notations, though at present the performance of our generated code is about an order of magnitude slower than that produced by commercial-grade generators.",
        "keywords": [
            "Model-driven engineering",
            "Code generation"
        ],
        "authors": [
            "Adam Prout",
            "Joanne M. Atlee",
            "Nancy A. Day",
            "Pourya Shaker"
        ],
        "file_path": "data/sosym-all/s10270-010-0176-6.pdf"
    },
    {
        "title": "The importance of ﬂow in software development",
        "submission-date": "2017/09",
        "publication-date": "2017/09",
        "abstract": "From social and psychological theories and studies [1], we know that there exists a mental state called “ﬂow” that allows individuals to concentrate deeply on a speciﬁc task without noticing the surrounding environment or the time, while remaining fully aware of the current work that they are doing.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-017-0621-x.pdf"
    },
    {
        "title": "A structured operational semantics for UML-statecharts",
        "submission-date": "2002/02",
        "publication-date": "2002/12",
        "abstract": "The Uniﬁed Modeling Language (UML) has gained wide acceptance in very short time because of its variety of well-known and intuitive graphical notations. However, this comes at the price of an unprecise and incomplete semantics deﬁnition. This insuﬃciency concerns single UML diagram notations on their own as well as their integration. In this paper, we focus on the notation of UML-statecharts. Starting with a precise textual syntax deﬁnition, we develop a precise structured operational semantics (SOS) for UML-statecharts. Besides the support of interlevel transitions and in contrast to related work, our semantics deﬁnition supports characteristic UML-statechart features like the history mechanism as well as entry and exit actions.",
        "keywords": [
            "Statecharts",
            "UML",
            "UML-statecharts",
            "Formal semantics",
            "Structured operational semantics (SOS)",
            "Labeled transition systems"
        ],
        "authors": [
            "Michael von der Beeck"
        ],
        "file_path": "data/sosym-all/s10270-002-0012-8.pdf"
    },
    {
        "title": "A methodology for the selection of requirements engineering techniques",
        "submission-date": "2004/11",
        "publication-date": "2007/06",
        "abstract": "The complexity of software projects as well as the multidisciplinary nature of requirements engineering (RE) requires developers to carefully select RE techniques andpracticesduringsoftwaredevelopment.Nevertheless,the selection of RE techniques is usually based on personal preference or existing company practice rather than on characteristics of the project at hand. Furthermore, there is a lack of guidance on which techniques are suitable for a certain project context. So far, only a limited amount of research has been done regarding the selection of RE techniques based on the attributes of the project under development. The few approaches that currently exist for the selection of RE techniques provide only little guidance for the actual selection process. We believe that the evaluation of RE techniques in the context of an application domain and a specific project is of great importance. This paper describes a Methodology for Requirements Engineering Techniques Selection (MRETS) as an approach that helps requirements engineers select suitable RE techniques for the project at hand. The MRETS has three aspects: Firstly, it aids requirements engineers in establishing a link between the attributes of the project and the attributes of RE techniques. Secondly, based on the evaluation schema proposed in our research, MRETS provides an opportunity to analyze RE techniques in detail using clustering. Thirdly, the objective function used in our approach provides an effective decision support mechanism for the selection of RE techniques. This paper makes contributions to RE techniques analysis, the application of RE techniques in practice, RE research, and software engineering in general. The application of the proposed methodology to an industrial project provides preliminary information on the effectiveness of MRETS for the selection of RE techniques.",
        "keywords": [
            "Requirements engineering",
            "Technique evaluation",
            "Decision support",
            "Clustering"
        ],
        "authors": [
            "Li Jiang",
            "Armin Eberlein",
            "Behrouz H. Far",
            "Majid Mousavi"
        ],
        "file_path": "data/sosym-all/s10270-007-0055-y.pdf"
    },
    {
        "title": "A modeling-based approach for dependability analysis of a constellation of satellites",
        "submission-date": "2024/01",
        "publication-date": "2024/07",
        "abstract": "Satellite constellations play critical roles across various sectors, encompassing communication, Earth observation and space exploration. Ensuring the dependable operation of these constellations is of utmost importance. This paper introduces a dependability modeling approach using stochastic Petri nets to analyze satellite constellations. The primary focus is on improving operational efﬁciency through the assessment of availability, reliability and maintainability. The approach helps satellite designers make informed decisions when selecting constellation conﬁgurations by assessing various dependability metrics. Using a global navigation satellite system as a case study, we conduct extensive numerical experiments to evaluate the feasibility of our approach. The results demonstrate quantitatively the signiﬁcant impact of redundant components on both reliability and availability. They also illustrate how utilizing satellites in repair and operational orbits can inﬂuence these metrics and highlight the direct correlation between reliability and maintainability.",
        "keywords": [
            "Dependability",
            "Constellation dependability approach",
            "Satellite constellations",
            "Stochastic Petri net"
        ],
        "authors": [
            "Daniel Farias",
            "Bruno Nogueira",
            "Ivaldir Farias Júnior",
            "Ermeson Andrade"
        ],
        "file_path": "data/sosym-all/s10270-024-01197-7.pdf"
    },
    {
        "title": "Model-based cloud resource management with TOSCA and OCCI",
        "submission-date": "2019/12",
        "publication-date": "2021/02",
        "abstract": "With the advent of cloud computing, different cloud providers with heterogeneous cloud services (compute, storage, network, applications, etc.) and their related Application Programming Interfaces (APIs) have emerged. This heterogeneity complicates the implementation of an interoperable cloud system. Several standards have been proposed to address this challenge and provide a uniﬁed interface to cloud resources. The Open Cloud Computing Interface (OCCI) thereby focuses on the standardization of a common API for Infrastructure-as-a-Service (IaaS) providers, while the Topology and Orchestration Speciﬁcation for Cloud Applications (TOSCA) focuses on the standardization of a template language to enable the proper deﬁnition of the topology of cloud applications and their orchestrations on top of a cloud system. TOSCA thereby does not deﬁne how the application topologies are created on the cloud. Therefore, we analyze the conceptual similarities between the two approaches and we study how we can integrate them to obtain a complete standard-based approach to manage both Cloud Infrastructure and Cloud application layers. We propose an automated extensive mapping between the concepts of the two standards, and we provide TOSCA Studio, a model-driven tool chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically design cloud applications as well as to deploy and manage them at runtime using a fully model-driven cloud orchestrator based on the two standards. Our contribution is validated by successfully transforming and deploying three cloud applications: WordPress, Node Cellar and Multi-Tier.",
        "keywords": [
            "Cloud computing",
            "Standards",
            "OCCI",
            "TOSCA",
            "Model-driven engineering",
            "Metamodels",
            "Cloud orchestrator",
            "Models@run.time"
        ],
        "authors": [
            "Stéphanie Challita",
            "Fabian Korte",
            "Johannes Erbel",
            "Faiez Zalila",
            "Jens Grabowski",
            "Philippe Merle"
        ],
        "file_path": "data/sosym-all/s10270-021-00869-y.pdf"
    },
    {
        "title": "Guest editorial to the theme issue on traceability in model-driven engineering",
        "submission-date": "2010/02",
        "publication-date": "2010/02",
        "abstract": "This issue of Software and Systems Modeling, and part of the issue that follows, are dedicated to the theme of traceability in model-driven engineering (MDE). Traceability is a fundamental concern in MDE processes, where models are related via application of different model management operations, such as model-to-model transformations, model-to-text transformations, model merging, model comparison, and many others. MDE emphasises on the application of automated model management operations, and substantial traceability information can be produced as a side-effect of applying these operations. In addition, in realistic MDE processes, traceability information can be produced by hand, through engineers manually relate MDE artefacts, or relate MDE artefacts with other artefacts (such as requirements documents). Overall, there are many challenges to traceability in MDE, ranging from managing large traceability models, to synchronizing models, and to keep traceability information consistent when models are being modiﬁed automatically and manually. This theme issue presents state-of-the-art research on these and other challenges.",
        "keywords": [],
        "authors": [
            "Richard F. Paige",
            "Goran K. Olsen",
            "Jon Oldevik",
            "Tor Neple"
        ],
        "file_path": "data/sosym-all/s10270-010-0153-0.pdf"
    },
    {
        "title": "TacoFlow: optimizing SAT program veriﬁcation using dataﬂow analysis",
        "submission-date": "2012/04",
        "publication-date": "2014/02",
        "abstract": "In previous work, we presented TACO, a tool for efﬁcient bounded veriﬁcation. TACO translates programs annotated with contracts to a SAT problem which is then solved resorting to off-the-shelf SAT-solvers. TACO may deem propositional variables used in the description of a program initial states as being unnecessary. Since the worst-case complexity of SAT (a known NP problem) depends on the number of variables, most times this allows us to obtain signiﬁcant speed ups. In this article, we present TacoFlow, an improvement over TACO that uses dataﬂow analysis in order to also discard propositional variables that describe intermediate program states. We present an extensive empirical evaluation that considers the effect of removing those variables at different levels of abstraction, and a discussion on the beneﬁts of the proposed approach.",
        "keywords": [
            "SAT-based veriﬁcation",
            "Dataﬂow analysis",
            "Java-like programs veriﬁcation"
        ],
        "authors": [
            "Bruno Cuervo Parrino",
            "Juan Pablo Galeotti",
            "Diego Garbervetsky",
            "Marcelo F. Frias"
        ],
        "file_path": "data/sosym-all/s10270-014-0401-9.pdf"
    },
    {
        "title": "Problem frame semantics for software development",
        "submission-date": "2003/11",
        "publication-date": "2004/07",
        "abstract": "This paper presents a framework for understanding Problem Frames that locates them within the Requirements Engineering model of Zave and Jackson, and its subsequent formalization in the Reference Model of Gunter et al. It distinguishes between problem frames, context diagrams and problem diagrams, and allows us to formally define the relationship between them as assumed in the Problem Frames framework.\nThe semantics of a problem diagram is given in terms of ‘challenges’, a notion that we also introduce. The notion of a challenge is interesting in its own right for two reasons: its proof theoretic derivation leads us to consider a challenge calculus that might underpin the Problem Frame operations of decomposition and recomposition; and it promises to extend the notion of formal refinement from software development to requirements engineering.\nIn addition, the semantics supports a textual representation of the diagrams in which Problem Frames capture problems and their relationship to solutions. This could open the way for graphical Problem Frames tools.",
        "keywords": [
            "Requirements engineering",
            "Problem Frames",
            "Semantics",
            "Reference model",
            "Framework"
        ],
        "authors": [
            "Jon G. Hall",
            "Lucia Rapanotti",
            "Michael Jackson"
        ],
        "file_path": "data/sosym-all/s10270-004-0062-1.pdf"
    },
    {
        "title": "Special section of business process modeling, development and support (BPMDS) 2019: transformative BPMDS",
        "submission-date": "2021/09",
        "publication-date": "2021/10",
        "abstract": "The business process modeling, development and support (BPMDS) working conference series is a meeting place for researchers and practitioners in the areas of business development and business applications development. By incorporating these views, BPMDS offers a unique community venue that integrates different streams of research on business processes and business information systems, and takes in a broad view on the whole range of BPMDS research and interrelationships among different perspectives. This makes it attractive for authors to publish cutting-edge research results at BPMDS. In this special section, a selection of the most influential contributions to the 2019 edition of the working conference are collected.",
        "keywords": [],
        "authors": [
            "Jens Gulden"
        ],
        "file_path": "data/sosym-all/s10270-021-00933-7.pdf"
    },
    {
        "title": "Temporal property patterns for model-based testing from UML/OCL",
        "submission-date": "2016/09",
        "publication-date": "2017/11",
        "abstract": "This article describes a new property- and model-based testing approach using UML/OCL models, driven by temporal property patterns and a tool for assisting the temporal properties formalization. The patterns are expressed in the TOCL language, an adaptation of Dwyer’s property patterns to OCL. The patterns are used to formalize temporal requirements without having to learn a complex temporal logics such as LTL or CTL. From these properties, automata are automatically computed. These can be used for two purposes. First, it is possible to evaluate the quality of a test suite by measuring the coverage of a property using its associated automaton. Second, the automaton can be used to drive the test generation in order to produce complementary test cases. To this end, we deﬁned dedicated coverage criteria, targeting speciﬁc events of the property, and aiming either at illustrating the expected behaviour of the system, or checking its robustness w.r.t. the property. However, it was observed that the semantics of the property language may be more subtle that it seems. To facilitate the adoption of the language by industrials, we have proposed a tool-supported assistant for property design, aiming to help the validation engineer choosing which constructs faithfully correspond to his intention. This approach has been experimented on several case studies with industrial partners. It has shown its interest for software validation, providing useful information thanks to adequate traceability features.",
        "keywords": [
            "Behavioural model",
            "Property patterns",
            "Coverage measure",
            "Test generation",
            "Property design"
        ],
        "authors": [
            "Frédéric Dadeau",
            "Elizabeta Fourneret",
            "Abir Bouchelaghem"
        ],
        "file_path": "data/sosym-all/s10270-017-0635-4.pdf"
    },
    {
        "title": "Toward an analytical method for SLA validation",
        "submission-date": "2015/06",
        "publication-date": "2017/06",
        "abstract": "Quantitative properties of modern software systems are often defined as a part of a service-level agreement (SLA) that fixes the maximal load to be submitted to a system and guarantees bounds for the response time or delay. The evaluation of software architectures in order to validate SLAs is a challenging task since the systems tend to be complex, highly dynamic and to some extent unpredictable. Thus, there is a need for fast and abstract techniques to evaluate the performance of modern software architectures based on the information available in the SLAs. The paper presents an efficient approach to compute bounds on the delay of composed systems based on available bounds for the load and the response times of components. The technique can be used by a user of a software architecture to validate SLAs of composed services based on SLAs of the components. It can also be used by a provider of a software architecture to validate whether additional users can be accepted or to compute required service capacities to fulfill an SLA.",
        "keywords": [
            "Service-level agreements",
            "Performance analysis",
            "Analytical techniques",
            "Quantitative validation",
            "Capacity planning"
        ],
        "authors": [
            "Peter Buchholz",
            "Sebastian Vastag"
        ],
        "file_path": "data/sosym-all/s10270-017-0604-y.pdf"
    },
    {
        "title": "Domain-speciﬁc discrete event modelling and simulation using graph transformation",
        "submission-date": "2011/04",
        "publication-date": "2012/03",
        "abstract": "Graph transformation is being increasingly used to express the semantics of domain-speciﬁc visual languages since its graphical nature makes rules intuitive. However, many application domains require an explicit handling of time to accurately represent the behaviour of a real system and to obtain useful simulation metrics to measure through- puts, utilization times and average delays. Inspired by the vast knowledge and experience accumulated by the discrete event simulation community, we propose a novel way of add- ing explicit time to graph transformation rules. In particu- lar, we take the event scheduling discrete simulation world view and provide rules with the ability to schedule the occur- rence of other rules in the future. Hence, our work combines standard, efﬁcient techniques for discrete event simulation (based on the handling of a future event set) and the intu- itive, visual nature of graph transformation. Moreover, we show how our formalism can be used to give semantics to other timed approaches and provide an implementation on top of the rewriting logic system Maude.",
        "keywords": [
            "Graph transformation",
            "Discrete event simulation",
            "Domain-speciﬁc modelling"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra",
            "Artur Boronat",
            "Reiko Heckel",
            "Paolo Torrini"
        ],
        "file_path": "data/sosym-all/s10270-012-0242-3.pdf"
    },
    {
        "title": "Bridging the gap between IEEE 1471, an architecture description language, and UML",
        "submission-date": "2002/02",
        "publication-date": "2002/12",
        "abstract": "A lot of attention has been paid to soft-ware architecture issues in academia, industrial research and standardization organizations working in the soft-ware area. The software architecture research community has focused on the creation and improvement of special-purpose languages: architecture description languages (ADLs). However, ADLs lack adequate support for separating various kinds of stakeholders’ concerns along diﬀerent viewpoints. But also, they do not address the diﬀerence between the architecture of a software system and its representations. In contrast, ANSI/IEEE-Std-1471 makes a clear distinction between the architecture and the architectural description of a software system. In this paper, we propose ConcernBASE, a UML-based approach to software architecture, which instanti-ates the conceptual framework deﬁned in ANSI/IEEE-Std-1471 and complements the abstractions and mech-anisms found in current ADLs. ConcernBASE provides a viewpoint for structural descriptions of software archi-tectures that supports key concepts of ADLs and deﬁnes a UML proﬁle as a viewpoint language. We validate this proﬁle through a mapping between a representative ADL, called Structural ADL (SADL), and the new proﬁle and by providing a UML-based tool prototype, called Con-cernBASE Modeler, which integrates with SADL tools.",
        "keywords": [
            "Software architecture",
            "Architecture de-scription",
            "UML",
            "ADL",
            "ANSI/IEEE-Std-1471",
            "SADL",
            "Advanced separation of concerns",
            "MDSOC",
            "Views",
            "Viewpoints",
            "Concern space"
        ],
        "authors": [
            "Mohamed M. Kand´e",
            "Valentin Crettaz",
            "Alfred Strohmeier",
            "Shane Sendall"
        ],
        "file_path": "data/sosym-all/s10270-002-0010-x.pdf"
    },
    {
        "title": "Assessing the usefulness of a visual programming IDE for large-scale automation software",
        "submission-date": "2022/04",
        "publication-date": "2023/02",
        "abstract": "Industrial control applications are usually designed by domain experts instead of software engineers. These experts frequently\nuse visual programming languages based on standards such as IEC 61131-3 and IEC 61499. The standards apply model-\nbased engineering concepts to abstract from hardware and low-level communication. Developing industrial control software\nis challenging due to the fact that control systems are usually unique and need to be maintained for many years. The\narising challenges, together with the growing complexity of control software, require very usable model-based development\nenvironmentsforvisualprogramminglanguages.However,sofaronlylittleempiricalresearchexistsonthepracticalusefulness\nof such environments, i.e., their usability and utility. In this paper, we discuss common control software maintenance tasks\nand tool capabilities based on existing research and show the realization of these capabilities in the 4diac IDE. We performed\na walkthrough of the demonstrated capabilities using the cognitive dimensions of notations framework from the ﬁeld of\nhuman–computer interaction. We then improved the tool and conducted a user study involving ten industrial automation\nengineers, who used the 4diac IDE in a realistic control software maintenance scenario. Based on lessons learnt from this\nstudy, we adapted the 4diac IDE to better handle large graphical models. We evaluated these changes in a reassessment\nstudy with automation engineers from seven industrial enterprises. We derive general implications with respect to large-scale\napplications for developers of IDEs that we deem applicable in the context of (visual) model-based engineering tools.",
        "keywords": [
            "Usefulness study",
            "Open source software",
            "IEC 61499",
            "Modeling tools",
            "Model-driven engineering"
        ],
        "authors": [
            "Bianca Wiesmayr",
            "Alois Zoitl",
            "Rick Rabiser"
        ],
        "file_path": "data/sosym-all/s10270-023-01084-7.pdf"
    },
    {
        "title": "Large language models as an “operating” system for software and systems modeling",
        "submission-date": "2023/09",
        "publication-date": "2023/09",
        "abstract": "Large Language Models (LLM), such as the variants of Chat-GPT or BERT, are currently under intensive development and enhancement, but it has become clear that they will contribute a signiﬁcant change to the way text and images are created in the future. It is therefore not surprising that the challenges, risks and opportunities of generative AI are discussed intensively in various scientiﬁc communities as well as in industry, government, education, and society, in general. There are of course technological aspects to be understood, but the impact on society and the industrial world has been significantly changed by the advantages and consequences of an expanded focus of generative AI, as supported by LLMs. Although there is much to learn from the emergence of this technology, it is becoming more evident how the usage of LLMs may impact the way we develop software and softwareintensivesystems.WeassumethatmanySoSyMreaders have made their own experiments to understand the capabilities and limitations of the currently available LLMs. Similar to any other complex system, it is challenging to understand analytically what the system does and why it produced a specific output. In particular, the traditional computer science formal and precise analytical methods often do not apply to the use of LLMs. Thus, we may need to rely on experimentation to gain initial insights into the capabilities and limitations of the potential for using LLMs in our own work. For a deeper validation of these insights and the hypotheses that will be built in the near future, it may make sense to (among other techniques) apply psychological methods to understand AI systems. Psychology has understood how to analyze brain capabilities and how to deal with uncertainty based on statistical evidence. Such statistical evidence may not be enough when a general AI system is used in safety critical applications (e.g., autonomous driving), but it may be helpful when trying to understand the usefulness of the conversation with such an AI, and how the AI is generally thinking. It may also be helpful to complement the analytical approaches to cope with uncertainty in the modeling activities. In this editorial, we pose another hypothesis on the future use of LLMs. The hypothesis is: 1. Variations of LLMs will emerge, with some very specialized to a distinct task or a specific domain, which focus on a particular underlying knowledge that is constrained by technical or societal concerns. 2. However, it will not be the case that there will be a unique LLM developed and trained for each purpose. Instead, there will be individual post-training of an underlying LLM that allows configuration of an AI model in such a way that it can be adapted easily to the individual functions that need assistance. 3. There will only be a few fully trained core LLMs and they will be provided in an open form. They will act like a kind of “operating system” for AI-systems where “application”-specific extensions are created.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01126-0.pdf"
    },
    {
        "title": "Model-based lifecycle management of software-intensive systems, applications, and services",
        "submission-date": "2013/06",
        "publication-date": "2013/06",
        "abstract": "The lifecycle of a successful system is the time period that covers all activities associated with developing, conﬁguring, deploying, operating, and retiring the system. Variations in system lifecycles can be expected, for example, differences may arise as a result of the inclusion of physical parts in the system and the number of installations. In addition, software retirement activities may extend over a long period of time, for example, in cases where access to data provided by a system may be required long after the system is terminated. Lifecycle management has a lot to do with managing the available information about a system. A signiﬁcant amount of this information can typically be found in the models produced during various development. Software models can thus play a vital role in system lifecycle management. For exam- ple, requirement models can be used to support management of requirements, feature models can be used to manage sys- tem and user speciﬁc variabilities as well as commonalities, and architecture and design models can provide information that support management of deployment and validation activ- ities. The potential role that models can play in lifecycle man- agement raises the following questions: “To what extent do the models produced during software development help (or hinder) lifecycle management?” “Should the software mod- eling activity be integrated with the lifecycle management of systems, and, if yes, how can this be done?” “What tools are needed to better leverage the use of models in lifecycle man- agement?” “Does a model also have a lifecycle that needs to be managed?”",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-013-0362-4.pdf"
    },
    {
        "title": "Introduction to theme section on requirements formalisation",
        "submission-date": "2024/10",
        "publication-date": "2024/11",
        "abstract": "Requirementsformalisation(RF)isaprocessinrequirements engineering which derives a precise requirements speciﬁcation, expressed, for example, in UML/OCL, from informal or semi-formal requirements documentation expressed using natural language, sketches, etc.\nRequirements formalisation can be a highly resource-intensive process if performed manually, so there has been much research interest in the automation of requirements formalisation. RF automation has signiﬁcant potential as a means of reducing software development costs, accelerating development processes and increasing the rigour of requirements engineering processes. Many approaches have been proposed for automated or semi-automated formalisation of software requirements, typically involving some form of nat-ural language processing (NLP) or machine learning (ML): [1, 2]. Heuristic rule-based approaches have been widely used for RF, based on NLP analysis of textual requirements, and rules that map identiﬁed linguistic constructs to mod-elling elements. For example, a recognised named entity in the requirements documents, that represents a category of system user, can be mapped to a UML use-case actor. Recent work in the ﬁeld has explored the use of large language mod-els (LLMs) such as GPT-4 to perform RF: [3–5].",
        "keywords": [],
        "authors": [
            "Kevin Lano",
            "Shekoufeh Rahimi",
            "Sobhan Tehrani",
            "Lola Burgueño",
            "Mohammad Aminu Umar"
        ],
        "file_path": "data/sosym-all/s10270-024-01241-6.pdf"
    },
    {
        "title": "A benchmark of incremental model transformation tools based on an industrial case study with AADL",
        "submission-date": "2020/09",
        "publication-date": "2022/03",
        "abstract": "Incremental model transformation (IMT) tools have been proposed to improve performances of model transformations by updating only the parts of a model that need to be changed when another model on which it depends has been changed. Yet, the question is how these tools are suitable for modeling large and complex systems with rich modeling languages as used in industry. In this paper, we report the results of a benchmark of the most mature IMT tools. Particularly, we benchmark MoTE, eMoﬂon, VIATRA and YAMTL to evaluate their usability, maintainability and runtime performances. Our benchmark is based on a model transformation of an industrial case study using the standard architecture description language AADL. We propose a reusable evaluation framework, available for tool developers and users. Besides the capability to process large models, our benchmark also assesses IMT tool performances according to the different kinds of complex structures that typically exist in models of rich languages, as well as the complexity of the transformation speciﬁcations. Our results show the promising potential of some tools to specify sophisticated speciﬁcations and transform large models with good performance, but their use still requires the help of their developers, especially to solve serious problems with their runtime performance.",
        "keywords": [
            "Incremental model transformations",
            "Benchmark",
            "AADL",
            "MoTE",
            "eMoﬂon",
            "VIATRA",
            "YAMTL"
        ],
        "authors": [
            "Hana Mkaouar",
            "Dominique Blouin",
            "Etienne Borde"
        ],
        "file_path": "data/sosym-all/s10270-022-00989-z.pdf"
    },
    {
        "title": "TEC-MAP: a taxonomy of evaluation criteria and its application to the multi-modelling of data and processes",
        "submission-date": "2023/11",
        "publication-date": "2024/08",
        "abstract": "The domain of Enterprise Information Systems Engineering uses many different conceptual modelling languages and methods to specify the requirements of a system under development. The complexity of the systems under development may require addressing different perspectives with different models, such as the data and process perspectives. The modeller will thus have to choose the appropriate (set of) modelling languages according to their speciﬁc modelling goal. Given that the different aspects relate to a single system, ideally, the models that capture the different perspectives should be aligned and consistent to ensure their integration. Each candidate (set of) modelling languages comes with advantages and disadvantages. To make an informed choice in this matter, the modeller should select a number of criteria relevant to their problem domain and compare candidate modelling languages based on these criteria. A comprehensive evaluation framework for integrated modelling approaches, that considers more general aspects such as understandability, ease of use, model quality, etc. besides the ability to model the desired aspects, does not yet exist and is therefore the focus of this paper. In recent years, several combinations of modelling languages have been investigated. Amongst these combinations, data + process modelling has attracted a lot of interest, and, interestingly, evaluation frameworks for this combination have been proposed as well. Therefore, this paper will primarily focus on the integrated multi-modelling of data and processes, including the process-related viewpoints of users and authorisations. The contribution of this paper is two-fold: on a theoretical level, the paper provides an overview of existing evaluation frameworks in the literature, builds a more complete set of evaluation criteria and proposes a uniﬁed taxonomy for the classiﬁcation of these evaluation criteria (TEC-MAP); on a practical level, the paper provides guidance and support to the modeller for selecting the appropriate evaluation criteria for their problem domain and presents three examples of the application of TEC-MAP.",
        "keywords": [
            "Data-centric process modelling",
            "Taxonomy",
            "Evaluation framework"
        ],
        "authors": [
            "Charlotte Verbruggen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-024-01198-6.pdf"
    },
    {
        "title": "MORGAN: a modeling recommender system based on graph kernel",
        "submission-date": "2022/04",
        "publication-date": "2023/04",
        "abstract": "Model-driven engineering (MDE) is an effective means of synchronizing among stakeholders, thereby being a crucial part of\nthe software development life cycle. In recent years, MDE has been on the rise, triggering the need for automatic modeling\nassistants to support metamodelers during their daily activities. Among others, it is crucial to enable model designers to\nchoose suitable components while working on new (meta)models. In our previous work, we proposed MORGAN, a graph\nkernel-based recommender system to assist developers in completing models and metamodels. To provide input for the rec-\nommendation engine, we convert training data into a graph-based format, making use of various natural language processing\n(NLP) techniques. The extracted graphs are then fed as input for a recommendation engine based on graph kernel simi-\nlarity, which performs predictions to provide modelers with relevant recommendations to complete the partially speciﬁed\n(meta)models. In this paper, we extend the proposed tool in different dimensions, resulting in a more advanced recommender\nsystem. Firstly, we equip it with the ability to support recommendations for JSON schema that provides a model representation\nof data handling operations. Secondly, we introduce additional preprocessing steps and a kernel similarity function based on\nitem frequency, aiming to enhance the capabilities, providing more precise recommendations. Thirdly, we study the proposed\nenhancements, conducting a well-structured evaluation by considering three real-world datasets. Although the increasing size\nof the training data negatively affects the computation time, the experimental results demonstrate that the newly introduced\nmechanisms allow MORGAN to improve its recommendations compared to its preceding version.",
        "keywords": [
            "Model-driven engineering",
            "Recommender systems",
            "Graph kernels"
        ],
        "authors": [
            "Claudio Di Sipio",
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Phuong T. Nguyen"
        ],
        "file_path": "data/sosym-all/s10270-023-01102-8.pdf"
    },
    {
        "title": "Model-based veriﬁcation of data protection mechanisms in collaborative business processes",
        "submission-date": "2022/05",
        "publication-date": "2025/01",
        "abstract": "In scenarios where multiple autonomous systems collaborate to execute a business process, it is often necessary for them to exchange conﬁdential or private data. In this setting, mechanisms need to be put in place to ensure that each participant accesses data in a way that respects conﬁdentiality or privacy constraints. The PE-BPMN notation is an extension of the Business Process Model and Notation (BPMN), speciﬁcally designed to model collections of autonomous systems that execute collaborative processes under the safeguard of privacy-enhancing technologies. Given a PE-BPMN speciﬁcation, we address the problem of verifying that the privacy-enhancing technologies captured in the speciﬁcation are used correctly, and no unexpected data disclosure may happen during process execution. To this end, we formalize the semantics of PE-BPMN collaboration speciﬁcations via a translation into process algebraic models. This makes it possible to check the correct usage of different kinds of privacy-enhancing technologies—e.g. secret sharing, encryption and multi-party computation—via model checking techniques. The approach has been implemented on top of the mCRL2 toolset and integrated into the Pleak toolset for privacy-enhanced business process analysis. The proposal has been evaluated using both real and synthetic scenarios.",
        "keywords": [
            "Collaborative distributed system",
            "Business process",
            "Privacy-enhancing technology",
            "Model checking",
            "MCRL2"
        ],
        "authors": [
            "Sara Belluccini",
            "Rocco De Nicola",
            "Marlon Dumas",
            "Pille Pullonen-Raudvere",
            "Barbara Re",
            "Francesco Tiezzi"
        ],
        "file_path": "data/sosym-all/s10270-024-01217-6.pdf"
    },
    {
        "title": "iDOCEM: deﬁning a common terminology for object-centric event logging and data-centric process modelling",
        "submission-date": "2023/10",
        "publication-date": "2024/07",
        "abstract": "In the business process lifecycle, models can be approached from two perspectives: on the one hand, models are used to create systems in the design phase, and on the other hand, systems in use produce (event) logs that are used to discover the models representing the structure of the systems. These discovered models can be the starting point of a new cycle of analysis, redesign, implementation, etc. Therefore, proper logging of implemented processes in line with system design is a critical element for process discovery. Recently, the consideration of the integration of data and process aspects has seen a surge in interest in both the model-for-design domain as in the automated-model-discovery domain. However, it seems that these domains use different conceptualizations of data/object-aware systems. A deﬁnition of how the captured event logs are related to the structure of the global system they are extracted from or are trying to discover is still missing. Especially the concept of an event needs to be aligned, as this is the main concept that the domains have in common. This paper investigates the concepts and terminology used in the different phases of the business process lifecycle: the design phase, the implementation phase (including the implementation of logging) and the discovery phase. The paper contains an extensive running example that is used to illustrate ﬁve misalignment issues. The main contribution of this paper is a meta-model that presents a uniﬁed terminology for modelling both domains and is demonstrated using the running example. The paper also shows how the concepts of iDOCEM relate to the concepts of a conceptual modelling approach and several event logging formats. iDOCEM is validated with the implementation of a log generator for the running case, demonstrating the feasibility of generating DOCEL-compliant logs from an application.",
        "keywords": [
            "Conceptual modelling",
            "Object-centric process logging",
            "Artefact-centric process modelling",
            "Data-aware process modelling",
            "Object-centric process discovery"
        ],
        "authors": [
            "Charlotte Verbruggen",
            "Alexandre Goossens",
            "Johannes De Smedt",
            "Jan Vanthienen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-024-01191-z.pdf"
    },
    {
        "title": "Adapting transformations to metamodel changes via external transformation composition",
        "submission-date": "2011/03",
        "publication-date": "2013/02",
        "abstract": "Evolution is inherent to software systems because of the rapid improvement of technologies and business logic. As a software development paradigm, model driven engineering (MDE) is also affected by this problem. More concretely, being metamodels the cornerstone of MDE, their evolution impacts the rest of software artefacts involved in a development process, i.e., models and transformations. The inﬂuence over models has been tackled and partially solved in previous works. This paper focuses on the impact over transformations. We propose an approach to adapt transformations by means of external transformation composition. That is, we chain impacted transformations to particular adaptation transformations which deal with either refactoring/destruction changes or construction changes. Our approach semi-automatically generates such transformations by using the AtlanMod matching language, a DSL to deﬁne model matching strategies. To provide with a proof of concept for our proposal, we adapt transformations written in terms of object-relational database metamodels when such metamodels evolve in time.",
        "keywords": [
            "Model-driven engineering",
            "Metamodel evolution",
            "Transformation adaptation"
        ],
        "authors": [
            "Kelly Garcés",
            "Juan M. Vara",
            "Frédéric Jouault",
            "Esperanza Marcos"
        ],
        "file_path": "data/sosym-all/s10270-012-0297-1.pdf"
    },
    {
        "title": "Design veriﬁcation in model-based µ-controller development using an abstract component",
        "submission-date": "2009/01",
        "publication-date": "2010/02",
        "abstract": "Component-based software development is a promising approach for controlling the complexity and quality of software systems. Nevertheless, recent advances in quality control techniques do not seem to keep up with the growing complexity of embedded software; embedded systemsoftenconsistofdozenstohundredsofsoftware/hardware components that exhibit complex interaction behav-ior. Unanticipated quality defects in a component can be a major source of system failure. To address this issue, this paper suggests a design veriﬁcation approach integrated into the model-driven, component-based development methodology Marmot. The notion of abstract components—the basic building blocks of Marmot—helps to lift the level of abstraction, facilitates high-level reuse, and reduces veriﬁcation complexity by localizing veriﬁcation problems between abstract components before reﬁnement and after reﬁnement. This enables the identiﬁcation of unanticipated design errors in the early stages of development. This work introduces Communicated by Dr. Perry Alexander. This paper is an extended version of Choi [11] and Choi and Bunse [13]. This work has been supported by the Korea Research Foundation Grant funded by the Korean Government (KRF-2008-331-D00525) and the Engineering Research Center of Excellence Program of the Korean Ministry of Education, Science and Technology (MEST)/Korea Science and Engineering Foundation (KOSEF), grant number R11-2008-007-03002-0. the Marmot methodology, presents a design veriﬁcation approach in Marmot, and demonstrates its application on the development of a μ-controller-based abstraction of a car mirror control system. An application on TinyOS shows that the approach helps to reuse models as well as their veriﬁca-tion results in the development process.",
        "keywords": [
            "Abstract component",
            "Model-driven development",
            "Design veriﬁcation",
            "Embedded systems"
        ],
        "authors": [
            "Yunja Choi",
            "Christian Bunse"
        ],
        "file_path": "data/sosym-all/s10270-010-0147-y.pdf"
    },
    {
        "title": "Refactoring goal-oriented models: a linguistic improvement using large language models",
        "submission-date": "2024/03",
        "publication-date": "Not found",
        "abstract": "Goal-oriented requirements engineering (GORE) facilitates effective communication and collaboration between stakeholders. Using goal models, GORE provides a structured approach to elicit, analyze, and manage requirements from the perspective of stakeholders’ goals and intentions. However, goal models are prone to several poor practices, called bad smells, which can obstruct effective communication between stakeholders. As a result, there might be misinterpretations and inconsistencies in the requirements. Goal models are particularly prone to linguistic bad smells, encompassing unclear or ambiguous goal statements, conﬂicting or contradictory requirements, and occurrences of misspellings. It is therefore imperative that linguistic bad smells are identiﬁed and addressed in goal models to ensure their quality and accuracy. In this paper, we build upon our previous research by enhancing the catalog of 17 goal-oriented requirements language (GRL) linguistic bad smells. We reﬁne the detection techniques using a combination of NLP-based and LLM-based techniques. These enhancements signiﬁcantly improved the tool’s detection capabilities compared to our previous work. Furthermore, we offer automated refactoring solutions for 9 of these bad smells through GPT prompts. The remaining four identiﬁed bad smells are left to the user’s discretion for refactoring, due to their subjective nature. The detection and refactoring processes are implemented in a tool, tailored to the Textual GRL (TGRL) language. We evaluated the bad smells refactoring approach and tool by administering a questionnaire to 13 participants, who assessed the correctness of the refactoring of 71 linguistic bad smells found in four (4) TGRL models. Participants perceived the refactored sentences as highly correct across the different types of linguistic bad smells.",
        "keywords": [
            "Textual goal-oriented requirement language (TGRL)",
            "Linguistic bad smells",
            "NLP",
            "Refactoring",
            "GPT",
            "LLM"
        ],
        "authors": [
            "Nouf Alturayeif",
            "Jameleddine Hassine"
        ],
        "file_path": "data/sosym-all/s10270-024-01254-1.pdf"
    },
    {
        "title": "Identifying and ﬁxing ambiguities in, and semantically accurate formalisation of, behavioural requirements",
        "submission-date": "2023/08",
        "publication-date": "2024/03",
        "abstract": "To correctly formalise requirements expressed in natural language, ambiguities must ﬁrst be identiﬁed and then ﬁxed. This paper focuses on behavioural requirements (i.e. requirements related to dynamic aspects and phenomena). Its ﬁrst objective is to show, based on a practical, public case study, that the disambiguation process cannot be fully automated: even though natural language processing (NLP) tools and machine learning might help in the identiﬁcation of ambiguities, ﬁxing them often requires a deep, application-speciﬁc understanding of the reasons of being of the system of interest, of the characteristics of its environment, of which trade-offs between conﬂicting objectives are acceptable, and of what is achievable and what is not; it may also require arduous negotiations between stakeholders. Such an understanding and consensus-making ability is not in the reach of current tools and technologies, and will likely remain so for a long while. Beyond ambiguity, requirements are often marred by various other types of defects that could lead to wholly unacceptable consequences. In particular, operational experience shows that requirements inadequacy (whereby, in some of the situations the system could face, what is required is woefully inappropriate or what is necessary is left unspeciﬁed) is a signiﬁcant cause for systems failing to meet expectations. The second objective of this paper is to propose a semantically accurate behavioural requirements formalisation format enabling tool-supported requirements veriﬁcation, notably with simulation. Such support is necessary for the engineering of large and complex cyber-physical and socio-technical systems to ensure, ﬁrst, that the speciﬁed requirements indeed reﬂect the true intentions of their authors and second, that they are adequate for all the situations the system could face. To that end, the paper presents an overview of the BASAALT (Behaviour Analysis and Simulation All Along systems Life Time) systems engineering method, and of FORM-L (FOrmal Requirements Modelling Language), its supporting language, which aims at representing as accurately and completely as possible the semantics expressed in the original, natural language behavioural requirements, and is markedly different from languages intended for software code generation. The paper shows that generally, semantically accurate formalisation is not a simple paraphrasing of the original natural language requirements: additional elements are often needed to fully and explicitly reﬂect all that is implied in natural language. To provide such complements for the case study presented in the paper, we had to follow different formalisation patterns, i.e. sequences of formalisation steps. For this paper, to avoid being skewed by what a particular automatic tool can and cannot do, BASAALT and FORM-L were applied manually. Still, the lessons learned could be used to specify and develop NLP tools that could assist the disambiguation and formalisation processes. However, more studies are needed to determine whether an exhaustive set of formalisation patterns can be identiﬁed to fully automate the formalisation process.",
        "keywords": [
            "Requirements",
            "Formalisation",
            "k3 case study",
            "Models",
            "Disambiguation"
        ],
        "authors": [
            "Thuy Nguyen",
            "Imen Sayar",
            "Sophie Ebersold",
            "Jean-Michel Bruel"
        ],
        "file_path": "data/sosym-all/s10270-023-01142-0.pdf"
    },
    {
        "title": "Testing Web applications by modeling with FSMs",
        "submission-date": "2005/01",
        "publication-date": "2005/01",
        "abstract": "Researchers and practitioners are still trying\nto ﬁnd eﬀective ways to model and test Web applications.\nThis paper proposes a system-level testing technique that\ncombines test generation based on ﬁnite state machines\nwith constraints. We use a hierarchical approach to model\npotentially large Web applications. The approach builds\nhierarchies of Finite State Machines (FSMs) that model\nsubsystems of the Web applications, and then generates\ntest requirements as subsequences of states in the FSMs.\nThese subsequences are then combined and reﬁned to\nform complete executable tests. The constraints are used\nto select a reduced set of inputs with the goal of reduc-\ning the state space explosion otherwise inherent in using\nFSMs. The paper illustrates the technique with a running\nexample of a Web-based course student information sys-\ntem and introduces a prototype implementation to sup-\nport the technique.",
        "keywords": [
            "Testing of Web applications",
            "System testing",
            "Finite state machines"
        ],
        "authors": [
            "Anneliese A. Andrews",
            "JeﬀOﬀutt",
            "Roger T. Alexander"
        ],
        "file_path": "data/sosym-all/s10270-004-0077-7.pdf"
    },
    {
        "title": "Petri nets for the control of discrete event systems",
        "submission-date": "2014/01",
        "publication-date": "2014/07",
        "abstract": "The interest in Petri nets has grown within the automatic control community in parallel with the development of the theory of discrete event systems. In this article, our goal is to give a ﬂavor of the features that make Petri nets a good model for discrete event systems and to point out the main areas where Petri nets have offered the most signiﬁcant contributions.",
        "keywords": [
            "Petri nets",
            "Discrete event systems",
            "Supervisory control",
            "Monitor places",
            "State estimation"
        ],
        "authors": [
            "Alessandro Giua",
            "Carla Seatzu"
        ],
        "file_path": "data/sosym-all/s10270-014-0425-1.pdf"
    },
    {
        "title": "Detecting cross-case associations in an event log: toward a pattern-based detection",
        "submission-date": "2022/02",
        "publication-date": "2023/04",
        "abstract": "Business process management, design, and analysis is mostly centered around a process model, which depicts the behavior of a process case (instance). As a result, behavior that associates several cases together has received less attention. Yet, it is important to understand and track associations among cases, as they bear substantial consequences for compliance with regulations, root cause analysis of performance issues, exception handling, and prediction. This paper presents a framework of cross-case association patterns, categorized as intended association patterns and contextual association patterns. It further conceptualizes two example patterns—one for each category, and proposes techniques for detecting these patterns in an event log. The “split-case” workaround is an example of a pattern in the intended association category, and its proposed detection method exempliﬁes how patterns in this category can be approached. The patterns of a shared entity and a shared resource are contextual association patterns, which we propose to detect by means of hidden concept drifts. Evaluation of the two detection approaches is reported, using simulated logs for assessing their internal validity as well as real-life ones for exploring their external validity.",
        "keywords": [
            "Process mining",
            "Cross-case patterns",
            "Split-case workaround"
        ],
        "authors": [
            "Yael Dubinsky",
            "Pnina Soffer",
            "Irit Hadar"
        ],
        "file_path": "data/sosym-all/s10270-023-01100-w.pdf"
    },
    {
        "title": "Formalizing and appling compliance patterns for business process compliance",
        "submission-date": "2013/05",
        "publication-date": "2014/02",
        "abstract": "Today’s enterprises demand a high degree of compliance of business processes to meet diverse regulations and legislations. Several industrial studies have shown that compliance management is a daunting task, and organizations are still struggling and spending billions of dollars annually to ensure and prove their compliance. In this paper, we introduce a comprehensive compliance management framework with a main focus on design-time compliance management as a ﬁrst step towards a preventive lifetime compliance support. The framework enables the automation of compliance-related activities that are amenable to automation, and therefore can signiﬁcantly reduce the expenditures spent on compliance. It can help experts to carry out their work more efﬁciently, cut the time spent on tedious manual activities, and reduce potential human errors. An evident candidate compliance activity for automation is the compliance checking, which can be achieved by utilizing formal reasoning and veriﬁcation techniques. However, formal languages are well known of their complexity as only versed users in mathematical theories and formal logics are able to use and understand them. However, this is generally not the case with businessandcompliancepractitioners.Therefore,intheheart of the compliance management framework, we introduce the Compliance Request Language (CRL), which is formally grounded on temporal logic and enables the abstract pattern-based speciﬁcation of compliance requirements. CRL constitutes a series of compliance patterns that spans three structural facets of business processes; control ﬂow, employed resources and temporal perspectives. Furthermore, CRL supports the speciﬁcation of compensations and non-monotonic requirements, which permit the relaxation of some compliance requirements to handle exceptional situations. An integrated tool suite has been developed as an instantiation artefact, and the validation of the approach is undertaken in several directions, which includes internal validity, controlled experiments, and functional testing.",
        "keywords": [
            "Business process compliance",
            "Compliance patterns",
            "Formal speciﬁcation",
            "Regulatory compliance",
            "Compliance management tool support",
            "Design-time compliance management"
        ],
        "authors": [
            "Amal Elgammal",
            "Oktay Turetken",
            "Willem-Jan van den Heuvel",
            "Mike Papazoglou"
        ],
        "file_path": "data/sosym-all/s10270-014-0395-3.pdf"
    },
    {
        "title": "A metrics suite for UML model stability",
        "submission-date": "2016/06",
        "publication-date": "2016/12",
        "abstract": "Software metrics have become an essential part of software development because of their importance in estimating cost, effort, and time during the development phase. Manymetricshavebeenproposedtoassessdifferentsoftware quality attributes, including stability. A number of software stability metrics have been proposed at the class, architecture, and system levels. However, these metrics typically target the source code. This paper proposes a software stability met-rics suite at the model level for three UML diagrams: class, use case, and sequence. These three diagrams represent the most common diagrams in the three UML views: structural, functional, and behavioral. We introduce a client–master assessment approach to avoid measurement duplication. We also theoretically and empirically validate the proposed met-rics suite. We also provide examples to demonstrate the use of the proposed metrics and their application as indicators of software stability.",
        "keywords": [
            "Model stability",
            "Software metrics",
            "Metrics suite"
        ],
        "authors": [
            "Amjad AbuHassan",
            "Mohammad Alshayeb"
        ],
        "file_path": "data/sosym-all/s10270-016-0573-6.pdf"
    },
    {
        "title": "A reconﬁguration pattern for distributed embedded systems",
        "submission-date": "2007/03",
        "publication-date": "2007/11",
        "abstract": "A reconﬁguration pattern for UML-based projects of embedded (real-time) systems is deﬁned. It enables to set up hardware/software conﬁgurations, and to specify conditions and methods for dynamic reconﬁguration. The reconﬁguration pattern was inspired by the reconﬁguration management solution of the Speciﬁcation PEARL methodology, which is based on the standard for Multiprocessor PEARL whose original idea it was to extend the language to enable the programming of distributed real-time applications in PEARL. In Speciﬁcation PEARL, the possibility for abstract descriptions of hardware and software architectures and for deﬁning mappings from software to hardware components has been enhanced in correspondence with the standard. Here, a UML pattern for reconﬁguration management in distributed embedded applications based on concepts from Speciﬁcation PEARL is presented. Its behavioural, structural and functional aspects are outlined. It addresses stereotype entities from the Speciﬁcation PEARL language, which were joined in a UML proﬁle, and outlines the related reconﬁguration management mechanisms, which were carried over to the mentioned UML pattern. The proposed reconﬁguration pattern is to facilitate the development of distributed embedded application in UML with consistent and temporally predictable reconﬁguration support. It should also support and enhance the applications’ ﬂexibility and portability.",
        "keywords": [
            "Real-time",
            "Distributed",
            "Embedded systems",
            "Dynamic reconﬁguration",
            "UML proﬁles and patterns",
            "UML-RT",
            "Speciﬁcation PEARL"
        ],
        "authors": [
            "Roman Gumzej",
            "Matjaž Colnariˇc",
            "Wolfgang A. Halang"
        ],
        "file_path": "data/sosym-all/s10270-007-0075-7.pdf"
    },
    {
        "title": "Using recommender systems to improve proactive modeling",
        "submission-date": "2019/11",
        "publication-date": "2021/01",
        "abstract": "This article investigates using recommender systems within graphical domain-speciﬁc modeling languages (DSMLs). The\nobjective of using recommender systems within a graphical DSML is to overcome a shortcoming of proactive modeling\nwhere the modeler must inform the model intelligence engine how to progress when it cannot automatically determine\nthe next modeling action to execute (e.g., add, delete, or edit). To evaluate our objective, we implemented a recommender\nsystem into the Proactive Modeling Engine, which is an add-on for the Generic Modeling Environment. We then conducted\nexperiments to subjectively and objectively evaluate enhancements to the Proactive Modeling Engine. The results of our\nexperiments show that extending proactive modeling with a recommender system results in an average reciprocal hit-rank of\n0.871. Likewise, the enhancements yield a System Usability Scale rating of 77. Finally, user feedback shows that integrating\nrecommender systems into DSMLs increases usability and learnability.",
        "keywords": [
            "Domain-speciﬁc modeling languages",
            "Proactive modeling",
            "Recommender systems"
        ],
        "authors": [
            "Arvind Nair",
            "Xia Ning",
            "James H. Hill"
        ],
        "file_path": "data/sosym-all/s10270-020-00841-2.pdf"
    },
    {
        "title": "Blended modeling in commercial and open-source model-driven software engineering tools: A systematic study",
        "submission-date": "2021/09",
        "publication-date": "2022/06",
        "abstract": "Blended modeling aims to improve the user experience of modeling activities by prioritizing the seamless interaction with models through multiple notations over the consistency of the models. Inconsistency tolerance, thus, becomes an important aspect in such settings. To understand the potential of current commercial and open-source modeling tools to support blended modeling, we have designed and carried out a systematic study. We identify challenges and opportunities in the tooling aspect of blended modeling. Specifically, we investigate the user-facing and implementation-related characteristics of existing modeling tools that already support multiple types of notations and map their support for other blended aspects, such as inconsistency tolerance, and elevated user experience. For the sake of completeness, we have conducted a multivocal study, encompassing an academic review, and grey literature review. We have reviewed nearly 5000 academic papers and nearly 1500 entries of grey literature. We have identified 133 candidate tools, and eventually selected 26 of them to represent the current spectrum of modeling tools.",
        "keywords": [
            "Model-driven development",
            "Inconsistency tolerance",
            "Multi-view modeling",
            "Modeling tools",
            "Survey"
        ],
        "authors": [
            "Istvan David",
            "Malvina Latifaj",
            "Jakob Pietron",
            "Weixing Zhang",
            "Federico Ciccozzi",
            "Ivano Malavolta",
            "Alexander Raschke",
            "Jan-Philipp Steghöfer",
            "Regina Hebig"
        ],
        "file_path": "data/sosym-all/s10270-022-01010-3.pdf"
    },
    {
        "title": "Quantifying Privacy Risk with Gaussian Mixtures",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "Data anonymization methods gain legal importance as data collection and analysis are expanding dramatically in data man-agement and statistical research. Yet applying anonymization, or understanding how well a given analytics program hides sensitive information, is non-trivial. Privug is a method to quantify privacy risks of data analytics programs by analyzing their source code. The method uses probability distributions to model attacker knowledge and Bayesian inference to update said knowledge based on observable outputs. Currently, Privug is equipped with approximate Bayesian inference methods (such as Markov Chain Monte Carlo), and an exact Bayesian inference method based on multivariate Gaussian distributions. This paper introduces a privacy risk analysis engine based on Gaussian mixture models that combines exact and approximate inference. It extends the multivariate Gaussian engine by supporting exact inference in programs with continuous and discrete distributions as well as if-statements. Furthermore, the engine allows for approximating attacker knowledge that is not normally distributed. We evaluate the method by analyzing privacy risks in programs to release public statistics, differential privacy mechanisms, randomized response and attribute generalization. Finally, we show that our engine can be used to analyze programs involving thousands of sensitive records.",
        "keywords": [
            "Privacy risk analysis",
            "Bayesian inference",
            "Probabilistic programming",
            "Data analytics programs"
        ],
        "authors": [
            "Rasmus C. Rønneberg",
            "Francesca Randone",
            "Raúl Pardo",
            "Andrzej Wa˛sowski"
        ],
        "file_path": "data/sosym-all/s10270-025-01298-x.pdf"
    },
    {
        "title": "A method for describing the syntax and semantics of UML statecharts",
        "submission-date": "2003/01",
        "publication-date": "2004/04",
        "abstract": "In this article we present a method for de-scribing the language of UML statecharts. Statecharts are syntactically deﬁned as attributed graphs, with well-formedness rules speciﬁed by a set of ﬁrst-order predicates over the abstract syntax of the graphs. The dynamic semantics of statecharts is deﬁned by Abstract State Machines parameterized with syntactically-correct attributed graphs. The presented approach covers many important constructs of UML statecharts, including internal, completion, interlevel and compound transitions as well as history pseudostates. It also contains strategies to handle state entry/exit actions, state activities, synch states and choice pseudostates.",
        "keywords": [
            "Visual languages",
            "UML",
            "Statecharts",
            "UML statecharts",
            "Syntax deﬁnition",
            "Formal operational semantics",
            "Abstract State Machines"
        ],
        "authors": [
            "Yan Jin",
            "Robert Esser",
            "J¨orn W. Janneck"
        ],
        "file_path": "data/sosym-all/s10270-003-0046-6.pdf"
    },
    {
        "title": "3LConOnt: a three-level ontology for context modelling in context-aware computing",
        "submission-date": "2016/09",
        "publication-date": "2017/08",
        "abstract": "Context-aware computing is the ability of Services and applications to adapt and react to context changes. Context modelling is a core feature of context-aware computing. Although a lot of research has been made in the ﬁeld of context modelling, most of the context-aware computing proposals prefer to design their own customized context model instead of reusing an existing one. The main reason for this behaviour is that current context models present some problems concerning reusability, extensibility and adaptation. To contribute solving these issues, in this paper we present 3LConOnt, a three-level context ontology that can be easily reused, extended and adapted for speciﬁc or generic purposes. The proposed context model consolidates the context knowledge already available from a modular perspective yielding a clear schema of knowledge reutilization. To do so, we gathered context knowledge pieces from different ontologies to be integrated into standardized and well-deﬁned levels of abstraction and modules. The proposal has been validated considering: (1) reusability, extensibility and adaptation by instantiating different smart scenarios; (2) consistency and reasoning by triggering queries to the proposed model based on some competence questions; and (3) reusability in existing ontologies by importing the needed module or level of the model. Additionally, we also illustrate its usability in context-aware Services by modelling a context-aware framework architecture for supporting the whole context life cycle: acquisition, modelling, reasoning and distribution.",
        "keywords": [
            "Context-aware computing",
            "Service-oriented computing",
            "Context life cycle",
            "Context modelling",
            "Context reasoning",
            "Context ontology"
        ],
        "authors": [
            "Oscar Cabrera",
            "Xavier Franch",
            "Jordi Marco"
        ],
        "file_path": "data/sosym-all/s10270-017-0611-z.pdf"
    },
    {
        "title": "From software extensions to product lines of dataflow programs",
        "submission-date": "2014/11",
        "publication-date": "2015/09",
        "abstract": "Dataflow programs are widely used. Each program is a directed graph where nodes are computations and edges indicate the flow of data. In prior work, we reverse-engineered legacy dataflow programs by deriving their optimized implementations from a simple specification graph using graph transformations called refinements and optimizations. In MDE speak, our derivations were PIM-to-PSM mappings. In this paper, we show how extensions complement refinements, optimizations, and PIM-to-PSM derivations to make the process of reverse engineering complex legacy dataflow programs tractable. We explain how optional functionality in transformations can be encoded, thereby enabling us to encode product lines of transformations as well as product lines of dataflow programs. We describe the implementation of extensions in the ReFlO tool and present two non-trivial case studies as evidence of our work’s generality.",
        "keywords": [
            "MDE",
            "PIM",
            "PSM",
            "Model transformations",
            "Software extensions",
            "Dataflow programs",
            "Software product lines"
        ],
        "authors": [
            "Rui C. Gonçalves",
            "Don Batory",
            "João L. Sobral",
            "Taylor L. Riché"
        ],
        "file_path": "data/sosym-all/s10270-015-0495-8.pdf"
    },
    {
        "title": "A powertype-based metamodelling framework",
        "submission-date": "2004/11",
        "publication-date": "2005/11",
        "abstract": "Software development methodologies may be de-scribed in the context of an underpinning metamodel, but theprecise mechanisms that permit them to be deﬁned in terms oftheir metamodels are usually difﬁcult to explain and do notcover all needs. For example, it is difﬁcult to devise a waythat allows the deﬁnition of properties of the elements thatcompose the methodology and, at the same time, of the entities(such as work products) created when the method-ology is applied. This article introduces a new approach toconstructing metamodels and deriving methodologies fromthem based on the concept of powertype. It combines keyadvantages of other metamodelling approaches and allows theseamless integration of process, modelling and docu-mentational aspects of methodologies. With this approach,both methodology components and project entities can bedescribed directly by the same metamodel.",
        "keywords": [
            "Metamodelling",
            "Powertype",
            "Software development methodologies"
        ],
        "authors": [
            "Cesar Gonzalez-Perez",
            "Brian Henderson-Sellers"
        ],
        "file_path": "data/sosym-all/s10270-005-0099-9.pdf"
    },
    {
        "title": "Qualifying input test data for model transformations",
        "submission-date": "2007/05",
        "publication-date": "2007/11",
        "abstract": "Model transformation is a core mechanism for model-driven engineering (MDE). Writing complex model transformations is error-prone, and efﬁcient testing techniques are required as for any complex program development. Testing a model transformation is typically performed by checking the results of the transformation applied to a set of input models. While it is fairly easy to provide some input models, it is difﬁcult to qualify the relevance of these models for testing. In this paper, we propose a set of rules and a framework to assess the quality of given input models for testing a given transformation. Furthermore, the framework identiﬁes missing model elements in input models and assists the user in improving these models.",
        "keywords": [
            "Software testing",
            "Model transformation",
            "Test criteria",
            "Test qualiﬁcation",
            "Metamodelling",
            "Model-based testing"
        ],
        "authors": [
            "Franck Fleurey",
            "Benoit Baudry",
            "Pierre-Alain Muller",
            "Yves Le Traon"
        ],
        "file_path": "data/sosym-all/s10270-007-0074-8.pdf"
    },
    {
        "title": "Goal-oriented modeling and veriﬁcation of feature-oriented product lines",
        "submission-date": "2012/08",
        "publication-date": "2014/02",
        "abstract": "Goal models represent requirements and intentions of a software system. They play an important role in the development life cycle of software product lines (SPLs). In the domain engineering phase, goal models guide the development of variability in SPLs by providing the rationale for the variability, while they are used for the conﬁguration of SPLs in the application engineering phase. However, variability in SPLs, which is represented by feature models, usually has design and implementation-induced constraints. When those constraints are not aligned with variability in goal models, the conﬁguration with goal models becomes error prone. To remedy this problem, we propose a description logic (DL)-based approach to represent both models and their relations in a common DL knowledge base. Moreover, we apply reasoning to detect inconsistencies in the variability of goal and feature models. A formal proof is provided to demonstrate the correctness of the reasoning approach. An empirical evaluation shows computational tractability of the inconsistency detection.",
        "keywords": [
            "Software engineering",
            "Feature oriented software families",
            "Goal-oriented requirements engineering",
            "Description Logic",
            "Feature Models",
            "Veriﬁcation"
        ],
        "authors": [
            "Mohsen Asadi",
            "Gerd Gröner",
            "Bardia Mohabbati",
            "Dragan Gaševi´c"
        ],
        "file_path": "data/sosym-all/s10270-014-0402-8.pdf"
    },
    {
        "title": "SoSyM at 7 years",
        "submission-date": "2008/11",
        "publication-date": "2008/11",
        "abstract": "Over its 7 years in circulation, SoSyM, the International Journal on Software and Systems Modeling, has received a reputation for publishing quality research and experience papers on building and using models in the development of software-based systems. During that time, we have witnessed a growing body of experience related to the use of modeling techniques in practice. The insights gained have led to the development of better solutions, but, more importantly, they have led to (1) a deeper understanding of some of the more critical problems that practitioners face when applying current modeling technologies, and (2) a broader vision of model-driven engineering (MDE) that encompasses not only development, but also software operation. Papers published in SoSyM, and in a number of related conferences and workshops that have contributed to special SoSyM issues, have made significant contributions to this growing body of experience and research on modeling software-based systems.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-008-0107-y.pdf"
    },
    {
        "title": "Development of logical and technical architectures for automotive systems",
        "submission-date": "2005/02",
        "publication-date": "2006/07",
        "abstract": "This paper presents a modeling approach for the development of software for electronic control units in the automotive domain. The approach supports the development of two related architecture models in the overall development process: the logical architecture provides a graphical, quite abstract representation of a typically large set of automotive functions. On this abstraction level no design decisions are taken. The technical architecture provides a software and a hardware representation in separated views: the software architecture describes the software realization of functions as software components, whereas the hardware architecture models hardware entities, on which the software components are deployed. Logical as well as technical architectures only model structural information, but no behavioural information. A tight integration of both architecture levels — on the conceptual and on the tool level — with related development phases such as requirements engineering, behaviour modeling, code generation as well as version and conﬁguration management resulting in a seamless overall development process is presented. This architecture modeling approach has been developed within a safety-relevant project at BMW Group. Positive as well as negative experiences with the application of this approach are described.",
        "keywords": [],
        "authors": [
            "Michael von der Beeck"
        ],
        "file_path": "data/sosym-all/s10270-006-0022-z.pdf"
    },
    {
        "title": "Analysing the cognitive effectiveness of the WebML visual notation",
        "submission-date": "2013/07",
        "publication-date": "2015/01",
        "abstract": "WebML is a domain-speciﬁc language used to design complex data-intensive Web applications at a conceptual level. As WebML was devised to support design tasks, the need to deﬁne a visual notation for the language was identiﬁed from the very beginning. Each WebML element is consequently associated with a separate graphical symbol which was mainly deﬁned with the idea of providing simple and expressive modelling artefacts rather than by adopting a rigorous scientiﬁc approach. As a result, the graphical models deﬁned with WebML may sometimes prevent proper communication from taking place between the various stakeholders. In fact, this is a common issue for most of the existing model-based proposals that have emerged during the last few years under the umbrella of model-driven engineering. In order to illustrate this issue and foster in using a scientiﬁc basis to design, evaluate, improve and compare visual notations, this paper analyses WebML according to a set of solid principles, based on the theoretical and empirical evidence concerning the cognitive effectiveness of visual notations. As a result, we have identiﬁed a set of possible improvements, some of which have been veriﬁed by an empirical study. Furthermore, a number of ﬁndings, experiences and lessons learnt on the assessment of visual notations are presented.",
        "keywords": [
            "Web modelling language (WebML)",
            "Visual notation",
            "Cognitive effectiveness",
            "Visual communication",
            "Visual syntax",
            "Concrete syntax"
        ],
        "authors": [
            "David Granada",
            "Juan Manuel Vara",
            "Marco Brambilla",
            "Verónica Bollati",
            "Esperanza Marcos"
        ],
        "file_path": "data/sosym-all/s10270-014-0447-8.pdf"
    },
    {
        "title": "SoSyM significantly reduces its backlog",
        "submission-date": "2019/02",
        "publication-date": "2019/02",
        "abstract": "SoSyM has been growing in two important areas: an increase in annual impact factor (IF) and number of submissions. The large uptick in the number of submissions is a very healthy sign for a journal, especially when coupled with a corresponding increase in IF. However, an increase in submissions also has an interesting side effect that can hamper the ability to process accepted articles in a timely manner. Editors and publishers are challenged with estimating the future set of incoming submitted papers. Also, the perception of a journal decreases when the number of papers published per issue is observed as decreasing or near empty. Publishers must carefully balance the number of issues released per year and the overall number of pages printed to match the projected incoming submissions.",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-019-00726-z.pdf"
    },
    {
        "title": "XTraQue: traceability for product line systems",
        "submission-date": "2006/09",
        "publication-date": "2007/09",
        "abstract": "Product line engineering has been increasingly\nused to support the development and deployment of soft-\nware systems that share a common set of features and are\ndeveloped based on the reuse of core assets. The large num-\nber and heterogeneity of documents generated during the\ndevelopment of product line systems may cause difﬁculties\nto identify common and variable aspects among applications,\nand to reuse core assets that are available under the product\nline. In this paper, we present a traceability approach for\nproduct line systems. Traceability has been recognised as an\nimportant task in software system development. Traceabil-\nity relations can improve the quality of the product being\ndeveloped and reduce development time and cost. We pres-\nent a rule-based approach to support automatic generation of\ntraceability relations between feature-based object-oriented\ndocuments. We deﬁne a traceability reference model with\nnine different types of traceability relations for eight types of\ndocuments. The traceability rules used in our work are clas-\nsiﬁed into two groups namely (a) direct rules, which support\nthe creation of traceability relations that do not depend on\nthe existence of other relations, and (b) indirect rules, which\nrequire the existence of previously generated relations. The\ndocuments are represented in XML and the rules are repre-\nsented in an extension of XQuery. A prototype tool called\nXTraQue has been implemented. This tool, together with a\nmobile phone product line case study, has been used to dem-\nonstrate and evaluate our work in various experiments. The\nresults of these experiments are encouraging and comparable\nwith other approaches that support automatic generation of\ntraceability relations.",
        "keywords": [
            "Software traceability",
            "Product line",
            "Traceability relations",
            "Traceability rules",
            "Feature-based\nobject-oriented documents"
        ],
        "authors": [
            "Waraporn Jirapanthong",
            "Andrea Zisman"
        ],
        "file_path": "data/sosym-all/s10270-007-0066-8.pdf"
    },
    {
        "title": "Metamodeling live sequence charts for code generation",
        "submission-date": "2007/11",
        "publication-date": "2009/02",
        "abstract": "This article presents a metamodeling study for Live Sequence Charts (LSCs) and Message Sequence Charts (MSCs) with an emphasis on code generation. The article discusses specifically the following points: the approach to building a metamodel for MSCs and LSCs, a metamodel extension from MSC to LSC, support for model-based code generation,andﬁnallyactionmodelanddomain-speciﬁcdata model integration. The metamodel is formulated in meta-GME, the metamodel language for the Generic Modeling Environment.",
        "keywords": [
            "Metamodeling",
            "Code generation",
            "Message sequence charts",
            "Live sequence charts"
        ],
        "authors": [
            "Okan Topçu",
            "Mehmet Adak",
            "Halit O˘guztüzün"
        ],
        "file_path": "data/sosym-all/s10270-009-0113-8.pdf"
    },
    {
        "title": "Toward an execution system for self-healing workﬂows in cyber-physical systems",
        "submission-date": "2015/10",
        "publication-date": "2016/08",
        "abstract": "Cyber-physical systems (CPS) represent a new class of information system that also takes real-world data and effects into account. Software-controlled sensors, actuators and smart objects enable a close coupling of the cyber and physical worlds. Introducing processes into CPS to automate repetitive tasks promises advantages regarding resource utilization and ﬂexibility of control systems for smart spaces. However, process execution systems face new challenges when being adapted for process execution in CPS: the automated processing of sensor events and data, the dynamic invocation of services, the integration of human interaction, and the synchronization of the cyber and physical worlds. Current workﬂow engines fulﬁll these requirements only to a certain degree. In this work, we present PROtEUS—an integrated system for process execution in CPS. PROtEUS integrates components for event processing, data routing, dynamic service selection and human interaction on the modeling and execution level. It is the basis for executing self-healing model-based workﬂowsin CPS. We demonstrate the applicability of PROtEUS within two case studies from the Smart Home domain and discuss its feasibility for introducing workﬂows into cyber-physical systems.",
        "keywords": [
            "Process execution",
            "Cyber-physical systems",
            "Workﬂow system",
            "Internet of things",
            "System architecture",
            "Middleware",
            "Event processing"
        ],
        "authors": [
            "Ronny Seiger",
            "Steffen Huber",
            "Thomas Schlegel"
        ],
        "file_path": "data/sosym-all/s10270-016-0551-z.pdf"
    },
    {
        "title": "Requirements speciﬁcation using templates: a model-driven approach",
        "submission-date": "2024/05",
        "publication-date": "2025/01",
        "abstract": "Requirements speciﬁcation and veriﬁcation are crucial processes of software development. These processes are particularly costly for safety critical systems due to the high number of requirements and their complexity. For such systems, it is important to use natural language for the speciﬁcation, as requirements need to be readable by non-technical stakeholders and certiﬁcation agents. To mitigate the inherent ambiguity caused by the use of natural language, controlled natural languages (CNL) are introduced as a means to constrain the speciﬁcation while maintaining readability. In this paper, we leverage model-driven engineering (MDE) to propose RESPECT, REquirements SPECiﬁcation using Templates, a CNL-based approach for requirements speciﬁcation and veriﬁcation. The fundamental idea of RESPECT is to use MDE techniques to: 1) model requirements’ templates and thus ease their creation, implementation and evolution and 2) link the template models to existing domain models to support, to some extent, requirements veriﬁcation and auto-ﬁlling. We provide a systematic process for the creation of customizable and reusable templates, which, to the best of our knowledge, represents a novel contribution. The application of this systematic process to a subset of the ARINC-653 standard from the avionics domain, resulted in seven templates that cover various types of requirements. We developed a tool, called MD-RSuT, that supports the speciﬁcation of requirements using the seven templates created for ARINC-653, and the automated veriﬁcation and auto-ﬁlling of requirements using an ARINC domain model. We evaluated the applicability of the approach across domains, and its effectiveness in improving requirements quality in terms of necessity, unambiguity, completeness, singularity, and veriﬁability. To do so, we applied the approach on three case studies coming from different domains, namely avionics, automotive, and general purpose software. This evaluation encompasses over a thousand requirements. We also performed a quantitative and qualitative analysis of the results. The results show that RESPECT is applicable across domains, and it yields requirements with higher quality.",
        "keywords": [
            "Model-driven engineering",
            "Requirements speciﬁcation",
            "Requirements veriﬁcation",
            "Controlled natural language",
            "Requirement templates",
            "Domain models",
            "Safety critical systems"
        ],
        "authors": [
            "Ikram Darif",
            "Ghizlane El Boussaidi",
            "Sègla Kpodjedo"
        ],
        "file_path": "data/sosym-all/s10270-025-01265-6.pdf"
    },
    {
        "title": "Partitioning of perfect synchroneous reactive speciﬁcations to distributed processors using µ-Charts",
        "submission-date": "2004/10",
        "publication-date": "2005/11",
        "abstract": "In this contribution, it is shown that perfect syn-chroneous speciﬁcations can be partitioned to and imple-mented on a distributed processor network. To this end, we introduce a lean visual formalism, called µ-Charts, that is similar to the speciﬁcation language Statecharts. This for-malism consists of fewer syntactic constructs than State-charts. Further syntax like hierarchical decomposition can be derived by means of syntactic abbreviation. µ-Charts’ semantics is based on the assumption of perfect synchrony. This paper is one of several contributions in this context; it gives a formal background and concentrates on the ques-tion how to use perfect synchroneous, state-based descrip-tion techniques as a basis for distributed implementations. The main contribution presented in this article is that the (formal and compositional) semantics of a perfect synchro-neous speciﬁcation is preserved when it is partitioned and implemented on distributed processors. We prove a theorem which guarantees that the communication ﬂow between dis-tributed parts of a perfect synchroneous speciﬁcation stabi-lizes in a ﬁxed point, i.e. terminates, independently of the processor speeds.",
        "keywords": [
            "Reactive systems",
            "Distributed systems",
            "Statecharts",
            "µ-charts",
            "Perfect synchrony",
            "Partitioning",
            "Fixed point semantics"
        ],
        "authors": [
            "Peter Scholz"
        ],
        "file_path": "data/sosym-all/s10270-005-0094-1.pdf"
    },
    {
        "title": "Automatic generation of UML proﬁle graphical editors for Papyrus",
        "submission-date": "2019/03",
        "publication-date": "2020/08",
        "abstract": "UML proﬁles offer an intuitive way for developers to build domain-speciﬁc modelling languages by reusing and extending\nUML concepts. Eclipse Papyrus is a powerful open-source UML modelling tool which supports UML proﬁling. However, with\npower comes complexity, implementing non-trivial UML proﬁles and their supporting editors in Papyrus typically requires\nthe developers to handcraft and maintain a number of interconnected models through a loosely guided, labour-intensive and\nerror-prone process. We demonstrate how metamodel annotations and model transformation techniques can help manage\nthe complexity of Papyrus in the creation of UML proﬁles and their supporting editors. We present Jorvik, an open-source\ntool that implements the proposed approach. We illustrate its functionality with examples, and we evaluate our approach by\ncomparing it against manual UML proﬁle speciﬁcation and editor implementation using a non-trivial enterprise modelling\nlanguage (Archimate) as a case study. We also perform a user study in which developers are asked to produce identical editors\nusing both Papyrus and Jorvik demonstrating the substantial productivity and maintainability beneﬁts that Jorvik delivers.",
        "keywords": [
            "Model-driven engineering",
            "UML proﬁling",
            "Papyrus"
        ],
        "authors": [
            "Ran Wei",
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Simos Gerasimou",
            "Dimitrios S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-020-00813-6.pdf"
    },
    {
        "title": "Modelling guidance in software engineering: a systematic literature review",
        "submission-date": "2022/06",
        "publication-date": "2023/07",
        "abstract": "Despite potential beneﬁts in Software Engineering, adoption of software modelling in industry is low. Technical issues such as tool support have gained signiﬁcant research before, but individual guidance and training have received little attention. As a ﬁrst step towards providing the necessary guidance in modelling, we conduct a systematic literature review to explore the current state of the art. We searched academic literature for guidance on model creation and selected 35 papers for full-text screening through three rounds of selection. We ﬁnd research on model creation guidance to be fragmented, with inconsistent usage of terminology, and a lack of empirical validation or supporting evidence. We outline the different dimensions commonly used to provide guidance on software and system model creation. Additionally, we provide deﬁnitions of the three terms modelling method, style, and guideline as current literature lacks a well-deﬁned distinction between them. These deﬁnitions can help distinguishing between important concepts and provide precise modelling guidance.",
        "keywords": [
            "Modelling styles",
            "Modelling training",
            "Modelling guidance",
            "Modelling method",
            "Systematic literature review"
        ],
        "authors": [
            "Shalini Chakraborty",
            "Grischa Liebel"
        ],
        "file_path": "data/sosym-all/s10270-023-01117-1.pdf"
    },
    {
        "title": "Understanding the need for assistance in software modeling: interviews with experts",
        "submission-date": "2022/03",
        "publication-date": "2023/05",
        "abstract": "Software modeling has shown for many years that it brings many advantages at the cost of various efforts and constraints. A large corpus of literature has indeed grown up over the years, pointing out the problems related to the modeling abstraction process, the usability of tools, or the practical difﬁculty of using modeling languages. While these works identify problems, few of them focus on proposing directions to explore in order to ﬁx them. To move toward a smoother and less constraining modeling experience and then increase the added value of modeling approaches, it is necessary to identify new paths to improve current tooling. In this paper, we explore one speciﬁc path by investigating how new software assistance features could support users performing modeling tasks that they perceive as complex. We used UML knowledge as a criterion for the selection of participants and built a questionnaire general to software modeling. We followed a user-centered research approach and collected the feedback from practitioners who use the modeling languages and the modeling tools on a regular basis in an industrial context. This article reports on a set of individual interview sessions with 16 modeling experts about how they perform modeling and how they imagine assistance in the context of their work. From the analysis of this qualitative study, we draw twelve observations on how to design software assistants for software modeling. These observations highlight research directions for both tool vendors and academics to explore, to identify and design new solutions to the friction points of the software modeling experience.",
        "keywords": [
            "Software modeling",
            "Practitioners",
            "Software assistant",
            "Interviews"
        ],
        "authors": [
            "Maxime Savary-Leblanc",
            "Xavier Le Pallec",
            "Sébastien Gérard"
        ],
        "file_path": "data/sosym-all/s10270-023-01104-6.pdf"
    },
    {
        "title": "Certifying delta-oriented programs",
        "submission-date": "2017/04",
        "publication-date": "2018/12",
        "abstract": "A major design concern in modern software development frameworks is to ensure that mechanisms for updating code running on remote devices comply with given safety speciﬁcations. This paper presents a delta-oriented approach for implementing product lines where software reuse is achieved at the three levels of state-diagram modeling, C/C++ source code and binary code. A safety speciﬁcation is expressed on the properties of reusable software libraries that can be dynamically loaded at run time after an over-the-air update. The compilation of delta-engineered code is certiﬁed using the framework of proof-carrying code in order to guarantee safety of software updates on remote devices. An empirical evaluation of the computational cost associated with formal safety checks is done by means of experimentation.",
        "keywords": [
            "Model-driven development",
            "Delta-oriented programming",
            "Safety properties",
            "Proof-carrying code",
            "Runtime systems"
        ],
        "authors": [
            "Vítor Rodrigues",
            "Simone Donetti",
            "Ferruccio Damiani"
        ],
        "file_path": "data/sosym-all/s10270-018-00704-x.pdf"
    },
    {
        "title": "Automatic generation of atomic multiplicity-preserving search operators for search-based model engineering",
        "submission-date": "2020/05",
        "publication-date": "2021/08",
        "abstract": "Recently, there has been increased interest in combining model-driven engineering and search-based software engineering. Such approaches use meta-heuristic search guided by search operators (model mutators and sometimes breeders) implemented as model transformations. The design of these operators can substantially impact the effectiveness and efﬁciency of the meta-heuristic search. Currently, designing search operators is left to the person specifying the optimisation problem. However, developing consistent and efﬁcient search-operator rules requires not only domain expertise but also in-depth knowledge about optimisation, which makes the use of model-based meta-heuristic search challenging and expensive. In this paper, we propose a generalised approach to automatically generate atomic multiplicity-preserving search operators for a given optimisation problem. This reduces the effort required to specify an optimisation problem and shields optimisation users from the complexity of implementing efﬁcient meta-heuristic search mutation operators. We evaluate our approach with a set of case studies and show that the automatically generated rules are comparable to, and in some cases better than, manually created rules at guiding evolutionary search towards near-optimal solutions.",
        "keywords": [
            "Model-driven optimisation",
            "Search-based software engineering",
            "Multi-objective optimisation"
        ],
        "authors": [
            "Alexandru Burdusel",
            "Steffen Zschaler",
            "Stefan John"
        ],
        "file_path": "data/sosym-all/s10270-021-00914-w.pdf"
    },
    {
        "title": "What makes a good process model? Lessons learned from process mining",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "There seems to be a never ending stream of\nnew process modeling notations. Some of these notations\nare foundational and have been around for decades (e.g.,\nPetri nets). Other notations are vendor speciﬁc, incremen-\ntal, or are only popular for a short while. Discussions on\nthe various competing notations concealed the more impor-\ntant question “What makes a good process model?”. Fortu-\nnately, large scale experiences with process mining allow us\nto address this question. Process mining techniques can be\nused to extract knowledge from event data, discover models,\nalign logs and models, measure conformance, diagnose bot-\ntlenecks, and predict future events. Today’s processes leave\nmany trails in data bases, audit trails, message logs, transac-\ntion logs, etc. Therefore, it makes sense to relate these event\ndata to process models independent of their particular nota-\ntion. Process models discovered based on the actual behavior\ntend to be very different from the process models made by\nhumans. Moreover, conformance checking techniques often\nreveal important deviations between models and reality. The\nlessons that can be learned from process mining shed a new\nlight on process model quality. This paper discusses the role\nof process models and lists seven problems related to process\nmodeling. Based on our experiences in over 100 process min-\ning projects, we discuss these problems. Moreover, we show\nthat these problems can be addressed by exposing process\nmodels and modelers to event data.",
        "keywords": [
            "Process mining",
            "Process modeling",
            "Process model quality"
        ],
        "authors": [
            "W. M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-012-0265-9.pdf"
    },
    {
        "title": "Computation orchestration",
        "submission-date": "2005/02",
        "publication-date": "2006/05",
        "abstract": "The widespread deployment of networked\napplications and adoption of the internet has fostered\n an environment in which many distributed services are\n available. There is great demand to automate business\n processes and workﬂows among organizations and indi-\n viduals. Solutions to such problems require orchestra-\n tion of concurrent and distributed services in the face of\n arbitrary delays and failures of components and com-\n munication. We propose a novel approach, called Orc\n for orchestration, that supports a structured model of\n concurrent and distributed programming. This model\n assumes that basic services, like sequential computa-\n tion and data manipulation, are implemented by prim-\n itive sites. Orc provides constructs to orchestrate the\n concurrent invocation of sites to achieve a goal – while\n managing time-outs, priorities, and failure of sites or\n communication.",
        "keywords": [
            "Wide-area computing",
            "Web services",
            "Computation orchestration",
            "Distributed computing",
            "Process algebra",
            "Thread-based programming"
        ],
        "authors": [
            "Jayadev Misra",
            "William R. Cook"
        ],
        "file_path": "data/sosym-all/s10270-006-0012-1.pdf"
    },
    {
        "title": "Formal speciﬁcation of non-functional properties of component-based software systems",
        "submission-date": "2007/09",
        "publication-date": "2009/02",
        "abstract": "Component-based software engineering (CBSE) is viewed as an opportunity to deal with the increasing complexity of modern-day software. Along with CBSE comes the notion of component markets, where more or less generic pieces of software are traded, to be combined into applications by third-party application developers. For such a component market to work successfully, all relevant properties of components must be precisely and formally described. This is especially true for non-functional properties, such as performance, memory foot print, or security. While the speciﬁcation offunctionalpropertiesiswellunderstood,non-functional properties areonlybeginningtobecomearesearch focus. This paper discusses semantic concepts for the speciﬁcation of non-functional properties, taking into account the speciﬁc needs of a component market. Based on these semantic concepts, we present a new speciﬁcation language QML/CS that can be used to model non-functional prod-uct properties of components and component-based software systems.",
        "keywords": [
            "Non-functional properties",
            "Formal speciﬁcation",
            "Component-based software engineering",
            "QML/CS"
        ],
        "authors": [
            "Steffen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-009-0115-6.pdf"
    },
    {
        "title": "Inferring physical units in formal models",
        "submission-date": "2014/02",
        "publication-date": "2015/03",
        "abstract": "Most state-based formal methods, like B, Event-B or Z, provide support for static typing. However, these methods and the associated tools lack support for annotating variables with (physical) units of measurement. There is thus no obvious way to reason about correct or incorrect usage of such units. We present a technique that analyzes the usage of physical units throughout B and Event-B machines infers missing units and notiﬁes the user of incorrectly handled units. The technique combines abstract interpretation with classical animation, constraint solving and model checking andhasbeenintegratedintotheProBvalidationtool,bothfor classicalBandforEvent-B.Itprovidessource-levelfeedback about errors detected in the models. We also describe how to extend our approach to TLA+, an untyped formal language. Weprovideanin-depthempirical evaluationanddemonstrate that our technique scales up to real-life industrial models.",
        "keywords": [
            "B-method",
            "Event-B",
            "Physical units",
            "Model checking",
            "Abstract interpretation"
        ],
        "authors": [
            "Sebastian Krings",
            "Michael Leuschel"
        ],
        "file_path": "data/sosym-all/s10270-015-0458-0.pdf"
    },
    {
        "title": "Template-based model generation",
        "submission-date": "2016/03",
        "publication-date": "2017/11",
        "abstract": "Given their vital roles in model-based software engineering, the performance of model-related operations (MOs, such as model transformations) must be systematically tested. However, how to produce a set of large input models that conform to structure-related constraints presents a major challenge to such test. This paper proposes a template-based approach to efﬁcient model generation. Firstly, a DSL is provided to describe templates that specify how to generate a valid model that conforms to structure-related constraints. Secondly, a folding semantic is deﬁned to converttemplatesintoawrappermetamodel.Thirdly,awrap-per model is generated using the existing model generators (e.g., a random model generator) according to the wrapper metamodel. Fourthly, an unfolding semantics is speciﬁed to translate the wrapper model into the desired test input. This paper also presents ﬁve case studies to evaluate the proposed approach, and the results demonstrate that such approach can generate large models based on structure-related constraints and facilitate the performance testing of MOs.",
        "keywords": [
            "Model generation",
            "Templates",
            "Performance testing",
            "Model-oriented operations",
            "Model-based engineering"
        ],
        "authors": [
            "Xiao He",
            "Tian Zhang",
            "Minxue Pan",
            "Zhiyi Ma",
            "Chang-Jun Hu"
        ],
        "file_path": "data/sosym-all/s10270-017-0634-5.pdf"
    },
    {
        "title": "On the modeling and generation of service-oriented tool chains",
        "submission-date": "2011/10",
        "publication-date": "2012/09",
        "abstract": "Tool chains have grown from ad-hoc solutions to complex software systems, which often have a service-oriented architecture. With service-oriented tool integration, development tools are made available as services, which can be orchestrated to form tool chains. Due to the increasing sophistication and size of tool chains, there is a need for a systematic development approach for service-oriented tool chains. We propose a domain-speciﬁc modeling language (DSML) that allows us to describe the tool chain on an appropriate level of abstraction. We present how this language supports three activities when developing service-oriented tool chains: communication, design and realization. A generative approach supports the realization of the tool chain using the service component architecture. We present experiences from an industrial case study, which applies the DSML to support the creation of a service-oriented tool chain. We evaluate the approach both qualitatively and quantitatively by comparing it with a traditional development approach.",
        "keywords": [
            "Domain speciﬁc modeling language",
            "Generative approach",
            "Service-oriented architecture",
            "Tool integration"
        ],
        "authors": [
            "Matthias Biehl",
            "Jad El-Khoury",
            "Frédéric Loiret",
            "Martin Törngren"
        ],
        "file_path": "data/sosym-all/s10270-012-0275-7.pdf"
    },
    {
        "title": "Addressing the evolution of automated user behaviour patterns by runtime model interpretation",
        "submission-date": "2012/04",
        "publication-date": "2013/09",
        "abstract": "The use of high-level abstraction models can facilitate and improve not only system development but also runtime system evolution. This is the idea of this work, in which behavioural models created at design time are also used at runtime to evolve system behaviour. These behavioural models describe the routine tasks that users want to be automated by the system. However, users’ needs may change after system deployment, and the routine tasks automated by the system must evolve to adapt to these changes. To facilitate this evolution, the automation of the speciﬁed routine tasks is achieved by directly interpreting the models at runtime. This turns models into the primary means to understand and interact with the system behaviour associated with the routine tasks as well as to execute and modify it. Thus, we provide tools to allow the adaptation of this behaviour by modifying the models at runtime. This means that the system behaviour evolution is performed by using high-level abstractions and avoiding the costs and risks associated with shutting down and restarting the system.",
        "keywords": [
            "System behaviour evolution",
            "Routine task automation",
            "Models at runtime",
            "Runtime interpretation of models"
        ],
        "authors": [
            "Estefanía Serral",
            "Pedro Valderas",
            "Vicente Pelechano"
        ],
        "file_path": "data/sosym-all/s10270-013-0371-3.pdf"
    },
    {
        "title": "Non-functional properties in the model-driven development of service-oriented systems",
        "submission-date": "2009/01",
        "publication-date": "2010/03",
        "abstract": "Systems based on the service-oriented architecture (SOA) principles have become an important cornerstone of the development of enterprise-scale software applications. They are characterized by separating functions into distinct software units, called services, which can be published, requested and dynamically combined in the production of business applications. Service-oriented systems (SOSs) promise high ﬂexibility, improved maintainability, and simple re-use of functionality. Achieving these properties requires an understanding not only of the individual arti-facts of the system but also their integration. In this context, non-functional aspects play an important role and should be analyzed and modeled as early as possible in the development cycle. In this paper, we discuss modeling of non-functional aspects of service-oriented systems, and the use of these models for analysis and deployment. Our contribution in this paper is threefold. First, we show how services and service compositions may be modeled in UML by using a proﬁle for SOA (UML4SOA) and how non-functional properties of service-oriented systems can be represented using the non-functional extension of UML4SOA (UML4SOA-NFP) and the MARTE proﬁle. This enables modeling of performance, security and reliable messaging. Second, we discuss formal analysis of models which respect this design, in particular we consider performance estimates and reliability analysis using the stochastically timed process algebra PEPA as the underlying analytical engine. Last but not least, our models are the source for the application of deployment mechanisms which comprise model-to-model and model-to-text transformations implemented in the framework VIATRA. All techniques presented in this work are illustrated by a running example from an eUniversity case study.",
        "keywords": [
            "Non-functional properties",
            "Service-oriented software",
            "SOA",
            "Modeling",
            "Model-driven engineering"
        ],
        "authors": [
            "Stephen Gilmore",
            "László Gönczy",
            "Nora Koch",
            "Philip Mayer",
            "Mirco Tribastone",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-010-0155-y.pdf"
    },
    {
        "title": "Process mining: a two-step approach to balance between underﬁtting and overﬁtting",
        "submission-date": "2008/04",
        "publication-date": "2010/09",
        "abstract": "Process mining includes the automated discovery of processes from event logs. Based on observed events (e.g., activities being executed or messages being exchanged) a process model is constructed. One of the essential problems in process mining is that one cannot assume to have seen all possible behavior. At best, one has seen a representative sub-set. Therefore, classical synthesis techniques are not suitable as they aim at ﬁnding a model that is able to exactly reproduce the log. Existing process mining techniques try to avoid such “overﬁtting” by generalizing the model to allow for more behavior. This generalization is often driven by the representation language and very crude assumptions about complete-ness. As a result, parts of the model are “overﬁtting” (allow only for what has actually been observed) while other parts maybe“underﬁtting”(allowformuchmorebehaviorwithout strong support for it). None of the existing techniques enables the user to control the balance between “overﬁtting” and “underﬁtting”. To address this, we propose a two-step approach. First, using a conﬁgurable approach, a transition system is constructed. Then, using the “theory of regions”, the model is synthesized. The approach has been imple-mented in the context of ProM and overcomes many of the limitations of traditional approaches.",
        "keywords": [],
        "authors": [
            "W. M. P. van der Aalst",
            "V. Rubin",
            "H. M. W. Verbeek",
            "B. F. van Dongen",
            "E. Kindler",
            "C. W. Günther"
        ],
        "file_path": "data/sosym-all/s10270-008-0106-z.pdf"
    },
    {
        "title": "Three decades of the OO-Method: fostering conceptual-model software engineering",
        "submission-date": "2025",
        "publication-date": "2025",
        "abstract": "The consolidation of the object-oriented (OO) programming paradigm in the early 1990s sparked a growing need for methodological support, leading to significant advancements in software engineering. Conceptual modeling took advantage of this OO-based wave to provide modeling techniques that were designed to incorporate that expressiveness. This period emphasized the precise conceptualization of complex problem domains as a foundational step toward developing robust technological solutions and formalizing practices that had been evolving since the 1980s. The OO-Method marked a significant milestone by introducing a model-driven approach to generate functional software directly from models defining the system’s structure, behavior, logic, and presentation. This innovation led to the development of a supporting tool capable of automating software generation, which has been continuously updated since the 2000s to align with evolving technologies. Additionally, the integration of requirements engineering approaches expanded the method’s scope, enhancing its ability to capture and formalize system needs with semantic consistency. This work reviews the evolution of the OO-Method, its derived and integrated initiatives, and the scientific studies that have historically validated its contributions. Adopting a historical perspective, the review explores the present and future of model-driven software engineering, from code generation based on software models to the strategic alignment of agile and current challenges for leveraging generative artificial intelligence techniques.",
        "keywords": [
            "Model-driven development",
            "Conceptual modeling",
            "Strategy modeling",
            "Model compiler"
        ],
        "authors": [
            "Oscar Pastor",
            "Rene Noel",
            "Jose Ignacio Panach"
        ],
        "file_path": "data/sosym-all/s10270-025-01299-w.pdf"
    },
    {
        "title": "Multidimensional context modeling applied to non-functional analysis of software",
        "submission-date": "2016/10",
        "publication-date": "2017/12",
        "abstract": "Context awareness is a ﬁrst-class attribute of today software systems. Indeed, many applications need to be aware of their context in order to adapt their structure and behavior for offering the best quality of service even in case the software and hardware resources are limited. Modeling the context, its evolution, and its inﬂuence on the services provided by (possibly resource constrained) applications are becoming primary activities throughout the whole software life cycle, although it is still difﬁcult to capture the multidimensional nature of context. We propose a framework for modeling and reasoning on the context and its evolution along multiple dimensions. Our approach enables (1) the representation of dependencies among heterogeneous context attributes through a formally deﬁned semantics for attribute composition and (2) the stochastic analysis of context evolution. As a result, context can be part of a model-based software development process, and multidimensional context analysis can be used for different purposes, such as non-functional analysis. We demonstrate how certain types of analysis, not feasible with context-agnostic approaches, are enabled in our framework by explicitly representing the interplay between context evolution and non-functional attributes. Such analyses allow the identiﬁcation of critical aspects or design errors that may not emerge without jointly taking into account multiple context attributes. The framework is shown at work on a case study in the eHealth domain.",
        "keywords": [
            "Context modeling",
            "Context evolution",
            "Reliability",
            "Performance",
            "Transient and steady-state analysis"
        ],
        "authors": [
            "Luca Berardinelli",
            "Marco Bernardo",
            "Vittorio Cortellessa",
            "Antinisca Di Marco"
        ],
        "file_path": "data/sosym-all/s10270-017-0645-2.pdf"
    },
    {
        "title": "The hidden models of model checking",
        "submission-date": "2011/12",
        "publication-date": "2012/08",
        "abstract": "In the past, applying formal analysis, such as model checking, to industrial problems required a team of formal methods experts and a great deal of effort. Model checking has become popular, because model checkers have evolved to allow domain-experts, who lack model check-ing expertise, to analyze their systems. What made this shift possible and what roles did models play in this? That is the main question we consider here. We survey approaches that transform domain-speciﬁc input models into alternative forms that are invisible to the user and which are amenable to model checking using existing techniques—we refer to these as hidden models. We observe that keeping these mod-els hidden from the user is in fact paramount to the success of the domain-speciﬁc model checker. We illustrate the value of hidden models by surveying successful examples of their use in different areas of model checking (hardware and soft-ware) and how a lack of suitable models hamper a new area (biological systems).",
        "keywords": [
            "Model checking",
            "Models temporal logic",
            "Biological systems"
        ],
        "authors": [
            "Willem Visser",
            "Matthew B. Dwyer",
            "Michael Whalen"
        ],
        "file_path": "data/sosym-all/s10270-012-0281-9.pdf"
    },
    {
        "title": "Supporting aspect orientation in business process management\nFrom process modelling to process enactment",
        "submission-date": "2015/01",
        "publication-date": "2015/09",
        "abstract": "Coping with complexity is an important issue in both research and industry. One strategy to deal with complexity is separation of concerns, which can be addressed using aspect-oriented paradigm. Despite being well researched in programming, this paradigm is still in a preliminary stage in the area of business process management (BPM). While some efforts have been made to introduce aspect orientation in business process modelling, there is no holistic approach with a formal underlying foundation to support aspect-oriented business process design and enactment, and this gap restricts aspect-oriented paradigm from being practically deployed in the area of BPM. Therefore, this paper proposes a sound systematic approach which builds on a formal syntax for modelling aspect-oriented business processes and a Petri Net-based operational semantics for enacting these processes. The approach enables the implementation of software system artefacts as a proof of concept to support design and enactment of aspect-oriented business processes in practice. The approach is demonstrated using a banking case study, where processes are modelled using a concrete notation that conforms to the proposed formal syntax and then executed in a state-of-the-art BPM system where the implemented artefacts are deployed.",
        "keywords": [
            "Business process management",
            "Aspect-oriented decomposition",
            "Process modelling",
            "Process enactment",
            "Weaving",
            "Cross-cutting concerns"
        ],
        "authors": [
            "Amin Jalali",
            "Chun Ouyang",
            "Petia Wohed",
            "Paul Johannesson"
        ],
        "file_path": "data/sosym-all/s10270-015-0496-7.pdf"
    },
    {
        "title": "Editorial",
        "submission-date": "2003/02",
        "publication-date": "2003/02",
        "abstract": "This editorial welcomes readers to the first 2003 issue of the Software and System Modeling (SoSyM) journal. It discusses the successful launch of the journal in 2002 and positive feedback received. It outlines the journal's aims to be a premier source of high-quality papers on modeling IT-based systems and highlights the content of this issue, including revised papers from the Modellierung 2002 workshop and a paper on object-relational database design using UML.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-003-0021-2.pdf"
    },
    {
        "title": "Model driven architecture: Principles and practice",
        "submission-date": "2004/08",
        "publication-date": "2004/08",
        "abstract": "Model Driven Architecture (MDA1) is an ap-\nproach to application modeling and generation that has\nreceived a lot of attention in recent months. Champi-\noned by the Object Management Group (OMG), many\norganizations are now looking at the ideas of MDA as\na way to organize and manage their application solutions,\ntool vendors are explicitly referring to their capabilities\nin terms of “MDA compliance”, and the MDA lexicon\nof platform-speciﬁc and platform-independent models is\nnow widely referenced in the industry.\nIn spite of this interest and market support, there is\nlittle clear guidance on what MDA means, where we are\nin its evolution, what is possible with today’s technology,\nand how to take advantage of it in practice. This paper ad-\ndresses that need by providing an analysis of how modeling\nis used in industry today, the relevance of MDA to today’s\nsystems, a classiﬁcation of MDA tooling support, and ex-\namples of its use. The paper concludes with a set of recom-\nmendations for how MDA can be successful in practice.",
        "keywords": [
            "Software architecture",
            "Software design",
            "Uniﬁed Modeling Language (UML)"
        ],
        "authors": [
            "Alan W. Brown"
        ],
        "file_path": "data/sosym-all/s10270-004-0061-2.pdf"
    },
    {
        "title": "Tool support for reﬁnement of non-functional speciﬁcations",
        "submission-date": "2005/02",
        "publication-date": "2006/07",
        "abstract": "Model driven architecture (MDA) views\napplication development as a continuous transforma-\ntion of models of the target system. We propose a\nmethodology which extends this view to non-functional\nproperties. In previous publications we have shown how\nwe can use so-called context models to make the speciﬁ-\ncation of non-functional measurements independent of\ntheir application in concrete system speciﬁcations. We\nhave also shown how this allows us to distinguish two\nroles in the development process: the measurement de-\nsigner and the application designer.\nIn this paper we use the notion of context models\nto allow the measurement designer to provide mea-\nsurement deﬁnitions at different levels of abstraction.\nA measurement in our terminology is a non-functional\ndimension that can be constrained to describe a non-\nfunctional\nproperty.\nRequiring\nthe\nmeasurement\ndesigner to deﬁne transformations between context\nmodels, and applying them to measurement deﬁnitions,\nenables us to provide tool support for reﬁnement of non-\nfunctional constraints to the application designer. The\npaper presents the concepts for such tool support as well\nas a prototype implementation.",
        "keywords": [
            "Non-functional properties",
            "Model\ntransformation",
            "Reﬁnement",
            "CASE tool support"
        ],
        "authors": [
            "Simone Röttger",
            "Steffen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-006-0024-x.pdf"
    },
    {
        "title": "Models in simulation",
        "submission-date": "2016/07",
        "publication-date": "2016/07",
        "abstract": "In the presence of increasing complexity, simulation has become a vital tool for obtaining additional information about a subset of the world, such as the interactions of all entities, processes, life forms, and other key components that aid in understanding a particular context. A key advantage of simulation is the Principle of Substitution, which is perhaps best summarized by a quote from Marvin Minsky: We use the term “model” in the following sense: To an observer B, an object A∗is a model of an object A to the extent that B can use A∗to answer questions that interest him about A. It is understood that B’s use of a model entails the use of encodings for input and output, both for A and A∗. If A is the world, questions for A are experiments. A∗is a good model of A, in B’s view, to the extent that A∗’s answers agree with those of A’s, on the whole, with respect to the questions important to B. (Minsky 1965) The following deﬁnition offers several insights into the ben-eﬁts of simulation and the importance of a model toward understanding parts of a real system: Simulation is the imitation of the operation of a real-world process or system over time. The act of simulating something ﬁrst requires that a model be developed; this model represents the key character-istics or behaviors/functions of the selected physical or abstract system or process. The model represents B the system itself, whereas the simulation represents the operation of the system over time. Simulation is used in many contexts, such as simulation of technology for performance optimization, safety engineering, testing, training, education, and video games. Often, computer experiments are used to study simulation models. Sim-ulation is also used with scientiﬁc modeling of natural systems or human systems to gain insight into their functioning. (Wikipedia 2016)",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-016-0544-y.pdf"
    },
    {
        "title": "Heuristic search for equivalence checking",
        "submission-date": "2013/07",
        "publication-date": "2014/05",
        "abstract": "Equivalence checking plays a crucial role in formal veriﬁcation since it is a natural relation for expressing the matching of a system implementation against its speciﬁcation. In this paper, we present an efﬁcient procedure, based on heuristic search, for checking well-known bisimulation equivalences for concurrent systems speciﬁed through process algebras. The method tries to improve, with respect to other solutions, both the memory occupation and the time required for proving the equivalence of systems. A prototype has been developed to evaluate the approach on several examples of concurrent system speciﬁcations.",
        "keywords": [
            "Heuristic search algorithms",
            "Bisimulation",
            "Concurrent systems",
            "Model checking"
        ],
        "authors": [
            "Nicoletta De Francesco",
            "Giuseppe Lettieri",
            "Antonella Santone",
            "Gigliola Vaglini"
        ],
        "file_path": "data/sosym-all/s10270-014-0416-2.pdf"
    },
    {
        "title": "Cyber-physical systems challenges: a needs analysis for collaborating embedded software systems",
        "submission-date": "2015/01",
        "publication-date": "2016/01",
        "abstract": "Embedding computing power in a physical environment has provided the functional flexibility and performance necessary in modern products such as automobiles, aircraft, smartphones, and more. As product features came to increasingly rely on software, a network infrastructure helped factor out common hardware and offered sharing functionality for further innovation. A logical consequence was the need for system integration. Even in the case of a single original end manufacturer who is responsible for the final product, system integration is quite a challenge. More recently, there have been systems coming online that must perform system integration even after deployment—that is, during operation. This has given rise to the cyber-physical systems (CPS) paradigm. In this paper, select key enablers for a new type of system integration are discussed. The needs andchallengesfordesigningandoperatingCPSareidentiﬁed along with corresponding technologies to address the chal- lenges and their potential impact. The intent is to contribute to a model-based research agenda in terms of design methods, implementation technologies, and organization challenges necessary to bring the next-generation systems online.",
        "keywords": [
            "Cyber-physical systems",
            "Computation",
            "Embedded systems",
            "Challenges",
            "Internet of Things",
            "Modeling and simulation"
        ],
        "authors": [
            "Pieter J. Mosterman",
            "Justyna Zander"
        ],
        "file_path": "data/sosym-all/s10270-015-0469-x.pdf"
    },
    {
        "title": "Assessing model quality",
        "submission-date": "2004/08",
        "publication-date": "2004/08",
        "abstract": "Students in software engineering courses that cover modeling often ask some variant of the following question: “How do I know that my model is a good model?”. It is not easy to provide a satisfactory response to this question. Good instructors provide students with some criteria and guidelines in the form of patterns (e.g., Craig Larman’s GRASP patterns), rules of thumb (e.g., “minimize coupling, maximize cohesion”, “keep inheritance depth shallow”), and exemplar models to better understand good modeling practices. While these help, the reality is that students ultimately rely on feedback from their instructors to determine the quality of their models. The instructors play the role of expert modelers and the students are their apprentices. The state of the practice in assessing model quality in the classroom and in industry seems to indicate that modeling is still in the craftsmanship phase.\nResearch on rigorous assessment of model quality has given us a glimpse of how we can progress to the next phase in which models are engineered. A number of researchers are working on developing rigorous static analysis techniques that are based on well-deﬁned models of behavior. Articles on model-checking of modeled behavior published in SoSyM are a good reﬂection of the work in this area. Another promising area of research is systematic model testing (i.e., systematic dynamic analysis of modeled behavior). Systematic dynamic analysis of code (i.e., code testing) involves executing programs on a selected set of test inputs that satisfy some test criteria. These ideas can be extended to the modeling phases when models with operational semantics are used. Most educators in the modeling community have heard students gripe about their inability to animate or execute the models they have created in order to explore the behavior they have modeled. Model testing is concerned with providing modelers with this ability. Systematic model testing techniques provide opportunities for automating the testing process and for reusing tests. Systematic regression testing techniques in particular can enable more rigorous model evolution. The notion of model testing is not new. For example, SDL (Speciﬁcation and Description Language) tools of provide facilities for exercising the state-machine based SDL models using an input set of test events. Work on executable variants of the UML also aims to provide modelers with feedback on the adequacy of their models. More recently a small, but growing, number of researchers have begun looking at developing systematic model testing techniques. This is an important area of research and helps pave the way towards more effective use of models during software development. There are a number of lessons from the systematic code testing community that can be applied, but the peculiarities of modeling languages also requires the development of new and innovative approaches. In particular, innovative work on deﬁning eﬀective test criteria that are based on coverage of model elements and on the generation of model-level test cases that provide desired levels of coverage is needed. It is also useful to look at how other engineering disciplines determine the quality of their models. Engineers in other disciplines typically explore answers to the following questions when determining the adequacy of their models: Is the model a good predictor of how the physical artifact will behave? What are the (simplifying) assumptions underlying the model and what impact will they have on actual behavior? The answer to the first question is often based on evidence gathered from past applications of the model. Evidence of model fidelity is built up by comparing the actual behavior of systems built using the models with the behavior predicted by the models. Each time engineers build a system the experience gained either reinforces their confidence in the predictive power of the models used or the experience is used to improve the predictive power of models.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-004-0068-8.pdf"
    },
    {
        "title": "On building location aware applications using an open platform based on the NEXUS Augmented World Model",
        "submission-date": "2002/09",
        "publication-date": "2004/04",
        "abstract": "How should the World Wide Web look like if it were for location-based information? And how would mobile, spatially aware applications deal with such a platform? In this paper we present the neXus Augmented World Model, an object oriented data model which plays a major role in an open framework for both providers of location-based information and new kinds of applications: the neXus platform. We illustrate the usability of the model with several sample applications and show the extensibility of this framework. At last we present a step-wise approach for building spatially aware applications in this environment.",
        "keywords": [
            "Location-awawe",
            "Infrastructure",
            "Augmented world model",
            "Nexus",
            "Open platform"
        ],
        "authors": [
            "Daniela Nicklas",
            "Bernhard Mitschang"
        ],
        "file_path": "data/sosym-all/s10270-004-0055-0.pdf"
    },
    {
        "title": "Instant and global consistency checking during collaborative engineering",
        "submission-date": "2020/12",
        "publication-date": "2022/04",
        "abstract": "Engineering projects involve a variety of artifacts such as requirements, design, or source code. These artifacts, many of which tend to be interdependent, are often manipulated concurrently. To keep artifacts consistent, engineers must continuously consider their work in relation to the work of multiple other engineers. Traditional consistency checking approaches reason efficiently over artifact changes and their consistency implications. However, they do so solely within the boundaries of specific tools and their specific artifacts (e.g., consistency checking between different UML models). This makes it difficult to examine the consistency between different types of artifacts (e.g., consistency checking between UML models and the source code). Global consistency checking can help addressing this problem. However, it usually requires a disruptive and time-consuming merging process for artifacts. This article presents a novel, cloud-based approach to global consistency checking in a multi-developer/-tool engineering environment. It allows for global consistency checking across all artifacts that engineers work on concurrently. Moreover, it reasons over artifact changes immediately after the change happened, while keeping the (memory/CPU) cost of consistency checking minimal. The feasibility and scalability of our approach were demonstrated by a prototype implementation and through an empirical validation.",
        "keywords": [
            "Consistency checking",
            "Multi-developer environment",
            "Model-driven engineering"
        ],
        "authors": [
            "Michael Alexander Tröls",
            "Luciano Marchezan",
            "Atif Mashkoor",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-022-00984-4.pdf"
    },
    {
        "title": "A model-driven framework for developing multi-agent systems in emergency response environments",
        "submission-date": "2016/12",
        "publication-date": "2017/10",
        "abstract": "In emergency response environments, variant entities with speciﬁc behaviors and interaction between them form a complex system that can be well modeled by multi-agent systems. To build such complex systems, instead of writing the code from scratch, one can follow the model-driven development approach, which aims to generate software from design models automatically. To achieve this goal, two important prerequisites are: a domain-speciﬁc modeling language for designing an emergency response environment model, and transformation programs for automatic code generation from a model. In addition, for modeling with the language, a modeling tool is required, and for executing the generated code there is a need to a platform. In this paper, a model-driven framework for developing multi-agent systems in emergency response environments is provided which includes several items. A domain-speciﬁc modeling language as well as a modeling tool is developed for this domain. The language and the tool are called ERE-ML and ERE-ML Tool, respectively. Using the ERE-ML Tool, a designer can model an emergency response situation and then validate the model against the predeﬁned constraints. Furthermore, several model to code transformations are deﬁned for automatic multi-agent system code generation from an emergency response environment model. For executing the generated code, an extension of JAMDER platform is also provided. To evaluate our framework, several case studies including the Victorian bushﬁre disaster are modeled to show theabilityoftheframeworkinmodelingreal-worldsituations and automatic transformation of the model into the code.",
        "keywords": [
            "Domain-speciﬁc modeling language",
            "Emergency response environment",
            "Multi-agent system",
            "Model-driven development",
            "ERE-ML",
            "Model to code transformation"
        ],
        "authors": [
            "Samaneh HoseinDoost",
            "Tahereh Adamzadeh",
            "Bahman Zamani",
            "Afsaneh Fatemi"
        ],
        "file_path": "data/sosym-all/s10270-017-0627-4.pdf"
    },
    {
        "title": "CooPS – Towards a method for coordinating personalized services",
        "submission-date": "2004/04",
        "publication-date": "2006/02",
        "abstract": "This paper presents CooPS, which is a method for Coordinating Personalized Services. These services are primarily offered to mobile users. The concept of services is the object of intense investigations from both academia and industry. However, very little has been accomplished so far regarding ﬁrst, personalizing services for the beneﬁt of mobile users, and second, providing the appropriate methodological support for those (i.e., designers) who will be specifying the operations of personalization. Various obstacles still exist such as lack of techniques for modeling and specifying the integration of personalization into services, and existing approaches for service composition typically facilitate orchestration only, while neglecting contexts of users and services. CooPS consists of several steps ranging from service deﬁnition and personalization to service deployment. Each step has some representation techniques, which aim at facilitating the speciﬁcation and validation of the operations of coordinating personalized services.",
        "keywords": [
            "Service",
            "Coordination",
            "Personalization",
            "Method"
        ],
        "authors": [
            "Zakaria Maamar",
            "Djamal Benslimane",
            "Michael Mrissa",
            "Chirine Ghedira"
        ],
        "file_path": "data/sosym-all/s10270-006-0006-z.pdf"
    },
    {
        "title": "An operational guide to monitorability with applications to regular properties",
        "submission-date": "2020/02",
        "publication-date": "2021/02",
        "abstract": "Monitorability underpins the technique of runtime veriﬁcation because it delineates what properties can be veriﬁed at runtime. Although many monitorability deﬁnitions exist, few are deﬁned explicitly in terms of the operational guarantees provided by monitors, i.e. the computational entities carrying out the veriﬁcation. We view monitorability as a spectrum, where the fewer guarantees that are required of monitors, the more properties become monitorable. Accordingly, we present a monitorability hierarchy based on this trade-off. For regular speciﬁcations, we give syntactic characterisations in Hennessy–Milner logic with recursion for its levels. Finally, we map existing monitorability deﬁnitions into our hierarchy. Hence, our work gives a uniﬁed framework that makes the operational assumptions and guarantees of each deﬁnition explicit. This provides a rigorous foundation that can inform design choices and correctness claims for runtime veriﬁcation tools.",
        "keywords": [
            "Runtime Veriﬁcation",
            "Monitors",
            "Monitorability",
            "Logical Fragments"
        ],
        "authors": [
            "Luca Aceto",
            "Antonis Achilleos",
            "Adrian Francalanza",
            "Anna Ingólfsdóttir",
            "Karoliina Lehtinen"
        ],
        "file_path": "data/sosym-all/s10270-020-00860-z.pdf"
    },
    {
        "title": "Exploring the interaction of design variability and stochastic operational uncertainties in software-intensive systems through the lens of modeling",
        "submission-date": "2024/02",
        "publication-date": "2024/11",
        "abstract": "In software-intensive systems, navigating the complexities that emerge from the interaction of design variability and stochastic operational uncertainties presents a daunting challenge. This paper delves into the dynamics between these two dimensions of uncertainty, offering novel insights about how modeling can contribute to the analysis of their combined impact upon system properties. By elevating the abstraction level at which probabilistic models are conceptualized, our approach enables an integrated analysis framework that considers both structural and quantitative dimensions of design spaces. Through the introduction of novel language constructs, our methodology facilitates the direct referencing of structural relationships within probabilistic behavioral speciﬁcations. Furthermore, the adoption of novel quantiﬁers in probabilistic temporal logic enables evaluating complex properties across diverse design variants, thereby streamlining the assessment of guarantees within the solution space. We demonstrate the feasibility of this approach on four case studies, showcasing its potential to offer comprehensive insights into the trade-offs and decision-making processes inherent in managing different types of structural design variability and operational uncertainties in software-intensive systems.",
        "keywords": [
            "Design variability",
            "Operational uncertainty",
            "Uncertainty interaction",
            "Quantitative veriﬁcation"
        ],
        "authors": [
            "Javier Cámara"
        ],
        "file_path": "data/sosym-all/s10270-024-01226-5.pdf"
    },
    {
        "title": "Separation of non-orthogonal concerns in software architecture and design",
        "submission-date": "2004/01",
        "publication-date": "2006/01",
        "abstract": "Abstract Separation of concerns represents an important\nprinciple for managing complexity in the design and ar-\nchitecture of large component-based software systems. The\nfundamental approach is to develop local solutions for indi-\nvidual concerns ﬁrst, and combine them later into an overall\nsolution for the complete system. However, comprehensive\nsupport for the integration of interdependent, possibly con-\nﬂicting concerns related to synchronization behavior is still\nmissing. In our work, we propose a sound solution for this\ncomplex type of composition, employing well-known UML\ndescription techniques as well as a rigorous formal model of\ncomponent synchronization behavior. Based on this founda-\ntion, we describe a constructive synthesis algorithm which\nreliably detects conﬂicting concerns or generates a maxi-\nmal synchronization behavior for software components with\nmultiple interactions. An optimized implementation of the\nalgorithm has been integrated into a CASE tool to illustrate\nfeasibility and scalability of the presented technique to the\nexample of a moderately large case study.",
        "keywords": [
            "Separation of concern",
            "Software architecture",
            "Consistency",
            "Behavior synthesis",
            "Design by contract"
        ],
        "authors": [
            "Holger Giese",
            "Alexander Vilbig"
        ],
        "file_path": "data/sosym-all/s10270-005-0103-4.pdf"
    },
    {
        "title": "Synthesizing veriﬁed components for cyber assured systems engineering",
        "submission-date": "2022/03",
        "publication-date": "2023/03",
        "abstract": "Safety-critical systems such as avionics need to be engineered to be cyber resilient meaning that systems are able to detect and recover from attacks or safely shutdown. As there are few development tools for cyber resiliency, designers rely on guidelines and checklists, sometimes missing vulnerabilities until late in the process where remediation is expensive. Our solution is a model-based approach with cyber resilience-improving transforms that insert high-assurance components such as ﬁlters to block malicious data or monitors to detect and alarm anomalous behavior. Novel is our use of model checking and a veriﬁed compiler to specify, verify, and synthesize these components. We deﬁne code contracts as formal speciﬁcations that designers write for high-assurance components, and test contracts as tests to validate their behavior. A model checker proves whether or not code contracts satisfy test contracts in an iterative development cycle. The same model checker also proves whether or not a system with the inserted components, assuming they adhere to their code contracts, provides the desired cyber resiliency for the system. We deﬁne an algorithm to synthesize implementations for code contracts in a semantics-preserving way that is backed by a veriﬁed compiler. The entire workﬂow is implemented as part of the open source BriefCASE toolkit. We report on our experience using BriefCASE with a case study on a UAV system that is transformed to be cyber resilient to communication and supply chain cyber attacks. Our case study demonstrates that writing code contracts and then synthesizing correct implementations from them are feasible in real-world systems engineering for cyber resilience.",
        "keywords": [
            "AADL",
            "OSATE",
            "Assume-guarantee reasoning",
            "Synthesis",
            "Cyber hardening",
            "Formal veriﬁcation",
            "Model-based systems engineering"
        ],
        "authors": [
            "Eric Mercer",
            "Konrad Slind",
            "Isaac Amundson",
            "Darren Cofer",
            "Junaid Babar",
            "David Hardin"
        ],
        "file_path": "data/sosym-all/s10270-023-01096-3.pdf"
    },
    {
        "title": "Bootstrapping MDE development from ROS manual code: Part 2—Model generation and leveraging models at runtime",
        "submission-date": "2020/03",
        "publication-date": "2021/04",
        "abstract": "Model-driven engineering (MDE) addresses central aspects of robotics software development. MDE could enable domain experts to leverage the expressiveness of models, while implementation details on different hardware platforms would be handled by automatic code generation. Today, despite strong MDE efforts in the robotics research community, most evidence points to manual code development being the norm. A possible reason for MDE not being accepted by robot software developers could be the wide range of applications and target platforms, which make all-encompassing MDE IDEs hard to develop and maintain. Therefore, we chose to leverage a large corpus of open-source software widely adopted by the robotics community to extract common structures and gain insight on how and where MDE can support the developers to work more efﬁciently. We pursue modeling as a complement, rather than imposing MDE as separate solution. Our previous work introduced metamodels to describe components, their interactions, and their resulting composition. In this paper, we present two methods based on metamodels for automated generation of models from manually written artifacts: (1) through static code analysis and (2) by monitoring the execution of a running system. For both methods, we present tools that leverage the potentials of our contributions, with a special focus on their application at runtime to observe and diagnose a real system during its execution. A comprehensive example is provided as a walk-through for robotics software practitioners.",
        "keywords": [
            "ROS",
            "Models",
            "MDE",
            "Robotics"
        ],
        "authors": [
            "Nadia Hammoudeh García",
            "Harshavardhan Deshpande",
            "André Santos",
            "Björn Kahl",
            "Mirko Bordignon"
        ],
        "file_path": "data/sosym-all/s10270-021-00873-2.pdf"
    },
    {
        "title": "A formal component model for UML based on CSP aiming at compositional veriﬁcation",
        "submission-date": "2022/03",
        "publication-date": "2023/10",
        "abstract": "Model-based engineering emerged as an approach to tackle the complexity of current system development. In particular, compositional strategies assume that systems can be built from reusable and loosely coupled units. However, it is still a challenge to ensure that desired properties hold for component integration. We present a component-based model for UML, including a metamodel, well-formedness conditions and formal semantics via translation into BRIC; the presentation of the semantics is given by a set of rules that cover all the metamodel elements and map them to their respective BRIC denotations. We use our previous work on BRIC as an underlying (and totally hidden) component development framework so that our approach beneﬁts from all the formal infrastructure developed for BRIC using CSP. Component composition, speciﬁed via UML structural diagrams, ensures adherence to classical concurrent properties: our focus is on the preservation of deadlock freedom. Automated support is developed as a plug-in to the Astah modelling tool. Veriﬁcation is carried out using FDR (a model checker for CSP); we address scalability using compositional reasoning (inherent to the approach) and behavioural patterns. The formal reasoning is transparent to the user: a distinguishing feature of our approach is its support for traceability. For instance, when FDR uncovers a deadlock, a sequence diagram is constructed from the deadlock trace and presented to the user at the modelling level. The overall approach is illustrated with a running example and two additional case studies.",
        "keywords": [
            "CSP",
            "Component",
            "Compositional veriﬁcation",
            "UML",
            "Deadlock analysis"
        ],
        "authors": [
            "Flávia Falcão",
            "Lucas Lima",
            "Augusto Sampaio",
            "Pedro Antonino"
        ],
        "file_path": "data/sosym-all/s10270-023-01127-z.pdf"
    },
    {
        "title": "Dynamic constraint satisfaction problems over models",
        "submission-date": "2010/07",
        "publication-date": "2011/01",
        "abstract": "In early phases of designing complex systems, models are not sufficiently detailed to serve as an input for automated synthesis tools. Instead, a design space is constituted by multiple models representing different valid design candidates. Design space exploration aims at searching through these candidates defined in the design space to find solutions that satisfy the structural and numeric design constraints and provide a balanced choice with respect to various quality metrics. Design space exploration in an model-driven engineering (MDE) context is frequently tackled as specific sort of constraint satisfaction problem (CSP). In CSP, declarative constraints capture restrictions over variables with finite domains where both the number of variables and their domains are required to be a priori finite. However, the existing formulation of constraint satisfaction problems can be too restrictive to capture design space exploration in many MDE applications with complex structural constraints expressed over the underlying models. In this paper, we interpret flexible and dynamic constraint satisfaction problems directly in the context of models. These extensions allow the relaxation of constraints during a solving process and address problems that are subject to change and require incremental re-evaluation. Furthermore, we present our prototype constraint solver for the domain of graph models built upon the Viatra2 model transformation framework and provide an evaluation of its performance with comparison to related tools.",
        "keywords": [
            "Constraint satisfaction programming",
            "Graph transformation",
            "Dynamic constraint satisfaction programming",
            "Flexible constraint satisfaction problem"
        ],
        "authors": [
            "Ákos Horváth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-010-0185-5.pdf"
    },
    {
        "title": "Exchanging information in cooperative software validation",
        "submission-date": "2023/02",
        "publication-date": "2024/03",
        "abstract": "Cooperative software validation aims at having veriﬁcation and/or testing tools cooperate on the task of correctness checking. Cooperation involves the exchange of information about currently achieved results in the form of (veriﬁcation) artifacts. These artifacts are typically specialized to the type of analysis performed by the tool, e.g., bounded model checking, abstract interpretation or symbolic execution, and hence require the deﬁnition of a new artifact for every new cooperation to be built. In this article, we introduce a uniﬁed artifact (called Generalized Information Exchange Automaton, short GIA) supporting the cooperation of over-approximating with under-approximating analyses. It provides information gathered by an analysis to its partner in a cooperation, independent of the type of analysis and usage context within software validation. We provide a formal deﬁnition of this artifact in the form of an automaton together with two operators on GIAs. The ﬁrst operation reduces a program by excluding these parts, where the information that they are already processed is encoded in the GIA. The second operation combines partial results from two GIAs into a single on. We show that computed analysis results are never lost when connecting tools via these operations. To experimentally demonstrate the feasibility, we have implemented two such cooperation: one for veriﬁcation and one for testing. The obtained results show the feasibility of our novel artifact in different contexts of cooperative software validation, in particular how the new artifact is able to overcome some drawbacks of existing artifacts.",
        "keywords": [
            "Cooperative software veriﬁcation",
            "Veriﬁcation artifact",
            "Test case generation",
            "Component-based CEGAR"
        ],
        "authors": [
            "Jan Haltermann",
            "Heike Wehrheim"
        ],
        "file_path": "data/sosym-all/s10270-024-01155-3.pdf"
    },
    {
        "title": "Tracing security requirements in industrial control systems using graph databases",
        "submission-date": "2021/06",
        "publication-date": "2022/07",
        "abstract": "We must explicitly capture relationships and hierarchies between the multitude of system and security standards requirements. Current security requirements speciﬁcation methods do not capture such structure effectively, making requirements management and traceability harder, consequently increasing costs and time to market for developing certified ICS. We propose a novel requirements repository model for ICS that uses labelled property graphs to structure and store system-specific and standards-based requirements using well-defined relationship types. Furthermore, we integrate the proposed requirements repository with design-time ICS tools to establish requirements traceability. A wind turbine case study illustrates the overall workflow in our framework. We demonstrate that a robust requirements traceability matrix is a natural consequence of using labelled property graphs. We also introduce a compatible requirements change management procedure that aids in adapting to changes in development and certification schemes.",
        "keywords": [
            "Requirements engineering",
            "Security",
            "Requirements repository",
            "Security standards",
            "Industrial control systems",
            "Traceability",
            "Graph databases",
            "Labelled property graphs"
        ],
        "authors": [
            "Awais Tanveer",
            "Chandan Sharma",
            "Roopak Sinha",
            "Matthew M. Y. Kuo"
        ],
        "file_path": "data/sosym-all/s10270-022-01019-8.pdf"
    },
    {
        "title": "Supporting timing analysis of vehicular embedded systems through the refinement of timing constraints",
        "submission-date": "2016/03",
        "publication-date": "2017/01",
        "abstract": "The collective use of several models and tools at various abstraction levels and phases during the development of vehicular distributed embedded systems poses many challenges. Within this context, this paper targets the challenges that are concerned with the unambiguous refinement of timing requirements, constraints and other timing information among various abstraction levels. Such information is required by the end-to-end timing analysis engines to provide pre-run-time verification about the predictability of these systems. The paper proposes an approach to represent and refine such information among various abstraction levels. As a proof of concept, the approach provides a representation of the timing information at the higher levels using the models that are developed with EAST-ADL and Timing Augmented Description Language. The approach then refines the timing information for the lower abstraction levels. The approach exploits the Rubus Component Model at the lower level to represent the timing information that cannot be clearly specified at the higher levels, such as trigger paths in distributed chains. A vehicular-application case study is conducted to show the applicability of the proposed approach.",
        "keywords": [
            "Distributed embedded systems",
            "Component-based development",
            "Timing model",
            "Component model",
            "End-to-end timing analysis"
        ],
        "authors": [
            "Saad Mubeen",
            "Thomas Nolte",
            "Mikael Sjödin",
            "John Lundbäck",
            "Kurt-Lennart Lundbäck"
        ],
        "file_path": "data/sosym-all/s10270-017-0579-8.pdf"
    },
    {
        "title": "Deriving performance-relevant infrastructure properties through model-based experiments with Ginpex",
        "submission-date": "2012/01",
        "publication-date": "2013/03",
        "abstract": "To predict the performance of an application, it is crucial to consider the performance of the underlying infrastructure. Thus, to yield accurate prediction results, performance-relevant properties and behaviour of the infrastructure have to be integrated into performance models. However, capturing these properties is a cumbersome and error-prone task, as it requires carefully engineered measurements and experiments. Existing approaches for creating infrastructure performance models require manual coding of these experiments, or ignore the detailed properties in the models. The contribution of this paper is the Goal-oriented INfrastructure Performance EXperi-ments (Ginpex) approach, which introduces goal-oriented and model-based speciﬁcation and generation of executable performance experiments for automatically detecting and quantifying performance-relevant infrastructure properties. Ginpex provides a metamodel for experiment speciﬁcation and comes with predeﬁned experiment templates that provide automated experiment execution on the target platform and also automate the evaluation of the experiment results. We evaluate Ginpex using three case studies, where experiments are executed to quantify various infrastructure properties.",
        "keywords": [
            "Metamodelling",
            "Experiments",
            "Measurements",
            "Infrastructure",
            "Deriving infrastructure properties",
            "Performance prediction"
        ],
        "authors": [
            "Michael Hauck",
            "Michael Kuperberg",
            "Nikolaus Huber",
            "Ralf Reussner"
        ],
        "file_path": "data/sosym-all/s10270-013-0335-7.pdf"
    },
    {
        "title": "How to write a successful SoSyM submission",
        "submission-date": "2016/09",
        "publication-date": "2016/09",
        "abstract": "Authors of research papers often want to gain insight on how to improve the chances of their research being published. There have been several “how to” guides created for this purpose. Within its first five years, OOPSLA’s popularity grew to the point that there was a panel focused on the topic of how to get a paper accepted [1]. Mary Shaw’s tutorial/paper on how to write a good software engineering contribution was directed to an ICSE audience, but has many timeless suggestions that are still relevant in many general contexts [2]. This issue’s editorial discusses the SoSyM review process and why a paper might be rejected at the various stages of review.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-016-0558-5.pdf"
    },
    {
        "title": "Special section of BPMDS’2021 business process improvement",
        "submission-date": "2023/10",
        "publication-date": "2023/12",
        "abstract": "The Business Process Modeling, Development and Support (BPMDS) working conference series, held in conjunction with CAiSE conferences, serve as a meeting place for researchers and practitioners in Business Process Modeling, Development, and Support. Business process analysis, design, and support, addressed by the BPMDS series, have been recognized as a central issue in information systems (IS) engineering. In 2011, BPMDS became a two-day working conference held in conjunction with CAiSE (Conference on Advanced Information Systems Engineering). The goals, format, and history of BPMDS can be found on the website http://www.bpmds.org/. ",
        "keywords": [],
        "authors": [
            "Adriano Augusto",
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-023-01139-9.pdf"
    },
    {
        "title": "Exploiting model driven technology: a tale of two startups",
        "submission-date": "2012/01",
        "publication-date": "2012/08",
        "abstract": "This article describes the experiences of two independent start-up companies that were created in the white-heat of the early days of model-based engineering. Each company aimed to revolutionise software development by raising the level of abstraction through modelling. The article describes the context, technical innovations, business experiences, demise and lessons learned by each company.",
        "keywords": [
            "Startup",
            "Tool company",
            "Model driven development"
        ],
        "authors": [
            "Tony Clark",
            "Pierre-Alain Muller"
        ],
        "file_path": "data/sosym-all/s10270-012-0260-1.pdf"
    },
    {
        "title": "Testing timed systems modeled by Stream X-machines",
        "submission-date": "2009/03",
        "publication-date": "2009/08",
        "abstract": "Stream X-machines have been used to specify real systems where complex data structures. They are a variety of extended ﬁnite state machine where a shared memory is used to represent communications between the components of systems. In this paper we introduce an extension of the Stream X-machines formalism in order to specify systems that present temporal requirements. We add time in two different ways. First, we consider that (output) actions take time to be performed. Second, our formalism allows to specify timeouts. Timeouts represent the time a system can wait for the environment to react without changing its internal state. Since timeous affect the set of available actions of the system, a relation focusing on the functional behavior of systems, that is, the actions that they can perform, must explicitly take into account the possible timeouts. In this paper we also propose a formal testing methodology allowing to systematically test a system with respect to a speciﬁcation. Finally, we introduce a test derivation algorithm. Given a speciﬁcation, the derived test suite is sound and complete, that is, a system under test successfully passes the test suite if and only if this system conforms to the speciﬁcation.",
        "keywords": [
            "Formal testing",
            "Timed systems",
            "Stream X-machines"
        ],
        "authors": [
            "Mercedes G. Merayo",
            "Manuel Núñez",
            "Robert M. Hierons"
        ],
        "file_path": "data/sosym-all/s10270-009-0126-3.pdf"
    },
    {
        "title": "Formal veriﬁcation and validation of embedded systems: the UML-based MADES approach",
        "submission-date": "2012/11",
        "publication-date": "2013/06",
        "abstract": "Formal veriﬁcation and validation activities from the early development phases can foster system consistency, correctness, and integrity, but they are often hard to carry out as most designers do not have the necessary background. To address this difﬁculty, a possible approach is to allow engineers to continue using familiar notations and tools, while veriﬁcation and validation are performed on demand, automatically, and transparently. In this paper we describe how the problem of making formal veriﬁcation and valida-tion tasks more designer-friendly is tackled by the MADES approach. Our solution is based on a tool chain that is built atop mature, popular, and widespread technologies. The paper focuses on the veriﬁcation and closed-loop simulation (validation) aspects of the approach and shows how it can be applied to signiﬁcant embedded software systems.",
        "keywords": [
            "Model-driven development",
            "Veriﬁcation",
            "Closed-loop simulation",
            "MARTE",
            "Embedded systems"
        ],
        "authors": [
            "Luciano Baresi",
            "Gundula Blohm",
            "Dimitrios S. Kolovos",
            "Nicholas Matragkas",
            "Alfredo Motta",
            "Richard F. Paige",
            "Alek Radjenovic",
            "Matteo Rossi"
        ],
        "file_path": "data/sosym-all/s10270-013-0330-z.pdf"
    },
    {
        "title": "Does model driven engineering tame complexity?",
        "submission-date": "2007/01",
        "publication-date": "2007/01",
        "abstract": "Advocates of model-driven (software) engineering (MDE) tout the need to raise the level of abstraction at which software is conceived, implemented, and evolved to better manage the inherent complexity of modern software-based systems. Examples of MDE-related languages, technologies, and techniques that have been proposed as “tamers” of inherent complexity are languages supporting multi-view modeling of systems (e.g., the UML), metamodeling approaches to specifying model transformations, metamodeling environments for creating and using domain speciﬁc languages, megamodeling environments for manipulating and managing models, and aspect-oriented modeling environments supporting multi-dimensional separation of concerns and composition. Experiences suggest that some forms of MDE technologies may introduce signiﬁcant accidental complexities that can detract from their use as managers of inherent software complexity.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-006-0041-9.pdf"
    },
    {
        "title": "Recommending metamodel concepts during modeling activities with pre-trained language models",
        "submission-date": "2021/04",
        "publication-date": "2022/02",
        "abstract": "The design of conceptually sound metamodels that embody proper semantics in relation to the application domain is particularly tedious in model-driven engineering. As metamodels deﬁne complex relationships between domain concepts, it is crucial for a modeler to deﬁne these concepts thoroughly while being consistent with respect to the application domain. We propose an approach to assist a modeler in the design of metamodel by recommending relevant domain concepts in several modeling scenarios. Our approach does not require knowledge from the domain or to hand-design completion rules. Instead, we design a fully data-driven approach using a deep learning model that is able to abstract domain concepts by learning from both structural and lexical metamodel properties in a corpus of thousands of independent metamodels. We evaluate our approach on a test set containing 166 metamodels, unseen during the model training, with more than 5000 test samples. Our preliminary results show that the trained model is able to provide accurate top 5 lists of relevant recommendations for concept renaming scenarios. Although promising, the results are less compelling for the scenario of the iterative construction of the metamodel, in part because of the conservative strategy we use to evaluate the recommendations.",
        "keywords": [
            "Intelligent modeling assistants",
            "Domain concepts",
            "Recommender systems",
            "Pre-trained language models"
        ],
        "authors": [
            "Martin Weyssow",
            "Houari Sahraoui",
            "Eugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-022-00975-5.pdf"
    },
    {
        "title": "Behavioral speciﬁcation of reactive systems using stream-based I/O tables",
        "submission-date": "2010/06",
        "publication-date": "2011/05",
        "abstract": "Acoreprobleminformalmethodsisthetransition from informal requirements to formal speciﬁcations. Especially when specifying the behavior of reactive systems, many formalisms require the user to either understand a complex mathematical theory and notation or to derive details not given in the requirements, such as the state space of the problem. For many approaches also a consistent set of requirements is needed, which enforces to resolve requirements conﬂicts prior to formalization. This paper describes a speciﬁcation technique, where not states but signal patterns are the main elements. The notation is based on tables of regular expressions and supports a piece-wise formalization of potentially inconsistent requirements. Many properties, such as input completeness and consistency, can be checked automatically for these speciﬁcations. The detection and resolution of conﬂicts can be performed within our framework after formalization. Besides the formal foundation of our approach, this paper presents prototypical tool support and results from an industrial case study.",
        "keywords": [
            "Tabular speciﬁcation",
            "Consistency",
            "Streams"
        ],
        "authors": [
            "Judith Thyssen",
            "Benjamin Hummel"
        ],
        "file_path": "data/sosym-all/s10270-011-0204-1.pdf"
    },
    {
        "title": "Verifying workﬂow processes: a transformation-based approach",
        "submission-date": "2008/10",
        "publication-date": "2010/02",
        "abstract": "Workﬂow modeling is a challenging activity and designers are likely to introduce errors, especially in complex industrial processes. Effective process veriﬁcation is essential at design time because the cost of ﬁxing errors during runtime is substantially higher. However, most user-oriented workﬂow modeling languages lack formal semantics that hinders such veriﬁcation. In this paper, we propose a generic approach based on the model transformation to verify workﬂow processes. The model transformation includes two steps: ﬁrst, it formalizes the desirable semantics of each modeling element; secondly, it translates a workﬂow process with clear semantics to an equivalent Petri net. Thus, we can verify the original workﬂow process using existing Petri net theory and analysis tools. As a comprehensive case study, verifying workﬂow processes in an industrial modeling language (TiPLM) is presented. Experimental evaluations on verifying real-world business processes validate our approach.",
        "keywords": [
            "Workﬂow",
            "Model transformation",
            "Process veriﬁcation",
            "Workﬂow veriﬁcation",
            "Petri net"
        ],
        "authors": [
            "Haiping Zha",
            "Wil M. P. van der Aalst",
            "Jianmin Wang",
            "Lijie Wen",
            "Jiaguang Sun"
        ],
        "file_path": "data/sosym-all/s10270-010-0149-9.pdf"
    },
    {
        "title": "Model checking LTL properties over ANSI-C programs with bounded traces",
        "submission-date": "2012/04",
        "publication-date": "2013/07",
        "abstract": "Context-bounded model checking has been used successfully to verify safety properties in multi-threaded systems automatically, even if they are implemented in low-level programming languages such as C. In this paper, we describe and experiment with an approach to extend context-bounded software model checking to safety and liveness properties expressed in linear-time temporal logic (LTL). Our approach checks the actual C program, rather than an extracted abstract model. It converts the LTL formulas into Büchi automata for the corresponding never claims and then further into C monitor threads that are interleaved with the execution of the program under analysis. This combined system is then checked using the ESBMC model checker. We use an extended, four-valued LTL semantics to handle the finite traces that bounded model checking explores; we thus check the combined system several times with different acceptance criteria to derive the correct truth value. In order to mitigate the state space explosion, we use a dedicated scheduler that selects the monitor thread only after updates to global variables occurring in the LTL formula. We demonstrate our approach on the analysis of the sequential firmware of a medical device and a small multi-threaded control application.",
        "keywords": [
            "Model checking",
            "Linear temporal logic",
            "Software verification"
        ],
        "authors": [
            "Jeremy Morse",
            "Lucas Cordeiro",
            "Denis Nicole",
            "Bernd Fischer"
        ],
        "file_path": "data/sosym-all/s10270-013-0366-0.pdf"
    },
    {
        "title": "Environment modeling and simulation for automated testing of soft real-time embedded software",
        "submission-date": "2012/03",
        "publication-date": "2013/04",
        "abstract": "Given the challenges of testing at the system level, only a fully automated approach can really scale up to industrial real-time embedded systems (RTES). Our goal is to provide a practical approach to the model-based testing of RTES by allowing system testers, who are often not familiar with the system’s design but are application domain experts, to model the system environment in such a way as to enable its black-box test automation. Environment models can support the automation of three tasks: the code generation of an environment simulator to enable testing on the development platform or without involving actual hardware, the selection of test cases, and the evaluation of their expected results (oracles). From a practical standpoint—and such considerations are crucial for industrial adoption—environment modeling should be based on modeling standards (1) that are at an adequate level of abstraction, (2) that software engineers are familiar with, and (3) that are well supported by commercial or open source tools. In this paper, we propose a precise environment modeling methodology fitting these requirements and discuss how these models can be used to generate environment simulators. The environment models are expressed using UML/MARTE and OCL, which are international standards for real-time systems and constraint modeling. The presented techniques are evaluated on a set of three artificial problems and on two industrial RTES.",
        "keywords": [
            "Environment modeling",
            "Environment simulation",
            "Automated testing",
            "Model-based testing",
            "Real-time embedded systems",
            "Search based software engineering"
        ],
        "authors": [
            "Muhammad Zohaib Iqbal",
            "Andrea Arcuri",
            "Lionel Briand"
        ],
        "file_path": "data/sosym-all/s10270-013-0328-6.pdf"
    },
    {
        "title": "MoDMaCAO: a model-driven framework for the design, validation and configuration management of cloud applications based on OCCI",
        "submission-date": "2021/02",
        "publication-date": "2022/09",
        "abstract": "To tackle the cloud-provider lock-in, the open grid forum is developing the open cloud computing interface (OCCI), a standardized interface for managing any kind of cloud resources. Besides the OCCI Core model, which defines the basic modeling elements for cloud resources, further standardized extensions exist that reflect the requirements of different cloud service levels, such as infrastructure and platform elements. However, so far the OCCI platform extension is very coarse-grained and lacks supporting use cases and implementations. Especially, it does not define how the components of the application itself can be managed. In this paper, we discuss the features of MoDMaCAO, a model-driven framework that extends the OCCI platform extension. The users of the framework are able to design and validate cloud application topologies and subsequently deploy them on OCCI compliant clouds by using configuration management tools.",
        "keywords": [
            "Cloud computing",
            "Open cloud computing interface",
            "OCCI",
            "Models@run.time"
        ],
        "authors": [
            "Faiez Zalila\nFabian Korte\nJohannes Erbel\nStéphanie Challita\nJens Grabowski\nPhilippe Merle"
        ],
        "file_path": "data/sosym-all/s10270-022-01024-x.pdf"
    },
    {
        "title": "MEMORIAL",
        "submission-date": "2023/02",
        "publication-date": "2023/03",
        "abstract": "This paper is a memorial to Heinrich Hussmann, detailing his contributions to software engineering, formal methods, and multimedia technologies. It highlights his research on algebraic specifications, the RAP tool, the SPECTRUM language, and his work on integrating relational databases with object-oriented applications.",
        "keywords": [],
        "authors": [
            "Manfred Broy",
            "Albrecht Schmidt",
            "Martin Wirsing"
        ],
        "file_path": "data/sosym-all/s10270-023-01099-0.pdf"
    },
    {
        "title": "CHECKSUM: tracking changes and measuring contributions in cooperative systems modeling",
        "submission-date": "2020/02",
        "publication-date": "2021/01",
        "abstract": "Models are often used to represent various types of systems. This is especially true for software systems, where cooperat-\ning teams create models using a modeling language (e.g., UML). In cooperative modeling scenarios, it is useful to identify \ncontributions and changes performed by individuals and teams. This paper presents a technique called CHECKSUM, which \nmonitors the cooperative work done on models and maintains an immutable changelog. CHECKSUM uses its changelog to \nmeasure contributions based on points, time, and quality, and to enable the auditing of a model’s change-history. This paper \nalso presents GEneric Meta-Model (GEMM). The latter unifies the underlying representation of different types of models \nthat follow varying visualization patterns including box and line, container, and interleaving. GEMM enables CHECKSUM \nto support an extensible variety of model types. We developed a prototype tool that realizes CHECKSUM’s concepts and \nintegrates it into two existing modeling tools. We conducted two studies to evaluate CHECKSUM from two perspectives: \ntechnical and user. The studies yielded positive results concerning various qualities including integrability into existing tools, \neffectiveness, efficiency, usability, and usefulness.",
        "keywords": [
            "Models",
            "Diagrams",
            "Changes",
            "Contributions",
            "Cooperative work",
            "Design tools and techniques"
        ],
        "authors": [
            "Pierre A. Akiki\nHoda W. Maalouf"
        ],
        "file_path": "data/sosym-all/s10270-020-00840-3.pdf"
    },
    {
        "title": "MUREQ: a multilayer framework for analyzing and operationalizing visualization requirements",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "Understanding and interpreting vast amounts of information is pivotal in the contemporary data-rich age. Data visualization has emerged as a signiﬁcant measure of comprehending these data. Similarly, an appropriate visualization can also enhance software modeling by providing straightforward and interactive representations. However, current data visualization methods predominantly require users to have data visualization-related expertise, which is usually challenging to obtain in reality. It is essential to bridge the gap between visualization requirements and visualization solutions for non-expert users, assisting them in automatically operationalizing their visualization requirements. This paper proposes a MUltilayer framework for analyzing andoperationalizingvisualizationREQuirementsthatautomaticallyderivesappropriatevisualizationsolutionsbasedonusers’ requirements. Speciﬁcally, we systematically investigate the connections among visualization requirements, visual variable characteristics, visual variable attributes, and visualization solutions, based on which we establish a conceptual framework that characterizes the relationships among different layers. Our proposal contributes to not only automatically operationalizing visualization requirements but also providing meaningful explanations for the derived visualization solutions. To promote our proposal and pragmatically beneﬁt real users, we have developed and deployed a prototype tool based on the proposed framework, which is publicly available at https://reqdv.vmasks.fun. To evaluate our proposed framework, we conducted an initial controlled experiment with 44 participants to test the performance of the evolved mappings within our framework. Based on the expert’s feedback, we reﬁned the mappings and incorporated a ranking system for visualization solutions tailored to speciﬁc requirements. To assess the current method, a subsequent experiment with another group of 44 participants and a focused case study involving two new participants were carried out. The results demonstrate that users perceive that the current method accelerates task completion, especially for complex tasks, by efﬁciently narrowing down options and prioritizing them. This approach is particularly advantageous for users with limited data visualization experience. Besides, the multilayer framework can be used to inspire the visualization of models in the software modeling community.",
        "keywords": [
            "Visualization requirements operationalization",
            "Multilayer analysis",
            "Empirical evaluation",
            "Prototype tool"
        ],
        "authors": [
            "Tong Li",
            "Yiting Wang",
            "Xiang Wei",
            "Xueying Zhang",
            "Yu Liu"
        ],
        "file_path": "data/sosym-all/s10270-024-01204-x.pdf"
    },
    {
        "title": "Modeling foundations for executable model-based testing of self-healing cyber-physical systems",
        "submission-date": "2018/01",
        "publication-date": "2018/11",
        "abstract": "Self-healing cyber-physical systems (SH-CPSs) detect and recover from faults by themselves at runtime. Testing such systems is challenging due to the complex implementation of self-healing behaviors and their interaction with the physical environment, both of which are uncertain. To this end, we propose an executable model-based approach to test self-healing behaviors under environmental uncertainties. The approach consists of a Modeling Framework of SH-CPSs (MoSH) and an accompanying Test Model Executor (TM-Executor). MoSH provides a set of modeling constructs and a methodology to specify executable test models, which capture expected system behaviors and environmental uncertainties. TM-Executor executes the test models together with the systems under test, to dynamically test their self-healing behaviors under uncertainties. We demonstrated the successful application of MoSH to specify 11 self-healing behaviors and 17 uncertainties for three SH-CPSs. The time spent by TM-Executor to perform testing activities was in the order of milliseconds, though the time spent was strongly correlated with the complexity of test models.",
        "keywords": [
            "Cyber-physical systems",
            "Self-healing",
            "Uncertainty",
            "Model execution",
            "Model-based testing"
        ],
        "authors": [
            "Tao Ma",
            "Shaukat Ali",
            "Tao Yue"
        ],
        "file_path": "data/sosym-all/s10270-018-00703-y.pdf"
    },
    {
        "title": "Formalised EMFTVM bytecode language for sound veriﬁcation of model transformations",
        "submission-date": "2015/12",
        "publication-date": "2016/08",
        "abstract": "Abstract Model-driven engineering is an effective approach for addressing the full life cycle of software devel-opment. Model transformation is widely acknowledged as one of its central ingredients. With the increasing complexity of model transformations, it is urgent to develop veriﬁcation tools that prevent incorrect transformations from generating faulty models. However, the development of sound veriﬁca-tion tools is a non-trivial task, due to unimplementable or erroneous execution semantics encoded for the target model transformation language. In this work, we develop a for-malisation for the EMFTVM bytecode language by using the Boogie intermediate veriﬁcation language. It ensures the model transformation language has an implementable exe-cution semantics by reliably prototyping the implementation of the model transformation language. It also ensures the absence of erroneous execution semantics encoded for the target model transformation language by using a translation validation approach.",
        "keywords": [
            "MDE",
            "EMFTVM",
            "Boogie",
            "Model transformation veriﬁcation",
            "Intermediate veriﬁcation language"
        ],
        "authors": [
            "Zheng Cheng",
            "Rosemary Monahan",
            "James F. Power"
        ],
        "file_path": "data/sosym-all/s10270-016-0553-x.pdf"
    },
    {
        "title": "Design science in action: developing a modeling technique for eliciting requirements on business process management (BPM) tools",
        "submission-date": "2012/10",
        "publication-date": "2014/05",
        "abstract": "Selecting a suitable business process management (BPM) tool to build a business process support system for a particular business process is difﬁcult. There are a number of BPM tools on the market that are available as systems to install locally and as services in the cloud. These tools are based on different BPM paradigms (e.g., workﬂow or case management) and provide different capabilities (e.g., enforcement of the control ﬂow, shared spaces, or a collaborative environment). This makes it difﬁcult for an organization to select a tool that would ﬁt the business processes at hand. The paper suggests a solution for this problem. The core of the solution is a modeling technique for business processes for eliciting their requirements for a suitable BPM tool. It produces a high-level, business process model, called a “step-relationship” model that depicts the essential characteristics of a process in a paradigm-independent way. The solution presented in this paper has been developed based on the paradigm of design science research, and the paper discusses the research project from the design science perspective. The solution has been applied in two case studies in order to demonstrate its feasibility.",
        "keywords": [
            "Business process modeling",
            "Workﬂow",
            "Case management",
            "Shared space",
            "Design science"
        ],
        "authors": [
            "Ilia Bider",
            "Erik Perjons"
        ],
        "file_path": "data/sosym-all/s10270-014-0412-6.pdf"
    },
    {
        "title": "Correction: Modeling competences in enterprise architecture: from knowledge, skills, and attitudes to organizational capabilities",
        "submission-date": "2024/05",
        "publication-date": "2024/05",
        "abstract": "In the published article Figs. 14, 15 and 16 were published wrong. During the proof correction stage typesetters did not update the correct ﬁgures provided by the author. The correct ﬁgures are given below: We apologize for the error. The correct ﬁgures are updated in the original publication.",
        "keywords": [],
        "authors": [
            "Rodrigo F. Calhau",
            "João Paulo A. Almeida",
            "Satyanarayana Kokkula",
            "Giancarlo Guizzardi"
        ],
        "file_path": "data/sosym-all/s10270-024-01182-0.pdf"
    },
    {
        "title": "SoSyM Special Section on Software Engineering and Formal Methods",
        "submission-date": "2004/09",
        "publication-date": "2006/06",
        "abstract": "This section of “Software & Systems Modeling” contains three papers presenting current trends on the use of formal methods and software engineering for the development of complex distributed applications. These articles are based on presentations at SEFM 2004, the Second IEEE International Conference on Software Engineering and Formal Methods, that took place during 28–30 September 2004 in Beijing, China. The best papers of the conference were invited to prepare revised and extended versions for publication in this special section.",
        "keywords": [],
        "authors": [
            "J. Cuellar",
            "Z. Liu"
        ],
        "file_path": "data/sosym-all/s10270-006-0010-3.pdf"
    },
    {
        "title": "Graph-based traceability: a comprehensive approach",
        "submission-date": "2009/01",
        "publication-date": "2009/11",
        "abstract": "In recent years, traceability has been globally accepted as being a key success factor of software development projects. However, the multitude of different, poorly integratedtaxonomies,approachesandtechnologiesimpedes the application of traceability techniques in practice. This paper presents a comprehensive view on traceability, pertaining to the whole software development process. Based on the state of the art, the ﬁeld is structured according to six speciﬁc activities related to traceability as follows: deﬁnition, recording, identiﬁcation, maintenance, retrieval, and utilization. Using graph technology, a comprehensive and seamless approach for supporting these activities is derived, combining them in one single conceptual framework. This approach supports the deﬁnition of metamodels for traceability information, recording of traceability information in graph-based repositories, identiﬁcation and maintenance of traceability relationships using transformations, as well as retrieval and utilization of traceability information using a graph query language. The approach presented here is applied in the context of the ReDSeeDS project (Requirements Driven Software Development System) that aims at requirements-based software reuse. ReDSeeDS makes use of traceability information to determine potentially reusable architectures, design, or code artifacts based on a given set of reusable requirements. The project provides case studies from different domains for the validation of the approach.",
        "keywords": [
            "Traceability",
            "Graph technology",
            "Model transformations",
            "Software engineering"
        ],
        "authors": [
            "Hannes Schwarz",
            "Jürgen Ebert",
            "Andreas Winter"
        ],
        "file_path": "data/sosym-all/s10270-009-0141-4.pdf"
    },
    {
        "title": "Guest editorial to the special section on PoEM’2021",
        "submission-date": "2023/01",
        "publication-date": "2023/02",
        "abstract": "This guest editorial presents the special section of 14th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modeling (PoEM 2021). The best papers of PoEM 2021 were invited to be revised and signiﬁcantly expanded. Eight papers were ﬁnally accepted for publication in the special section. These papers are an excellent representation of the state of the art on Enterprise Modeling, showing as well the importance of applying the research contributions into practice.",
        "keywords": [
            "Enterprise modeling",
            "Conceptual modeling",
            "Modeling methods"
        ],
        "authors": [
            "Estefanía Serral",
            "Janis Stirna",
            "Jolita Ralyté",
            "Janis Grabis"
        ],
        "file_path": "data/sosym-all/s10270-023-01089-2.pdf"
    },
    {
        "title": "On the comprehension of workﬂows modeled with a precise style: results from a family of controlled experiments",
        "submission-date": "2013/03",
        "publication-date": "2013/11",
        "abstract": "In this paper, we present the results from a family of experiments conducted to assess whether the level of formality/precision in workﬂow modeling, based on UML activity diagrams, inﬂuences two aspects of construct comprehensibility: correctness of understanding and task completion time. In particular, we have considered two styles for workﬂow modeling with different levels of formality: a precise style (with speciﬁc rules and imposed constraints) and an ultra-light style (no rules, no imposed constraints). Experiments were conducted with 111 participants (Bachelor and Master students). In each experiment, participants accomplished comprehension tasks on two workﬂows, modeled either with the precise style or with a lighter variant. The main results from our data analysis can be summarized as follows: (i) all participants achieved a signiﬁcantly better comprehension of workﬂows written in the precise style, (ii) the style had no signiﬁcant impact on task completion time, (iii) more experienced participants beneﬁted more, with respect to less experienced ones, from the precise style, as for their correctness of understanding, and (iv) all participants found the precise style useful in comprehending workﬂows.",
        "keywords": [
            "Family of experiments",
            "Precise and Ultra-light styles",
            "UML activity diagrams",
            "Workﬂow modeling"
        ],
        "authors": [
            "Gianna Reggio",
            "Filippo Ricca",
            "Giuseppe Scanniello",
            "Francesco Di Cerbo",
            "Gabriella Dodero"
        ],
        "file_path": "data/sosym-all/s10270-013-0386-9.pdf"
    },
    {
        "title": "Theme section on performance modelling and engineering of software and systems",
        "submission-date": "2017/08",
        "publication-date": "2017/09",
        "abstract": "The modelling of the performance of today’s computer systems is getting more and more challenging. While most models traditionally focused on utilization and response time of a single system, technologies such as cloud and containerization require new approaches to efficiently handle the complexity of current installations. The meaning of the word performance is constantly changing—currently covers additional non-functional aspects including security and reliability. In a world full of software systems composed of huge number of microservices running on cloud infrastructures, a single service is less important than in the traditional system architectures. In contrast to the classical systems, the entire network of containerized services needs to be efficiently operated, monitored and orchestrated. Containers are automatically deployed, scaled and operated by orchestration middleware, IaaS is re-arranging their resources dynamically, and several independent services might compete for the same resources. To optimize efficiency users, developers and operators need new approaches to model systems and their behaviour, allowing not only to evaluate the systems retrospectively, but also to take the right actions proactively in near real time.",
        "keywords": [],
        "authors": [
            "Catalina M. Lladó",
            "Kai Sachs"
        ],
        "file_path": "data/sosym-all/s10270-017-0624-7.pdf"
    },
    {
        "title": "Modeling robustness behavior using aspect-oriented modeling to support robustness testing of industrial systems",
        "submission-date": "2010/12",
        "publication-date": "2011/06",
        "abstract": "Model-based robustness testing requires precise and complete behavioral, robustness modeling. For example, state machines can be used to model software behavior when hardware (e.g., sensors) breaks down and be fed to a tool to automate test case generation. But robustness behav-ior is a crosscutting behavior and, if modeled directly, often results in large, complex state machines. These in practice tend to be error prone and difﬁcult to read and understand. As a result, modeling robustness behavior in this way is not scalable for complex industrial systems. To overcome these problems, aspect-oriented modeling (AOM) can be employed to model robustness behavior as aspects in the form of state machines specifically designed to model robustness behavior. In this paper, we present a RobUstness Model-ing Methodology (RUMM) that allows modeling robustness behavior as aspects. Our goal is to have a complete and practical methodology that covers all features of state machines and aspect concepts necessary for model-based robustness testing. At the core of RUMM is a UML proﬁle (AspectSM) that allows modeling UML state machine aspects as UML state machines (aspect state machines). Such an approach, relying on a standard and using the target notation as the basis to model the aspects themselves, is expected to make Communicated by Dr. Jean-Michel Bruel.",
        "keywords": [
            "Aspect-oriented modeling",
            "UML state machines",
            "Robustness",
            "UML proﬁle",
            "Crosscutting behavior",
            "Robustness testing"
        ],
        "authors": [
            "Shaukat Ali",
            "Lionel C. Briand",
            "Hadi Hemmati"
        ],
        "file_path": "data/sosym-all/s10270-011-0206-z.pdf"
    },
    {
        "title": "Editorial to the theme section on model-based engineering of smart systems",
        "submission-date": "2019/09",
        "publication-date": "2020/01",
        "abstract": "The term ‘smart’ is widely applied to products and systems that are enabled by, and depend upon, computing and communication technology to analyse and respond to changing conditions in their environment. The papers in this thematic section address some of these important and intriguing challenges.",
        "keywords": [],
        "authors": [
            "John Fitzgerald",
            "Fuyuki Ishikawa",
            "Peter Gorm Larsen"
        ],
        "file_path": "data/sosym-all/s10270-019-00758-5.pdf"
    },
    {
        "title": "FLAME: a formal framework for the automated analysis of software product lines validated by automated speciﬁcation testing",
        "submission-date": "2013/11",
        "publication-date": "2015/12",
        "abstract": "In a literature review on the last 20 years of automated analysis of feature models, the formalization of analysis operations was identiﬁed as the most relevant challenge in the ﬁeld. This formalization could provide very valuable assets for tool developers such as a precise deﬁnition of the analysis operations and, what is more, a reference implementation, i.e., a trustworthy, not necessarily efﬁcient implementation to compare different tools outputs. In this article, we present the FLAME framework as the result of facing this challenge. FLAME is a formal framework that can be used to formally specify not only feature models, but other variability modeling languages (VMLs) as well. This reusability is achieved by its two-layered architecture. The abstract foundation layer is the bottom layer in which all VML-independent analysis operations and concepts are speciﬁed. On top of the foundation layer, a family of characteristic model layers—one for each VML to be formally speciﬁed—can be developed by redeﬁning some abstract types and relations. The veriﬁcation and validation of FLAME has followed a process in which formal veriﬁcation has been performed traditionally by manual theorem proving, but validation has been performed by integrating our experience on metamorphic testing of variability analysis tools, something that has shown to be much more effective than manually designed test cases. To follow this automated, test-based validation approach, the speciﬁcation of FLAME, written in Z, was translated into Prolog and 20,000 random tests were automatically generated and executed. Tests results helped to discover some inconsistencies not only in the formal speciﬁcation, but also in the previous informal deﬁnitions of the analysis operations and in current analysis tools. After this process, the Prolog implementation of FLAME is being used as a reference implementation for some tool developers, some analysis operations have been formally speciﬁed for the ﬁrst time with more generic semantics, and more VMLs are being formally speciﬁed using FLAME.",
        "keywords": [
            "Formal speciﬁcation",
            "Speciﬁcation testing",
            "Software product lines",
            "Feature models"
        ],
        "authors": [
            "Amador Durán",
            "David Benavides",
            "Sergio Segura",
            "Pablo Trinidad",
            "Antonio Ruiz-Cortés"
        ],
        "file_path": "data/sosym-all/s10270-015-0503-z.pdf"
    },
    {
        "title": "Handling index-out-of-bounds in safety-critical embedded C code using model-based development",
        "submission-date": "2017/03",
        "publication-date": "2018/10",
        "abstract": "Embedded C code for safety critical systems faces some substantial challenges: like every other embedded SW code it must be efﬁcient in terms of code size, data size and execution time, but it must also behave safely under all circumstances, without a user or operator who could handle the errors. One kind of problem is array accesses where the index is outside the speciﬁed value range. The C language does not specify the behaviour in such cases, which clearly violates the requirements for safe code. In this paper, the approach of the model-based development tool “ASCET” is explained, and the experiences of three case studies that describe the adoption of index protection by the users are presented.",
        "keywords": [
            "Domain-speciﬁc languages",
            "Functional safety",
            "Software adaptation",
            "Embedded software",
            "Automotive engineering"
        ],
        "authors": [
            "Gunter Blache"
        ],
        "file_path": "data/sosym-all/s10270-018-0697-y.pdf"
    },
    {
        "title": "ModelSet: a dataset for machine learning in model-driven engineering",
        "submission-date": "2021/03",
        "publication-date": "2021/10",
        "abstract": "The application of machine learning (ML) algorithms to address problems related to model-driven engineering (MDE) is currently hindered by the lack of curated datasets of software models. There are several reasons for this, including the lack of large collections of good quality models, the difﬁculty to label models due to the required domain expertise, and the relative immaturity of the application of ML to MDE. In this work, we present ModelSet, a labelled dataset of software models intended to enable the application of ML to address software modelling problems. To create it we have devised a method designed to facilitate the exploration and labelling of model datasets by interactively grouping similar models using off-the-shelf technologies like a search engine. We have built an Eclipse plug-in to support the labelling process, which we have used to label 5,466 Ecore meta-models and 5,120 UML models with its category as the main label plus additional secondary labels of interest. We have evaluated the ability of our labelling method to create meaningful groups of models in order to speed up the process, improving the effectiveness of classical clustering methods. We showcase the usefulness of the dataset by applying it in a real scenario: enhancing the MAR search engine. We use ModelSet to train models able to infer useful metadata to navigate search results. The dataset and the tooling are available at https://ﬁgshare.com/s/5a6c02fa8ed20782935c and a live version at http://modelset.github.io.",
        "keywords": [
            "Dataset",
            "Machine learning",
            "Model-driven engineering"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Javier Luis Cánovas Izquierdo",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-021-00929-3.pdf"
    },
    {
        "title": "Automated formal veriﬁcation of visual modeling languages by model checking",
        "submission-date": "2003/02",
        "publication-date": "2004/04",
        "abstract": "Graph transformation has recently become more and more popular as a general, rule-based visual speciﬁcation paradigm to formally capture (a) requirements or behavior of user models (on the model-level), and (b) the operational semantics of modeling languages (on the meta-level) as demonstrated by benchmark applications around the Uniﬁed Modeling Language (UML). The current paper focuses on the model checking-based automated formal veriﬁcation of graph transformation systems used either on the model-level or meta-level. We present a general translation that inputs (i) a meta-model of an arbitrary visual modeling language, (ii) a set of graph transformation rules that deﬁnes a formal operational semantics for the language, and (iii) an arbitrary well-formed model instance of the language and generates a transitions system (TS) that serve as the underlying mathematical speciﬁcation formalism of various model checker tools. The main theoretical beneﬁt of our approach is an optimization technique that projects only the dynamic parts of the graph transformation system into the target transition system, which results in a drastical reduction in the state space. The main practical beneﬁt is the use of existing back-end model checker tools, which directly provides formal veriﬁcation facilities (without additional eﬀorts required to implement an analysis tool) for many practical applications captured in a very high-level visual notation. The practical feasibility of the approach is demonstrated by modeling and analyzing the well-known veriﬁcation benchmark of dining philosophers both on the model and meta-level.",
        "keywords": [
            "Graph transformation",
            "Metamodeling",
            "Formal veriﬁcation",
            "Model checking",
            "Model transform-ation"
        ],
        "authors": [
            "D´aniel Varr´o"
        ],
        "file_path": "data/sosym-all/s10270-003-0050-x.pdf"
    },
    {
        "title": "A collection operator for graph transformation",
        "submission-date": "2009/12",
        "publication-date": "2011/02",
        "abstract": "Algebraic graph transformation has a well-\nestablished theory and associated tools that can be used to\nperform model transformations. However, the lack of a con-\nstructtomatchandtransformcollectionsofsimilarsubgraphs\nmakes graph transformation complex or even impractical to\nuse in a number of transformation cases. This is addressed\nin this paper, by deﬁning a collection operator which is\npowerful, yet simple to model and understand. A rule can\ncontain multiple collection operators, each with lower and\nupper bound cardinalities, and the collection operators can be\nnested. An associated matching process dynamically builds a\ncollection free rule that enables us to reuse the existing graph\ntransformation apparatus. We present model transformation\nexamples from different modeling domains to illustrate the\nbeneﬁt of the approach.",
        "keywords": [
            "Graph transformation",
            "Model transformation",
            "Matching"
        ],
        "authors": [
            "Roy Grønmo",
            "Stein Krogdahl",
            "Birger Møller-Pedersen"
        ],
        "file_path": "data/sosym-all/s10270-011-0190-3.pdf"
    },
    {
        "title": "Guest editorial for the special section on MODELS 2020",
        "submission-date": "2022/08",
        "publication-date": "2022/09",
        "abstract": "The MODELS conference series is the premier venue for model-based software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. This special section presents the nine articles that resulted from an invitation to authors of the best papers at MODELS 2020, followed by a full SoSyM review cycle.",
        "keywords": [],
        "authors": [
            "Silvia Abrahão",
            "Juan de Lara",
            "Houari Sahraoui",
            "Eugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-022-01044-7.pdf"
    },
    {
        "title": "Example-driven modeling: on effects of using examples on structural model comprehension, what makes them useful, and how to create them",
        "submission-date": "2017/01",
        "publication-date": "2018/01",
        "abstract": "We present a controlled experiment for the empirical evaluation of example-driven modeling (EDM), an approach that systematically uses examples for model comprehension and domain knowledge transfer. We conducted the experiment with 26 graduate (Masters and Ph.D. level) and undergraduate (Bachelor level) students from electrical and computer engineering, computer science, and software engineering programs at the University of Waterloo. The experiment involves a domain model, with UML class diagrams representing the domain abstractions and UML object diagrams representing examples of using these abstractions. The goal is to provide empirical evidence of the effects of suitable examples on model comprehension, compared to having model abstractions only, by having the participants perform model comprehension tasks. Our results show that EDM is superior to having model abstractions only, with an improvement of 39% for diagram completeness, 33% for questions completeness, 71% for efﬁciency, and a reduction in the number of mistakes by 80%. We provide qualitative results showing that participants receiving model abstractions augmented with examples experienced lower perceived difﬁculty in performing the comprehension tasks, higher perceived conﬁdence in their tasks’ solutions, and asked 90% fewer clarifying domain questions. We also present participants’ feedback regarding the usefulness of the provided examples, their number and types, as well as the use of partial examples. We present a taxonomy of the different types of examples, explain their signiﬁcance, and propose guidelines for manual and automatic creation of useful examples.",
        "keywords": [
            "Software engineering",
            "Structural modeling",
            "Empirical study",
            "Example-driven modeling"
        ],
        "authors": [
            "Dina Zayan",
            "Atrisha Sarkar",
            "Michał Antkiewicz",
            "Rita Suzana Pitangueira Maciel",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-017-0652-3.pdf"
    },
    {
        "title": "Fairness, assumptions, and guarantees for extended bounded response LTL+P synthesis",
        "submission-date": "2022/06",
        "publication-date": "2023/08",
        "abstract": "Realizability and reactive synthesis from temporal logics are fundamental problems in formal veriﬁcation. The complexity\nof these problems for linear temporal logic with past (LTL+P) led to the identiﬁcation of fragments with lower complexities\nand simpler algorithms. Recently, the logic of extended bounded response LTL+P(LTLEBR+Pfor short) has been introduced.\nIt allows one to express safety languages deﬁnable in LTL+Pand it is provided with an efﬁcient, fully symbolic algorithm for\nreactive synthesis. This paper features four related contributions. First, we introduce GR-EBR, an extension of LTLEBR+Pwith\nfairness conditions, assumptions, and guarantees that, on the one hand, allows one to express properties beyond the safety\nfragment and, on the other, it retains the efﬁciency of LTLEBR+Pin practice. Second, we the expressiveness of GR-EBRstarting\nfrom the expressiveness of its fragments. In particular, we prove that: (1) LTLEBR+Pis expressively complete with respect to\nthe safety fragment of LTL+P, (2) the removal of past operators from LTLEBR+Presults into a loss of expressive power, and\n(3) GR-EBRis expressively equivalent to the logic GR(1)of Bloem et al. Third, we provide a fully symbolic algorithm for\nthe realizability problem from GR-EBRspeciﬁcations, that reduces it to a number of safety subproblems. Fourth, to ensure\nsoundness and completeness of the algorithm, we propose and exploit a general framework for safety reductions in the context\nof realizability of (fragments of) LTL+P. The experimental evaluation shows promising results.",
        "keywords": [
            "Reactive synthesis",
            "Temporal logics",
            "Safety reductions",
            "Expressiveness"
        ],
        "authors": [
            "Alessandro Cimatti\nLuca Geatti\nNicola Gigante\nAngelo Montanari\nStefano Tonetta"
        ],
        "file_path": "data/sosym-all/s10270-023-01122-4.pdf"
    },
    {
        "title": "Detecting feature interaction in CPL",
        "submission-date": "2003/02",
        "publication-date": "2005/11",
        "abstract": "This article addresses the problem of detecting feature interactions in the area of telephony systems design. The proposed approach consists of two phases: filtering and testing. The filtering phase detects possible interactions by identifying incoherencies in a logic specification of the main elements of the features, consisting of preconditions, triggers, results and constraints. If incoherencies are identified, then an interaction is suspected, test cases corresponding to the suspected interaction are generated and testing is applied to see if the interaction actually exists. Two case studies, carried out on established benchmarks, show that this approach gives good results in practice.",
        "keywords": [
            "Telephony software",
            "Feature interaction",
            "Detection method",
            "Formal techniques"
        ],
        "authors": [
            "Nicolas Gorse",
            "Luigi Logrippo",
            "Jacques Sincennes"
        ],
        "file_path": "data/sosym-all/s10270-005-0101-6.pdf"
    },
    {
        "title": "Networcat: applying analysis techniques of shared memory software on message-passing distributed systems",
        "submission-date": "2024/02",
        "publication-date": "2025/02",
        "abstract": "Communicationmodelsareakeyaspectinthedesignandimplementationofdistributedsystemarchitectures.Applicationlogic\nmust consider the guarantees of these models, which fundamentally inﬂuence its correctness. Modern multi-core processor\narchitectures face a similar problem when it comes to accessing shared memory: the guarantees of an architecture have a\nfundamental impact on the observable behavior of software. The formalization of these guarantees in a declarative way has led\nto powerful tools and algorithms to deﬁne reusable constraints on patterns of memory access events and their relationships,\nenabling the efﬁcient description and automatic formal analysis of software properties with respect to a speciﬁc architecture.\nThe Cat memory modeling language provides a standard means of specifying these constraints. Despite the parallels, the\naxiomatic modeling and analysis of communication models in distributed systems remain a relatively unexplored area. In\nthis paper, we address this gap and demonstrate how communication models can be mapped to the Cat language. We create\na standard library of reusable patterns and demonstrate our approach, called NetworCat, on the simple examples of UDP\nand TCP, and we also present its applicability to the vastly conﬁgurable OMG-DDS service. This adaptation-based approach\nenables the use of ever-improving veriﬁcation tools built for shared memory concurrency on distributed systems. We believe\nthis not only beneﬁts distributed system analyses by broadening the toolset for veriﬁcation but also positively impacts the\nﬁeld of memory-model-aware veriﬁcation by widening its audience to another domain.",
        "keywords": [
            "Distributed systems",
            "Systems modeling",
            "Formal veriﬁcation",
            "Cat",
            "OMG-DDS"
        ],
        "authors": [
            "Levente Bajczi\nVince Molnár"
        ],
        "file_path": "data/sosym-all/s10270-024-01258-x.pdf"
    },
    {
        "title": "A reference framework for process-oriented software development organizations",
        "submission-date": "2003/12",
        "publication-date": "2004/07",
        "abstract": "In this paper, a proposal of a generic framework for process-oriented software development organizations is presented. Additionally, the respective way of managing the process model, and the instantiation of their processes with the Rational Uniﬁed Process (RUP) disciplines, whenever they are available, or with other kind of processes is suggested. The proposals made here were consolidated with experiences from real projects and we report the main results from one of those projects.",
        "keywords": [
            "RUP",
            "Process-oriented organizations",
            "Software development process",
            "Business modelling"
        ],
        "authors": [
            "João M. Fernandes",
            "Francisco J. Duarte"
        ],
        "file_path": "data/sosym-all/s10270-004-0063-0.pdf"
    },
    {
        "title": "Simulation of system architectures using optimization and machine learning: the state of the art and research opportunities",
        "submission-date": "2024/01",
        "publication-date": "2025/02",
        "abstract": "Most software-intensive systems present large and complex architectures, which should satisfy different quality attributes, such as performance, reliability, and security. Some of these attributes could only be measured at runtime, which is undesired, particularly for critical systems whose attributes should still be evaluated at design time to avoid failures at runtime and losses, including human lives. Simulation has been considered a powerful solution to predict and evaluate different architectural arrangements at design time and, combined with optimization and machine learning, and it can ﬁnd suitable or even optimal architectures. However, there is a lack of an overview of such combinations and how they can work better. This work presents the state of the art of simulation using optimization and/or machine learning techniques. For this, we examined the literature of 1,342 studies retrieved from three publications databases and systematically selected 87 studies and scrutinized them. There is a variety of combinations of simulation with different optimization and/or machine learning techniques, each requiring speciﬁc simulation models and simulators. At the same time, studies are still isolated, lacking maturity in the area and remaining important future work to discover the beneﬁts of such combinations.",
        "keywords": [
            "Optimization",
            "Machine learning",
            "Simulation",
            "System architecture"
        ],
        "authors": [
            "Wallace Manzano",
            "Valdemar Vicente Graciano Neto",
            "Thiago Bianchi",
            "Mohamad Kassab",
            "Elisa Yumi Nakagawa"
        ],
        "file_path": "data/sosym-all/s10270-025-01280-7.pdf"
    },
    {
        "title": "A participative end-user method for multi-perspective business process elicitation and improvement",
        "submission-date": "2014/10",
        "publication-date": "2015/08",
        "abstract": "A business process can be characterized by multiple perspectives (intentional, organizational, operational, functional, interactional, informational, etc). Business process modeling must allow different stakeholders to analyze and represent process models according to these different perspectives. This representation is traditionally built using classical data acquisition methods together with a process representation language such as BPMN or UML. These techniques and specialized languages can easily become hard, complex and time consuming. In this paper, we propose ISEA, a participative end-user modeling approach that allows the stakeholders in a business process to collaborate together in a simple way to communicate and improve the business process elicitation in an accurate and understandable manner. Our approach covers the organizational perspective of business processes, exploits the information compiled during the elicitation of the organizational perspective and touches lightly an interactional perspective allowing users to create customized interface sketches to test the user interface navigability and the coherence within the processes. Thus, ISEA can be seen as a participative end-user modeling approach for business process elicitation and improvement.",
        "keywords": [
            "Business process management",
            "Requirements engineering",
            "Domain modeling",
            "User interfaces modeling",
            "Participative approach"
        ],
        "authors": [
            "Agnès Front",
            "Dominique Rieu",
            "Marco Santorum",
            "Fatemeh Movahedian"
        ],
        "file_path": "data/sosym-all/s10270-015-0489-6.pdf"
    },
    {
        "title": "Modeling big smart data",
        "submission-date": "2014/04",
        "publication-date": "2014/04",
        "abstract": "Large volumes of data are generated continuously by billions of human data producers, sensors, surveillance systems, communication devices and networks (e.g., the Internet). Proper analysis of this data can lead to new scientific insights, new products and services, more creative outputs (e.g., new recipes, music scores, fashion styles), improved performanceofbusinessandcivicorganizations,andtobetter informed government and non-government organizations. In other words, deriving information from these large volumes of data can lead to, among other things, smarter individuals capable of making scientific breakthroughs, producing innovative products, and making effective decisions.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-014-0409-1.pdf"
    },
    {
        "title": "Incorporating measurement uncertainty into OCL/UML primitive datatypes",
        "submission-date": "2018/12",
        "publication-date": "2019/07",
        "abstract": "The correct representation of the relevant properties of a system is an essential requirement for the effective use and wide adoption of model-based practices in industry. Uncertainty is one of the inherent properties of any measurement or estimation that is obtained in any physical setting; as such, it must be considered when modeling software systems deal with real data. Although a few modeling languages enable the representation of measurement uncertainty, these aspects are not normally incorporated into their type systems. Therefore, operating with uncertain values and propagating their uncertainty become cumbersome processes, which hinder their realization in real environments. This paper proposes an extension of OCL/UML primitive datatypes that enables the representation of the uncertainty that comes from physical measurements or user estimates into the models, together with an algebra of operations that are deﬁned for the values of these types.",
        "keywords": [
            "Measurement uncertainty",
            "OCL",
            "UML",
            "Primitive datatypes"
        ],
        "authors": [
            "Manuel F. Bertoa",
            "Loli Burgueño",
            "Nathalie Moreno",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-019-00741-0.pdf"
    },
    {
        "title": "Suggesting model transformation repairs for rule-based languages using a contract-based testing approach",
        "submission-date": "2020/06",
        "publication-date": "2021/05",
        "abstract": "Model transformations play an essential role in most model-driven software projects. As the size and complexity of model transformations increase, their reuse, evolution and maintenance become a challenge. This work further details the Model Transformation TEst Speciﬁcation (MoTES) approach, which leverages contract-based model testing techniques to assist engineers in model transformation evolution and repairing. The main novelty of our approach is to use contract-based model transformation testing as a foundation to derive suggestions of concrete adaptation actions. MoTES uses contracts to specify the expected behaviour of the model transformation under test. These contracts are transformed into model transformations which act as oracles on input–output model pairs, previously generated by executing the transformation under test on provided input models. By further processing, the oracles’ output model, precision and recall metrics are calculated for every output pattern (testing results). These metrics are then categorised to increase the user’s ability to interpret and act on them. The MoTES approach deﬁnes 8 cases for precision and recall values classiﬁcation (test result cases). As traceability information is retained from transformation rules to each output pattern, it is possible to classify each transformation rule involved according to its impact on the metrics, e.g. the number of true positives generated. The MoTES approach deﬁnes 37 cases for these classiﬁcations, with each one linked to a particular (abstract) action suggested on a rule, such as relaxation of the rules. A comprehensive evaluation of this approach is also presented, consisting of three case studies. Two previous case studies performed over two model transformations (UML2ER and E2M) are replicated to contrast MoTES with an existing model transformation fault localisation approach. An additional case study presents how MoTES helps with the evolution of an existing model transformation in the context of a reverse engineering project. Main evaluation results show that our approach can not only detect the errors introduced in the transformations but also localise the faulty rule and suggest the proper repair actions, which signiﬁcantly reduce testers’ effort. From a quantitative perspective, in the third case study, MoTES was able to indicate one faulty rule from 19 possibilities for each result case and suggest one or two repair actions from 6 possibilities for each faulty rule.",
        "keywords": [
            "Model Transformation",
            "Evolution",
            "Testing",
            "Repairing",
            "Testing Oracle",
            "Adaptations",
            "Veriﬁcation",
            "Fault Localisation"
        ],
        "authors": [
            "Roberto Rodriguez-Echeverria",
            "Fernando Macías",
            "Adrian Rutle",
            "José M. Conejero"
        ],
        "file_path": "data/sosym-all/s10270-021-00891-0.pdf"
    },
    {
        "title": "Special section of BPMDS’2012: artefacts and processes for business process modeling and management",
        "submission-date": "2012/06",
        "publication-date": "2014/06",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a two-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of Information Systems theory. The goals, format and history of BPMDS can be found on the web site http://www.bpmds.org/. According to Seligman et al. [1], an information systems engineering method has four pillars: a way of thinking, a way of modeling, a way of working and a way of supporting. The framework presented in [2] was inspired from this work and aims to evaluate the capacity of business process engineering methods to design ﬂexible business process models. According to this framework (i) the way of thinking verbalizes the assumptions and viewpoints of the method on the kinds of problem domains, solutions and modelers; (ii) the way of modeling provides information on the modeling concepts, on their properties and on their relationships; gives a formalism and notation to express business process models; (iii) the way of working structures the way in which business process models are designed; deﬁnes the possible tasks to be performed as part of the design and development S. Nurcan (B) University Paris 1 Panthéon Sorbonne, Paris, France e-mail: nurcan@univ-paris1.fr R. Schmidt Munich University of Applied Sciences, Munich, Germany e-mail: Rainer.Schmidt@hm.edu process; provides heuristics on how these tasks should be performed; (iv) the way of supporting refers to the tools that support the design and development of business process models and offers a repository to store and to exploit them. We observe in the literature and also in BPMDS working conferences series that research developing way of work-ings and methodological guidelines for designing appropri-ate and valuable business process models is spreading more and more. This special section presents ﬁve of them.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-014-0419-z.pdf"
    },
    {
        "title": "Models in software engineering – an introduction",
        "submission-date": "2002/10",
        "publication-date": "2003/02",
        "abstract": "Modelling is a concept fundamental for software engineering. In this paper, the word is defined and discussed from various perspectives. The most important types of models are presented, and examples are given. Models are very useful, but sometimes also dangerous, in particular to those who use them unconsciously. Such problems are shown. Finally, the role of models in software engineering research is discussed.",
        "keywords": [
            "Models",
            "Software engineering",
            "Metaphors",
            "SESAM"
        ],
        "authors": [
            "Jochen Ludewig"
        ],
        "file_path": "data/sosym-all/s10270-003-0020-3.pdf"
    },
    {
        "title": "Behavioral interfaces for executable DSLs",
        "submission-date": "2019/03",
        "publication-date": "2020/04",
        "abstract": "Executable domain-speciﬁc languages (DSLs) enable the execution of behavioral models. While an execution is mostly driven by the model content (e.g., control structures), many use cases require interacting with the running model, such as simulating scenarios in an automated or interactive way, or coupling the model with other models of the system or environment. The management of these interactions is usually hardcoded into the semantics of the DSL, which prevents its reuse for other DSLs and the provision of generic interaction-centric tools (e.g., event injector). In this paper, we propose a metalanguage for complementing the deﬁnition of executable DSLs with explicit behavioral interfaces to enable external tools to interact with executed models in a uniﬁed way. We implemented the proposed metalanguage in the GEMOC Studio and show how behavioral interfaces enable the realization of tools that are generic and thus usable for different executable DSLs.",
        "keywords": [
            "Language engineering",
            "Domain-speciﬁc language",
            "Model execution"
        ],
        "authors": [
            "Dorian Leroy",
            "Erwan Bousse",
            "Manuel Wimmer",
            "Tanja Mayerhofer",
            "Benoit Combemale",
            "Wieland Schwinger"
        ],
        "file_path": "data/sosym-all/s10270-020-00798-2.pdf"
    },
    {
        "title": "Applying UML/MARTE on industrial projects: challenges, experiences, and guidelines",
        "submission-date": "2013/06",
        "publication-date": "2014/03",
        "abstract": "ModelingandAnalysisofReal-TimeandEmbed- ded Systems (MARTE) is a Uniﬁed Modeling Language (UML) proﬁle, which has been developed to model concepts speciﬁc to Real-Time and Embedded Systems (RTES). In the last 5years, we have applied UML/MARTE to three distinct industrial problems in three industry sectors: architecture modeling and conﬁguration of large-scale and highly conﬁgurableintegratedcontrolsystems,model-basedrobust- nesstestingofcommunication-intensivesystems,andmodel- based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experience of solving these problems by applying UML/MARTE on four industrial case studies. We highlight the challenges we faced with respect to the industrial adoption of MARTE. Based on our combined experience, we derive a framework to guide practitioners for future applications of UML/MARTE in an industrial context. The framework provides a set of detailed guidelines that help reduce the gap between the modeling notations and real-world industrial application needs.",
        "keywords": [
            "UML",
            "MARTE",
            "Real-Time Embedded Systems",
            "Architecture Modeling",
            "Model-based Testing",
            "Industrial Case Studies"
        ],
        "authors": [
            "Muhammad Zohaib Iqbal",
            "Shaukat Ali",
            "Tao Yue",
            "Lionel Briand"
        ],
        "file_path": "data/sosym-all/s10270-014-0405-5.pdf"
    },
    {
        "title": "An exception handling framework for case management",
        "submission-date": "2020/11",
        "publication-date": "2022/04",
        "abstract": "In order to achieve their business goals, organizations heavily rely on the operational excellence of their business processes. In traditional scenarios, business processes are usually well-structured, clearly specifying when and how certain tasks have to be executed. Flexible and knowledge-intensive processes are gathering momentum, where a knowledge worker drives the execution of a process case and determines the exact process path at runtime. In the case of an exception, the knowledge worker decides on an appropriate handling. While there is initial work on exception handling in well-structured business processes, exceptions in case management have not been sufﬁciently researched. This paper proposes an exception handling framework for stage-oriented case management languages, namely Guard Stage Milestone Model, Case Management Model and Notation, and Fragment-based Case Management. The effectiveness of the framework is evaluated with two real-world use cases showing that it covers all relevant exceptions and proposed handling strategies.",
        "keywords": [
            "Exception handling",
            "Knowledge-intensive processes",
            "Flexible processes",
            "Case management"
        ],
        "authors": [
            "Kerstin Andree",
            "Sven Ihde",
            "Mathias Weske",
            "Luise Pufahl"
        ],
        "file_path": "data/sosym-all/s10270-022-00993-3.pdf"
    },
    {
        "title": "Discovering architecture-aware and sound process models of multi-agent systems: a compositional approach",
        "submission-date": "2021/05",
        "publication-date": "2022/05",
        "abstract": "A process model discovered from an event log of a multi-agent system often does not fully cover certain viewpoints of its architecture. We consider those concerned with the structure of a model explicitly reﬂecting agent behavior and interactions. Thedirectdiscoveryfromaneventlogofamulti-agentsystemmayresultinanunclearmodelstructureandover-generalizations of agent behavior. We suggest applying a compositional approach that yields architecture-aware process models of multi-agent systems. An event log of a multi-agent system is ﬁltered by the behavior of individual agents. Then, a multi-agent system model is a composition of agent models discovered from ﬁltered logs. We use an intermediate model, called an interface pattern, specifying agent interactions and representing the architecture of a multi-agent system. We design a collection of speciﬁc interface patterns modeling typical agent interactions. An interface pattern provides an abstract speciﬁcation of interactions and has a part corresponding to the behavior of each agent. We use structural transformations to map agent models discovered from ﬁltered logs on the respective parts in an interface pattern. If such a mapping exists, we guarantee that a composition of agent models preserves their soundness. We conduct a series of experiments to evaluate the compositional approach. Experimental results conﬁrm the improvement in the structure of process models discovered using the compositional approach compared to those discovered directly from event logs.",
        "keywords": [
            "Multi-agent systems",
            "Event logs",
            "Process mining",
            "Process discovery",
            "Petri nets",
            "Composition",
            "Transformations",
            "Interface patterns"
        ],
        "authors": [
            "Roman Nesterov",
            "Luca Bernardinello",
            "Irina Lomazova",
            "Lucia Pomello"
        ],
        "file_path": "data/sosym-all/s10270-022-01008-x.pdf"
    },
    {
        "title": "Uncertainty representation in software models: a survey",
        "submission-date": "2020/06",
        "publication-date": "2021/01",
        "abstract": "This paper provides a comprehensive overview and analysis of research work on how uncertainty is currently represented in software models. The survey presents the deﬁnitions and current research status of different proposals for addressing uncertainty modeling and introduces a classiﬁcation framework that allows to compare and classify existing proposals, analyze their current status and identify new trends. In addition, we discuss possible future research directions, opportunities and challenges.",
        "keywords": [
            "Software models",
            "Uncertainty",
            "Modeling languages",
            "UML",
            "Systematic literature review"
        ],
        "authors": [
            "Javier Troya",
            "Nathalie Moreno",
            "Manuel F. Bertoa",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-020-00842-1.pdf"
    },
    {
        "title": "Requirements document relations\nA reuse perspective on traceability through standards",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Relations between requirements are part of nearly every requirements engineering approach. Yet, relations of views, such as requirements documents, are scarcely considered. This is remarkable as requirements documents and their structure are a key factor in requirements reuse, which is still challenging. Explicit formalized relations between documents can help to ensure consistency, improve completeness, and facilitate review activities in general. For example, this is relevant in space engineering, where many challenges related to complex document dependencies occur: 1. Several contractors contribute to a project. 2. Requirements from standards have to be applied in several projects. 3. Requirements from previous phases have to be reused. We exploit the concept of “layered traceability”, explicitly considering documents as views on sets of individual requirements and speciﬁc traceability relations on and between all of these representation layers. Different types of relations and their dependencies are investigated with a special focus on requirement reuse through standards and formalized in an Object-Role Modelling (ORM) conceptual model. Automated analyses of requirement graphs based on this model are able to reveal document inconsistencies. We show examples of such queries in Neo4J/Cypher for the EagleEye case study. This work aims to be a step toward a better support to handle highly complex requirement document dependencies in large projects with a special focus on requirements reuse and to enable automated quality checks on dependent documents to facilitate requirements reviews.",
        "keywords": [
            "Requirement document relations",
            "Requirement reuse",
            "Standards",
            "Space engineering requirements",
            "ECSS",
            "Traceability"
        ],
        "authors": [
            "Katharina Großer",
            "Volker Riediger",
            "Jan Jürjens"
        ],
        "file_path": "data/sosym-all/s10270-021-00958-y.pdf"
    },
    {
        "title": "Aspectual templates in UML\nEnhancing the semantics of UML templates in OCL",
        "submission-date": "2013/10",
        "publication-date": "2015/04",
        "abstract": "UML Templates allow to capture reusable\nmodels through parameterization. The construct is general\nenough to be used in many ways, ranging from the rep-\nresentation of generic components (such as Java generics\nor C++ templates) to aspectual usage, including pattern-,\naspect- and view-oriented modeling. We concentrate on this\nlast usage and so-called aspectual templates which require\nthat parameters must form a model of systems in which\nto inject new functionalities. Starting from this strict con-\nstraint, we derive an in-depth semantic enhancement of the\nstandard. It is formalized as a fully UML-compliant inter-\npretation in OCL of the template construct and its binding\nmachanism. In particular, this aspectual interpretation must\nbe ensured in case of partial binding (not all parameters are\nvalued). Partial binding of UML is a powerful technique\nwhich allows to obtain richer templates from the compo-\nsition of other ones. As a major result, the present semantic\nenhancement is consistent with this capacity so that partial\nbinding of aspectual templates produces aspectual templates.\nFinally, at an operational level, an algorithm for aspectual\ntemplate (partial) bindingoperation is formulated and conse-\nquent reusable technology made available in EMF (Eclipse\nModeling Framework) is presented.",
        "keywords": [
            "Model templates",
            "UML",
            "OCL",
            "Metamodeling",
            "Aspects",
            "Patterns",
            "Template composition"
        ],
        "authors": [
            "Gilles Vanwormhoudt",
            "Olivier Caron",
            "Bernard Carré"
        ],
        "file_path": "data/sosym-all/s10270-015-0463-3.pdf"
    },
    {
        "title": "MDWA: a model-driven Web augmentation approach—coping with client- and server-side support",
        "submission-date": "2018/08",
        "publication-date": "2020/02",
        "abstract": "Web augmentation is a set of techniques allowing users to deﬁne and execute software which is dependent on the presentation layer of a concrete Web page. Through the use of specialized Web augmentation artifacts, the end users may satisfy several kinds of requirements that were not considered by the analysts, developers and stakeholders that built the application. Although some augmentation approaches are contemplating a server-side counterpart (to support aspects such as collaboration or cross-browser session management), the augmentation artifacts are usually purely client-side. The server-side support increases the capabilities of the augmentations, since it may allow sharing information among users and devices. So far, this support is often deﬁned and developed in an ad hoc way. Although it is clear that server-side support brings new possibilities, it is also true that developing and deploying server-side Web applications is a challenging task that end users hardly may handle. This work presents a novel approach for designing Web augmentation applications based on client-side and server-side components. We propose a model-driven approach that raises the abstraction level of both, client- and server-side developments. We provide a set of tools for designing the composition of the core application with new features on the back-end and the augmentation of pages in the front-end. The usability and the value of the produced augmentations have been evaluated through two experiments involving 30 people in total.",
        "keywords": [
            "Model-driven Web engineering",
            "Augmentation",
            "Web development",
            "Separation of concern"
        ],
        "authors": [
            "Matias Urbieta",
            "Sergio Firmenich",
            "Gabriela Bosetti",
            "Pedro Maglione",
            "Gustavo Rossi",
            "Miguel Angel Olivero"
        ],
        "file_path": "data/sosym-all/s10270-020-00779-5.pdf"
    },
    {
        "title": "Constraint-based test generation for automotive operating systems",
        "submission-date": "2014/02",
        "publication-date": "2015/01",
        "abstract": "This work suggests a method for systematically constructing a software-level environment model for safety checking automotive operating systems by introducing a constraint speciﬁcation language, OSEK_CSL. OSEK_CSL is designed to specify the usage constraints of automotive operating systems using a pre-deﬁned set of constraint types identiﬁed from the international standard OSEK/VDX. Each constraint speciﬁed in OSEK_CSL is interpreted as either a regular language or a context-free language that can be checked by a ﬁnite automaton or a pushdown automaton. The set of usage constraints is used to systematically classify the universal usage model of OSEK-/VDX-based operating systems and to generate test sequences with varying degrees of constraint satisfaction using LTL model checking. With pre-deﬁned constraint patterns and the full support of automation, test engineers can choose the degree of constraint satisfaction and generate test cases using combinatorial intersections of selected constraints that cover all corner cases classiﬁed by constraints. A series of experiments on an open-source automotive operating system show that our approach finds safety issuesmoreeffectivelythanconventionalspeciﬁcation-based testing, scenario-based testing, and conformance testing.",
        "keywords": [
            "Veriﬁcation",
            "Constraint speciﬁcation",
            "Operating system",
            "Automotive software",
            "Test generation"
        ],
        "authors": [
            "Yunja Choi",
            "Taejoon Byun"
        ],
        "file_path": "data/sosym-all/s10270-014-0449-6.pdf"
    },
    {
        "title": "The MDENET education platform: zero-install directed activities for learning MDE",
        "submission-date": "2024/06",
        "publication-date": "2025/04",
        "abstract": "Setting up and conﬁguring model-driven engineering (MDE) tools is not straightforward because the MDE tooling landscape is highly fragmented and because many MDE tools are research prototypes with limited documentation. This creates signiﬁcant accidental complexity for learners of MDE, who have to overcome installation and conﬁguration hurdles before they can even begin to focus on the core MDE concepts they should be learning. This is further complicated by the complexity of modern MDE tools, which can overwhelm new learners, making it difﬁcult for them to work out what they should do next to achieve a given goal. To address these challenges, we have developed a web-based playground platform that enables learners to engage with MDE learning activities without the need to install anything. The playground metaphor allows teachers to expose only those functionalities directly required for the completion of a particular learning activity. We present the general architecture of the platform, our approach to the declarative integration of new MDE tools, and the way in which teachers can ﬂexibly and declaratively deﬁne new MDE learning activities. We have used our platform in a range of different contexts, from live tutorials and 10-week university courses, to developing documentation webpages for MDE tools. We describe examples of such uses, showcasing the ﬂexible conﬁgurability of the platform for different types of activities and contexts.",
        "keywords": [
            "MDE",
            "Education",
            "Online",
            "No installation",
            "Playground"
        ],
        "authors": [
            "Steffen Zschaler",
            "Will Barnett",
            "Artur Boronat",
            "Antonio Garcia-Dominguez",
            "Dimitris Kolovos"
        ],
        "file_path": "data/sosym-all/s10270-025-01292-3.pdf"
    },
    {
        "title": "Using internal domain-speciﬁc languages to inherit tool support and modularity for model transformations",
        "submission-date": "2016/03",
        "publication-date": "2017/01",
        "abstract": "Model-driven engineering (MDE) has proved to be a useful approach to cope with today’s ever-growing complexity in the development of software systems; nevertheless, it is not widely applied in industry. As suggested by multiple studies, tool support is a major factor for this lack of adoption. Inparticular,thedevelopmentofmodeltransformationslacks good tool support. Additionally, modularization techniques are inevitable for the development of larger model transfor- mations to keep them maintainable. Existing tools for MDE, in particular model transformation approaches, are often developed by small teams and cannot keep up with advanced tool support for mainstream general-purpose programming languages, such as IntelliJ or Visual Studio. Internal DSLs are a promising solution to these problems. In this paper, we investigate the impact of design decisions of an internal DSL to the reuse of tool support and modularization concepts from the host language. We validate our ﬁndings in terms of understandability, applicability, tool support, and extensibil- ity using three case studies from academia, a model-driven engineering platform, and the industrial automation domain where we apply an implementation of an internal model transformation language on the .NET platform. The results conﬁrm the value of inherited modularity and tool support while conciseness and understandability are still competi- tive.",
        "keywords": [
            "Model-driven engineering",
            "Model transformation",
            "Domain-speciﬁc language",
            "Tool support",
            "Extensibility"
        ],
        "authors": [
            "Georg Hinkel",
            "Thomas Goldschmidt",
            "Erik Burger",
            "Ralf Reussner"
        ],
        "file_path": "data/sosym-all/s10270-017-0578-9.pdf"
    },
    {
        "title": "Mapping feature models onto domain models: ensuring consistency of configured domain models",
        "submission-date": "2011/06",
        "publication-date": "2012/12",
        "abstract": "We present an approach to model-driven software product line engineering which is based on feature models and domain models. A feature model describes both common and varying properties of the instances of a software product line. The domain model is composed of a structural model (package and class diagrams) and a behavioral model (story diagrams). Features are mapped onto the domain model by annotating elements of the domain model with features. An element of a domain model is specific to the features included in its feature annotation. An instance of the product line is defined by a set of selected features (a feature configuration). A configuration of the domain model is built by excluding all elements whose feature set is not included in the feature configuration. To ensure consistency of the configured domain model, we define constraints on the annotations of inter-dependent domain model elements. These constraints guarantee that a model element may be selected only when the model elements are also included on which it depends. Violations of dependency constraints may be removed automatically with the help of an error repair tool which propagates features to dependent model elements.",
        "keywords": [
            "Model-driven software product line engineering",
            "Feature models",
            "Domain models",
            "Feature mappings",
            "Dependency constraints"
        ],
        "authors": [
            "Thomas Buchmann",
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-012-0305-5.pdf"
    },
    {
        "title": "From types to type requirements: genericity for model-driven engineering",
        "submission-date": "2011/03",
        "publication-date": "2011/11",
        "abstract": "Model-driven engineering (MDE) is a software engineering paradigm that proposes an active use of models during the development process. This paradigm is inherently type-centric, in the sense that models and their manipulation are deﬁned over the types of speciﬁc meta-models. This fact hinders the reuse of existing MDE artefacts with other meta-models in new contexts, even if all these meta-models share common characteristics. To increase the reuse opportunities of MDE artefacts, we propose a paradigm shift from type-centric to requirement-centric speciﬁcations by bringing genericity into models, meta-models and model management operations. For this purpose, we introduce so-called concepts gathering structural and behavioural requirements for models and meta-models. In this way, model management operations are deﬁned over concepts, enabling the application of the operations to any meta-model satisfying the requirements imposed by the concept. Model templates rely on concepts to deﬁne suitable interfaces, hence enabling the definition of reusable model components. Finally, similar to mixinlayers,templatescanbedeﬁnedatthemeta-modellevel as well, to deﬁne languages in a modular way, as well as layers of functionality to be plugged-in into other meta-models. These ideas have been implemented in MetaDepth, a multi-level meta-modelling tool that integrates action languages from the Epsilon family for model management and code generation.",
        "keywords": [
            "Model-driven engineering",
            "Language engineering",
            "Meta-modelling",
            "Genericity",
            "Reutilization"
        ],
        "authors": [
            "Juan de Lara",
            "Esther Guerra"
        ],
        "file_path": "data/sosym-all/s10270-011-0221-0.pdf"
    },
    {
        "title": "The Unit-B method: reﬁnement guided by progress concerns",
        "submission-date": "2013/11",
        "publication-date": "2015/03",
        "abstract": "We present Unit-B, a formal method inspired by Event-B and UNITY. Unit-B aims at the stepwise design of software systems, satisfying safety and liveness properties. The method features the novel notion of coarse and ﬁne schedules, a generalisation of weak and strong fairness for specifying events’ scheduling assumptions. Based on events schedules, we propose proof rules to reason about progress properties and a reﬁnement order preserving both liveness and safety properties. We illustrate our approach by an example to show that systems development can be driven by not only safety but also liveness requirements.",
        "keywords": [
            "Progress properties",
            "Reﬁnement",
            "Fairness",
            "Scheduling",
            "Unit-B",
            "Proof-based formal methods",
            "Veriﬁcation of cyber-physical systems"
        ],
        "authors": [
            "Simon Hudon",
            "Thai Son Hoang",
            "Jonathan S. Ostroff"
        ],
        "file_path": "data/sosym-all/s10270-015-0456-2.pdf"
    },
    {
        "title": "CalcGraph: taming the high costs of deep learning using models",
        "submission-date": "2020/07",
        "publication-date": "2022/10",
        "abstract": "Models based on differential programming, like deep neural networks, are well established in research and able to outperform\nmanually coded counterparts in many applications. Today, there is a rising interest to introduce this ﬂexible modeling to\nsolve real-world problems. A major challenge when moving from research to application is the strict constraints on computa-\ntional resources (memory and time). It is difﬁcult to determine and contain the resource requirements of differential models,\nespecially during the early training and hyperparameter exploration stages. In this article, we address this challenge by intro-\nducing CalcGraph, a model abstraction of differentiable programming layers. CalcGraph allows to model the computational\nresources that should be used and then CalcGraph’s model interpreter can automatically schedule the execution respecting\nthe speciﬁcations made. We propose a novel way to efﬁciently switch models from storage to preallocated memory zones\nand vice versa to maximize the number of model executions given the available resources. We demonstrate the efﬁciency of\nour approach by showing that it consumes less resources than state-of-the-art frameworks like TensorFlow and PyTorch for\nsingle-model and multi-model execution.",
        "keywords": [
            "Differentiable programming",
            "Computational graph model",
            "Edge AI"
        ],
        "authors": [
            "Joe Lorentz",
            "Thomas Hartmann",
            "Assaad Moawad",
            "Francois Fouquet",
            "Djamila Aouada",
            "Yves Le Traon"
        ],
        "file_path": "data/sosym-all/s10270-022-01052-7.pdf"
    },
    {
        "title": "Scope in model transformations",
        "submission-date": "2015/10",
        "publication-date": "2016/08",
        "abstract": "Abstract A notion of hierarchical scope is commonplace in\nmany programmatic systems. In the context of model, and in\nparticular graph transformation, the use of scope can present\ntwo advantages: ﬁrst, more natural expression of transforma-\ntion application locality, and second, reduction in the number\nof match candidates, promising performance improvements.\nPrevious work on scope, however, has focused on applying\nit to rule hierarchies, which reduces the number of matches\nperformed, but not necessarily the cost of ﬁnding a single\nmatch. In this paper we deﬁne and explore a hierarchical\nscope formalism applied to the input graph, with associ-\nated modiﬁcations to the transformation rule deﬁnition. We\nthen experimentally evaluate the beneﬁts and challenges\nof our scoped model transformations in the state-of-the-\nart graph rewriting tool GrGen and our research-oriented,\nmeta-modeling and rule-based model transformation tool\nAToMPM. We use a non-trivial “ﬁre spreading” simulation\nCommunicated by Mr. Juan de Lara.",
        "keywords": [
            "Scope",
            "Graph scoping",
            "Graph grammar",
            "Rule-based model transformations",
            "Search plans",
            "Efﬁcient\npattern matching"
        ],
        "authors": [
            "M¯aris Jukšs",
            "Clark Verbrugge",
            "Maged Elaasar",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-016-0555-8.pdf"
    },
    {
        "title": "Code generation for classical-quantum software systems modeled in UML",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Quantum computing is gaining an increasing interest since it can solve certain problems exponentially faster than classical computing. Thus, many organizations are researching and launching investments for integrating quantum software into their existing systems. Software modernization (as based on Model-Driven Engineering) has been proposed to migrate from/to the so-called hybrid software systems, which integrate classical and quantum software. In that process, both, reverse engineering and restructuring phases, have already been investigated. However, forward engineering phase for generating hybrid source code from high-level design models has not yet been addressed. Thus, this research proposes a quantum code generation technique from extended UML design models. It consists of a set of Model-to-Text transformations (deﬁned through Epsilon Generation Language) to generate both Python and Qiskit code, which, respectively, integrate classical and quantum code. The transformation has been validated through a multi-case study with 7 hybrid software systems modeled in UML, which demonstrated that the transformation is effective and efﬁcient. The implication of this work is that the software modernization process for hybrid software systems can be completed by tackling forward engineering phase, and that Model-Driven Engineering can therefore globally facilitate industry adoption of quantum software.",
        "keywords": [
            "Quantum software",
            "Code generation",
            "MDE",
            "UML",
            "EGL"
        ],
        "authors": [
            "Luis Jiménez-Navajas",
            "Ricardo Pérez-Castillo",
            "Mario Piattini"
        ],
        "file_path": "data/sosym-all/s10270-024-01259-w.pdf"
    },
    {
        "title": "Turning event logs into process movies: animating what has really happened",
        "submission-date": "2013/12",
        "publication-date": "2014/09",
        "abstract": "Today’s information systems log vast amounts of data. These collections of data (implicitly) describe events (e.g. placing an order or taking a blood test) and, hence, provide information on the actual execution of business processes. The analysis of such data provides an excellent starting point for business process improvement. This is the realm of process mining, an area which has provided a repertoire of many analysis techniques. Despite the impressive capabilities of existing process mining algorithms, dealing with the abundance of data recorded by contemporary systems and devices remains a challenge. Of particular importance is the capability to guide the meaningful interpretation of “oceans of data” by process analysts. To this end, insights fromtheﬁeldofvisualanalyticscanbeleveraged.Thisarticle proposes an approach where process states are reconstructed from event logs and visualised in succession, leading to an animated history of a process. This approach is customisable in how a process state, partially deﬁned through a collec-tion of activity instances, is visualised: one can select a map and specify a projection of events on this map based on the properties of the events. This paper describes a comprehensive implementation of the proposal. It was realised using the open-source process mining framework ProM. Moreover, this paper also reports on an evaluation of the approach conducted with Suncorp, one of Australia’s largest insurance companies.",
        "keywords": [
            "Process mining",
            "Visual analytics",
            "Event-log animation",
            "Process visualisation"
        ],
        "authors": [
            "Massimiliano de Leoni",
            "Suriadi Suriadi",
            "Arthur H. M. ter Hofstede",
            "Wil M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-014-0432-2.pdf"
    },
    {
        "title": "An analysis of capability meta‑models for expressing dynamic business transformation",
        "submission-date": "2019/10",
        "publication-date": "2020/12",
        "abstract": "Environmental dynamism is gaining ground as a driving force for enterprise transformation. To address the changes, the capabilities of digital enterprises need to adapt. Capability modeling can facilitate this process of transformation. However, a plethora of approaches for capability modeling exist. This study explores how concepts relevant to change have been implemented in the meta-models of these approaches, aiming to visualize relationships among change-related concepts, and identify ways to improve capability modeling toward a more efficient depiction of capability change. The concepts are visualized in concept maps, and a framework is developed to assist the classification of concepts relevant to change functions. Similarities and differences among the existing models are discussed, leading to suggestions toward improvements of capability modeling for capability adaptation.",
        "keywords": [
            "Capability",
            "Enterprise modeling",
            "Change",
            "Adaptability",
            "Transformation"
        ],
        "authors": [
            "Georgios Koutsopoulos",
            "Martin Henkel",
            "Janis Stirna"
        ],
        "file_path": "data/sosym-all/s10270-020-00843-0.pdf"
    },
    {
        "title": "Integrating the analysis of multiple non-functional properties in model-driven engineering",
        "submission-date": "2021/06",
        "publication-date": "2021/11",
        "abstract": "This paper discusses the progress made so far and future challenges in integrating the analysis of multiple Non-Functional Properties (NFP) (such as performance, schedulability, reliability, availability, scalability, security, safety, and maintainability) intotheModel-DrivenEngineering(MDE)process.Thegoalistoguidethedesignchoicesfromanearlystageandtoensurethat the system under construction will meet its non-functional requirements. The evaluation of the NFPs considered in this paper uses various kinds of NFP analysis models (also known as quality models) based on existent formalisms and tools developed over the years. Examples are queueing networks, stochastic Petri nets, stochastic process algebras, Markov chains, fault trees, probabilistic time automata, etc. In the MDE context, these models are automatically derived by model transformations from the software models built for development. Developing software systems that exhibit a good trade-off between multiple NFPs is difﬁcult because the design of the software under construction and its underlying platforms have a large number of degrees of freedom spanning a very large discontinuous design space, which cannot be exhaustively explored. Another challenge in balancing the NFPs of a system under construction is due to the fact that some NFPs are conﬂicting—when one gets better the other gets worse—so an appropriate software process is needed to evaluate and balance all the non-functional requirements. The integration approach discussed in this paper is based on an ecosystem of inter-related heterogeneous modeling artifacts intended to support the following features: feedback of analysis results, consistent co-evolution of the software and analysis models, cross-model traceability, incremental propagation of changes across models, (semi)automated software process steps, and metaheuristics for reducing the design space size to be explored.",
        "keywords": [
            "Non-functional properties",
            "Model-driven engineering",
            "Analysis integration"
        ],
        "authors": [
            "Dorina C. Petriu"
        ],
        "file_path": "data/sosym-all/s10270-021-00953-3.pdf"
    },
    {
        "title": "A graph-theoretic method for the inductive development of reference process models",
        "submission-date": "2014/10",
        "publication-date": "2015/09",
        "abstract": "Business process management is one of the most widely discussed topics in information systems research. As process models advance in both complexity and maturity, reference models, serving as reusable blueprints for the development of individual models, gain more and more importance. Only a few business domains have access to commonly accepted reference models, so there is a widespread need for the development of new ones. This article describes a new inductive approach for the development of reference models, based on existing individual models from the respective domain. It employs a graph-based paradigm, exploiting the underlying graph structures of process models by identifying frequent common subgraphs of the individual models, analyzing their order relations, and merging them into a new model. This newly developed approach is outlined and evaluated in this contribution. It is applied in three different case studies and compared to other approaches to the inductive development of reference models in order to highlight its characteristics as well as assets and drawbacks.",
        "keywords": [
            "Reference modeling",
            "Frequent subgraphs",
            "Order matrices",
            "Inductive development of reference models"
        ],
        "authors": [
            "Jana-Rebecca Rehse",
            "Peter Fettke",
            "Peter Loos"
        ],
        "file_path": "data/sosym-all/s10270-015-0490-0.pdf"
    },
    {
        "title": "Deontic BPMN: a powerful extension of BPMN with a trusted model transformation",
        "submission-date": "2012/04",
        "publication-date": "2013/03",
        "abstract": "The Business Process Model and Notation (BPMN) is a widely-used standard for process modelling. A drawback of BPMN, however, is that modality is implicitly expressed through the structure of the process ﬂow but not directly within the corresponding activity. Thus, an extension of BPMN with deontic logic has been proposed in previous work, called Deontic BPMN. Deontic BPMN reduces the structural complexity of the process ﬂow and increases the readability by explicitly highlighting obligatory and permissibleactivities.Inaddition,analgebraicgraphtransformation from a subset of BPMN to Deontic BPMN, called Deontic BpmnGTS, has been deﬁned. The goal of the current research is to show that DeonticBpmnGTS is terminating and confluent, resulting in a globally deterministic transformation. Moreover, the semantic equivalence of BPMN models and the respective Deontic BPMN models is proven based on Abstract State Machines (ASMs). Thus, DeonticBpmnGTS can be called a trusted model transformation.",
        "keywords": [
            "Business process modelling",
            "BPMN",
            "Deontic logic",
            "Graph transformation",
            "Deterministic transformation",
            "Semantic analysis"
        ],
        "authors": [
            "Christine Natschläger",
            "Felix Kossak",
            "Klaus-Dieter Schewe"
        ],
        "file_path": "data/sosym-all/s10270-013-0329-5.pdf"
    },
    {
        "title": "Model-driven engineering city spaces via bidirectional model transformations",
        "submission-date": "2020/03",
        "publication-date": "2021/02",
        "abstract": "Engineering cyber-physical systems inhabiting contemporary urban spatial environments demands software engineering facilities to support design and operation. Tools and approaches in civil engineering and architectural informatics produce artifacts that are geometrical or geographical representations describing physical spaces. The models we consider conform to the CityGML standard; although relying on international standards and accessible in machine-readable formats, such physical space descriptions often lack semantic information that can be used to support analyses. In our context, analysis as commonly understood in software engineering refers to reasoning on properties of an abstracted model—in this case a city design. We support model-based development, firstly by providing a way to derive analyzable models from CityGML descriptions, and secondly, we ensure that changes performed are propagated correctly. Essentially, a digital twin of a city is kept synchronized, in both directions, with the information from the actual city. Specifically, our formal programming technique and accompanying technical framework assure that relevant information added, or changes applied to the domain (resp. analyzable) model are reflected back in the analyzable (resp. domain) model automatically and coherently. The technique developed is rooted in the theory of bidirectional transformations, which guarantees that synchronization between models is consistent and well behaved. Produced models can bootstrap graph-theoretic, spatial or dynamic analyses. We demonstrate that bidirectional transformations can be achieved in practice on real city models.",
        "keywords": [
            "Bidirectional model transformations",
            "Model-driven engineering",
            "CityGML",
            "Digital twins"
        ],
        "authors": [
            "Ennio Visconti",
            "Christos Tsigkanos",
            "Zhenjiang Hu",
            "Carlo Ghezzi"
        ],
        "file_path": "data/sosym-all/s10270-020-00851-0.pdf"
    },
    {
        "title": "A generic model decomposition technique and its application to the Eclipse modeling framework",
        "submission-date": "2012/08",
        "publication-date": "2013/06",
        "abstract": "Model-driven software development aims at easing the process of software development by using models as primary artifacts. Although less complex than the real systems, they are based on models tend to be complex nevertheless, thus making the task of handling them non-trivial in many cases. In this paper, we propose a generic model decomposition technique to facilitate model management by decomposing complex models into smaller sub-models that conform to the same metamodel as the original model. The technique is based upon a formal foundation that consists of a formal capturing of the concepts of models, metamodels, and model conformance; a formal constraint language based on EssentialOCL; and a set of formally proved properties of the technique. We organize the decomposed sub-models in a mathematical structure as a lattice, and design a linear-time algorithm for constructing this decomposition. The generic model decomposition technique is applied to the Eclipse modeling framework, and the result is used to build a solution to a specific model comprehension problem of Ecore models based upon model pruning. We report two case studies of the model comprehension method: one in BPMN and the other in fUML.",
        "keywords": [
            "MDE",
            "EMF",
            "Model decomposition",
            "Model comprehension",
            "Linear-time algorithm",
            "Sub-model lattice",
            "OCL",
            "EssentialOCL",
            "BPMN",
            "fUML"
        ],
        "authors": [
            "Qin Ma",
            "Pierre Kelsen",
            "Christian Glodt"
        ],
        "file_path": "data/sosym-all/s10270-013-0348-2.pdf"
    },
    {
        "title": "Survey and classiﬁcation of model transformation tools",
        "submission-date": "2017/05",
        "publication-date": "2018/03",
        "abstract": "Model transformation lies at the very core of model-driven engineering, and a large number of model transformation languages and tools have been proposed over the last few years. These tools can be used to develop, transform, merge, exchange, compare, and verify models and metamodels. In this paper, we present a comprehensive catalog of existing metamodel-based transformation tools and compare them using a qualitative framework. We begin by organizing the 60 tools we identiﬁed into a general classiﬁcation based on the transformation approach used. We then compare these tools using a number of particular facets, where each facet belongs to one of six different categories and may contain several attributes. The results of the study are discussed in detail and made publicly available in a companion website with a capability to search for tools using the speciﬁed facets as search criteria. Our study provides a thorough picture of the state-of-the-art in model transformation techniques and tools. Our results are potentially beneﬁcial to many stakeholders in the modeling community, including practitioners, researchers, and transformation tool developers.",
        "keywords": [
            "Model-driven development",
            "Model transformation tools",
            "Metamodel",
            "Classiﬁcation",
            "Survey"
        ],
        "authors": [
            "Naﬁseh Kahani",
            "Mojtaba Bagherzadeh",
            "James R. Cordy",
            "Juergen Dingel",
            "Daniel Varró"
        ],
        "file_path": "data/sosym-all/s10270-018-0665-6.pdf"
    },
    {
        "title": "Experimental evaluation of a novel equivalence class partition testing strategy",
        "submission-date": "2016/07",
        "publication-date": "2017/03",
        "abstract": "In this paper, a complete model-based equivalence class testing strategy recently developed by the authors is experimentally evaluated. This black-box strategy applies to deterministic systems with inﬁnite input domains and ﬁnite internal state and output domains. It is complete with respect to a given fault model. This means that conforming behaviours will never be rejected, and all non-conforming behaviours inside a given fault domain will be uncovered. We investigate the question how this strategy performs for systems under test whose behaviours lie outside the fault domain. Furthermore, a strategy extension is presented, that is based on randomised data selection from input equivalence classes. While this extension is still complete with respect to the given fault domain, it also promises a higher test strength when applied against members outside this domain. This is conﬁrmed by an experimental evaluation that compares mutation coverage achieved by the original and the extended strategy with the coverage obtained by random testing. For mutation generation, not only typical software errors, but also critical HW/SW integration errors are considered. The latter can be caused by mismatches between hardware and software design, even in the presence of totally correct software.",
        "keywords": [
            "Model-based testing",
            "Equivalence class partition testing",
            "Adaptive random testing",
            "SysML",
            "SystemC",
            "State transition systems"
        ],
        "authors": [
            "Felix Hübner",
            "Wen-ling Huang",
            "Jan Peleska"
        ],
        "file_path": "data/sosym-all/s10270-017-0595-8.pdf"
    },
    {
        "title": "Trade-off analysis for SysML models using decision points and CSPs",
        "submission-date": "2018/02",
        "publication-date": "2019/01",
        "abstract": "The expected beneﬁts of model-based systems engineering (MBSE) include assistance to the system designer in ﬁnding the set of optimal architectures and making trade-off analysis. Design objectives such as cost, performance and reliability are often conﬂicting. The SysML-based methods OOSEM and the ARCADIA method focus on the design and analysis of one alternative of the system. They freeze the topology and the execution platform before optimization starts. Further, their limitation quickly appears when a large number of alternatives were evaluated. The paper avoids these problems and improves trade-off analysis in a MBSE approach by combining the SysML modeling language and so-called “decision points.” An enhanced SysML model with decision points shows up alternatives for component redundancy and instance selection and allocation. The same SysML model is extended with constraints and objective functions using an optimization context and parametric diagrams. Then, a representation of a constraint satisfaction multi-criteria objective problem is generated and solved with a combination of solvers. A demonstrator implements the proposed approach into an Eclipse plug-in; it uses the Papyrus and CSP solvers, both are open-source tools. A case study illustrates the methodology: a mission controller for an Unmanned Aerial Vehicle that includes a stereoscopic camera sensor module.",
        "keywords": [
            "MBSE",
            "Optimization",
            "SysML",
            "CSP",
            "Papyrus",
            "System engineering",
            "Optimal architecture design",
            "Decision points"
        ],
        "authors": [
            "Patrick Leserf",
            "Pierre de Saqui-Sannes",
            "Jérôme Hugues"
        ],
        "file_path": "data/sosym-all/s10270-019-00717-0.pdf"
    },
    {
        "title": "An Analysis of the Three Types of AASs and their Feasibility for Digital Twin Engineering",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Engineering digital twins is a software and systems engineering challenge for which no systematic approach exists. The\nAsset Administration Shell is becoming a popular foundation for digital twins in Industry 4.0 and it comes in different types\nthat support the engineering of different kinds and parts of digital twins. We investigate how it supports realizing common\nrequirements for digital twins. To this end, we investigate how each of the three Asset Administration Shell types can\ncontribute to the systematic engineering of speciﬁc components of digital twins. Therefore, we analyzed popular deﬁnitions\nand conceptual models of digital twins and extracted requirements that at least two of them share. We compare the resulting\nrequirements with Asset Administration Shells of different types and conclude with open challenges in the implementation\nof digital twins with this technology. This supports practitioners and researchers in identifying the most suitable type of Asset\nAdministration Shell for their speciﬁc digital twin engineering needs and identiﬁes gaps worthy of future research toward a\nsystematic engineering of digital twins.",
        "keywords": [
            "Asset administration shell",
            "Digital twin",
            "Requirements",
            "Manufacturing"
        ],
        "authors": [
            "Jingxi Zhang",
            "Carsten Ellwein",
            "Malte Heithoff",
            "Judith Michael",
            "Andreas Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-024-01255-0.pdf"
    },
    {
        "title": "Hazard-driven realization views for Component Fault Trees",
        "submission-date": "2019/06",
        "publication-date": "2020/03",
        "abstract": "Traditionally, the preferred means of documentation used by safety engineers have been sheets- and text-based solutions. However, in the last decades, the introduction of model-driven engineering in conjunction with Component-Based Design has been influencing the way safety engineers perform their tasks; especially in the area of fault analysis, model-driven approaches have been developed aimed at coupling fault trees with architecture models. Doing this fosters communication between engineers, may reduce design effort, and makes artifacts easier to maintain and reuse. In this paper, we want to move forward in this direction and take another step in the modeling of Component Fault Trees in combination with the modeling of the architecture design. We propose a hazard-centric approach for the definition of multiple realization views for fault analysis using Component Fault Trees. The approach is composed of a modeling method and a tool solution. We illustrate our approach with a real-life example from the automotive industry.",
        "keywords": [
            "Model-driven engineering",
            "Component-based",
            "Hazard-centric",
            "Component Fault Trees",
            "Realization view"
        ],
        "authors": [
            "David Santiago Velasco Moncada"
        ],
        "file_path": "data/sosym-all/s10270-020-00792-8.pdf"
    },
    {
        "title": "Standards harmonization: theory and practice",
        "submission-date": "2011/06",
        "publication-date": "2011/08",
        "abstract": "As software engineering (and other) standards are\ndeveloped over a period of years or decades, the suite of\nstandards thus developed often begins to lose any cohesion\nthat it originally possessed. This has led to discussions in\nthe standards communities of possible collaborative devel-\nopment, interoperability and harmonization of their existing\nstandards. Here, I assess how such harmonization efforts may\nbe aided by recent research results to create better quality\nstandards to replace the status quo.",
        "keywords": [
            "Standards",
            "Modelling",
            "Metamodels",
            "Software development",
            "Quality"
        ],
        "authors": [
            "B. Henderson-Sellers"
        ],
        "file_path": "data/sosym-all/s10270-011-0213-0.pdf"
    },
    {
        "title": "Towards a model-driven engineering approach for the assessment\nof non-functional properties using multi-formalism",
        "submission-date": "2016/02",
        "publication-date": "2018/02",
        "abstract": "Model-driven techniques can be used to automatically produce formal models from different views of a system realised by\nusing several modelling languages and notations. Speciﬁcations are transformed into formal models so facilitating the analysis\nof complex system for design, validation or veriﬁcation purposes. However, no single formalism suits for representing all\nsystem’s views. In particular, the assessment of non-functional properties often requires integrated modelling approaches. The\nultimate goal of the research work described in this paper is to develop a comprehensive, theoretical and practical framework\nable to support the development and the integration of new or existing model-driven approaches for the automatic generation\nof multi-formalism models. This paper deﬁnes the core theoretical ideas on which the framework is based and demonstrates\ntheir concrete applicability to the development of a multi-formalism approach for performability assessment.",
        "keywords": [
            "Multi-formalism",
            "UML proﬁle",
            "Performability",
            "Model-driven engineering",
            "Generalised Stochastic Petri Nets",
            "Repairable fault trees"
        ],
        "authors": [
            "Simona Bernardi",
            "Stefano Marrone",
            "José Merseguer",
            "Roberto Nardone",
            "Valeria Vittorini"
        ],
        "file_path": "data/sosym-all/s10270-018-0663-8.pdf"
    },
    {
        "title": "Where does model-driven engineering help? Experiences from three industrial cases",
        "submission-date": "2010/09",
        "publication-date": "2011/10",
        "abstract": "There have been few experience reports from industry on how Model-Driven Engineering (MDE) is applied and what the beneﬁts are. This paper summarizes the experiences of three large industrial participants in a European research project with the objective of developing techniques and tools for applying MDE on the development of large and complex software systems. The participants had varying degrees of previous experience with MDE. They found MDE to be particularly useful for providing abstractions of complex systems at multiple levels or from different viewpoints, for the development of domain-speciﬁc models that facilitate communication with non-technical experts, for the purposes of simulation and testing, and for the consumptionofmodelsforanalysis,suchasperformance-related decision support and system design improvements. From the industrial perspective, a methodology is considered to be use-ful and cost-efﬁcient if it is possible to reuse solutions in multiple projects or products. However, developing reusable solutions required extra effort and sometimes had a negative impact on the performance of tools. While the companies identiﬁed several beneﬁts of MDE, merging different tools with one another in a seamless development environment required several transformations, which increased the required implementation effort and complexity. Additionally, user-friendliness of tools and the provision of features for managing models of complex systems were identiﬁed as crucial for a wider industrial adoption of MDE.",
        "keywords": [
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Simulation",
            "Experience report",
            "Eclipse",
            "Complex systems"
        ],
        "authors": [
            "Parastoo Mohagheghi",
            "Wasif Gilani",
            "Alin Stefanescu",
            "Miguel A. Fernandez",
            "Bjørn Nordmoen",
            "Mathias Fritzsche"
        ],
        "file_path": "data/sosym-all/s10270-011-0219-7.pdf"
    },
    {
        "title": "Unique identiﬁcation of elements in evolving software models",
        "submission-date": "2011/03",
        "publication-date": "2013/02",
        "abstract": "Evolving models are often managed in ﬁle-based software conﬁguration management systems. This causes the identiﬁcationproblem:ifthemodelelementsarenotassigned with globally unique identiﬁers, we cannot identify them over time. However, if such identiﬁers would be given, they can be misleading because the elements to which they are assigned might change completely. As a consequence, evo- lution becomes incomprehensible, partial transformation is hampered, and sufﬁcient management of inter-model relationships (e.g. traceability links) is impeded. This article presents an approach to identify model elements or even complete model fragments over time. It establishes a ﬁne- grained history representation to describe model evolution. The representation contains identiﬁcation links between the elements of different model revisions allowing us to identify elements of a given revision in other revisions or variants of the model. Due to the explicit expression of model evo- lution, it further enables the capturing of changes that have been applied to the ﬁne-grained elements inside a model.",
        "keywords": [
            "Model evolution",
            "Traceability",
            "Identiﬁcation"
        ],
        "authors": [
            "Sven Wenzel"
        ],
        "file_path": "data/sosym-all/s10270-012-0311-7.pdf"
    },
    {
        "title": "An integrated conceptual model for information system security risk management supported by enterprise architecture management",
        "submission-date": "2016/09",
        "publication-date": "2018/02",
        "abstract": "Risk management is today a major steering tool for any organisation wanting to deal with information system (IS) security. However, IS security risk management (ISSRM) remains a difﬁcult process to establish and maintain, mainly in a context of multi-regulations with complex and inter-connected IS. We claim that a connection with enterprise architecture management (EAM) contributes to deal with these issues. A ﬁrst step towards a better integration of both domains is to deﬁne an integrated EAM-ISSRM conceptual model. This paper is about the elaboration and validation of this model. To do so, we improve an existing ISSRM domain model, i.e. a conceptual model depicting the domain of ISSRM, with the concepts of EAM. The validation of the EAM-ISSRM integrated model is then performed with the help of a validation group assessing the utility and usability of the model.",
        "keywords": [
            "Risk management",
            "Security",
            "Enterprise architecture",
            "ArchiMate"
        ],
        "authors": [
            "Nicolas Mayer\nJocelyn Aubert\nEric Grandry\nChristophe Feltus\nElio Goettelmann\nRoel Wieringa"
        ],
        "file_path": "data/sosym-all/s10270-018-0661-x.pdf"
    },
    {
        "title": "Guest editorial for EMMSAD’2019 special section",
        "submission-date": "2020/11",
        "publication-date": "2021/01",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) conference series organized 24 events from 1996 to 2019, associated with CAiSE (Conference on Advanced Information Systems Engineering). From 2009, EMMSAD has become a two-days working conference.From2017,EMMSADbestpapersareinvitedtosubmit extended versions for considering their publication in the Journal of Software and Systems Modeling (SoSyM). The main topics of the EMMSAD series have the focus on models and modeling methods for software and information systems development, requirements engineering, enterprise modeling and architecture, and business process management. The conferencefurtheraddressesevaluationofmodelingmethods through a variety of empirical and nonempirical approaches. The aims, topics, and history of EMMSAD can be also found on its website at http://www.emmsad.org/. ",
        "keywords": [],
        "authors": [
            "Iris Reinhartz-Berger",
            "Jelena Zdravkovic"
        ],
        "file_path": "data/sosym-all/s10270-020-00845-y.pdf"
    },
    {
        "title": "Models@run.time: a guided tour of the state of the art and research challenges",
        "submission-date": "2018/02",
        "publication-date": "2019/01",
        "abstract": "More than a decade ago, the research topic models@run.time was coined. Since then, the research area has received increasing\nattention. Given the proliﬁc results during these years, the current outcomes need to be sorted and classiﬁed. Furthermore,\nmany gaps need to be categorized in order to further develop the research topic by experts of the research area but also\nnewcomers. Accordingly, the paper discusses the principles and requirements of models@run.time and the state of the art of\nthe research line. To make the discussion more concrete, a taxonomy is deﬁned and used to compare the main approaches and\nresearch outcomes in the area during the last decade and including ancestor research initiatives. We identiﬁed and classiﬁed 275\npapers on models@run.time, which allowed us to identify the underlying research gaps and to elaborate on the corresponding\nresearch challenges. Finally, we also facilitate sustainability of the survey over time by offering tool support to add, correct\nand visualize data.",
        "keywords": [
            "Models@run.time",
            "Self-reﬂection",
            "Causal connection",
            "Systematic literature review"
        ],
        "authors": [
            "Nelly Bencomo",
            "Sebastian Götz",
            "Hui Song"
        ],
        "file_path": "data/sosym-all/s10270-018-00712-x.pdf"
    },
    {
        "title": "Speciﬁcation and automated validation of staged reconﬁguration processes for dynamic software product lines",
        "submission-date": "2014/10",
        "publication-date": "2015/05",
        "abstract": "Dynamic software product lines (DSPLs) propose elaborated design and implementation principles for engineering highly conﬁgurable runtime-adaptive systems in a sustainable and feature-oriented way. For this, DSPLs add to classical software product lines (SPL) the notions of (1) staged (pre-)conﬁgurations with dedicated binding times for each individual feature, and (2) continuous run-time reconﬁgurations of dynamic features throughout the entire product life cycle. Especially in the context of safety-and mission-critical systems, the design of reliable DSPLs requires capabilities for accurately specifying and validating arbitrary complex constraints among conﬁguration parame-ters and/or respective reconﬁguration options. Compared to classical SPL domain analysis which is usually based on Boolean constraint solving, DSPL validation, therefore, further requires capabilities for checking temporal properties of reconﬁguration processes. In this article, we present a comprehensive approach for modeling and automatically verifying essential validity properties of staged reconﬁgura-tion processes with complex binding time constraints during DSPL domain engineering. The novel modeling concepts introduced are motivated by (re-)conﬁguration constraints apparent in a real-world industrial case study from the automation engineering domain, which are not properly expressibleandanalyzableusingstate-of-the-artSPLdomain modeling approaches. We present a prototypical tool imple-mentation based on the model checker SPIN and present evaluation results obtained from our industrial case study, demonstrating the applicability of the approach.",
        "keywords": [
            "Dynamic software product lines",
            "Staged conﬁguration",
            "Model-based domain engineering and validation",
            "Model checking"
        ],
        "authors": [
            "Malte Lochau",
            "Johannes Bürdek",
            "Stefan Hölzle",
            "Andy Schürr"
        ],
        "file_path": "data/sosym-all/s10270-015-0470-4.pdf"
    },
    {
        "title": "Extending single- to multi-variant model transformations by trace-based propagation of variability annotations",
        "submission-date": "2019/07",
        "publication-date": "2020/03",
        "abstract": "Model-driven engineering involves the construction of models on different levels of abstraction. Software engineers are supported by model transformations, which automate the transition from high- to low-level models. Product line engineering denotes a systematic process that aims at developing different product variants from a set of reusable assets. When model-driven engineering is combined with product line engineering, engineers have to deal with multi-variant models. In annotative approaches to product line engineering, model elements are decorated with annotations, i.e., Boolean expressions that define the product variants in which model elements are to be included. In model-driven product line engineering, domain engineers require multi-variant transformations, which create multi-variant target models from multi-variant source models. We propose a reuse-based gray-box approach to realizing multi-variant model transformations. We assume that single-variant transformations already exist, which have been developed for model-driven engineering, without considering product lines. Furthermore, we assume that single-variant transformations create traces, which comprise the steps executed in order to derive target models from source models. Single-variant transformations are extended into multi-variant transformations by trace-based propagation: after executing a single-variant transformation, the resulting single-variant target model is enriched with annotations that are calculated with the help of the transformation’s trace. This approach may be applied to single-variant transformations written in different languages and requires only access to the trace, not to the respective transformation definition. We also provide a correctness criterion for trace-based propagation, and a proof that this criterion is satisfied under the prerequisites of a formal computational model.",
        "keywords": [
            "Model transformation",
            "Software product line",
            "Annotative variability"
        ],
        "authors": [
            "Bernhard Westfechtel",
            "Sandra Greiner"
        ],
        "file_path": "data/sosym-all/s10270-020-00791-9.pdf"
    },
    {
        "title": "An executable formal semantics for UML-RT",
        "submission-date": "2012/08",
        "publication-date": "2014/02",
        "abstract": "We propose a formal semantics for UML-RT, a UML proﬁle for real-time and embedded systems. The formal semantics is given by mapping UML-RT models into a language called kiltera, a real-time extension of the π-calculus. Previous attempts to formalize the semantics of UML-RT have fallen short by considering only a very small subset of the language and providing fundamentally incomplete semantics based on incorrect assumptions, such as a one-to-one correspondence between “capsules” and threads. Our semantics is novel in several ways: (1) it deals with both state machine diagrams and capsule diagrams; (2) it deals with aspects of UML-RT that have not been formalized before, such as thread allocation, service provision points, and service access points; (3) it supports an action language; and (4) the translation has been implemented in the form of a transformation from UML-RT models created with IBM’s RSA-RTE tool, into kiltera code. To our knowledge, this is the most comprehensive formal semantics for UML-RT to date.",
        "keywords": [
            "UML-RT",
            "RTE",
            "Modelling",
            "Semantics"
        ],
        "authors": [
            "Ernesto Posse",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-014-0399-z.pdf"
    },
    {
        "title": "Using language workbenches and domain-speciﬁc languages for safety-critical software development",
        "submission-date": "2017/11",
        "publication-date": "2018/05",
        "abstract": "Language workbenches support the efﬁcient creation, integration, and use of domain-speciﬁc languages. Typically, they\nexecute models by code generation to programming language code. This can lead to increased productivity and higher\nquality. However, in safety-/mission-critical environments, generated code may not be considered trustworthy, because of\nthe lack of trust in the generation mechanisms. This makes it harder to justify the use of language workbenches in such an\nenvironment. In this paper, we demonstrate an approach to use such tools in critical environments. We argue that models\ncreated with domain-speciﬁc languages are easier to validate and that the additional risk resulting from the transformation to\ncode can be mitigated by a suitably designed transformation and veriﬁcation architecture. We validate the approach with an\nindustrial case study from the healthcare domain. We also discuss the degree to which the approach is appropriate for critical\nsoftware in space, automotive, and robotics systems.",
        "keywords": [
            "Domain-speciﬁc languages",
            "Safety-critical software development",
            "Case study",
            "Language workbenches"
        ],
        "authors": [
            "Markus Voelter",
            "Bernd Kolb",
            "Klaus Birken",
            "Federico Tomassetti",
            "Patrick Alff",
            "Laurent Wiart",
            "Andreas Wortmann",
            "Arne Nordmann"
        ],
        "file_path": "data/sosym-all/s10270-018-0679-0.pdf"
    },
    {
        "title": "Extending the Uniﬁed Modeling Language for ontology development",
        "submission-date": "2002/02",
        "publication-date": "2002/12",
        "abstract": "There is rapidly growing momentum for web enabled agents that reason about and dynamically integrate the appropriate knowledge and services at run-time. The dynamic integration of knowledge and services depends on the existence of explicit declarative semantic models (ontologies). We have been building tools for ontology development based on the Uniﬁed Modeling Language (UML). This allows the many mature UML tools, models and expertise to be applied to knowledge representation systems, not only for visualizing complex ontologies but also for managing the ontology development process. UML has many features, such as proﬁles, global modularity and extension mechanisms that are not generally available in most ontology languages. However, ontology languages have some features that UML does not support. Our paper iden-tiﬁes the similarities and diﬀerences (with examples) between UML and the ontology languages RDF and DAML+OIL. To reconcile these diﬀerences, we pro-pose a modiﬁcation to the UML metamodel to address some of the most problematic diﬀerences. One of these is the ontological concept variously called a property, relation or predicate. This notion corresponds to the UML concepts of association and attribute. In ontology languages properties are ﬁrst-class modeling elem-ents, but UML associations and attributes are not ﬁrst-class. Our proposal is backward-compatible with existing UML models while enhancing its viability for ontology modeling. While we have focused on RDF and DAML+OIL in our research and development activities, the same issues apply to many of the knowledge represen-tation languages. This is especially the case for semantic network and concept graph approaches to knowledge representations.",
        "keywords": [
            "Ontology",
            "Semantic web",
            "Agents",
            "OO modeling",
            "UML",
            "RDF",
            "DAML"
        ],
        "authors": [
            "Kenneth Baclawski",
            "Mieczyslaw K. Kokar",
            "Paul A. Kogut",
            "Lewis Hart",
            "Jeﬀrey Smith",
            "Jerzy Letkowski",
            "Pat Emery"
        ],
        "file_path": "data/sosym-all/s10270-002-0008-4.pdf"
    },
    {
        "title": "On challenges of model transformation from UML to Alloy",
        "submission-date": "2008/03",
        "publication-date": "2008/12",
        "abstract": "The Uniﬁed Modeling Language (UML) is the de facto language used in the industry for software speciﬁcations. Once an application has been speciﬁed, Model Driven Architecture (MDA) techniques can be applied to generate code from such speciﬁcations. Since implementing a sys-tem based on a faulty design requires additional cost and effort, it is important to analyse the UML models at ear-lier stages of the software development lifecycle. This paper focuses on utilizing MDA techniques to deal with the analysis of UML models and identify design faults within a spec-iﬁcation. Specifically, we show how UML models can be automatically transformed into Alloy which, in turn, can be automatically analysed by the Alloy Analyzer. The proposed approach relies on MDA techniques to transform UML mod-els to Alloy. This paper reports on the challenges of the model transformation from UML class diagrams and OCL to Alloy. Those issues are caused by fundamental differences in the design philosophy of UML and Alloy. To facilitate better the representation of Alloy concepts in the UML, the paper draws on the lessons learnt and presents a UML proﬁle for Alloy.",
        "keywords": [],
        "authors": [
            "Kyriakos Anastasakis",
            "Behzad Bordbar",
            "Geri Georg",
            "Indrakshi Ray"
        ],
        "file_path": "data/sosym-all/s10270-008-0110-3.pdf"
    },
    {
        "title": "Special section of BPMDS’2013: coping with complexity in business processes",
        "submission-date": "2015/03",
        "publication-date": "2015/05",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a 2-day working conference attached to Conference on Advanced Information Systems Engineering (CAiSE). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of information systems theory. Today, business processes have to cope with increasing complexity in several areas. This special section is targeted at both researchers and practitioners in the information systems community with a focus on “Coping with Complexity in Business Processes”.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-015-0468-y.pdf"
    },
    {
        "title": "Redesign of UML class diagrams: a formal approach",
        "submission-date": "2005/05",
        "publication-date": "2007/11",
        "abstract": "Contracts provide a precise way of specifying object-oriented systems. When a class structure is modiﬁed, the corresponding contracts must be modiﬁed accordingly. This paper presents a method of transforming contracts, which allows the extension of a mapping deﬁned on a few model elements, to—what we call—an interpretation function, and to use this function to automatically translate OCL-constraints. Interestingly, such functions preserve reasoning using propositional calculi, resolution, equations, and induction. Interpretation functions can be used to trace model elements throughout multiple redesigns of UML class diagrams in both the forward, and the backward direction. The applicability of our approach is demonstrated in several examples, including some of Fowler’s refactoring patterns.",
        "keywords": [
            "UML",
            "OCL",
            "Formal methods",
            "Refactoring",
            "Requirements tracing"
        ],
        "authors": [
            "Piotr Kosiuczenko"
        ],
        "file_path": "data/sosym-all/s10270-007-0068-6.pdf"
    },
    {
        "title": "Consistency management in industrial continuous model-based development settings: a reality check",
        "submission-date": "2021/10",
        "publication-date": "2022/04",
        "abstract": "This article presents the state of practice of consistency management in thirteen industrial model-based development settings. Our analysis shows a tight coupling between adopting shorter development cycles and increasingly pressing consistency management challenges. We ﬁnd that practitioners desire to adopt shorter development cycles, but immature modeling practices slow them down. We describe the different patterns that emerge from the various industrial settings. There is an opportunity for researchers to provide practitioners with a migration path toward practices that enable more automated consistency management, and ultimately, continuous model-based development.",
        "keywords": [
            "Model-based development",
            "Consistency management",
            "Agile"
        ],
        "authors": [
            "Robbert Jongeling",
            "Federico Ciccozzi",
            "Jan Carlson",
            "Antonio Cicchetti"
        ],
        "file_path": "data/sosym-all/s10270-022-01000-5.pdf"
    },
    {
        "title": "Model-based requirements speciﬁcation of real-time systems with UML, SysML and MARTE",
        "submission-date": "2015/09",
        "publication-date": "2016/04",
        "abstract": "Activities of speciﬁcation, analysis and design of real-time systems (RTS) are highly dependent on an effec-tive understanding of the application domain and on the thorough representation of their basic requirements. Model-based approaches using modeling languages such as UML are often applied to contribute to handle complexity of RTS development. However, UML alone does not com-pletely represent important features associated with these systems, such as relationship with hardware elements and an effective representation of timing constraints. This article explores the combined use of UML and its proﬁles SysML and MARTE for modeling hardware and software requirements of RTS, with application to a case of controlling urban road trafﬁc. The SysML proﬁle alone does not present the representation of temporal, behavioral and performance requirements. The MARTE proﬁle provides key resources to specify non-functional requirements for RTS, in addition to a clear description of the various relevant aspects of require-Communicated by Prof. Jean-Michel Bruel. ments deﬁnition of RTS, as for instance, temporal aspects and constraints. The main objective is to present the com-binedapplicationofSysMLwithMARTEstereotypes,which enables the speciﬁcation of different features of individual software requirements. Thus, in addition to the factors men-tioned above, we can say that the proposed approach has an important role to specify RTS at different levels of detail and levels of abstraction.",
        "keywords": [
            "Real-time systems",
            "Requirements engineering",
            "UML",
            "SysML",
            "MARTE"
        ],
        "authors": [
            "Fabíola Gonçalves C. Ribeiro",
            "Carlos E. Pereira",
            "Achim Rettberg",
            "Michel S. Soares"
        ],
        "file_path": "data/sosym-all/s10270-016-0525-1.pdf"
    },
    {
        "title": "Guest editorial for the special section on MODELS 2022",
        "submission-date": "2025/05",
        "publication-date": "2025/07",
        "abstract": "The MODELS conference series is the premier venue for model-driven software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. The ACM/IEEE 25th International Conference on Model Driven Engineering Languages and Systems (MODELS 2022) took place in Montreal, Canada, from 23 to 28 October 2022. A total of 125 papers were submitted to the conference. The Foundations track of MODELS 2022 received 86 submissions of which 23 were accepted, while the Practice and Innovation track received 39 submissions of which 12 were accepted. Together, both tracks had an acceptance rate of 28%. It is a tradition that authors of the best papers at each MODELS conference edition are invited to submit revised and extended versions of their papers for publication in a special section in SoSyM. The selection of these papers is based on the reviews provided by the Program Committee members and on the reception of the papers at the conference. This special section comprises the twelve articles that resulted from this invitation and review process. Each article was subject to the full SoSyM review cycle and the authors received anonymous feedback from three expert reviewers. As a result, each article has been thoroughly revised and substantially extended compared to its conference version.",
        "keywords": [],
        "authors": [
            "Nelly Bencomo",
            "Houari Sahraoui",
            "Eugene Syriani",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-025-01303-3.pdf"
    },
    {
        "title": "Assert and negate revisited: Modal semantics for UML sequence diagrams",
        "submission-date": "2006/08",
        "publication-date": "2007/05",
        "abstract": "Live Sequence Charts (LSC) extend Message Sequence Charts (MSC), mainly by distinguishing possible from necessary behavior. They thus enable the speciﬁcation of rich multi-modal scenario-based properties, such as mandatory, possible and forbidden scenarios. The sequence diagrams of UML 2.0 enrich those of previous versions of UML by two new operators, assert and negate, for specifying required and forbidden behaviors, which appear to have been inspired by LSC. The UML 2.0 semantics of sequence diagrams, however, being based on pairs of valid and invalid sets of traces, is inadequate, and prevents the new operators from being used effectively.\nWe propose an extension of, and a different semantics for this UML language—Modal Sequence Diagrams (MSD)— based on the universal/existential modal semantics of LSC. In particular, in MSD assert and negate are really modalities, not operators. We deﬁne MSD as a UML 2.0 proﬁle, thus paving the way to apply formal veriﬁcation, synthesis, and scenario-based execution techniques from LSC to the mainstream UML standard.",
        "keywords": [
            "UML Interactions",
            "Sequence diagrams",
            "Live sequence charts",
            "Standardization",
            "Formal semantics"
        ],
        "authors": [
            "David Harel",
            "Shahar Maoz"
        ],
        "file_path": "data/sosym-all/s10270-007-0054-z.pdf"
    },
    {
        "title": "Supporting different process views through a Shared Process Model",
        "submission-date": "2014/03",
        "publication-date": "2015/01",
        "abstract": "Different stakeholders in the business process management (BPM) life cycle benefit from having different views onto a particular process model. Each view can show, and offer to change, the details relevant to the particular stakeholder, leaving out the irrelevant ones. However, introducing different views on a process model entail the problem to synchronize changes in case that one view evolves. This problem is especially relevant and challenging for views at different abstraction levels. In this paper, we propose a Shared Process Model that provides different stakeholder views at different abstraction levels and synchronizes changes made to any view. We present detailed requirements and a solution design for the Shared Process Model. We also present an overview of our prototypical implementation to demonstrate the feasibility of the approach. Finally, we report on a comprehensive evaluation of the approach on real Business–IT modeling scenarios.",
        "keywords": [
            "Business process modeling",
            "Business",
            "IT gap",
            "Model synchronization"
        ],
        "authors": [
            "Jochen Küster",
            "Hagen Völzer",
            "Cédric Favre",
            "Moisés Castelo Branco",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-015-0453-5.pdf"
    },
    {
        "title": "Comparing relational model transformation technologies: implementing Query/View/Transformation with Triple Graph Grammars",
        "submission-date": "2008/03",
        "publication-date": "2009/07",
        "abstract": "The Model Driven Architecture (MDA) is an approach to develop software based on different models. There are separate models for the business logic and for platform speciﬁc details. Moreover, code can be generated automatically from these models. This makes transformations a core technology for MDA and for model-based software engineering approaches in general. Query/View/Transformation (QVT) is the transformation technology recently proposed for this purpose by the OMG. Triple Graph Grammars (TGGs) are another transformation technology proposed in the mid-nineties, used for example in the FUJ-ABA CASE tool. In contrast to many other transformation technologies, both QVT and TGGs declaratively deﬁne the relation between two models. With this definition, a transformation engine can execute a transformation in either direction and, based on the same definition, can also propagate changes from one model to the other. In this paper, we compare the concepts of the declarative languages of QVT and TGGs. It turns out that TGGs and declarative QVT have many concepts in common. In fact, QVT-Core can be mapped to TGGs. We show that QVT-Core can be implemented by transforming QVT-Core mappings to TGG rules, which can then be executed by a TGG transformation engine that performs the actual QVT transformation. Furthermore, we discuss an approach for mapping QVT-Relations to TGGs. Based on the semantics of TGGs, we clarify semantic gaps that we identiﬁed in the declarative languages of QVT and, furthermore, we show how TGGs can beneﬁt from the concepts of QVT.",
        "keywords": [
            "MDA",
            "Model-based software engineering",
            "Model transformation",
            "Query/View/Transformation (QVT)",
            "Triple Graph Grammar (TGG)"
        ],
        "authors": [
            "Joel Greenyer",
            "Ekkart Kindler"
        ],
        "file_path": "data/sosym-all/s10270-009-0121-8.pdf"
    },
    {
        "title": "Reusing model validation methods for the continuous validation of digital twins of cyber-physical systems",
        "submission-date": "2023/09",
        "publication-date": "2024/10",
        "abstract": "One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reﬂects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) We provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) We demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.",
        "keywords": [
            "Modeling and simulation",
            "Model validation",
            "Validation metrics",
            "Digital twin"
        ],
        "authors": [
            "Joost Mertens",
            "Joachim Denil"
        ],
        "file_path": "data/sosym-all/s10270-024-01225-6.pdf"
    },
    {
        "title": "Transformation challenges: from software models to performance models",
        "submission-date": "2011/09",
        "publication-date": "2013/10",
        "abstract": "A software model can be analysed for non-functional requirements by extending it with suitable annotations and transforming it into analysis models for the corresponding non-functional properties. For quantitative performance evaluation, suitable annotations are standardized in the “UML Proﬁle for Modeling and Analysis of Real-Time Embedded systems” (MARTE) and its predecessor, the “UML Proﬁle for Schedulability, Performance and Time”. A range of different performance model types (such as queueing networks, Petri nets, stochastic process algebra) may be used for analysis. In this work, an intermediate “Core Scenario Model” (CSM) is used in the transformation from the source software model to the target performance model. CSM focuses on how the system behaviour uses the system resources. The semantic gap between the software model and the performance model must be bridged by (1) information supplied in the performance annotations, (2) in interpretation of the global behaviour expressed in the CSM and (3) in the process of constructing the performance model. Flexibility is required for specifying sets of alternative cases, for choosing where this bridging information is supplied, and for overriding values. It is also essential to be able to trace the source of values used in a particular performance estimate. The performance model in turn can be used to verify responsiveness and scalability of a software system, to discover architectural limitations at an early stage of development, and to develop efﬁcient performance tests. This paper describes how the semantic gap between software models in UML+MARTE and performance models (based on queueing or Petri nets) can be bridged using transformations based on CSMs, and how the transformation challenges are addressed.",
        "keywords": [
            "Software performance",
            "Performance analysis",
            "Model transformation",
            "UML",
            "MARTE proﬁle",
            "Layered queueing networks"
        ],
        "authors": [
            "Murray Woodside",
            "Dorina C. Petriu",
            "José Merseguer",
            "Dorin B. Petriu",
            "Mohammad Alhaj"
        ],
        "file_path": "data/sosym-all/s10270-013-0385-x.pdf"
    },
    {
        "title": "Formal analysis of human operator behavioural patterns in interactive surveillance systems",
        "submission-date": "2006/05",
        "publication-date": "2007/12",
        "abstract": "An important area of Human Reliability Assessment in interactive systems is the ability to understand the causes of human error and to model their occurrence. This paper investigates a new approach to analysis of task failures based on patterns of operator behaviour, in contrast with more traditional event-based approaches. It considers, as a case study, a formal model of an Air Trafﬁc Control system operator’s task which incorporates a simple model of the high-level cognitive processes involved. The cognitive model is formalised in the CSP process algebra. Various patterns of behaviour that could lead to task failure are descri-bed using temporal logic. Then a model-checking technique is used to verify whether the set of selected behavioural pat-terns is sound and complete with respect to the deﬁnition of task failure. The decomposition is shown to be incomplete and a new behavioural pattern is identiﬁed, which appears to have been overlooked in the informal analysis of the problem. This illustrates how formal analysis of operator models can yield fresh insights into how failures may arise in interactive systems.",
        "keywords": [],
        "authors": [
            "Antonio Cerone",
            "Simon Connelly",
            "Peter Lindsay"
        ],
        "file_path": "data/sosym-all/s10270-007-0072-x.pdf"
    },
    {
        "title": "Knowledge and software modeling using UML",
        "submission-date": "2002/09",
        "publication-date": "2004/04",
        "abstract": "Ontology can be considered as a comprehen-sive knowledge model which enables the developer to practice knowledge, instead of code, reuse. In the devel-opment of knowledge-based systems, diﬀerent modeling languages are employed at diﬀerent stages of the develop-ment process. By using a common modeling language for the knowledge and software models, knowledge instead of software reuse can be achieved. We illustrate the process by ﬁrst presenting an ontology developed for an industrial domain and then investigate Uniﬁed Modeling Language (UML) as an ontology modeling tool. Since any model ex-pressed in UML can be translated into a software model, the transition from the knowledge model to system im-plementation is better supported with the proposed ap-proach. The industrial domain of selecting a remediation technique for petroleum contaminated sites is adopted for the illustration case study.",
        "keywords": [
            "Ontology",
            "Knowledge Reuse",
            "Uniﬁed Modeling Language (UML)"
        ],
        "authors": [
            "Christine W. Chan"
        ],
        "file_path": "data/sosym-all/s10270-004-0057-y.pdf"
    },
    {
        "title": "Modeling research in recent years: special section on ECMFA 2017 and ECMFA 2018",
        "submission-date": "2020/07",
        "publication-date": "2020/08",
        "abstract": "The modeling community has made signiﬁcant progress in recent years regarding developing software and systems with enhanced productivity and quality. Leveraging the critical entities emerging in the development process and their correspondences allows us to design, analyze, and develop software and systems relying on formalized notations that are amenable to computer-based automation. The European Conference on Modelling Foundations and Applications (ECMFA) is one of the primary scientiﬁc venues where almost any aspect of modeling is discussed among world-class experts from academia and industry.",
        "keywords": [],
        "authors": [
            "Anthony Anjorin",
            "Alfonso Pierantonio",
            "Salvador Trujillo",
            "Huascar Espinoza Ortiz"
        ],
        "file_path": "data/sosym-all/s10270-020-00821-6.pdf"
    },
    {
        "title": "Clafer: unifying class and feature modeling",
        "submission-date": "2013/09",
        "publication-date": "2014/12",
        "abstract": "We present Clafer (class, feature, reference), a\nclass modeling language with ﬁrst-class support for feature\nmodeling. We designed Clafer as a concise notation for meta-\nmodels, feature models, mixtures of meta- and feature mod-\nels (such as components with options), and models that cou-\nple feature models and meta-models via constraints (such\nas mapping feature conﬁgurations to component conﬁgura- \ntions or model templates). Clafer allows arranging models\ninto multiple specialization and extension layers via con- \nstraints and inheritance. We identify several key mechanisms\nallowing a meta-modeling language to express feature mod- \nels concisely. Clafer uniﬁes basic modeling constructs, such\nas class, association, and property, into a single construct,\ncalled clafer. We provide the language with a formal seman-\ntics built in a structurally explicit way. The resulting seman-\ntics explains the meaning of hierarchical models whereby\nproperties can be arbitrarily nested in the presence of inher-\ntance and feature modeling constructs. The semantics also\nenables building consistent automated reasoning support for\nthe language: To date, we implemented three reasoners for\nClafer based on Alloy, Z3 SMT, and Choco3 CSP solvers.",
        "keywords": [
            "Language design",
            "Feature modeling",
            "OOM",
            "Semantics",
            "Uniﬁcation"
        ],
        "authors": [
            "Kacper Ba˛k",
            "Zinovy Diskin",
            "Michał Antkiewicz",
            "Krzysztof Czarnecki",
            "Andrzej Wa˛sowski"
        ],
        "file_path": "data/sosym-all/s10270-014-0441-1.pdf"
    },
    {
        "title": "A UML Proﬁle for the Design, Quality Assessment and Deployment of Data-intensive Applications",
        "submission-date": "2018/02",
        "publication-date": "2019/04",
        "abstract": "Big Data or Data-Intensive applications (DIAs) seek to mine, manipulate, extract or otherwise exploit the potential intelligence hidden behind Big Data. However, several practitioner surveys remark that DIAs potential is still untapped because of very difﬁcult and costly design, quality assessment and continuous reﬁnement. To address the above shortcoming, we propose the use of a UML domain-speciﬁc modeling language or proﬁle speciﬁcally tailored to support the design, assessment and continuous deployment of DIAs. This article illustrates our DIA-speciﬁc proﬁle and outlines its usage in the context of DIA performance engineering and deployment. For DIA performance engineering, we rely on the Apache Hadoop technology, while for DIA deployment, we leverage the TOSCA language. We conclude that the proposed proﬁle offers a powerful language for data-intensive software and systems modeling, quality evaluation and automated deployment of DIAs on private or public clouds.",
        "keywords": [
            "UML",
            "Proﬁle",
            "Data-intensive applications",
            "Software design",
            "Big Data",
            "Performance assessment",
            "Model-driven deployment",
            "Apache Hadoop",
            "TOSCA language"
        ],
        "authors": [
            "Diego Perez-Palacin",
            "José Merseguer",
            "José I. Requeno",
            "M. Guerriero",
            "Elisabetta Di Nitto",
            "D. A. Tamburri"
        ],
        "file_path": "data/sosym-all/s10270-019-00730-3.pdf"
    },
    {
        "title": "Beyond loop bounds: comparing annotation languages for worst-case execution time analysis",
        "submission-date": "2009/01",
        "publication-date": "2010/04",
        "abstract": "Worst-case execution time (WCET) analysis is concerned with computing a precise-as-possible bound for the maximum time the execution of a program can take. This information is indispensable for developing safety-critical real-time systems, e.g., in the avionics and automotive ﬁelds. Starting with the initial works of Chen, Mok, Puschner, Shaw, and others in the mid and late 1980s, WCET analysis turned intoawell-establishedandvibrantﬁeldofresearchanddevel-opment in academia and industry. The increasing number and diversity of hardware and software platforms and the ongo-ing rapid technological advancement became drivers for the development of a wide array of distinct methods and tools for WCET analysis. The precision, generality, and efﬁciency of these methods and tools depend much on the expressive-ness and usability of the annotation languages that are used to describe feasible and infeasible program paths. In this article we survey the annotation languages which we con-sider formative for the ﬁeld. By investigating and comparing their individual strengths and limitations with respect to a set of pivotal criteria, we provide a coherent overview of the state of the art. Identifying open issues, we encourage further research. This way, our approach is orthogonal and comple-mentary to a recent approach of Wilhelm et al. who provide a thorough survey of WCET analysis methods and tools that have been developed and used in academia and industry.",
        "keywords": [
            "Worst-case execution time (WCET) analysis",
            "Annotation languages",
            "Path-oriented",
            "constraint-oriented",
            "and hierarchy-oriented WCET annotation languages",
            "WCET annotation language challenge"
        ],
        "authors": [
            "Raimund Kirner",
            "Jens Knoop",
            "Adrian Prantl",
            "Markus Schordan",
            "Albrecht Kadlec"
        ],
        "file_path": "data/sosym-all/s10270-010-0161-0.pdf"
    },
    {
        "title": "Bridging MDE and AI: a systematic review of domain-speciﬁc languages and model-driven practices in AI software systems engineering",
        "submission-date": "2023/07",
        "publication-date": "2024/09",
        "abstract": "Technical systems are becoming increasingly complex due to the increasing number of components, functions, and involve-ment of different disciplines. In this regard, model-driven engineering techniques and practices tame complexity during the development process by using models as primary artifacts. Modeling can be carried out through domain-speciﬁc languages whose implementation is supported by model-driven techniques. Today, the amount of data generated during product devel-opment is rapidly growing, leading to an increased need to leverage artiﬁcial intelligence algorithms. However, using these algorithms in practice can be difﬁcult and time-consuming. Therefore, leveraging domain-speciﬁc languages and model-driven techniques for formulating AI algorithms or parts of them can reduce these complexities and be advantageous. This study aims to investigate the existing model-driven approaches relying on domain-speciﬁc languages in support of the engineering of AI software systems to sharpen future research further and deﬁne the current state of the art. We conducted a Systemic Literature Review (SLR), collecting papers from ﬁve major databases resulting in 1335 candidate studies, eventually retaining 18 primary studies. Each primary study will be evaluated and discussed with respect to the adoption of (1) MDE principles and practices and (2) the phases of AI development support aligned with the stages of the CRISP-DM methodology. The study’s ﬁndings show that language workbenches are of paramount importance in dealing with all aspects of modeling language development (metamodel, concrete syntax, and model transformation) and are leveraged to deﬁne domain-speciﬁc languages (DSL) explicitly addressing AI concerns. The most prominent AI-related concerns are training and modeling of the AI algorithm, while minor emphasis is given to the time-consuming preparation of the data sets. Early project phases that support interdisciplinary communication of requirements, such as the CRISP-DM Business Understanding phase, are rarely reﬂected. The study found that the use of MDE for AI is still in its early stages, and there is no single tool or method that is widely used. Additionally, current approaches tend to focus on speciﬁc stages of development rather than providing support for the entire development process. As a result, the study suggests several research directions to further improve the use of MDE for AI and to guide future research in this area.",
        "keywords": [
            "Model-driven engineering",
            "Artiﬁcial intelligence",
            "MDE4AI",
            "Domain-speciﬁc language",
            "SLR",
            "Literature review",
            "Machine learning"
        ],
        "authors": [
            "Simon Rädler",
            "Luca Berardinelli",
            "Karolin Winter",
            "Abbas Rahimi",
            "Stefanie Rinderle-Ma"
        ],
        "file_path": "data/sosym-all/s10270-024-01211-y.pdf"
    },
    {
        "title": "Guest editorial to the special issue on MODELS 2010",
        "submission-date": "2010/10",
        "publication-date": "2013/02",
        "abstract": "Welcome to this special issue of Software and Systems Modeling devoted to selected papers of MODELS 2010. The MODELS series of conferences is the premier venue for exchange of innovative ideas and practical experience focusing on a very important new technical discipline: model-driven software and systems engineering. The expansion of this discipline is a direct consequence of the increasing significance and success of model-based methods in practice.",
        "keywords": [],
        "authors": [
            "Dorina C. Petriu"
        ],
        "file_path": "data/sosym-all/s10270-013-0324-x.pdf"
    },
    {
        "title": "Performance modeling and analysis of message-oriented event-driven systems",
        "submission-date": "2011/07",
        "publication-date": "2012/02",
        "abstract": "Message-oriented event-driven systems are becoming increasingly ubiquitous in many industry domains including telecommunications, transportation and supply chain management. Applications in these areas typically have stringent requirements for performance and scalability. To guarantee adequate quality-of-service, systems must be subjected to a rigorous performance and scalability analysis before they are put into production. In this paper, we present a comprehensive modeling methodology for message-oriented event-driven systems in the context of a case study of a representative application in the supply chain management domain. The methodology, which is based on queueing Petri nets, provides a basis for performance analysis and capacity planning. We study a deployment of the SPECjms2007 standard benchmark on a leading commercial middleware platform. A detailed system model is built in a step-by-step fashion and then used to predict the system performance under various workload and configuration scenarios. After the case study, we present a set of generic performance modeling patterns that can be used as building blocks when modeling message-oriented event-driven systems. The results demonstrate the effectiveness, practicality and accuracy of the proposed modeling and prediction approach.",
        "keywords": [
            "Event-based",
            "Performance model",
            "Performance evaluation",
            "Message-oriented middleware",
            "Performance pattern"
        ],
        "authors": [
            "Kai Sachs",
            "Samuel Kounev",
            "Alejandro Buchmann"
        ],
        "file_path": "data/sosym-all/s10270-012-0228-1.pdf"
    },
    {
        "title": "Improving query performance on dynamic graphs",
        "submission-date": "2020/01",
        "publication-date": "2020/11",
        "abstract": "Querying large models efﬁciently often imposes high demands on system resources such as memory, processing time, disk access or network latency. The situation becomes more complicated when data are highly interconnected, e.g. in the form of graph structures, and when data sources are heterogeneous, partly coming from dynamic systems and partly stored in databases. These situations are now common in many existing social networking applications and geo-location systems, which require specialized and efﬁcient query algorithms in order to make informed decisions on time. In this paper, we propose an algorithm to improve the memory consumption and time performance of this type of queries by reducing the amount of elements to be processed, focusing only on the information that is relevant to the query but without compromising the accuracy of its results. To this end, the reduced subset of data is selected depending on the type of query and its constituent ﬁlters. Three case studies are used to evaluate the performance of our proposal, obtaining signiﬁcant speedups in all cases.",
        "keywords": [
            "Data stream processing",
            "Dynamic graphs",
            "Performance optimization",
            "Precomputing systems",
            "Data queries"
        ],
        "authors": [
            "Gala Barquero",
            "Javier Troya",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-020-00832-3.pdf"
    },
    {
        "title": "Understanding Declare models: strategies, pitfalls, empirical results",
        "submission-date": "2013/10",
        "publication-date": "2014/09",
        "abstract": "Declarative approaches to business process modeling are regarded as well suited for highly volatile environments, as they enable a high degree of flexibility. However, problems in understanding and maintaining declarative process models often impede their adoption. Likewise, little research has been conducted into the understanding of declarative process models. This paper takes a first step toward addressing this fundamental question and reports on an empirical investigation consisting of an exploratory study and a follow-up study focusing on the system analysts’ sense-making of declarative process models that are specified in Declare. For this purpose, we distributed real-world Declare models to the participating subjects and asked them to describe the illustrated process and to perform a series of sense-making tasks. The results of our studies indicate that two main strategies for reading Declare models exist: either considering the execution order of the activities in the process model, or orienting by the layout of the process model. In addition, the results indicate that single constraints can be handled well by most subjects, while combinations of constraints pose significant challenges. Moreover, the study revealed that aspects that are similar in both imperative and declarative process modeling languages at a graphical level, while having different semantics, cause considerable troubles. This research not only helps guiding the future development of tools for supporting system analysts, but also gives advice on the design of declarative process modeling notations and points out typical pitfalls to teachers and educators of future systems analysts.",
        "keywords": [
            "Declarative process models",
            "Empirical research",
            "Understandability"
        ],
        "authors": [
            "Cornelia Haisjackl",
            "Irene Barba",
            "Stefan Zugal",
            "Pnina Soffer",
            "Irit Hadar",
            "Manfred Reichert",
            "Jakob Pinggera",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-014-0435-z.pdf"
    },
    {
        "title": "Model-based tool support for Tactical Data Links: an experience report from the defence domain",
        "submission-date": "2014/05",
        "publication-date": "2015/08",
        "abstract": "The Tactical Data Link (TDL) allows the exchange of information between cooperating platforms as part of an integrated command and control (C2) system. Information exchange is facilitated by adherence to a complex, message-based protocol deﬁned by document-centric standards. In this paper, we report on a recent body of work investigating migration from a document-centric to a model-centric approach within the context of the TDL domain, motivated by a desire to achieve a positive return on investment. The model-centric approach makes use of the Epsilon technology stack and provides a signiﬁcant improvement to both the level of abstraction and rigour of the network design. It is checkable by a machine and, by virtue of an MDA-like approach to the separation of domains and model transformation between domains, is open to integration with other models to support more complex workﬂows, such as by providing the results of interoperability analyses in human-readable domain-speciﬁc reports conforming to an accepted standard.",
        "keywords": [
            "Tactical data link",
            "Model-based development",
            "Interoperability",
            "Metamodelling",
            "Model management",
            "Eclipse Modeling Framework",
            "Epsilon"
        ],
        "authors": [
            "Suraj Ajit",
            "Chris Holmes",
            "Julian Johnson",
            "Dimitrios S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-015-0480-2.pdf"
    },
    {
        "title": "User journey games: automating user-centric analysis",
        "submission-date": "2023/03",
        "publication-date": "2024/03",
        "abstract": "The servitization of business is moving industry to business models driven by customer demand. Customer satisfaction is connected with ﬁnancial rewards, forcing companies to invest in their users’ experience. User journeys describe how users maneuver through a service. Today, user journeys are typically modeled graphically, and lack formalization and analysis support. This paper proposes a formalization of user journeys as weighted games between the user and the service provider and a systematic data-driven method to derive these user journey games from system logs, using process mining techniques. As the derived games may contain cycles, we deﬁne an algorithm to transform user journeys games with cycles into acyclic weighted games, which can be model checked using Uppaal Stratego to uncover potential challenges in a company’s interactions with its users and derive company strategies to guide users through their journeys. Finally, we propose a user journey sliding-window analysis to detect changes in the user journey over time by model checking a sequence of generated games. Our analysis pipeline has been evaluated on an industrial case study; it revealed design challenges within the studied service and could be used to derive actionable recommendations for improvement.",
        "keywords": [
            "User journeys",
            "Data-driven model construction",
            "Time-series analysis",
            "Games",
            "Model checking",
            "UPPAAL."
        ],
        "authors": [
            "Paul Kobialka\nS. Lizeth Tapia Tarifa\nGunnar R. Bergersen\nEinar Broch Johnsen"
        ],
        "file_path": "data/sosym-all/s10270-024-01148-2.pdf"
    },
    {
        "title": "Style-based modeling and refinement of service-oriented architectures",
        "submission-date": "2005/05",
        "publication-date": "2006/04",
        "abstract": "Service-oriented architectures (SOA) provide a flexible and dynamic platform for implementing business solutions. In this paper, we address the modeling of such architectures by refining business-oriented architectures, which abstract from technology aspects, into service-oriented ones, focusing on the ability of dynamic reconfiguration (binding to new services at run-time) typical for SOA. The refinement is based on conceptual models of the platforms involved as architectural styles, formalized by graph transformation systems. Based on a refinement relation between abstract and platform-specific styles we investigate how to realize business-specific scenarios on the SOA platform by automatically deriving refined, SOA-specific reconfiguration scenarios.",
        "keywords": [
            "Service-oriented architecture",
            "Architectural style",
            "Architecture refinement",
            "Graph transformation"
        ],
        "authors": [
            "Luciano Baresi",
            "Reiko Heckel",
            "Sebastian Th¨one",
            "D´aniel Varr´o"
        ],
        "file_path": "data/sosym-all/s10270-006-0001-4.pdf"
    },
    {
        "title": "A demonstration-based model transformation approach to automate model scalability",
        "submission-date": "2011/09",
        "publication-date": "2013/09",
        "abstract": "An important aspect during software development is the ability to evolve and scale software models in order to handle design forces, such as enlarging and upgrading system features, or allocating more resources to handle additional users. Model scalability is the ability to refactor a base model, by adding or replicating the base model elements, connections or substructures, in order to build a larger and more complex model to satisfy new design requirements. Although a number of modeling tools have been developed to create and edit models for different purposes, mechanisms to scale models have not been well supported. In most situations, models are manually scaled using the basic point-and-click editing operations provided by the modeling environment. Manual model scaling is often tedious and error-prone, especially when the model to be scaled has hundreds or thousands of elements and the scaling process involves entirely manual operations. Although model scaling tasks can be automated by using model transformation languages, writing model transformation rules requires learning a model transformation language, as well as possessing a great deal of knowledge about the metamodel. Model transformation languages and metamodel concepts are often difficult for domain experts to understand. This requirement to learn a complex model transformation language exerts a negative influence on the usage of models by domain experts in software development. For instance, domain experts may be prevented from contributing to model scalability tasks from which they have significant domain experience. This paper presents a demonstration-based approach to automate model scaling. Instead of writing model transformation rules explicitly, users demonstrate how to scale models by directly editing the concrete model instances and simulate the model replication processes. By recording a user’s operations, an inference engine analyzes the user’s demonstration and generalizes model transformation patterns automatically, which can be reused to scale up other model instances. Using this approach, users are able to automate scaling tasks without learning a complex model transformation language. In addition, because the demonstration is performed on model instances, users are isolated from the underlying abstract metamodel definitions.",
        "keywords": [
            "Model evolution",
            "Model scalability",
            "Model transformation by demonstration"
        ],
        "authors": [
            "Yu Sun",
            "Jeff Gray",
            "Jules White"
        ],
        "file_path": "data/sosym-all/s10270-013-0374-0.pdf"
    },
    {
        "title": "Semantics of OCL speciﬁed with QVT",
        "submission-date": "2007/03",
        "publication-date": "2008/03",
        "abstract": "The Object Constraint Language (OCL) has been\nfor many years formalized both in its syntax and seman-\ntics in the language standard. While the ofﬁcial definition\nof OCL’s syntax is already widely accepted and strictly sup-\nported by most OCL tools, there is no such agreement on\nOCL’s semantics, yet. In this paper, we propose an approach\nbased on metamodeling and model transformations for for-\nmalizing the semantics of OCL. Similarly to OCL’s ofﬁcial\nsemantics, our semantics formalizes the semantic domain of\nOCL, i.e. the possible values to which OCL expressions can\nevaluate, by a metamodel. Contrary to OCL’s ofﬁcial seman-\ntics, the evaluation of OCL expressions is formalized in our\napproach by model transformations written in QVT. Thanks\nto the chosen format, our semantics definition for OCL can be\nautomatically transformed into a tool, which evaluates OCL\nexpressions in a given context. Our work on the formaliza-\ntion of OCL’s semantics resulted also in the identiﬁcation\nand better understanding of important semantic concepts, on\nwhich OCL relies. These insights are of great help when OCL\nhas to be tailored as a constraint language of a given DSL.\nWe show on an example, how the semantics of OCL has to\nredefined in order to become a constraint language in a\ndatabase domain.",
        "keywords": [
            "QVT",
            "OCL Semantics",
            "Graph-transformations",
            "DSL"
        ],
        "authors": [
            "Slaviša Markovi´c",
            "Thomas Baar"
        ],
        "file_path": "data/sosym-all/s10270-008-0083-2.pdf"
    },
    {
        "title": "Analysis of variability models: a systematic literature review",
        "submission-date": "2019/06",
        "publication-date": "2020/11",
        "abstract": "Dealing with variability, during Software Product Line Engineering (SPLE), means trying to allow software engineers to\ndevelop a set of similar applications based on a manageable range of variable functionalities according to expert users’ needs.\nParticularly, variability management (VM) is an activity that allows ﬂexibility and a high level of reuse during software\ndevelopment. In the last years, we have witnessed a proliferation of methods, techniques and supporting tools for VM in\ngeneral, and for its analysis in particular. More precisely, a speciﬁc ﬁeld has emerged, named (automated) variability analysis,\nfocusing on verifying variability models across the SPLE’s phases. In this paper, we introduce a systematic literature review of\nexisting proposals (as primary studies) focused on analyzing variability models. We deﬁne a classiﬁcation framework, which\nis composed of 20 sub-characteristics addressing general aspects, such as scope and validation, as well as model-speciﬁc\naspects, such as variability primitives, reasoner type. The framework allows to look at the analysis of variability models\nduring its whole life cycle—from design to derivation—according to the activities involved during an SPL development.\nAlso, the framework helps us answer three research questions deﬁned for showing the state of the art and drawing challenges\nfor the near future. Among the more interesting challenges, we can highlight the needs of more applications in industry, the\nexistence of more mature tools, and the needs of providing more semantics in the way of variability primitives for identifying\ninconsistencies in the models.",
        "keywords": [
            "Variability analysis",
            "Software Product Line",
            "Variability management",
            "Supporting tools"
        ],
        "authors": [
            "Matias Pol’la",
            "Agustina Buccella",
            "Alejandra Cechich"
        ],
        "file_path": "data/sosym-all/s10270-020-00839-w.pdf"
    },
    {
        "title": "IoT meets BPM: a bidirectional communication architecture for IoT-aware process execution",
        "submission-date": "2018/10",
        "publication-date": "2020/03",
        "abstract": "Business processes are frequently executed within application systems that involve humans, computer systems as well as objects of the Internet of Things (IoT). Nevertheless, the usage of IoT technology for system supported process execution is still constrained by the absence of a common system architecture that manages the communication between both worlds. In this paper, we introduce an integrated approach for IoT-aware business process execution that exploits IoT for BPM by providing IoT data in a process-compatible way, providing an IoT data provenance framework, considering IoT data for interaction in a pre-deﬁned process model, and providing wearable user interfaces with context-speciﬁc IoT data provision. The approach has been implemented on top of contemporary BPM modeling concepts and system technology. The introduced technique has evaluated extensively in different use cases in industry.",
        "keywords": [
            "Process Execution",
            "Internet of Things",
            "Wearables"
        ],
        "authors": [
            "Stefan Schönig",
            "Lars Ackermann",
            "Stefan Jablonski",
            "Andreas Ermer"
        ],
        "file_path": "data/sosym-all/s10270-020-00785-7.pdf"
    },
    {
        "title": "Deep speciﬁcation and proof preservation for the CoqTL transformation language",
        "submission-date": "2021/03",
        "publication-date": "2022/05",
        "abstract": "Executable engines for relational model-transformation languages evolve continuously because of language extension, performance improvement and bug ﬁxes. While new versions generally change the engine semantics, end-users expect to get backward-compatibility guarantees, so that existing transformations do not need to be adapted at every engine update. The CoqTL model-transformation language allows users to deﬁne model transformations, theorems on their behavior and machine-checked proofs of these theorems in Coq. Backward-compatibility for CoqTL involves also the preservation of these proofs. However, proof preservation is challenging, as proofs are easily broken even by small refactorings of the code they verify. In this paper, we present the solution we designed for the evolution of CoqTL. We provide a deep speciﬁcation of the transformation engine, including a set of theorems that must hold against the engine implementation. Then, at each milestone in the engine development, we certify the new version of the engine against this speciﬁcation, by providing proofs of the impacted theorems. The certiﬁcation formally guarantees end-users that all the proofs they write using the provided theorems will be preserved through engine updates. We illustrate the structure of the deep speciﬁcation theorems, we produce a machine-checked certiﬁcation of three versions of CoqTL against it, and we show examples of user proofs that leverage this speciﬁcation and are thus preserved through the updates. Finally, we discuss the evolution of the deep speciﬁcation by an extension mechanism, we present an evolution that introduces trace links in the speciﬁcation, and we show which user proofs are preserved through speciﬁcation evolutions.",
        "keywords": [
            "MDE",
            "Model transformation",
            "Programming language implementation",
            "Certiﬁcation",
            "Theorem proving",
            "Coq"
        ],
        "authors": [
            "Zheng Cheng",
            "Massimo Tisi"
        ],
        "file_path": "data/sosym-all/s10270-022-01004-1.pdf"
    },
    {
        "title": "A Methodological Approach for Object-Relational Database Design using UML",
        "submission-date": "2002/01",
        "publication-date": "2003/01",
        "abstract": "The most common way of designing databases is by means of a conceptual model, such as E/R, without taking into account other views of the system. New object-oriented design languages, such as UML (Uniﬁed Modelling Language), allow the whole system, including the database schema, to be modelled in a uniform way. Moreover, as UML is an extendable language, it allows for any necessary introduction of new stereotypes for specific applications. Proposals exist to extend UML with stereotypes for database design but, unfortunately, they are focused on relational databases. However, new applications require complex objects to be represented in complex relationships, object-relational databases being more appropriate for these requirements. The framework of this paper is an Object-Relational Database Design Methodology, which defines new UML stereotypes for Object-Relational Database Design and proposes some guidelines to translate a UML conceptual schema into an object-relational schema. The guidelines are based on the SQL:1999 object-relational model and on Oracle8i as a product example.",
        "keywords": [
            "UML extensions",
            "Stereotypes",
            "Object-Relational Databases",
            "Database Design",
            "Object Persistence",
            "Design Methodology",
            "UML",
            "SQL:1999",
            "Oracle"
        ],
        "authors": [
            "Esperanza Marcos",
            "Belén Vela",
            "José María Cavero"
        ],
        "file_path": "data/sosym-all/s10270-002-0001-y.pdf"
    },
    {
        "title": "ESUML-EAF: a framework to develop an energy-efﬁcient design model for embedded software",
        "submission-date": "2012/08",
        "publication-date": "2013/03",
        "abstract": "There is a growing interest in developing embed- ded systems that consume low energy in such application areas as mobile communications or wireless sensor networks. To especially provide the complex and diverse functions of embedded software with limited energy consumption, many studies of low-energy software are being performed. The existing studies to analyze energy consumption of embed- ded software have mainly focused on source code. How- ever, some studies recently explored model-based energy consumption analysis to fulﬁll the requirement of energy consumption in the early phase of software development process. This paper proposes a model-based energy con- sumption analysis framework to develop an energy-efﬁcient design model of embedded software. The proposed frame- work can analyze energy consumption without building an additional analysis model in software development and pro- vide the chance to fulﬁll the energy consumption require- ments in the early phase of the software development process, which can reduce the feedback efforts.",
        "keywords": [
            "UML",
            "Energy-efﬁcient design model",
            "Embedded software"
        ],
        "authors": [
            "Doo-Hwan Kim",
            "Jang-Eui Hong"
        ],
        "file_path": "data/sosym-all/s10270-013-0337-5.pdf"
    },
    {
        "title": "Editorial to theme issue on model-driven engineering of component-based software systems",
        "submission-date": "2017/02",
        "publication-date": "2017/03",
        "abstract": "This theme issue aims at providing a forum for disseminating latest trends in the use and combination of model-driven engineering (MDE) and component-based software engineering (CBSE). One of the main aims of MDE is to increase productivity in the development of complex systems, while reducing the time to market. Regarding CBSE, one of the main goals is to deliver and then support the exploitation of reusable “off-the-shelf” software components to be incorporated into larger applications. An effective interplay of MDE and CBSE can bring beneﬁts to both communities: on the one hand, the CBSE community would beneﬁt from implementation and automation capabil-ities of MDE, and on the other hand, MDE would beneﬁt from the foundational nature of CBSE. In total, we received 23 submissions to this theme issue, and each submission was reviewed by at least three reviewers. Thanks to the high qual-ity of the submissions that we received, we could eventually accept six papers for publication.",
        "keywords": [],
        "authors": [
            "Federico Ciccozzi",
            "Jan Carlson",
            "Patrizio Pelliccione",
            "Massimo Tivoli"
        ],
        "file_path": "data/sosym-all/s10270-017-0589-6.pdf"
    },
    {
        "title": "Generating relational database transactions from eb3 attribute definitions",
        "submission-date": "2006/08",
        "publication-date": "2008/10",
        "abstract": "eb3 is a trace-based formal language created for the speciﬁcation of information systems. In eb3, each entity and association attribute is independently deﬁned by a recursive function on the valid traces of external events. This paper describes an algorithm that generates, for each external event, a transaction that updates the value of affected attributes in their relational database representation. The beneﬁts are two-fold: eb3 attribute speciﬁcations are automatically translated into executable programs, eliminating system design and implementation steps; the construction of information systems is streamlined, because eb3 speciﬁcations are simpler and shorter to write than corresponding traditional speciﬁcations, design and implementations. In particular, the paper shows that simple eb3 constructs can replace complex SQL queries which are typically difﬁcult to write.",
        "keywords": [
            "Information systems",
            "Attributes",
            "Pattern matching",
            "SELECT statements",
            "Transactions"
        ],
        "authors": [
            "Frédéric Gervais",
            "Marc Frappier",
            "Régine Laleau"
        ],
        "file_path": "data/sosym-all/s10270-008-0104-1.pdf"
    },
    {
        "title": "Guest editorial for the special section on MODELS 2021",
        "submission-date": "2021/10",
        "publication-date": "2023/05",
        "abstract": "The MODELS conference series is the premier venue for model-based software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. MODELS 2021 took place online (originally planned in Fukuoka, Japan), from October 10 to October 15, 2021, as the ACM/IEEE 24th International Conference on Model Driven Engineering Languages and Systems. This special section presents the 14 articles that resulted from an invitation based on the best papers at the conference.",
        "keywords": [],
        "authors": [
            "Shiva Nejati",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-023-01108-2.pdf"
    },
    {
        "title": "An epistemic approach to the formal speciﬁcation of statistical machine learning",
        "submission-date": "2020/03",
        "publication-date": "2020/09",
        "abstract": "We propose an epistemic approach to formalizing statistical properties of machine learning. Speciﬁcally, we introduce a formal model for supervised learning based on a Kripke model where each possible world corresponds to a possible dataset and modal operators are interpreted as transformation and testing on datasets. Then, we formalize various notions of the classiﬁcation performance, robustness, and fairness of statistical classiﬁers by using our extension of statistical epistemic logic. In this formalization, we show relationships among properties of classiﬁers, and relevance between classiﬁcation performance and robustness. As far as we know, this is the ﬁrst work that uses epistemic models and logical formulas to express statistical properties of machine learning, and would be a starting point to develop theories of formal speciﬁcation of machine learning.",
        "keywords": [
            "Modal logic",
            "Possible world semantics",
            "Machine learning",
            "Classiﬁcation performance",
            "Robustness",
            "Fairness"
        ],
        "authors": [
            "Yusuke Kawamoto"
        ],
        "file_path": "data/sosym-all/s10270-020-00825-2.pdf"
    },
    {
        "title": "Gamifying model-based engineering: the PapyGame experience",
        "submission-date": "2022/08",
        "publication-date": "2023/03",
        "abstract": "Modeling is an essential and challenging activity in any engineering environment. It implies some hard-to-train skills such as abstraction and communication. Teachers, project leaders, and tool vendors have a hard time teaching or training their students, co-workers, or users. Gamiﬁcation refers to the exploitation of gaming mechanisms for serious purposes, like pro-moting behavioral changes, soliciting participation and engagement in activities, etc. We investigate the introduction of gaming mechanisms in modeling tasks with the primary goal of supporting learning/training. The result has been the realization of a gamiﬁed modeling environment named PapyGame. In this article, we present the approach adopted for PapyGame imple-mentation, the details on the gamiﬁcation elements involved, and the derived conceptual architecture required for applying gamiﬁcation in any modeling environment. Moreover, to demonstrate the beneﬁts of using PapyGame for learning/training modeling, a set of user experience evaluations have been conducted. Correspondingly, we report the obtained results together with a set of future challenges we consider as critical to make gamiﬁed modeling a more effective education/training approach.",
        "keywords": [
            "Model-based engineering",
            "Education",
            "Gamiﬁcation",
            "Papyrus"
        ],
        "authors": [
            "Antonio Bucchiarone",
            "Maxime Savary-Leblanc",
            "Xavier Le Pallec",
            "Antonio Cicchetti",
            "Sébastien Gérard",
            "Simone Bassanelli",
            "Federica Gini",
            "Annapaola Marconi"
        ],
        "file_path": "data/sosym-all/s10270-023-01091-8.pdf"
    },
    {
        "title": "Foundations for Streaming Model Transformations by Complex Event Processing",
        "submission-date": "2015/05",
        "publication-date": "2016/05",
        "abstract": "Streaming model transformations represent a novel class of transformations to manipulate models whose elements are continuously produced or modiﬁed in high volume and with rapid rate of change. Executing streaming transformations requires efﬁcient techniques to recognize activated transformation rules over a live model and a potentially inﬁnite stream of events. In this paper, we propose foundations of streaming model transformations by innovatively integrating incremental model query, complex event processing (CEP) and reactive (event-driven) transformation techniques. Complex event processing allows to identify relevant patterns and sequences of events over an event stream. Our approach enables event streams to include model change events which are automatically and continuously populated by incremental model queries. Furthermore, a reactive rule engine carries out transformations on identiﬁed complex event patterns. We provide an integrated domain-speciﬁc language with precise semantics for capturing complex event patterns and streaming transformations together with an execution engine, all of which is now part of the Viatra reactive transformation framework. We demonstrate the feasibility of our approach with two case studies: one in an advanced model engineering workﬂow; and one in the context of on-the-ﬂy gesture recognition.",
        "keywords": [
            "Streaming model transformations",
            "Complex event processing",
            "Live models",
            "Change-driven transformations",
            "Reactive transformations"
        ],
        "authors": [
            "István Dávid",
            "István Ráth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-016-0533-1.pdf"
    },
    {
        "title": "Business process improvement with AB testing and reinforcement learning: grounded theory-based industry perspectives",
        "submission-date": "2023/10",
        "publication-date": "2024/11",
        "abstract": "In order to better facilitate the need for continuous business process improvement, the application of DevOps principles has been proposed. In particular, the AB-BPM methodology applies AB testing—a DevOps practice—and reinforcement learning to increase the speed and quality of business process improvement efforts. In this paper, we provide an industry perspective on this approach, assessing prerequisites, suitability, requirements, risks, and additional aspects of the AB-BPM methodology and supporting tools. Our qualitative study follows the grounded theory research methodology, including 16 semi-structured interviews with BPM practitioners. The main ﬁndings indicate: (1) a need for expert control during reinforcement learning-driven experiments in production, (2) the importance of involving the participants and aligning the method culturally with the respective setting, (3) the necessity of an integrated process execution environment, and (4) the long-term potential of the methodology for effective and efﬁcient validation of algorithmically (co-)created business process variants, and their continuous management.",
        "keywords": [
            "Business process improvement",
            "Process redesign",
            "Reinforcement learning",
            "AB testing",
            "Grounded theory",
            "Business process management"
        ],
        "authors": [
            "Aaron Friedrich Kurz",
            "Timotheus Kampik",
            "Luise Pufahl",
            "Ingo Weber"
        ],
        "file_path": "data/sosym-all/s10270-024-01229-2.pdf"
    },
    {
        "title": "Modeling Multi-agent systems with ANote",
        "submission-date": "2003/12",
        "publication-date": "2004/08",
        "abstract": "An important issue in getting the agent technology into mainstream software development is the development of appropriate methodologies for developing agent-oriented systems. This paper presents an approach to model distributed systems based on a goal-oriented requirements acquisition. These models are acquired as instances of a conceptual meta-model. The latter can be represented as a graph where each node captures a concept such as, e.g., goal, action, agent, or scenario, and where the edges capture semantic links between such abstractions. This approach is supported by a modeling language, the ANote, which presents views that capture the most important modeling aspects according to the concept currently under consideration.",
        "keywords": [
            "Software agents",
            "Multi-agent system analysis",
            "Modeling language",
            "Views"
        ],
        "authors": [
            "Ricardo Choren",
            "Carlos Lucena"
        ],
        "file_path": "data/sosym-all/s10270-004-0065-y.pdf"
    },
    {
        "title": "Gamiﬁcation of business process modeling education: an experimental analysis",
        "submission-date": "2023/08",
        "publication-date": "2024/04",
        "abstract": "Gamiﬁcation, the practice of using game elements in non-recreational contexts to increase user participation and interest, has been applied more and more throughout the years in software engineering. Business process modeling is a skill considered fundamental for software engineers, with Business Process Modeling Notation (BPMN) being one of the most commonly used notations for this discipline. BPMN modeling is present in different curricula in speciﬁc Master’s Degree courses related to software engineering but is usually seen by students as an unappealing or uninteresting activity. Gamiﬁcation could potentially solve this issue, though there have been no relevant attempts in research yet. This paper aims at collecting preliminary insights on how gamiﬁcation affects students’ motivation in performing BPMN modeling tasks and—as a consequence—their productivity and learning outcomes. A web application for modeling BPMN diagrams augmented with gamiﬁcation mechanics such as feedback, rewards, progression, and penalization has been compared with a non-gamiﬁed version that provides more limited feedback in an experiment involving 200 students. The diagrams modeled by the students are collected and analyzed after the experiment. Students’ opinions are gathered using a post-experiment questionnaire. Statistical analysis showedthatgamiﬁcationleadsstudentstocheckmoreoftenfortheirsolutions’correctness,increasingthesemanticcorrectness of their diagrams, thus showing that it can improve students’ modeling skills. The results, however, are mixed and require additional experiments in the future to ﬁne-tune the tool for actual classroom use.",
        "keywords": [
            "Gamiﬁcation",
            "BPMN modeling",
            "Teaching",
            "Information systems"
        ],
        "authors": [
            "Giacomo Garaccione",
            "Riccardo Coppola",
            "Luca Ardito",
            "Marco Torchiano"
        ],
        "file_path": "data/sosym-all/s10270-024-01171-3.pdf"
    },
    {
        "title": "How are informal diagrams used in software engineering? An exploratory study of open-source and industrial practices",
        "submission-date": "2024/03",
        "publication-date": "2024/12",
        "abstract": "In software engineering practice, models created for communication and documentation are often informal. This limits the applicability of powerful model-driven engineering mechanisms. Understanding the motivations and use of informal diagrams can improve modelling techniques and tools, by bringing together the beneﬁts of both informal diagramming and modelling using modelling languages and modelling tools. In this paper, we report on an initial exploration effort to investigate the use of informal diagramming in both open-source software repositories and industrial software engineering practices. We carried out a repository mining study on open-source software repositories seeking informal diagrams and classiﬁed them according to what they represent and how they are used. Additionally, we describe industrial practices that rely to some extent on informal diagramming, as gathered through unstructured interviews with practitioners. We compare the ﬁndings from these data sources and discuss how informal diagrams are used in practice.",
        "keywords": [
            "Informal diagramming",
            "Repository mining",
            "Flexible modelling"
        ],
        "authors": [
            "Robbert Jongeling",
            "Antonio Cicchetti",
            "Federico Ciccozzi"
        ],
        "file_path": "data/sosym-all/s10270-024-01252-3.pdf"
    },
    {
        "title": "Mining team compositions for collaborative work in business processes",
        "submission-date": "2015/10",
        "publication-date": "2016/10",
        "abstract": "Process mining aims at discovering processes by extracting knowledge about their different perspectives from event logs. The resource perspective (or organisational perspective) deals, among others, with the assignment of resources to process activities. Mining in relation to this perspective aims to extract rules on resource assignments for the process activities. Prior research in this area is limited by the assumption that only one resource is responsible for each process activity, and hence, collaborative activities are disregarded. In this paper, we leverage this assumption by developing a process mining approach that is able to discover team compositions for collaborative process activities from event logs. We evaluate our novel mining approach in terms of computational performance and practical applicability.",
        "keywords": [
            "Business process management",
            "Declarative process mining",
            "Event log analysis",
            "Resource perspective",
            "Teamwork"
        ],
        "authors": [
            "Stefan Schönig",
            "Cristina Cabanillas",
            "Claudio Di Ciccio",
            "Stefan Jablonski",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-016-0567-4.pdf"
    },
    {
        "title": "GoRIM: a model-driven method for enhancing regulatory intelligence",
        "submission-date": "2020/08",
        "publication-date": "2021/11",
        "abstract": "Regulators are under constant pressure to demonstrate if and how the regulations they administer, which impose many requirements on various systems and processes, achieve intended societal outcomes. Traditionally, regulators have relied on impact assessments, risk analysis, and cost–beneﬁt analysis to assess compliance with regulations. These methods, however, are effort and time intensive and focus on the efﬁciency of regulatory processes rather than on the effectiveness of the regulatory initiatives meant to improve compliance to regulations and the latter’s impact on intended societal outcomes. Goal-oriented modelling and data analytics approaches provide the basis for the development of more sophisticated methods and tools to better address the needs of regulators. This paper introduces the goal-oriented regulatory intelligence method (GoRIM), which enables effective management of regulations through modelling and data analytics. Through continuous monitoring, assessing, and reporting on efﬁciency and effectiveness aspects, GoRIM is meant to facilitate the analysis of feedback loops between regulations, regulatory initiatives, and societal outcomes. To demonstrate the applicability and perceived usefulness of GoRIM in addressing the ﬁrst feedback loop between regulations and initiatives, we evaluated it through three case studies involving regulators from different contexts, with positive results. GoRIM extends the concept of regulatory intelligence beyond the analysis of compliance. It also provides practical guidelines and tools to regulators for making, in a timely way, evidence-based decisions related to the addition, modiﬁcation, or repeal of regulations and related regulatory initiatives. In addition, GoRIM helps better identify software and information needs for enabling such decisions.",
        "keywords": [
            "Data analytics",
            "Evidence-based decision-making",
            "Goal-oriented modelling",
            "GRL",
            "Regulations modelling",
            "Regulatory intelligence"
        ],
        "authors": [
            "Okhaide Akhigbe",
            "Daniel Amyot",
            "Gregory Richards",
            "Lysanne Lessard"
        ],
        "file_path": "data/sosym-all/s10270-021-00949-z.pdf"
    },
    {
        "title": "Integrated revision and variation control for evolving model-driven software product lines",
        "submission-date": "2018/01",
        "publication-date": "2019/02",
        "abstract": "Software engineering projects are faced with abstraction, which is achieved by software models, historical evolution, which is addressed by revision control, and variability, which is managed with the help of software product line engineering. Addressing these phenomena by separate tools ignores obvious overlaps and therefore fails at exploiting synergies between revision and variation control for models. In this article, we present a conceptual framework for integrated revision and variation control of model-driven software projects. The framework reuses the abstractions of revision graphs and feature models and follows an iterative, revision-control-like approach to software product line engineering called product-based product line development. A single version (i.e., a variant of a selected revision) is made available in a workspace, where the user may apply arbitrary modiﬁcations. Based on a user-provided speciﬁcation of the affected variants, the changes are automatically written back to a transparent repository that relies on an internal multi-version storage. The uniform handling of revisions and variants of models is achieved by transparently mapping version concepts to a semantic base layer, which is deﬁned upon propositional logic. At the heart of the conceptual framework is a dynamic ﬁltered editing model, which allows that the versioned artifacts and the feature model co-evolve. We contribute algorithms for checkout and commit, which satisfy a set of consistency constraints referring to variant speciﬁcations in an evolving feature model. This article furthermore addresses the orchestration of collaborative development by distributed replication and the well formedness of text and model artifacts to be checked out into the workspace. The Eclipse-based tool SuperMod demonstrates the feasibility of the conceptual framework. It allows the user to reuse arbitrary editing tools for text-based programming and/or Ecore-based modeling languages. An evaluation based on three case studies investigates the properties of SuperMod with a speciﬁc focus on ﬁltered editing. The evaluation demonstrates that the dynamic ﬁltered editing model reduces the cognitive complexity and the amount of user interaction necessary for variation control when compared to unﬁltered model-driven approaches to software product line engineering.",
        "keywords": [
            "Model versioning",
            "Model-driven product lines",
            "Variation control systems",
            "Tool integration",
            "Integrated historical and logical versioning"
        ],
        "authors": [
            "Felix Schwägerl",
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-019-00722-3.pdf"
    },
    {
        "title": "Correction: iDOCEM",
        "submission-date": "2024/08",
        "publication-date": "2024/08",
        "abstract": "In this article, the title of the paper was published as “iDO-CEM”. It should be corrected as “iDOCEM: deﬁning a common terminology for object-centric event logging and data-centric process modelling”.\nThe original article has been updated.",
        "keywords": [],
        "authors": [
            "Charlotte Verbruggen",
            "Alexandre Goossens",
            "Johannes De Smedt",
            "Jan Vanthienen",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-024-01200-1.pdf"
    },
    {
        "title": "Refactoring OCL annotated UML class diagrams",
        "submission-date": "2006/03",
        "publication-date": "2007/05",
        "abstract": "Refactoring of UML class diagrams is an emerging research topic and heavily inspired by refactoring of program code written in object-oriented implementation languages. Current class diagram refactoring techniques concentrate on the diagrammatic part but neglect OCL constraints that might become syntactically incorrect by changing the underlying class diagram. This paper formalizes the most important refactoring rules for class diagrams and classifies them with respect to their impact on attached OCL constraints. For refactoring rules that have an impact on OCL constraints, we formalize the necessary changes of the attached constraints. Our refactoring rules are specified in a graph-grammar inspired formalism. They have been implemented as QVT transformation rules. We finally discuss for our refactoring rules the problem of syntax preservation and show, by using the KeY-system, how this can be resolved.",
        "keywords": [
            "Refactoring",
            "QVT",
            "Imperative OCL",
            "Graph-transformations",
            "Syntax preserving refactoring rules",
            "Source code verification"
        ],
        "authors": [
            "Slaviša Markovi´c",
            "Thomas Baar"
        ],
        "file_path": "data/sosym-all/s10270-007-0056-x.pdf"
    },
    {
        "title": "Advanced prefetching and caching of models with PrefetchML",
        "submission-date": "2017/03",
        "publication-date": "2018/03",
        "abstract": "Caching and prefetching techniques have been used for decades in database engines and ﬁle systems to improve the perfor- mance of I/O-intensive application. A prefetching algorithm typically beneﬁts from the system’s latencies by loading into main memory elements that will be needed in the future, speeding up data access. While these solutions can bring a signiﬁcant improvement in terms of execution time, prefetching rules are often deﬁned at the data level, making them hard to understand, maintain, and optimize. In addition, low-level prefetching and caching components are difﬁcult to align with scalable model persistence frameworks because they are unaware of potential optimizations relying on the analysis of metamodel-level infor- mation and are less present in NoSQL databases, a common solution to store large models. To overcome this situation, we propose PrefetchML, a framework that executes prefetching and caching strategies over models. Our solution embeds a DSL to conﬁgure precisely the prefetching rules to follow and a monitoring component providing insights on how the prefetching execution is working to help designers optimize his performance plans. Our experiments show that PrefetchML is a suitable solution to improve query execution time on top of scalable model persistence frameworks. Tool support is fully available online as an open-source Eclipse plug-in.",
        "keywords": [
            "Prefetching",
            "MDE",
            "DSL",
            "Scalability",
            "Persistence framework",
            "NoSQL"
        ],
        "authors": [
            "Gwendal Daniel",
            "Gerson Sunyé",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-018-0671-8.pdf"
    },
    {
        "title": "Enhanced graph rewriting systems for complex software domains",
        "submission-date": "2013/12",
        "publication-date": "2014/09",
        "abstract": "Methodologies for correct by construction rec-onﬁgurations can efﬁciently solve consistency issues in dynamic software architecture. Graph-based models are appropriate for designing such architectures and methods. At the same time, they may be unfit to characterize a system from a non-functional perspective. This stems from efﬁciency and applicability limitations in handling time-varying character-istics and their related dependencies. In order to lift these restrictions, an extension to graph rewriting systems is proposed herein. The suitability of this approach, as well as the restraints of currently available ones, is illustrated, analyzed, and experimentally evaluated with reference to a concrete example. This investigation demonstrates that the conceived solution can (i) express any kind of algebraic dependencies between evolving requirements and properties; (ii) significantly ameliorate the efﬁciency and scalability of system modiﬁcations with respect to classic methodologies; (iii) provide an efﬁcient access to attribute values; (iv) be fruitfully exploited in software management systems; and (v) guarantee theoretical properties of a grammar, like its termination.",
        "keywords": [
            "Constrained and attributed rewriting systems",
            "Graph rewriting systems",
            "Non-functional requirements",
            "Dynamic software architecture",
            "Correctness by construction"
        ],
        "authors": [
            "Cédric Eichler",
            "Thierry Monteil",
            "Patricia Stolf",
            "Luigi Alfredo Grieco",
            "Khalil Drira"
        ],
        "file_path": "data/sosym-all/s10270-014-0433-1.pdf"
    },
    {
        "title": "A method of reﬁnement in UML-B",
        "submission-date": "2012/05",
        "publication-date": "2013/12",
        "abstract": "UML-B is a ‘UML-like’ graphical front-end for Event-B that provides support for object-oriented and state machine modelling concepts, which are not available in Event-B. In particular, UML-B includes class diagram and state machine diagram editors with automatic generation of corresponding Event-B. In Event-B, reﬁnement is used to relate system models at different abstraction levels. The same reﬁnement concepts are also applicable in UML-B but require special consideration due to the higher-level modelling concepts. In previous work, we described a case study to introduce support for reﬁnement in UML-B. We now provide a more complete presentation of the technique of reﬁnement in UML-B including a formalisation of the reﬁnement rules and a deﬁnition of the extensions to the abstract syntax of UML-B notation. The provision of gluing invariants to discharge the proof obligations associated with a reﬁnement is a signiﬁcant step in providing veriﬁable models. We discuss and compare two approaches for constructing gluing invariants in the context of UML-B reﬁnement.",
        "keywords": [
            "Visual modelling languages",
            "Formal speciﬁcation",
            "UML-B",
            "Event-B",
            "Class diagram",
            "State machine"
        ],
        "authors": [
            "Mar Yah Said",
            "Michael Butler",
            "Colin Snook"
        ],
        "file_path": "data/sosym-all/s10270-013-0391-z.pdf"
    },
    {
        "title": "A system-theoretic assurance framework for safety-driven systems engineering",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "The complexity of safety-critical systems is continuously increasing. To create safe systems despite the complexity, the\nsystem development requires a strong integration of system design and safety activities. A promising choice for integrating\nsystem design and safety activities are model-based approaches. They can help to handle complexity through abstraction,\nautomation, and reuse and are applied to design, analyze, and assure systems. In practice, however, there is often a disconnect\nbetween the model-based design and safety activities. At the same time, there is often a delay until recent approaches are\navailable in model-based frameworks. As a result, the advantages of the models are often not fully utilized. Therefore, this\narticle proposes a framework that integrates recent approaches for system design (model-based systems engineering), safety\nanalysis (system-theoretic process analysis), and safety assurance (goal structuring notation). The framework is implemented\nin the systems modeling language (SysML), and the focus is placed on the connection between the safety analysis and\nsafety assurance activities. It is shown how the model-based integration enables tool assistance for the systematic creation,\nanalysis, and maintenance of safety artifacts. The framework is demonstrated with the system design, safety analysis, and\nsafety assurance of a collision avoidance system for aircraft. The model-based nature of the design and safety activities is\nutilized to support the systematic generation, analysis, and maintenance of safety artifacts.",
        "keywords": [
            "MBSE",
            "Safety",
            "STPA",
            "SysML",
            "GSN"
        ],
        "authors": [
            "Alexander Ahlbrecht",
            "Jasper Sprockhoff",
            "Umut Durak"
        ],
        "file_path": "data/sosym-all/s10270-024-01209-6.pdf"
    },
    {
        "title": "Optimization framework for DFG-based automated process discovery approaches",
        "submission-date": "2020/02",
        "publication-date": "2021/02",
        "abstract": "The problem of automatically discovering business process models from event logs has been intensely investigated in the past two decades, leading to a wide range of approaches that strike various trade-offs between accuracy, model complexity, and execution time. A few studies have suggested that the accuracy of automated process discovery approaches can be enhanced by means of metaheuristic optimization techniques. However, these studies have remained at the level of proposals without validation on real-life datasets or they have only considered one metaheuristic in isolation. This article presents a metaheuristic optimization framework for automated process discovery. The key idea of the framework is to construct a directly-follows graph (DFG) from the event log, to perturb this DFG so as to generate new candidate solutions, and to apply a DFG-based automated process discovery approach in order to derive a process model from each DFG. The framework can be instantiated by linking it to an automated process discovery approach, an optimization metaheuristic, and the quality measure to be optimized (e.g., ﬁtness, precision, F-score). The article considers several instantiations of the framework corresponding to four optimization metaheuristics, three automated process discovery approaches (Inductive Miner—directly-follows, Fodina, and Split Miner), and one accuracy measure (Markovian F-score). These framework instances are compared using a set of 20 real-life event logs. The evaluation shows that metaheuristic optimization consistently yields visible improvements in F-score for all the three automated process discovery approaches, at the cost of execution times in the order of minutes, versus seconds for the baseline approaches.",
        "keywords": [
            "Automated process discovery",
            "Metaheuristic optimization",
            "Process mining"
        ],
        "authors": [
            "Adriano Augusto",
            "Marlon Dumas",
            "Marcello La Rosa",
            "Sander J. J. Leemans",
            "Seppe K. L. M. vanden Broucke"
        ],
        "file_path": "data/sosym-all/s10270-020-00846-x.pdf"
    },
    {
        "title": "Formalization of UML state machines using temporal logic",
        "submission-date": "2003/03",
        "publication-date": "2003/11",
        "abstract": "The main purpose of this paper is to approach the use of formal methods in computing. In more specific terms, we use a temporal logic to formalize the most fundamental aspects of the semantics of UML state machines. We pay special attention to the dynamic aspects of the different operations associated with states and transitions, as well as the behaviour of transitions related with composite states. This, to the best of our knowledge, has not been done heretofore using temporal logic.\n\nOur formalization is based on a temporal logic that combines points, intervals, and dates. Moreover this new temporal logic is built over an innovative and simple topological semantics, which simplifies the metatheory development.",
        "keywords": [
            "Statechart diagrams",
            "interval temporal logic",
            "specification",
            "formal semantics",
            "UML"
        ],
        "authors": [
            "Carlos Rossi",
            "Manuel Enciso",
            "Inmaculada P. de Guzm´an"
        ],
        "file_path": "data/sosym-all/s10270-003-0029-7.pdf"
    },
    {
        "title": "Counterexample classification",
        "submission-date": "2022/06",
        "publication-date": "2023/07",
        "abstract": "In model checking, when a model fails to satisfy the desired speciﬁcation, a typical model checker provides a counterexample that illustrates how the violation occurs. In general, there exist many diverse counterexamples that exhibit distinct violating behaviors, which the user may wish to examine before deciding how to repair the model. Unfortunately, (1) the number of counterexamples may be too large to enumerate one by one, and (2) many of these counterexamples are redundant, in that they describe the same type of violating behavior. In this paper, we propose a technique called counterexample classiﬁcation. The goal of classiﬁcation is to cover the space of all counterexamples into a ﬁnite set of counterexample classes, each of which describes a distinct type of violating behavior for the given speciﬁcation. These classes are then presented as a summary of possible violating behaviors in the system, freeing the user from manually having to inspect or analyze numerous counterexamples to extract the same information. We have implemented a prototype of our technique on top of an existing formal modeling and veriﬁcation tool, the Alloy Analyzer, and evaluated the effectiveness of the technique on case studies involving the well-known Needham–Schroeder and TCP protocols with promising results.",
        "keywords": [
            "Model checking",
            "Formal modelling",
            "debugging"
        ],
        "authors": [
            "Cole Vick",
            "Eunsuk Kang",
            "Stavros Tripakis"
        ],
        "file_path": "data/sosym-all/s10270-023-01118-0.pdf"
    },
    {
        "title": "The shape of feature code: an analysis of twenty C-preprocessor-based systems",
        "submission-date": "2014/10",
        "publication-date": "2015/07",
        "abstract": "Feature annotations (e.g., code fragments guarded by #ifdef C-preprocessor directives) control code extensions related to features. Feature annotations have long been said to be undesirable. When maintaining features that control many annotations, there is a high risk of ripple effects. Also, excessive use of feature annotations leads to code clutter, hinder program comprehension and harden maintenance. To prevent such problems, developers should monitor the use of feature annotations, for example, by setting acceptable thresholds. Interestingly, little is known about how to extract thresholds in practice, and which values are representative for feature-related metrics. To address this issue, we analyze the statistical distribution of three feature-related metrics collected from a corpus of 20 well-known and long-lived C-preprocessor-based systems from different domains. We consider three metrics: scattering degree of feature constants, tangling degree of feature expressions, and nesting depth of preprocessor annotations. Our ﬁndings show that feature scattering is highly skewed; in 14 systems (70%), the scattering distributions match a power law, making averages and standard deviations unreliable limits. Regarding tangling and nesting, the values tend to follow a uniform distribution; although outliers exist, they have little impact on the mean, suggesting that central statistics measures are reliable thresholds for tangling and nesting. Following our ﬁndings, we then propose thresholds from our benchmark data, as a basis for further investigations.",
        "keywords": [
            "Software families",
            "Preprocessor",
            "Feature-related metrics",
            "Thresholds",
            "Power-law distribution"
        ],
        "authors": [
            "Rodrigo Queiroz",
            "Leonardo Passos",
            "Marco Tulio Valente",
            "Claus Hunsen",
            "Sven Apel",
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-015-0483-z.pdf"
    },
    {
        "title": "Mesoscale performance simulation of multicore processor systems",
        "submission-date": "2011/07",
        "publication-date": "2012/01",
        "abstract": "Modern microprocessor design relies heavily on detailed full-chip performance simulations to evaluate complex trade-offs. Typically, different design alternatives are tried out for a specific sub-system or component, while keeping the rest of the system unchanged. We observe that full-chip simulations for such studies is overkill. This paper introduces mesoscale simulation, which employs high-level modeling for the unchanged parts of a design and uses detailed cycle-accurate simulations for the components being modified. This combination of high-level and low-level modeling enables accuracy on par with detailed full-chip modeling while achieving much higher simulation speeds than detailed full-chip simulations. Consequently, mesoscale models can be used to quickly explore vast areas of the design space with high fidelity. We describe a proof-of-concept mesoscale implementation of the memory subsystem of the Cell/B.E. processor and discuss results from running various workloads.",
        "keywords": [
            "Microprocessor design",
            "Performance simulations",
            "Performance modeling"
        ],
        "authors": [
            "Peter Altevogt",
            "Tibor Kiss",
            "Mike Kistler",
            "Ram Rangan"
        ],
        "file_path": "data/sosym-all/s10270-012-0231-6.pdf"
    },
    {
        "title": "Analysing refactoring dependencies using graph transformation",
        "submission-date": "2005/02",
        "publication-date": "2007/01",
        "abstract": "Refactoring is a widely accepted technique\nto improve the structure of object-oriented software.\nNevertheless, existing tool support remains restricted\nto automatically applying refactoring transformations.\nDeciding what to refactor and which refactoring to apply\nstill remains a difﬁcult manual process, due to the many\ndependencies and interrelationships between relevant\nrefactorings. In this paper, we represent refactorings as\ngraph transformations, and we propose the technique\nof critical pair analysis to detect the implicit dependen-\ncies between refactorings. The results of this analysis\ncan help the developer to make an informed decision of\nwhich refactoring is most suitable in a given context and\nwhy. We report on several experiments we carried out\nin the AGG graph transformation tool to support our\nclaims.",
        "keywords": [
            "Refactoring",
            "Graph transformation",
            "Critical pair analysis",
            "Dependency analysis",
            "AGG"
        ],
        "authors": [
            "Tom Mens",
            "Gabriele Taentzer",
            "Olga Runge"
        ],
        "file_path": "data/sosym-all/s10270-006-0044-6.pdf"
    },
    {
        "title": "CEViNEdit: improving the process of creating cognitively effective graphical editors with GMF",
        "submission-date": "2019/02",
        "publication-date": "2020/10",
        "abstract": "The rise of domain-speciﬁc (Visual) languages and the inherent complexity of developing graphical editors for these languages have led to the emergence of proposals that provide support for this task. Most of these proposals are principally based on EMF and GMF, which effectively help to simplify and increase the level of automation of the development process of the editors, but it is important to recall that these proposals have some important disadvantages, mainly related to the learning curve of these technologies, poor documentation or the complexity of providing all the customisation possibilities to the user. In addition, in the process of developing a domain-speciﬁc language, issues related to graphical conventions have historically been undervalued, while most of the effort has been focused on semantic aspects. In fact, deﬁnitions of the concrete (visual) syntax of modelling languages in Software Engineering are usually based on common sense, intuition, the reuse of existing notations or emulation of common practices. In order to alleviate the inherent complexity of the EMF/GMF approach for the development of graphical editors and to support the evaluation of the quality of visual notations of modelling languages, this article presents CEViNEdit, an intuitive tool that simultaneously supports the semi-automatic generation of graphical editors and the assessment of the cognitive effectiveness of the visual notation implemented by the editor.",
        "keywords": [
            "Model-driven engineering (MDE)",
            "Domain-Speciﬁc Language (DSL)",
            "Visual notation",
            "Cognitive effectiveness"
        ],
        "authors": [
            "David Granada",
            "Juan M. Vara",
            "Mercedes Merayo",
            "Esperanza Marcos"
        ],
        "file_path": "data/sosym-all/s10270-020-00833-2.pdf"
    },
    {
        "title": "Identifying duplicate functionality in textual use cases by aligning semantic actions",
        "submission-date": "2013/12",
        "publication-date": "2014/08",
        "abstract": "Developing high-quality requirements speciﬁcations often demands a thoughtful analysis and an adequate levelofexpertisefromanalysts.Althoughrequirementsmod- eling techniques provide mechanisms for abstraction and clarity, fostering the reuse of shared functionality (e.g., via UML relationships for use cases), they are seldom employed in practice. A particular quality problem of textual requirements, such as use cases, is that of having duplicate pieces of functionality scattered across the speciﬁcations. Duplicate functionality can sometimes improve readability for end users, but hinders development-related tasks such as effort estimation, feature prioritization, and maintenance, among others. Unfortunately, inspecting textual requirements by hand in order to deal with redundant functionality can be an arduous, time-consuming, and error-prone activity for ana- lysts. In this context, we introduce a novel approach called ReqAligner that aids analysts to spot signs of duplication in use cases in an automated fashion. To do so, ReqAligner com- bines several text processing techniques, such as a use case-aware classiﬁer and a customized algorithm for sequence alignment. Essentially, the classiﬁer converts the use cases into an abstract representation that consists of sequences of semantic actions, and then these sequences are compared pairwise in order to identify action matches, which become possible duplications. We have applied our technique to ﬁve real-world speciﬁcations, achieving promising results and identifying many sources of duplication in the use cases.",
        "keywords": [
            "Use case modeling",
            "Use case refactoring",
            "Natural language processing",
            "Sequence alignment",
            "Requirements engineering",
            "Machine learning"
        ],
        "authors": [
            "Alejandro Rago",
            "Claudia Marcos",
            "J. Andres Diaz-Pace"
        ],
        "file_path": "data/sosym-all/s10270-014-0431-3.pdf"
    },
    {
        "title": "The triptych of conceptual modeling\nA framework for a better understanding of conceptual modeling",
        "submission-date": "2020/09",
        "publication-date": "2020/11",
        "abstract": "We understand this paper as a contribution to the “anatomy” of conceptual models. We propose a signature of conceptual models for their characterization, which allows a clear distinction from other types of models. The motivation for this work arose from the observation that conceptual models are widely discussed in science and practice, especially in computer science, but that their potential is far from being exploited. We combine our proposal of a more transparent explanation of the nature of conceptual models with an approach that classifies conceptual models as a link between the dimension of linguistic terms and the encyclopedic dimension of notions. As a paradigm we use the triptych, whose central tableau represents the model dimension. The effectiveness of this explanatory approach is illustrated by a number of examples. We derive a number of open research questions that should be answered to complete the anatomy of conceptual models.",
        "keywords": [
            "Conceptual modeling",
            "Modeling languages",
            "Model characteristics",
            "Model hierarchies",
            "Language hierarchies",
            "Concept",
            "Notion",
            "Term"
        ],
        "authors": [
            "Heinrich C. Mayr\nBernhard Thalheim"
        ],
        "file_path": "data/sosym-all/s10270-020-00836-z.pdf"
    },
    {
        "title": "Numeric semantics of class diagrams with multiplicity and uniqueness constraints",
        "submission-date": "2012/03",
        "publication-date": "2012/11",
        "abstract": "We translate class diagrams with multiplicity constraintsanduniquenessattributestoinequalitiesovernon-negative integers. Based on this numeric semantics we check the satisﬁability and consistency of class diagrams and compute minimal models. We show that this approach is efﬁcient and provides succinct user feedback in the case of errors. In an experimental section we demonstrate that general off-the-shelf solvers for integer linear programming perform as well on real-world and synthetic benchmarks as specialised algorithms do, facilitating the extension of the formal model by further numeric constraints like cost functions. Our results are embedded in a research programme on reasoning about class diagrams and are motivated by applications in configuration management. Compared to other (for instance logic-based) approaches our aim is to hide the complexity of formal methods behind familiar user interfaces like class diagrams and to concentrate on problems that can be solved efficiently in order to be able to provide immediate feedback to users.",
        "keywords": [
            "Model engineering",
            "Formal methods",
            "Reasoning about class diagrams",
            "Integer linear programming",
            "Configuration management"
        ],
        "authors": [
            "Ingo Feinerer",
            "Gernot Salzer"
        ],
        "file_path": "data/sosym-all/s10270-012-0294-4.pdf"
    },
    {
        "title": "Heterogeneous megamodel management using collection operators",
        "submission-date": "2018/06",
        "publication-date": "2019/06",
        "abstract": "Modelmanagementtechniqueshelptamethecomplexitycausedbythemanymodelsusedinlarge-scalesoftwaredevelopment; however, these techniques have focused on operators to manipulate individual models rather than entire collections of them. In this work, we begin to address this gap by adapting the widely used map, reduce and ﬁlter collection operators for collections of models represented by megamodels. Key parts of this adaptation include the special handling of relationships between models and the use of polymorphism to support heterogeneous model collections. We evaluate the complexity of our operators analytically and demonstrate their applicability on six diverse megamodel management scenarios. We describe our tool support for the approach and evaluate its scalability experimentally as well as its applicability on a practical application from the automotive domain.",
        "keywords": [
            "Megamodel",
            "Model management",
            "Heterogeneous"
        ],
        "authors": [
            "Rick Salay",
            "Sahar Kokaly",
            "Alessio Di Sandro",
            "Nick L. S. Fung",
            "Marsha Chechik"
        ],
        "file_path": "data/sosym-all/s10270-019-00738-9.pdf"
    },
    {
        "title": "Enhancing process mining with visual resource analytics",
        "submission-date": "2024/11",
        "publication-date": "Not found",
        "abstract": "Resource analysis in process mining focuses on understanding the behavior and performance of resources involved in business processes. While previous research has provided various insights into resource-related aspects, the visualization of these insights remains insufﬁciently developed. This paper addresses this gap by proposing a novel resource analytics technique that integrates metrics from four critical resource-related areas: resource allocation, resource performance, workload distribution, and capacity utilization. The technique incorporates interactive visualizations and process model views to support process analysts in performing resource analysis tasks such as identifying bottlenecks and inefﬁciencies. Results from a user evaluation demonstrate that the proposed technique enhances the accuracy of resource analysis tasks and is highly regarded for its ease of use and perceived usefulness.",
        "keywords": [
            "Process mining",
            "Resource analysis",
            "Performance analysis",
            "Visual analytics"
        ],
        "authors": [
            "Alana Hoogmoed",
            "Djordje Djurica",
            "Maxim Vidgof",
            "Christoﬀer Rubensson",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-025-01315-z.pdf"
    },
    {
        "title": "A modeling methodology for collaborative evaluation of future automotive innovations",
        "submission-date": "2020/03",
        "publication-date": "2021/04",
        "abstract": "The rapid introduction of innovations plays a major role in automotive industries. Today, once a member of the automotive value chain devises an innovation, the time-to-market could be years due to traditional forms of collaboration. It is thus indispensable for companies to collaborate on reducing the time to find and design innovations in order to remain competitive. One idea is to streamline the innovation process outside current product development cycles (7 + years) within the automotive value chain. In our work, we propose a modeling methodology offering a better collaboration in this innovation design phase. Beginning with the innovation idea, our approach allows capturing requirements, functional, and structural aspects of the innovation under design in an innovation model with defined semantics. This innovation model can then be exchanged between automotive partners, who in their turn can refine and exchange the refined model within an iterative process toward an initial innovation evaluation. Additionally, we propose a generic description of how such innovation models can be captured in SysML, a system modeling standard. Our approach is illustrated by a “wireless car charging” innovation case-study showing how a possible collaboration at different modeling abstraction levels could take place and how consistency of the models exchanged can be verified. The consistency check is exemplified by timing specifications.",
        "keywords": [
            "Innovation Modeling Grid (IMoG)",
            "Automotive innovations",
            "Model-based design",
            "SysML"
        ],
        "authors": [
            "Maher Fakih\nOliver Klemp\nStefan Puch\nKim Grüttner"
        ],
        "file_path": "data/sosym-all/s10270-021-00864-3.pdf"
    },
    {
        "title": "Using boundary objects and methodological island (BOMI) modeling in large-scale agile systems development",
        "submission-date": "2023/05",
        "publication-date": "2024/08",
        "abstract": "Large-scale systems development commonly faces the challenge of managing relevant knowledge between different organizational groups, particularly in increasingly agile contexts. Here, there is a conflict between coordination and group autonomy, and it is challenging to determine what necessary coordination information must be shared by what teams or groups, and what can be left to local team management. We introduce a way to manage this complexity using a modeling framework based on two core concepts: methodological islands (i.e., groups using different development methods than the surrounding organization) and boundary objects (i.e., artifacts that create a common understanding across team borders). However, we found that companies often lack a systematic way of assessing coordination issues and the use of boundary objects between methodological islands. As part of an iterative design science study, we have addressed this gap by producing a modeling framework (BOMI: Boundary Objects and Methodological Islands) to better capture and analyze coordination and knowledge management in practice. This framework includes a metamodel, as well as a list of bad smells over this metamodel that can be leveraged to detect inter-team coordination issues. The framework also includes a methodology to suggest concrete modeling steps and broader guidelines to help apply the approach successfully in practice. We have developed Eclipse-based tool support for the BOMI method, allowing for both graphical and textual model creation, and including an implementation of views over BOMI instance models in order to manage model complexity. We have evaluated these artifacts iteratively together with five large-scale companies developing complex systems. In this work, we describe the BOMI framework and its iterative evaluation in several real cases, reporting on lessons learned and identifying future work. We have produced a matured and stable modeling framework which facilitates understanding and reflection over complex organizational configurations, communication, governance, and coordination of knowledge artifacts in large-scale agile system development.",
        "keywords": [
            "Boundary objects",
            "Agile development",
            "Empirical studies"
        ],
        "authors": [
            "Jörg Holtmann",
            "Jennifer Horkoﬀ",
            "Rebekka Wohlrab",
            "Victoria Vu",
            "Rashidah Kasauli",
            "Salome Maro",
            "Jan-Philipp Steghöfer",
            "Eric Knauss"
        ],
        "file_path": "data/sosym-all/s10270-024-01193-x.pdf"
    },
    {
        "title": "A case study about the improvement of business process models driven by indicators",
        "submission-date": "2015/02",
        "publication-date": "2015/07",
        "abstract": "Organizations are increasingly concerned about business process model improvement in their efforts to guarantee improved operational efﬁciency. Quality assurance of business process models should be addressed in the most objective manner, e.g., through the application of measures, but the assessment of measurement results is not a straightforward task and it requires the identiﬁcation of relevant indicators and threshold values, which are able to distinguish different levels of process model quality. Furthermore, indicators must support the improvements of the models by using suitable guidelines. In this paper, we present a case study to evaluate the BPMIMA framework for BP model improvement. This framework is composed of empirically validated measures related to quality characteristics of the models, a set of indicators with validated thresholds associated with modeling guidelines and a prototype supporting tool. The obtained data suggest that the redesign by applying guidelines driven by the indicator results was successful, as the understandability and modiﬁability of the models were improved. In addition, the changes in the models according to guidelines were perceived as acceptable by the practitioners who participated in the case study.",
        "keywords": [
            "Businessprocessimprovement",
            "Measurement",
            "Indicators",
            "Redesign guidelines"
        ],
        "authors": [
            "Laura Sánchez-González",
            "Félix García",
            "Francisco Ruiz",
            "Mario Piattini"
        ],
        "file_path": "data/sosym-all/s10270-015-0482-0.pdf"
    },
    {
        "title": "Bridging proprietary modelling and open-source model management tools: the case of PTC Integrity Modeller and Epsilon",
        "submission-date": "2018/07",
        "publication-date": "2019/05",
        "abstract": "While the majority of research on Model-Based Software Engineering revolves around open-source modelling frameworks such as the Eclipse Modelling Framework, the use of commercial and closed-source modelling tools such as RSA, Rhapsody, MagicDraw and Enterprise Architect appears to be the norm in industry at present. This technical gap can prohibit industrial users from reaping the beneﬁts of state-of-the-art research-based tools in their practice. In this paper, we discuss an attempt to bridge a proprietary UML modelling tool (PTC Integrity Modeller), which is used for model-based development of safety-critical systems at Rolls-Royce, with an open-source family of languages for automated model management (Epsilon). We present the architecture of our solution, the challenges we encountered in developing it, and a performance comparison against the tool’s built-in scripting interface. In addition, we use the bridge in a real-world industrial case study that involves the coordination with other bridges between proprietary tools and Epsilon.",
        "keywords": [
            "Model-driven engineering",
            "Model management",
            "Open-source"
        ],
        "authors": [
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Stuart Hutchesson",
            "Beatriz Sanchez Pina",
            "Alan Grigg",
            "Mole Li",
            "Dimitrios S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-019-00732-1.pdf"
    },
    {
        "title": "How effective is UML modeling ? An empirical perspective on costs and beneﬁts",
        "submission-date": "2011/10",
        "publication-date": "2012/08",
        "abstract": "Modeling has become a common practice in modern software engineering. Since the mid 1990s the Uniﬁed Modeling Language (UML) has become the de facto standard for modeling software systems. The UML is used in allphasesofsoftwaredevelopment:rangingfromtherequire-ment phase to the maintenance phase. However, empirical evidence regarding the effectiveness of modeling in software development is few and far apart. This paper aims to synthe-size empirical evidence regarding the effectiveness of mod-eling using UML in software development, with a special focus on the cost and beneﬁts.",
        "keywords": [
            "Uniﬁed Modeling Language",
            "Costs and beneﬁts",
            "Quality",
            "Productivity",
            "Effectiveness"
        ],
        "authors": [
            "Michel R. V. Chaudron",
            "Werner Heijstek",
            "Ariadi Nugroho"
        ],
        "file_path": "data/sosym-all/s10270-012-0278-4.pdf"
    },
    {
        "title": "Statistical prioritization for software product line testing: an experience report",
        "submission-date": "2014/10",
        "publication-date": "2015/07",
        "abstract": "Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.",
        "keywords": [
            "Software product line testing",
            "Prioritization",
            "Statistical testing"
        ],
        "authors": [
            "Xavier Devroey",
            "Gilles Perrouin",
            "Maxime Cordy",
            "Hamza Samih",
            "Axel Legay",
            "Pierre-Yves Schobbens",
            "Patrick Heymans"
        ],
        "file_path": "data/sosym-all/s10270-015-0479-8.pdf"
    },
    {
        "title": "DALEC: a framework for the systematic evaluation of data-centric approaches to process management software",
        "submission-date": "2017/07",
        "publication-date": "2018/09",
        "abstract": "The increasing importance of data in business processes has led to the emergence of data-centric business process management, which deviates from the widely used activity-centric paradigm. Data-centric approaches set their focus on data, aiming at supporting data-intensive business processes and increased process ﬂexibility. The objective of this article is to gain profound insights into the maturity of different data-centric approaches as well as their capabilities. In particular, this article will provide a framework for systematically evaluating and comparing data-centric approaches, with regard to the phases of the business process lifecycle. To this end, a systematic literature review (SLR) was conducted with the goal of evaluating the capabilities of data-centric process management approaches. The SLR comprises 38 primary studies which were thoroughly analyzed. The studies were categorized into different approaches, whose capabilities were thoroughly assessed. Special focus was put on the tooling and software of the approaches. The article provides the empirically grounded DALEC framework to evaluate and compare data-centric approaches. Furthermore, the results of the SLR offer insights into existing data-centric approaches and their capabilities. Data-centric approaches promise better support of loosely structured and data-intensive business processes, which may not be adequately represented by activity-centric paradigms.",
        "keywords": [
            "Systematic literature review",
            "Data-centric BPM",
            "DALEC framework",
            "Systematic evaluation"
        ],
        "authors": [
            "Sebastian Steinau",
            "Andrea Marrella",
            "Kevin Andrews",
            "Francesco Leotta",
            "Massimo Mecella",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-018-0695-0.pdf"
    },
    {
        "title": "Ensuring conformance of relational model transformation speciﬁcations and implementations",
        "submission-date": "2011/04",
        "publication-date": "2012/04",
        "abstract": "The correctness of model transformations is a crucial element for model-driven engineering of high-quality software. A prerequisite to verify model transformations at the level of the model transformation speciﬁcation is that an unambiguous formal semantics exists and that the implemen-tation of the model transformation language adheres to this semantics. However, for existing relational model transfor-mation approaches, it is usually not really clear under which constraints particular implementations really conform to the formal semantics. In this paper, we will bridge this gap for the formal semantics of triple graph grammars (TGG) and an existing efﬁcient implementation. While the formal seman-tics assumes backtracking and ignores non-determinism, practical implementations do not support backtracking, re-quire rule sets that ensure determinism, and include further optimizations. Therefore, we capture how the considered TGGimplementationrealizesthetransformationbymeansof operational rules, deﬁne required criteria, and show confor-mance to the formal semantics if these criteria are fulﬁlled.",
        "keywords": [],
        "authors": [
            "Holger Giese",
            "Stephan Hildebrandt",
            "Leen Lambers"
        ],
        "file_path": "data/sosym-all/s10270-012-0247-y.pdf"
    },
    {
        "title": "Precise null-pointer analysis",
        "submission-date": "2009/02",
        "publication-date": "2009/10",
        "abstract": "In Java, C or C++, attempts to dereference the\nnull value result in an exception or a segmentation fault.\nHence, it is important to identify those program points where\nthis undesired behaviour might occur or prove the other pro-\ngram points (and possibly the entire program) safe. To that\npurpose, null-pointer analysis of computer programs checks\nor infers non-null annotations for variables and object\nﬁelds. With few notable exceptions, null-pointer analyses\ncurrently use run-time checks or are incorrect or only verify\nmanually provided annotations. In this paper, we use abstract\ninterpretation to build and prove correct a ﬁrst, ﬂow and con-\ntext-sensitive static null-pointer analysis for Java bytecode\n(and hence Java) which infers non-null annotations. It is\nbased on Boolean formulas, implemented with binary deci-\nsion diagrams. For better precision, it identiﬁes instance or\nstatic ﬁelds that remain always non-null after being initial-\nised. Our experiments show this analysis faster and more pre-\ncise than the correct null-pointer analysis by Hubert, Jensen\nand Pichardie. Moreover, our analysis deals with exceptions,\nwhich is not the case of most others; its formulation is the-\noretically clean and its implementation strong and scalable.\nWe subsequently improve that analysis by using local reason-\ning about ﬁelds that are not always non-null, but happen to\nhold a non-null value when they are accessed. This is a fre-\nquent situation, since programmers typically check a ﬁeld for\nnon-nullness before its access. We conclude with an exam-\nple of use of our analyses to infer null-pointer annotations\nwhich are more precise than those that other inference tools\ncan achieve.",
        "keywords": [
            "Null-pointer analysis",
            "Java bytecode",
            "Static\nanalysis",
            "Abstract interpretation",
            "Automatic software\nveriﬁcation"
        ],
        "authors": [
            "Fausto Spoto"
        ],
        "file_path": "data/sosym-all/s10270-009-0132-5.pdf"
    },
    {
        "title": "Special section of BPMDS’2017: enabling business transformation by business process modeling, development and support",
        "submission-date": "2019/11",
        "publication-date": "2019/12",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a two-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of information systems theory. The goals, format and history of BPMDS can be found on the Web site http://www.bpmds.org/. This special section follows the 18th edition of the BPMDS (Business Process Modeling, Development and Support) series, organized in conjunction with CAiSE’17, which was held in Essen, Germany, June 2017. BPMDS’2017 received 24 submissions from 18 countries, and 11 papers were selected and published in Springer LNBIP 287 volume.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-019-00771-8.pdf"
    },
    {
        "title": "Safe reuse in modelling language engineering using model subtyping with OCL constraints",
        "submission-date": "2021/08",
        "publication-date": "2022/09",
        "abstract": "Low-code software development promises rapid delivery of software cloud applications by employing domain-speciﬁc lan-guages (DSLs), requiring minimal traditional coding. Model-driven engineering (MDE) provides tools, modelling notations and practices suited for engineering such DSLs, both from a syntactic and semantic perspective. However, low-code software development is heavily reliant on software reuse. It is imperative to provide safe mechanisms that guarantee valid semantic reuse of structural components and their behaviour, most often in a stepwise manner. This article presents a semantic reuse technique based on model subtyping over metamodels to manage correct model-driven engineering of DSLs. Model sub-typing is generalized to structural semantics by considering OCL constraints. Moreover, model subtyping is generalized to behavioural semantics by considering speciﬁcations of model transformation operations, which may encode operational or translational semantics. Model subtyping facilitates structural and behavioural reﬁnement. It has been implemented atop a bounded model checker, realizing a semi-decidable procedure for verifying that DSL elements are safely reused. The algo-rithm ﬁnds semantic witnesses of inconsistencies when reﬁnement principles are not satisﬁed, fostering a correct stepwise engineering of DSLs. Moreover, the algorithm produces an extension metamodel that permits the as-is reuse of implementa-tions of model transformation operation speciﬁcations. Finally, the versatility of the model subtyping technique is illustrated with common use cases extracted from the research literature.",
        "keywords": [
            "Model subtyping",
            "DSL engineering",
            "Software speciﬁcation",
            "Stepwise reﬁnement"
        ],
        "authors": [
            "Artur Boronat"
        ],
        "file_path": "data/sosym-all/s10270-022-01028-7.pdf"
    },
    {
        "title": "PSL: A semantic domain for ﬂow models",
        "submission-date": "2003/06",
        "publication-date": "2004/11",
        "abstract": "Flow models underlie popular programming languages and many graphical behavior speciﬁcation tools. However, their semantics is typically ambiguous, causing miscommunication between modelers and unexpected implementation results. This article introduces a way to disambiguate common ﬂow modeling constructs, by expressing their semantics as constraints on runtime sequences of behavior execution. It also shows that reduced ambiguity enables more powerful modeling abstractions, such as partial behavior speciﬁcations. The runtime representation considered in this paper uses the Process Speciﬁcation Language (PSL), which is deﬁned in ﬁrst-order logic, making it amenable to automated reasoning.",
        "keywords": [
            "Flow model",
            "Flow semantics",
            "PSL",
            "Process speciﬁcation",
            "Control ﬂow",
            "Data ﬂow",
            "Concurrency",
            "UML",
            "Activity model"
        ],
        "authors": [
            "Conrad Bock",
            "Michael Gruninger"
        ],
        "file_path": "data/sosym-all/s10270-004-0066-x.pdf"
    },
    {
        "title": "A feature-based classiﬁcation of formal veriﬁcation techniques for software models",
        "submission-date": "2016/07",
        "publication-date": "2017/03",
        "abstract": "Software models are the core development arti-\nfact in model-based engineering (MBE). The MBE paradigm\npromotes the use of software models to describe structure\nand behavior of the system under development and proposes\nthe automatic generation of executable code from the mod-\nels. Thus, defects in the models most likely propagate to\nexecutable code. To detect defects already at the modeling\nlevel, many approaches propose to use formal veriﬁcation\ntechniques to ensure the correctness of these models. These\napproaches are the subject of this survey. We review the state\nof the art of formal veriﬁcation techniques for software mod-\nels and provide a feature-based classiﬁcation that allows us\nto categorize and compare the different approaches.",
        "keywords": [
            "Model-based engineering",
            "Veriﬁcation",
            "Model\nchecking",
            "Theorem proving"
        ],
        "authors": [
            "Sebastian Gabmeyer",
            "Petra Kaufmann",
            "Martina Seidl",
            "Martin Gogolla",
            "Gerti Kappel"
        ],
        "file_path": "data/sosym-all/s10270-017-0591-z.pdf"
    },
    {
        "title": "Empirical analysis of the tool support for software product lines",
        "submission-date": "2020/04",
        "publication-date": "2022/06",
        "abstract": "For the last ten years, software product line (SPL) tool developers have been facing the implementation of different variability requirements and the support of SPL engineering activities demanded by emergent domains. Despite systematic literature reviews identifying the main characteristics of existing tools and the SPL activities they support, these reviews do not always help to understand if such tools provide what complex variability projects demand. This paper presents an empirical research in which we evaluate the degree of maturity of existing SPL tools focusing on their support of variability modeling characteristics and SPL engineering activities required by current application domains. We ﬁrst identify the characteristics and activities that are essential for the development of SPLs by analyzing a selected sample of case studies chosen from application domains with high variability. Second, we conduct an exploratory study to analyze whether the existing tools support those characteristics and activities. We conclude that, with the current tool support, it is possible to develop a basic SPL approach. But we have also found out that these tools present several limitations when dealing with complex variability requirements demanded by emergent application domains, such as non-Boolean features or large conﬁguration spaces. Additionally, we identify the necessity for an integrated approach with appropriate tool support to completely cover all the activities and phases of SPL engineering. To mitigate this problem, we propose different road map using the existing tools to partially or entirely support SPL engineering activities, from variability modeling to product derivation.",
        "keywords": [
            "Empirical analysis",
            "Case studies analysis",
            "Software product lines",
            "State of the art",
            "Tool support",
            "Tooling road map",
            "Variability modeling"
        ],
        "authors": [
            "José Miguel Horcas",
            "Mónica Pinto",
            "Lidia Fuentes"
        ],
        "file_path": "data/sosym-all/s10270-022-01011-2.pdf"
    },
    {
        "title": "Thirteen years of SysML: a systematic mapping study",
        "submission-date": "2018/03",
        "publication-date": "2019/05",
        "abstract": "The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research ﬁelds. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main ﬁndings: (i) there is a growing scientiﬁc interest in SysML in the last years particularly in the research ﬁeld of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-speciﬁc requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a speciﬁc UML proﬁle mostly used in systems engineering; however, the language has to be customized to accommodate domain-speciﬁc aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for ﬁnding speciﬁc approaches about SysML.",
        "keywords": [
            "SysML",
            "Systematic mapping study",
            "Systems engineering"
        ],
        "authors": [
            "Sabine Wolny",
            "Alexandra Mazak",
            "Christine Carpella",
            "Verena Geist",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-019-00735-y.pdf"
    },
    {
        "title": "HoloFlows: modelling of processes for the Internet of Things in mixed reality",
        "submission-date": "2019/11",
        "publication-date": "2021/01",
        "abstract": "Our everyday lives are increasingly pervaded by digital assistants and smart devices forming the Internet of Things (IoT). While user interfaces to directly monitor and control individual IoT devices are becoming more sophisticated and end-user friendly, applications to connect standalone IoT devices and create more complex IoT processes for automating and assisting users with repetitive tasks still require a high level of technical expertise and programming knowledge. Related approaches for process modelling in IoT mostly suggest extensions to complex modelling languages, require high levels of abstraction and technical knowledge, and rely on unintuitive tools. We present a novel approach for end-user oriented-no-code-IoT process modelling using Mixed Reality (MR) technology: HoloFlows. Users are able to explore the IoT environment and model processes among sensors and actuators as ﬁrst-class citizens by simply “drawing” virtual wires among physical IoT devices. MR technology hereby facilitates the understanding of the physical contexts and relations among the IoT devices and provides a new and more intuitive way of modelling IoT processes. The results of a user study comparing HoloFlows with classical modelling approaches show an increased user experience and decrease in required modelling knowledge and technical expertise to create IoT processes.",
        "keywords": [
            "Process modelling",
            "Mixed reality",
            "Internet of Things",
            "IoT processes",
            "End-user development"
        ],
        "authors": [
            "Ronny Seiger",
            "Romina Kühn",
            "Mandy Korzetz",
            "Uwe Aßmann"
        ],
        "file_path": "data/sosym-all/s10270-020-00859-6.pdf"
    },
    {
        "title": "Guest editorial to the special section on PoEM’2022",
        "submission-date": "2024/06",
        "publication-date": "2024/07",
        "abstract": "This guest editorial presents the papers contributing to the 15th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modelling (PoEM 2022). The best papers were selected for invitation for revision and signiﬁcant expansion. Five papers were ﬁnally accepted in the special section. Collectively, these papers provide an excellent representation of the state of the art of Enterprise Modelling in both research and practice.",
        "keywords": [
            "Enterprise modelling",
            "Meta models",
            "Conceptual modelling",
            "Modelling methods"
        ],
        "authors": [
            "Balbir S. Barn",
            "Kurt Sandkuhl",
            "Souvik Barat",
            "Tony Clark"
        ],
        "file_path": "data/sosym-all/s10270-024-01189-7.pdf"
    },
    {
        "title": "DataMock: An Agile Approach for Building Data Models from User Interface Mockups",
        "submission-date": "2015/12",
        "publication-date": "2017/02",
        "abstract": "In modern software development, much time is devoted and much attention is paid to the activity of data modeling and the translation of data models into databases. This has motivated the proposal of different approaches and tools to support this activity, such as semiautomatic approaches that generate data models from requirements artifacts using text analysis and sets of heuristics, among other techniques. However, these approaches still suffer from important limitations, including the lack of support for requirements traceability, the poor support for detecting and solving conﬂicts in domain-speciﬁc requirements, and the considerable effort required for manually checking the generated models. This paper introduces DataMock, an Agile approach that enables the iterative building of data models from requirements speciﬁcations, while supporting traceability and allowing inconsistencies detection in data requirements and speciﬁcations. The paper also describes how the approach effectively allows improving traceabil- ity and reducing errors and effort to build data models in comparison with traditional, state-of-the-art, data modeling approaches.",
        "keywords": [
            "Data modeling",
            "Agile methods",
            "Mockups",
            "Annotations",
            "Requirements engineering",
            "Requirements traceability",
            "Model-driven development"
        ],
        "authors": [
            "José Matías Rivero",
            "Julián Grigera",
            "Damiano Distante",
            "Francisco Montero",
            "Gustavo Rossi"
        ],
        "file_path": "data/sosym-all/s10270-017-0586-9.pdf"
    },
    {
        "title": "Extending UML with coordination contracts",
        "submission-date": "2004/09",
        "publication-date": "2005/09",
        "abstract": "Coordination contracts are a software analysis and design construct which enable separation between the stable components of a system and the rules which define the interactions of these components. This separation supports rapid evolution of rules without requiring modification to components. In this paper we show that contracts can be defined in UML, and we define an MDA-based development process which makes use of contracts.",
        "keywords": [
            "Coordination contracts",
            "UML",
            "MDA"
        ],
        "authors": [
            "K. Lano",
            "J. L. Fiadeiro"
        ],
        "file_path": "data/sosym-all/s10270-005-0095-0.pdf"
    },
    {
        "title": "Model transformation intents and their properties",
        "submission-date": "2013/09",
        "publication-date": "2014/07",
        "abstract": "The notion of model transformation intent is proposed to capture the purpose of a transformation. In this paper, a framework for the description of model transformation intents is defined, which includes, for instance, a description of properties a model transformation has to satisfy to qualify as a suitable realization of an intent. Several common model transformation intents are identified, and the framework is used to describe six of them in detail. A case study from the automotive industry is used to demonstrate the usefulness of the proposed framework for identifying crucial properties of model transformations with different intents and to illustrate the wide variety of model transformation intents that an industrial model-driven software development process typically encompasses.",
        "keywords": [
            "Model transformation",
            "Intent",
            "Property",
            "Verification",
            "Description framework"
        ],
        "authors": [
            "Levi Lúcio",
            "Moussa Amrani",
            "Juergen Dingel",
            "Leen Lambers",
            "Rick Salay",
            "Gehan M. K. Selim",
            "Eugene Syriani",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-014-0429-x.pdf"
    },
    {
        "title": "Towards clone detection in UML domain models",
        "submission-date": "2010/11",
        "publication-date": "2011/10",
        "abstract": "Code clones (i.e., duplicate fragments of code) have been studied for long, and there is strong evidence that they are a major source of software faults. Anecdotal evidence suggests that this phenomenon occurs similarly in models, suggesting that model clones are as detrimental to model quality as they are to code quality. However, programming language code and visual models have significant differences that make it difﬁcult to directly transfer notions and algorithms developed in the code clone arena to model clones. In this article, we develop and propose a definition of the notion of “model clone” based on the thorough analysis of practical scenarios. We propose a formal definition of model clones, specify a clone detection algorithm for UML domain models, and implement it prototypically. We investigate different similarity heuristics to be used in the algorithm, and report the performance of our approach. While we believe that our approach advances the state of the art significantly, it is restricted to UML models, its results leave room for improvements, and there is no validation by ﬁeld studies.",
        "keywords": [
            "Model clones",
            "Model management",
            "Model evolution",
            "Model maintenance",
            "Model similarity"
        ],
        "authors": [
            "Harald Störrle"
        ],
        "file_path": "data/sosym-all/s10270-011-0217-9.pdf"
    },
    {
        "title": "Modeling compliance speciﬁcations in linear temporal logic, event processing language and property speciﬁcation patterns: a controlled experiment on understandability",
        "submission-date": "2018/05",
        "publication-date": "2019/02",
        "abstract": "Mature veriﬁcation and monitoring approaches, such as complex event processing and model checking, can be applied for checking compliance speciﬁcations at design time and runtime. Little is known about the understandability of the different formal and technical languages associated with these approaches. This uncertainty regarding understandability might be a major obstacle for the broad practical adoption of those techniques. This article reports a controlled experiment with 215 participants on the understandability of modeling compliance speciﬁcations in representative modeling languages, namely linear temporal logic (LTL), the complex event processing-based event processing language (EPL) and property speciﬁcation patterns (PSP). The formalizations in PSP were overall more correct. That is, the pattern-based approach provides a higher level of understandability than EPL and LTL. More advanced users, however, seemingly are able to cope equally well with PSP and EPL in modeling compliance speciﬁcations.",
        "keywords": [
            "Controlled experiment",
            "Understandability",
            "Linear temporal logic",
            "Property speciﬁcation patterns",
            "Complex event processing",
            "Event processing language"
        ],
        "authors": [
            "Christoph Czepa\nAmirali Amiri\nEvangelos Ntentos\nUwe Zdun"
        ],
        "file_path": "data/sosym-all/s10270-019-00721-4.pdf"
    },
    {
        "title": "Model-driven engineering of SAP core data services - the BIGER2CDS modeling tool",
        "submission-date": "2024/10",
        "publication-date": "2025/08",
        "abstract": "This paper introduces bigER2CDS, a novel model-driven engineering approach and tool support for SAP Core Data Services (CDS). bigER2CDS addresses the need for a higher abstraction level in CDS development, enabling blended, i.e., textual and graphical modeling of CDS Views through a domain-speciﬁc modeling language. Based on web technologies and the Language Server Protocol (LSP), we realized a modeling tool for SAP CDS. Our tool supports the hybrid modeling of CDS and the import of existing SAP CDS view entities for analysis and development support. This model-driven approach aims to enable domain experts to develop CDS views, mitigating the need for extensive programming skills. We report on the development of the ER2CDS domain-speciﬁc language (DSL) and the implementation of the corresponding bigER2CDS modeling tool. Finally, bigER2CDS is evaluated in the form of a controlled experiment and a case study with domain experts and CDS developers. The results show a high usability score for our tool and a willingness by domain experts and CDS developers to use it. The tool can be freely downloaded from the VS Code marketplace: https://marketplace.visualstudio.com/items?itemName=BIGModelingTools.er2cds.",
        "keywords": [
            "Model-driven engineering",
            "SAP Core Data Services",
            "Domain-speciﬁc language",
            "CDS",
            "Modeling tool",
            "LSP",
            "Langium",
            "Sprotty"
        ],
        "authors": [
            "Gallus Huber",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-025-01320-2.pdf"
    },
    {
        "title": "Component-based veriﬁcation using incremental design and invariants",
        "submission-date": "2013/02",
        "publication-date": "2014/04",
        "abstract": "We propose invariant-based techniques for the efﬁcient veriﬁcation of safety and deadlock-freedom properties of component-based systems. Components and their interactions are described in the BIP language. Global invariants of composite components are obtained by combining local invariants of their constituent components with interaction invariants that take interactions into account. We study new techniques for computing interaction invariants. Some of these techniques are incremental, i.e., interaction invariants of a composite hierarchically structured component are computed by reusing invariants of its constituents. We formalize incremental construction of components in the BIP language as the process of building progressively complex components by adding interactions (synchronization constraints) to atomic components. We provide sufﬁcient conditions ensuring preservation of invariants when new interactions are added. When these conditions are not satisﬁed, we propose methods for generating new invariants in an incremental manner by reusing existing invariants from the constituents in the incremental construction. The reuse of existing invariants reduces considerably the overall veriﬁcation effort. The techniques have been implemented in the D-Finder toolset. Among the experiments conducted, we have been capable of verifying safety properties and deadlock-freedom of sub-systems of the functional level of the DALA autonomous robot. This work goes far beyond the capacity of existing monolithic veriﬁcation tools.",
        "keywords": [
            "Veriﬁcation method",
            "Invariant",
            "Component-based systems",
            "Incremental design",
            "Veriﬁcation tools",
            "Deadlock-freedom",
            "BIP"
        ],
        "authors": [
            "Saddek Bensalem",
            "Marius Bozga",
            "Axel Legay",
            "Thanh-Hung Nguyen",
            "Joseph Sifakis",
            "Rongjie Yan"
        ],
        "file_path": "data/sosym-all/s10270-014-0410-8.pdf"
    },
    {
        "title": "Guest editorial for EMMSAD’2020 special section",
        "submission-date": "2021/06",
        "publication-date": "2021/06",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) conference series organized 25 events from 1996 to 2020, associated with CAISE (Conference on Advanced Information Systems Engineering). From 2009, EMMSAD has become a two-day working conference. From 2017, EMMSAD best papers are invited to submit extended versions for considering their publication in the Journal of Software and Systems Modeling (SoSyM). The main topics of the EMMSAD series have the focus on models and modeling methods for software and information systems development.",
        "keywords": [],
        "authors": [
            "Iris Reinhartz‑Berger",
            "Jelena Zdravkovic"
        ],
        "file_path": "data/sosym-all/s10270-021-00903-z.pdf"
    },
    {
        "title": "Language-speciﬁc model checking of UML-RT models",
        "submission-date": "2014/09",
        "publication-date": "2015/07",
        "abstract": "Abstract Model-driven development (MDD) deals with\ncomplexities of modern software development by using mod-\nels. Their veriﬁcation is one of the opportunities of MDD,\nsince it can be performed in the early stages of the develop-\nment.TheprevailingtrendinveriﬁcationofMDDmodelshas\nbeen to translate them to an input language of one of the exist-\ning tools, most notably model checkers. Such an approach\nhas advantages; for instance, we can use tools that achieved\na higher level of maturity, including SPIN, NuSMV and Java\nPathFinder. However, the input languages of model checkers\nare typically not compatible with MDD models, which can\nmake the translations very complex and difﬁcult to maintain.\nMoreover,itismoredifﬁculttotakeadvantageofspeciﬁcfea-\ntures of the structure and semantics of models to, e.g., speed\nup analysis. In this paper, we depart from the translational\ntrendandpresent moredirect anddedicatedapproach. Weuse\nan MDD language, namely UML-RT (used in IBM Rational\nSoftware Architect RealTime Edition), and we introduce a\nveriﬁcation method built around its main features such as\nhierarchical structures, action code and asynchronous com-\nmunication. In our method we use a formalization tailored to\nUML-RT models. This enables very easy transformation of\nmodels, but also reduces the necessary translations of ver-\niﬁcation results and directly supports the most important\nfeatures of UML-RT. The proposed method includes an on-\nthe-ﬂy model checking algorithm based on the original CTL\nlabeling. This algorithm is further optimized to include lazy\ncomposition. In the paper, we present all necessary compo-\nnents of the checking algorithms. Additionally, we also show\ntheresultsofexperimentswithourimplementationusingsev-\neral UML-RT models and CTL formulas. The experiments\nprovide some evidence of the viability of a language-speciﬁc\nanalysis of MDD models and of the effectiveness of our opti-\nmizations in certain cases.",
        "keywords": [
            "UML-RT",
            "Model checking",
            "Lazy composition"
        ],
        "authors": [
            "Karolina Zurowska",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-015-0484-y.pdf"
    },
    {
        "title": "Heuristics for composite Web service decentralization",
        "submission-date": "2011/09",
        "publication-date": "2012/08",
        "abstract": "A composite service is usually speciﬁed by means of a process model that captures control-ﬂow and data-ﬂow relations between activities that are bound to underlying component services. In mainstream service orchestration platforms, this process model is executed by a centralized orchestrator through which all interactions are channeled. This architecture is not optimal in terms of communication overhead and has the usual problems of a single point of failure. In previous work, we proposed a method for executing composite services in a decentralized manner. However, this and similar methods for decentralized composite service execution do not optimize the communication overhead between the services participating in the composition. This paper studies the problem of optimizing the selection of services assigned to activities in a decentralized composite service, both in terms of communication overhead and overall quality of service, and taking into account collocation and separation constraints that may exist between activities in the composite service. This optimization problem is formulated as a quadratic assignment problem. The paper puts forward a greedy algorithm to compute an initial solution as well as a tabu search heuristic to identify improved solutions. An experimental evaluation shows that the tabu search heuristic achieves signiﬁcant improvements over the initial greedy solution. It is also shown that the greedy algorithm combined with the tabu search heuristic scale up to models of realistic size.",
        "keywords": [
            "Service composition",
            "Decentralized service execution",
            "Quality of service"
        ],
        "authors": [
            "Walid Fdhila",
            "Marlon Dumas",
            "Claude Godart",
            "Luciano García-Bañuelos"
        ],
        "file_path": "data/sosym-all/s10270-012-0262-z.pdf"
    },
    {
        "title": "Integrating business process simulation and information system simulation for performance prediction",
        "submission-date": "2014/02",
        "publication-date": "2015/03",
        "abstract": "Business process (BP) designs and enterprise information system (IS) designs are often not well aligned. Missing alignment may result in performance problems at run-time, such as large process execution time or overloaded IS resources. The complex interrelations between BPs and ISs are not adequately understood and considered in development so far. Simulation is a promising approach to predict performance of both BP and IS designs. Based on prediction results, design alternatives can be compared and verified against requirements. Thus, BP and IS designs can be aligned to improve performance. In current simulation approaches, BP simulation and IS simulation are not adequately integrated. This results in limited prediction accuracy due to neglected interrelations between the BP and the IS in simulation. In this paper, we present the novel approach Integrated Business IT Impact Simulation (IntBIIS) to adequately reflect the mutual impact between BPs and ISs in simulation. Three types of mutual impact between BPs and ISs in terms of performance are specified. We discuss several solution alternatives to predict the impact of a BP on the performance of ISs and vice versa. It is argued that an integrated simulation of BPs and ISs is best suited to reflect their interrelations. We propose novel concepts for continuous modeling and integrated simulation. IntBIIS is implemented by extending the Palladio tool chain with BP simulation concepts. In a real-life case study with a BP and IS from practice, we validate the feasibility of IntBIIS and discuss the practicability of the corresponding tool support.",
        "keywords": [
            "Business process",
            "Information system",
            "Alignment",
            "Performance"
        ],
        "authors": [
            "Robert Heinrich",
            "Philipp Merkle",
            "Jörg Henss",
            "Barbara Paech"
        ],
        "file_path": "data/sosym-all/s10270-015-0457-1.pdf"
    },
    {
        "title": "Modeling competences in enterprise architecture: from knowledge, skills, and attitudes to organizational capabilities",
        "submission-date": "2023/03",
        "publication-date": "2024/03",
        "abstract": "Competence-basedapproacheshavereceivedincreasedattention,asthedemandforqualiﬁedpeoplewiththerightcombination\nof competences establishes itself as a major factor of organizational performance. This paper examines how competences\ncan be incorporated into Enterprise Architecture modeling: (i) we identify a key set of competence-related concepts such as\nknowledge, skills, and attitudes, (ii) analyze and relate them using a reference ontology (grounded on the Uniﬁed Foundational\nOntology), and (iii) propose a representation strategy for modeling competences and their constituent elements leveraging\nthe ArchiMate language, discussing how the proposed models can ﬁt in enterprise competence-based practices. Our approach\nis intended to cover two tasks relevant to the combined application of Enterprise Architecture and Competence Modeling:\n‘zooming in’ on competences, revealing the relations between competences, knowledge, skills, attitudes and other personal\ncharacteristics that matter in organizational performance, and ‘zooming out’ of competences, placing them in the wider context\nof other personal competences and overall organizational capabilities. An assessment of the representation is offered in the\nform of an empirical survey.",
        "keywords": [
            "Competences",
            "Ontologies",
            "Competence Modeling",
            "Enterprise Architecture"
        ],
        "authors": [
            "Rodrigo F. Calhau",
            "João Paulo A. Almeida",
            "Satyanarayana Kokkula",
            "Giancarlo Guizzardi"
        ],
        "file_path": "data/sosym-all/s10270-024-01151-7.pdf"
    },
    {
        "title": "Graphical composite modeling and simulation for multi-aircraft collision avoidance",
        "submission-date": "2019/06",
        "publication-date": "2020/11",
        "abstract": "Modeling and simulation for multi-aircraft collision avoidance to understand the mechanistic behavior is an important activity. Building models using general programming language typically requires specialist knowledge, and this limits the spread of modeling and simulation approach among multi-aircraft collision avoidance scenario. Thus, a software environment is needed to support convenient development of models by assembling components, when analysis demands changes. In this work, the graphical composite modeling and simulation software (GMAS extended) for multi-aircraft collision avoidance is introduced, with the basic graphical components and a graphical assembly editor. We deﬁne the serial and parallel execution semantics of GMASE-based model and then introduce the high-level graphical modeling interface, the low-level runtime engine of GMAS, and the simulation-based decision tree, which transforms a complex decision-making process into a collection of simpler decisions of ﬁnding the no collision or optimal sequence from some initial state to the goal state. To validate its efﬁciency and practicability, a three-aircraft collision avoidance model with TCAS operations is built on GMAS, which shows that using GMAS increases reusability and hiding complexity in graphical programming by splitting complex behavior into data ﬂow and function components. The experimental result proves that GMAS not only provides a better representation for multi-aircraft collision avoidance, but also a useful approach for analyzing the potential collision occurrences.",
        "keywords": [
            "Graphical composite modeling",
            "Multi-aircraft collision avoidance",
            "Event-driven modeling and simulation",
            "Simulation-based decision tree"
        ],
        "authors": [
            "Feng Zhu",
            "Jun Tang"
        ],
        "file_path": "data/sosym-all/s10270-020-00830-5.pdf"
    },
    {
        "title": "A framework to support alignment of secure software engineering with legal regulations",
        "submission-date": "2009/01",
        "publication-date": "2010/03",
        "abstract": "Regulation compliance is getting more and more important for software systems that process and manage sensitive information. Therefore, identifying and analysing relevant legal regulations and aligning them with security requirements become necessary for the effective development of secure software systems. Nevertheless, Secure Software Engineering Modelling Languages (SSEML) use different concepts and terminology from those used in the legal domain for the description of legal regulations. This situation, together with the lack of appropriate background and knowledge of laws and regulations, introduces a challenge for software developers. In particular, it makes difficult to perform (i) the elicitation of appropriate security requirements from the relevant laws and regulations; and (ii) the correct tracing of the security requirements throughout the development stages. This paper presents a framework to support the consideration of laws and regulations during the development of secure software systems. In particular, the framework enables software developers (i) to correctly elicit security requirements from the appropriate laws and regulations; and (ii) to trace these requirements throughout the development stages in order to ensure that the design indeed supports the required laws and regulations. Our framework is based on existing work from the area of secure software engineering, and it complements this work with a novel and structured process and a well-defined method. A practical case study is employed to demonstrate the applicability of our work.",
        "keywords": [
            "Secure software engineering",
            "Non-functional properties",
            "Security requirements",
            "Secure Tropos",
            "UMLsec",
            "Modelling regulations",
            "Legal constraints"
        ],
        "authors": [
            "Shareeful Islam",
            "Haralambos Mouratidis",
            "Jan Jürjens"
        ],
        "file_path": "data/sosym-all/s10270-010-0154-z.pdf"
    },
    {
        "title": "Model-based safety assessment with SysML and component fault trees: application and lessons learned",
        "submission-date": "2019/07",
        "publication-date": "2020/02",
        "abstract": "Mastering the complexity of safety assurance for modern, software-intensive systems is challenging in several domains, such as automotive, robotics, and avionics. Model-based safety analysis techniques show promising results to handle this challenge by automating the generation of required artifacts for an assurance case. In this work, we adapt prominent approaches and propose to augment of SysML models with component fault trees (CFTs) to support the fault tree analysis and the failure mode and effects analysis. While most existing approaches based on CFTs are only targeting the system topology, e.g., UML class diagrams, we propose an integration of CFTs with SysML internal block diagrams as well as SysML activity diagrams. We realized our approach in a prototypical tool. We conclude with best practices and lessons learned that emerged from our case studies with an electronic power steering system and a boost recuperation system.",
        "keywords": [
            "Model-based systems engineering",
            "MBSE",
            "Model-based safety analysis",
            "MBSA",
            "Fault trees",
            "Fault tree analysis",
            "FTA",
            "Component fault tree",
            "CFT",
            "Failure mode and effects analysis",
            "FMEA",
            "Safety",
            "Reliability",
            "Dependability"
        ],
        "authors": [
            "Peter Munk",
            "Arne Nordmann"
        ],
        "file_path": "data/sosym-all/s10270-020-00782-w.pdf"
    },
    {
        "title": "An ontological metamodel for cyber-physical system safety, security, and resilience coengineering",
        "submission-date": "2020/06",
        "publication-date": "2021/06",
        "abstract": "Cyber-physical systems are complex systems that require the integration of diverse software, ﬁrmware, and hardware to be practical and useful. This increased complexity is impacting the management of models necessary for designing cyber-physical systems that are able to take into account a number of “-ilities”, such that they are safe and secure and ultimately resilient to disruption of service. We propose an ontological metamodel for system design that augments an already existing industry metamodel to capture the relationships between various model elements (requirements, interfaces, physical, and functional) and safety, security, and resilient considerations. Employing this metamodel leads to more cohesive and structured modeling efforts with an overall increase in scalability, usability, and uniﬁcation of already existing models. In turn, this leads to a mission-oriented perspective in designing security defenses and resilience mechanisms to combat undesirable behaviors. We illustrate this metamodel in an open-source GraphQL implementation, which can interface with a number of modeling languages. We support our proposed metamodel with a detailed demonstration using an oil and gas pipeline model.",
        "keywords": [],
        "authors": [
            "Georgios Bakirtzis",
            "Tim Sherburne",
            "Stephen Adams",
            "Barry M. Horowitz",
            "Peter A. Beling",
            "Cody H. Fleming"
        ],
        "file_path": "data/sosym-all/s10270-021-00892-z.pdf"
    },
    {
        "title": "Systematic literature review of the objectives, techniques, kinds, and architectures of models at runtime",
        "submission-date": "2013/04",
        "publication-date": "2013/12",
        "abstract": "In the context of software development, models provide an abstract representation of a software system or a part of it. In the software development process, they are primarily used for documentation and communication purposes in analysis, design, and implementation activities. Model-Driven Engineering (MDE) further increases the importance of models, as in MDE models are not only used for documentation and communication, but as central artefacts of the software development process. Various recent research approaches take the idea of using models as central artefacts one step further by using models at runtime to cope with dynamic aspects of ever-changing software and its environment. In this article, we analyze the usage of models at runtime in the existing research literature using the Systematic Literature Review (SLR) research method. The main goals of our SLR are building a common classiﬁcation and surveying the existing approaches in terms of objectives, techniques, architectures, and kinds of models used in these approaches. The contribution of this article is to provide an overview and classiﬁcation of current research approaches using models at runtime and to identify research areas not covered by models at runtime so far.",
        "keywords": [
            "Models",
            "Runtime",
            "Literature review"
        ],
        "authors": [
            "Michael Szvetits",
            "Uwe Zdun"
        ],
        "file_path": "data/sosym-all/s10270-013-0394-9.pdf"
    },
    {
        "title": "Model checking of spacecraft operational designs: a scalability analysis",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "Ensuring the correct and safe behavior of a spacecraft is a main objective in space-system design. Since spacecraft consist of highly complex and tightly integrated components developed by large teams of engineers from various different disciplines, this is a challenging task. Increasingly, formal veriﬁcation methods such as model checking are applied to establish the correctness of safety-critical parts or subsystems. Generally, the often limited scalability of model checking due to the state-space explosion problem hinders the wide-spread adoption of this technique. In this paper, we systematically examine the scalability of model checking for verifying behavioral models that arise within early space-system design phases. For this, we created a representative model for the mode management of a satellite that can be scaled in terms of its size and the complexity of interactions between system components. The model can be transformed into the input languages of various model-checking tools, which enables a comparative study of various model-checking algorithms and also facilitates analyzing the impact of different communication schemes on the scalability. The evaluation shows promising results regarding the applicability of model checking within the spacecraft design process.",
        "keywords": [
            "Aerospace",
            "Space systems",
            "State machines",
            "Model checking",
            "Scalability"
        ],
        "authors": [
            "Philipp Chrszon",
            "Paulina Maurer",
            "George Saleip",
            "Sascha Müller",
            "Philipp M. Fischer",
            "Andreas Gerndt",
            "Michael Felderer"
        ],
        "file_path": "data/sosym-all/s10270-025-01281-6.pdf"
    },
    {
        "title": "Imperative versus declarative constraint speciﬁcation languages: a controlled experiment",
        "submission-date": "2019/09",
        "publication-date": "2020/05",
        "abstract": "Model-based software engineering gains further attention these days. To better support it, the use of constraint languages is important in order to bridge expressiveness gaps and eliminate ambiguity. Nevertheless, the use of model-based constraint languages, like the Object Constraint Language (OCL), is quite limited and the speciﬁcation of constraints is left to the implementation stage. One option for these practices might be the misconception that model-based constraint languages are difficult to work with. In this paper, we examine the usages of representative constraint languages, namely OCL, for model-based constraint languages, and Java, for implementation-based constraint languages. In particular, we examine their usage in understanding and developing constraints. We evaluate these usages via a controlled experiment with 110 Information Systems Engineering undergraduate students. We found out that using OCL outperforms using Java for both understanding and developing constraints. Yet, the students had more conﬁdence with Java. The results indicate that the aforementioned misconception is wrong and there is a need for further education regarding model-based constraints languages, so to get more practice and conﬁdence.",
        "keywords": [
            "Modeling",
            "Constraint language",
            "OCL",
            "Java",
            "Evaluation",
            "Controlled experiment",
            "Imperative language",
            "Declarative language"
        ],
        "authors": [
            "Azzam Maraee",
            "Arnon Sturm"
        ],
        "file_path": "data/sosym-all/s10270-020-00796-4.pdf"
    },
    {
        "title": "A query-retyping approach to model transformation co-evolution",
        "submission-date": "2019/02",
        "publication-date": "2020/06",
        "abstract": "In rule-based approaches, a model transformation deﬁnition tells how an instance of a source metamodel should be transformed to an instance of a target metamodel. As these metamodels undergo changes, model transformations deﬁned over these metamodels may get out of sync. Restoring conformance between model transformations and the metamodels is a complex and error-prone task. In this paper, we propose a formal approach to automatically co-evolve model transformations according to the evolution of the metamodels. The approach is based on encoding the model transformation deﬁnition as a query-retyping combination and the evolution of the metamodels as applications of graph transformation rules. These rules are used to obtain an evolved query over the evolved metamodel together with a new retyping from the target metamodel. We will identify the criteria which need to be fulﬁlled in order to make this automatic co-evolution possible. We provide a tool support for this procedure, in which, from a traceability model that represents the original model transformation deﬁnition, we derive a co-evolved traceability model that represents the evolved transformation deﬁnition. Moreover, we use a case study to evaluate the approach with a set of commonly performed metamodel evolutions.",
        "keywords": [
            "MDE",
            "Migration",
            "Co-evolution",
            "Graph transformations"
        ],
        "authors": [
            "Adrian Rutle",
            "Ludovico Iovino",
            "Harald König",
            "Zinovy Diskin"
        ],
        "file_path": "data/sosym-all/s10270-020-00805-6.pdf"
    },
    {
        "title": "What practitioners really want: requirements for visual notations in conceptual modeling",
        "submission-date": "2017/09",
        "publication-date": "2018/02",
        "abstract": "This research was aimed at eliciting the requirements of practitioners who use conceptual modeling in their professional work for the visual notations of modeling languages. While the use of conceptual modeling in practice has been addressed, what practitioners in fact require of the visual notation of the modeling languages they use has received little attention. This work was thus motivated by the need to understand to what extent practitioners’ requirements are acknowledged and accommodated by visual notation research efforts. A mixed-method study was conducted, with a survey being offered over the course of several months to LinkedIn professional groups. The requirements included in the survey were based on a leading design theory for visual notations, the Physics of Notations. After preprocessing, 104 participant responses were analyzed. Data analysis included descriptive coding and qualitative analysis of purposes for modeling and additional requirements beyond the scope of visual design. Statistical and factorial analysis was used to explore potential correlations between the importance of different requirements as perceived by practitioners and the demographic factors (e.g., domain, purpose, topics). The results indicate several correlations between demographic factors and the perceived importance of visual notation requirements, as well as differences in the perceived relative importance of different requirements for models used to communicate with modeling experts as compared to non-experts. Furthermore, the results show an evolution from trends identiﬁed in studies conducted in the previous decade. The identiﬁed correlations with practitioners’ demographics reveal several research challenges that should be addressed, as well as the potential beneﬁts of more purpose-speciﬁc tailoring of visual notation design. Furthermore, the shift in practitioner demographics as compared to those found in earlier work indicates that the research and development of conceptual modeling efforts needs to stay up-to-date with the way practitioners employ conceptual modeling.",
        "keywords": [
            "Visual notations",
            "Requirements",
            "Conceptual modeling"
        ],
        "authors": [
            "Dirk van der Linden",
            "Irit Hadar",
            "Anna Zamansky"
        ],
        "file_path": "data/sosym-all/s10270-018-0667-4.pdf"
    },
    {
        "title": "A toolkit for model manipulation",
        "submission-date": "2002/02",
        "publication-date": "2003/10",
        "abstract": "We present a toolkit to develop scripts to process software models. It can be used to create applications to check, transform and generate derived artifacts from a model. The toolkit is based on the current OMG standards and it can be used with the Uniﬁed Modeling Language (UML) and other user-deﬁned languages based on the Meta Object Facility.",
        "keywords": [
            "UML",
            "Transformation",
            "Metamodeling",
            "Tools",
            "Scripting"
        ],
        "authors": [
            "Ivan Porres"
        ],
        "file_path": "data/sosym-all/s10270-003-0034-x.pdf"
    },
    {
        "title": "Contract-based modeling and veriﬁcation of timed safety requirements within SysML",
        "submission-date": "2014/07",
        "publication-date": "2015/07",
        "abstract": "In order to cope with the growing complexity of critical real-time embedded systems, systems engineering has adopted a component-based design technique driven by requirements. Yet, such an approach raises several issues since it does not explicitly prescribe how system requirements can be decomposed on components nor how components contribute to the satisfaction of requirements. The envisioned solution is to design, with respect to each requirement and for each involved component, an abstract speciﬁcation, tractable at each design step, that models how the component is concerned by the satisfaction of the requirement and that can be further reﬁned toward a correct implementation. In this paper, we consider such speciﬁcations in the form of contracts. A contract for a component consists in a pair (assumption, guarantee) where the assumption models an abstract behavior of the component’s environment and the guarantee models an abstract behavior of the component given that the environment behaves according to the assumption. Therefore, contracts are a valuable asset for the correct design of systems, but also for mapping and tracing requirements to components, for tracing the evolution of requirements during design and, most importantly, for compositional veriﬁcation of requirements. The aim of this paper is to introduce contract-based reasoning for the design of critical real-time systems made of reactive components modeled with UML and/or SysML. We propose an extension of UML and SysML languages with a syntax and semantics for contracts and the reﬁnement relations that they must satisfy. The semantics of components and contracts is formalized by a variant of timed input/output automata on top of which we build a formal contract-based theory. We prove that the contract-based theory is sound and can be applied for a relatively large class of SysML system models. Finally, we show on a case study extracted from the automated transfer vehicle (http://www.esa.int/ATV) that our contract-based theory allows to verify requirement satisfaction for previously intractable models.",
        "keywords": [
            "Contract-based reasoning",
            "Safety requirement",
            "Component-based design",
            "UML/SysML",
            "Compositional veriﬁcation",
            "Timed input/output automata"
        ],
        "authors": [
            "Iulia Dragomir",
            "Iulian Ober",
            "Christian Percebois"
        ],
        "file_path": "data/sosym-all/s10270-015-0481-1.pdf"
    },
    {
        "title": "SoSyM reﬂections of 2017: a journal status report",
        "submission-date": "2018/01",
        "publication-date": "2018/01",
        "abstract": "2017 has been an exciting year for SoSyM, which has now passed its 16th year of publication! We have continued to intensify our collaboration with the MODELS conference by presenting the annual SoSyM Awards at MODELS and strengthening the SoSyM “Journal-First” arrangement with MODELS. The ﬁrst issue of this new year’s volume summarizes the status of SoSyM in terms of recent statistics and milestone events over the past year. As an additional testament to the interest and growth in our ﬁeld, recent full professorship positions have been offered in the area of software and systems modeling, which means the area has now advanced from a subdomain of software engineering, data modeling, formal methods, and related areas to its own sustainable research domain.",
        "keywords": [],
        "authors": [
            "Geri Georg",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-018-0656-7.pdf"
    },
    {
        "title": "Precise visual modeling: A case-study",
        "submission-date": "2003/12",
        "publication-date": "2005/04",
        "abstract": "We develop an abstract model for our case-study: software to support a “video rental service.” This illustrates how a visual formalism, constraint diagrams, may be used in order to specify such systems precisely.",
        "keywords": [
            "Constraint diagrams",
            "Formal methods"
        ],
        "authors": [
            "John Howse",
            "Steve Schuman"
        ],
        "file_path": "data/sosym-all/s10270-004-0074-x.pdf"
    },
    {
        "title": "Gamiﬁcation of conceptual modeling education with UML class diagrams: an experimental analysis",
        "submission-date": "2024/07",
        "publication-date": "Not found",
        "abstract": "UML has become, throughout the years, the most popular modeling language for the conceptual design of software. However,\nUML diagrams are frequently ﬂawed with semantic and syntactical errors. One of the main root causes for such issues can\nbe traced back to software modeling education in software engineering curricula, which is typically given less attention\nthan core development activities. The objective of this manuscript is to describe the application of gamiﬁcation (i.e., the\nuse of game-related mechanics in non-gameful contexts) to increase the motivation and engagement of Master’s students in\nlearning the core concepts of UML modeling. Our tool prototype includes typical gamiﬁcation mechanics such as avatars,\nachievements, scoring mechanisms, and leaderboards and incorporates a system for automatic validation of the correctness of\nthe student’s solution. We empirically evaluated the beneﬁts achieved through the tool by performing a controlled experiment\nwith 280 Master’s students. We found that the use of gamiﬁcation signiﬁcantly increased the student commitment to perform\nexercises, the completeness of the exercises, and the semantic quality of the produced diagrams. Through standard usability\nquestionnaires, we also gathered positive responses and attitudes toward the usage of the tool.",
        "keywords": [
            "Gamiﬁcation",
            "UML modeling",
            "Teaching",
            "Information systems"
        ],
        "authors": [
            "Giacomo Garaccione\nRiccardo Coppola\nLuca Ardito\nMarco Torchiano"
        ],
        "file_path": "data/sosym-all/s10270-025-01282-5.pdf"
    },
    {
        "title": "Recent and simple algorithms for Petri nets",
        "submission-date": "2014/01",
        "publication-date": "2014/07",
        "abstract": "We show how inductive invariants can be used to solve coverability, boundedness and reachability problems for Petri nets. This approach provides algorithms that are conceptually simpler than previously published ones.",
        "keywords": [
            "Petri nets",
            "Veriﬁcation of reachability properties",
            "Simple algorithms"
        ],
        "authors": [
            "Alain Finkel",
            "Jérôme Leroux"
        ],
        "file_path": "data/sosym-all/s10270-014-0426-0.pdf"
    },
    {
        "title": "Leveraging annotation-based modeling with JUMP",
        "submission-date": "2015/05",
        "publication-date": "2016/05",
        "abstract": "The capability of UML proﬁles to serve as anno-tation mechanism has been recognized in both research and industry. Today’s modeling tools offer proﬁles speciﬁc to platforms, such as Java, as they facilitate model-based engineering approaches. However, considering the large number of possible annotations in Java, manually develop-ing the corresponding proﬁles would only be achievable by huge development and maintenance efforts. Thus, leveraging annotation-based modeling requires an automated approach capable of generating platform-speciﬁc proﬁles from Java libraries. To address this challenge, we present the fully automated transformation chain realized by Jump, thereby continuing existing mapping efforts between Java and UML by emphasizing on annotations and proﬁles. The evaluation of Jump shows that it scales for large Java libraries and gen-erates proﬁles of equal or even improved quality compared to proﬁles currently used in practice. Furthermore, we demon-strate the practical value of Jump by contributing proﬁles that facilitate reverse engineering and forward engineering processes for the Java platform by applying it to a modern-ization scenario.",
        "keywords": [
            "Java annotations",
            "UML proﬁles",
            "Model-based software engineering",
            "Forward engineering",
            "Reverse engineering"
        ],
        "authors": [
            "Alexander Bergmayr",
            "Michael Grossniklaus",
            "Manuel Wimmer",
            "Gerti Kappel"
        ],
        "file_path": "data/sosym-all/s10270-016-0528-y.pdf"
    },
    {
        "title": "Model-driven development of asynchronous message-driven architectures with AsyncAPI",
        "submission-date": "2020/10",
        "publication-date": "2021/12",
        "abstract": "IntheInternet-of-Things(IoT)vision,everydayobjectsevolveintocyber-physicalsystems.Themassiveuseanddeploymentof\nthese systems has given place to the Industry 4.0 or Industrial IoT (IIoT). Due to its scalability requirements, IIoT architectures\nare typically distributed and asynchronous. In this scenario, one of the most widely used paradigms is publish/subscribe,\nwhere messages are sent and received based on a set of categories or topics. However, these architectures face interoperability\nchallenges. Consistency in message categories and structure is the key to avoid potential losses of information. Ensuring this\nconsistency requires complex data processing logic both on the publisher and the subscriber sides. In this paper, we present\nour proposal relying on AsyncAPI to automate the design and implementation of these asynchronous architectures using\nmodel-driven techniques for the generation of (part of) message-driven infrastructures. Our proposal offers two different\nways of designing the architectures: either graphically, by modeling and annotating the messages that are sent among the\ndifferent IoT devices, or textually, by implementing an editor compliant with the AsyncAPI speciﬁcation. We have evaluated\nour proposal by conducting a set of experiments with 25 subjects with different expertise and background. The experiments\nshow that one-third of the subjects were able to design and implement a working architecture in less than an hour without\nprevious knowledge of our proposal, and an additional one-third estimated that they would only need less than two hours in\ntotal.",
        "keywords": [
            "Publish/subscribe",
            "Cyber-physical systems (CPS)",
            "Message-driven architectures",
            "Asynchronous communication",
            "AsyncAPI",
            "Industrial Internet of Things (IIoT)"
        ],
        "authors": [
            "Abel Gómez",
            "Markel Iglesias-Urkia",
            "Lorea Belategi",
            "Xabier Mendialdua",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-021-00945-3.pdf"
    },
    {
        "title": "Modeling customer-centric value of system architecture investments",
        "submission-date": "2010/11",
        "publication-date": "2012/02",
        "abstract": "System architecture investments aim at improving the quality of the system in alignment with (current and future) business goals. While the costs of architecture changes are routinely calculated, identifying beneﬁts of architecture changes and translating them to a monetary value has been a challenge in practice. Currently, architecture value estimation is largely based on cost-savings or on risk mitigation, without much reliance on potential customer beneﬁts. This article reports on our experience in modeling the customer value and evaluating its potential use in choosing between different system architectures in two case studies conducted in an organization developing healthcare systems. To model the customer value, we exploit best practices in management and marketing. Management tools, in particular strategy maps and balanced scorecards, are used to identify customer-centric beneﬁts caused by architecture design decisions. Furthermore, two marketing concepts, customer value-in-use and customer segments, are adopted to quantify the value of architecture changes for a single customer and multiple customers, respectively. The paper shows that using the customer value in addition to the existing value indicators in the organization has several advantages but also calls for future improvements to be adopted in practice.",
        "keywords": [
            "Customer value",
            "Architecture investments",
            "Software architecture",
            "Decision making",
            "Customer value-in-use",
            "Customer segments",
            "Value-based software engineering",
            "Strategy maps",
            "Balanced scorecards",
            "Case study"
        ],
        "authors": [
            "Ana Ivanovi´c",
            "Pierre America",
            "Chris Snijders"
        ],
        "file_path": "data/sosym-all/s10270-012-0235-2.pdf"
    },
    {
        "title": "Editorial for FACS 2021 special section (SoSyM)",
        "submission-date": "2023/01",
        "publication-date": "2023/02",
        "abstract": "This special section of the International Journal on Software and Systems Modeling (SoSyM) contains a selection of the best papers accepted at the 17th edition of the international conference series on Formal Aspects of Component Software (FACS). This conference was held online on October 28–29, 2021. The aim of the conference series is to study how formal methods can be applied to component-based software and system development. Component-based software development proposes sound engineering principles and techniques to cope with the complexity of modern software systems. Many challenging conceptual and technological issues remain in component-based software development theory and practice. Furthermore, the advent of service-oriented and cloud computing, cyber-physical systems, and the Internet of Things comes with new challenges, such as quality of service and robustness to withstand faults, which require revisiting established concepts and developing new ones. Formal methods have provided foundations for component-based software through research on mathematical models for components, composition and adaptation, and rigorous approaches to veriﬁcation, deployment, testing, and certiﬁcation. After the FACS’21 conference took place, the ﬁve best papers were selected, and the authors of these papers were invited to submit a revised and extended version of their work to this special section. After a meticulous review process, we ﬁnally accepted three of the ﬁve invited papers.",
        "keywords": [],
        "authors": [
            "Gwen Salaün"
        ],
        "file_path": "data/sosym-all/s10270-023-01088-3.pdf"
    },
    {
        "title": "A model framework-based domain-speciﬁc composable modeling method for combat system effectiveness simulation",
        "submission-date": "2014/09",
        "publication-date": "2016/01",
        "abstract": "Combat system effectiveness simulation (CoSES) plays an irreplaceable role in the effectiveness measurement of combat systems. According to decades of research and practice, composable modeling and multi-domain modeling are recognized as two major modeling requirements in CoSES. Current effectiveness simulation researches attempt to cope with the structural and behavioral complexity of CoSES based on a uniﬁed technological space, and they are limited to their existing modeling paradigms and fail to meet these two requirements. In this work, we propose a model framework-based domain-speciﬁc composable modeling method to solve this problem. This method builds a common model framework using application invariant knowledge for CoSES, and designs domain-speciﬁc modeling infrastructures for subdomains as corresponding extension points of the framework to support the modeling of application variant knowledge. Therefore, this method supports domain-speciﬁc modeling in multiple subdomains and the composition of subsystem models across different subdomains based on the model framework. The case study shows that this method raises the modeling abstraction level, supports generative modeling, and promotes model reuse and composability.",
        "keywords": [
            "Modeling and simulation",
            "Composable modeling",
            "Domain-speciﬁc modeling",
            "Simulation model framework",
            "System effectiveness simulation"
        ],
        "authors": [
            "Xiao-bo Li",
            "Feng Yang",
            "Yong-lin Lei",
            "Wei-ping Wang",
            "Yi-fan Zhu"
        ],
        "file_path": "data/sosym-all/s10270-015-0513-x.pdf"
    },
    {
        "title": "Dual deep modeling: multi-level modeling with dual potencies and its formalization in F-Logic",
        "submission-date": "2015/02",
        "publication-date": "2016/04",
        "abstract": "An enterprise database contains a global, integrated, and consistent representation of a company’s data. Multi-level modeling facilitates the deﬁnition and maintenance of such an integrated conceptual data model in a dynamic environment of changing data requirements of diverse applications. Multi-level models transcend the traditional separation of class and object with clabjects as the central modeling primitive, which allows for a more ﬂexible and natural representation of many real-world use cases. In deep instantiation, the number of instantiation levels of a clabject or property is indicated by a single potency. Dual deepmodeling(DDM)differentiatesbetweensourcepotency and target potency of a property or association and supports the ﬂexible instantiation and reﬁnement of the property by statements connecting clabjects at different modeling lev-els. DDM comes with multiple generalization of clabjects, subsetting/specialization of properties, and multi-level cardinalityconstraints.ExamplesarepresentedusingaUML-style notation for DDM together with UML class and object diagrams for the representation of two-level user views derived from the multi-level model. Syntax and semantics of DDM are formalized and implemented in F-Logic, supporting the modeler with integrity checks and rich query facilities.",
        "keywords": [
            "Database modeling",
            "Deep instantiation",
            "Clabject",
            "UML",
            "Deep modeling notation"
        ],
        "authors": [
            "Bernd Neumayr",
            "Christoph G. Schuetz",
            "Manfred A. Jeusfeld",
            "Michael Schreﬂ"
        ],
        "file_path": "data/sosym-all/s10270-016-0519-z.pdf"
    },
    {
        "title": "Guest editorial to the Theme Section on enterprise modelling",
        "submission-date": "2013/03",
        "publication-date": "2013/03",
        "abstract": "Modern organizations rely on complex configurations of distributed IT systems that implement key business processes, provide databases, data warehousing, and business intelligence. The current business environment requires organizations to comply with a range of externally defined regulations such as Sarbanes-Oxley and BASEL II. Organizations need to be increasingly agile, robust, and be able to react to complex events, possibly in terms of dynamic reconfiguration. In order to satisfy these complex requirements, large organizations are increasingly using enterprise modelling (EM) technologies to analyse their business units, processes, resources and IT systems, and to show how these elements satisfy the goals of the business. EM describes all aspects of the construction and analysis of organizational models and supports enterprise use cases including: Business alignment where elements of a business are shown to meet its goals and in particular establishing that the IT systems and processes that run the business are consistent with the goals set by the Chief Executive. Business change management where as-is and to-be models are used to plan how a business is to be changed based on a precise definition of business component dependencies.",
        "keywords": [],
        "authors": [
            "Tony Clark",
            "Florian Matthes",
            "Balbir Barn",
            "Alan Brown"
        ],
        "file_path": "data/sosym-all/s10270-013-0327-7.pdf"
    },
    {
        "title": "Establishing interoperability between EMF and MSDKVS: an M3-level-bridge to transform metamodels and models",
        "submission-date": "2023/03",
        "publication-date": "2024/04",
        "abstract": "Many powerful metamodeling platforms enabling model-driven software engineering (MDSE) exist, each with its strengths, weaknesses, functionalities, programming language(s), and developer community. Platform interoperability would enable users to exploit their mutual beneﬁts. Such interoperability would allow the transformation of metamodels and models created in one platform into equivalent metamodels and models in other platforms. Language engineers could then freely choose the metamodeling platform without risking a lock-in effect. Two well-documented and publicly available metamodeling platforms are the eclipse modeling framework (EMF) and the modeling SDK for visual studio (MSDKVS). In this paper, we propose an M3-level-bridge (M3B) that establishes interoperability between EMF and MSDKVS on the abstract syntax level and on the graphical concrete syntax level. To establish such interoperability we (i) compare the two platforms, (ii) present a conceptual mapping between them, and (iii) implement a bidirectional transformation bridge including both the metamodel and model layer. We evaluate our approach by transforming a collection of publicly available metamodels and automatically generated or manually created models thereof. The transformation outcomes are then used to quantitatively and qualitatively evaluate the transformation’s validity, executability, and expressiveness.",
        "keywords": [
            "MSDKVS",
            "EMF",
            "Metamodeling",
            "Model transformation",
            "MDSE",
            "Sirius",
            "Graphical concrete syntax",
            "Abstract syntax",
            "M3B",
            "DSL"
        ],
        "authors": [
            "Florian Cesal",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-024-01169-x.pdf"
    },
    {
        "title": "Requirements for modelling tools for teaching",
        "submission-date": "2024/03",
        "publication-date": "2024/07",
        "abstract": "Modelling is an important activity in software development and it is essential that students learn the relevant skills. Modelling relies on dedicated tools and these can be complex to install, conﬁgure, and use—distracting students from learning key modelling concepts and creating accidental complexity for teachers. To address these challenges, we believe that modelling tools speciﬁcally aimed at use in teaching are required. Based on discussions at a working session organised at MODELS 2023 and the results from an internationally shared questionnaire, we report on requirements for such modelling tools for teaching. We also present examples of existing modelling tools for teaching and how they address some of the requirements identiﬁed.",
        "keywords": [
            "Modelling",
            "Education",
            "Tools",
            "Requirements"
        ],
        "authors": [
            "Jörg Kienzle",
            "Steﬀen Zschaler",
            "William Barnett",
            "Timur Sa˘glam",
            "Antonio Bucchiarone",
            "Silvia Abrahão",
            "Eugene Syriani",
            "Dimitris Kolovos",
            "Timothy Lethbridge",
            "Sadaf Mustaﬁz",
            "Soﬁa Meacham"
        ],
        "file_path": "data/sosym-all/s10270-024-01192-y.pdf"
    },
    {
        "title": "Exploiting practical limitations of UML diagrams for model validation and execution",
        "submission-date": "2004/12",
        "publication-date": "2005/06",
        "abstract": "We suggest a framework for UML diagram validation and execution that takes advantage of some of the practical restrictions induced by diagrammatic representations (as compared to Turing equivalent programming languages) by exploiting possible gains in decidability. In particular, within our framework we can prove that an object interaction comes to an end, or that one action is always performed before another. Even more appealingly, we can compute efficiently whether two models are equivalent (aiding in the redesign or refactoring of a model), and what the differences between two models are. The framework employs a simple modelling object language (called MOL) for which we present formal syntax and semantics. A first generation of tools has been implemented that allows us to collect experience with our approach, guiding its further development.",
        "keywords": [
            "Modelling",
            "Modelling language",
            "Validation of models",
            "UML"
        ],
        "authors": [
            "Friedrich Steimann",
            "Heribert Vollmer"
        ],
        "file_path": "data/sosym-all/s10270-005-0097-y.pdf"
    },
    {
        "title": "Metamodel specialization for graphical language support",
        "submission-date": "2017/03",
        "publication-date": "2018/03",
        "abstract": "Most of current modeling languages are based on graphical diagrams. The concrete graphical syntax of these languages typically is deﬁned informally—by text and diagram examples. Only recently, starting from UML 2.5, a formalism is offered for deﬁning the graphical syntax of UML. This formalism is based on Diagram Deﬁnition standard by OMG, where the main emphasis is on enabling diagram interchange between different tools implementing the given language. While this is crucial for standardized languages such as UML, this aspect is not so important for domain-speciﬁc languages. In this paper, an approach is offered for a simple direct deﬁnition of concrete graphical syntax by means of metamodels. Metamodels are typically used for a language deﬁnition, but mainly the MOF-inspired approach via meta-metamodel instantiation is used. We offer an alternative approach based on core metamodel specialization which leads to a more direct and understandable deﬁnition staying at the same meta-layer. In addition, our approach permits a natural extension—facility for a graphical editor deﬁnition for the given language, which is vital in the world of DSLs. In contrast to most DSL development platforms, which are based on the abstract syntax metamodel of the language and a mapping to graphics, our facility is based directly on the graphical syntax. But we show that in those cases where the relation to the DSL abstract syntax is really required, a mapping from the graphical syntax to abstract syntax can be relatively easily deﬁned by the specialization approach.",
        "keywords": [
            "Metamodeling",
            "Metamodel specialization",
            "Graphical syntax deﬁnition",
            "Graphical DSL",
            "Graphical editors"
        ],
        "authors": [
            "Audris Kalnins",
            "Janis Barzdins"
        ],
        "file_path": "data/sosym-all/s10270-018-0668-3.pdf"
    },
    {
        "title": "Model transformations for migrating legacy deployment models in the automotive industry",
        "submission-date": "2012/11",
        "publication-date": "2013/07",
        "abstract": "Many companies in the automotive industry have adopted model-driven development in their vehicle software development. As a major automotive company, General Motors (GM) has been using a custom-built, domain-specific modeling language, implemented as an internal proprietary metamodel, to meet the modeling needs in its control software development. Since AUTomotive Open System ARchitecture (AUTOSAR) has been developed as a standard to ease the process of integrating components provided by different suppliers and manufacturers, there has been a growing demand to migrate these GM-specific, legacy models to AUTOSAR models. Given that AUTOSAR defines its own metamodel for various system artifacts in automotive software development, we explore applying model transformations to address the challenges in migrating GM-specific, legacy models to their AUTOSAR equivalents. As a case study, we have built and validated a model transformation using the MDWorkbench tool, the Atlas Transformation Language, and the Metamodel Coverage Checker tool. This paper reports on the case study, makes observations based on our experience to assist in the development of similar types of transformations, and provides recommendations for further research.",
        "keywords": [
            "Model-driven development (MDD)",
            "Model transformations",
            "AUTOSAR",
            "Transformation languages and tools",
            "Automotive control software",
            "Black-box testing"
        ],
        "authors": [
            "Gehan M. K. Selim",
            "Shige Wang",
            "James R. Cordy",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-013-0365-1.pdf"
    },
    {
        "title": "A semi-formal description of migrating domain-speciﬁc models with evolving domains",
        "submission-date": "2011/03",
        "publication-date": "2013/01",
        "abstract": "One of the main advantages of deﬁning a domain-speciﬁc modeling language (DSML) is the ﬂexibility to adjust the language deﬁnition to changing requirements or in response to a deeper understanding of the domain. With the industrial applications of domain-speciﬁc modeling environments, models are valuable investments. If the modeling language evolves, these models must be seamlessly migrated to the evolved DSML. Although the changes stemming from the language evolution are not abrupt in nature, migrating existing models to a new language is still a challenging task. Our solution is the Model Change Language (MCL) tool set, which deﬁnes a DSML to describe the migration rules and then performs the model migration automatically. In this paper, we describe the precise semantics of MCL and its execution, along with the conﬂuence of the migration.",
        "keywords": [],
        "authors": [
            "Tihamer Levendovszky",
            "Daniel Balasubramanian",
            "Anantha Narayanan",
            "Feng Shi",
            "Chris van Buskirk",
            "Gabor Karsai"
        ],
        "file_path": "data/sosym-all/s10270-012-0313-5.pdf"
    },
    {
        "title": "How do humans inspect BPMN models: an exploratory study",
        "submission-date": "2015/10",
        "publication-date": "2016/10",
        "abstract": "Even though considerable progress regarding the technical perspective on modeling and supporting business processes has been achieved, it appears that the human perspective is still often left aside. In particular, we do not have an in-depth understanding of how process models are inspected by humans, what strategies are taken, what challenges arise, and what cognitive processes are involved. This paper contributes toward such an understanding and reports an exploratory study investigating how humans identify and classify quality issues in BPMN process models. Providing preliminary answers to initial research questions, we also indicate other research questions that can be investigated using this approach. Our qualitative analysis shows that humans adapt different strategies on how to identify quality issues. In addition, we observed several challenges appearing when humans inspect process models. Finally, we present different manners in which classification of quality issues was addressed.",
        "keywords": [
            "Process model quality",
            "Process model maintainability",
            "Empirical research",
            "Human-centered support"
        ],
        "authors": [
            "Cornelia Haisjackl",
            "Pnina Soffer",
            "Shao Yi Lim",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-016-0563-8.pdf"
    },
    {
        "title": "Model engineering",
        "submission-date": "2003/07",
        "publication-date": "2003/07",
        "abstract": "The heightened awareness of the beneﬁts that can be derived from model-driven software and system development approaches is evident in discussions that take place in academic conferences, workshops, and industry meetings. On the other hand, there is a signiﬁcant number of developers and researchers who question the feasibility and beneﬁts of model-driven approaches. Model-based software description techniques use models, expressed in a formal language, to describe the architecture of a system, and the behavior of software artifacts.",
        "keywords": [],
        "authors": [],
        "file_path": "data/sosym-all/s10270-003-0025-y.pdf"
    },
    {
        "title": "Model execution tracing: a systematic mapping study",
        "submission-date": "2018/09",
        "publication-date": "2019/02",
        "abstract": "Model-Driven Engineering is a development paradigm that uses models instead of code as primary development artifacts. In\nthis paper, we focus on executable models, which are used to abstract the behavior of systems for the purpose of verifying and\nvalidating (V&V) a system’s properties. Model execution tracing (i.e., obtaining and analyzing traces of model executions)\nis an important enabler for many V&V techniques including testing, model checking, and system comprehension. This may\nexplain the increase in the number of proposed approaches on tracing model executions in the last years. Despite the increased\nattention, there is currently no clear understanding of the state of the art in this research ﬁeld, making it difﬁcult to identify\nresearch gaps and opportunities. The goal of this paper is to survey and classify existing work on model execution tracing, and\nidentify promising future research directions. To achieve this, we conducted a systematic mapping study where we examined\n64 primary studies out of 645 found publications. We found that the majority of model execution tracing approaches has been\ndeveloped for the purpose of testing and dynamic analysis. Furthermore, most approaches target speciﬁc modeling languages\nand rely on custom trace representation formats, hindering the synergy among tools and exchange of data. This study also\nrevealed that most existing approaches were not validated empirically, raising doubts as to their effectiveness in practice.\nOur results suggest that future research should focus on developing a common trace exchange format for traces, designing\nscalable trace representations, as well as conducting empirical studies to assess the effectiveness of proposed approaches.",
        "keywords": [
            "Model-driven engineering",
            "Executable models",
            "Model execution tracing",
            "Dynamic analysis of model-driven\nsystems",
            "Systematic mapping study"
        ],
        "authors": [
            "Fazilat Hojaji",
            "Tanja Mayerhofer",
            "Bahman Zamani",
            "Abdelwahab Hamou-Lhadj",
            "Erwan Bousse"
        ],
        "file_path": "data/sosym-all/s10270-019-00724-1.pdf"
    },
    {
        "title": "From enterprise architecture to business models and back",
        "submission-date": "2011/11",
        "publication-date": "2012/12",
        "abstract": "In this study, we argue that important IT change processes affecting an organization’s enterprise architecture are also mirrored by a change in the organization’s business model. An analysis of the business model may establish whether the architecture change has value for the business. Therefore, in order to facilitate such analyses, we propose an approach to relate enterprise models speciﬁed in ArchiMate to business models, modeled using Osterwalder’s Business Model Canvas. Our approach is accompanied by a method that supports business model-driven migration from a baseline architecture to a target architecture and is demonstrated by means of a case study.",
        "keywords": [
            "Business modeling",
            "Enterprise architecture",
            "Business Model Canvas",
            "ArchiMate",
            "Business Model Ontology",
            "Cost/revenue analysis"
        ],
        "authors": [
            "M. E. Iacob",
            "L. O. Meertens",
            "H. Jonkers",
            "D. A. C. Quartel",
            "L. J. M. Nieuwenhuis",
            "M. J. van Sinderen"
        ],
        "file_path": "data/sosym-all/s10270-012-0304-6.pdf"
    },
    {
        "title": "A reﬁnement-based approach to safe smart contract deployment and evolution",
        "submission-date": "2023/02",
        "publication-date": "2024/01",
        "abstract": "In our previous work, we proposed a veriﬁcation framework that shifts from the “code is law” to a new “speciﬁcation is law” paradigm related to the safe evolution of smart contracts. The framework proposed there relaxed the well-established requirement that, once a smart contract is deployed in a blockchain, its code is expected to be immutable. More ﬂexibly, contracts are allowed to be created and upgraded provided they meet a corresponding formal speciﬁcation that was ﬁxed. In the current paper, we extend this framework to allow speciﬁcations to evolve, provided a reﬁnement notion is preserved. We propose a notion of speciﬁcation reﬁnement tailored for smart contracts and a methodology for checking it. In addition to weakening preconditions and strengthening postconditions and invariants, we allow for the change of data representation and interface extension. Thus, we are able to reason about a signiﬁcantly wider class of smart contract evolution histories, when contrasted with the original framework. The new framework is centred around a trusted deployer: an off-chain service that formally veriﬁes and enforces the notions of implementation conformance and speciﬁcation reﬁnement. We have investigated its applicability to the safe deployment and upgrade of contracts implementing widely used Ethereum standards (the ERC20 Token Standard, the ERC3156 Flash Loans, the ERC1155 Multi Token Standard and The ERC721 standard for Non-Fungible Tokens); we handle evolutions possibly involving changes in data representation and interface extensions.",
        "keywords": [
            "Formal veriﬁcation",
            "Smart contracts",
            "Ethereum",
            "Solidity",
            "Safe deployment",
            "Safe upgrade"
        ],
        "authors": [
            "Pedro Antonino",
            "Juliandson Ferreira",
            "Augusto Sampaio",
            "A. W. Roscoe",
            "Filipe Arruda"
        ],
        "file_path": "data/sosym-all/s10270-023-01143-z.pdf"
    },
    {
        "title": "DSMCompare: domain-speciﬁc model differencing for graphical domain-speciﬁc languages",
        "submission-date": "2021/02",
        "publication-date": "2022/01",
        "abstract": "During the development of a software project, different developers collaborate on creating and changing models. These models evolve and need to be versioned. Over the past several years, progress has been made in offering dedicated support for model versioning that improves on what is being supported by text-based version control systems. However, there is still need to understand model differences in terms of the semantics of the modeling language, and to visualize the changes using its concrete syntax. To address these issues, we propose a comprehensive approach—called DSMCompare—that considers both the abstract and the concrete syntax of a domain-speciﬁc language (DSL) when expressing model differences, and which supports deﬁning domain-speciﬁc semantics for speciﬁc difference patterns. The approach is based on the automatic extension of the DSL to enable the representation of changes and on the automatic adaptation of its graphical concrete syntax to visualize the differences. In addition, we allow for the deﬁnition of semantic differencing rules to capture recurrent domain-speciﬁc difference patterns. Since these rules can be conﬂicting with each other, we introduce algorithms for conﬂict resolution and rule scheduling. To demonstrate the applicability and effectiveness of our approach, we report on evaluations based on synthetic models and on version histories of models developed by third parties.",
        "keywords": [
            "Model-driven engineering",
            "Model differencing",
            "Domain-speciﬁc languages",
            "Graphical concrete syntax"
        ],
        "authors": [
            "Manouchehr Zadahmad",
            "Eugene Syriani",
            "Omar Alam",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-021-00971-1.pdf"
    },
    {
        "title": "Introduction to the theme section on Agile model-driven engineering",
        "submission-date": "2022/05",
        "publication-date": "2022/05",
        "abstract": "Agile methods have become a widely used software development approach across many industry sectors [14], with established benefits in terms of increased responsiveness to change and decreased time to market. Agile practices emphasise lightweight and iterative development, designed to deliver value to customers quickly. The model-driven approach to software development (MDE) originated at about the same time as Agile, in the late 1990s, and has been most widely utilised in high-integrity domains such as vehicle control systems and aerospace. It has the benefits of a rigorous and systematic approach to software construction, based on models which represent the key concepts of the application. Because Agile and MDE both have attractive features, the idea arose of producing an integration of the two approaches, in order to gain the advantages of both. From the perspective of MDE researchers, incorporating Agile practices into MDE appeared to be a way to widen the uptake of MDE to more general software application areas. From the perspective of industrialists using Agile approaches, the introduction of MDE techniques was considered as a means of increasing the rigour and precision of their practices.",
        "keywords": [],
        "authors": [
            "Kevin Lano",
            "Shekoufeh Kolahdouz-Rahimi",
            "Javier Troya",
            "Hessa Alfraihi"
        ],
        "file_path": "data/sosym-all/s10270-022-01016-x.pdf"
    },
    {
        "title": "Conceptual modelling of temporality and subjectivity as cross-cutting concerns",
        "submission-date": "2024/02",
        "publication-date": "2024/12",
        "abstract": "Conceptual models represent the world, but the world changes over time, and is different, to some extent, depending on who you ask. To cater for these aspects, conceptual models must address the representation of temporality and subjectivity. Although some work has been done to incorporate these aspects into conceptual modelling languages, no mainstream language incorporates explicit support for temporality and subjectivity in relation to the existence and predication on the represented entities. In this article, I propose an approach to conceptualising and implementing temporality and subjectivity into conceptual modelling languages as cross-cutting aspects that provide built-in language primitives to the modeller while not imposing any particular manner to represent time or subjects. The approach has been adopted by ConML, a conceptual modelling language geared towards the humanities and social sciences, implemented in the Bundt software tool, and applied to a number of research projects.",
        "keywords": [
            "Temporality",
            "Subjectivity",
            "Conceptual modelling",
            "Cross-cutting aspects",
            "ConML"
        ],
        "authors": [
            "Cesar Gonzalez-Perez"
        ],
        "file_path": "data/sosym-all/s10270-024-01247-0.pdf"
    },
    {
        "title": "OCLFO: ﬁrst-order expressive OCL constraints for efﬁcient integrity checking",
        "submission-date": "2017/02",
        "publication-date": "2018/08",
        "abstract": "OCL is the standard language for deﬁning constraints in UML class diagrams. Unfortunately, as we show in this paper, full\nOCL is so expressive that it is not possible to check general OCL constraints efﬁciently. In particular, we show that checking\ngeneral OCL constraints is not only not polynomial, but not even semidecidable. To overcome this situation, we identify\nOCLFO, a fragment of OCL which is expressively equivalent to relational algebra (RA). By equivalent we mean that any\nOCLFO constraint can be checked through a RA query (which guarantees that OCLFO checking is efﬁcient, i.e., polynomial),\nand any RA query encoding some constraint can be written as an OCLFO constraint (which guarantees expressiveness of\nOCLFO). In this paper we deﬁne the syntax of OCLFO, we concisely determine its semantics through set theory, and we prove\nits equivalence to RA. Additionally, we identify the core of this language, i.e., a minimal subset of OCLFO equivalent to RA.",
        "keywords": [
            "OCL",
            "Relational algebra",
            "Integrity checking"
        ],
        "authors": [
            "Enrico Franconi",
            "Alessandro Mosca",
            "Xavier Oriol",
            "Guillem Rull",
            "Ernest Teniente"
        ],
        "file_path": "data/sosym-all/s10270-018-0688-z.pdf"
    },
    {
        "title": "Models for the digital transformation",
        "submission-date": "2017/04",
        "publication-date": "2017/04",
        "abstract": "“Digital transformation” is currently an important trend that penetrates many industrial and societal domains. The phrase is also emerging as a buzzword that allows different stakeholders to inject various forms of innovation into their respective company, business, government, academic institution, or other public services. The nuances of digital transformation are broad and have not yet been deﬁned precisely, but even job advertisements often contain the phrase. Deconstructing the term from its two primary words, we identify two well-known concepts. “Transformation” describes a general process that starts with some initial situation that moves toward a changed, and supposedly better situation. May be that in this case the word transformation is not the best word choice because the underlying transformation may never meet a stable end, but rather undergo a continual set of evolutionary optimizations related to new forms of business, production, logistics, medicine or other changes within the targeted domain. “Digital” suggests that many changes in society, business and industry will be driven by information technologies that allow data to be processed in real-time and even used to intelligently derive information to finally to provide stakeholders with improved knowledge about their processes and products. Downstream digitization would also allow optimization, automation activities and production techniques of various forms.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-017-0596-7.pdf"
    },
    {
        "title": "Model development guidelines for UML-RT: conventions, patterns and antipatterns",
        "submission-date": "2016/01",
        "publication-date": "2016/07",
        "abstract": "Software development guidelines are a set of rules which can help improve the quality of software. These rules are deﬁned on the basis of experience gained by the software developmentcommunityovertime.Thispaperdiscussesaset of design guidelines for model-based development of complex real-time embedded software systems. To be precise, we propose nine design conventions, three design patterns and thirteen antipatterns for developing UML-RT models. These guidelines have been identiﬁed based on our analysis of around 100 UML-RT models from industry and academia. Most of the guidelines are explained with the help of examples, and standard templates from the current state of the art are used for documenting the design rules.",
        "keywords": [
            "UML-RT",
            "Patterns",
            "AntiPatterns",
            "Model development guidelines"
        ],
        "authors": [
            "Tuhin Kanti Das",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-016-0549-6.pdf"
    },
    {
        "title": "Relational interprocedural veriﬁcation of concurrent programs",
        "submission-date": "2010/06",
        "publication-date": "2012/03",
        "abstract": "We propose a general analysis method for recursive, concurrent programs that track effectively procedure calls and return in a concurrent context, even in the presence of unbounded recursion and inﬁnite-state variables like integers. This method generalizes the relational interprocedural analysis of sequential programs to the concurrent case, and extends it to backward or coreachability analysis. We implemented it for programs with scalar variables and experimented with several classical synchronization protocols in order to illustrate the precision of our technique and also to analyze the approximations it performs.",
        "keywords": [
            "Concurrent program analysis",
            "Interprocedural analysis",
            "Abstract interpretation",
            "Numerical abstract domains",
            "Forward and backward analysis"
        ],
        "authors": [
            "Bertrand Jeannet"
        ],
        "file_path": "data/sosym-all/s10270-012-0230-7.pdf"
    },
    {
        "title": "A newly introduced Industry Voice Column",
        "submission-date": "2013/06",
        "publication-date": "2013/06",
        "abstract": "We are proud to announce a new column for the journal that is about to start with this very issue, where a first paper is included. We have started it, because we think modeling is an essential part of modern software and systems development processes. It is used in a variety of ways including informal design sketches, the basis of system analysis such as security and safety, and as a basis for industrializing software production from models. The science of software and systems modeling has been developed from the earliest days of software engineering when it was realized that abstraction is a fundamental tool for system design. Modeling research has matured through the development of many well-defined theories and notations, and is well served in terms of prominent conferences and journals including SoSyM. The application of modeling within industry is now widespread and has developed through the application of research results and as a result of standardization efforts, most notably the UML notation. Industrial model based development promises a great deal in terms of the increase of system quality while at the same time reducing the cost of system development. However, many challenges remain to be addressed by the research community. The SoSyM Industry Voice aims to provide a regular update on the application and challenges of model-based approaches to system development in industry. It provides researchers with a valuable resource of information about modeling success stories, standardization updates, and specific technology requirements that can be used to scope the research agenda. It is a regular SoSyM column (approx. 2–5 pages). The column solicits contributions in the following areas: Industrial case studies and experience reports, News from standards organizations, Industrial strategies for deployment of model-based approaches, Commercial and professional aspects of model-based engineering, Vision statements by industrial thought leaders and evangelists, Industrial input to the model-based research agenda, Historical perspectives on model-based approaches.",
        "keywords": [
            "• Industrial case studies and experience reports\n• News from standards organizations\n• Industrial strategies for deployment of model-based approaches\n• Commercial and professional aspects of model-based engineering\n• Vision statements by industrial thought leaders and evangelists\n• Industrial input to the model-based research agenda\n• Historical perspectives on model-based approaches."
        ],
        "authors": [
            "Tony Clark",
            "Gabor Karsai",
            "Roel J. Wieringa",
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-013-0361-5.pdf"
    },
    {
        "title": "Uncertainty-aware environment simulation of medical devices digital twins",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Smart medical devices are an integral component of the healthcare Internet of Things (IoT), providing patients with various healthcare services through an IoT-based application. Ensuring the dependability of such applications through system and integration-level testing mandates the physical integration of numerous medical devices, which is costly and impractical. In this context, digital twins of medical devices play an essential role in facilitating testing automation. Testing with digital twins without accounting for uncertain environmental factors of medical devices leaves many functionalities of IoT-based healthcare applications untested. In addition, digital twins operating without environmental factors remain out of sync and uncalibrated with their corresponding devices functioning in the real environment. To deal with these challenges, in this paper, we propose a model-based approach (EnvDT) for modeling and simulating the environment of medical devices’ digital twins under uncertainties. We empirically evaluate the EnvDT using three medicine dispensers, Karie, Medido, and Pilly connected to a real-world IoT-based healthcare application. Our evaluation targets analyzing the coverage of environment models and the diversity of uncertain scenarios generated for digital twins. Results show that EnvDT achieves approximately 61% coverage of environment models and generates diverse uncertain scenarios (with a near-maximum diversity value of 0.62) during multiple environmental simulations.",
        "keywords": [
            "Healthcare Internet of Things (IoT)",
            "Environment modeling",
            "Digital twins",
            "Uncertainty"
        ],
        "authors": [
            "Hassan Sartaj",
            "Shaukat Ali",
            "Julie Marie Gjøby"
        ],
        "file_path": "data/sosym-all/s10270-024-01223-8.pdf"
    },
    {
        "title": "MBFair: a model-based veriﬁcation methodology for detecting violations of individual fairness",
        "submission-date": "2022/11",
        "publication-date": "2024/06",
        "abstract": "Decision-making systems are prone to discrimination against individuals with regard to protected characteristics such as gender and ethnicity. Detecting and explaining the discriminatory behavior of implemented software is difﬁcult. To avoid the possibility of discrimination from the onset of software development, we propose a model-based methodology called MBFair that allows for verifying UML-based software designs with regard to individual fairness. The veriﬁcation in MBFair is performed by generating temporal logic clauses, whose veriﬁcation results enable reporting on the individual fairness of the targeted software. We study the applicability of MBFair using three case studies in real-world settings including a bank services system, a delivery system, and a loan system. We empirically evaluate the necessity of MBFair in a user study and compare it against a baseline scenario in which no modeling and tool support is offered. Our empirical evaluation indicates that analyzing the UML models manually produces unreliable results with a high chance of 46% that analysts overlook true-positive discrimination. We conclude that analysts require support for fairness-related analysis, such as our MBFair methodology.",
        "keywords": [
            "Software fairness",
            "Individual fairness",
            "Model-based veriﬁcation",
            "UML"
        ],
        "authors": [
            "Qusai Ramadan",
            "Marco Konersmann",
            "Amir Shayan Ahmadian",
            "Jan Jürjens",
            "Steﬀen Staab"
        ],
        "file_path": "data/sosym-all/s10270-024-01184-y.pdf"
    },
    {
        "title": "Editorial to the theme issue on metamodelling",
        "submission-date": "2009/08",
        "publication-date": "2009/08",
        "abstract": "Linguistics, Mathematics, and Computer Science are inherently reﬂective disciplines. All of them require care in avoiding paradoxes which may be caused by uncontrolled self-reference. Early “meta” success stories in computer science date back as far as 1960. Metamodelling, as applied in such tools, has a number of associated advantages.",
        "keywords": [],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-009-0124-5.pdf"
    },
    {
        "title": "Extraction and evolution of architectural variability models in plugin-based systems",
        "submission-date": "2012/02",
        "publication-date": "2013/07",
        "abstract": "Variability management is a key issue when building and evolving software-intensive systems, making it possible to extend, configure, customize and adapt such systems to customers’ needs and specific deployment contexts. A wide form of variability can be found in extensible software systems, typically built on top of plugin-based architectures that offer a (large) number of configuration options through plugins. In an ideal world, a software architect should be able to generate a system variant on-demand, corresponding to a particular assembly of plugins. To this end, the variation points and constraints between architectural elements should be properly modeled and maintained over time (i.e., for each version of an architecture). A crucial, yet error-prone and time-consuming, task for a software architect is to build an accurate representation of the variability of an architecture, in order to prevent unsafe architectural variants and reach the highest possible level of flexibility. In this article, we propose a reverse engineering process for producing a variability model (i.e., a feature model) of a plugin-based architecture. We develop automated techniques to extract and combine different variability descriptions, including a hierarchical software architecture model, a plugin dependency model and the software architect knowledge. By computing and reasoning about differences between versions of architectural feature models, software architect can control both the variability extraction and evolution processes. The proposed approach has been applied to a representative, large-scale plugin-based system (FraSCAti), considering different versions of its architecture. We report on our experience in this context.",
        "keywords": [
            "Variability",
            "Product lines",
            "Reverse engineering",
            "Software evolution",
            "Architecture recovery",
            "Configuration management"
        ],
        "authors": [
            "Mathieu Acher",
            "Anthony Cleve",
            "Philippe Collet",
            "Philippe Merle",
            "Laurence Duchien",
            "Philippe Lahire"
        ],
        "file_path": "data/sosym-all/s10270-013-0364-2.pdf"
    },
    {
        "title": "A method for digital business ecosystem design: situational method engineering in an action research project",
        "submission-date": "2022/03",
        "publication-date": "2022/11",
        "abstract": "Digital business ecosystem (DBE) is a paradigm that enables developing and monitoring novel business models of collabo-rating organisations and individuals using ICT as the foundation. Different from traditional online networked models such as manufacturer, retailer, or franchise centred, using a shared digital environment, DBE fosters heterogeneity, symbiosis, coevo-lution, and self-organisation of its multiple actors, which enables it to span different business domains as well as exhibits diverse interests. For many organisations and individuals, DBE presents a new collaborative approach to leverage offered and desired resources among the involved members to meet each of their goals. As such, it is foreseen to be of high value to the involved actors, but at the same time, it is often complex due to many correlated interactions of these actors and thus difﬁcult to design and manage. Furthermore, the current state of the art shows a lack of methodological guidance for DBE design. We propose a method for DBE design that follows the requirements collected from industry experts and practitioners by applying situational method engineering to enable its modularised design. The method for design is validated by action research in the setting of Digital Vaccine, a Swedish DBE managing health-related services.",
        "keywords": [
            "Digital business ecosystem",
            "Situational method engineering",
            "Method chunks",
            "Action research"
        ],
        "authors": [
            "Chen Hsi Tsai",
            "Jelena Zdravkovic",
            "Fredrik Söder"
        ],
        "file_path": "data/sosym-all/s10270-022-01068-z.pdf"
    },
    {
        "title": "Modularization of model transformations through a phasing mechanism",
        "submission-date": "2007/07",
        "publication-date": "2008/06",
        "abstract": "In recent years a great effort has been devoted\nto understanding the nature of model transformations. As\na result, several mechanisms to improve model transforma-\ntion languages have been proposed. Phasing has been men-\ntioned in some works as a rule scheduling or organization\nmechanism, but without any detail. In this paper, we present\na phasing mechanism in the context of rule-based transfor-\nmation languages. We explain the structure and the behavior\nof the mechanism, and how it can be integrated in a language.\nWe also analyze how the mechanism promotes modularity,\ninternal transformation composition and helps to solve usual\ntransformationproblems.Besides,weshowseveralexamples\nof application to illustrate the usefulness of the mechanism.",
        "keywords": [
            "Model transformation",
            "Transformation\nlanguages",
            "Phasing mechanism",
            "Modularity",
            "Internal\ntransformation composition"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Jesús García Molina"
        ],
        "file_path": "data/sosym-all/s10270-008-0093-0.pdf"
    },
    {
        "title": "Reducing accidental complexity in domain models",
        "submission-date": "2006/12",
        "publication-date": "2007/06",
        "abstract": "A fundamental principle in engineering, including software engineering, is to minimize the amount of accidental complexity which is introduced into engineering solutions due to mismatches between a problem and the technology used to represent the problem. As model-driven development moves to the center stage of software engineering, it is particularly important that this principle be applied to the technologies used to create and manipulate models, especially models that are intended to be free of solution decisions. At present, however, there is a signiﬁcant mismatch between the “two level” modeling paradigm used to construct mainstream domain models and the conceptual information such models are required to represent—a mis-match that makes such models more complex than they need be. In this paper, we identify the precise nature of the mis-match, discuss a number of more or less satisfactory worka-rounds, and show how it can be avoided.",
        "keywords": [
            "Domain modeling",
            "Model quality",
            "Accidental complexity",
            "Modeling languages",
            "Modeling paradigm",
            "Stereotypes",
            "Powertypes",
            "Deep instantiation"
        ],
        "authors": [
            "Colin Atkinson",
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-007-0061-0.pdf"
    },
    {
        "title": "Ark: a constraint-based method for architectural synthesis of smart systems",
        "submission-date": "2017/09",
        "publication-date": "2019/11",
        "abstract": "As smart systems leverage capabilities of heterogeneous systems for accomplishing complex combined behaviors, they pose\nnew challenges to traditional software engineering practices that considered software architectures to be mostly static and\nstable. The software architecture of a smart system is inherently dynamic due to uncertainty surrounding its operational envi-\nronment. While the abstract architecture offers a way to implicitly describe different forms taken by the software architecture\nat run time, it is still not sufﬁcient to guarantee that all concrete architectures will automatically adhere to it. To address this\nissue, this work presents a formal method named Ark supporting the architectural synthesis of smart systems. This is achieved\nby expressing abstract architectures as a set of constraints that must be valid for any concrete architecture of the smart system.\nThis way, we can beneﬁt from existing model-checking techniques to guarantee that all concrete architectures realized from\nsuch an abstract model will comply with well-formed rules. We also describe how this method can be incorporated to a\nmodel-driven approach for bridging the gap between abstract and concrete architectural models. We demonstrate our method\nin an illustrative case study, showing how Ark can be used to support the synthesis of concrete architectures as well check the\ncorrectness and completeness of abstract architecture descriptions. Finally, we elaborate on future directions to consolidating\na process for the synthesis of run-rime architectures that are correct-by-construction.",
        "keywords": [
            "Smart system",
            "Software architecture",
            "Formal method",
            "Architectural synthesis",
            "Constraints",
            "Alloy"
        ],
        "authors": [
            "Milena Guessi",
            "Flavio Oquendo",
            "Elisa Yumi Nakagawa"
        ],
        "file_path": "data/sosym-all/s10270-019-00764-7.pdf"
    },
    {
        "title": "Business process management as the “Killer App” for Petri nets",
        "submission-date": "2014/01",
        "publication-date": "2014/06",
        "abstract": "Since their inception in 1962, Petri nets have been used in a wide variety of application domains. Although Petri nets are graphical and easy to understand, they have formal semantics and allow for analysis techniques ranging from model checking and structural analysis to process mining and performance analysis. Over time Petri nets emerged as a solid foundation for Business Process Management (BPM) research. The BPM discipline develops methods, techniques, and tools to support the design, enactment, management, and analysis of operational business processes. Mainstream business process modeling notations and workﬂow management systems are using token-based semantics borrowed from Petri nets. Moreover, state-of-the-art BPM analysis techniques are using Petri nets as an internal representation. Users of BPM methods and tools are often not aware of this. This paper aims to unveil the seminal role of Petri nets in BPM.",
        "keywords": [
            "Business process management",
            "Petri nets",
            "Process modeling",
            "Process mining"
        ],
        "authors": [
            "W. M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-014-0424-2.pdf"
    },
    {
        "title": "F-Alloy: a relational model transformation language based on Alloy",
        "submission-date": "2016/06",
        "publication-date": "2017/11",
        "abstract": "Abstract Model transformations are one of the core arti-\nfacts of a model-driven engineering approach. The relational\nlogic language Alloy has been used in the past to verify\nproperties of model transformations. In this paper we intro-\nduce the concept of functional Alloy modules. In essence a\nfunctional Alloy module can be viewed as an Alloy module\nrepresenting a model transformation. We describe a sub-\nlanguage of Alloy called F-Alloy speciﬁcally designed to\nconcisely specify functional Alloy modules. The restrictions\non F-Alloy’s syntax are meant to allow efﬁcient execution\nof the speciﬁed transformation, without the use of back-\ntracking, by an adapted interpretation algorithm. F-Alloy’s\nsemantics is given in this paper as a direct translation\nto Alloy; hence, F-Alloy speciﬁcations are also analyz-\nable using the powerful automatic analysis features of\nAlloy.",
        "keywords": [
            "Model transformation",
            "F-Alloy",
            "Alloy",
            "Analysis",
            "Formal method",
            "Endogenous",
            "Exogenous"
        ],
        "authors": [
            "Loïc Gammaitoni",
            "Pierre Kelsen"
        ],
        "file_path": "data/sosym-all/s10270-017-0630-9.pdf"
    },
    {
        "title": "Model-based qualitative risk assessment for availability of IT infrastructures",
        "submission-date": "2009/06",
        "publication-date": "2010/06",
        "abstract": "For today’s organisations, having a reliable information system is crucial to safeguard enterprise revenues(thinkofon-linebanking,reservationsfore-ticketsetc.). Such a system must often offer high guarantees in terms of its availability; in other words, to guarantee business continuity, IT systems can afford very little downtime. Unfortunately, making an assessment of IT availability risks is difﬁcult: incidents affecting the availability of a marginal component of the system may propagate in unexpected ways to other more essential components that functionally depend on them. General-purpose risk assessment (RA) methods do not provide technical solutions to deal with this problem. In this paper we present the qualitative time dependency (QualTD) model and technique, which is meant to be employed together with standard RA methods for the qualitative assessment of availability risks based on the propagation of availability incidents in an IT architecture. The QualTD model is based on our previous quantitative time dependency (TD) model (Zam-bon et al. in BDIM ’07: Second IEEE/IFIP international Communicated by Prof. Ketil Stølen. workshop on business-driven IT management. IEEE Computer Society Press, pp 75–83, 2007), but provides more ﬂexible modelling capabilities for the target of assessment. Furthermore, the previous model required quantitative data which is often too costly to acquire, whereas QualTD applies only qualitative scales, making it more applicable to indus-trial practice. We validate our model and technique in a real-world case by performing a risk assessment on the authen-tication and authorisation system of a large multinational company and by evaluating the results with respect to the goals of the stakeholders of the system. We also perform a review of the most popular standard RA methods and discuss which type of method can be combined with our technique.",
        "keywords": [
            "Information risk management",
            "Risk assessment",
            "Availability",
            "Information security",
            "System modelling"
        ],
        "authors": [
            "Emmanuele Zambon",
            "Sandro Etalle",
            "Roel J. Wieringa",
            "Pieter Hartel"
        ],
        "file_path": "data/sosym-all/s10270-010-0166-8.pdf"
    },
    {
        "title": "Guest Editorial to the Special Issue on Language Engineering for Model-Driven Software Development",
        "submission-date": "2006/08",
        "publication-date": "2006/08",
        "abstract": "Model-driven approaches to software development require precise definitions and tool support for modelling languages, their syntax and semantics, consistency, refinement and transformation. In order to support model-driven development for a variety of application contexts and platforms, efficient ways of designing languages are needed, accepting that languages are evolving and that tools need to be delivered in a timely fashion. In this respect, languages are not unlike software: A discipline of language engineering is required to support their design, implementation, verification and validation, delivering languages of high quality at low cost. An important contribution of any engineering approach, besides the actual technology provided, is the meta knowledge about how different technologies are related and for which classes of problems they provide solutions. Well-known examples of such technologies, used by contributions in this special issue, include UML/MOF-based metamodelling, graph transformation, algebra and logic, data base technology, etc. This special issue, published in two sections in this and the next volume of this journal, presents six contributions from two scientific events.",
        "keywords": [],
        "authors": [
            "Jean Bézivin",
            "Reiko Heckel"
        ],
        "file_path": "data/sosym-all/s10270-006-0028-6.pdf"
    },
    {
        "title": "Model-based a-posteriori integration of engineering tools for incremental development processes",
        "submission-date": "2003/12",
        "publication-date": "2004/11",
        "abstract": "A-posteriori integration of heterogeneous engin-eering tools supplied by different vendors constitutes a chal-lenging task. In particular, this statement applies to incremental development processes where small changes have to be propagated – potentially bidirectionally – through a set of inter-dependent design documents which have to be kept consistent with each other. Responding to these challenges, we have developed an approach to tool integration which puts strong emphasis on software architecture and model-driven development. Starting from an abstract description of a software architecture, the architecture is gradually reﬁned down to an implementation level. To integrate heterogeneous en-gineering tools, wrappers are constructed for abstracting from technical details and for providing homogenized data access. On top of these wrappers, incremental integration tools provide for inter-document consistency. These tools are based on graph models of the respective document classes and graph transformation rules for maintaining inter-document consis-tency. Altogether, the collection of support tools and the respective infrastructure considerably leverage the problem of composing a tightly integrated development environment from a set of heterogeneous engineering tools.",
        "keywords": [
            "A-posteriori integration",
            "Incremental consistency management",
            "Graph transformation",
            "UML",
            "Wrapping",
            "Software architecture"
        ],
        "authors": [
            "Simon M. Becker",
            "Thomas Haase",
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-004-0071-0.pdf"
    },
    {
        "title": "Model-based testing of software for automation systems using heuristics and coverage criterion",
        "submission-date": "2016/10",
        "publication-date": "2018/08",
        "abstract": "The aim of this work is to increase the conﬁdence on software for automation systems deﬁning a coverage criterion to measure the quality level of generated tests and the time interval needed to execute them. This coverage criterion called At Least N (ALN) is based on the Effect Predicate Heuristic (EPH) that provides all effect predicate for ISA 5.2 diagrams. The ALN and EPH have been incorporated into the Gungnir tool that was built using model-based testing concepts. The Gungnir uses timed automata to model the speciﬁcation, in the ISA 5.2 diagrams, and the implementation, in the Ladder language. The timed automata models are automatically extracted, data tests are generated and the tool automatically veriﬁes if the implementation is in conformance with the speciﬁcation, given a quality level deﬁned by the user.",
        "keywords": [
            "Model-based testing",
            "Conformance tests",
            "Automation systems",
            "Timed automata",
            "Programmable logic controllers",
            "Ladder",
            "ISA 5.2"
        ],
        "authors": [
            "Rodrigo José Sarmento Peixoto",
            "Leandro Dias da Silva",
            "Angelo Perkusich"
        ],
        "file_path": "data/sosym-all/s10270-018-0690-5.pdf"
    },
    {
        "title": "An approach for modeling and detecting software performance antipatterns based on ﬁrst-order logics",
        "submission-date": "2011/04",
        "publication-date": "2012/04",
        "abstract": "The problem of interpreting the results of performance analysis is quite critical in the software performance domain. Mean values, variances and probability distributions are hard to interpret for providing feedback to software architects. Instead, what architects expect are solutions to performance problems, possibly in the form of architectural alternatives (e.g. split a software component in two components and re-deploy one of them). In a soft-ware performance engineering process, the path from analysis results to software design or implementation alternatives is still based on the skills and experience of analysts. In this paper, we propose an approach for the generation of feedback based on performance antipatterns. In particular, we focus on the representation and detection of antipatterns. To this goal, we model performance antipatterns as logical predicates and we build an engine, based on such predicates, aimed at detect-ing performance antipatterns in an XML representation of the software system. Finally, we show the approach at work on a case study.",
        "keywords": [
            "Software performance antipatterns",
            "Performance analysis",
            "Software architectures"
        ],
        "authors": [
            "Vittorio Cortellessa",
            "Antinisca Di Marco",
            "Catia Trubiani"
        ],
        "file_path": "data/sosym-all/s10270-012-0246-z.pdf"
    },
    {
        "title": "Transitive-closure-based model checking (TCMC) in Alloy",
        "submission-date": "2017/10",
        "publication-date": "2020/01",
        "abstract": "We present transitive-closure-based model checking (TCMC): a symbolic representation of the semantics of computational tree logic with fairness constraints (CTLFC) for ﬁnite models in ﬁrst-order logic with transitive closure (FOLTC). TCMC is an expression of the complete model checking problem for CTLFC as a set of constraints in FOLTC without induction, iteration, or invariants. We implement TCMC in the Alloy Analyzer, showing how a transition system can be expressed declaratively and concisely in the Alloy language. Since the total state space is rarely representable due to the state-space explosion problem, we present scoped TCMC where the property is checked for state spaces of a size smaller than the total state space. We address the problem of spurious instances and carefully describe the meaning of results from scoped TCMC with respect to the complete model checking problem. Using case studies, we demonstrate scoped TCMC and compare it with bounded model checking, highlighting how TCMC can check inﬁnite paths.",
        "keywords": [
            "Symbolic model checking",
            "Alloy",
            "Declarative models"
        ],
        "authors": [
            "Sabria Farheen\nNancy A. Day\nAmirhossein Vakili\nAli Abbassi"
        ],
        "file_path": "data/sosym-all/s10270-019-00763-8.pdf"
    },
    {
        "title": "Introductory paper",
        "submission-date": "2004/04",
        "publication-date": "2004/04",
        "abstract": "The evolution of diagrammatic notations usually follows a pattern that, from their usage as illustrations of sentences written in some formal or natural language, leads to the definition of “modeling languages”. To achieve this status, the definition of a diagrammatic language requires formal methods to precisely specify the syntax and semantics of such diagrams. In particular, when visual models of systems or processes constitute executable specification of systems, not only is a non-ambiguous specifications of their static syntax and semantics needed, but also an adequate notion of diagram dynamics. Such a notion must establish links (e.g., morphisms) which relate diagram transformations and transformations of the objects of the underlying domain. The field of Graph Grammars and Graph Transformation Systems has contributed much insight into the solution of these problems, but also other approaches (e.g., meta modeling, constraint-based and other rule-based systems) have been developed to tackle specific issues. This special section on graph transformations and visual modeling techniques gives a vivid account of the diversity of problems and approaches that the community of researchers on visual modeling techniques are pursuing. The contributions here range from theoretical issues to the generation of language specific tools, as well as general techniques for language specification. Much attention is payed to dynamic aspects of diagrammatic languages, either proposing formal semantics for languages specifying system behaviors, or directly expressing behaviors through language transformations.",
        "keywords": [],
        "authors": [
            "Paolo Bottoni",
            "Mark Minas"
        ],
        "file_path": "data/sosym-all/s10270-003-0049-3.pdf"
    },
    {
        "title": "GRAPHxx: DSML engineering for knowledge graph building and streamlining with GraphRAG",
        "submission-date": "2024/03",
        "publication-date": "2025/05",
        "abstract": "Symbolic AI has been facing long-term adoption obstacles and a slow uptake caused by the limited availability of frictionless tooling—that can support not only knowledge engineers, but also novice users and educators, with designing and presenting graph exemplars that can be visually communicated, edited and ad hoc processed. There’s still a shortage of tools that can democratize knowledge graph (KG) creation, something that is increasingly needed—ﬁrstly by educators trying to discuss examples with novices without stumbling on OWL jargon at the earliest step, but also for more recent integration cases such as (i) GraphRAG, where private KGs are called to augment or ensure factuality of large language model services; or (ii) in search engine optimization (SEO), where SEO practitioners lacking knowledge engineering background must embed Schema.org graph data into their Web content. Most visual KG tools are visualizers of KGs created by other means—either in OWL-centric ontology editors posing high expertise barriers, or converted from available serializations, or lifted from legacy data sources. The few actual KG editors are mostly OWL editors neglecting to support the creation of schemaless graph datasets as well as of ﬂexible combinations of graph data and schema fragments. This paper reports on a Design Science effort adopting metamodeling means, traditionally employed for the engineering of domain-speciﬁc modeling languages, toward deﬁning a KG development and integration method that facilitates both the visual design of KG exemplars and their operationalization. We aim to balance a diagrammatic look and feel with machine readability of the semantic content being produced—further streamlined in an architecting proposition for integration with LLM services and for the production of Schema.org graph snippets. The DSML was deployed and evaluated as a tool implemented on the ADOxx metamodeling platform, using RDF and LangChain as mediators that streamline the content toward triplestores and LLM services.",
        "keywords": [
            "Knowledge graphs",
            "Domain-speciﬁc modeling language",
            "RDF",
            "Retrieval augmented generation",
            "LLM",
            "ADOxx"
        ],
        "authors": [
            "Andrei Chis",
            "Ana-Maria Ghiran",
            "Robert Andrei Buchmann"
        ],
        "file_path": "data/sosym-all/s10270-025-01297-y.pdf"
    },
    {
        "title": "Modeling should be an independent scientific discipline",
        "submission-date": "2022/07",
        "publication-date": "2022/08",
        "abstract": "Software modeling started as a paradigm to help developers build better software faster by enabling them to specify, reason and manipulate software systems at a higher-abstraction level while ignoring irrelevant low-level technical details. But this same principle manifests in any other domain that has to deal with complex systems, software-based or not. We argue that bringing to other engineering and scientific fields, our modeling expertise is a win–win opportunity where we can all learn from each other as we all model, but in complementary ways. Nevertheless, to fully unleash the benefits of this collaboration, we must go beyond individual efforts trying to adapt single techniques from one field to another. It requires a deeper reformulation of modeling as a whole. It is time for modeling to become an independent discipline where all fields of knowledge can contribute and benefit from.",
        "keywords": [
            "Modeling",
            "Abstraction",
            "Discipline",
            "Transdisciplinarity",
            "Science",
            "Engineering"
        ],
        "authors": [
            "Jordi Cabot",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-022-01035-8.pdf"
    },
    {
        "title": "A4WSN: an architecture-driven modelling platform for analysing and developing WSNs",
        "submission-date": "2016/07",
        "publication-date": "2018/07",
        "abstract": "This paper proposes A4WSN, an architecture-driven modelling platform for the development and the analysis of wireless sensor networks (WSNs). A WSN consists of spatially distributed sensor nodes that cooperate in order to accomplish a speciﬁc task. Sensor nodes are cheap, small, and battery-powered devices with limited processing capabilities and memory. WSNs are mostly developed directly on the top of the operating system. They are tied to the hardware conﬁguration of the sensor nodes, and their design and implementation can require cooperation with a myriad of system stakeholders with different backgrounds. The peculiarities of WSNs and current development practices bring a number of challenges, ranging from hardware and software coupling, limited reuse, and the late assessment of WSN quality properties. As a way to overcome a number of existing limitations, this study presents a multi-view modelling approach that supports the development and analysis of WSNs. The framework uses different models to describe the software architecture, hardware conﬁguration, and physical deployment of a WSN. A4WSN allows engineers to perform analysis and code generation in earlier stages of the WSN development life cycle. The A4WSN platform can be extended with third-party plug-ins providing additional analysis or code generation engines. We provide evidence of the applicability of the proposed platform by developing PlaceLife, an A4WSN plug-in for estimating the WSN lifetime by taking various physical obstacles in the deployment environment into account. In turn, PlaceLife has been applied to a real-world case study in the health care domain as a running example.",
        "keywords": [
            "MDE",
            "Software engineering",
            "Software architecture",
            "WSNs",
            "Energy"
        ],
        "authors": [
            "Ivano Malavolta",
            "Leonardo Mostarda",
            "Henry Muccini",
            "Enver Ever",
            "Krishna Doddapaneni",
            "Orhan Gemikonakli"
        ],
        "file_path": "data/sosym-all/s10270-018-0687-0.pdf"
    },
    {
        "title": "Model projection relative to submetamodeling dimensions",
        "submission-date": "2022/07",
        "publication-date": "2023/07",
        "abstract": "Model-based engineering (MBE) recognizes models as central in software construction with the possibility of their management in libraries and repositories with proper structuring of their spaces and operations. Due to this success, models (and metamodels) are becoming larger and larger and technics are needed in order to comprehend and exploit them, such as circumscribing sub(meta)models of interest, which is the subject of this paper. Following MBE, there are mainly two ways for circumscribing submodels: only at the model level (by selecting model elements of interest) or through the meta level (by selecting a submetamodeling dimension of interest). In a preceding paper, we deeply studied the first way. Here we concentrate on the second way. Model projection deeply relies on the concepts of submodels and submetamodels with their inclusion qualities for model space structuring and has to be systematically examined from this point of view. It is important to point out that model treatment has to deal with full models (as offered by “off the shelf” libraries) but also with not necessarily well-formed ones, such as unspeciﬁed model chunks, due, for example, to the storage in repositories of incomplete engineering choices or of intermediate results of operations. It is a difficulty to encompass all these forms of models, being well-formed or not, in a homogeneous manner through MBE operations. The operation for “Model projection relative to submetamodeling dimensions” presented here does take this difficulty into account.",
        "keywords": [
            "Model projection",
            "Submodel extraction",
            "Submodel",
            "Submetamodel"
        ],
        "authors": [
            "Bernard Carré\nGilles Vanwormhoudt\nOlivier Caron"
        ],
        "file_path": "data/sosym-all/s10270-023-01116-2.pdf"
    },
    {
        "title": "Operation-based versioning as a foundation for live executable models",
        "submission-date": "2024/03",
        "publication-date": "2024/10",
        "abstract": "Live modeling is the ability to edit an executable model at run-time, and to subsequently continue the execution instead of having to restart it. Few modeling frameworks support this feature. Much of the research concerning live modeling attempts to bring “liveness” to existing modeling languages and environments, which is a complex, and often ad hoc endeavor. We instead argue to build modeling environments on an operation-based versioning foundation, to not only record edit operations, but also execution steps on an explicit run-time model. This reduces the complexity of patching the run-time state with edit operations to a simple merge-operation, while getting powerful features such as collaborative editing and debugging “for free.”",
        "keywords": [
            "Model-driven engineering",
            "Live programming",
            "Live modeling",
            "Versioning"
        ],
        "authors": [
            "Joeri Exelmans",
            "Ciprian Teodorov",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-024-01212-x.pdf"
    },
    {
        "title": "Building European software architecture community: how far have we come?",
        "submission-date": "2013/04",
        "publication-date": "2013/04",
        "abstract": "The text on this page does not contain an explicit abstract.",
        "keywords": [],
        "authors": [
            "Muhammad Ali Babar",
            "Ian Gorton",
            "Flavio Oquendo"
        ],
        "file_path": "data/sosym-all/s10270-013-0339-3.pdf"
    },
    {
        "title": "A domain‑specific modeling milestone",
        "submission-date": "2021/08",
        "publication-date": "2021/08",
        "abstract": "In October 2021, the workshop on Domain-Specific Modeling (held at the SPLASH conference series) celebrates its twentieth anniversary since the first workshop in 2001 (http://​www.​dsmfo​rum.​org/​events/​DSM21/). The history of the workshop can be traced back to the 2000 OOPSLA conference (Minneapolis MN) and a Birds of a Feather (BoF) session that brought together those attendees who were interested in DSM topics. From that original BoF meeting, some of the attendees (Juha-Pekka Tolvanen, Jeff Gray, Steven Kelly, and Joern Bettin) decided that it was time for an official forum for collecting initial research efforts in the DSM area. The first workshop (actually called DSVL—Domain-Specific Visual Languages, and first called “DSM” in 2003) was held in October 2001 in Tampa, FL. That workshop, which almost did not happen due to the proximity of the 9/11 tragedy in the USA, received 20 submissions, from which 14 papers were presented. Details about that first workshop are still available at: http://​www.​dsmfo​rum.​org/​events/​DSVL01/​DSVL01.​html\nAlthough concepts such as metamodeling and other approaches for supporting customized domain-specific modeling languages are commonly discussed today, such was not the case during the formation of the DSM workshop. The early editions of the UML conference were not as receptive to DSM techniques, and the 2001 DSM workshop was the first open forum that welcomed researchers working in the specific area. In fact, it was not until the name change of the 2005 MODELS/UML conference that the modeling community began widespread acceptance of a broad range of DSM approaches over those focused just on UML extension mechanisms (coincidentally, the 2005 MODELS/UML conference had a panel that discussed these differences, called “A DSL Or UML Profile. Which Would You Use?”).\nThe peak period of the DSM workshop was from 2006 to 2008, with an average of nearly 40 participants and 20 paper presentations. As DSM topics became more accepted at other venues, the depth of participation of the DSM workshop waned as the maturity of the area was realized. The DSM workshop also produced three special journal issues, including 5 papers in the Journal of Visual Languages and Computing (June 2004), 6 papers in IEEE Software (July/August 2009) and 6 papers in a SoSyM theme issue (January 2013).\nOver the past 20 years, the DSM workshop witnessed the evolution of the area, with specific observation of the following trends:",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe",
            "Juha-Pekka Tolvanen"
        ],
        "file_path": "data/sosym-all/s10270-021-00921-x.pdf"
    },
    {
        "title": "Quantitative modelling and analysis of BDI agents",
        "submission-date": "2022/06",
        "publication-date": "2023/08",
        "abstract": "Belief–desire–intention (BDI) agents are a popular agent architecture. We extend conceptual agent notation (Can)—a BDI programming language with advanced features such as failure recovery and declarative goals—to include probabilistic action outcomes, e.g. to reﬂect failed actuators, and probabilistic policies, e.g. for probabilistic plan and intention selection. The extension is encoded in Milner’s bigraphs. Through application of our BigraphER tool and the PRISM model checker, the probability of success (intention completion) under different probabilistic outcomes and plan/event/intention selection strategies can be investigated and compared. We present a smart manufacturing use case. A signiﬁcant result is that plan selection has limited effect compared with intention selection. We also see that the impact of action failures can be marginal—even when failure probabilities are large—due to the agent making smarter choices.",
        "keywords": [
            "BDI agents",
            "Quantitative analysis",
            "Bigraphs",
            "Probabilistic modelling",
            "Robotic software"
        ],
        "authors": [
            "Blair Archibald",
            "Muﬀy Calder",
            "Michele Sevegnani",
            "Mengwei Xu"
        ],
        "file_path": "data/sosym-all/s10270-023-01121-5.pdf"
    },
    {
        "title": "Guest editorial to the special issue on “modeling: foundations and applications” (MODELS 2013)",
        "submission-date": "2015/07",
        "publication-date": "2015/11",
        "abstract": "Since its beginnings, the use of models has always been a core principle in Computer Science. Recently, model-based engineering is gaining rapid popularity across various engineering disciplines. The pervasive use of models as the essential artifacts of the development process, and model-driven development of complex systems, has been strengthened by a focus on executable models and automatic transformations supporting the generation of more refined models and implementations. Software models have become industrially accepted best practices in many application areas. Domains like automotive systems and avionics, interactive systems, business engineering, games, and Web-based applications commonly apply a tool-supported, model-based, or model-driven approach toward software development. The potential for early validation and verification, coupled with the generation of production code, has shown to cover a large percentage of implemented functionality with improved productivity and reliability. This increased success of using models in software and systems engineering also opens up new challenges, requiring collaborative research across multiple disciplines, ranging from offering suitable domain-specific modeling concepts to supporting legacy needs through models. The MODELS conference is devoted to model-based development for software and systems engineering, covering all types of modeling languages, methods, tools, and their applications. The 2013 edition of the MODELS conference took place in the “magic” city of Miami, USA, from September 29 to October 4, 2013. The MODELS 2013 conference offered an opportunity for researchers, practitioners, educators, and students to come together, to reflect on and discuss our progress as a community, and to identify the important challenges still to be addressed and overcome. In its sixteenth edition, the MODELS community was challenged to demonstrate the maturity and effectiveness of model-based and model-driven engineering and to explore their limits by investigating new application areas and combinations with other emerging technologies. This challenge resulted in 180 papers submitted to the MODELS 2013 Foundations (130) and Applications Tracks (50). The Foundations Track papers provide significant contributions to the core software modeling body of knowledge in the form of new ideas and results that advance the state of the art. Two categories of Foundations Track papers were invited: technical papers, describing original scientifically rigorous solutions to challenging model-driven development problems, and exploratory papers, describing new, non-conventional modeling research positions or approaches that challenge the status quo and describe solutions that are based on new ways of looking at software modeling problems.",
        "keywords": [],
        "authors": [
            "Ana Moreira",
            "Bernhard Schätz",
            "Peter Clarke",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-015-0500-2.pdf"
    },
    {
        "title": "How does your model represent the system? A note on model ﬁdelity, underspeciﬁcation, and uncertainty",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "Model-driven engineering (MDE) was elaborated over two decades ago based on a very limited set of seminal concepts: system, model, metamodel, and seminal relationships (i.e., representation of, and conforms to), as shown in Fig. 1. Since the early development of MDE, the modeling community has produced a large body of knowledge about conformity. This also allows metamodeling techniques to be used as deﬁnitions of the modeling languages themselves. Beyond a pure conformity relation between model and meta-model, various detailed “typing” relationships have been explored to support a better decoupling and modularity of models, and thus a certain degree of reuse. Some ﬂexibility in conformity has been explored to support different levels of formality during modeling activities, hence supporting the process from informal to formal modeling. However, very little literature can be found on the representativity of the model with regard to the system under study, raising the question: “How well does a given model actually represent the system?” Indeed, the core modeling activity has been conceptually deﬁned, e.g., by Stachowiack in terms of characteristics of a model: a model is an abstraction of an original (i.e., the “system”), and fulﬁll a speciﬁc purpose with regard to this original. Several relaxations have been identiﬁed. For example, the system might still be under construction and does not exist yet, a model may have many purposes (dependent on the different stakeholders) and a model may be composed of other models. However, all of these techniques do not provide a formal way to capture and reason about the actual representativity of the models built. In this editorial, we discuss some ways of qualifying the representativity relationship in terms of ﬁdelity and under-speciﬁcation, leading to some degrees of (possibly explicit) uncertainty.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "JeffGray",
            "Jean-Marc Jézéquel",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01210-z.pdf"
    },
    {
        "title": "Metric propositional neighborhood logics on natural numbers",
        "submission-date": "2010/06",
        "publication-date": "2011/02",
        "abstract": "Interval logics formalize temporal reasoning on interval structures over linearly (or partially) ordered domains, where time intervals are the primitive ontological entities and truth of formulae is deﬁned relative to time intervals, rather than time points. In this paper, we introduce and study Metric Propositional Neighborhood Logic (MPNL) over natural numbers. MPNL features two modalities referring, respectively, to an interval that is “met by” the current one and to an interval that “meets” the current one, plus an inﬁnite set of length constraints, regarded as atomic propositions, to constrain the length of intervals. We argue that MPNL can be successfully used in different areas of computer science to combine qualitative and quantitative interval temporal reasoning, thus providing a viable alternative to well-established logical frameworks such as Duration Calculus. We show that MPNL is decidable in double exponential time and expressively complete with respect to a well-deﬁned sub-fragment of the two-variable fragment FO2[N, =, <, s] of first-order logic for linear orders with successor function, interpreted over natural numbers. Moreover, we show that MPNL can be extended in a natural way to cover full FO2[N, =, <, s], but, unexpectedly, the latter (and hence the former) turns out to be undecidable.",
        "keywords": [
            "Metric temporal logic",
            "Interval logic",
            "Decidability",
            "Complexity",
            "Expressiveness"
        ],
        "authors": [
            "Davide Bresolin",
            "Dario Della Monica",
            "Valentin Goranko",
            "Angelo Montanari",
            "Guido Sciavicco"
        ],
        "file_path": "data/sosym-all/s10270-011-0195-y.pdf"
    },
    {
        "title": "Producing robust use case diagrams via reverse engineering of use case descriptions",
        "submission-date": "2006/03",
        "publication-date": "2007/01",
        "abstract": "In a use case driven development process,\na use case model is utilized by a development team to\nconstruct an object-oriented software system. The large\ndegree of informality in use case models, coupled with\nthe fact that use case models directly affect the quality of\nall aspects of the development process, is a very danger-\nous combination. Naturally, informal use case models\nare prone to contain problems, which lead to the injec-\ntion of defects at a very early stage in the development\ncycle. In this paper, we propose a structure that will aid\nthe detection and elimination of potential defects caused\nby inconsistencies present in use case models. The struc-\nture contains a small set of formal constructs that will\nallow use case models to be machine readable while\nretaining their readability by retaining a large degree\nof unstructured natural language. In this paper we also\npropose a process which utilizes the structured use cases\nto systematically generate their corresponding use case\ndiagrams and vice versa. Finally a tool provides support\nfor the new structure and the new process. To demon-\nstrate the feasibility of this approach, a simple study is\nconducted using a mock online hockey store system.",
        "keywords": [],
        "authors": [
            "Mohamed El-Attar",
            "James Miller"
        ],
        "file_path": "data/sosym-all/s10270-006-0039-3.pdf"
    },
    {
        "title": "Guided architecture trade space exploration: fusing model-based engineering and design by shopping",
        "submission-date": "2020/03",
        "publication-date": "2021/06",
        "abstract": "Advances in model-based system engineering have greatly increased the predictive power of models and the analyses that can be run on them. At the same time, designs have become more modular and component-based. It can be difﬁcult to manually explore all possible system designs due to the sheer number of possible architectures and conﬁgurations; trade space exploration has arisen as a solution to this challenge. In this work, we present a new software tool: the Guided Architecture Trade Space Explorer (GATSE), which connects an existing model-based engineering language (AADL) and modeling environment (OSATE) to an existing trade space exploration tool (ATSV). GATSE, AADL, and OSATE are all designed to be easily extended by users, which enables relatively straightforward domain-customizations. ATSV, combined with these customizations, lets system designers “shop” for candidate architectures and interactively explore the architectural trade space according to any quantiﬁable quality attribute or system characteristic. We evaluate GATSE according to an established framework for variable system architectures, and demonstrate its use on an avionics subsystem.",
        "keywords": [
            "Design space exploration",
            "Search-based system engineering",
            "Model-based engineering",
            "Guided optimization",
            "Architecture analysis and design language (AADL)",
            "Open source AADL tool environment (OSATE)",
            "ARL trade space visualizer (ATSV)"
        ],
        "authors": [
            "Sam Procter\nLutz Wrage"
        ],
        "file_path": "data/sosym-all/s10270-021-00889-8.pdf"
    },
    {
        "title": "A framework for deadlock detection in core ABS",
        "submission-date": "2013/11",
        "publication-date": "2015/01",
        "abstract": "We present a framework for statically detecting deadlocks in a concurrent object-oriented language with asynchronous method calls and cooperative scheduling of method activations. Since this language features recursion and dynamic resource creation, deadlock detection is extremely complex and state-of-the-art solutions either give imprecise answers or do not scale. In order to augment precision and scalability, we propose a modular framework that allows several techniques to be combined. The basic component of the framework is a front-end inference algorithm that extracts abstract behavioral descriptions of methods, called contracts, which retain resource dependency information. This component is integrated with a number of possible different back-ends that analyze contracts and derive deadlock information. As a proof-of-concept, we discuss two such back-ends: (1) an evaluator that computes a ﬁxpoint semantics and (2) an evaluator using abstract model checking.",
        "keywords": [
            "Type inference",
            "Deadlock analysis",
            "Asynchronous method invocation",
            "Concurrent object groups"
        ],
        "authors": [
            "Elena Giachino",
            "Cosimo Laneve",
            "Michael Lienhardt"
        ],
        "file_path": "data/sosym-all/s10270-014-0444-y.pdf"
    },
    {
        "title": "Removing redundant multiplicity constraints in UML class models",
        "submission-date": "2017/10",
        "publication-date": "2018/09",
        "abstract": "Models are central for the development and management of complex systems. In order to be useful along the entire life cycle of software they must provide reliable support and enable automatic management. For that purpose, they must be precise, consistent, correct, and be subject to stringent quality veriﬁcation and control criteria. Model automation calls for deep formal study of models, so that tools can provide inclusive support to users. This paper deals with optimization of multiplicity constraints in Class Models, i.e., models that provide abstraction on the static structure of software. The paper introduces an in-depth analysis of redundancy of multiplicity constraints, i.e., multiplicities that cannot be realized in any legal instance. The analysis includes: (1) a formal study of gaps of redundancies in multiplicity intervals; (2) algorithmic and rule-based methods for removing redundancies in multiplicity constraints; (3) a formal study of completeness of the algorithmic procedures, with respect to most UML class model constraints. The algorithmic procedures are implemented in our FiniteSatUSE tool. To the best of our knowledge, there is no previous study of properties of multiplicity redundancy, no completeness analysis of tightening methods, and no systematic study of these features with respect to most UML class model constraints.",
        "keywords": [
            "Class models",
            "MBSE",
            "Formal semantics",
            "Multiplicity constraints",
            "Correctness",
            "Quality",
            "Redundancy",
            "Veriﬁcation",
            "Boundary tight",
            "Tightening methods",
            "Model optimization",
            "Multiplicity tightening",
            "Tightening rules"
        ],
        "authors": [
            "Mira Balaban",
            "Azzam Maraee"
        ],
        "file_path": "data/sosym-all/s10270-018-0696-z.pdf"
    },
    {
        "title": "On the relationship between modeling and programming languages",
        "submission-date": "2012/01",
        "publication-date": "2012/01",
        "abstract": "At the MODELS 2011 conference in New Zealand, Colin Atkinson, held a panel on “When will Code become Irrele-vant?”. The panelists were requested to answer the following six questions:\n1. Do you agree with the implied assumption in the panel abstract that “code” is different to “models, at least in the minds of practicing software engineers?\n2. If so, do you agree with the premise that code is still the primary artifact, or at least still an important artifact, in software engineering?\n3. If so, do you think it matters?\n4. If so, when if ever, will the situation change and what will it take to make it happen?\n5. What can the research community do to help bring this about?\n6. What will the future of modeling look like? Does modeling have a future as an independent activity or will it fade away in importance?\nSoftware developers create and use models for a variety of purposes. For example, the following are three common forms of uses:\n• Models as sketches: Developers ﬁnd it useful to sketch descriptions of requirements, design or deployment concepts on whiteboards or paper when discussing their ideas with other developers or customer representatives. This use of models supports exploratory development of concepts and ideas that may or may not later ﬁnd their way into more formal models or implementations.\n• Models as analysis artifacts: Developers build analyzable models to check speciﬁed properties (e.g., consistency and satisﬁability properties), to predict implementation qualities (e.g., performance), or to simulate implemented behavior. Included in this category of models are formal, non-executable speciﬁcations that can be statically analyzed, and executable models that support more dynamic forms of analysis. These models typically contain only the information needed to analyze target properties, and thus may not include information that is needed to generate full implementations.\n• Models as the basis for code generation or synthesis of software artifacts: Models can be built for the purpose of generating implementations, test cases, deployment or software conﬁguration scripts, or other software artifacts. These models must contain all necessary information in a form that allow a generator to mechanically synthesize software artifacts.",
        "keywords": [],
        "authors": [
            "Bernhard Rumpe",
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-011-0224-x.pdf"
    },
    {
        "title": "UML – the Good, the Bad or the Ugly? Perspectives from a panel of experts",
        "submission-date": "2005/01",
        "publication-date": "2005/01",
        "abstract": "This paper presents a panel discussion on the health of UML (Unified Modeling Language) version 2.0, featuring perspectives from experts involved in its creation. The panelists discuss the pros and cons of UML2.0, its usability, semantics, and its potential as a foundation for Model Driven Architecture (MDA).",
        "keywords": [],
        "authors": [
            "Brian Henderson-Sellers",
            "Steve Cook",
            "Steve Mellor",
            "Joaquin Miller",
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-004-0076-8.pdf"
    },
    {
        "title": "A proﬁle and tool for modelling safety information with design information in SysML",
        "submission-date": "2013/05",
        "publication-date": "2014/02",
        "abstract": "Communication both between development teams and between individual developers is a common source of safety-related faults in safety–critical system design. Communication between experts in different ﬁelds can be particularly challenging due to gaps in assumed knowledge, vocabulary and understanding. Faults caused by communication failures must be removed once found, which can be expensive if they are found late in the development process. Aiding communication earlier in development can reduce faults and costs. Modelling languages for design have been shown through practical experience to improve communication through better information presentation and increased information consistency. In this paper, we describe a SysML proﬁle designed for modelling the safety-related concerns of a system. The proﬁle models common safety concepts from safety standards and safety analysis techniques integrated withsystemdesigninformation.Wedemonstratethatthepro-ﬁle is capable of modelling the concepts through examples. Wealsoshowtheuseofsupportingtoolstoaidtheapplication of the proﬁle through analysis of the model and generation of reports presenting safety information in formats appropriate to the target reader. Through increased traceability and inte- gration, the proﬁle allows for greater consistency between safety information and system design information and can aid in communicating that information to stakeholders.",
        "keywords": [
            "SysML",
            "UML/SysML proﬁle",
            "Safety analysis",
            "System design"
        ],
        "authors": [
            "Geoffrey Biggs",
            "Takeshi Sakamoto",
            "Tetsuo Kotoku"
        ],
        "file_path": "data/sosym-all/s10270-014-0400-x.pdf"
    },
    {
        "title": "Editorial to the theme section on model-driven engineering for digital twins",
        "submission-date": "2025/03",
        "publication-date": "2025/04",
        "abstract": "Digital twins are virtual, digital representations of real-world systems or objects that are kept in sync with data from the real-world system and can be used for advanced analysis, predictive exploration, control, and (semi-) automated transformation of these systems and objects [1]. Digital twins promise tremendous potential in domains such as automotive, avionics, manufacturing, and healthcare.\nDigital twins are based on models representing aspects of a real system. Whether the model is the basis of what-if simulation, or more sophisticated control and adaptation, digital twins present an interesting research challenge for the modeling community in terms of quality-based development and deployment. Digital twin engineering is currently ad hoc [2], which is a challenge for quality-controlled development, deployment, and operation. Hence, MDE is crucial to leveraging the potential of digital twins fully and systematically.\nThis theme section of SoSyM provides a platform for digital twin researchers and practitioners to report emerging results, evidence of success and good practice, and to outline roadmaps to deliver high-quality digital twins using model-driven engineering.",
        "keywords": [],
        "authors": [
            "Djamel Eddine Khelladi",
            "Tony Clark",
            "Vinay Kulkarni",
            "Steffen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-025-01288-z.pdf"
    },
    {
        "title": "Meta-modelling and graph grammars for multi-paradigm modelling in AToM3",
        "submission-date": "2003/01",
        "publication-date": "2004/04",
        "abstract": "This paper presents the combined use of meta-modelling and graph grammars for the generation of visual modelling tools for simulation formalisms. In meta-modelling, formalisms are described at a meta-level. This information is used by a meta-model processor to generate modelling tools for the described formalisms. We combine meta-modelling with graph grammars to extend the model manipulation capabilities of the gen-erated modelling tools: edit, simulate, transform into another formalism, optimize and generate code. We store all (meta-)models as graphs, and thus, express model ma-nipulations as graph grammars. We present the design and implementation of these concepts in AToM3 (A Tool for Multi-formalism, Meta-Modelling). AToM3 supports modelling of complex systems using diﬀerent formalisms, all meta-modelled in their own right. Models in diﬀerent formalisms may be transformed into a single common formalism for further processing. These transformations are speciﬁed by graph grammars. Mosterman and Vangheluwe [18] in-troduced the term multi-paradigm modelling to denote the combination of multiple formalisms, multiple ab-straction levels, and meta-modelling. As an example of multi-paradigm modelling we present a meta-model for the Object-Oriented Continuous Simulation Language OOCSMP, in which we combine ideas from UML class diagrams (to express the OOCSMP model structure), Causal Block Diagrams (CBDs), and Statecharts (to specify the methods of the OOCSMP classes). A graph grammar is able to generate OOCSMP code, and then a compiler for this language (C-OOL) generates Java app-lets for the simulation execution.",
        "keywords": [
            "Meta-modelling",
            "Multi-formalism",
            "Multi-paradigm modelling",
            "Graph grammars",
            "Model transformation",
            "Code generation",
            "Causal block diagrams",
            "Statecharts",
            "AToM3",
            "OOCSMP"
        ],
        "authors": [
            "Juan de Lara",
            "Hans Vangheluwe",
            "Manuel Alfonseca"
        ],
        "file_path": "data/sosym-all/s10270-003-0047-5.pdf"
    },
    {
        "title": "Applying design patterns in the search-based optimization of software product line architectures",
        "submission-date": "2016/08",
        "publication-date": "2017/08",
        "abstract": "The design of the product line architecture (PLA) is a difﬁcult activity that can beneﬁt from the application of design patterns and from the use of a search-based optimization approach, which is generally guided by different objectives related, for instance, to cohesion, coupling and PLA extensibility. The use of design patterns for PLAs is a recent research ﬁeld, not completely explored yet. Some works apply the patterns manually and for a speciﬁc domain. Approaches to search-based PLA design do not consider the usage of these patterns. To allow such use, this paper introduces a mutation operator named “Pattern-Driven Mutation Operator” that includes methods to automatically identify suitable scopes and apply the patterns Strategy, Bridge and Mediator with the search-based approach multi-objective optimization approach for PLA. A metamodel is proposed to represent and identify suitable scopes to receive each one of the patterns, avoiding the introduction of architectural anomalies. Empirical results are also presented, showing evidences that the use of the proposed operator produces a greater diversity of solutions and improves the quality of the PLAs obtained in the search-based optimization process, regarding the values of software metrics.",
        "keywords": [
            "Design pattern",
            "Search-based software engineering",
            "Software product line architecture"
        ],
        "authors": [
            "Giovani Guizzo",
            "Thelma Elita Colanzi",
            "Silvia Regina Vergilio"
        ],
        "file_path": "data/sosym-all/s10270-017-0614-9.pdf"
    },
    {
        "title": "A security requirements modelling language for cloud computing environments",
        "submission-date": "2017/09",
        "publication-date": "2019/09",
        "abstract": "This paper presents a novel security modelling language and a set of original analysis techniques, for capturing and analysing security requirements for cloud computing environments. The novelty of the language lies in the integration of concepts from cloud computing, with concepts from security and goal-oriented requirements engineering to elicit, model and analyse security requirements for cloud infrastructures. We then propose three analysis techniques, which support an automated process where given a model of a cloud computing system, developed with the proposed language, will enhance the model with new security knowledge, for example threats and vulnerabilities, mitigation strategies and assets and actor responsibilities. This is, to the best of our knowledge, the first attempt in the literature to develop a language for cloud computing security modelling and analysis, based on such integration, and support it with a set of automated techniques that enhanced the stakeholder-created models with security knowledge. The proposed modelling language and techniques are illustrated through walking examples and a case study based on our work in the VisiOn European project.",
        "keywords": [
            "Security modelling language",
            "Secure Tropos",
            "Cloud computing security",
            "Cloud threat analysis"
        ],
        "authors": [
            "Haralambos Mouratidis",
            "Shaun Shei",
            "Aidan Delaney"
        ],
        "file_path": "data/sosym-all/s10270-019-00747-8.pdf"
    },
    {
        "title": "Repeat, reorder, rephrase: data augmentation for process information extraction",
        "submission-date": "2024/11",
        "publication-date": "2025/06",
        "abstract": "Automatic retrieval of formal business process models from their natural language descriptions is a well-established way to facilitate the time- and cost-intensive modeling procedure. Yet, a lack of data usable for developing and training new retrieval methods is impeding progress in this ﬁeld of research. This issue can be overcome by either using methods less reliant on high-quality data, such as large language models, or creating bigger datasets. The latter is often preferable in the context of business process modeling, especially when internal workﬂows of organizations have to be treated conﬁdentially. It is the more data-intensive solution, though, which is costly. Data augmentation techniques aim to improve both quality and quantity of existing datasets, by deliberate perturbations resulting in new, synthetic data. In this article, we present a collection of data augmentation techniques, which are speciﬁcally selected for the task of improving data quality in the context of process information extraction. We show why data augmentation techniques from the wider ﬁeld of natural language processing are often not applicable to process information extraction, and how the resulting data differ in terms of linguistic variety, structure, and feature space coverage. In our experiments, data augmentation results in an absolute improvement in the F1 measure of 5.7% for extracting process-relevant entities from text and 4.5% for extracting relations between those entities. We make all code available at https://github.com/JulianNeuberger/pet-data-augmentation and results for our experiments at https://zenodo.org/doi/10.5281/zenodo.10941423.",
        "keywords": [
            "Business Process Extraction",
            "Data Augmentation",
            "Natural Language Processing"
        ],
        "authors": [
            "Julian Neuberger",
            "Lars Ackermann",
            "Stefan Jablonski"
        ],
        "file_path": "data/sosym-all/s10270-025-01305-1.pdf"
    },
    {
        "title": "ReFlO: an interactive tool for pipe-and-ﬁlter domain speciﬁcation and program generation",
        "submission-date": "2013/03",
        "publication-date": "2014/03",
        "abstract": "ReFlO is a framework and interactive tool to record and systematize domain knowledge used by experts to derive complex pipe-and-ﬁlter (PnF) applications. Domain knowledge is encoded as transformations that alter PnF graphs by reﬁnement (adding more details), ﬂattening (removing modular boundaries), and optimization (substitut-ing inefﬁcient PnF graphs with more efﬁcient ones). All three kinds of transformations arise in reverse-engineering legacy PnF applications. We present the conceptual foundation and tool capabilities of ReFlO, illustrate how parallel PnF appli-cations are designed and generated, and how domain-speciﬁc libraries of transformations are developed.",
        "keywords": [
            "MDE",
            "Tools",
            "Software architectures",
            "Design by transformation",
            "Reﬁnement",
            "Optimization",
            "Graph transformations"
        ],
        "authors": [
            "Rui C. Gonçalves",
            "Don Batory",
            "João L. Sobral"
        ],
        "file_path": "data/sosym-all/s10270-014-0403-7.pdf"
    },
    {
        "title": "A pattern-based approach for improving model quality",
        "submission-date": "2013/04",
        "publication-date": "2014/01",
        "abstract": "UML class diagrams play a central role in modeling activities, and it is essential that class diagrams keep their high quality all along a product life cycle. Correctness problems in class diagrams are mainly caused by complex interactions among class-diagram constraints. Detection, identification, and repair of such problems require background training. In order to improve modelers’ capabilities in these directions, we have constructed a catalog of anti-patterns of correctness and quality problems in class diagrams, where an anti-pattern analyzes a typical constraint interaction that causes a correctness or a quality problem and suggests possible repairs. This paper argues that exposure to correctness anti-patterns improves modeling capabilities. The paper introduces the catalog and its pattern language, and describes experiments that test the impact of awareness to modeling problems in class diagrams (via concrete examples and anti-patterns) on the analysis capabilities of modelers. The experiments show that increased awareness implies increased identification. The improvement is remarkably noticed when the awareness is stimulated by anti-patterns, rather than by concrete examples.",
        "keywords": [
            "Anti-patterns",
            "Pattern languages",
            "Pattern awareness",
            "Experiments",
            "Modeling problems",
            "Analysis capabilities",
            "Software engineering education",
            "Correctness",
            "Quality"
        ],
        "authors": [
            "Mira Balaban",
            "Azzam Maraee",
            "Arnon Sturm",
            "Pavel Jelnov"
        ],
        "file_path": "data/sosym-all/s10270-013-0390-0.pdf"
    },
    {
        "title": "Graphic modeling in Distributed Autonomous and Asynchronous Automata ­(DA3)",
        "submission-date": "2020/04",
        "publication-date": "2021/11",
        "abstract": "Automated verification of distributed systems becomes very important in distributed computing. The graphical insight into the system in the early and late stages of the project is essential. In the design phase, the visual input helps to articulate the collaborative distributed components clearly. The formal verification gives evidence of correctness or malfunction, but in the latter case, graphical simulation of counterexample helps for better understanding design errors. For these purposes, we invented Distributed Autonomous and Asynchronous Automata ­(DA3), which have the same semantics as the formal verification base—Integrated Model of Distributed Systems (IMDS). The IMDS model reflects the natural characteristics of distributed systems: unicasting, locality, autonomy, and asynchrony. Distributed automata have all of these features because they share the same semantics as IMDS. In formalism, the unified system definition has two views: the server view of the cooperating distributed nodes and the agent view of the migrating agents performing distributed computations. The automata have two formally equivalent forms that reflect two views: Server ­DA3 for observing servers exchanging messages, and Agent ­DA3 for tracking agents, which visit individual servers in their progress of distributed calculations. We present the ­DA3 formulation based on the IMDS formalism and their application to design and verify distributed systems in the Dedan environment. ­DA3 formalism is compared with other concepts of distributed automata known from the literature.",
        "keywords": [
            "Distributed systems",
            "Distributed system modeling",
            "Distributed automata",
            "Graphic modeling",
            "Formal methods"
        ],
        "authors": [
            "Wiktor B. Daszczuk"
        ],
        "file_path": "data/sosym-all/s10270-021-00917-7.pdf"
    },
    {
        "title": "A graph-based algorithm for consistency maintenance in incremental and interactive integration tools",
        "submission-date": "2005/02",
        "publication-date": "2007/01",
        "abstract": "Development processes in engineering disciplines are inherently complex. Throughout the development process, the system to be built is mod-eled from different perspectives, on different levels of abstraction, and with different intents. Since state-of-the-art development processes are highly incremental and iterative, models of the system are not constructed in one shot; rather, they are extended and improved repeatedly. Furthermore, models are related by man-ifold dependencies and need to be maintained mutu-ally consistent with respect to these dependencies. Thus, tools are urgently needed which assist developers in maintaining consistency between inter-dependent and evolving models. These tools have to operate incremen-tally, i.e., they have to propagate changes performed on one model into related models which are affected by these changes. In addition, they need to support user interactions in settings where the effects of changes can-not be determined automatically and deterministically. We present an algorithm for incremental and interac-tive consistency maintenance which meets these require-ments. The algorithm is based on graphs, which are used as the data model for representing the models to be inte-grated, and graph transformation rules, which describe the modiﬁcations of the graphs to be performed on a high level of abstraction.",
        "keywords": [
            "Incremental consistency maintenance",
            "Graph transformation",
            "Triple graph grammars"
        ],
        "authors": [
            "Simon M. Becker",
            "Sebastian Herold",
            "Sebastian Lohmann",
            "Bernhard Westfechtel"
        ],
        "file_path": "data/sosym-all/s10270-006-0045-5.pdf"
    },
    {
        "title": "Managing the evolution of data-intensive Web applications by model-driven techniques",
        "submission-date": "2009/11",
        "publication-date": "2011/02",
        "abstract": "The adoption of Model-Driven Engineering (MDE) in the development of Web Applications permitted to decouple the functional description of applications from the underlying implementation platform. This is of paramount relevance for preserving the intellectual property encoded in models and making applications, languages and processes resilient to technological changes. This paper proposes a model-driven approach for supporting the migration and evolution of data-intensive Web applications. In particular, model differencing techniques are considered to realize a migration facility capable of detecting the modiﬁcations a model underwent during its lifecycle and to automatically derive from them the programs that are capable of migrating/adapting also those aspects which are not directly derivable from the source models, as for instance the data persistently stored in a database and the page layout usually written usinggraphictemplates.Theapproachisvalidatedbyconsidering applications described with the beContent and WebML modeling languages.",
        "keywords": [
            "Migration",
            "Data-intensive Web applications",
            "Model differencing",
            "Coupled evolution",
            "Ecore"
        ],
        "authors": [
            "Antonio Cicchetti",
            "Davide Di Ruscio",
            "Ludovico Iovino",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-011-0193-0.pdf"
    },
    {
        "title": "Modeling the Complex Living World",
        "submission-date": "2006/03",
        "publication-date": "2006/03",
        "abstract": "In the software development domain, models of software systems are typically used to understand and predict properties of the system or to produce implementations. A model is normally created before the software system is implemented and, in many cases, acts as a specification of behavioral and structural properties that must be present in implemented systems. In theory, implementations can be generated from formal models in a manner that ensures their correctness with respect to the models. In other non-engineering disciplines, models are used differently. For example, physicists use models primarily to understand and explain phenomena that occur in the world around them. They build models that are consistent with their observations of the phenomena, and they test the models to determine their fidelity and the circumstances under which the models make accurate predictions. Unlike software models, formal models of physical systems typically describe continuous behavior (with very few non-continuous disruptions) and therefore use concepts from continuous mathematics (e.g., differential equations). It may seem that scientists in non-engineering disciplines have very little use for software modeling techniques, but the complex problems that are currently tackled in the Life Sciences area indicate otherwise. Scientists in the Life Sciences tackle highly-complex problems that involve study of organs, cells, proteins and organic molecules that exhibit continuous as well as discrete, non-deterministic behavior that can be described in terms of state transitions. Bio-technological models describe state-based phenomena and are primarily used to understand the phenomena and to predict behavior in a variety of situations. Accurate models pave the way for the engineering of medicines and for the development of sophisticated “biological tools” to further improve our lives.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-006-0008-x.pdf"
    },
    {
        "title": "Modeling and veriﬁcation of a telecommunication application using live sequence charts and the Play-Engine tool",
        "submission-date": "2005/12",
        "publication-date": "2008/01",
        "abstract": "We apply the scenario-based approach to modeling, via the language of live sequence charts (LSCs) and the Play-Engine tool to a real-world complex telecommunication service, Depannage. It allows a user to call for help from a doctor, the ﬁre brigade, a car maintenance service, etc. These kinds of services are built on top of an embedded platform, using both new and existing service components, and their complexity stems from their distributed architecture, the various time constraints they entail, and their rapidly evolving underlying systems. A well known problem in this class of telecommunication applications is that of feature interaction, whereby a new feature might cause problems in the execution of existing features. Our approach provides a methodology for high-level modeling of telecommunication applications that can help in detecting feature interaction at early development stages. We exhibit the results of applying the methodology to the speciﬁcation, animation and formal veriﬁcation of the Depannage service.",
        "keywords": [
            "Live sequence charts (LSCs)",
            "Requirements engineering",
            "Veriﬁcation",
            "Telecommunication"
        ],
        "authors": [
            "Pierre Combes",
            "David Harel",
            "Hillel Kugler"
        ],
        "file_path": "data/sosym-all/s10270-007-0069-5.pdf"
    },
    {
        "title": "A model-driven runtime environment for Web applications",
        "submission-date": "2004/05",
        "publication-date": "2005/06",
        "abstract": "Abstract A large part of software development these days\ndeals with building so-called Web applications. Many of\nthese applications are database-powered and exhibit a page\nlayout and navigational structure that is close to the class\nstructure of the entities being managed by the system.\nAlso, there is often only limited application-speciﬁc busi-\nness logic. This makes the usual three-tier architectural ap-\nproach unappealing, because it results in a lot of unneces-\nsary overhead. One possible solution to this problem is the\nuse of model-driven architecture (MDA). A simple platform-\nindependent domain model describing only the entity struc-\nture of interest could be transformed into a platform-speciﬁc\nmodel that incorporates a persistence mechanism and a user\ninterface. Yet, this raises a number of additional problems\ncaused by the one-way, multi-transformational nature of the\nMDA process. To cope with these problems, the authors pro-\npose the notion of a model-driven runtime (MDR) environ-\nment that is able to execute a platform-independent model\nfor a speciﬁc purpose instead of transforming it. The pa-\nper explains the concepts of an MDR that interprets OCL-\nannotated class diagrams and state machines to realize Web\napplications. It shows the authors’ implementation of the ap-\nproach, the Infolayer system, which is already used by a\nnumber of applications. Experiences from these applications\nare described, and the approach is compared to others.",
        "keywords": [
            "UML",
            "OCL",
            "Action semantics",
            "MDA",
            "Web applications",
            "UML virtual machnies"
        ],
        "authors": [
            "Stefan Haustein",
            "Joerg Pleumann"
        ],
        "file_path": "data/sosym-all/s10270-005-0093-2.pdf"
    },
    {
        "title": "Dealing with forward and backward jumps in workﬂow management systems",
        "submission-date": "2002/10",
        "publication-date": "2003/02",
        "abstract": "Workﬂow management systems (WfMS) offer a promising technology for the realization of process-centered application systems. A deﬁciency of existing WfMS is their inadequate support for dealing with exceptional deviations from the standard procedure. In the ADEPT project, therefore, we have developed advanced concepts for workﬂow modeling and execution, which aim at the increase of ﬂexibility in WfMS. On the one hand we allow workﬂow designers to model exceptional execution paths already at buildtime provided that these deviations are known in advance. On the other hand authorized users may dynamically deviate from the pre-modeled workﬂow at runtime as well in order to deal with unforeseen events. In this paper, we focus on forward and backward jumps needed in this context. We describe sophisticated modeling concepts for capturing deviations in workﬂow models already at buildtime, and we show how forward and backward jumps (of diﬀerent semantics) can be correctly applied in an ad-hoc manner during runtime as well. We work out basic requirements, facilities, and limitations arising in this context. Our experiences with applications from diﬀerent domains have shown that the developed concepts will form a key part of process ﬂexibility in process-centered information systems.",
        "keywords": [
            "Workﬂow management",
            "Adaptive workﬂow",
            "Exception handling",
            "Forward/backward jump"
        ],
        "authors": [
            "Manfred Reichert",
            "Peter Dadam",
            "Thomas Bauer"
        ],
        "file_path": "data/sosym-all/s10270-003-0018-x.pdf"
    },
    {
        "title": "VPML: an approach to detect design patterns of MOF-based modeling languages",
        "submission-date": "2012/05",
        "publication-date": "2013/03",
        "abstract": "A design pattern is a recurring and well-understooddesignfragment.Inamodel-drivenengineeringmethodology, detecting occurrences of design patterns supports the activities of model comprehension and maintenance. With the recent explosion of domain-speciﬁc modeling languages, each with its own syntax and semantics, there has been a corresponding explosion in approaches to detecting design patterns that are so much tailored to those many languages that they are difﬁcult to reuse. This makes developing generic analysis tools extremely hard. Such a generic tool is however desirable to reduce the learning curve for pattern design-ers as they specify patterns for different languages used to model different aspects of a system. In this paper, we propose a uniﬁed approach to detecting design patterns of MOF-based modeling languages. MOF is increasingly used to deﬁne modeling languages, including UML and BPMN. In our approach, a pattern is modeled with a Visual Pattern Modeling Language and mapped to a corresponding QVT-Relations transformation. Such a transformation runs over an input model where pattern occurrences are to be detected and reports those occurrences in a result model. The approach is prototyped on Eclipse and validated in two large case studies that involve detecting design patterns—speciﬁcally a subset of GoF patterns in a UML model and a subset of Control Flow patterns in a BPMN model. Results show that the approach is adequate for modeling complex design patterns for MOF-based modeling languages and detecting their occurrences with high accuracy and performance.",
        "keywords": [
            "Design pattern",
            "Modeling",
            "MOF",
            "UML",
            "BPMN",
            "QVT",
            "GoF",
            "VPML"
        ],
        "authors": [
            "Maged Elaasar",
            "Lionel C. Briand",
            "Yvan Labiche"
        ],
        "file_path": "data/sosym-all/s10270-013-0325-9.pdf"
    },
    {
        "title": "Integrated model-driven development of self-adaptive user interfaces",
        "submission-date": "2019/02",
        "publication-date": "2020/01",
        "abstract": "Modern user interfaces (UIs) are increasingly expected to be plastic, in the sense that they retain a constant level of usability, even when subjected to context changes at runtime. Self-adaptive user interfaces (SAUIs) have been promoted as a solution for context variability due to their ability to automatically adapt to the context-of-use at runtime. The development of SAUIs is a challenging and complex task as additional aspects like context management and UI adaptation have to be covered. In classical model-driven UI development approaches, these aspects are not fully integrated and hence introduce additional complexity as they represent crosscutting concerns. In this paper, we present an integrated model-driven development approach where a classical model-driven development of UIs is coupled with a model-driven development of context-of-use and UI adaptation rules. We base our approach on the core UI modeling language IFML and introduce new modeling languages for context-of-use (ContextML) and UI adaptation rules (AdaptML). The generated UI code, based on the IFML model, is coupled with the context and adaptation services, generated from the ContextML and AdaptML model, respectively. The integration of the generated artifacts, namely UI code, context, and adaptation services in an overall rule-based execution environment, enables runtime UI adaptation. The beneﬁt of our approach is demonstrated by two case studies, showing the development of SAUIs for different application scenarios and a usability study which has been conducted to analyze end-user satisfaction of SAUIs.",
        "keywords": [
            "Model-driven UI development",
            "UI adaptation",
            "Self-adaptive UIs",
            "Context-aware applications"
        ],
        "authors": [
            "Enes Yigitbas",
            "Ivan Jovanovikj",
            "Kai Biermeier",
            "Stefan Sauer",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-020-00777-7.pdf"
    },
    {
        "title": "Correction: MBFair: a model-based veriﬁcation methodology for detecting violations of individual fairness",
        "submission-date": "2024/06",
        "publication-date": "2025/01",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Qusai Ramadan",
            "Marco Konersmann",
            "Amir Shayan Ahmadian",
            "Jan Jürjens",
            "Steffen Staab"
        ],
        "file_path": "data/sosym-all/s10270-024-01262-1.pdf"
    },
    {
        "title": "Graph and model transformation tools for model migration\nEmpirical results from the transformation tool contest",
        "submission-date": "2011/04",
        "publication-date": "2012/04",
        "abstract": "We describe the results of the Transformation Tool Contest 2010 workshop, in which nine graph and model transformation tools were compared for specifying model migration. The model migration problem—migration of UML activity diagrams from version 1.4 to version 2.2—is non-trivial and practically relevant. The solutions have been compared with respect to several criteria: correctness, conciseness, understandability, appropriateness, maturity and support for extensions to the core migration task. We describe in detail the comparison method, and discuss the strengths andweaknesses of thesolutions withaspecial focus on the differences between graph and model transformation for model migration. The comparison results demonstrate tool and language features that strongly impact the efﬁcacy of solutions, such as support for retyping of model elements. The results are used to motivate an agenda for future model migration research (including suggestions for areas in which the tools need to be further improved).",
        "keywords": [
            "Model transformation",
            "Graph transformation",
            "Co-evolution"
        ],
        "authors": [
            "Louis M. Rose",
            "Markus Herrmannsdoerfer",
            "Steffen Mazanek",
            "Pieter Van Gorp",
            "Sebastian Buchwald",
            "Tassilo Horn",
            "Elina Kalnina",
            "Andreas Koch",
            "Kevin Lano",
            "Bernhard Schätz",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-012-0245-0.pdf"
    },
    {
        "title": "Facilitating the migration to the microservice architecture via model-driven reverse engineering and reinforcement learning",
        "submission-date": "2021/03",
        "publication-date": "2022/02",
        "abstract": "The microservice architecture has gained remarkable attention in recent years. Microservices allow developers to implement and deploy independent services, so they are a naturally effective architecture for continuously deployed systems. Because of this, several organizations are undertaking the costly process of manually migrating their traditional software architectures to microservices. The research in this paper aims at facilitating the migration from monolithic software architectures to microservices. We propose a framework which enables software developers/architects to migrate their software systems more efficiently by helping them remodularize the source code of their systems. The framework leverages model-driven reverse engineering to obtain a model of the legacy system and reinforcement learning to propose a mapping of this model toward a set of microservices.",
        "keywords": [
            "Microservice architecture",
            "Reinforcement learning",
            "Model-driven reverse engineering",
            "Migration"
        ],
        "authors": [
            "MohammadHadi Dehghani",
            "Shekoufeh Kolahdouz-Rahimi",
            "Massimo Tisi",
            "Dalila Tamzalit"
        ],
        "file_path": "data/sosym-all/s10270-022-00977-3.pdf"
    },
    {
        "title": "Distributed implementation of message sequence charts",
        "submission-date": "2011/11",
        "publication-date": "2013/06",
        "abstract": "This work revisits the problem of program synthesis from speciﬁcations described by high-level message sequence charts. We first show that in the general case, synthesis by a simple projection on each component of the system allows more behaviors in the implementation than in the specification. We then show that differences arise from loss of ordering among messages and show that behaviors can be preserved by addition of communication controllers that intercept messages to add stamping information before resending them and deliver messages to processes in the order described by the specification.",
        "keywords": [
            "Scenarios",
            "Implementation",
            "Distributed system synthesis"
        ],
        "authors": [
            "Rouwaida Abdallah",
            "Loïc Hélouët",
            "Claude Jard"
        ],
        "file_path": "data/sosym-all/s10270-013-0357-1.pdf"
    },
    {
        "title": "Performance analysis of aspect-oriented UML models",
        "submission-date": "2006/02",
        "publication-date": "2007/04",
        "abstract": "Aspect-Oriented Modeling (AOM) techniques allow software designers to isolate and address separately solutions for crosscutting concerns (such as security, reliability, new functional features, etc.). Current AOM research is concerned not only with the separate expression of concerns and their composition into a complete system model, but also with the analysis of different properties of such models. This paper proposes an approach for analyzing the performance effects of a given aspect on the overall system performance, after the composition of the aspect model with the system’s primary model. Performance analysis of UML models is enabled by the “UML Performance Proﬁle for Schedulability, Performance and Time” (SPT) standardized by OMG, which deﬁnes a set of quantitative performance annotations to be added to a UML model. The ﬁrst step of the proposed approach is to add performance annotations to both the primary and the aspect models. An aspect model is generic at ﬁrst, and therefore its performance annotations must be parameterized. A generic model is converted into a context-speciﬁc aspect model with concrete values assigned to its performance annotations. The latter is composed with the primary model, generating a complete annotated UML model. The composition is performed in both structural and behavioural views. A novel approach for composing activity diagrams based on graph-rewriting concepts is proposed in the paper. The next step is to transform automatically the composed model into a Layered Queueing Network (LQN) performance model, by using techniques developed in pre-vious work. The proposed approach is illustrated with a case study system, whose primary model is enhanced with some security features by using AOM. The performance effects of the security aspect under consideration are analyzed in two design alternatives, by solving and analyzing the LQN model of the composed system.",
        "keywords": [],
        "authors": [
            "Dorina C. Petriu",
            "Hui Shen",
            "Antonino Sabetta"
        ],
        "file_path": "data/sosym-all/s10270-007-0053-0.pdf"
    },
    {
        "title": "Generation of hazard relation diagrams: formalization and tool support",
        "submission-date": "2018/11",
        "publication-date": "2020/06",
        "abstract": "Developing safety-critical, software-intensive embedded systems are characterized by the need to identify hazards and to define hazard-mitigating requirements at the earliest possible stage of development, i.e., during requirements engineering. These hazard-mitigating requirements must be adequate in the sense that they must specify the functionality required by the stakeholders in addition to rendering the system sufficiently safe during operation. The adequacy of hazard-mitigating requirements is determined during requirements validation. Yet, the validation of the adequacy of hazard-mitigating requirements is burdened by the fact that hazards and contextual information about hazards are a work product of safety assessment, and hazard-mitigating requirements are a work product of requirements engineering. These work products are poorly integrated such that during validation, the information needed to determine the adequacy of hazard-mitigating requirements is not avail-able to stakeholders. In consequence, there is the risk that inadequate hazard-mitigating requirements remain covert and the system is falsely considered safe. To alleviate this issue, we have previously proposed (Tenbergen et al., in: Proceedings of the 21st international working conference on requirements engineering: foundation for software quality, pp 17–32, 2015), improved, and evaluated (Tenbergen et al. in Requir Eng J 23(2):291–329, 2018. https​://doi.org/10.1007/s0076​6-017-0267-9) a novel diagram type called “Hazard Relation Diagrams.” In this paper, we present a semiautomated formal approach and tool support for their generation. We make use of a running example to illustrate the concepts.",
        "keywords": [
            "Safety requirements",
            "Hazards",
            "Hazard-mitigating requirements",
            "Safety assessment",
            "Validation",
            "Reviews",
            "Inspections",
            "Mitigation",
            "Adequacy",
            "Modeling",
            "Safety-critical embedded systems",
            "Model-based engineering",
            "Hazard relation diagrams"
        ],
        "authors": [
            "Bastian Tenbergen",
            "Thorsten Weyer"
        ],
        "file_path": "data/sosym-all/s10270-020-00799-1.pdf"
    },
    {
        "title": "On the application of process management and process mining to Industry 4.0",
        "submission-date": "2023/01",
        "publication-date": "2024/04",
        "abstract": "The continuous evolution of digital technologies applied to the more traditional world of industrial automation led to Industry 4.0, which envisions production processes subject to continuous monitoring and able to dynamically respond to changes that can affect the production at any stage (resilient factory). The concept of agility, which is a core element of Industry 4.0, is defined as the ability to quickly react to breaks and quickly adapt to changes. Accurate approaches should be implemented aiming at managing, optimizing and improving production processes. In this vision paper, we show how process management (BPM) can benefit from the availability of raw data from the industrial internet of things to obtain agile processes by using a top-down approach based on automated synthesis and a bottom-up approach based on mining.",
        "keywords": [
            "Industry 4.0",
            "Process management",
            "BPM",
            "Process mining",
            "Process adaptation"
        ],
        "authors": [
            "Flavia Monti",
            "Jerin George Mathew",
            "Francesco Leotta",
            "Agnes Koschmider",
            "Massimo Mecella"
        ],
        "file_path": "data/sosym-all/s10270-024-01175-z.pdf"
    },
    {
        "title": "Mixed-semantics composition of statecharts for the component-based design of reactive systems",
        "submission-date": "2019/06",
        "publication-date": "2020/07",
        "abstract": "The increasing complexity of reactive systems can be mitigated with the use of components and composition languages in model-driven engineering. Designing composition languages is a challenge itself as both practical applicability (support for different composition approaches in various application domains), and precise formal semantics (support for veriﬁcation and code generation) have to be taken into account. In our Gamma Statechart Composition Framework, we designed and implemented a composition language for the synchronous, cascade synchronous and asynchronous composition of statechart-based reactive components. We formalized the semantics of this composition language that provides the basis for generating composition-related Java source code as well as mapping the composite system to a back-end model checker for formal veriﬁcation and model-based test case generation. In this paper, we present the composition language with its formal semantics, putting special emphasis on design decisions related to the language and their effects on veriﬁability and applicability. Furthermore, we demonstrate the design and veriﬁcation functionality of the composition framework by presenting case studies from the cyber-physical system domain.",
        "keywords": [
            "Component-based design",
            "Statecharts",
            "Composition language",
            "Formal semantics",
            "Formal veriﬁcation"
        ],
        "authors": [
            "Bence Graics",
            "Vince Molnár",
            "András Vörös",
            "István Majzik",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-020-00806-5.pdf"
    },
    {
        "title": "Applying static code analysis for domain-speciﬁc languages",
        "submission-date": "2018/09",
        "publication-date": "2019/04",
        "abstract": "The use of code quality control platforms for analysing source code is increasingly gaining attention in the developer com-\nmunity. These platforms are prepared to parse and check source code written in a variety of general-purpose programming\nlanguages. The emergence of domain-speciﬁc languages enables professionals from different areas to develop and describe\nproblem solutions in their disciplines. Thus, source code quality analysis methods and tools can also be applied to software\nartefacts developed with a domain-speciﬁc language. To evaluate the quality of domain-speciﬁc language code, every soft-\nware component required by the quality platform to parse and query the source code must be developed. This becomes a\ntime-consuming and error-prone task, for which this paper describes a model-driven interoperability strategy that bridges the\ngap between the grammar formats of source code quality parsers and domain-speciﬁc text languages. This approach has been\ntested on the most widespread platforms for designing text-based languages and source code analysis. This interoperability\napproach has been evaluated on a number of speciﬁc contexts in different domain areas.",
        "keywords": [
            "Text-based languages",
            "Static analysis",
            "Model-driven interoperability",
            "Xtext",
            "SonarQube"
        ],
        "authors": [
            "Iván Ruiz-Rube",
            "Tatiana Person",
            "Juan Manuel Dodero",
            "José Miguel Mota",
            "Javier Merchán Sánchez-Jara"
        ],
        "file_path": "data/sosym-all/s10270-019-00729-w.pdf"
    },
    {
        "title": "Investigating a ﬁle transfer protocol using CSP and B",
        "submission-date": "2005/05",
        "publication-date": "2005/05",
        "abstract": "In this paper a ﬁle transmission protocol spe-ciﬁcation is developed using the combination of two for-mal methods: CSP and B. The aim is to demonstrate that it is possible to integrate two well established formal methods whilst maintaining their individual advantages. We discuss how to compositionally verify the speciﬁca-tion and ensure that it preserves some abstract prop-erties. We also discuss how the structure of the speci-ﬁcation follows a particular style which may be gener-ally applicable when modelling other protocols using this combination.",
        "keywords": [
            "CSP",
            "B",
            "Combining formalisms",
            "Compo-sitional veriﬁcation"
        ],
        "authors": [
            "Neil Evans",
            "Helen Treharne"
        ],
        "file_path": "data/sosym-all/s10270-005-0084-3.pdf"
    },
    {
        "title": "An algebra of product families",
        "submission-date": "2009/03",
        "publication-date": "2009/08",
        "abstract": "Experience from recent years has shown that it is often advantageous not to build a single product but rather a family of similar products that share at least one common functionality while having well-identiﬁed variabilities. Such product families are built from elementary features that reach from hardware parts to software artefacts such as requirements, architectural elements or patterns, components, middleware, or code. We use the well established mathematical structure of idempotent semirings as the basis for a product family algebra that allows a formal treatment of the above notions. A particular application of the algebra concerns the multi-view reconciliation problem that arises when complex systems are modelled. We use algebraic integration constraints linking features in one view to features in the same or a different view and show in several examples the suitability of this approach for a wide class of integration constraint formulations. Our approach is illustrated with a Haskell prototype implementation of one particular model of product family algebra.",
        "keywords": [
            "Product family",
            "Product line",
            "Idempotent semiring",
            "Multi-view reconciliation",
            "Formal family speciﬁcation",
            "Feature modelling"
        ],
        "authors": [
            "Peter Höfner",
            "Ridha Khedri",
            "Bernhard Möller"
        ],
        "file_path": "data/sosym-all/s10270-009-0127-2.pdf"
    },
    {
        "title": "Models as the subject of education",
        "submission-date": "2020/07",
        "publication-date": "2020/07",
        "abstract": "The title of the editorial for SoSyM vol. 16 (9) was “Models as the Subject of Research” [1], which focused on modeling as a pure topic for research. In a similar theme to that past editorial, we also believe that the modeling community has much to contribute on the topic of education, as related specifically to modeling, and also how modeling is used within the general context of computer science (CS) and software engineering (SE) education. However, the global interest and influence of modeling as a topic of education seems to be silent outside of the modeling community. We ask, “Why?”",
        "keywords": [],
        "authors": [
            "Huseyin Ergin",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-020-00818-1.pdf"
    },
    {
        "title": "SEFM: software engineering and formal methods",
        "submission-date": "2014/02",
        "publication-date": "2014/02",
        "abstract": "This paper discusses the importance of formal methods in software engineering, highlighting the need for rigorous techniques in software development and validation. It introduces a special issue collecting papers from the 9th International Conference on Software Engineering and Formal Methods (SEFM) held in Montevideo, Uruguay, aiming to bridge the gap between formal methods research and practical software engineering.",
        "keywords": [],
        "authors": [
            "Gilles Barthe",
            "Alberto Pardo",
            "Gerardo Schneider"
        ],
        "file_path": "data/sosym-all/s10270-014-0404-6.pdf"
    },
    {
        "title": "Guest editorial for EMMSAD’2021 special section",
        "submission-date": "2022/09",
        "publication-date": "2022/10",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) conference series organized 26 events from 1996 to 2021, associated with CAISE (Conference on Advanced Information Systems Engineering). From 2009, EMMSAD has become a two-day working conference. From 2017, EMMSAD best papers are invited to submit extended versions for considering their publication in the Journal of Software and Systems Modeling (SoSyM). The main topics of the EMMSAD series have the focus on models and modeling methods for the analysis and development of software information systems of any kind.",
        "keywords": [],
        "authors": [
            "Iris Reinhartz-Berger",
            "Jelena Zdravkovic",
            "Asif Gill"
        ],
        "file_path": "data/sosym-all/s10270-022-01058-1.pdf"
    },
    {
        "title": "Model-driven process enactment for NFV systems with MAPLE",
        "submission-date": "2019/02",
        "publication-date": "2020/02",
        "abstract": "The network functions virtualization (NFV) advent is making way for the rapid deployment of network services (NS) for telecoms. Automation of network service management is one of the main challenges currently faced by the NFV community. Explicitly deﬁning a process for the design, deployment, and management of network services and automating it is therefore highly desirable and beneﬁcial for NFV systems. The use of model-driven orchestration means has been advocated in this context. As part of this effort to support automated process execution, we propose a process enactment approach with NFV systems as the target application domain. Our process enactment approach is megamodel-based. An integrated process modelling and enactment environment, MAPLE, has been built into Papyrus for this purpose. Process modelling is carried out with UML activity diagrams. The enactment environment transforms the process model to a model transformation chain, and then orchestrates it with the use of megamodels. In this paper, we present our approach and environment MAPLE, its recent extension with new features as well as application to an enriched case study consisting of NS design and onboarding process.",
        "keywords": [
            "Process enactment",
            "Megamodelling",
            "Papyrus",
            "Network functions virtualization (NFV)"
        ],
        "authors": [
            "Sadaf Mustaﬁz",
            "Omar Hassane",
            "Guillaume Dupont",
            "Ferhat Khendek",
            "Maria Toeroe"
        ],
        "file_path": "data/sosym-all/s10270-020-00783-9.pdf"
    },
    {
        "title": "Reengineering component-based software systems with Archimetrix",
        "submission-date": "2012/01",
        "publication-date": "2013/04",
        "abstract": "Many software development, planning, or analysis tasks require an up-to-date software architecture documentation. However, this documentation is often outdated, unavailable, or at least not available as a formal model which analysis tools could use. Reverse engineering methods try to fill this gap. However, as they process the system’s source code, they are easily misled by design deficiencies (e.g., violations of component encapsulation) which leaked into the code during the system’s evolution. Despite the high impact of design deficiencies on the quality of the resulting software architecture models, none of the surveyed related works is able to cope with them during the reverse engineering process. Therefore, we have developed the Archimetrix approach which semiautomatically recovers the system’s concrete architecture in a formal model while simultaneously detecting and removing design deficiencies. We have validated Archimetrix on a case study system and two implementation variants of the CoCoME benchmark system. Results show that the removal of relevant design deficiencies leads to an architecture model which more closely matches the system’s conceptual architecture.",
        "keywords": [
            "Reengineering",
            "Reverse engineering",
            "Software architecture",
            "Component-based software systems",
            "Architecture reconstruction",
            "Design deficiencies",
            "Deficiency detection",
            "Code metrics",
            "CoCoME"
        ],
        "authors": [
            "Markus von Detten",
            "Marie Christin Platenius",
            "Steffen Becker"
        ],
        "file_path": "data/sosym-all/s10270-013-0341-9.pdf"
    },
    {
        "title": "Model-checking software library API usage rules",
        "submission-date": "2013/11",
        "publication-date": "2015/05",
        "abstract": "Modern software increasingly relies on using\nthird-party libraries which are accessed via application\nprogramming interfaces (APIs). Libraries usually impose\nconstraints on how API functions can be used (API usage\nrules) and programmers have to obey these API usage rules.\nHowever, API usage rules often are not well documented\nor documented informally. In this work, we show how to\nuse the SCTPL and SLTPL logics to precisely and formally\nspecify API usage rules in libraries, where SCTPL/SLTPL\ncan be seen as an extension of the branching/linear tempo-\nral logic CTL/LTL with variables, quantiﬁers and predicates\nover the stack. This allows library providers to formally\ndescribe API usage rules without knowing how their libraries\nwill be used by programmers. We propose an automated\napproach to check whether programs using libraries violate\nAPI usage rules or not. Our approach consists in modeling\nprograms as pushdown systems (PDSs) and checking API\nusage rules by SCTPL/SLTPL model-checking for PDSs. To\nmake the model-checking procedure more efﬁcient and pre-\ncise, we propose an abstraction that reduces drastically the\nsize of the program model and integrate may-alias analysis\ninto our approach to reduce false alarms. Moreover, we char-\nacterize two sublogics rSCTPL and rSLTPL of SCTPL and\nSLTPL that are preserved by the abstraction. We implement\nour techniques in a tool and apply the tool to check sev-\neral open-source programs. Our tool ﬁnds several previously\nunknown bugs in several programs. The may-alias analysis\navoids most of the false alarms that occur using SCTPL or\nSLTPL model-checking techniques without may-alias analy-\nsis.",
        "keywords": [
            "Pushdown systems",
            "Model-checking",
            "Software API usage rules"
        ],
        "authors": [
            "Fu Song\nTayssir Touili"
        ],
        "file_path": "data/sosym-all/s10270-015-0473-1.pdf"
    },
    {
        "title": "Modular language product lines: concept, tool and analysis",
        "submission-date": "2023/05",
        "publication-date": "2024/05",
        "abstract": "Modelling languages are intensively used in paradigms like model-driven engineering to automate all tasks of the development\nprocess. These languages may have variants, in which case the need arises to deal with language families rather than with\nindividual languages. However, specifying the syntax and semantics of each language variant separately in an enumerative\nway is costly, hinders reuse across variants, and may yield inconsistent semantics between variants. Hence, we propose a novel, \nmodular and compositional approach to describing product lines of modelling languages. It enables the incremental deﬁnition\nof language families by means of modules comprising meta-model fragments, graph transformation rules, and rule extensions.\nLanguage variants are conﬁgured by selecting the desired modules, which entails the composition of a language meta-model\nand a set of rules deﬁning its semantics. This paper describes: a theory for checking well-formedness, instantiability, and\nconsistent semantics of all languages within the family; an implementation as an Eclipse plugin; and an evaluation reporting\ndrastic speciﬁcation size and analysis time reduction in comparison to an enumerative approach.",
        "keywords": [
            "Model-driven engineering",
            "Graph transformation",
            "Product lines",
            "Meta-modelling",
            "Software language\nengineering"
        ],
        "authors": [
            "Juan de Lara\nEsther Guerra\nPaolo Bottoni"
        ],
        "file_path": "data/sosym-all/s10270-024-01179-9.pdf"
    },
    {
        "title": "A lightweight approach to nontermination inference using Constrained Horn Clauses",
        "submission-date": "2022/06",
        "publication-date": "2024/03",
        "abstract": "Nontermination is an unwanted program property for some software systems, and a safety property for other systems. In either case, automated discovery of preconditions for nontermination is of interest. We introduce NtHorn, a fast lightweight nontermination analyser, which is able to deduce non-trivial sufﬁcient conditions for nontermination. Using Constrained Horn Clauses (CHCs) as a vehicle, we show how established techniques for CHC program transformation and abstract interpretation can be exploited for the purpose of nontermination analysis. NtHorn is comparable in effectiveness to the state-of-the-art nontermination analysis tools, as measured on standard competition benchmark suites (consisting of integer manipulating programs), while typically solving problems faster by one order of magnitude.",
        "keywords": [
            "Nontermination",
            "Precondition inference",
            "Constrained Horn clauses",
            "Program transformation",
            "Abstract interpretation"
        ],
        "authors": [
            "Bishoksan Kaﬂe",
            "Graeme Gange",
            "Peter Schachte",
            "Harald Søndergaard",
            "Peter J. Stuckey"
        ],
        "file_path": "data/sosym-all/s10270-024-01161-5.pdf"
    },
    {
        "title": "A survey of approaches for verifying model transformations",
        "submission-date": "2012/01",
        "publication-date": "2013/06",
        "abstract": "As with other software development artifacts, model transformations are not bug-free and so must be systematically veriﬁed. Their nature, however, means that transformations require specialist veriﬁcation techniques. This paper brings together current research on model transformation veriﬁcation by classifying existing approaches along two dimensions. Firstly, we present a coarse-grained classiﬁcation based on the technical details of the approach (e.g., testing, theorem proving, model checking). Secondly, we present a ﬁner-grained classiﬁcation which categorizes approaches according to criteria such as level of formality, transformation language, properties veriﬁed. The purpose of the survey is to bring together research in model transformation veriﬁcation to act as a resource for the community. Furthermore, based on the survey, we identify a number of trends in current and past research on model transformation veriﬁcation.",
        "keywords": [
            "Model transformations",
            "Veriﬁcation",
            "Survey"
        ],
        "authors": [
            "Lukman Ab. Rahim",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-013-0358-0.pdf"
    },
    {
        "title": "Softw Syst Model (2011) 10:143–145",
        "submission-date": "2010/07",
        "publication-date": "2010/07",
        "abstract": "Originally motivated by the need to increase the reliability and robustness of a software and hardware design a few decades ago, formal methods have now established themselves as a distinct important area of computer science. However, the original software engineering motivation of formal methods has been largely weaken throughout the years by the very high cost of their application to system development. As a result, formal methods are now moving towards two different directions, involving two fundamentally distinct research communities. On one side are those who are interested in theoretical aspects of formal methods and focus on the definition of complex mathematical frameworks, which are only loosely inspired by practical problems. On the other side are those who are interested in looking for practical problems, not just in software and hardware development but also in the application areas such as system biology and chemistry, where using formal methods is expected to be worthy in terms of time, effort and financial cost.",
        "keywords": [],
        "authors": [
            "Antonio Cerone",
            "Stefan Gruner"
        ],
        "file_path": "data/sosym-all/s10270-010-0169-5.pdf"
    },
    {
        "title": "Knowledge-based construction of distributed constrained systems",
        "submission-date": "2013/11",
        "publication-date": "2015/02",
        "abstract": "The problem of deriving distributed implementations from global specifications has been extensively studied for a number of application domains. We explore it here from the knowledge perspective: A process may decide to take a local action when it has enough knowledge to do so. Such knowledge may be acquired by communication through primitives available on the platform or by static analysis. In this paper, we want to combine control and distribution, that is, we need to impose some global control constraint on a system executed in a distributed fashion. To reach that goal, we compare two approaches: either build a centralized controlled system, distribute its controller and then implement this controlled system on a distributed platform; or alternatively, directly enforce the control constraint while implementing the distributed system on the platform. We show how to achieve a solution following the second approach and explain why this is a pragmatic and more efficient strategy than the other, previously proposed one.",
        "keywords": [
            "Distributed implementations",
            "Knowledge",
            "Controlled system",
            "Correct-by-construction",
            "Implementation relation",
            "Knowledge preservation"
        ],
        "authors": [
            "Susanne Graf",
            "Sophie Quinton"
        ],
        "file_path": "data/sosym-all/s10270-014-0451-z.pdf"
    },
    {
        "title": "Styles in business process modeling: an exploration and a model",
        "submission-date": "2012/09",
        "publication-date": "2013/05",
        "abstract": "Business process models are an important means to design, analyze, implement, and control business processes. As with every type of conceptual model, a business process model has to meet certain syntactic, semantic, and pragmatic quality requirements to be of value. For many years, such quality aspects were investigated by centering on the properties of the model artifact itself. Only recently, the process of model creation is considered as a factor that inﬂuences the resulting model’s quality. Our work contributes to this stream of research and presents an explorative analysis of the process of process modeling (PPM). We report on two large-scale modeling sessions involving 115 students. In these sessions, the act of model creation, i.e., the PPM, was automatically recorded. We conducted a cluster analysis on this data and identiﬁed three distinct styles of modeling. Further, we investigated how both task- and modeler-speciﬁc factors inﬂuence particular aspects of those modeling styles. Based thereupon, we propose a model that captures our insights. It lays the foundations for future research that may unveil how high-quality process models can be established through better modeling support and modeling instruction.",
        "keywords": [
            "Business process modeling",
            "Process of process modeling",
            "Modeling styles",
            "Cluster analysis"
        ],
        "authors": [
            "Jakob Pinggera",
            "Pnina Soffer",
            "Dirk Fahland",
            "Matthias Weidlich",
            "Stefan Zugal",
            "Barbara Weber",
            "Hajo A. Reijers",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-013-0349-1.pdf"
    },
    {
        "title": "An integrated semantics for reasoning about SysML design models using refinement",
        "submission-date": "2014/11",
        "publication-date": "2015/09",
        "abstract": "SysML is a variant of UML for systems design. Several formalisations of SysML (and UML) are available. Our work is distinctive in two ways: a semantics for refinement and for a representative collection of elements from the UML4SysML profile (blocks, state machines, activities, and interactions) used in combination. We provide a means to analyse and refine design models specified using SysML. Thisfacilitatesthediscoveryofproblemsearlierinthesystem development lifecycle, reducing time, and costs of production. Here, we describe our semantics, which is defined using a state-rich process algebra and implemented in a tool for automatic generation of formal models. We also show how the semantics can be used for refinement-based analysis and development. Our case study is a leadership-election protocol, a critical component of an industrial application. Our major contribution is a framework for reasoning using refinement about systems specified by collections of SysML diagrams.",
        "keywords": [
            "Process algebra",
            "CML",
            "Refinement",
            "Automation",
            "SysML",
            "Semantics"
        ],
        "authors": [
            "Lucas Lima",
            "Alvaro Miyazawa",
            "Ana Cavalcanti",
            "Márcio Cornélio",
            "Juliano Iyoda",
            "Augusto Sampaio",
            "Ralph Hains",
            "Adrian Larkham",
            "Vaughan Lewis"
        ],
        "file_path": "data/sosym-all/s10270-015-0492-y.pdf"
    },
    {
        "title": "Introduction to the Software Engineering and Formal Methods 2013 special issue",
        "submission-date": "2015/03",
        "publication-date": "2015/05",
        "abstract": "We are in the world in which society is increasingly dependent on software, and so, the quality of this software is more important than ever. Unfortunately, the development of high-quality software is becoming increasingly challenging as complexity grows and systems are often concurrent and distributed. The Software Engineering and Formal Methods communities have developed a range of approaches that help address this problem, but initially there was relatively little interaction between these areas and some saw them as rivals. Thankfully, these attitudes have gradually changed, with the communities accepting that each makes a useful contribution in tackling an important problem.",
        "keywords": [],
        "authors": [
            "Mario Bravetti",
            "Robert M. Hierons",
            "Mercedes G. Merayo"
        ],
        "file_path": "data/sosym-all/s10270-015-0467-z.pdf"
    },
    {
        "title": "Improving user productivity in modeling tools by explicitly modeling workﬂows",
        "submission-date": "2017/04",
        "publication-date": "2018/05",
        "abstract": "Software engineering aims to create software tools that allow people to solve particular problems in an easy and efﬁcient way. In this regard, model-driven engineering (MDE) enables to generate software tools, by systematically modeling and transforming models. To do so, MDE relies on language workbenches: Integrated Development Environment for engineering modeling languages, designing models, executing them, and verifying them. However, the usability of these tools is far from efﬁcient. Common MDE activities, such as creating a domain-speciﬁc language or developing a model transformation, are non-trivial and often require repetitive tasks. This results in unnecessary risings of development time. The goal of this paper is to increase the productivity of modelers in their daily activities by automating the tasks performed in current MDE tools. We propose an MDE-based solution where the user deﬁnes a reusable workﬂow that can be parameterized at run-time and executed. We have implemented workﬂows in the graphical modeling tool AToMPM. An empirical evaluation shows that the users’ productivity is signiﬁcantly improved.",
        "keywords": [
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Enactment",
            "Model transformation",
            "User study"
        ],
        "authors": [
            "Miguel Gamboa",
            "Eugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-018-0678-1.pdf"
    },
    {
        "title": "Verifying object-based graph grammars\nAn assume-guarantee approach",
        "submission-date": "2004/11",
        "publication-date": "2006/06",
        "abstract": "The development of concurrent and reactive\nsystems is gaining importance since they are well-suited\nto modern computing platforms, such as the Internet.\nHowever, the development of correct concurrent and\nreactive systems is a non-trivial task. Object-based graph\ngrammar (OBGG) is a visual formal language suitable\nfor the speciﬁcation of this class of systems. In previous\nwork, a translation from OBGG to PROMELA (the in-\nput language of the SPIN model checker) was deﬁned,\nenabling the veriﬁcation of OBGG models using SPIN.\nIn this paper we extend this approach in two different\nways: (1) the approach for property speciﬁcation is\nimproved, enabling to prove properties not only about\npossible OBGG derivations, but also about the internal\nstate of involved objects; (2) an approach is deﬁned to\ninterpret PROMELA traces as OBGG derivations, gen-\nerating graphical counter-examples for properties that\nare not true for a given OBGG model. Another con-\ntribution of this paper is (3) the deﬁnition of a method\nfor model checking partial systems (isolated objects or\na set of objects) using an assume-guarantee approach.\nA gas station system modeled with OBGGs is used to\nillustrate the contributions.",
        "keywords": [
            "Model checking",
            "Partial systems",
            "Graph\ngrammars",
            "Object-based systems",
            "Reactive systems"
        ],
        "authors": [
            "Fernando Luís Dotti",
            "Leila Ribeiro",
            "Osmar Marchi dos Santos",
            "Fábio Pasini"
        ],
        "file_path": "data/sosym-all/s10270-006-0014-z.pdf"
    },
    {
        "title": "Integrating deductive veriﬁcation and symbolic execution for abstract object creation in dynamic logic",
        "submission-date": "2013/11",
        "publication-date": "2014/12",
        "abstract": "We present a fully abstract weakest precondi-\ntion calculus and its integration with symbolic execution.\nOur assertion language allows both specifying and verifying\nproperties of objects at the abstraction level of the program-\nming language, abstracting from a speciﬁc implementation\nof object creation. Objects which are not (yet) created never\nplay any role. The corresponding proof theory is discussed\nand justiﬁed formally by soundness theorems. The usage of\nthe assertion language and proof rules is illustrated with an\nexample of a linked list reachability property. All proof rules\npresented are fully implemented in a version of the KeY ver-\niﬁcation system for Java programs.",
        "keywords": [
            "Speciﬁcation",
            "Veriﬁcation",
            "Program logic",
            "Dynamic logic",
            "Object creation"
        ],
        "authors": [
            "Stijn de Gouw",
            "Frank de Boer",
            "Wolfgang Ahrendt",
            "Richard Bubel"
        ],
        "file_path": "data/sosym-all/s10270-014-0446-9.pdf"
    },
    {
        "title": "Supporting data-aware processes with MERODE",
        "submission-date": "2022/02",
        "publication-date": "2023/03",
        "abstract": "Most data-aware process modelling approaches have been developed from a process perspective and lack a full-ﬂedged data modelling approach. In addition, the evaluation of data-centric process approaches reveals that, even though their value is acknowledged, their usability is a point of concern. This paper presents a data-aware process modelling approach combining full-ﬂedged domain modelling based on UML class diagrams and state charts with BPMN and DMN. The approach is illustrated by means of an elaborated example with multiple business processes on top of a joint domain model. A proof-of-concept has been implemented using the MERODE code generator, linking the resulting prototype application to a Camunda BPM engine, making use of RESTful web services. The proposed approach is evaluated against 20 requirements for data-aware processes and demonstrates that the majority of these are already satisﬁed by the out-of-the-box combination of the Camunda BPM engine with the prototype generated from a MERODE domain model.",
        "keywords": [
            "Conceptual modelling",
            "Process modelling",
            "Data-aware processes",
            "Model-driven engineering"
        ],
        "authors": [
            "Monique Snoeck",
            "Charlotte Verbruggen",
            "Johannes De Smedt",
            "Jochen De Weerdt"
        ],
        "file_path": "data/sosym-all/s10270-023-01095-4.pdf"
    },
    {
        "title": "Supporting UML-based development of embedded systems by formal techniques",
        "submission-date": "2005/12",
        "publication-date": "2007/02",
        "abstract": "We describe an approach to support UML-based development of embedded systems by formal techniques. A subset of UML is extended with timing annotations and given a formal semantics. UML models are translated, via XMI, to the input format of formal tools, to allow timed and non-timed model checking and interactive theorem proving. Moreover, the Play-Engine tool is used to execute and analyze requirements by means of live sequence charts. We apply the approach to a part of an industrial case study, the MARS system, and report about the experiences, results and conclusions.",
        "keywords": [
            "Formal methods",
            "UML",
            "Embedded systems",
            "Real-time"
        ],
        "authors": [
            "Jozef Hooman",
            "Hillel Kugler",
            "Iulian Ober",
            "Anjelika Votintseva",
            "Yuri Yushtein"
        ],
        "file_path": "data/sosym-all/s10270-006-0043-7.pdf"
    },
    {
        "title": "Experimenting with modeling-speciﬁc word embeddings",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "The application of machine learning techniques to address MDE problems often requires transforming raw information (e.g., software models) to a numerical representation which can be used by machine learning algorithms. To this end, pretrained embeddings are a key technology to facilitate the construction of such applications. However, previous works have demonstrated that these embeddings struggle to generalize effectively in the MDE domain due to their training on general-purpose corpora. To tackle this issue, we developed WordE4MDE, which are specialized word embeddings trained specifically on modeling documents. In this study, we aim to overcome several limitations of WordE4MDE and conduct additional experiments to assess its efﬁcacy. Key limitations we address include: (1) mitigating the out-of-vocabulary issue through the utilization of sub-word embeddings, (2) adding contextualization to the embeddings by training a BERT model on our specific modeling corpus and (3) addressing the constraint of limited training data by investigating the augmentation of our modeling corpus with StackOverﬂow and StackExchange data.",
        "keywords": [
            "Embeddings",
            "Classiﬁcation",
            "Clustering",
            "Recommendation",
            "Machine Learning",
            "Model-Driven Engineering"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Carlos Durá",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-024-01250-5.pdf"
    },
    {
        "title": "Enabling automated requirements reuse and configuration",
        "submission-date": "2017/02",
        "publication-date": "2017/11",
        "abstract": "A system product line (PL) often has a large number of reusable and configurable requirements, which in practice are organized hierarchically based on the architecture of the PL. However, the current literature lacks approaches that can help practitioners to systematically and automatically develop structured and configuration-ready PL requirements repositories. In the context of product line engineering and model-based engineering, automatic requirements structuring can benefit from models. Such a structured PL requirements repository can greatly facilitate the development of product-specific requirements repository, the product configuration at the requirements level, and the smooth transition to downstream product configuration phases (e.g., at the architecture design phase). In this paper, we propose a methodology with tool support, named as Zen-ReqConfig, to tackle the above challenge. Zen-ReqConfig is built on existing model-based technologies, natural language processing, and similarity measure techniques. It automatically devises a hierarchical structure for a PL requirements repository, automatically identifies variabilities in textual requirements, and facilitates the configuration of products at the requirements level, based on two types of variability modeling techniques [i.e., cardinality-based feature modeling (CBFM) and a UML-based variability modeling methodology (named as SimPL)]. We evaluated Zen-ReqConfig with five case studies. Results show that Zen-ReqConfig can achieve a better performance based on the character-based similarity measure Jaro than the term-based similarity measure Jaccard. With Jaro, Zen-ReqConfig can allocate textual requirements with high precision and recall, both over 95% on average and identify variabilities in textual requirements with high precision (over 97% on average) and recall (over 94% on average). Zen-ReqConfig achieved very good time performance: with less than a second for generating a hierarchical structure and less than 2 s on average for allocating a requirement. When comparing SimPL and CBFM, no practically significant difference was observed, and they both performed well when integrated with Zen-ReqConfig.",
        "keywords": [
            "Requirements",
            "Product line",
            "Configuration",
            "Reuse",
            "Feature model"
        ],
        "authors": [
            "Yan Li",
            "Tao Yue",
            "Shaukat Ali",
            "Li Zhang"
        ],
        "file_path": "data/sosym-all/s10270-017-0641-6.pdf"
    },
    {
        "title": "Introduction to the STAF 2015 special section",
        "submission-date": "2018/06",
        "publication-date": "2018/07",
        "abstract": "Software Technologies: Applications and Foundations (STAF) is a federation of a number of leading conferences on software technologies. It provides a loose umbrella organization for practical software technologies conferences. In 2015 the STAF federated event has been held in L’Aquila (Italy) with a special focus on practical and foundational aspects of software technology, from object-oriented design, testing, mathematical approaches to modeling and veriﬁcation, model transformation, graph transformation, model-driven engineering, aspect-oriented development, and tools. Besides the main and satellite events (14 overall), three conferences: The International Conference on Model Transformation (ICMT 2015), The European Conference on Modelling Foundations and Applications (ECMFA 2015), The International Conference on Tests & Proofs (TAP 2015) collected papers on relevant topics of software and system modelingandveriﬁcationtechniques;amongthem,10papers have been selected and extended to be part of this special section.",
        "keywords": [],
        "authors": [
            "Jasmin Blanchette",
            "Francis Bordeleau",
            "Alfonso Pierantonio",
            "Nikolai Kosmatov",
            "Gabriele Taentzer",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-018-0686-1.pdf"
    },
    {
        "title": "Supporting inheritance hierarchy changes in model-based regression test selection",
        "submission-date": "2016/09",
        "publication-date": "2017/12",
        "abstract": "Models can be used to ease and manage the development, evolution, and runtime adaptation of a software system. When models are adapted, the resulting models must be rigorously tested. Apart from adding new test cases, it is also important to perform regression testing to ensure that the evolution or adaptation did not break existing functionality. Since regression testing is performed with limited resources and under time constraints, regression test selection (RTS) techniques are needed to reduce the cost of regression testing. Applying model-level RTS for model-based evolution and adaptation is more convenient than using code-level RTS because the test selection process happens at the same level of abstraction as that of evolution and adaptation. In earlier work, we proposed a model-based RTS approach called MaRTS to be used with a ﬁne-grained model-based adaptation framework that targets applications implemented in Java. MaRTS uses UML models consisting of class and activity diagrams. It classiﬁes test cases as obsolete, reusable, or retestable based on changes made to UML class and activity diagrams of the system being adapted. However, MaRTS did not take into account the changes made to the inheritance hierarchy in the class diagram and the impact of these changes on the selection of test cases. This paper extends MaRTS to support such changes and demonstrates that the extended approach performs as well as or better than code-based RTS approaches in safely selecting regression test cases. While MaRTS can generally be used during any model-driven development or model-based evolution activity, we have developed it in the context of runtime adaptation. We evaluated the extended MaRTS on a set of applications and compared the results with code-based RTS approaches that also support changes to the inheritance hierarchy. The results showed that the extended MaRTS selected all the test cases relevant to the inheritance hierarchy changes and that the fault detection ability of the selected test cases was never lower than that of the baseline test cases. The extended MaRTS achieved comparable results to a graph-walk code-based RTS approach (DejaVu) and showed a higher reduction in the number of selected test cases when compared with a static analysis code-based RTS approach (ChEOPSJ).",
        "keywords": [
            "Executable UML models",
            "Inheritance hierarchy",
            "Model-based adaptation",
            "Model-based regression test selection",
            "Model validation",
            "Runtime adaptation",
            "UML activity diagram",
            "UML class diagram"
        ],
        "authors": [
            "Mohammed Al-Refai",
            "Sudipto Ghosh",
            "Walter Cazzola"
        ],
        "file_path": "data/sosym-all/s10270-017-0636-3.pdf"
    },
    {
        "title": "Mutation testing for temporal alloy models (extended version)",
        "submission-date": "2024/04",
        "publication-date": "Not found",
        "abstract": "Writing declarative models has numerous beneﬁts, ranging from automated reasoning and correction of design-level properties before systems are built, to automated testing and debugging of their implementations after they are built. Alloy is a declarative modeling language that is well-suited for verifying system designs. A key strength of Alloy is its scenario-ﬁnding toolset, the Analyzer, which allows users to explore all valid scenarios that adhere to the model’s constraints up to a user-provided scope. Despite the Analyzer, writing correct Alloy models remains a difﬁcult task, partly due to Alloy’s expressive operators, which allow for succinct formulations of complex properties but can be difﬁcult to reason over manually. To further add to the complexity, Alloy’s grammar was recently expanded to support linear temporal logic, increasing both the expressibility of Alloy and the burden for accurately expressing properties. To address this, this paper presents μAlloyτ, an extension to Alloy’s mutation testing framework that accounts for the newly introduced temporal logic, including updating μAlloyτ’s test generation capability to produce temporal test cases. Experimental results reveal μAlloyτ is efﬁcient at generating and checking mutations and μAlloyτ’s automatically generated tests are effective at detecting faulty temporal models.",
        "keywords": [
            "Alloy",
            "Mutation testing",
            "Test generation"
        ],
        "authors": [
            "Ana Jovanovic",
            "Allison Sullivan"
        ],
        "file_path": "data/sosym-all/s10270-024-01220-x.pdf"
    },
    {
        "title": "Literature review of reuse in business process modeling",
        "submission-date": "2011/11",
        "publication-date": "2012/09",
        "abstract": "Business process models play an important role\nin the analysis and improvement of the performance of an\nenterprise. Evidently, the quality of a business process model\nhas a direct effect on the business performance. This evidence\nhas motivated both the academic and industrial communi-\nties to look for suitable methods for creating good quality\nbusiness process models. In particular, there is a wide agree-\nment that reuse can accelerate the design process and pro-\nduce high quality solutions by adopting best practices and\nagreed-up-on solutions. However, faced with various types\nof reusable artifacts, business process designers need a set of\ncriteria to determine which type would suit best their needs\nand design context. To assist designers in their choice, we\nfirst present a set of criteria inﬂuencing the design phase in\nterms of effort required and the quality of the resulting model.\nSecondly, we use this set of criteria to present a state of the\nart on the most signiﬁcant reusable design artifacts.",
        "keywords": [
            "Workﬂow pattern",
            "Activity pattern",
            "Action pattern",
            "Semantic business process pattern",
            "Reference model",
            "Business process modeling"
        ],
        "authors": [
            "Nahla Zaaboub Haddar",
            "Lobna Makni",
            "Hanene Ben Abdallah"
        ],
        "file_path": "data/sosym-all/s10270-012-0286-4.pdf"
    },
    {
        "title": "Expert’s voice: The BabyUML discipline of programming (where a Program = data + Communication + Algorithms)",
        "submission-date": "2006/03",
        "publication-date": "2006/03",
        "abstract": "I want increased conﬁdence in my programs. I want my own and other people’s programs to be more readable. I want a new discipline of programming that augments my thought processes. Therefore, I create and explore a new discipline of programming in my BabyUML laboratory. I select, simplify and twist UML and other languages to demonstrate how they help bridge the gap between me as a programmer and the objects running in my computer The focus is on the run time objects; their structure, their interaction, and their individual behaviors.",
        "keywords": [
            "Object-oriented programming",
            "Object oriented methods",
            "Data structures",
            "Object communication",
            "Object algorithms",
            "Latently-typed languages",
            "Stored program object computers"
        ],
        "authors": [
            "Trygve Reenskaug"
        ],
        "file_path": "data/sosym-all/s10270-006-0005-0.pdf"
    },
    {
        "title": "Guidelines for representing complex cardinality constraints in binary and ternary relationships",
        "submission-date": "2010/11",
        "publication-date": "2012/02",
        "abstract": "Ternary relationships represent the association among three entities whose constraints database designers do not always know how to manage. In other words, it is very difﬁcult for the designer to detect, represent and add constraints in a ternary relationship according to the domain requirements. To remedy the shortcomings in capturing the semantics required for the representation of this kind of relationship, the present paper discusses a practical method to motivate the designer’s use of ternary relationships in a methodological framework. The method shows how to calculate cardinality constraints in binary and ternary relationships and to preserve the associated semantics until the implementation phase of the database development method.",
        "keywords": [
            "Ternary associations",
            "Conceptual models",
            "Logical models",
            "Model transformations",
            "Database methodology"
        ],
        "authors": [
            "Dolores Cuadra",
            "Paloma Martínez",
            "Elena Castro",
            "Harith Al-Jumaily"
        ],
        "file_path": "data/sosym-all/s10270-012-0234-3.pdf"
    },
    {
        "title": "Automatic Model Transformation and Formal Veriﬁcation for Function Block of IEC 61499",
        "submission-date": "2024/05",
        "publication-date": "2025/06",
        "abstract": "The IEC 61499 standard describes the structure and behavior of distributed control systems, providing a design language\nat the system level and a speciﬁcation for distributed systems. In the design of the control ﬂow, the function block of the\nIEC 61499 standard will use an execution control chart to describe its behavior. An operation state machine is designed to\ndescribe the operation of the function block. It is necessary to consider the interaction between the execution control chart\nand the operation state machine for control behavior descriptions. The function block model cannot be directly used as the\ninput of model checkers, so it needs to be transformed formally and semantically equivalently to the input models of formal\nveriﬁcation tools. This paper proposes a method to automatically transform the function block model into a ﬁnite automata\nmodel. We use bisimulation to prove that the behavior of the transformed ﬁnite automata model is consistent with that of the\nfunction block. We further demonstrate that the transformed ﬁnite automata model is used as the input to the model checker\nfor formal veriﬁcation.",
        "keywords": [
            "IEC61499 standard",
            "Automatic model transformation",
            "Formal veriﬁcation",
            "Model checking"
        ],
        "authors": [
            "Yean-Ru Chen",
            "Chia-Hao Hsu",
            "Tien-Fu Li",
            "Cheng-Yuan Lin",
            "Shao-Chia Weng",
            "Min-Yan Tsai"
        ],
        "file_path": "data/sosym-all/s10270-025-01316-y.pdf"
    },
    {
        "title": "Adopting the concept of a function as an underlying semantic paradigm for modeling languages",
        "submission-date": "2023/11",
        "publication-date": "2023/11",
        "abstract": "There are researchers and practitioners in the areas of modeling and modeling language design who focus mainly on a syntactic point of view. For example, they may look at language design for language agglomerates like UML or SysML, as well as more customized DSLs, to consider the optimal syntactic constructs that are needed to cover all of the different kinds of phenomena occurring in a real-world problem context. Other researchers may focus on the semantic point of view to understand the meaning of a specific model as expressed in a language. A general challenge is when the same syntactic construct can be interpreted differently in various contexts of usage.\n\nA well known but often confusing example is the class diagram. A class “Person” in such a diagram may have different meanings based on the phase of development in which it is used. If the diagram has been defined during business requirements elicitation, the class actually represents human beings. In this case, “Persons” are real objects in the world of physical things. When the same class diagram is then used for design, the very same class “Person” suddenly describes the data structure that is capable of collecting data about human beings andthus apurelyvirtual concept emerges. Somewhere between requirements elicitation and design, the interpretationaddsastepofindirectionthatisnotreﬂectedinthesyntax itself. Other prominent examples use different interpretations of various modeling elements, which frequently lead to confusion. The situation is even more challenging when physical systems are accompanied with digital twins, where software components intelligently control a physical representation.\n\nIt would be helpful to have an underlying paradigm that connects all of the different interpretations and views (e.g., structure, data structure, interaction, behavior, state, agents, or activities) that are represented through various modeling techniques.\n\nIn the context of the upcoming SysML 2.0 deﬁnition, it is evident that a semantically sound and useful integration could play a major role toward addressing this challenge of differing interpretations. One possibility is to consider the often used paradigm of “function”.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01140-2.pdf"
    },
    {
        "title": "Informal description and analysis of geographic requirements: an approach based on problems",
        "submission-date": "2005/07",
        "publication-date": "2006/08",
        "abstract": "Software requirements describe a problem in the real world that a software system is intended to solve. Describing requirements is challenging because usually too much attention is given to the ﬁnal soft-ware product instead of concentrating on the problem itself and the real world. The area of geographic appli-cations is no exception. Existing approaches to software development that are speciﬁc to the geographic area, for example, GIS tools, spatial databases, geographic query languages, and spatial data structures, are suitable for designing and implementing geographic applications and are, therefore, solution-oriented. We present a prob-lem-oriented approach for requirements description of geographic applications. Most geographic applications are composed of well-known geographic subproblems. The proposed approach provides classes of common geographic subproblems that can be used to promote analysis and description of real-world problems. Each class of problems is presented as a problem frame show-ing domain properties, requirements and speciﬁcations. The problem frames discussed in this work are based on Jackson’s general purpose problem frames and are tailored here for the geographic area. The approach is validated through a case study.",
        "keywords": [],
        "authors": [
            "Maria Augusta V. Nelson",
            "Paulo S. C. Alencar",
            "Donald D. Cowan"
        ],
        "file_path": "data/sosym-all/s10270-006-0031-y.pdf"
    },
    {
        "title": "Modeling presentation layers of web applications for testing",
        "submission-date": "2008/08",
        "publication-date": "2009/08",
        "abstract": "Websoftwareapplicationshavebecomecomplex,\nsophisticated programs that are based on novel computing\ntechnologies. Their most essential characteristic is that they\nrepresent a different kind of software deployment—most of\nthe software is never delivered to customers’ computers, but\nremains on servers, allowing customers to run the software\nacross the web. Although powerful, this deployment model\nbrings new challenges to developers and testers. Checking\nstatic HTML links is no longer sufﬁcient; web applications\nmust be evaluated as complex software products. This paper\nfocuses on three aspects of web applications that are unique\nto this type of deployment: (1) an extremely loose form of\ncoupling that features distributed integration, (2) the abil-\nity that users have to directly change the potential ﬂow of\nexecution, and (3) the dynamic creation of HTML forms.\nTaken together, these aspects allow the potential control\nﬂow to vary with each execution, thus the possible con-\ntrol ﬂows cannot be determined statically, prohibiting several\nstandard analysis techniques that are fundamental to many\nsoftware engineering activities. This paper presents a new\nway to model web applications, based on software couplings\nthat are new to web applications, dynamic ﬂow of control,\ndistributed integration, and partial dynamic web application\ndevelopment. This model is based on the notion of atomic\nsections, which allow analysis tools to build the analog of\na control ﬂow graph for web applications. The atomic sec-\ntion model has numerous applications in web applications;\nthis paper applies the model to the problem of testing web\napplications.",
        "keywords": [
            "Web applications",
            "Web modeling",
            "Test criteria"
        ],
        "authors": [
            "Jeff Offutt",
            "Ye Wu"
        ],
        "file_path": "data/sosym-all/s10270-009-0125-4.pdf"
    },
    {
        "title": "Toward an ontology for EA modeling and EA model quality",
        "submission-date": "2023/04",
        "publication-date": "2024/02",
        "abstract": "Models have long since been used, in different shapes and forms, to understand, communicate about, and (re)shape, the world around us; including many different social, economic, biological, chemical, physical, and digital aspects. This is also the case in the context of enterprise architecture (EA), where we see a wide range of models in many different shapes and forms being used as well. Researchers in EA modeling usually introduce their own lexicon, and perspective of what a model actually is, while accepting (often implicitly) the accompanying ontological commitments. Similarly, practitioners of EA modeling implicitly also commit to (different) ontologies, resulting in models that have an uncertain ontological standing. This is because, for the subject domain of enterprise architecture models (as opposed to the content of such models), no single ontology has gained major traction. As a result, studies into aspects of enterprise architecture models, such as “model quality” and “return on modeling effort”, are fragmented, and cannot readily be compared or combined. This paper proposes a comprehensive applied ontology, speciﬁcally geared to enterprise architecture modeling. Ontologies represent structured knowledge about a particular subject domain. It allows for study into, and reasoning about, that subject domain. Our ontology is derived from a theory of modeling, while clarifying concepts such as “enterprise architecture model”, and introduces novel concepts such as “model audience” and “model objective”. Furthermore, the relevant interrelations between these different concepts are identiﬁed and deﬁned. The resulting ontology for enterprise architecture models is represented in OntoUML, and shown to be consistent with the foundational ontology for modeling, Uniﬁed Foundational Ontology.",
        "keywords": [
            "Enterprise architecture",
            "Ontology",
            "Domain model",
            "Enterprise architecture modeling",
            "Enterprise architecture model",
            "Architecture",
            "Model quality"
        ],
        "authors": [
            "Jan A. H. Schoonderbeek",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-023-01146-w.pdf"
    },
    {
        "title": "Measurement and classification of inter-actor dependencies in goal models",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Goal-oriented requirements engineering approaches aim to capture desired goals and strategies of relevant stakeholders during early requirements engineering stages, using goal models. Socio-technical systems (STSs) involve a rich interplay of human actors (traditional stakeholders, described as actors in goal models) and technical systems. Actors may depend on each other for goals to be achieved, activities to be performed, and resources to be supplied. These dependencies create new opportunities by extending actors’ capabilities but may make the actor vulnerable if the dependee fails to deliver the dependum (knowingly or unintentionally). This paper proposes a novel quantitative metric, called Actor Interaction Metric (AIM), to measure inter-actor dependencies in Goal-oriented Requirements Language (GRL) models. The metric is used to categorize inter-actor dependencies into positive (beneﬁcial), negative (harmful), and neutral (no impact). Furthermore, the AIM metric is used to identify the most harmful/beneﬁcial dependency for each actor. The proposed approach is implemented in a tool targeting the textual GRL language, part of the User Requirements Notation (URN) standard. We evaluate experimentally our approach using 13 GRL models, with positive results on applicability and scalability.",
        "keywords": [
            "Goal-oriented requirements",
            "Metric",
            "Inter-actor dependencies",
            "GRL",
            "URN"
        ],
        "authors": [
            "Jameleddine Hassine",
            "Muhammad Tukur"
        ],
        "file_path": "data/sosym-all/s10270-021-00961-3.pdf"
    },
    {
        "title": "Evaluating user interface generation approaches: model-based versus model-driven development",
        "submission-date": "2017/06",
        "publication-date": "2018/10",
        "abstract": "Advances in software design possibilities have led to a growing interest in the study of user interfaces (UIs). Many Model-Based User Interface Development Environments (MB-UIDEs) have been proposed to deal with the generation of the UIs (semi-) automatically by using models with different levels of abstraction. Often, this generation is limited to the UI part of an application. However, achieving true model-driven development (MDD) requires the co-development of application and UI and, hence, needs to go a step further. This paper analyzes a large set of existing MB-UIDEs, evaluates, from a critical perspective, to what extent they can be considered full MDD environments, and adequately addresses the co-design of UI and application. Following the guidelines proposed by Kitchenham and Charters (Engineering 2, 2007), we performed a systematic literature review. A total of 96 papers were examined. Based on these papers, an assessment framework containing 10 criteria with speciﬁc metrics to evaluate MB-UIDEs was deﬁned and 30 different environments were evaluated following these criteria. The evaluation identiﬁes several gaps in the existing state of the art and highlights the areas of promising improvement. The evaluation shows that, although a strong progress has being achieved over the last years, the existing environments do not yet fully exploit the beneﬁts and potentialities of MDD, nor do they adequately integrate UI design with application logic design and generation. Further research needs to be done to support the MDD of UIs and the co-design of the underlying application. The difﬁculty of use of the existing MB-UIDEs, the lack of UI design ﬂexibility, and the lack of complete and integrated development support are the main research gaps that need to be addressed.",
        "keywords": [
            "Model-based user interface software tools",
            "User interface generation",
            "Model-driven development",
            "Integration with application development"
        ],
        "authors": [
            "Jenny Ruiz",
            "Estefanía Serral",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-018-0698-x.pdf"
    },
    {
        "title": "An extensible approach to implicit incremental model analyses",
        "submission-date": "2017/11",
        "publication-date": "2019/01",
        "abstract": "As systems evolve, analysis results based on models of the system must be updated, in many cases as fast as possible. Since usually only small parts of the model change, large parts of the analysis’ intermediate results could be reused in an incremental fashion. Manually invalidating these intermediate results at the right places in the analysis is a non-trivial and error-prone task that conceals the codes intention. A possible solution for this problem is implicit incrementality, i.e., an incremental algorithm is derived from the batch speciﬁcation, aiming for an increased performance without the cost of degraded maintainability. Current approaches are either specialized to a subset of analyses or require explicit state management. In this paper, we propose an approach to implicit incremental model analysis capable of integrating custom dynamic algorithms. For this, we formalize incremental derivation using category theory, gaining type-safety and correctness properties. We implement an extensible implicit incremental computation system and validate its applicability by integrating incremental queries. We evaluate the performance using a micro-benchmark and a community benchmark where the integration of explicit query incrementalization was multiple orders of magnitude faster than rerunning the analysis after every change.",
        "keywords": [
            "Incremental computation",
            "Model-driven",
            "Monads"
        ],
        "authors": [
            "Georg Hinkel",
            "Robert Heinrich",
            "Ralf Reussner"
        ],
        "file_path": "data/sosym-all/s10270-019-00719-y.pdf"
    },
    {
        "title": "Guest editorial to the special section on PoEM’2020",
        "submission-date": "2022/05",
        "publication-date": "2022/06",
        "abstract": "This paper is a guest editorial to the special section on PoEM’2020, a working conference on the Practice of Enterprise Modeling. It discusses the conference, its focus on Enterprise Modeling in the Digital Age, and the selection process for papers published in this special section of the Journal of Software and Systems Modeling.",
        "keywords": [
            "Enterprise modeling",
            "Conceptual modeling"
        ],
        "authors": [
            "J¯anis Grabis",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-022-01017-w.pdf"
    },
    {
        "title": "On the persistent rumors of the programmer’s imminent demise",
        "submission-date": "2023/07",
        "publication-date": "2023/11",
        "abstract": "Since the dawn of programming, several developments in programming language design and programming methodology have been hailed as the end of the profession of programmer; they have all proven to be exaggerated rumors, to echo the words attributed to Mark Twain. In this short paper, we ponder the question of whether the emergence of large language models finally realizes these prophecies? Also, we discuss why even if this prophecy is finally realized, it does not change the job of the researcher in programming.",
        "keywords": [
            "Programming languages",
            "Automatic programming",
            "Programming profession",
            "Large language models"
        ],
        "authors": [
            "Hessam Mohammadi",
            "Wided Ghardallou",
            "Elijah Brick",
            "Ali Mili"
        ],
        "file_path": "data/sosym-all/s10270-023-01136-y.pdf"
    },
    {
        "title": "Ten years of software and systems modeling",
        "submission-date": "2012/09",
        "publication-date": "2012/09",
        "abstract": "This editorial reflects on the ten years of the Journal of Software and Systems Modeling (SoSyM), reviewing changes in the field and looking towards future developments. It introduces a special issue containing ten invited papers from well-respected experts, focusing on the state-of-practice and potential future research agendas.",
        "keywords": [],
        "authors": [
            "Gregor Engels",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-012-0280-x.pdf"
    },
    {
        "title": "Modelling assistants based on information reuse: a user evaluation for language engineering",
        "submission-date": "2021/03",
        "publication-date": "2023/04",
        "abstract": "Model-driven engineering (MDE) uses models as ﬁrst-class artefacts during the software development lifecycle. MDE often relies on domain-speciﬁc languages (DSLs) to develop complex systems. The construction of a new DSL implies a deep understanding of a domain, whose relevant knowledge may be scattered in heterogeneous artefacts, like XML doc-uments, (meta-)models, and ontologies, among others. This heterogeneity hampers their reuse during (meta-)modelling processes. Under the hypothesis that reusing heterogeneous knowledge helps in building more accurate models, more efﬁciently, in previous works we built a (meta-)modelling assistant called Extremo. Extremo represents heterogeneous information sources with a common data model, supports its uniform querying and reusing information chunks for building (meta-)models. To understand how and whether modelling assistants—like Extremo—help in designing a new DSL, we conducted an empirical study, which we report in this paper. In the study, participants had to build a meta-model, and we measured the accuracy of the artefacts, the perceived usability and utility and the time to completion of the task. Interestingly, our results show that using assistance did not lead to faster completion times. However, participants using Extremo were more effective and efﬁcient, produced meta-models with higher levels of completeness and correctness, and overall perceived the assistant as useful. The results are not only relevant to Extremo, but we discuss their implications for future modelling assistants.",
        "keywords": [
            "Modelling",
            "Modelling assistants",
            "Language engineering",
            "Modelling process",
            "Empirical studies"
        ],
        "authors": [
            "Ángel Mora Segura",
            "Juan de Lara",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-023-01094-5.pdf"
    },
    {
        "title": "The evolution of model editors: browser- and cloud-based solutions",
        "submission-date": "2016/04",
        "publication-date": "2016/04",
        "abstract": "In March 2006, Google purchased Upstartle to gain access to their browser-based word processor called Writely [1]. This acquisition from over a decade ago led to what we now know as Google Docs, which ushered in a new form of collabo-rative authoring tools. The idea of using a Web browser as an editing platform, coupled with the storage options avail-able within the cloud, provides powerful new capabilities that have transformed the way we interact with colleagues to design and create documents, as well as all other sorts of artifacts. Specialized text processing solutions, like the LaTeX-focused Overleaf environment [2], bring a fresh new approach to collaboration using long-standing traditional tools.Furthermore,browser-andcloud-basedauthoringtools have penetrated many domains. For example, in computer science education, tools such as Scratch help new program-mers learn block-based coding in a browser, where programs are stored in the cloud with a large repository (over 13.M shared Scratch programs are available at the time of this writ-ing) of user-shared examples [3].",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-016-0524-2.pdf"
    },
    {
        "title": "SOCAM: a service-oriented computing architecture modeling method",
        "submission-date": "2021/01",
        "publication-date": "2021/11",
        "abstract": "Software architecture models are considered ﬁrst-class artifacts in current software engineering best practices. Thus, usable\nand well-understood modeling methods are required for software architects. For this aim, several speciﬁc software architecture\nmodeling methods as well as generic design methods included in software development methodologies are available. However,\nwe believe, there is the lack of more speciﬁc guidance in current software architecture methods. One of the principal causes\nof such a lack of speciﬁc guidance is the general-purpose nature of these methods. Therefore, further efforts are required to\ndefine domain-speciﬁc software architecture methods. In this paper, we present SOCAM, a software architecture modeling\nmethod for Web Service-Oriented Systems. We illustrate the use of SOCAM with a customization of the well-known SOA\ntest application case: the Sun Adventure Builder system. A comparative analysis of SOCAM with other methods reveals a\nnumber of beneﬁts of our method over the other approaches. Also, a survey research method evaluation conﬁrms some of\nthese beneﬁts such as the fact that SOCAM is perceived as more useful than certain general-purpose methods.",
        "keywords": [
            "Software architecture",
            "SOC",
            "SOA",
            "Web-based business systems"
        ],
        "authors": [
            "Paola Y. Reyes-Delgado",
            "Hector A. Duran-Limon",
            "Manuel Mora",
            "Laura C. Rodriguez-Martinez"
        ],
        "file_path": "data/sosym-all/s10270-021-00946-2.pdf"
    },
    {
        "title": "Models and temporal logical speciﬁcations for timed component connectors",
        "submission-date": "2005/02",
        "publication-date": "2006/08",
        "abstract": "Component-based software engineering advocates construction of software systems through composition of coordinated autonomous components. Signiﬁcant beneﬁts of this approach include software reuse, simpler and faster construction, enhanced reli- ability, and dramatic reductions in the complexity of construction of provably correct critical systems, many of which involve real-time concerns. Effective, ﬂexible component composition by itself still poses a challenge today and yet the special nature of real-time constraints makes component-based construction of real-time systems even more demanding. The coordination language Reo supports compositional system construction through connectors that exogenously coor- dinate the interactions among the constituent compo- nents which unawarely comprise a complex system, into a coherent collaboration. The simple, yet surprisingly rich, calculus of channel composition that underlies Reo offers a ﬂexible framework for compositional construction of coordinating component connectors with real-time properties. In this paper, we present an operational semantics for the channel-based component connectors of Reo in terms of Timed Constraint Auto- mata and introduce a temporal-logic for speciﬁcation and veriﬁcation of their real-time properties.",
        "keywords": [
            "Coordination",
            "Real-time",
            "Composition",
            "Reo",
            "Constraint automata",
            "Timed automata",
            "Linear temporal logic",
            "Timed data streams"
        ],
        "authors": [
            "Farhad Arbab",
            "Christel Baier",
            "Frank de Boer",
            "Jan Rutten"
        ],
        "file_path": "data/sosym-all/s10270-006-0009-9.pdf"
    },
    {
        "title": "VMTL: a language for end-user model transformation",
        "submission-date": "2015/10",
        "publication-date": "2016/07",
        "abstract": "Model transformation is a key enabling technology of Model-Driven Engineering (MDE). Existing model transformation languages are shaped by and for MDE practitioners—a user group with needs and capabilities which are not necessarily characteristic of modelers in general. Consequently, these languages are largely ill-equipped for adoption by end-user modelers in areas such as requirements engineering, business process management, or enterprise architecture. We aim to introduce a model transformation language addressing the skills and requirements of end-user modelers. With this contribution, we hope to broaden the application scope of model transformation and MDE technology in general. We discuss the proﬁle of end-user modelers and propose a set of design guidelines for model transformation languages addressing them. We then introduce Visual Model Transformation Language (VMTL) following these guidelines. VMTL draws on our previous work on the usability-oriented Visual Model Query Language. We implement VMTL using the Henshin model transformation engine, and empirically investigate its learnability via two user experiments and a think-aloud protocol analysis. Our experiments, although conducted on computer science students exhibiting only some of the characteristics of end-user modelers, show that VMTL compares favorably in terms of learnability with two state-of the-art model transformation languages: Epsilon and Henshin. Our think-aloud protocol analysis conﬁrms many of the design decisions adopted for VMTL, while also indicating possible improvements.",
        "keywords": [
            "End-user modelers",
            "Transparent model transformation",
            "VMTL",
            "Henshin",
            "Epsilon",
            "Learnability",
            "Experiment",
            "Think-aloud protocol"
        ],
        "authors": [
            "Vlad Acre¸toaie",
            "Harald Störrle",
            "Daniel Strüber"
        ],
        "file_path": "data/sosym-all/s10270-016-0546-9.pdf"
    },
    {
        "title": "Controllable and decomposable multidirectional synchronizations",
        "submission-date": "2020/08",
        "publication-date": "2021/04",
        "abstract": "Studying large-scale collaborative systems engineering projects across teams with differing intellectual property clearances,\nor healthcare solutions where sensitive patient data needs to be partially shared, or similar multi-user information systems over\ndatabases, all boils down to a common mathematical framework. Updateable views (lenses) and more generally bidirectional\ntransformations are abstractions to study the challenge of exchanging information between participants with different read\naccess privileges. The view provided to each participant must be different due to access control or other limitations, yet also\nconsistent in a certain sense, to enable collaboration towards common goals. A collaboration system must apply bidirectional\nsynchronization to ensure that after a participant modiﬁes their view, the views of other participants are updated so that\nthey are consistent again. While bidirectional transformations (synchronizations) have been extensively studied, there are\nnew challenges that are unique to the multidirectional case. If complex consistency constraints have to be maintained, \nsynchronizations that work ﬁne in isolation may not compose well. We demonstrate and characterize a failure mode of\nthe emergent behaviour, where a consistency restoration mechanism undoes the work of other participants. On the other\nend of the spectrum, we study the case where synchronizations work especially well together: we characterize very well-\nbehaved multidirectional transformations, a non-trivial generalization from the bidirectional case. For the former challenge, we \nintroduceanovel concept of controllability, whilefor thelatter one, weproposeanovel formal notionof faithful decomposition.\nAdditionally, the paper proposes several novel properties of multidirectional transformations.",
        "keywords": [
            "Bidirectional transformation",
            "Very well behaved",
            "Collaborative engineering"
        ],
        "authors": [
            "Gábor Bergmann"
        ],
        "file_path": "data/sosym-all/s10270-021-00879-w.pdf"
    },
    {
        "title": "Location-aware business process modeling and execution",
        "submission-date": "2023/10",
        "publication-date": "2024/11",
        "abstract": "Locally distributed processes include several process participants working on tasks at different locations, e.g., craftspeople working on construction sites. Compared to classical IT environments, new challenges emerge due to the spatial context of a process. Real-time location data from Internet of Things (IoT) devices can help businesses implement more efﬁcient and effective processes through business process management (BPM). However, only small parts of existing research have touched on those advantages, while the architecture and implementation of actual executable location-aware processes area has only been vaguely considered. Therefore, we introduce and present a non-exhaustive list of patterns for using location data in BPM while also including an actual implementation of a location-aware approach using a multilayer system architecture based on standard BPM technology. These can be used to leverage the location perspective of process entities as contextual data in BPM.",
        "keywords": [
            "Process execution",
            "Location-awareness",
            "Distributed processes"
        ],
        "authors": [
            "Leo Poss",
            "Stefan Schönig"
        ],
        "file_path": "data/sosym-all/s10270-024-01224-7.pdf"
    },
    {
        "title": "From use case maps to executable test procedures: a scenario-based approach",
        "submission-date": "2016/06",
        "publication-date": "2017/08",
        "abstract": "Testing embedded systems software has become a costly activity as these systems become more complex to fulfill rising needs. Testing processes should be both effective and affordable. An ideal testing process should begin with validated requirements and begin as early as possible so that requirements defects can be fixed before they propagate and become more difficult to address. Furthermore, the testing process should facilitate test procedures creation and automate their execution. We propose a novel methodology for testing functional requirements. The methodology activities include standard notations, such as UCM for modeling scenarios derived from requirements, TDL for describing test cases and TTCN-3 for executing test procedures; other test scripting languages can also be used with our methodology. Furthermore, the automation of the methodology generates test artifacts through model transformation. The main goals of this test methodology are to leverage requirements represented as scenarios, to replace the natural language test case descriptions with test scenarios in TDL, and to generate executable test procedures. Demonstration of the feasibility of the proposed approach is based on a public case study. An empirical evaluation of our approach is given using a case study from the avionics domain.",
        "keywords": [
            "Model-driven testing",
            "Testing methodology",
            "Embedded systems",
            "Test generation",
            "TTCN-3",
            "TDL",
            "UCM"
        ],
        "authors": [
            "Nader Kesserwan",
            "Rachida Dssouli",
            "Jamal Bentahar",
            "Bernard Stepien",
            "Pierre Labrèche"
        ],
        "file_path": "data/sosym-all/s10270-017-0620-y.pdf"
    },
    {
        "title": "From subsets of model elements to submodels A characterization of submodels and their properties",
        "submission-date": "2012/01",
        "publication-date": "2013/04",
        "abstract": "Model-driven engineering (MDE) generalized the status of models from documentation or model-driven architecture (MDA) modeling steps to full artifacts, members of a so-called structured “model space”. We concentrate here on the submodel relationship which contributes a lot to this structuring effort. Many works and MDE practices resort to this notion and call for its precise characterization, which is the intent of this paper. A typical situation is model management through repositories. We start from the deﬁnition of a model as a set of model elements plus a set of dependency constraints that it asserts over these elements. This allows to isolate the notions of closed, covariant and invariant submodels. As a major result, we show that submodel transitivity can be guaranteed thanks to submodel invariance. This formalization offers keys to analyze operations which manipulate submodels. For example, we deeply study the operator which consists in extracting a model from another one, when selecting some subset of its elements. The same can be applied to many other model operations and the last part of the paper is dedicated to a synthesis on related works which could proﬁt from this characterization. More practically, we show how the results were exploited in our Eclipse modeling environment.",
        "keywords": [
            "Submodel",
            "Set-theoretic formalization",
            "Model extraction",
            "Model composition"
        ],
        "authors": [
            "Bernard Carré",
            "Gilles Vanwormhoudt",
            "Olivier Caron"
        ],
        "file_path": "data/sosym-all/s10270-013-0340-x.pdf"
    },
    {
        "title": "METAMORPH: formalization of domain-speciﬁc conceptual modeling methods—an evaluative case study, juxtaposition and empirical assessment",
        "submission-date": "2021/11",
        "publication-date": "2022/10",
        "abstract": "Models have evolved from mere pictures supporting human understanding and communication to sophisticated knowledge structures processable by machines and establish value through their processing capabilities. This entails an inevitable need for computer-understandable modeling languages and causes formalization to be a crucial part in the lifecycle of engineering a modeling method. An appropriate formalism must be a means for providing a structural deﬁnition to enable a theoretical investigation of conceptual modeling languages and a unique, unambiguous way of specifying the syntax and semantics of an arbitrary modeling language. For this purpose, it must be generic and open to capturing any domain and any functionality. This paper provides a pervasive description of the formalism MetaMorph based on logic and model theory—an approach fulﬁlling the requirements above for modeling method engineering. The evaluation of the formalism is presented following three streams of work: First, two evaluative case studies illustrate the applicability of MetaMorph formalism concept by concept on the modeling language ProVis from the domain of stochastic education and the well-known Entity-Relationship language. ProVis as well as ER comprise only a few objects and relation types but with high interconnection and expressive power and are therefore considered interesting specimens for formalization. Second, a comprehensive juxtaposition of MetaMorph to three other formalization approaches based on different foundational theories is outlined concept by concept to underpin the formalism design. Third, an empirical evaluation has been performed, assessing the usability and adequacy of the formalism within a classroom assessment. The results allow for conclusions on the completeness, intuitiveness, and complexity as well as on interdependencies with engineers’ skills.",
        "keywords": [
            "Conceptual modeling",
            "Agile modeling method engineering",
            "Domain-speciﬁc modeling language",
            "Formal language",
            "Logic",
            "Evaluation"
        ],
        "authors": [
            "Victoria Döller",
            "Dimitris Karagiannis",
            "Wilfrid Utz"
        ],
        "file_path": "data/sosym-all/s10270-022-01047-4.pdf"
    },
    {
        "title": "Editorial",
        "submission-date": "2002/10",
        "publication-date": "2003/11",
        "abstract": "This editorial provides a report on the first year of the Software and System Modeling (SoSyM) journal. It discusses the journal's launch, growth, and challenges, as well as acknowledging the contributions of reviewers, editors, and publication staff.",
        "keywords": [],
        "authors": [
            "Robert B. France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-003-0041-y.pdf"
    },
    {
        "title": "Unifying classes and processes",
        "submission-date": "2005/06",
        "publication-date": "2005/06",
        "abstract": "Previously, we presented Circus, an integration of Z, CSP, and Morgan’s refinement calculus, with a semantics based on the unifying theories of programming. Circus provides a basis for development of state-rich concurrent systems; it has a formal semantics, a refinement theory, and a development strategy. The design of Circus is our solution to combining data and behavioural specifications. Here, we further explore this issue in the context of object-oriented features. Concretely, we present an object-oriented extension of Circus called OhCircus. We present its syntax, describe its semantics, explain the formalisation of method calls, and discuss our approach to refinement.",
        "keywords": [
            "Z",
            "CSP",
            "Refinement",
            "Integration"
        ],
        "authors": [
            "Ana Cavalcanti",
            "Augusto Sampaio",
            "Jim Woodcock"
        ],
        "file_path": "data/sosym-all/s10270-005-0085-2.pdf"
    },
    {
        "title": "Design methods for the new database era: a systematic literature review",
        "submission-date": "2018/09",
        "publication-date": "2019/06",
        "abstract": "Over the last decade, a range of new database solutions and technologies have emerged, in line with the new types of applications and requirements that they facilitate. Consequently, various new methods for designing these new databases have evolved, in order to keep pace with progress in the field. In this paper, we systematically review these methods, with a view to better understanding their suitability for designing new database solutions. The study shows that while research in the field has expanded continuously, a range of factors still require further attention. The study identified important criteria in database design and analyzed existing studies accordingly. This analysis will assist in defining and recommending key areas for future research, guiding the evolution of design methods, their usability and adaptability in real-world scenarios. The study found that current database design methods do not address non-functional requirements; tend to refer to a preselected database; and are lacking in their evaluation.",
        "keywords": [
            "Database design",
            "Database modeling",
            "NoSQL",
            "Design methods"
        ],
        "authors": [
            "Noa Roy‑Hubara",
            "Arnon Sturm"
        ],
        "file_path": "data/sosym-all/s10270-019-00739-8.pdf"
    },
    {
        "title": "Guest editorial for EMMSAD’2018 special section",
        "submission-date": "2019/10",
        "publication-date": "2019/12",
        "abstract": "The exploring modeling methods for systems analysis and development (EMMSAD) series has produced 23 events, associated with conference on advanced information systems engineering (CAiSE), from 1996 to 2018. From 2009, EMMSAD has become a two-day working conference. The topics addressed by the EMMSAD series focus on modeling methods for software and information systems development [4], enterprise management [3], and business process management [1]. It further refers to evaluation of modeling methods through a variety of empirical and non-empirical approaches (a review and comparative analysis of such evaluation techniques can be found at [6]). The aims, topics, and history of EMMSAD can be found on the Website at http://www.emmsa​d.org/. ",
        "keywords": [],
        "authors": [
            "Iris Reinhartz‑Berger",
            "Sérgio Guerreiro"
        ],
        "file_path": "data/sosym-all/s10270-019-00769-2.pdf"
    },
    {
        "title": "What makes life for process mining analysts difﬁcult? A reﬂection of challenges",
        "submission-date": "2023/01",
        "publication-date": "2023/11",
        "abstract": "Over the past few years, several software companies have emerged that offer process mining tools to assist enterprises in gaining insights into their process executions. However, the effective application of process mining technologies depends on analysts who need to be proﬁcient in managing process mining projects and providing process insights and improvement opportunities. To contribute to a better understanding of the difﬁculties encountered by analysts and to pave the way for the development of enhanced and tailored support for them, this work reveals the challenges they perceive in practice. In particular, we identify 23 challenges based on interviews with 41 analysts, which we validate using a questionnaire survey. We provide insights into the relevancy of the process mining challenges and present mitigation strategies applied in practice to overcome them. While mitigation strategies exist, our ﬁndings imply the need for further research to provide support for analysts along all phases of process mining projects on the individual level, but also the technical, group, and organizational levels.",
        "keywords": [
            "Process mining",
            "Challenges",
            "Mitigation strategies",
            "Process analysis",
            "Work practices"
        ],
        "authors": [
            "Lisa Zimmermann",
            "Francesca Zerbato",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-023-01134-0.pdf"
    },
    {
        "title": "A survey on the design space of end-user-oriented languages for specifying robotic missions",
        "submission-date": "2019/11",
        "publication-date": "2021/02",
        "abstract": "Mobile robots are becoming increasingly important in society. Fulﬁlling complex missions in different contexts and envi-ronments, robots are promising instruments to support our everyday live. As such, the task of deﬁning the robot’s mission is moving from professional developers and roboticists to the end-users. However, with the current state-of-the-art, deﬁning missions is non-trivial and typically requires dedicated programming skills. Since end-users usually lack such skills, many commercial robots are nowadays equipped with environments and domain-speciﬁc languages tailored for end-users. As such, the software support for deﬁning missions is becoming an increasingly relevant criterion when buying or choosing robots. Improving these environments and languages for specifying missions toward simplicity and ﬂexibility is crucial. To this end, we need to improve our empirical understanding of the current state-of-the-art of such languages and their environments. In this paper, we contribute in this direction. We present a survey of 30 mission speciﬁcation environments for mobile robots that come with a visual and end-user-oriented language. We explore the design space of these languages and their environments, identify their concepts, and organize them as features in a feature model. We believe that our results are valuable to prac-titioners and researchers designing the next generation of mission speciﬁcation languages in the vibrant domain of mobile robots.",
        "keywords": [
            "Speciﬁcation environments",
            "Language concepts",
            "Visual languages",
            "Robotic missions",
            "Empirical study"
        ],
        "authors": [
            "Swaib Dragule",
            "Thorsten Berger",
            "Claudio Menghi",
            "Patrizio Pelliccione"
        ],
        "file_path": "data/sosym-all/s10270-020-00854-x.pdf"
    },
    {
        "title": "Model-based development",
        "submission-date": "2007/11",
        "publication-date": "2007/11",
        "abstract": "This issue contains the second part of the regular papers that have been nurtured from Models’05 conference.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-007-0071-y.pdf"
    },
    {
        "title": "Application of reﬂection in a model transformation language",
        "submission-date": "2008/11",
        "publication-date": "2009/11",
        "abstract": "Computational reﬂection is a well-known tech-\nnique applied in many existing programming languages\nranging from functional to object-oriented languages. In this\npaper we study the possibilities and beneﬁts of introducing\nand using reﬂection in a rule-based model transformation\nlanguage. The paper identiﬁes some language abstractions to\nachieve structural and behavioral reﬂection. Reﬂective fea-\ntures are motivated by examples of problems derived from\nthe experience with currently used transformation languages.\nExample solutions are given by using an experimental lan-\nguage with reﬂective capabilities. The paper also outlines\npossible implementation strategies for adding reﬂection to a\nlanguage and discusses their advantages and disadvantages.",
        "keywords": [
            "Reﬂection",
            "Model transformation languages",
            "MDE",
            "MISTRAL"
        ],
        "authors": [
            "Ivan Kurtev"
        ],
        "file_path": "data/sosym-all/s10270-009-0138-z.pdf"
    },
    {
        "title": "Model‑based test case generation and prioritization: a systematic literature review",
        "submission-date": "2020/12",
        "publication-date": "2021/09",
        "abstract": "Model-based test case generation (MB-TCG) and prioritization (MB-TCP) utilize models that represent the system under test (SUT) for test generation and prioritization in software testing. They are based on model-based testing (MBT), a technique that facilitates automation in testing. Automated testing is indispensable for testing complex and industrial-size systems because of its advantages over manual testing. In recent years, MB-TCG and MB-TCP publications have shown an encourag-ing growth. However, the empirical studies done to validate these approaches must not be taken lightly because they reflect the results' validity and whether these approaches are generalizable to the industrial context. This systematic review aims at identifying and reviewing the state-of-the-art for MB-TCG, MB-TCP, and the approaches that combined MB-TCG and MB-TCP. The needs for this review were used to design the research questions. Keywords extracted from the research questions were utilized to search for studies in the literature that will answer the research questions. Prospective studies also underwent a quality assessment to ensure that only studies with sufficient quality were selected. All the research data of this review are also available in a public repository for full transparency. 122 primary studies were finalized and selected. There were 100, 15, and seven studies proposed for MB-TCG, MB-TCP, and MB-TCG and MB-TCP combination approaches, respectively. One of the main findings is that the most common limitations in the existing approaches are the dependency on specifica-tions, the need for manual interventions, and the scalability issue.",
        "keywords": [
            "Model-based testing",
            "Test case prioritization",
            "Test case generation",
            "Systematic literature review"
        ],
        "authors": [
            "Muhammad Luqman Mohd‑Shafie",
            "Wan Mohd Nasir Wan Kadir",
            "Horst Lichter",
            "Muhammad Khatibsyarbini",
            "Mohd Adham Isa"
        ],
        "file_path": "data/sosym-all/s10270-021-00924-8.pdf"
    },
    {
        "title": "Software and systems modeling with graph transformations theme issue of the Journal on Software and Systems Modeling",
        "submission-date": "2012/06",
        "publication-date": "2012/06",
        "abstract": "Over the years model-based development has rapidly gained popularity in various engineering disciplines. Numerous efforts have resulted in the invention of an abundance of appropriate modeling concepts, languages, and tools. Today modeling activities often span multiple disciplines and have to be addressed by collaborative efforts across disci-plines such as industrial automation, business engineering, hardware/softwareco-design,real-timesystemdevelopment, Web 2.0 application design, and so forth. As a consequence, model-based development techniques related to the analysis, synchronization, and execution of families of models that are concurrently developed by different engineers on differ-ent levels of abstraction play a major role in many software and systems development projects. Graphs, on the other hand, are among the simplest and most universal models for a variety of systems, not just in computer science, but throughout engineering and the life sciences. Graph transformations combine the idea of graphs as a universal modeling paradigm with a rule-based mathematically well-founded approach to specify their evolution.",
        "keywords": [],
        "authors": [
            "Andy Schürr",
            "Arend Rensink"
        ],
        "file_path": "data/sosym-all/s10270-012-0254-z.pdf"
    },
    {
        "title": "IAT/ML: a metamodel and modelling approach for discourse analysis",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "Languagetechnologiesaregainingmomentumastextualinformationsaturatessocialnetworksandmediaoutlets,compounded\nby the growing role of fake news and disinformation. In this context, approaches to represent and analyse public speeches,\nnews releases, social media posts and other types of discourses are becoming crucial. Although there is a large body of\nliterature on text-based machine learning, it tends to focus on lexical and syntactical issues rather than semantic or pragmatic.\nBeing useful, these advances cannot tackle the nuanced and highly context-dependent problems of discourse evaluation that\nsociety demands. In this paper, we present IAT/ML, a metamodel and modelling approach to represent and analyse discourses.\nIAT/ML focuses on semantic and pragmatic issues, thus tackling a little researched area in language technologies. It does so by\ncombining three different modelling approaches: ontological, which focuses on what the discourse is about; argumentation,\nwhich deals with how the text justiﬁes what it says; and agency, which provides insights into the speakers’ beliefs, desires\nand intentions. Together, these three modelling approaches make IAT/ML a comprehensive solution to represent and analyse\ncomplex discourses towards their understanding, evaluation and fact checking.",
        "keywords": [
            "Natural language",
            "Discourse",
            "Argumentation",
            "Ontologies",
            "Metamodel",
            "IAT/ML"
        ],
        "authors": [
            "Cesar Gonzalez-Perez",
            "Martín Pereira-Fariña",
            "Beatriz Calderón-Cerrato",
            "Patricia Martín-Rodilla"
        ],
        "file_path": "data/sosym-all/s10270-024-01208-7.pdf"
    },
    {
        "title": "The next evolution of MDE: a seamless integration of machine learning into domain modeling",
        "submission-date": "2016/08",
        "publication-date": "2017/05",
        "abstract": "Machine learning algorithms are designed to resolveunknownbehaviorsbyextractingcommonalitiesover massive datasets. Unfortunately, learning such global behaviors can be inaccurate and slow for systems composed of heterogeneous elements, which behave very differently, for instance as it is the case for cyber-physical systems and Internet of Things applications. Instead, to make smart decisions, such systems have to continuously reﬁne the behavior on a per-element basis and compose these small learning units together. However, combining and composing learned behaviors from different elements is challenging and requires domain knowledge. Therefore, there is a need to structure and combine the learned behaviors and domain knowl-edge together in a ﬂexible way. In this paper we propose to weave machine learning into domain modeling. More speciﬁcally, we suggest to decompose machine learning into reusable, chainable, and independently computable small learning units, which we refer to as microlearning units. These microlearning units are modeled together with and at the same level as the domain data. We show, based on a smart grid case study, that our approach can be signiﬁcantly more accurate than learning a global behavior, while the per-formance is fast enough to be used for live learning.",
        "keywords": [
            "Domain modeling",
            "Live learning",
            "Model-driven engineering",
            "Metamodeling",
            "Cyber-physical systems",
            "Smart grids"
        ],
        "authors": [
            "Thomas Hartmann",
            "Assaad Moawad",
            "Francois Fouquet",
            "Yves Le Traon"
        ],
        "file_path": "data/sosym-all/s10270-017-0600-2.pdf"
    },
    {
        "title": "Guest editorial to the special issue on UML 2004",
        "submission-date": "2006/03",
        "publication-date": "2006/07",
        "abstract": "The UML 2004 was held in Lisbon (Portugal), 11–15, October 2004 and was the seventh conference in the series of annual UML conferences. The 2004 edition of the UML conference proved to be a milestone event since it was the last conference under the UML banner, and the announcement of the new format, MoDELS (MOdel Driven Engineering, Languages and Systems).",
        "keywords": [],
        "authors": [
            "Thomas Baar",
            "Ana Moreira"
        ],
        "file_path": "data/sosym-all/s10270-006-0021-0.pdf"
    },
    {
        "title": "Completion of SysML state machines from Given–When–Then requirements",
        "submission-date": "2023/08",
        "publication-date": "2024/11",
        "abstract": "MDE enables the centrality of the models in semi-automated development processes. However, its level of usage in industrial settings is still not adequate for the beneﬁts MDE can introduce. This paper proposes a semi-automatic approach for the completion of high-level models in the lifecycle of critical systems, which exhibit an event-driven behaviour. The proposal suggests a speciﬁcation guideline that starts from a partial SysML model of a system and on a set of requirements, expressed in the well-known Given–When–Then paradigm. On the basis of such requirements, the approach enables the semi-automatic generation of new SysML state machines model elements. Accordingly, the approach focuses on the completion of the state machines by adding proper transitions (with triggers, guards and effects) among pre-existing states. Also, traceability modelling elements are added to the model. Two case studies demonstrate the feasibility of the proposed approach.",
        "keywords": [
            "Behaviour-driven development",
            "Requirements engineering",
            "SysML",
            "Critical systems design",
            "Event-driven systems design"
        ],
        "authors": [
            "Maria Stella de Biase",
            "Simona Bernardi",
            "Stefano Marrone",
            "José Merseguer",
            "Angelo Palladino"
        ],
        "file_path": "data/sosym-all/s10270-024-01228-3.pdf"
    },
    {
        "title": "A formal veriﬁcation framework for static analysis",
        "submission-date": "2013/11",
        "publication-date": "2015/07",
        "abstract": "Static analysis tools, such as resource analyzers, give useful information on software systems, especially in real-time and safety-critical applications. Therefore, the question of the reliability of the obtained results is highly important. State-of-the-art static analyzers typically combine a range of complex techniques, make use of external tools, and evolve quickly. To formally verify such systems is not a realistic option. In this work, we propose a different approach whereby, instead of the tools, we formally verify the results of the tools. The central idea of such a formal veriﬁcation framework for static analysis is the method-wise translation of the information about a program gathered during its static analysis into speciﬁcation contracts that contain enough information for them to be veriﬁed automatically. We instantiate this framework with costa, a state-of-the-art static analysis system for sequential Java programs, for producing resource guarantees and KeY, a state-of-the-art veriﬁcation tool, for formally verifying the correctness of such resource guarantees. Resource guarantees allow to be certain that programs will run within the indicated amount of resources, which may refer to memory consumption, number of instructions executed, etc. Our results show that the proposed tool cooperation can be used for automatically producing veriﬁed resource guarantees.",
        "keywords": [
            "Cost analysis",
            "Closed-form upper bounds",
            "Resource analysis",
            "Resource guarantees"
        ],
        "authors": [
            "Elvira Albert\nRichard Bubel\nSamir Genaim\nReiner Hähnle\nGermán Puebla\nGuillermo Román-Díez"
        ],
        "file_path": "data/sosym-all/s10270-015-0476-y.pdf"
    },
    {
        "title": "Design for service compatibility Behavioural compatibility checking and diagnosis",
        "submission-date": "2012/08",
        "publication-date": "2012/08",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Georg Grossmann",
            "Michael Schreﬂ",
            "Markus Stumptner"
        ],
        "file_path": "data/sosym-all/s10270-012-0267-7.pdf"
    },
    {
        "title": "Modeling for the cloud",
        "submission-date": "2010/03",
        "publication-date": "2010/03",
        "abstract": "Cloud computing is poised to become a major driving force behind European and American businesses. Long-standing projects like the SETI@Home project and facilities such as SourceForge leverage third party distributed storage and computational resources to deliver services. Companies are seeking to commercialize this approach to service delivery through the use of cloud computing technologies. Cloud computing commerce can take several forms: customers can rent an infrastructure, a platform, or predeﬁned services. While predeﬁned cloud-based services for email, blogs, wikis, and media storage are well known, more complex business oriented applications like customer relationship management are starting to appear. While companies such as Amazon, Google, and Force.com are providing services for and from the cloud there are aspects of cloud com- puting that can beneﬁt from research in the model-driven software development area. For example, software and sys- tem modeling research can yield results that address prob- lems related to the safety and integrity of data (where the user does not control the physical location of the storage anymore), efﬁciency of storage and retrieval, and decou- pling of applications from underlying operating systems, and other computing platforms and infrastructures. Software and system modeling research can also produce results that head-off future problems related to migration of services to new cloud computing environments that will inevitably arise as technologies evolve. For example, enabling inter- operability across cloud computing environments, and inte- grating mobile and cloud-based applications are challenging problems that will arise.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-010-0159-7.pdf"
    },
    {
        "title": "Change propagation and bidirectionality in internal transformation DSLs",
        "submission-date": "2016/06",
        "publication-date": "2017/08",
        "abstract": "Despite good results in several industrial projects, model-driven engineering (MDE) has not been widely adopted in industry. Although MDE has existed for more than a decade now, the lack of tool support is still one of the major problems, according to studies by Staron and Mohaghegi (Staron, in: Model driven engineering languages and systems, Springer, Berlin, 2006; Mohagheghi et al. in Empir Softw Eng 18(1):89–116, 2013). Internal languages offer a solution to this problem for model transformations, which are a key part of MDE. Developers can use exist-ing tools of host languages to create model transformations in a familiar environment. These internal languages, however, typically lack key features such as change propagation or bidirectional transformations. In our opinion, one reason is that existing formalisms for these properties are not well suited for textual languages. In this paper, we present a new formalism describing incremental, bidirectional model syn-chronizations using synchronization blocks. We prove the ability of this formalism to detect and repair inconsistencies and show its hippocraticness. We use this formalism to create a single internal model transformation language for unidirec-tional and bidirectional model transformations with optional change propagation. In total, we currently provide 18 opera-tion modes based on a single speciﬁcation. At the same time, the language may reuse tool support for C#. We validate the applicabilityofourlanguageusingasyntheticexamplewitha transformation from ﬁnite state machines to Petri nets where we achieved speedups of up to multiple orders of magnitude compared to classical batch transformations.",
        "keywords": [
            "Model-driven engineering",
            "Model synchronization",
            "Domain-speciﬁc language",
            "Change propagation",
            "Bidirectional",
            "Incremental"
        ],
        "authors": [
            "Georg Hinkel",
            "Erik Burger"
        ],
        "file_path": "data/sosym-all/s10270-017-0617-6.pdf"
    },
    {
        "title": "Handling causality and schedulability when designing and prototyping cyber-physical systems",
        "submission-date": "2020/02",
        "publication-date": "2021/02",
        "abstract": "Cyberphysicalsystemsarebuiltupondigitalandanalogcircuits,makingitnecessarytohandledifferentmodelsofcomputation during their design and veriﬁcation (e.g., by simulation). When designing these systems, an important aspect to consider is the causality between the different domains. For this, we introduce a new model-driven framework able to identify causality problems and to suggest a valid schedule between the analog and digital domains. Once a valid schedule has been computed, our framework can generate cycle and bit accurate virtual prototypes (in SystemC/SystemC AMS) from high-level SysML models.",
        "keywords": [
            "Cyber-physical systems",
            "Virtual prototyping",
            "Co-simulation"
        ],
        "authors": [
            "Rodrigo Cortés Porto1",
            "2",
            "Daniela Genius1",
            "Ludovic Apvrille3"
        ],
        "file_path": "data/sosym-all/s10270-021-00866-1.pdf"
    },
    {
        "title": "An executable metamodel refactoring catalog",
        "submission-date": "2022/06",
        "publication-date": "2022/08",
        "abstract": "Like any software artifacts, metamodels are evolving entities that constantly change over time for different reasons. Changing metamodels by keeping them consistent with other existing artifacts is an error-prone and tedious activity without the availability of automated support. In this paper, we foster the adoption of metamodel refactorings collected in a curated catalog. The Edelta framework is proposed as an operative environment to provide modelers with constructs for specifying basic refactorings and evolution operators, to deﬁne a complete metamodel refactoring catalog. The proposed environment has been used to implement the metamodel refactorings available in the literature and make them executable. A detailed discussion on how modelers can use and contribute to the deﬁnition of the catalog is also given.",
        "keywords": [
            "Metamodels",
            "Evolution",
            "Refactoring",
            "Catalog"
        ],
        "authors": [
            "Lorenzo Bettini",
            "Davide Di Ruscio",
            "Ludovico Iovino",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-022-01034-9.pdf"
    },
    {
        "title": "Guest editorial for EMMSAD’2023 special section",
        "submission-date": "2024/09",
        "publication-date": "2024/09",
        "abstract": "The Exploring Modeling Methods for Systems Analysis and Development (EMMSAD) conference series organized 29 events\nfrom 1996 to 2024, associated with Conference on Advanced Information Systems Engineering. In 2009, EMMSAD became\na two-day working conference. Since 2017, the authors of EMMSAD’s best papers are invited to submit extended versions\nof their paper, for consideration to be published in the Journal of Software and Systems Modeling. The main topics of the\nEMMSAD series focus on models and modeling methods for the analysis and development of software information systems\nof any kind. These are organized into ﬁve tracks: (1) Foundations of Modeling and Method Engineering; (2) Enterprise, Business, Process, and Capability Modeling; (3) Information Systems and Requirements Modeling; (4) Domain-Speciﬁc and Knowledge Modeling; and (5) Evaluation of Models and Modeling Approaches. The aims, topics, and history of EMMSAD\ncan be also found on its website at http://www.emmsad.org/.",
        "keywords": [],
        "authors": [
            "Dominik Bork",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-024-01213-w.pdf"
    },
    {
        "title": "Fault localization in DSLTrans model transformations by combining symbolic execution and spectrum-based analysis",
        "submission-date": "2022/09",
        "publication-date": "2023/09",
        "abstract": "The veriﬁcation of model transformations is important for realizing robust model-driven engineering technologies and quality-assured automation. Many approaches for checking properties of model transformations have been proposed. Most of them have focused on the effective and efﬁcient detection of property violations by contract checking. However, there remains the fault localization step between identifying a failing contract for a transformation based on veriﬁcation feedback and precisely identifying the faulty rules. While there exist fault localization approaches in the model transformation veriﬁcation literature, these require the creation and maintenance of test cases, which imposes an additional burden on the developer. In this paper, we combine transformation veriﬁcation based on symbolic execution with spectrum-based fault localization techniques for identifying the faulty rules in DSLTrans model transformations. This fault localization approach operates on the path condition output of symbolic transformation checkers instead of requiring a set of test input models. In particular, we introduce a workﬂow for running the symbolic execution of a model transformation, evaluating the deﬁned contracts for satisfaction, and computing different measures for tracking the faulty rules. We evaluate the effectiveness of spectrum-based analysis techniques for tracking faulty rules and compare our approach to previous works. We evaluate our technique by introducing known mutations into ﬁve model transformations. Our results show that the best spectrum-based analysis techniques allow for effective fault localization, showing an average EXAM score below 0.30 (less than 30% of the transformation needs to be inspected). These techniques are also able to locate the faulty rule in the top-three ranked rules in 70% of all cases. The impact of the model transformation, the type of mutation and the type of contract on the results is discussed. Finally, we also investigate the cases where the technique does not work properly, including discussion of a potential pre-check to estimate the prospects of the technique for a certain transformation.",
        "keywords": [],
        "authors": [
            "Bentley James Oakes",
            "Javier Troya",
            "Jessie Galasso",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-023-01123-3.pdf"
    },
    {
        "title": "Grand challenges in model-driven engineering: an analysis of the state of the research",
        "submission-date": "2019/11",
        "publication-date": "2020/01",
        "abstract": "In 2017 and 2018, two events were held—in Marburg, Germany, and San Vigilio di Marebbe, Italy, respectively—focusing on an analysis of the state of research, state of practice, and state of the art in model-driven engineering (MDE). The events brought together experts from industry, academia, and the open-source community to assess what has changed in research in MDE over the last 10 years, what challenges remain, and what new challenges have arisen. This article reports on the results of those meetings, and presents a set of grand challenges that emerged from discussions and synthesis. These challenges could lead to research initiatives for the community going forward.",
        "keywords": [
            "Model-driven engineering",
            "Grand challenge",
            "Research roadmap"
        ],
        "authors": [
            "Antonio Bucchiarone",
            "Jordi Cabot",
            "Richard F. Paige",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-019-00773-6.pdf"
    },
    {
        "title": "Editorial to theme section on interplay of model-driven and component-based software engineering",
        "submission-date": "2020/06",
        "publication-date": "2020/07",
        "abstract": "This theme section aims to disseminate the latest trends in the use and combination of Model-Driven Engineering (MDE) and Component-Based Software Engineering (CBSE). On the one hand, MDE aims to increase productivity in the development of complex systems while reducing the time to market. On the other hand, CBSE aims to deliver and then support the exploitation of reusable “off-the-shelf” software components that can be incorporated into larger applications. An effective interplay of MDE and CBSE can yield benefits to both communities: the CBSE community would benefit from implementation and automation capabilities of MDE, the MDE community would benefit from the foundational nature of CBSE.",
        "keywords": [],
        "authors": [
            "Federico Ciccozzi",
            "Antonio Cicchetti",
            "Andreas Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-020-00812-7.pdf"
    },
    {
        "title": "Mining Frequent Structures in Conceptual Models",
        "submission-date": "2024/06",
        "publication-date": "2025/04",
        "abstract": "The challenge of using structured methods to represent knowledge is a well-documented issue in conceptual modeling and has been the focus of extensive research. It is widely recognized that adopting modeling patterns offers an effective structural approach for designing conceptual models. Patterns, in this context, refer to generalizable, recurring structures that provide solutions to common design problems. They signiﬁcantly enhance both the understanding and improvement of the modeling process. Numerous experimental studies have demonstrated the undeniable value of using patterns in conceptual modeling. Despite this, the task of identifying patterns in conceptual models remains highly complex, and there is currently no systematic method for pattern discovery. To address this gap, this paper proposes a general approach for discovering frequent structures in conceptual modeling languages as a means to support pattern identiﬁcation. Speciﬁcally, we focus on uncovering recurring structures that reﬂect the usage patterns of a given conceptual modeling language. As proof of concept, we implement our approach by focusing on two widely used conceptual modeling languages. This implementation includes an exploratory tool that integrates a frequent subgraph mining algorithm with graph manipulation techniques, such as graph visualization, graph clustering, and graph transformation. The tool processes multiple conceptual models and identiﬁes recurrent structures based on various criteria. We validate the tool using two state-of-the-art curated datasets: one consisting of models encoded in OntoUML and the other in ArchiMate. The primary objective of our approach is to provide a support tool for language engineers. This tool can be used to identify both effective and ineffective modeling practices, enabling the reﬁnement and evolution of conceptual modeling languages. Furthermore, it facilitates the reuse of accumulated expertise, ultimately supporting the creation of higher-quality models in a given language.",
        "keywords": [
            "Conceptual modeling",
            "Mining conceptual models",
            "Frequent subgraph mining",
            "Recurrent modeling structures",
            "Modeling patterns"
        ],
        "authors": [
            "Mattia Fumagalli",
            "Tiago Prince Sales",
            "Pedro Paulo F. Barcelos",
            "Giovanni Micale",
            "Philipp-Lorenz Glaser",
            "Dominik Bork",
            "Vadim Zaytsev",
            "Diego Calvanese",
            "Giancarlo Guizzardi"
        ],
        "file_path": "data/sosym-all/s10270-025-01295-0.pdf"
    },
    {
        "title": "A fundamental approach to model versioning based on graph modiﬁcations: from theory to implementation",
        "submission-date": "2011/04",
        "publication-date": "2012/04",
        "abstract": "In model-driven engineering, models are primary artifacts that can evolve heavily during their life cycle. Therefore, versioning of models is a key technique to be offered by integrated development environments for model-driven engineering. In contrast to text-based versioning systems, we present an approach that takes model structures and their changes over time into account. Considering model structures as graphs, we deﬁne a fundamental approach where model revisions are considered as graph modiﬁcations consisting of delete and insert actions. Two different kinds of conﬂict detection are presented: (1) the check for operation-based conﬂicts between different graph modiﬁcations, and (2) the check for state-based conﬂicts on merged graph modiﬁcations. For the merging of graph modiﬁcations, a two-phase approach is proposed: First, operational conﬂicts are temporarily resolved by always giving insertion priority over deletion to keep as much information as possible. There- after, this tentative merge result is the basis for manual con- ﬂict resolution as well as for the application of repair actions that resolve state-based conﬂicts. If preferred by the user, giving deletion priority over insertion might be one solution. The fundamental concepts are illustrated by versioning sce- narios for simpliﬁed statecharts. Furthermore, we show an implementation of this fundamental approach to model ver- sioning based on the Eclipse Modeling Framework as tech- nical space.",
        "keywords": [
            "Model versioning",
            "Graph modiﬁcation",
            "Conﬂict detection",
            "Conﬂict resolution"
        ],
        "authors": [
            "Gabriele Taentzer",
            "Claudia Ermel",
            "Philip Langer",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-012-0248-x.pdf"
    },
    {
        "title": "Introduction to the theme issue on variability modeling of software-intensive systems",
        "submission-date": "2015/08",
        "publication-date": "2017/01",
        "abstract": "Variability in software-intensive systems (such as engine control or driver assistance systems in modern vehicles, ﬂight control systems in the aviation sector) is a major challenge when developing high quality systems efﬁciently and in short release cycles. New technologies and architectural paradigms will make the development and operation of variable-products even more difﬁcult and risky in future. This theme issue focuses broadly on innovative work in the area of variability modeling and management. We have particularly invited contributions with a strong variability modeling aspect, but also addressing the wider area of variability management, e.g., requirements, architecture, analysis, implementation, evolution and teaching of variability modeling. The purpose is to present new results for mastering variability throughout the whole lifecycle of systems, system families, and product lines.",
        "keywords": [],
        "authors": [
            "Andrzej W˛asowski",
            "Thorsten Weyer"
        ],
        "file_path": "data/sosym-all/s10270-015-0501-1.pdf"
    },
    {
        "title": "A veriﬁed catalogue of OCL optimisations",
        "submission-date": "2019/02",
        "publication-date": "2019/07",
        "abstract": "OCL is widely used by model-driven engineering tools with different purposes like writing integrity constraints for meta-models, as a navigation language in model transformation languages or to deﬁne transformation speciﬁcations. Another scenario is the automatic generation of OCL code by a repair system. These generated expressions tend to be complex and unreadable due to the nature of the generative process. However, to be useful this code should be simple and resemble manually written code as much as possible when a developer must manually maintain it. There exists refactorings approaches for manually written OCL code, but there is no tool targeted to the optimisation of OCL expressions which have been automatically synthesised. Moreover, there is no available catalogue of OCL refactorings which can be integrated seamlessly into a tool. In this work, we contribute a set of refactorings intended to optimise OCL expressions, notably covering cases likely to arise in generated OCL code. We also contribute the implementation of these refactorings, built as a generic transformation catalogue using bent¯o, a transformation reuse tool for ATL. This makes it possible to specialise the catalogue for any OCL variant based on Ecore. Moreover, we propose a method to verify the correctness of the implemented catalogue based on translation validation and model ﬁnding. We describe the design and implementation of the catalogue and evaluate it by optimising a large amount of OCL expressions and proving the correctness of each optimisation execution. We also derive working implementations of the catalogue for ATL, EMF/OCL and SimpleOCL made available in a tool called BeautyOCL.",
        "keywords": [
            "Model transformations",
            "OCL",
            "Refactoring",
            "Veriﬁcation"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-019-00740-1.pdf"
    },
    {
        "title": "A unifying framework for homogeneous model composition",
        "submission-date": "2018/01",
        "publication-date": "2019/01",
        "abstract": "The growing use of models for separating concerns in complex systems has lead to a proliferation of model composition operators. These composition operators have traditionally been deﬁned from scratch following various approaches differing in formality, level of detail, chosen paradigm, and styles. Due to the lack of proper foundations for deﬁning model composition (concepts, abstractions, or frameworks), it is difﬁcult to compare or reuse composition operators. In this paper, we stipulate the existence of a unifying framework that reduces all structural composition operators to structural merging, and all composition operators acting on discrete behaviors to event scheduling. We provide convincing evidence of this hypothesis by discussing how structural and behavioral homogeneous model composition operators (i.e., weavers) can be mapped onto this framework. Based on this discussion, we propose a conceptual model of the framework and identify a set of research challenges, which, if addressed, lead to the realization of this framework to support rigorous and efﬁcient engineering of model composition operators for homogeneous and eventually heterogeneous modeling languages.",
        "keywords": [
            "Model composition",
            "Symmetric merge",
            "Event scheduling",
            "Event structures",
            "Separation of concerns"
        ],
        "authors": [
            "Jörg Kienzle",
            "Gunter Mussbacher",
            "Benoit Combemale",
            "Julien Deantoni"
        ],
        "file_path": "data/sosym-all/s10270-018-00707-8.pdf"
    },
    {
        "title": "Claimed advantages and disadvantages of (dedicated) model transformation languages: a systematic literature review",
        "submission-date": "2019/11",
        "publication-date": "2020/07",
        "abstract": "There exists a plethora of claims about the advantages and disadvantages of model transformation languages compared to general-purpose programming languages. With this work, we aim to create an overview over these claims in the literature and systematize evidence thereof. For this purpose, we conducted a systematic literature review by following a systematic process for searching and selecting relevant publications and extracting data. We selected a total of 58 publications, categorized claims about model transformation languages into 14 separate groups and conceived a representation to track claims and evidence through the literature. From our results, we conclude that: (i) the current literature claims many advantages of model transformation languages but also points towards certain deﬁcits and (ii) there is insufﬁcient evidence for claimed advantages and disadvantages and (iii) there is a lack of research interest into the veriﬁcation of claims.",
        "keywords": [
            "Model transformation language",
            "DSL",
            "Model transformation",
            "MDSE",
            "Advantages",
            "Disadvantages"
        ],
        "authors": [
            "Stefan Götz",
            "Matthias Tichy",
            "Raﬀaela Groner"
        ],
        "file_path": "data/sosym-all/s10270-020-00815-4.pdf"
    },
    {
        "title": "Design and automation of a COSMIC measurement procedure based on UML models",
        "submission-date": "2018/01",
        "publication-date": "2019/04",
        "abstract": "Context. Many organizations are adopting the COSMIC method to size software products for estimating and controlling their development costs and performances. Using a functional size measurement method requires specialized expertise and can be time-consuming. Objectives. Since UML is the de facto industrial modeling language standard for object-oriented systems, it is very useful to understand how to exploit UML models for measuring software systems and for developing tools that can automatically derive the COSMIC size from them. This paper provides an answer to these needs. Method. We present a measurement procedure to derive the COSMIC functional size from UML software artifacts and a tool, named J-UML COSMIC, for the automation of the procedure. Based on the observation that different development processes are characterized by the use of different UML models, the tool has been designed to work with different UML artifacts (such as use case models, package diagrams, component diagrams, class diagrams, activity diagrams, and sequence diagrams) and to adapt to the speciﬁc employed process. To assess the measurement procedure and J-UML COSMIC, we have carried out two case studies and compared the measurement results provided by the tool with the ones obtained by experts applying the standard COSMIC method. Results. Using the proposed measurement procedure the tool is able to identify from UML software models all the COSMIC concepts and data movements identiﬁed by the experts. Moreover, the tool allows us to obtain incremental accurate measurements when new models are considered or existing ones are detailed. Conclusions. The designed approach is able to automatically measure the functional size starting from UML artifacts and providing higher accurate results when more data is available.",
        "keywords": [
            "Functional size measurement",
            "Automation tool",
            "COSMIC-ISO 19761",
            "Uniﬁed modeling language"
        ],
        "authors": [
            "Gabriele De Vito",
            "Filomena Ferrucci",
            "Carmine Gravino"
        ],
        "file_path": "data/sosym-all/s10270-019-00731-2.pdf"
    },
    {
        "title": "Supporting the internet-based evaluation of research software with cloud infrastructure",
        "submission-date": "2009/10",
        "publication-date": "2010/05",
        "abstract": "Due to license restrictions and installation issues, it is often not feasible to experiment with software without making substantial investments. Especially in the case of legacy tools, it turns out that even free software is often too costly (i.e., time-consuming) to be installed for evaluating the quality of a research contribution. After organizing a series of events related to software modeling, we have constructed (and started to use) SHARE, a system for sharing practically any type of software artifact to reviewers and to other participants who have very limited time available. The system relies on cloud-computing technologies to provide online access to interactive environments containing all the tools, documentation, input and output models to reproduce alleged research results. The system also enables one to clone such an environment and add additional models or tools in order to extend a contribution or pinpoint a problem. In retrospect, we observe that the approach is not limited to software modeling and SHARE is in fact gaining acceptance in other ﬁelds already.",
        "keywords": [
            "Reproducible research",
            "Model transformation",
            "Tool contest",
            "Peer review",
            "Cloud computing"
        ],
        "authors": [
            "Pieter Van Gorp",
            "Paul Grefen"
        ],
        "file_path": "data/sosym-all/s10270-010-0163-y.pdf"
    },
    {
        "title": "Opportunities in intelligent modeling assistance",
        "submission-date": "2020/06",
        "publication-date": "2020/07",
        "abstract": "Modeling is requiring increasingly larger efforts while becoming indispensable given the complexity of the problems we are solving. Modelers face high cognitive load to understand a multitude of complex abstractions and their relationships. There is an urgent need to better support tool builders to ultimately provide modelers with intelligent modeling assistance that learns from previous modeling experiences, automatically derives modeling knowledge, and provides context-aware assistance. However, current intelligent modeling assistants (IMAs) lack adaptability and ﬂexibility for tool builders, and do not facilitate understanding the differences and commonalities of IMAs for modelers. Such a patchwork of limited IMAs is a lost opportunity to provide modelers with better support for the creative and rigorous aspects of software engineering. In this expert voice, we present a conceptual reference framework (RF-IMA) and its properties to identify the foundations for intelligent modeling assistance. For tool builders, RF-IMA aims to help build IMAs more systematically. For modelers, RF-IMA aims to facilitate comprehension, comparison, and integration of IMAs, and ultimately to provide more intelligent support. We envision a momentum in the modeling community that leads to the implementation of RF-IMA and consequently future IMAs. We identify open challenges that need to be addressed to realize the opportunities provided by intelligent modeling assistance.",
        "keywords": [
            "Model-based software engineering",
            "Intelligent modeling assistance",
            "Integrated development environment",
            "Artiﬁcial intelligence",
            "Development data",
            "Feedback"
        ],
        "authors": [
            "Gunter Mussbacher",
            "Benoit Combemale",
            "Jörg Kienzle",
            "Silvia Abrahão",
            "Hyacinth Ali",
            "Nelly Bencomo",
            "Márton Búr",
            "Loli Burgueño",
            "Gregor Engels",
            "Pierre Jeanjean",
            "Jean-Marc Jézéquel",
            "Thomas Kühn",
            "Sébastien Mosser",
            "Houari Sahraoui",
            "Eugene Syriani",
            "Dániel Varró",
            "Martin Weyssow"
        ],
        "file_path": "data/sosym-all/s10270-020-00814-5.pdf"
    },
    {
        "title": "Reasoning over time into models with DataTime",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Models at runtime have been initially investigated for adaptive systems. Models are used as a reﬂective layer of the current state of the system to support the implementation of a feedback loop. More recently, models at runtime have also been identified as key for supporting the development of full-ﬂedged digital twins. However, this use of models at runtime raises new challenges, such as the ability to seamlessly interact with the past, present, and future states of the system. In this paper, we propose a framework called DataTime to implement models at runtime which capture the state of the system according to the dimensions of both time and space, here modeled as a directed graph where both nodes and edges bear local states (i.e., values of properties of interest). DataTime offers a unifying interface to query the past, present, and future (predicted) states of the system. This unifying interface provides (i) an optimized structure of the time series that capture the past states of the system, possibly evolving over time, (ii) the ability to get the last available value provided by the system’s sensors, and (iii) a continuous micro-learning over graph edges of a predictive model to make it possible to query future states, either locally or more globally, thanks to a composition law. The framework has been developed and evaluated in the context of the Intelligent Public Transportation Systems of the city of Rennes (France). This experimentation has demonstrated how DataTime can be used for managing data from the past, the present, and the future and facilitate the development of digital twins.",
        "keywords": [
            "Models at runtime",
            "Digital twins",
            "Data analysis",
            "Intelligent Public Transportation Systems"
        ],
        "authors": [
            "Gauthier Lyan",
            "Jean-Marc Jézéquel",
            "David Gross-Amblard",
            "Romain Lefeuvre",
            "Benoit Combemale"
        ],
        "file_path": "data/sosym-all/s10270-023-01080-x.pdf"
    },
    {
        "title": "Composable partial multiparty session types for open systems",
        "submission-date": "2022/03",
        "publication-date": "2022/09",
        "abstract": "Session types are a well-established framework for the speciﬁcation of interactions between components of a distributed systems. An important issue is how to determine the type for an open system, i.e., obtained by assembling subcomponents, some of which could be missing. To this end, we introduce partial sessions and partial (multiparty) session types. Partial sessions can be composed, and the type of the resulting system is derived from those of its components without knowing any suitable global type nor the types of missing parts. To deal with this incomplete information, partial session types represent the subjective views of the interactions from participants’ perspectives; when sessions are composed, different partial views can be merged if compatible, yielding a uniﬁed view of the session. Incompatible types, due to, e.g., miscommunications or deadlocks, are detected at the merging phase. In fact, in this theory the distinction between global and local types vanishes. We apply these types to a process calculus for which we prove subject reduction and progress, so that well-typed systems never violate the prescribed constraints. In particular, we introduce a generalization of the progress property, in order to accommodate the case when a partial session cannot progress not due to a deadlock, but because some participants are still missing. Therefore, partial session types support the development of systems by incremental assembling of components.",
        "keywords": [
            "Multiparty session types",
            "Process algebras",
            "Open systems"
        ],
        "authors": [
            "Claude Stolze",
            "Marino Miculan",
            "Pietro Di Gianantonio"
        ],
        "file_path": "data/sosym-all/s10270-022-01040-x.pdf"
    },
    {
        "title": "CMMN evaluation: the modelers’ perceptions of the main notation elements",
        "submission-date": "2020/04",
        "publication-date": "2021/05",
        "abstract": "Case Management Model and Notation (CMMN) has been introduced as a graphical modeling language targeting the modeling of human-centric processes. Despite its growing reputation since 2016, when the OMG standant was released, the usage and the adoption potential of CMMN is not yet evaluated. The goal of this paper is to evaluate CMMN language and the contribution of its main notation elements to its future adoption, based on the experience of modelers. A CMMN workshop was conducted, where groups of modelers modeled two different human-centric, real-world processes with CMMN. The effectiveness and efﬁciency of the language and modelers’ usage experience were evaluated. Their perception of the role of the CMMN notation elements to their future adoption CMMN have been recorded through a survey. A multi-criteria decision making method (Analytic Hierarchy Process–AHP) was utilized for analyzing the answers and generating the results. The evaluation results showed that CMMN language could be adopted for modeling non-structural processes and the study participants showed a positive attitude towards adopting CMMN driven by the fact that they overall perceived it as useful. To the best of our knowledge, this is the ﬁrst attempt to evaluate CMMN language’s usability and prospects of adoption. Moreover, this is the first empirical study that explores the syntax of a process modeling language and its effect on its usage and adoption. Overall, since interest in CMMN is increasing, this work could inspire future researchers and practitioners to further explore the CMMN usage and adoption potential.",
        "keywords": [
            "CMMN",
            "Human-centric processes",
            "Modeling language evaluation",
            "AHP method"
        ],
        "authors": [
            "Ioannis Routis",
            "Cleopatra Bardaki",
            "Georgia Dede",
            "Mara Nikolaidou",
            "Thomas Kamalakis",
            "Dimosthenis Anagnostopoulos"
        ],
        "file_path": "data/sosym-all/s10270-021-00880-3.pdf"
    },
    {
        "title": "A framework for embedded software portability and veriﬁcation: from formal models to low-level code",
        "submission-date": "2022/06",
        "publication-date": "2024/02",
        "abstract": "Porting software to new target architectures is a common challenge, particularly when dealing with low-level functionality in drivers or OS kernels that interact directly with hardware. Traditionally, adapting code for different hardware platforms has been a manual and error-prone process. However, with the growing demand for dependability and the increasing hardware diversity in systems like the IoT, new software development approaches are essential. This includes rigorous methods for verifying and automatically porting Real-Time Operating Systems (RTOS) to various devices. Our framework addresses this challenge through formal methods and code generation for embedded RTOS. We demonstrate a hardware-speciﬁc part of a kernel model in Event-B, ensuring correctness according to the speciﬁcation. Since hardware details are only added in late modeling stages, we can reuse most of the model and proofs for multiple targets. In a proof of concept, we reﬁne the generic model for two different architectures, also ensuring safety and liveness properties. We then showcase automatic low-level code generation from the model. Finally, a hardware-independent factorial function model illustrates more potential of our approach.",
        "keywords": [
            "Embedded systems",
            "RTOS",
            "Formal methods",
            "Event-B",
            "Veriﬁcation",
            "Code generation",
            "Portability"
        ],
        "authors": [
            "Renata Martins Gomes",
            "Bernhard Aichernig",
            "Marcel Baunach"
        ],
        "file_path": "data/sosym-all/s10270-023-01144-y.pdf"
    },
    {
        "title": "Efﬁcient analysis of pattern-based constraint speciﬁcations",
        "submission-date": "2008/06",
        "publication-date": "2009/08",
        "abstract": "Precision and consistency are important prereq-uisites for class models to conform to their intended domain semantics. Precision can be achieved by augmenting models with design constraints and consistency can be achieved by avoiding contradictory constraints. However, there are differ-ent views of what constitutes a contradiction for design constraints. Moreover, state-of-the-art analysis approaches for proving constrained models consistent either scale poorly or require the use of interactive theorem proving. In this paper, we present a heuristic approach for efﬁciently analyz-ing constraint speciﬁcations built from constraint patterns. This analysis is based on precise notions of consistency for constrained class models and exploits the semantic properties of constraint patterns, thereby enabling syntax-based consis-tency checking in polynomial-time. We introduce a consis-tencycheckerimplementingtheseideasandwereportoncase studies in applying our approach to analyze industrial-scale models. These studies show that pattern-based constraint development supports the creation of concise speciﬁcations and provides immediate feedback on model consistency.",
        "keywords": [
            "UML",
            "OCL",
            "Constraints",
            "Patterns",
            "Consistency"
        ],
        "authors": [
            "Michael Wahler",
            "David Basin",
            "Achim D. Brucker",
            "Jana Koehler"
        ],
        "file_path": "data/sosym-all/s10270-009-0123-6.pdf"
    },
    {
        "title": "Model-based intelligent user interface adaptation: challenges and future directions",
        "submission-date": "2021/05",
        "publication-date": "2021/07",
        "abstract": "Adapting the user interface of a software system to the requirements of the context of use continues to be a major challenge, particularly when users become more demanding in terms of adaptation quality. A considerable number of methods have, over the past three decades, provided some form of modelling with which to support user interface adaptation. There is, however, a crucial issue as regards in analysing the concepts, the underlying knowledge, and the user experience afforded by these methods as regards comparing their beneﬁts and shortcomings. These methods are so numerous that positioning a new method in the state of the art is challenging. This paper, therefore, deﬁnes a conceptual reference framework for intelligent user interface adaptation containing a set of conceptual adaptation properties that are useful for model-based user interface adaptation. The objective of this set of properties is to understand any method, to compare various methods and to generate new ideas for adaptation. We also analyse the opportunities that machine learning techniques could provide for data processing and analysis in this context, and identify some open challenges in order to guarantee an appropriate user experience for end-users. The relevant literature and our experience in research and industrial collaboration have been used as the basis on which to propose future directions in which these challenges can be addressed.",
        "keywords": [
            "Context of use",
            "Intelligent user interface",
            "Machine learning",
            "Model-based software engineering",
            "Model-driven engineering",
            "User interface adaptation",
            "Conceptual reference framework"
        ],
        "authors": [
            "Silvia Abrahão",
            "Emilio Insfran",
            "Arthur Sluÿters",
            "Jean Vanderdonckt"
        ],
        "file_path": "data/sosym-all/s10270-021-00909-7.pdf"
    },
    {
        "title": "Towards high-level fuzzy control speciﬁcations for building automation systems",
        "submission-date": "2018/09",
        "publication-date": "2019/09",
        "abstract": "The control logic underlying building automation systems has consisted, traditionally, of embedded discrete programs created\nusing either low-level or proprietary scripting languages, or using general purpose fourth-generation visual languages like\nSimulink. It is also well known that programs developed in this way are hard to evolve, test, and maintain. These difﬁculties\nare intensiﬁed when continuous control problems have to be tackled or when the actuation must vary continually subject\nto the sensor inputs. Such is the case in day-lighting or occupancy-based control applications. In this paper, we propose a\ndeclarative high-level Domain-Speciﬁc Language that aims to reduce the effort required to specify the control logic of building\nautomation systems. Our language combines fuzzy logic and temporal logic, enabling to deﬁne the behaviour in terms of\ndomain abstractions. Finally, the approach has been validated in two ways: (i) in a case study that simulates the control system\nof an automated ofﬁce room and (ii) by means of an empirical study to conﬁrm usability (with a System Usability Scale\nquestionnaire) and effectiveness, here regarded from the perspective of correctness, of the proposed language with respect to\na well-known language like Simulink.",
        "keywords": [
            "Ambient intelligence",
            "Context-aware systems",
            "Building automation",
            "Fuzzy control systems",
            "Domain-Speciﬁc Languages"
        ],
        "authors": [
            "Juan C. Vidal",
            "Paulo Carreira",
            "Vasco Amaral",
            "Joao Aguiam",
            "João Sousa"
        ],
        "file_path": "data/sosym-all/s10270-019-00755-8.pdf"
    },
    {
        "title": "Toward testing from ﬁnite state machines with symbolic inputs and outputs",
        "submission-date": "2016/09",
        "publication-date": "2017/08",
        "abstract": "After 60 or so years of development, the theory of checking experiments for FSM still continues to attract a lot of attention of research community. One of the reasons is that it offers test generation techniques which under well-deﬁned assumptions guarantee complete fault coverage for a given fault model of a speciﬁcation FSM. Checking experiments have already been extended to remove assumptions that the speciﬁcation Mealy machine need to be reduced, deterministic, and completely speciﬁed, while keeping the input, output and state sets ﬁnite. In our recent work, we investigated possibilities of removing the assumption about the ﬁniteness of the input set, introducing the model FSM with symbolic inputs. In this paper, we report the results that further lift the theory of checking experiments for Mealy machines with symbolic inputs and symbolic outputs. The former are predicates deﬁned over input variables and the latter are output variable valuations computed by assignments on input variables. Both types of variables can have large or even inﬁnite domains. Inclusion of assignments in the model complicates even output fault detection, as different assignments may produce the same output valuations for some input valuations. We address this issue by using a transition cover composed of symbolic inputs on which the assignments produce different outputs. The enhanced transition cover is then used in checking experiments, which detect assignment/output faults and more general transition faults under certain assumptions.",
        "keywords": [
            "Finite state machines",
            "Extended ﬁnite state machines",
            "Symbolic automata",
            "Conformance testing",
            "Checking experiments",
            "Fault model-based test generation"
        ],
        "authors": [
            "Alexandre Petrenko"
        ],
        "file_path": "data/sosym-all/s10270-017-0613-x.pdf"
    },
    {
        "title": "An interdisciplinary comparison of sequence modeling methods for next-element prediction",
        "submission-date": "2018/10",
        "publication-date": "2020/04",
        "abstract": "Data of sequential nature arise in many application domains in the form of, e.g., textual data, DNA sequences, and software execution traces. Different research disciplines have developed methods to learn sequence models from such datasets: (i) In the machine learning ﬁeld methods such as (hidden) Markov models and recurrent neural networks have been developed and successfully applied to a wide range of tasks, (ii) in process mining process discovery methods aim to generate human-interpretable descriptive models, and (iii) in the grammar inference ﬁeld the focus is on ﬁnding descriptive models in the form of formal grammars. Despite their different focuses, these ﬁelds share a common goal: learning a model that accurately captures the sequential behavior in the underlying data. Those sequence models are generative, i.e., they are able to predict what elements are likely to occur after a given incomplete sequence. So far, these ﬁelds have developed mainly in isolation from each other and no comparison exists. This paper presents an interdisciplinary experimental evaluation that compares sequence modeling methods on the task of next-element prediction on four real-life sequence datasets. The results indicate that machine learning methods, which generally do not aim at model interpretability, tend to outperform methods from the process mining and grammar inference ﬁelds in terms of accuracy.",
        "keywords": [
            "Process mining",
            "Machine learning",
            "Grammar inference",
            "Sequence prediction"
        ],
        "authors": [
            "Niek Tax",
            "Irene Teinemaa",
            "Sebastiaan J. van Zelst"
        ],
        "file_path": "data/sosym-all/s10270-020-00789-3.pdf"
    },
    {
        "title": "Enhancing secure business process design with security process patterns",
        "submission-date": "2017/11",
        "publication-date": "2019/07",
        "abstract": "Business process deﬁnition and analysis are an important activity for any organisation. As research has demonstrated, well-deﬁned business processes can reduce cost, improve productivity and provide organisations with competitive advantages. In the last few years, the need to ensure the security of business processes has been identiﬁed as a major research challenge. Limited security expertise of business process developers together with a clear lack of appropriate methods and techniques to support the security analysis of business processes is important prohibitors to providing answers to that research challenge. This paper introduces the ﬁrst attempt in the literature to produce a novel pattern-based approach to support the design and analysis of secure business processes. Our work draws on elements from the security requirements engineering area and the security patterns area, combined with business process modelling, and it produces a set of process-level security patterns which are used to implement security in a given business process model. Such an approach advances the existing literature by providing a structured way of operationalising security at the business process level of abstraction. The applicability of the work is illustrated through an application to a real-life information system, and the effectiveness and usability of the work are evaluated via a workshop-based experiment. The evaluation clearly indicates that non-experts are able to comprehend and utilise the developed patterns to construct secure business process designs.",
        "keywords": [
            "Security requirements engineering",
            "Business process modelling",
            "Security process patterns",
            "Business process security"
        ],
        "authors": [
            "Nikolaos Argyropoulos",
            "Haralambos Mouratidis",
            "Andrew Fish"
        ],
        "file_path": "data/sosym-all/s10270-019-00743-y.pdf"
    },
    {
        "title": "Guest editorial to the special section on SEFM 2009",
        "submission-date": "2012/03",
        "publication-date": "Not found",
        "abstract": "This guest editorial introduces the special section focusing on the Seventh IEEE International Conference on Software Engineering and Formal Methods (SEFM 2009). It discusses the importance of formal methods in software engineering, the scope of the special section, and the selection process of the included papers. The editorial highlights the integration of software engineering techniques with formal methods and aims to appeal to both researchers and postgraduate students.",
        "keywords": [],
        "authors": [
            "Padmanabhan Krishnan",
            "Dang Van Hung",
            "Antonio Cerone"
        ],
        "file_path": "data/sosym-all/s10270-012-0238-z.pdf"
    },
    {
        "title": "Emerging OCL tools",
        "submission-date": "2002/06",
        "publication-date": "2003/10",
        "abstract": "The Object Constraint Language (OCL) is\na notational language for analysis and design of software\nsystems, which is used in conjunction with the Uniﬁed\nModelling Language (UML) to specify the semantics of\nthe building blocks precisely. OCL can also be used by\nother languages, notations, methods and software tools\nin order to specify restrictions and other expressions of\ntheir models. Likewise, OCL is used by the Object Man-\nagement Group (OMG) in the deﬁnition of other fast\nspreading industrial standards such as Meta Object Facil-\nity (MOF) or XML Metadata Interchange (XMI).\nSupport tools aimed at making this language easier\nto use are becoming available. These tools are capable\nof supporting and handling OCL expressions. This paper\npresents a comparative study of the main tools currently\navailable, both commercial and freely available ones. The\nstudy is very practical, with the advantages and disad-\nvantages of the diﬀerent tools being pointed out. The\nevaluations made may be of use in helping those develop-\ners and analysts who already use the language, as well as\nthose who intend to use it in the near future, to choose the\nOCL tool which best adapts to their requirements.",
        "keywords": [
            "Object Constraint Language",
            "OCL tools",
            "Analysis",
            "Comparison"
        ],
        "authors": [
            "Ambrosio Toval",
            "V´ıctor Requena",
            "Jos´e Luis Fern´andez"
        ],
        "file_path": "data/sosym-all/s10270-003-0031-0.pdf"
    },
    {
        "title": "EDITORIAL",
        "submission-date": "2003/10",
        "publication-date": "2005/09",
        "abstract": "The UML series of conferences began in 1998, in the early days of the Uniﬁed Modelling Language (UML). Since then, both the language and its community have changed out of all recognition. The application of UML has been much wider than was originally envisaged by most of those involved. It has spawned an extraordinary number of discussions, tools, books, variants, and not least, academic papers. UML itself is, at the time of writing, about to complete the upgrade to UML2.0. At the time of the conference to which this special issue refers, the revision was in progress; some papers in this special issue refer to UML2.0, others to UML1.x. However, many of the most interesting developments in the ﬁeld are not speciﬁc to UML at all. This is reﬂected in the new change to the name of the conference: from 2005, it will be known as MoDELS (Model Drivel Engineering, Languages and Systems), not as UML. Several of the papers in this special issue already make the point that their work applies outside UML.",
        "keywords": [],
        "authors": [
            "Perdita Stevens",
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-005-0086-1.pdf"
    },
    {
        "title": "Recommender systems in model-driven engineering",
        "submission-date": "2020/12",
        "publication-date": "2021/07",
        "abstract": "Recommender systems are information ﬁltering systems used in many online applications like music and video broadcasting and e-commerce platforms. They are also increasingly being applied to facilitate software engineering activities. Following this trend, we are witnessing a growing research interest on recommendation approaches that assist with modelling tasks and model-based development processes. In this paper, we report on a systematic mapping review (based on the analysis of 66 papers) that classiﬁes the existing research work on recommender systems for model-driven engineering (MDE). This study aims to serve as a guide for tool builders and researchers in understanding the MDE tasks that might be subject to recommendations, the applicable recommendation techniques and evaluation methods, and the open challenges and opportunities in this ﬁeld of research.",
        "keywords": [
            "Model-driven engineering",
            "Recommender systems",
            "Systematic mapping review"
        ],
        "authors": [
            "Lissette Almonte",
            "Esther Guerra",
            "Iván Cantador",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-021-00905-x.pdf"
    },
    {
        "title": "SustainScrum: integrating sustainability assessment in a tailored Scrum process for computing quantitative sustainability indicators",
        "submission-date": "2024/03",
        "publication-date": "2025/02",
        "abstract": "In the wake of a rapidly growing global focus on sustainable practices, the year 2023 marked a signiﬁcant regulatory mile-\nstone, with the enactment of the Corporate Sustainability Reporting Directive. This directive signiﬁcantly expands the scope\nof sustainability reporting, encompassing a broader array of enterprises, including large-scale, and small- and medium-sized\nEnterprises within the EU. This regulatory development has profound implications for the software industry, as these com-\npanies need to provide comprehensive sustainability reporting for their software products. However, the industry still lacks\nmodels, tools, and methodologies for quantitatively assessing sustainability indicators during an agile software develop-\nment process. In this paper, we introduce SustainScrum, a customized Scrum process model that incorporates sustainability\nevaluations into the development lifecycle of software products. More precisely, SustainScrum integrates the assessment of\nsustainability aspects into the backlog, user story management, and development stages of the Scrum process. Its primary\nobjective is to ensure that sustainability considerations are systematically captured, evaluated, and addressed throughout the\ndevelopment process. It integrates the computation of quantitative sustainability indicators thereby advancing the ability to\naddress sustainability challenges within software engineering practices. We performed an initial validation, investigating the\napplicability of SustainScrum on an open-source, publicly available requirements data set for agile development.",
        "keywords": [
            "Sustainability",
            "Agile development",
            "Process model",
            "Scrum"
        ],
        "authors": [
            "Alexandra Mazak-Huemer",
            "Michael Vierhauser",
            "Iris Groher"
        ],
        "file_path": "data/sosym-all/s10270-025-01266-5.pdf"
    },
    {
        "title": "UML speciﬁcation of access control policies and their formal veriﬁcation",
        "submission-date": "2004/01",
        "publication-date": "2006/10",
        "abstract": "Security requirements have become an integral part of most modern software systems. In order to produce secure systems, it is necessary to provide software engineers with the appropriate systematic support. We propose a methodology to integrate the speciﬁcation of access control policies into Uniﬁed Modeling Language (UML) and provide a graph-based formal semantics for theUMLaccess control speciﬁcationwhich permits to reason about the coherence of the access control speciﬁcation. The main concepts in the UML access control speciﬁcation are illustrated with an example access control model for distributed object systems.",
        "keywords": [],
        "authors": [
            "Manuel Koch",
            "Francesco Parisi-Presicce"
        ],
        "file_path": "data/sosym-all/s10270-006-0030-z.pdf"
    },
    {
        "title": "A framework for automated multi-stage and multi-step product configuration of cyber-physical systems",
        "submission-date": "2019/08",
        "publication-date": "2020/06",
        "abstract": "Product line engineering (PLE) has been employed to large-scale cyber-physical systems (CPSs) to provide customization based on users’ needs. A PLE methodology can be characterized by its support for capturing and managing the abstractions as commonalities and variabilities and the automation of the configuration process for effective selection and customization of reusable artifacts. The automation of a configuration process heavily relies on the captured abstractions and formally specified constraints using a well-defined modeling methodology. Based on the results of our previous work and a thorough literature review, in this paper, we propose a conceptual framework to support multi-stage and multi-step automated product configuration of CPSs, including a comprehensive classification of constraints and a list of automated functionalities of a CPS configuration solution. Such a framework can serve as a guide for researchers and practitioners to evaluate an existing CPS PLE solution or devise a novel CPS PLE solution. To validate the framework, we conducted three real-world case studies. Results show that the framework fulfills all the requirements of the case studies in terms of capturing and managing variabili-ties and constraints. Results of the literature review indicate that the framework covers all the functionalities concerned by the literature, suggesting that the framework is complete for enabling the maximum automation of configuration in CPS PLE.",
        "keywords": [
            "Cyber-physical systems",
            "Product line engineering",
            "Automated configuration",
            "Multi-stage and multi-step configuration process",
            "Constraint classification",
            "Variability modeling",
            "Real-world case studies"
        ],
        "authors": [
            "Safdar Aqeel Safdar",
            "Hong Lu",
            "Tao Yue",
            "Shaukat Ali",
            "Kunming Nie"
        ],
        "file_path": "data/sosym-all/s10270-020-00803-8.pdf"
    },
    {
        "title": "Model synchronization based on triple graph grammars: correctness, completeness and invertibility",
        "submission-date": "2012/04",
        "publication-date": "2013/01",
        "abstract": "Triple graph grammars (TGGs) have been used successfully to analyze correctness and completeness of bidirectional model transformations, but a corresponding formal approach to model synchronization has been missing. This paper closes this gap by providing a formal synchronization framework with bidirectional update propagation operations. TheyaregeneratedfromagivenTGG, whichspeciﬁes the language of all consistently integrated source and target models. As our main result, we show that the generated synchronization framework is correct and complete, provided that forward and backward propagation operations are deterministic. Correctness essentially means that the propagation operations preserve and establish consistency while completeness ensures that the operations are deﬁned for all possible inputs. Moreover, we analyze the conditions under which the operations are inverse to each other. All constructions and results are motivated and explained by a running example,",
        "keywords": [
            "Model synchronization",
            "Correctness",
            "Bidirectional model transformation",
            "Triple graph grammars"
        ],
        "authors": [
            "Frank Hermann",
            "Hartmut Ehrig",
            "Fernando Orejas",
            "Krzysztof Czarnecki",
            "Zinovy Diskin",
            "Yingfei Xiong",
            "Susann Gottmann",
            "Thomas Engel"
        ],
        "file_path": "data/sosym-all/s10270-012-0309-1.pdf"
    },
    {
        "title": "A model template for reachability-based containment checking of imprecise observations in timed automata",
        "submission-date": "2023/07",
        "publication-date": "2024/09",
        "abstract": "Verifying safety requirements by model checking becomes increasingly important for safety-critical applications. For the validity of such proof in practice, the model needs to capture the actual behavior of the real system, which could be tested by containment checks of real observation traces. Basic equivalence checks, however, are not applicable if the system is only partially or imprecisely observable, if the model abstracts from explicit states with symbolic semantics, or if the checks are not expressible in the logics supported by a model checker. In this article, we solve the problem of observation containment checking in timed automata via reachability checking on tester systems. We introduce the logic SRL (sequence reachability logic) to express observations as sequences of delayed reachability properties. Through SBLL (introduced by Aceto et al.) as intermediate logic, we synthesize a set of matcher model templates for partial and imprecise observations and further extend these templates for the case of limited state accessibility in a model. For the obtained matching traces, we deﬁne the back-transformation into the original model domain and formally prove the correctness of the transformation. We implemented the observation matching approach, and apply it to a set of 7 demo and 3 case study models with different levels of observability. The results show that all positive and negative observations are correctly classiﬁed, and that the most advanced matcher model instance still offers average run times between 0.1 and 1s in all but 3 scenarios.",
        "keywords": [
            "Containment checking",
            "Reachability problem",
            "Model transformation",
            "Timed automata",
            "Observation matching"
        ],
        "authors": [
            "Sascha Lehmann",
            "Sibylle Schupp"
        ],
        "file_path": "data/sosym-all/s10270-024-01205-w.pdf"
    },
    {
        "title": "Empirical study on the effectiveness and efﬁciency of model-driven architecture techniques",
        "submission-date": "2018/02",
        "publication-date": "2019/01",
        "abstract": "Previous studies have reported conﬂicting opinions on the feasibility of model-driven architecture (MDA). Studies have investigated the mechanics of MDA, but few have examined its effectiveness and efﬁciency from a developer’s perception. This study conducted empirical research in which a system was implemented by subjects using MDA; afterward, evaluated its perceived efﬁciency and effectiveness. In the model construction phase, Uniﬁed Modeling Language and Object Constraint Language were perceived as effective and efﬁcient. In the model transformation phase, the query/view/transformation standard was perceived as marginally efﬁcient rather than effective, and the round-trip engineering phase was not perceived as effective or efﬁcient. These ﬁndings are explained using 12 themes identiﬁed in subjects’ opinions. This study may help scholars understand the importance of efﬁciency and effectiveness on MDA techniques and facilitate the development of more acceptable and practical MDA.",
        "keywords": [
            "Model-driven architecture",
            "Effectiveness",
            "Efﬁciency",
            "Model transformation",
            "Uniﬁed Modeling Language",
            "Query/view/transformation"
        ],
        "authors": [
            "Shin-Shing Shin"
        ],
        "file_path": "data/sosym-all/s10270-018-00711-y.pdf"
    },
    {
        "title": "SoSyM special section on service-based software engineering",
        "submission-date": "2005/12",
        "publication-date": "2006/04",
        "abstract": "Services offer the possibility to describe the functionality of a system exceeding the limited modularization supplied by component-based approaches. For that reason, service-based systems engineering has traditionally proven useful in the development of telecommunication systems, by separating individual services – or features – with high degree of interaction between the components of the system and describing the complete behavior of the system when combined. Due to their modular character, services are especially suited for the development of configurable or extendable systems, supporting product lines as well as incremental development. Thus, beyond telecommunication, especially the notion of web services has received a lot of interest as a means for publishing and accessing software functions via web standards to yield flexibly configurable systems. For similar reasons, increasingly the notion of a service is also gaining ground in other application domains, such as spontaneous networks, ubiquitous computing, and even safety critical systems from the automotive or avionics domain. Precise specification and correct implementation of requirements for services are essential in most of these application domains. Many notions of service refer only to syntactic interfaces. Typical examples include the service notions inherent in WSDL/SOAP and technologies such as .NET/J2EE building on these service notions. This is inadequate for more elaborate service specifications that include, for instance, quality-of-service properties. In many approaches, services are not treated as first-class modeling elements, say, in UML/UML-RT and SDL. Therefore, especially in the application domains mentioned above, a suitable notion of service is needed to support service-based software engineering beyond simplistic syntactic approaches.",
        "keywords": [
            "Service",
            "Semantics",
            "Model",
            "Description",
            "Architecture",
            "Composition"
        ],
        "authors": [
            "Manfred Broy",
            "Heinrich Hussmann",
            "Ingolf H. Kr¨uger",
            "Bernhard Sch¨atz"
        ],
        "file_path": "data/sosym-all/s10270-006-0002-3.pdf"
    },
    {
        "title": "Guest editorial",
        "submission-date": "2007/08",
        "publication-date": "2007/10",
        "abstract": "This edition and the next of “Software and Systems Modeling” contain eight papers discussing the use of models in software and systems development. These articles are based on papers presented at MODELS 2005, the eighth international conference on Model Driven Engineering Languages and Systems held in Montego Bay, Jamaica, 2–7 October 2005. This was the inaugural edition of MODELS, which is the follow-on conference series to the highly popular UML series of conferences. MODELS represents the broadening of the UML series to include topics such as languages, methods, and systems relevant to the development and use of complex systems.",
        "keywords": [],
        "authors": [
            "Lionel Briand",
            "Geri Georg"
        ],
        "file_path": "data/sosym-all/s10270-007-0067-7.pdf"
    },
    {
        "title": "The Software Language Extension Problem",
        "submission-date": "2019/09",
        "publication-date": "2019/12",
        "abstract": "The problem of software language extension and composition drives much of the research in Software Language Engineering (SLE). Although various solutions have already been proposed, there is still little understanding of the speciﬁc ins and outs of this problem, which hinders the comparison and evaluation of existing solutionsIn this SoSyM Expert Voice, we introduce the Language Extension Problem as a way to better qualify the scope of the challenges related to language extension and compositionThe formulation of the problem is similar to the seminal Expression Problem introduced by Wadler in the late 1990s and lifts it from the extensibility of single constructs to the extensibility of groups of constructs, i.e., software languages. We provide a comprehensive deﬁnition of the actual constraints when considering language extension and believe the Language Extension Problem will drive future research in SLE, the same way the original Expression Problem helped to understand the strengths and weaknesses of programming languages and drove much research in programming languages.",
        "keywords": [
            "Domain-speciﬁc language",
            "Extension",
            "Composition",
            "Expression problem"
        ],
        "authors": [
            "Manuel Leduc",
            "Thomas Degueule",
            "Eric Van Wyk",
            "Benoit Combemale"
        ],
        "file_path": "data/sosym-all/s10270-019-00772-7.pdf"
    },
    {
        "title": "A UML/OCL framework for the analysis of graph transformation rules",
        "submission-date": "2008/11",
        "publication-date": "2009/08",
        "abstract": "In this paper we present an approach for the analysis of graph transformation rules based on an intermediate OCL representation. We translate different rule semantics into OCL, together with the properties of interest (like rule applicability, conﬂicts or independence). The intermediate representation serves three purposes: (1) it allows the seamless integration of graph transformation rules with the MOF and OCL standards, and enables taking the meta-model and its OCL constraints (i.e. well-formedness rules) into account when verifying the correctness of the rules; (2) it permits the interoperability of graph transformation concepts with a number of standards-based model-driven development tools; and (3) it makes available a plethora of OCL tools to actually perform the rule analysis. This approach is especially useful to analyse the operational semantics of Domain Spe-ciﬁc Visual Languages. We have automated these ideas by providing designers with tools for the graphical speciﬁcation and analysis of graph transformation rules, including a back-annotation mechanism that presents the analysis results in terms of the original language notation.",
        "keywords": [
            "Graph transformation",
            "OCL",
            "Meta-modelling",
            "Domain Speciﬁc Visual Languages",
            "Veriﬁcation and validation"
        ],
        "authors": [
            "Jordi Cabot",
            "Robert Clarisó",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-009-0129-0.pdf"
    },
    {
        "title": "A concern architecture view for aspect-oriented software design",
        "submission-date": "2005/11",
        "publication-date": "2006/10",
        "abstract": "Although aspect-oriented programming is becoming popular, support for the independent description of aspect designs and for the incremental design of aspects themselves has been neglected. A conceptual framework for the design of aspects is presented, where aspects are viewed as augmentations that map an existing design into a new one with changes or additions. The principles of a Concern Architecture model are defined both to group designs of aspects, and to make explicit their dependencies and potential interferences in the design of a system with multiple aspects. The aspects are described generically, where any design element can be either required or provided. The required elements resemble formal parameters, and their binding to an existing design shows the context in which the provided parts are to modify that design. Overlap and a partial order among aspects and concerns are visualized in a Concern Architecture Diagram. An instantiation of the ideas as a UML profile is outlined, and the design of a digital sound recorder is used to demonstrate the utility of the approach.",
        "keywords": [
            "Aspect orientation",
            "Design concepts",
            "UML"
        ],
        "authors": [
            "Mika Katara",
            "Shmuel Katz"
        ],
        "file_path": "data/sosym-all/s10270-006-0032-x.pdf"
    },
    {
        "title": "Change patterns\nCo-evolving requirements and architecture",
        "submission-date": "2011/03",
        "publication-date": "2012/08",
        "abstract": "Change, such as in the requirements or the\nassumptions of a system, has a far-reaching impact across\nseveral software artifacts. This paper argues that patterns of\nco-evolution (or change patterns) can be observed between\nintertwined pairs of artifacts, like the requirements speci-\nfication and the architectural design. The paper introduces\nchange patterns as a precise framework to systematically\ncapture and handle change. The approach is based on model-\ndriven engineering concepts and is accompanied by a tool-\nsupported process. Changing trust assumptions are presented\nas an example of security-related evolution, and are used to\nillustrate the approach. The approach is empirically validated\nby means of a controlled experiment involving 12 subjects,\nand a case study involving an industrial partner.",
        "keywords": [
            "Co-evolution",
            "Model-driven engineering",
            "Security requirements",
            "Software architecture"
        ],
        "authors": [
            "Koen Yskout",
            "Riccardo Scandariato",
            "Wouter Joosen"
        ],
        "file_path": "data/sosym-all/s10270-012-0276-6.pdf"
    },
    {
        "title": "On tracing reactive systems",
        "submission-date": "2009/06",
        "publication-date": "2010/03",
        "abstract": "We present a rich and highly dynamic technique for analyzing, visualizing, and exploring the execution traces of reactive systems. The two inputs are a designer’s inter-object scenario-based behavioral model, visually described using a UML2-compliant dialect of live sequence charts (LSC), and an execution trace of the system. Our method allows one to visualize, navigate through, and explore, the activation and progress of the scenarios as they “come to life” during execution. Thus, a concrete system’s run-time is recorded and viewed through abstractions provided by behavioral models used for its design, tying the visualization and exploration of system execution traces to model-driven engineering. We support both event-based and real-time-based tracing, and use details-on-demand mechanisms, multi-scaling grids, and gradient coloring methods. Novel model exploration techniques include semantics-based navigation, ﬁltering, and trace comparison. The ideas are implemented and tested in a prototype tool called the Tracer.",
        "keywords": [
            "Software visualization",
            "UML interactions",
            "Sequence diagrams",
            "Live sequence charts",
            "Model-based traces",
            "Dynamic analysis"
        ],
        "authors": [
            "Shahar Maoz",
            "David Harel"
        ],
        "file_path": "data/sosym-all/s10270-010-0151-2.pdf"
    },
    {
        "title": "A generic LSTM neural network architecture to infer heterogeneous model transformations",
        "submission-date": "2020/07",
        "publication-date": "2021/05",
        "abstract": "Models capture relevant properties of systems. During the models’ life-cycle, they are subjected to manipulations with different\ngoals such as managing software evolution, performing analysis, increasing developers’ productivity, and reducing human\nerrors. Typically, these manipulation operations are implemented as model transformations. Examples of these transformations\nare (i) model-to-model transformations for model evolution, model refactoring, model merging, model migration, model\nreﬁnement, etc., (ii) model-to-text transformations for code generation and (iii) text-to-model ones for reverse engineering.\nThese operations are usually manually implemented, using general-purpose languages such as Java, or domain-speciﬁc\nlanguages (DSLs) such as ATL or Acceleo. Even when using such DSLs, transformations are still time-consuming and error-\nprone. We propose using the advances in artiﬁcial intelligence techniques to learn these manipulation operations on models and\nautomate the process, freeing the developer from building speciﬁc pieces of code. In particular, our proposal is a generic neural\nnetwork architecture suitable for heterogeneous model transformations. Our architecture comprises an encoder–decoder long\nshort-term memory with an attention mechanism. It is fed with pairs of input–output examples and, once trained, given an\ninput, automatically produces the expected output. We present the architecture and illustrate the feasibility and potential of\nour approach through its application in two main operations on models: model-to-model transformations and code generation.\nThe results conﬁrm that neural networks are able to faithfully learn how to perform these tasks as long as enough data are\nprovided and no contradictory examples are given.",
        "keywords": [
            "Model manipulation",
            "Code generation",
            "Model transformation",
            "Artiﬁcial intelligence",
            "Machine learning",
            "Neural networks"
        ],
        "authors": [
            "Loli Burgueño",
            "Jordi Cabot",
            "Shuai Li",
            "Sébastien Gérard"
        ],
        "file_path": "data/sosym-all/s10270-021-00893-y.pdf"
    },
    {
        "title": "Special section of SoSyM dedicated to 50 years of Petri nets",
        "submission-date": "2014/03",
        "publication-date": "2014/11",
        "abstract": "Very few concepts of computer science are so closely linked to the names of their creators as Petri nets are to their inventor Carl Adam Petri. More than 50 years ago, in June 1962, he laid the foundation for a modeling technique that has been continuously worked on ever since. With his stable foundation, numerous and broadly applicable concepts and techniques, special cases, generalizations and adaptations, Petri nets are used in a variety of ﬁelds inside and outside computer science. Just as ﬁnite automata and their numerous variants are the formal basis for many modeling languages for sequential systems, Petri nets are a basic and precise description of essential concepts and phenomena of discrete distributed systems, which implicitly or explicitly inﬂuenced many custom-designed modeling languages. Efﬁcient techniques for proving and checking relevant properties of a system model can be transferred from Petri nets to other modeling languages. The same is true for information and results on those properties that cannot be ascertained or require a very high algorithmic effort. Thus, results of Petri net theory are relevant in many areas where the name “Petri” is not explicitly mentioned. Vice versa, new and custom-designed problems are often easier to analyze and solve in the more general context of Petri nets. This was the impetus for discussing the basic concepts of Petri nets and highlight current ﬁelds of successful application in this special section.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-014-0439-8.pdf"
    },
    {
        "title": "FLEXMI: a generic and modular textual syntax for domain-speciﬁc modelling",
        "submission-date": "2022/02",
        "publication-date": "2022/11",
        "abstract": "Domain-speciﬁc languages allow engineers and domain experts to express problems and design solutions using domain-focused vocabularies and abstractions, by means of graphical or textual syntaxes. In the case of textual syntaxes, language engineers can opt for creating a language-speciﬁc syntax by deﬁning and maintaining a BNF-style grammar, or use an existing general-purpose reﬂective syntax such as the XML Metadata Interchange (XMI) or the Human Usable Textual Notation (HUTN), which do not require any development and maintenance effort, but which are more verbose and cannot be customised. We present Flexmi: a new general-purpose textual syntax for deﬁning models that conform to Eclipse Modelling Framework’s Ecore-based metamodels. Flexmi offers XML and YAML/JSON syntax ﬂavours, it can be fuzzily parsed to reduce verbosity, and it includes a templating system to facilitate encapsulation of reusable composite model element structures, thus enabling more concise model speciﬁcations. We have evaluated Flexmi for verbosity and model loading performance against XMI, HUTN, and a bespoke (i.e. custom) textual syntax for Ecore (Emfatic). Our results indicate that the use of fuzzy parsing and templates allow Flexmi to achieve a signiﬁcant reduction in the verbosity of models compared to XMI/HUTN and can become almost as concise as a bespoke textual syntax, with a moderate performance penalty.",
        "keywords": [
            "Domain-speciﬁc languages",
            "Generic textual syntaxes",
            "Model-driven engineering"
        ],
        "authors": [
            "Dimitris Kolovos",
            "Alfonso de la Vega"
        ],
        "file_path": "data/sosym-all/s10270-022-01064-3.pdf"
    },
    {
        "title": "Spider Graphs: a graph transformation system for spider diagrams",
        "submission-date": "2012/08",
        "publication-date": "2013/09",
        "abstract": "The use of diagrammatic logic as a reasoning mechanism to produce inferences on subsets of some universe could provide a way to overcome the current limitations of visual modelling methods, which have to be integrated with textual languages to express complex constraints. On the other hand, graph transformations are becoming widespread as a way to express formal semantics for visual modelling languages, so that a mechanisation of diagrammatic logic based on graph transformation would facilitate language integration, based on a common underlying machinery. In this paper, we propose such a mechanisation for spider diagrams (SDs), an established language for reasoning with diagrams modelling relations between sets and constraints on their cardinalities. The concrete syntax of SDs extends that of Euler diagrams that use closed curves and the enclosed regions to represent sets and their intersections. The language is augmented with reasoning rules, i.e. syntactic transformation rules corresponding to logical inference rules. However, these rules are typically defined in procedural terms, so that a completely formal specification and an adequate mechanisation of them has not been achieved yet. We propose an abstract syntax for SDs in terms of typed graphs and define the corresponding language of Spider Graphs (SGs), expressing reasoning rules for SDs as graph transformation units. This enables a direct realisation of the reasoning system via graph transformation tools without resorting to ad hoc implementations, and we provide an implementation in AGG. Techniques for static analysis become available to reason on proof strategies and on possible optimisations.",
        "keywords": [
            "Diagrammatic reasoning",
            "Graph transformations",
            "Spider diagrams",
            "Spider Graphs",
            "Reasoning strategies"
        ],
        "authors": [
            "Paolo Bottoni",
            "Andrew Fish",
            "Francesco Parisi Presicce"
        ],
        "file_path": "data/sosym-all/s10270-013-0381-1.pdf"
    },
    {
        "title": "Reusing semi-speciﬁed behavior models in systems analysis and design",
        "submission-date": "2006/01",
        "publication-date": "2008/02",
        "abstract": "As the structural and behavioral complexity of systems has increased, so has interest in reusing modules in early development phases. Developing reusable modules and then weaving them into speciﬁc systems has been addressed by many approaches, including plug-and-play software component technologies, aspect-oriented techniques, design patterns, superimposition, and product line techniques. Most of these ideas are expressed in an object-oriented framework, so they reuse behaviors after dividing them into methods that are owned by classes. In this paper, we present a crosscutting reuse approach that applies object-process methodology (OPM). OPM, which uniﬁes system structure and behavior in a single view, supports the notion of a process class that does not belong to and is not encapsulated in an object class, but rather stands alone, capable of getting input objects and producing output objects. The approach features the ability to specify modules generically and concretize them in the target application. This is done in a three-step process: designing generic and target modules, weaving them into the system under development, and reﬁning the combined speciﬁcation in a way that enables the individual modules to be modiﬁed after their reuse. Rules for specifying and combining modules are deﬁned and exempliﬁed, showing the ﬂexibility and beneﬁts of this approach.",
        "keywords": [
            "Software reuse",
            "Aspect-oriented software engineering",
            "Aspect-oriented modeling",
            "Object-Process Methodology",
            "Modularity"
        ],
        "authors": [
            "Iris Reinhartz-Berger",
            "Dov Dori",
            "Shmuel Katz"
        ],
        "file_path": "data/sosym-all/s10270-007-0079-3.pdf"
    },
    {
        "title": "Agile MERODE: a model-driven software engineering method for user-centric and value-based development",
        "submission-date": "2021/11",
        "publication-date": "2022/06",
        "abstract": "Agile is often associated with a lack of architectural thinking causing technical debt but has the advantage of user centricity and a strong focus on value. Model-driven software engineering (MDSE) strongly performs for building a quality architecture and code, but lacks focus on user requirements and tends to consider development as a monolithic whole. The combination of Agile and MDSE has been explored, but a convincing integrated method has not been proposed yet. This paper addresses this gap by exploring the speciﬁc combination of MERODE—as an example of a proven MDSE method—with Scrum, a reference agile method offering a concrete (sprint-based) life cycle management on the basis of user stories. The method resulting of this integration is called Agile MERODE; it is driven by user stories, themselves associated with behavior-driven development scenarios. It allows for domain-driven design and permits fast development from domain models by means of code generation. An illustrative example further clariﬁes the practical application of Agile MERODE, while a case study shows the planning game application in the case’s context. While the approach, in its entirety, allows reducing technical debt by building the architecture in a logical, consistent and complete manner, introducing MDSE involves a trade-off with pure value-driven development. Agile MERODE contributes to the state of the art by showing how to increase user centricity in MDSE, how to align model-driven engineering with the Scrum cycle, and how to reduce the technical debt of agile developments yet remaining value-focused.",
        "keywords": [
            "Model-driven engineering",
            "Agile",
            "MERODE",
            "User story",
            "User stories",
            "BDD",
            "Behavior-driven development"
        ],
        "authors": [
            "Monique Snoeck",
            "Yves Wautelet"
        ],
        "file_path": "data/sosym-all/s10270-022-01015-y.pdf"
    },
    {
        "title": "Editorial to the theme section on model-based testing",
        "submission-date": "2018/09",
        "publication-date": "2018/10",
        "abstract": "This theme on model-based testing (MBT) was organized in the context of advances in model-based testing (A-MOST) workshop series. Now in its fourteenth edition, this workshop covers all aspects of MBT from theoretical developments to industrial implementations. Following the twelfth edition in Chicago (2016), we invited the MBT community to submit their latest and ﬁnest research in the ﬁeld via an open call. We selected seven articles that are presented in this theme section.",
        "keywords": [],
        "authors": [
            "Mike Papadakis",
            "Shaukat Ali",
            "Gilles Perrouin"
        ],
        "file_path": "data/sosym-all/s10270-018-0699-9.pdf"
    },
    {
        "title": "Eugenia: towards disciplined and automated development of GMF-based graphical model editors",
        "submission-date": "2013/07",
        "publication-date": "2015/02",
        "abstract": "EMF and GMF are powerful frameworks for implementing tool support for modelling languages in Eclipse. However, with power comes complexity, implementing a graphical editor for a modelling language using EMF and GMF requires developers to handcraft and maintain several detailed interconnected models through a loosely guided, labour-intensive, and error-prone process. We demonstrate how the application of metamodel annotation and model transformation techniques can help to manage the complexity of GMF and EMF and deliver signiﬁcant productivity, quality, and maintainability beneﬁts. We present Eugenia, an open-source tool that implements the proposed approach, illustrate its functionality with an example, evaluate it through an empirical study, and report on the community’s response to the tool.",
        "keywords": [
            "Graphical modelling",
            "Model transformation",
            "Eclipse",
            "GMF"
        ],
        "authors": [
            "Dimitrios S. Kolovos",
            "Antonio García-Domínguez",
            "Louis M. Rose",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-015-0455-3.pdf"
    },
    {
        "title": "From misuse cases to mal-activity diagrams: bridging the gap between functional security analysis and design",
        "submission-date": "2011/03",
        "publication-date": "2012/03",
        "abstract": "Secure software engineering is concerned with developing software systems that will continue delivering its intended functionality despite a multitude of harmful software technologies that can attack these systems from anywhere and at anytime. Misuse cases and mal-activity diagrams are two techniques to model functional security requirements address security concerns early in the develop-ment life cycle. This allows system designers to equip their systems with security mechanisms built within system design rather than relying on external defensive mechanisms. In a model-driven engineering process, misuse cases are expected to drive the construction of mal-activity diagrams. However, a systematic approach to transform misuse cases into mal-activity diagrams is missing. Therefore, this process remains dependent on human skill and judgment, which raises the risk of developing mal-activity diagrams that are inconsistent with the security requirements described in misuse cases, leading to the development of an insecure system. This paper presents an authoring structure for misuse cases and a transformation technique to systematically perform this desired model transformation. A study was conducted to evaluate the proposed technique using 46 attack stories outlined in a book by a former well-known hacker (Mitnick and Simon in The art of deception: controlling the human element of security, Wiley, Indianapolis, 2002). The results indicate that applying the proposed technique produces correct mal-activ-ity diagrams from misuse cases.",
        "keywords": [
            "Model transformation",
            "Misuse cases",
            "Mal-activity diagrams",
            "Metamodels"
        ],
        "authors": [
            "Mohamed El-Attar"
        ],
        "file_path": "data/sosym-all/s10270-012-0240-5.pdf"
    },
    {
        "title": "On the automation-supported derivation of domain-speciﬁc UML proﬁles considering static semantics",
        "submission-date": "2019/05",
        "publication-date": "2021/05",
        "abstract": "In the light of standardization, the model-driven engineering (MDE) is becoming increasingly important for the development of DSLs, in addition to traditional approaches based on grammar formalisms. Metamodels deﬁne the abstract syntax and static semantics of a DSL and can be created by using the language concepts of the Meta Object Facility (MOF) or by deﬁning a UML proﬁle.\nBoth metamodels and UML proﬁles are often provided for standardized DSLs, and the mappings of metamodels to UML proﬁles are usually speciﬁed informally in natural language, which also applies for the static semantics of metamodels and/or UML proﬁles, which has the disadvantage that ambiguities can occur, and that the static semantics must be manually translated into a machine-processable language.\nTo address these weaknesses, we propose a new automated approach for deriving a UML proﬁle from the metamodel of a DSL. One novelty is that subsetting or redeﬁning metaclass attributes are mapped to stereotype attributes whose values are computed at runtime via automatically created OCL expressions. The automatic transfer of the static semantics of a DSL to a UML proﬁle is a further contribution of our approach. Our DSL Metamodeling and Derivation Toolchain (DSL-MeDeTo) implements all aspects of our proposed approach in Eclipse. This enabled us to successfully apply our approach to the two DSLs Test Description Language (TDL) and Speciﬁcation and Description Language (SDL).",
        "keywords": [],
        "authors": [
            "Alexander Kraas"
        ],
        "file_path": "data/sosym-all/s10270-021-00890-1.pdf"
    },
    {
        "title": "On the formal interpretation and behavioural consistency checking of SysML blocks",
        "submission-date": "2014/10",
        "publication-date": "2015/12",
        "abstract": "The Systems Modeling Language (SysML) is a semi-formal, graphical modelling language used in the speciﬁcation and design of systems. We describe how Communicating Sequential Processes (CSP) and its associated reﬁnement checker, FDR3, may be used to underpin an approach that facilitates the reﬁnement checking of the behavioural consistency of SysML diagrams. We achieve this by utilising CSP as a semantic domain for reasoning about SysML behavioural aspects: activities and state machines are given a formal, process-algebraic semantics. These behaviours execute within the context of the structural diagrams to which they relate, and this is reﬂected in the CSP descriptions that depict their characteristic patterns of interaction. We describe how CSP and FDR3 can be used in conjunction with SysML in a formal, top-down approach to systems engineering. Moreover, the compositionality afforded by CSP alleviates the state space explosion problem frequently encountered with complex formal models and complements the top-down approach of SysML. Typically, a system is composed from constituent systems using the concept of blocks. SysML permits two alternative interpretations with regard to the behaviour of the resulting composition. We argue that the use of a process-algebraic formalism enables us to explore the relationships between these interpretations in a more rigorous fashion than would otherwise be the case.",
        "keywords": [
            "SysML",
            "CSP",
            "State machines",
            "Activities",
            "Blocks"
        ],
        "authors": [
            "Jaco Jacobs",
            "Andrew Simpson"
        ],
        "file_path": "data/sosym-all/s10270-015-0511-z.pdf"
    },
    {
        "title": "The 2011 “State of the Journal” Report\nEditorial for the SoSyM Issue 2012/01: Part 2",
        "submission-date": "2011/12",
        "publication-date": "2011/12",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Robert France",
            "Geri Georg",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-011-0225-9.pdf"
    },
    {
        "title": "Effective application of process improvement patterns to business processes",
        "submission-date": "2013/09",
        "publication-date": "2014/12",
        "abstract": "Improving the operational effectiveness and efﬁ-ciency of processes is a fundamental task of business process management (BPM). There exist many proposals of process improvement patterns (PIPs) as practices that aim at support-ing this goal. Selecting and implementing relevant PIPs are therefore an important prerequisite for establishing process-aware information systems in enterprises. Nevertheless, there is still a gap regarding the validation of PIPs with respect to their actual business value for a speciﬁc application scenario before implementation investments are incurred. Based on empirical research as well as experiences from BPM projects, this paper proposes a method to tackle this challenge. Our approach toward the assessment of process improvement pat-terns considers real-world constraints such as the role of senior stakeholders or the cost of adapting available IT sys-tems. In addition, it outlines process improvement poten-tials that arise from the information technology infrastruc-ture available to organizations, particularly regarding the combination of enterprise resource planning with business process intelligence. Our approach is illustrated along a real-world business process from human resource manage-ment. The latter covers a transactional volume of about 29,000 process instances over a period of 1year. Overall, our approach enables both practitioners and researchers to reasonably assess PIPs before taking any process implemen-tation decision.",
        "keywords": [
            "Business process modeling",
            "Business process design",
            "Business process optimization",
            "Business process intelligence",
            "Business process quality"
        ],
        "authors": [
            "Matthias Lohrmann",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-014-0443-z.pdf"
    },
    {
        "title": "Editorial for the SoSyM issue 2006/02",
        "submission-date": "2006/05",
        "publication-date": "2006/05",
        "abstract": "This issue consists of two parts: The ﬁrst part has three regular papers. The second part contains a special sec-tion on Service-Based Software Engineering edited by Manfred Broy, Heinrich Hussmann, Ingolf H. Krüger and Bernhard Schätz.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-006-0019-7.pdf"
    },
    {
        "title": "Why Johnny can’t model",
        "submission-date": "2009/03",
        "publication-date": "2009/03",
        "abstract": "I recently taught an undergraduate C++ course for second year undergraduate students. Teaching this course provided me with some insight into why some students have difﬁculty grasping the abstraction and modeling concepts introduced in the advanced software development courses we offer to third and fourth year students. While grading their programming assignments it became evident that students were not thinking of their solutions in an object-oriented (OO) manner. The students were becoming skilled at stringing together C++ statements to produce working programs, but they were having problems thinking about solutions in terms of collaborating objects. Many students used objects more as passive maintainers of data rather than as active participants in a collaborative effort to accomplish functional goals. For example, classes with references to other classes were rare; many classes had only basic get and set methods, and had attributes that uniquely identiﬁed objects. What was surprising was the extensive use of globally declared data structures in some of the programs. If students do not understand how to effectively use OOP concepts to solve problems then it is not surprising that they have problems building good models of OO solutions. Their underdeveloped OOP skills make it difﬁcult for them to distinguish good and bad OO abstractions. The link between modeling and programming skills should not be undervalued. A good modeler should also be a skilled programmer. A great modeler is invariably an expert programmer. On the other hand, good programmers are not necessarilygoodmodelers.Highly-skilledprogrammersmay rely on mentally-held patterns and abstractions when constructing programs, but using those patterns and abstractions to produce good models is an acquired skill. Good programming knowledge should not be equated to good knowledge of the syntax and semantics of a programming language. A few students in my C++ class did have good knowledge of the C++ syntax and semantics, but they also had poor OOP skills. Learning how to program is not the same as learning the syntax and semantics of a programming language. Similarly, learning how to model is not the same as learning the syntax and semantics of a modeling language such as the UML. As an analogy, consider how abstractions are developed and used in mathematics. For example, Category Theory provides abstractions over mathematical structures (e.g., sets) and their relationships (e.g., functions). The developers of these abstractions had in-depth knowledge of the mathematical structures (including in-depth knowledge of their manipulations) they were abstracting over. Few will argue that effective use of these abstractions requires at least a good understanding of the mathematical structures that the theory abstracts over. Attempting to use Category Theory without such knowledge can confuse rather than enlighten. Given the above, how we can better lay the foundation on whichstudentsdevelopgoodmodelingskills?Weshouldcer- tainly continue to expose students to programming concepts as early as possible in the curriculum. A problem with many introductory programming courses is that emphasis has been more on covering programming language syntax and seman- tics at the expense of material that addresses how abstrac- tions provided by a programming language can be used to develop good quality solutions. The growing complexity of programming languages is partially to blame for this state of affairs, but I would argue that teaching students how to pro- gram trumps the need to expose students to a wide range of program language features. The decision to cover a language",
        "keywords": [],
        "authors": [
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-009-0117-4.pdf"
    },
    {
        "title": "A systematic literature review on IoT-aware business process modeling views, requirements and notations",
        "submission-date": "2021/07",
        "publication-date": "2022/10",
        "abstract": "The Internet of things has been adopted in several sectors both influencing how people work and enhancing organizations’ business processes. This resulted in the rise of relevant research topics such as IoT-aware business processes. The modeling of these processes makes it possible to better understand working scenarios and to support the adoption of model-driven development approaches for IoT-aware and process-oriented software systems. Since much research has been performed on this topic, a better awareness of the current status is needed. This paper reports a systematic literature review to develop a map on modeling notations for IoT-aware business processes. The survey mainly adopts an academic point of view, resulting in the detailed analysis of 84 research works from the leading computer science digital libraries. The output of the review is in the form of schemes and reflections. In particular, our research aims to shed light on (1) the relevant modeling views referring to different types of IoT-aware business processes; (2) the IoT requirements supported by the modeling notations; and (3) the modeling notations proposed and/or adopted to model IoT-aware business processes. Finally, our research work highlights possible future research lines needing further investigations.",
        "keywords": [
            "IoT-aware business process",
            "IoT",
            "Business process management",
            "Modeling notation"
        ],
        "authors": [
            "Ivan Compagnucci",
            "Flavio Corradini",
            "Fabrizio Fornari",
            "Andrea Polini",
            "Barbara Re",
            "Francesco Tiezzi"
        ],
        "file_path": "data/sosym-all/s10270-022-01049-2.pdf"
    },
    {
        "title": "A model-driven method for enacting the design-time QoS analysis of business processes",
        "submission-date": "2011/10",
        "publication-date": "2013/05",
        "abstract": "Business Process Management (BPM) is a holistic approach for describing, analyzing, executing, managing, and improving large enterprise business processes. A business process can be seen as a flow of tasks that are orchestrated to accomplish well-defined goals such as goods production or services delivery. From an IT perspective, BPM is closely related to a business process automation approach carried out by use of IT standards and technologies, such as service-oriented architectures (SOAs) and Web Services. This paper specifically focuses on fully automated business processes that are defined and executed as orchestrations of software services. In a BPM context, the ability to predict at design time the business process behavior assumes a strategic relevance, both to early assess whether or not the business goals are achieved and to gain a competitive advantage. A business process is typically specified by use of Business Process Modeling Notation (BPMN), the standard language for the high-level description of business processes. Unfortunately, BPMN does not support the characterization of the business process in terms of nonfunctional or QoS properties, such as performance and reliability. To overcome such a limitation, this paper introduces Performability-enabled BPMN (PyBPMN), a lightweight BPMN extension for the specification of performance and reliability properties. PyBPMN enables the design time prediction of the business processes behavior, in terms of performance and reliability properties. Such prediction activity requires the use of models that are to be first built and then evaluated. In this respect, this work introduces a model-driven method that exploits PyBPMN to predict, at design time, the performance and the reliability of a business process, either to select the process configuration that provides the best behavior or to check if a given configuration satisfies the overall requirements. The proposed model-driven method that enacts the automated analysis of a business process behavior embraces the complete business process development cycle, from the specification phase down to the implementation phase. The paper also describes how the proposed model-driven method is implemented. The several model transformations at the core of the method have been implemented by use of QVT, and the standard language for specifying model transformations provided by OMG’s MDA. The availability of such automated model transformations allows business analysts to predict the process behavior with no extra effort and without being required to own specific skills of performance or reliability theory, as shown by use of an example application.",
        "keywords": [
            "Business process",
            "MDA",
            "BPMN",
            "Performance",
            "QoS",
            "LQN"
        ],
        "authors": [
            "Paolo Bocciarelli",
            "Andrea D’Ambrogio"
        ],
        "file_path": "data/sosym-all/s10270-013-0345-5.pdf"
    },
    {
        "title": "The extended EA ModelSet—a FAIR dataset for researching and reasoning enterprise architecture modeling practices",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Conceptual modeling research is increasingly investigating the application of artiﬁcial intelligence (AI) and machine learning (ML) to automate tasks like model creation, completion, analysis, and processing. This trend also applies to enterprise architecture (EA) research. In contrast to its neighboring disciplines, such as business process management, EA lacks proper guidelines, patterns, and best practices to create high-quality EA models. A currently limiting factor for conducting AI-based research to bridge these gaps is the scarcity of openly available models of adequate quality and quantity. With this paper, our aim is to address this limitation by introducing the extended EA ModelSet, a curated and FAIR repository of enterprise architecture models represented in the ArchiMate modeling language that can be used by the research and practitioner community. We report on our efforts to build the EA ModelSet and elaborate on exemplary future empirical and ML-based research that can facilitate the dataset. We hope that this paper sparks a community effort toward the further development and maintenance of the EA ModelSet.",
        "keywords": [
            "Enterprise modeling",
            "Machine learning",
            "FAIR",
            "Enterprise architecture",
            "Dataset",
            "Artiﬁcial intelligence",
            "Conceptual modeling",
            "ArchiMate"
        ],
        "authors": [
            "Philipp-Lorenz Glaser",
            "Emanuel Sallinger",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-025-01278-1.pdf"
    },
    {
        "title": "KIPO: the knowledge-intensive process ontology",
        "submission-date": "2012/09",
        "publication-date": "2014/04",
        "abstract": "A business process is a sequence of activities that aims at creating products or services, granting value to the customer, and is generally represented by a business process model. Business process models play an important role in bridging the gap between the business domain and the information technology, increasing the weight of business model-ing as ﬁrst step of software development. However, the tra-ditional way of representing a process is not suitable for the so-called Knowledge-Intensive Processes (KIP). This type of process comprises sequences of activities based on intensive acquisition,sharing,storageand(re)useofknowledge,sothat the amount of value added to the organization depends on the actor knowledge. Current research in the literature points to the lack of approaches to make this kind of process explicit and strategies for handling information that is necessary for their understanding and support. The goal of this paper is to present KIPO—a knowledge-intensive process ontology, which encompasses a clear and semantically rich deﬁnition of KIPs, and to discuss the results of a case study to eval-uate KIPO with regard to its applicability and capability of making all relevant knowledge embedded in a KIP explicit.",
        "keywords": [
            "Knowledge-intensive process",
            "Knowledge-intensive process ontology",
            "Process representation",
            "Foundational ontology"
        ],
        "authors": [
            "Juliana Baptista dos Santos França",
            "Joanne Manhães Netto",
            "Juliana do E. S. Carvalho",
            "Flávia Maria Santoro",
            "Fernanda Araujo Baião",
            "Mariano Pimentel"
        ],
        "file_path": "data/sosym-all/s10270-014-0397-1.pdf"
    },
    {
        "title": "Continuous Evolution of Digital Twins using the DarTwin Notation",
        "submission-date": "2023/09",
        "publication-date": "2024/11",
        "abstract": "Despite best efforts, various challenges remain in the creation and maintenance processes of digital twins (DTs). One of those primary challenges is the constant, continuous and omnipresent evolution of systems, their user’s needs and their environment, demanding the adaptation of the developed DT systems. DTs are developed for a speciﬁc purpose, which generally entails the monitoring, analysis, simulation or optimisation of a speciﬁc aspect of an actual system, referred to as the actual twin (AT). As such, when the twin system changes, that is either the AT itself changes, or the scope/purpose of a DT is modiﬁed, the DTs usually evolve in close synchronicity with the AT. As DTs are software systems, the best practices or methodologies for software evolution can be leveraged. This paper tackles the challenge of maintaining a (set of) DT(s) throughout the evolution of the user’s requirements and priorities and tries to understand how this evolution takes place. In doing so, we provide two contributions: (i) we develop DarTwin, a visual notation form that enables reasoning on a twin system, its purposes, properties and implementation, and (ii) we introduce a set of architectural transformations that describe the evolution of DT systems. The development of these transformations is driven and illustrated by the evolution and transformations of a family home’s DT, whose purpose is expanded, changed and re-prioritised throughout its ongoing lifecycle. Additionally, we evaluate the transformations on a laboratory-scale gantry crane’s DT.",
        "keywords": [
            "System evolution",
            "System design",
            "Composability",
            "Digital twin",
            "Smart home"
        ],
        "authors": [
            "Joost Mertens",
            "Stefan Klikovits",
            "Francis Bordeleau",
            "Joachim Denil",
            "Øystein Haugen"
        ],
        "file_path": "data/sosym-all/s10270-024-01216-7.pdf"
    },
    {
        "title": "STAIRS towards formal design with sequence diagrams",
        "submission-date": "2004/05",
        "publication-date": "2005/10",
        "abstract": "The paper presents STAIRS [1], an approach to the compositional development of UML interactions supporting the speciﬁcation of mandatory as well as potential behavior. STAIRS has been designed to facilitate the use of interactions for requirement capture as well as test speciﬁcation. STAIRS assigns a precise interpretation to the various steps in incremental system development based on an approach to reﬁnement known from the ﬁeld of formal methods and provides thereby a foundation for compositional analysis. An interaction may characterize three main kinds of traces. A trace may be (1) positive in the sense that it is valid, legal or desirable, (2) negative meaning that it is invalid, illegal or undesirable, or (3) inconclusive meaning that it is considered irrelevant for the interaction in question. The basic increments in system development proposed by STAIRS, are structured into three main kinds referred to as supplementing, narrowing and detailing. Supplementing categorizes inconclusive traces as either positive or negative. Narrowing reduces the set of positive traces to capture new design decisions or to match the problem more adequately. Detailing involves introducing a more detailed description without signiﬁcantly altering the externally observable behavior.",
        "keywords": [
            "UML interactions",
            "Formal semantics",
            "Explicit non-determinism",
            "Reﬁnement",
            "Sequence diagrams"
        ],
        "authors": [
            "Øystein Haugen",
            "Knut Eilif Husa",
            "Ragnhild Kobro Runde",
            "Ketil Stølen"
        ],
        "file_path": "data/sosym-all/s10270-005-0087-0.pdf"
    },
    {
        "title": "Towards an integrated graph-based semantics for UML",
        "submission-date": "2005/08",
        "publication-date": "2008/08",
        "abstract": "This paper shows how a central part of the Uniﬁed Modeling Language (UML) can be integrated into a single visual semantic model. It discusses UML models compo-sed of class, object, state, sequence and collaboration dia-grams and presents an integrated semantics of these models. As formal basis the theoretically well-founded area of graph transformation is employed which supports a visual and rule-based transformation of UML model states. For the transla-tion of a UML model into a graph transformation system the operations in class diagrams and the transitions in state diagrams are associated with graph transformation rules that are then combined into one system in order to obtain a single coherent semantic description. Operation calls in sequence and collaboration diagrams can be associated with applica-tions of graph transformation rules in the constructed graph transformation system so that valid sequence and collabora-tion diagrams correspond to derivations, i.e., to sequences of graph transformation rule applications. The main aim of this paper is to provide a formal framework that supports visual simulation of integrated UML speciﬁcations in which system statesandstatechangesaremodeledinastraightforwardway.",
        "keywords": [
            "UML diagram",
            "Graph transformation",
            "Formal semantics"
        ],
        "authors": [
            "Sabine Kuske",
            "Martin Gogolla",
            "Hans-Jörg Kreowski",
            "Paul Ziemann"
        ],
        "file_path": "data/sosym-all/s10270-008-0101-4.pdf"
    },
    {
        "title": "Modelling the interplay of security, privacy and trust in sociotechnical systems: a computer-aided design approach",
        "submission-date": "2018/03",
        "publication-date": "2019/07",
        "abstract": "Personal data have become a central asset for multiple enterprise applications and online services offered by private companies, public organisations or a combination of both. The sensitivity of such data and the continuously growing legislation that accompanies their management dictate the development of methods that allow the development of more secure, trustworthy software systems with focus on privacy protection. The contribution of this paper is the deﬁnition of a novel requirements engineering method that supports both early and late requirements speciﬁcation, giving emphasis on security, privacy and trust. The novelty of our work is that it provides the means for software designers and security experts to analyse the system-to-be from multiple aspects, starting from identifying high-level goals to the deﬁnition of business process composition, and elicitation of mechanisms to fortify the system from external threats. The method is supported by two CASE tools. To demonstrate the applicability and usefulness of our work, the paper shows its applications to a real-world case study.",
        "keywords": [
            "Security",
            "Privacy",
            "Trust",
            "Sociotechnical systems",
            "CASE tools"
        ],
        "authors": [
            "Mattia Salnitri",
            "Konstantinos Angelopoulos",
            "Michalis Pavlidis",
            "Vasiliki Diamantopoulou",
            "Haralambos Mouratidis",
            "Paolo Giorgini"
        ],
        "file_path": "data/sosym-all/s10270-019-00744-x.pdf"
    },
    {
        "title": "Multi-level modeling: cornerstones of a rationale",
        "submission-date": "2020/05",
        "publication-date": "2022/01",
        "abstract": "This expert voice paper presents a comprehensive rationale of multi-level modeling. It aims not only at a systematic assessment of its prospects, but also at encouraging applications of multi-level modeling in business information systems and at providing a motivation for future research. The assessment is developed from a comparison of multi-level modeling with object-oriented, general-purpose modeling languages (GPMLs) and domain-speciﬁc modeling languages (DSMLs). To foster a differentiated evaluation, we propose a multi-perspective framework that accounts, among others, for essential design conﬂicts, different types of users, as well as economic aspects. Besides the assessment of the additional abstraction offered by multi-level modeling, the evaluation also identiﬁes speciﬁc drawbacks and remaining challenges. Based on the results of the comparative assessment, in order to foster the adoption and further development of multi-level modeling, we discuss the prospects of supplementing multi-level modeling languages with multi-level programming languages and suggest possible dissemination strategies customized for different groups of users. The paper concludes with an outline of future research.",
        "keywords": [
            "Essential design conﬂicts",
            "Multi-perspective evaluation framework",
            "Multi-level programming languages",
            "Integration of models and code",
            "Multi-level dissemination strategies"
        ],
        "authors": [
            "Ulrich Frank"
        ],
        "file_path": "data/sosym-all/s10270-021-00955-1.pdf"
    },
    {
        "title": "A benchmark for OCL engine accuracy, determinateness, and efﬁciency",
        "submission-date": "2009/05",
        "publication-date": "2010/09",
        "abstract": "Since several years, the Object Constraint Language (OCL) is a central component in modeling and transformation languages like the Uniﬁed Modeling Language, the Meta Object Facility, and Query View Transformation. Consequently, approaches MDE (Model-Driven Engineering) depend on this language. OCL is present not only in areas inﬂuenced by the OMG but also in the Eclipse Modeling Framework (EMF). Thus the quality of OCL and its realization in tools seems to be crucial for the success of model-driven development. Surprisingly, up to now a benchmark for OCL to measure quality properties has not been proposed. This paper puts forward in the ﬁrst part the concepts of a comprehensive OCL benchmark. Our benchmark covers (1) OCL engine accuracy (e.g., for the handling of the undeﬁned value, the use of variables and the implementation of OCL standard operations), (2) OCL engine determinateness properties (e.g., for the collection operations ‘any’ and ‘ﬂatten’), and (3) OCL engine efﬁciency (for data type and user-deﬁned operations). In the second part, this paper empirically evaluates the proposed benchmark concepts by examining several OCL tools. The paper clariﬁes a number of differences in handling particular language features and under speciﬁcations in the OCL standard.",
        "keywords": [
            "OCL",
            "Benchmark",
            "UML",
            "MDE",
            "Accuracy",
            "Determinateness",
            "Efﬁciency"
        ],
        "authors": [
            "Mirco Kuhlmann",
            "Lars Hamann",
            "Martin Gogolla",
            "Fabian Büttner"
        ],
        "file_path": "data/sosym-all/s10270-010-0174-8.pdf"
    },
    {
        "title": "A model-based design approach for simulation and virtual prototyping of automotive control systems using port-Hamiltonian systems",
        "submission-date": "2016/03",
        "publication-date": "2017/12",
        "abstract": "Cyber–physical systems (CPS) such as automotive control systems consist of various interacting cyber and physical com-\nponents. Heterogeneous domains, composition of multiple components, complex dynamics, and nonlinearities result in\nsigniﬁcant challenges for design, modeling, and simulation of CPS. Model-based design can be used to address such chal-\nlenges, but it is very important to use physically accurate heterogeneous models that can be composed to represent the overall\nsystem behavior. Further, it is important to preserve the properties derived from analyses based on the mathematical models\nin the control system implementation in order to reduce costly testing and design changes late in the development cycle. This\npaper proposes a model-based design methodology for automotive control software using port-Hamiltonian systems (PHS).\nPHS are used to model the vehicle dynamics, speed and steering control systems, and the interactions between physical and\ncyber components. Passivity analysis is used to design the controllers and ensure system stability. More importantly, the\nproposed approach guarantees that passivity is preserved after time-discretization and quantization of the controllers. The\nmodels are then used for code generation and compilation, scheduling, and software deployment, ensuring that passivity is\npreserved by the control system implementation. We evaluate the methodology using an automotive control design case study\nimplemented on a hardware-in-the-loop simulation platform and present simulation results to demonstrate its effectiveness.",
        "keywords": [
            "Cyber",
            "physical systems",
            "Model-based design",
            "Port-Hamiltonian systems",
            "Passivity",
            "Automotive control software"
        ],
        "authors": [
            "Siyuan Dai",
            "Zhenkai Zhang",
            "Xenofon Koutsoukos"
        ],
        "file_path": "data/sosym-all/s10270-017-0646-1.pdf"
    },
    {
        "title": "A method for testing and validating executable statechart models",
        "submission-date": "2016/08",
        "publication-date": "2018/05",
        "abstract": "Statecharts constitute an executable language for modelling event-based reactive systems. The essential complexity of\nstatechart models solicits the need for advanced model testing and validation techniques. In this article, we propose a method\nimed at enhancing statechart design with a range of techniques that have proven their usefulness to increase the quality\nand reliability of source code. The method is accompanied by a process that ﬂexibly accommodates testing and validation\ntechniques such as test-driven development, behaviour-driven development, design by contract, and property statecharts that\ncheck for violations of behavioural properties during statechart execution. The method is supported by the Sismic tool, an\nopen-source statechart interpreter library in Python, which supports all the aforementioned techniques. Based on this tool-\ning, we carry out a controlled user study to evaluate the feasibility, usefulness and adequacy of the proposed techniques for\nstatechart testing and validation.",
        "keywords": [
            "Statechart",
            "Executable modeling",
            "Behaviour-driven development",
            "Design by contract",
            "Runtime veriﬁcation"
        ],
        "authors": [
            "Tom Mens",
            "Alexandre Decan",
            "Nikolaos I. Spanoudakis"
        ],
        "file_path": "data/sosym-all/s10270-018-0676-3.pdf"
    },
    {
        "title": "Integration of clinical and genomic data to enhance precision medicine: a case of study applied to the retina-macula",
        "submission-date": "2021/11",
        "publication-date": "2022/09",
        "abstract": "Age-related macular degeneration is a complex, multifactorial, and neurodegenerative disease that is the third cause of blindness after cataracts and glaucoma. To date, there are no effective remedies available for treating the disease. Therefore, the main goal of the scientiﬁc community is to uncover the underlying role that both genetics and environmental factors play in the development of the disease. Nevertheless, the complexity of the domain, the heterogeneity of the information, and the massive amounts of existing data hinder the daily work of clinical experts to provide an accurate diagnosis and treatment. In this work, we present how clinicians can beneﬁt from the development of ontologically well-grounded information systems to support the management of both clinical and genomic data. First, we summarize the results obtained in a previous work that cover the clinical perspective using an information system called G-MAC, that has been specially developed for the management of clinical data. Then, we present the results of an exhaustive study of the genetic factors of age-related macular degeneration by using an information system that was developed with the aim of enhancing the management of complex genomic data. Finally, we state how the connection of both perspectives through the use of conceptual models can beneﬁt clinicians and patients through a more accurate Medicine of Precision.",
        "keywords": [
            "Conceptual modeling",
            "Information systems",
            "Precision medicine",
            "Age-related macular degeneration",
            "Genomics"
        ],
        "authors": [
            "José Fabián Reyes Román",
            "Ana León Palacio",
            "Alberto García Simón",
            "Rubén Cabrera Beyrouti",
            "Oscar Pastor"
        ],
        "file_path": "data/sosym-all/s10270-022-01039-4.pdf"
    },
    {
        "title": "Integration of data validation and user interface concerns in a DSL for web applications",
        "submission-date": "2009/11",
        "publication-date": "2010/09",
        "abstract": "Data validation rules constitute the constraints that data input and processing must adhere to in addition to the structural constraints imposed by a data model. Web mod-eling tools do not make all types of data validation explicit in their models, hampering full code generation and model expressivity. Web application frameworks do not offer a consistent interface for data validation. In this paper, we present a solution for the integration of declarative data val-idation rules with user interface models in the domain of web applications, unifying syntax, mechanisms for error han-dling, and semantics of validation checks, and covering value well-formedness, data invariants, input assertions, and actionassertions.WehaveimplementedtheapproachinWeb-DSL, a domain-speciﬁc language for the definition of web applications.",
        "keywords": [
            "Web application",
            "Domain-speciﬁc language",
            "Data validation"
        ],
        "authors": [
            "Danny M. Groenewegen",
            "Eelco Visser"
        ],
        "file_path": "data/sosym-all/s10270-010-0173-9.pdf"
    },
    {
        "title": "AI simulation by digital twins: systematic survey, reference framework, and mapping to a standardized architecture",
        "submission-date": "2024/10",
        "publication-date": "2025/06",
        "abstract": "Insufﬁcient data volume and quality are particularly pressing challenges in the adoption of modern subsymbolic AI. To alleviate these challenges, AI simulation uses virtual training environments in which AI agents can be safely and efﬁciently developed with simulated, synthetic data. Digital twins open new avenues in AI simulation, as these high-ﬁdelity virtual replicas of physical systems are equipped with state-of-the-art simulators and the ability to further interact with the physical system for additional data collection. In this article, we report on our systematic survey of digital twin-enabled AI simulation. By analyzing 22 primary studies, we identify technological trends and derive a reference framework to situate digital twins and AI components. Based on our ﬁndings, we derive a reference framework and provide architectural guidelines by mapping it onto the ISO 23247 reference architecture for digital twins. Finally, we identify challenges and research opportunities for prospective researchers.",
        "keywords": [
            "AI",
            "Artiﬁcial intelligence",
            "Data science",
            "Deep neural networks",
            "Digital twins",
            "Lifecycle model",
            "Machine learning",
            "Neural networks",
            "Reinforcement learning",
            "SLR",
            "Subsymbolic AI",
            "Survey",
            "Training"
        ],
        "authors": [
            "Xiaoran Liu",
            "Istvan David"
        ],
        "file_path": "data/sosym-all/s10270-025-01306-0.pdf"
    },
    {
        "title": "Converting metamodels to graph grammars: doing without advanced graph grammar features",
        "submission-date": "2012/12",
        "publication-date": "2013/09",
        "abstract": "In this paper, we present a method to convert a metamodel in the form of a UML class diagram into a context-sensitive graph grammar whose language comprises precisely the set of model graphs (UML object diagrams) that conform to the input metamodel. Compared to other approaches that deal with the same problem, we use a graph grammarformalismthatdoesnotemployanyadvancedgraph grammar features, such as application conditions, precedence rules, and production schemes. Specifically, we use Rekers and Schürr’s Layered Graph Grammars, which may be regarded as a pure generalization of standard context-sensitive string grammars. We show that elementary grammatical features, i.e., grammar labels and context-sensitive graph rewrite rules, sufﬁce to represent metamodels with arbitrary multiplicities and inheritance. Inspired by attribute string grammars, we also propose a graph-grammar-based approach to the semantic analysis of model graphs.",
        "keywords": [
            "Metamodel",
            "UML",
            "Graph grammar",
            "Parsability",
            "Parsing",
            "Semantic analysis"
        ],
        "authors": [
            "Luka Fürst",
            "Marjan Mernik",
            "Viljan Mahniˇc"
        ],
        "file_path": "data/sosym-all/s10270-013-0380-2.pdf"
    },
    {
        "title": "Formalizing the four-layer metamodeling stack with METAMORPH: potential and beneﬁts",
        "submission-date": "2021/03",
        "publication-date": "2022/03",
        "abstract": "Enterprise modeling deals with the increasing complexity of processes and systems by operationalizing model content and by linking complementary models and languages, thus amplifying the model value beyond mere comprehensible pictures. To enable this ampliﬁcation and turn models into computer-processable structures, a comprehensive formalization is needed. This paper presents the formalism MetaMorph based on typed ﬁrst-order logic and provides a perspective on the potential and beneﬁts of formalization that arise for a variety of research issues in conceptual modeling. MetaMorph deﬁnes modeling languages as formal languages with a signature Σ—comprising object types, relation types, and attributes through types and function symbols—and a set of constraints. Four case studies are included to show the effectiveness of this approach. Applying the MetaMorph formalism to the next level in the hierarchy of models, we create M2FOL, a formal modeling language for metamodels. We show that M2FOL is self-describing and therefore complete the formalization of the full four-layer metamodelingstack.Onthebasisofourgenericformalismapplicabletoarbitrarymodelinglanguages,weexaminefourcurrent research topics—modeling with power types, language interleaving & consistency, operations on models, and automatic translation of formalizations to platform-speciﬁc code—and how to approach them with the MetaMorph formalism. This shows that the rich knowledge stack on formal languages in logic offers new tools for old problems.",
        "keywords": [
            "Conceptual modeling",
            "Metamodeling",
            "Modeling language",
            "Formal language",
            "Predicate logic"
        ],
        "authors": [
            "Victoria Döller"
        ],
        "file_path": "data/sosym-all/s10270-022-00986-2.pdf"
    },
    {
        "title": "Editorial for the theme issue on model-based interoperability",
        "submission-date": "2011/02",
        "publication-date": "2011/02",
        "abstract": "The commercial benefits claimed for software based on visual and graphical modeling languages are well documented. Many domain specific modeling tools exist and are being used as point solutions. Tailoring of notations to the specific application domain and combined use of several languages define the nature of the approach, and constitute the source of the achievable benefits. Unfortunately, data representations and the mechanisms used to integrate modeling languages tend to be highly tool specific. This compromises the use of modeling languages in building tool chains that may contain components from several suppliers.",
        "keywords": [],
        "authors": [
            "Tony Clark",
            "Jorn Bettin"
        ],
        "file_path": "data/sosym-all/s10270-010-0184-6.pdf"
    },
    {
        "title": "A model-based architecture for interactive run-time monitoring",
        "submission-date": "2019/06",
        "publication-date": "2020/01",
        "abstract": "We present a model-based architecture for monitoring executions of models of real-time and embedded systems. This architecture is highly conﬁgurable and allows for the combination of various run-time monitoring tools, not only for observing the system execution, but also for interacting with it. Using a variety of case studies, we illustrate the use of the architecture for connecting the code generated from a model with a range of external tools for different purposes, including execution animation and run-time veriﬁcation. However, the external tool can not only consume information from the execution, but also generate input for it and thus inﬂuence and steer it.",
        "keywords": [
            "Model-driven engineering",
            "Run-time monitoring",
            "Observation",
            "Model instrumentation",
            "UML-RT"
        ],
        "authors": [
            "Nicolas Hili",
            "Mojtaba Bagherzadeh",
            "Karim Jahed",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-020-00780-y.pdf"
    },
    {
        "title": "ChatGPT in software modeling",
        "submission-date": "2023/05",
        "publication-date": "2023/05",
        "abstract": "We wonder—how many SoSyM readers have tried using OpenAI’sChatGPTtoformulatepromptstoexploretheinter-esting results that may be produced? It is really fascinating to consider the results when the query is deﬁned appropriately. Of course, we abstained from using ChatGPT to write any part of this editorial, but if we actually would generate portions of our text, we wonder how difﬁcult it would be for readers to identify the generated parts. ChatGPT is rather chatty, but even the produced text can be adapted by asking it for a speciﬁc answering style. The options and future extensions seem to be endless. In the context of this editorial, we do not consider legal or ethical issues of ChatGPT—these have to be sorted out eventually. We also do not discuss the problems of having ChatGPT text, or text generated by other Large Language Models (LLMs), in scientiﬁc papers. Rather, in this editorial, we focus more positively and ask the main question: How will ChatGPT be able to help us in a development process, especially in the tasks of developing or using models for analysis, production or understanding of software and systems?",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01106-4.pdf"
    },
    {
        "title": "Interactive web interfaces modeling, simulation and analysis using Colored Petri Nets",
        "submission-date": "2016/04",
        "publication-date": "2017/03",
        "abstract": "Interaction modeling is a relevant activity during software development processes. Created relying on Petri Nets theory and aiming to represent discrete time events, Colored Petri Nets (CPNs) are a graphical formal language developed and widely employed for system modeling. While traditional CPNs only have elements with ordinary stylization and behaviors, in this article we explore the key ideas behind Web Interaction Modeling Using Colored Petri Nets (wiCPN), a modeling style developed with focus on representing Web interactions as an incremental improvement of CPNs. We review wiCPN’s reﬁnements over CPNs and the modeling of the Web interface of Classroom eXperience (CX), a ubiquitous educational platform, thus verifying the modelspropertiestoensureitwasabletorepresentthedifferent access levels among its users and how wiCPN displayed suitability to comprehend this requirement on the gener-ated model. We have also improved the originally developed model with the modiﬁcation of elements to make it ﬁnite and fully analyzable. Also, we added temporization capabil-ities to the model and ran corresponding user simulation to observe the average time that users with different roles tend to spend during interactions. We compared wiCPN results with Uniﬁed Modeling Language (UML) Activity and Use Case diagrams, observing, as outcomes, that the generated model represented CX’s interactive ﬂow correctly and maintained a concise notation—a single wiCPN diagram was sufﬁcient to depict the same interactive ﬂow that, in UML, would require several diagrams, something that could overload the design team in actual software development scenarios. We also included new user experiments comprising qualitative results from experts. Finally, we created a reachability graph for the new model and generated a full state space report, analyzing Petri Nets properties such as boundedness, live-ness and home marking.",
        "keywords": [
            "Web interaction modeling",
            "Human",
            "computer interaction",
            "Web interfaces",
            "Formal methods",
            "Colored Petri Nets"
        ],
        "authors": [
            "Taffarel Brant-Ribeiro",
            "Rafael D. Araújo",
            "Igor E. Mendonça",
            "Michel S. Soares",
            "Renan G. Cattelan"
        ],
        "file_path": "data/sosym-all/s10270-017-0593-x.pdf"
    },
    {
        "title": "A graph-based framework for model-driven optimization facilitating impact analysis of mutation operator properties",
        "submission-date": "2021/10",
        "publication-date": "2023/01",
        "abstract": "Optimization problems in software engineering typically deal with structures as they occur in the design and maintenance of software systems. In model-driven optimization (MDO), domain-speciﬁc models are used to represent these structures while evolutionary algorithms are often used to solve optimization problems. However, designing appropriate models and evolutionary algorithms to represent and evolve structures is not always straightforward. Domain experts often need deep knowledge of how to conﬁgure an evolutionary algorithm. This makes the use of model-driven meta-heuristic search difﬁcult and expensive. We present a graph-based framework for MDO that identiﬁes and clariﬁes core concepts and relies on mutation operators to specify evolutionary change. This framework is intended to help domain experts develop and study evolutionary algorithms based on domain-speciﬁc models and operators. In addition, it can help in clarifying the critical factors for conducting reproducible experiments in MDO. Based on the framework, we are able to take a ﬁrst step toward identifying and studying important properties of evolutionary operators in the context of MDO. As a showcase, we investigate the impact of soundness and completeness at the level of mutation operator sets on the effectiveness and efﬁciency of evolutionary algorithms.",
        "keywords": [
            "Search-Based Software Engineering",
            "Model-Driven Engineering",
            "Evolutionary Computation"
        ],
        "authors": [
            "Stefan John",
            "Jens Kosiol",
            "Leen Lambers",
            "Gabriele Taentzer"
        ],
        "file_path": "data/sosym-all/s10270-022-01078-x.pdf"
    },
    {
        "title": "Asynchronous session subtyping as communicating automata refinement",
        "submission-date": "2020/02",
        "publication-date": "2021/01",
        "abstract": "We study the relationship between session types and behavioural contracts, representing Communicating Finite State Machines (CFSMs), under the assumption that processes communicate asynchronously. Session types represent a syntax-based approach for the description of communication protocols, while behavioural contracts, formally expressing CFSMs, follow an operational approach. We show the existence of a fully abstract interpretation of session types into a fragment of contracts that maps session subtyping into binary compliance-preserving CFSMs/behavioural contract refinement. In this way, on the one hand, we enrich the theory of session types with an operational characterization and, on the other hand, we use recent undecidability results for asynchronous session subtyping to obtain an original undecidability result for asynchronous CFSMs/behavioural contract refinement.",
        "keywords": [
            "Session types",
            "Behavioural contracts",
            "Communicating finite-state machines"
        ],
        "authors": [
            "Mario Bravetti",
            "Gianluigi Zavattaro"
        ],
        "file_path": "data/sosym-all/s10270-020-00838-x.pdf"
    },
    {
        "title": "Design of blockchain-based applications using model-driven engineering and low-code/no-code platforms: a structured literature review",
        "submission-date": "2022/10",
        "publication-date": "2023/06",
        "abstract": "The creation of blockchain-based software applications requires today considerable technical knowledge, particularly in software design and programming. This is regarded as a major barrier in adopting this technology in business and making it accessible to a wider audience. As a solution, low-code and no-code approaches have been proposed that require only little or no programming knowledge for creating full-fledged software applications. In this paper we extend a review of academic approaches from the discipline of model-driven engineering as well as industrial low-code and no-code development platforms for blockchains. This includes a content-based, computational analysis of relevant academic papers and the derivation of major topics. In addition, the topics were manually evaluated and refined. Based on these analyses we discuss the spectrum of approaches in this field and derive opportunities for further research.",
        "keywords": [
            "Blockchain",
            "Low-code",
            "No-code",
            "Model-driven engineering",
            "Software development"
        ],
        "authors": [
            "Simon Curty\nFelix Härer\nHans-Georg Fill"
        ],
        "file_path": "data/sosym-all/s10270-023-01109-1.pdf"
    },
    {
        "title": "Understanding and improving UML package merge",
        "submission-date": "2007/03",
        "publication-date": "2007/12",
        "abstract": "Package merge allows the content of one package to be combined with that of another package. Package merge is used extensively in the UML 2 speciﬁcation to modularize the definition of the UML 2 meta model and to deﬁne the four compliance levels of UML 2. Package merge is a novel construct in UML and currently not well understood. This paper summarizes our work to understand and improve package merge. First, we identify ambiguous and missing rules in the package merge definition and suggest corrections. Then, we formalize package merge and analyze it with respect to some desirable properties. Our analyses employs Alloy, a ﬁrst-order modelling language with tool support, and concepts from mathematical logic which allow us to develop a general taxonomy of package extension mechanisms. The analyses reveal the unexpected failure of important properties.",
        "keywords": [
            "UML",
            "Semantics formalization",
            "Model composition",
            "Metamodeling techniques"
        ],
        "authors": [
            "J. Dingel",
            "Z. Diskin",
            "A. Zito"
        ],
        "file_path": "data/sosym-all/s10270-007-0073-9.pdf"
    },
    {
        "title": "Linking models and their storage artifacts",
        "submission-date": "2011/06",
        "publication-date": "2011/06",
        "abstract": "Many developers working on model-based development projects experience a problem that highlights the need for better model management technologies. If someone wants to adapt an OO class that was generated from a model, the ﬁrst thing is to locate the “sources” used to generate the class (i.e., the model elements used to produce the class implementa-tion). This turned out to be a surprisingly difﬁcult and time-consuming task. It is worth shedding some light on why this task typically is difﬁcult if only to remind us that research on software modeling needs to extend its focus to include model management issues if we are to see more widespread adoption of software-modeling approaches.",
        "keywords": [],
        "authors": [
            "Bernhard Rumpe",
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-011-0208-x.pdf"
    },
    {
        "title": "Empirically evaluating OCL and Java for specifying constraints on UML models",
        "submission-date": "2013/11",
        "publication-date": "2014/11",
        "abstract": "The Object Constraint Language (OCL) has been applied, along with UML models, for various purposes such as supporting model-based testing, code generation, and automated consistency checking of UML models. However, a lot of challenges have been raised in the literature regarding its applicability in industry such as extensive training, slow learning curve, and significant effort to use OCL due to lack of familiarity of practitioners. To confirm these challenges, empirical evidence is needed, which is severely lacking in the literature. To build such preliminary evidence, we report a controlled experiment that was designed to evaluate OCL by comparing it with Java; a programming language that has also been used to specify constraints on UML models. Results show that the participants using OCL perform as good as the participants working with Java in terms of three objective quality metrics (i.e., completeness, conformance and redundancy) and two subjective metrics (i.e., applicability and confidence level). In addition, the participants using OCL performed consistently well for all the constraints of varying complexity, while fluctuating results were obtained for the participants using Java for the same constraints. Based on the empirical evidence, we can conclude that it does not make much difference to use OCL or Java for specifying constraints on UML models. However, the participants working with OCL performed consistently well on specifying constraints of varying complexity suggesting that OCL can be used to model complicated constraints (commonly observed in industrial applications) with the same quality as for simpler constraints. Moreover, additional analyses on the constraints when using Java and OCL tools revealed that tools are needed to specify fully correct constraints that can be used to support automation.",
        "keywords": [
            "OCL",
            "Java",
            "Controlled experiment",
            "Empirical study",
            "Constraints"
        ],
        "authors": [
            "Tao Yue",
            "Shaukat Ali"
        ],
        "file_path": "data/sosym-all/s10270-014-0438-9.pdf"
    },
    {
        "title": "SoSyM at three",
        "submission-date": "2004/11",
        "publication-date": "2004/11",
        "abstract": "This issue marks the end of three years since the ﬁrst issue of the International Journal on Software and System Modeling (SoSyM) was published. We sincerely thank the authors, reviewers, and editors who have contributed to the success of the journal as a media for collecting and disseminating high quality papers on software and system modeling. As we have done on previous anniversaries, we take this opportunity to give a “state of the journal” report and to acknowledge the reviewers, editors, and publication staﬀthat have contributed to another year of publication that has exceeded our expectations.",
        "keywords": [],
        "authors": [
            "Robert B. France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-004-0075-9.pdf"
    },
    {
        "title": "Harvesting models from web 2.0 databases",
        "submission-date": "2009/11",
        "publication-date": "2011/02",
        "abstract": "Data rather than functionality are the sources of competitive advantage for Web2.0 applications such as wikis, blogs and social networking websites. This valuable information might need to be capitalized by third-party applications or be subject to migration or data analysis. Model-Driven Engineering (MDE) can be used for these purposes. However, MDE first requires obtaining models from the wiki/blog/website database (a.k.a. model harvesting). This can be achieved through SQL scripts embedded in a program. However, this approach leads to laborious code that exposes the iterations and table joins that serve to build the model. By contrast, a Domain-Speciﬁc Language (DSL) can hide these “how” concerns, leaving the designer to focus on the “what”, i.e. the mapping of database schemas to model classes. This paper introduces Schemol, a DSL tailored for extracting models out of databases which considers Web2.0 specifics. Web2.0 applications are often built on top of general frameworks (a.k.a. engines) that set the databaseschema(e.g.,MediaWiki,Blojsom).Hence,table names offer little help in automating the extraction process. In addition, Web2.0 data tend to be annotated. User-provided data (e.g., wiki articles, blog entries) might contain semantic markups which provide helpful hints for model extraction. Unfortunately, these data end up being stored as opaque strings. Therefore, there exists a considerable conceptual gap between the source database and the target metamodel. Schemol offers extractive functions and view-like mechanisms to confront these issues. Examples using Blojsom as the blog engine are available for download.",
        "keywords": [
            "Model-driven engineering",
            "Web2.0",
            "Harvesting",
            "Data re-engineering",
            "Databases"
        ],
        "authors": [
            "Oscar Díaz",
            "Gorka Puente",
            "Javier Luis Cánovas Izquierdo",
            "Jesús García Molina"
        ],
        "file_path": "data/sosym-all/s10270-011-0194-z.pdf"
    },
    {
        "title": "Learning minimal automata with recurrent neural networks",
        "submission-date": "2023/02",
        "publication-date": "2024/03",
        "abstract": "In this article, we present a novel approach to learning ﬁnite automata with the help of recurrent neural networks. Our goal\nis not only to train a neural network that predicts the observable behavior of an automaton but also to learn its structure,\nincluding the set of states and transitions. In contrast to previous work, we constrain the training with a speciﬁc regularization\nterm. We iteratively adapt the architecture to learn the minimal automaton, in the case where the number of states is unknown.\nWe evaluate our approach with standard examples from the automata learning literature, but also include a case study of\nlearning the ﬁnite-state models of real Bluetooth Low Energy protocol implementations. The results show that we can ﬁnd\nan appropriate architecture to learn the correct minimal automata in all considered cases.",
        "keywords": [
            "Automata learning",
            "Machine learning",
            "Recurrent neural networks",
            "Bluetooth Low Energy",
            "Model inference"
        ],
        "authors": [
            "Bernhard K. Aichernig",
            "Sandra König",
            "Cristinel Mateis",
            "Andrea Pferscher",
            "Martin Tappler"
        ],
        "file_path": "data/sosym-all/s10270-024-01160-6.pdf"
    },
    {
        "title": "Enhancing the OPEN Process Framework with service-oriented method fragments",
        "submission-date": "2011/04",
        "publication-date": "2011/11",
        "abstract": "Service orientation is a promising paradigm that enables the engineering of large-scale distributed software systems using rigorous software development processes. The existing problem is that every service-oriented software development project often requires a customized development process that provides speciﬁc service-oriented software engineering tasks in support of requirements unique to that project. To resolve this problem and allow situational method engineering, we have deﬁned a set of method fragments in support of the engineering of the project-speciﬁc service-oriented software development processes. We have derived the proposed method fragments from the recurring features of 11 prominent service-oriented software development methodologies using a systematic mining approach. We have added these new fragments to the repository of OPEN Process Framework to make them available to software engi-neers as reusable fragments using this well-known method repository.",
        "keywords": [
            "Service-oriented software development",
            "OPEN Process Framework",
            "OPF repository",
            "Method fragment",
            "Situational method engineering"
        ],
        "authors": [
            "Mahdi Fahmideh Gholami",
            "Mohsen Shariﬁ",
            "Pooyan Jamshidi"
        ],
        "file_path": "data/sosym-all/s10270-011-0222-z.pdf"
    },
    {
        "title": "Editorial to the theme issue on multi-level modeling",
        "submission-date": "2016/09",
        "publication-date": "2016/10",
        "abstract": "Multi-level modeling, i.e., the explicit use of multiple levels of classification in modeling, is a conservative extension of the well-established, traditional two-level object-oriented paradigm. Two-level object-oriented technology has been tremendously successful in both modeling (e.g., UML) and programming (e.g., Java). However, it has been shown that attempting to capture certain domains or systems with only two classification levels (i.e., objects and their types) results in accidental complexity that stems from an impedance mismatch between the subject at hand and the solution technology used to capture it [2]. Examples for domains that can be much more elegantly captured using multiple classification levels are biological taxonomies, process (meta-) modeling, enforced software architectures, and systems with dynamic type levels [4].",
        "keywords": [],
        "authors": [
            "Colin Atkinson",
            "Thomas Kühne",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-016-0565-6.pdf"
    },
    {
        "title": "A UML and OWL description of Bunge’s upper-level ontology model",
        "submission-date": "2006/02",
        "publication-date": "2008/03",
        "abstract": "A prominent high-level ontology is that proposed by Mario Bunge. While it has been extensively used for research in IS analysis and conceptual modelling, it has not been employed in the more formal settings of semantic web research. We claim that its speciﬁcation in natural language is the key inhibitor to its wider use. Consequently, this paper offers a description of this ontology in open, standardized knowledge representation formats. The ontology is described both in UML and OWL in order to address needs of both semantic web and conceptual modelling communities.",
        "keywords": [],
        "authors": [
            "Joerg Evermann"
        ],
        "file_path": "data/sosym-all/s10270-008-0082-3.pdf"
    },
    {
        "title": "Formalization of the classiﬁcation pattern: survey of classiﬁcation modeling in information systems engineering",
        "submission-date": "2015/02",
        "publication-date": "2016/04",
        "abstract": "Formalization is becoming more common in all stages of the development of information systems, as a better understanding of its beneﬁts emerges. Classiﬁcation systems are ubiquitous, no more so than in domain model-ing. The classiﬁcation pattern that underlies these systems provides a good case study of the move toward formaliza-tion in part because it illustrates some of the barriers to formalization, including the formal complexity of the pat-tern and the ontological issues surrounding the “one and the many.” Powersets are a way of characterizing the (com-plex) formal structure of the classiﬁcation pattern, and their formalization has been extensively studied in mathemat-ics since Cantor’s work in the late nineteenth century. One can use this formalization to develop a useful benchmark. There are various communities within information systems engineering (ISE) that are gradually working toward a for-malization of the classiﬁcation pattern. However, for most of these communities, this work is incomplete, in that they have not yet arrived at a solution with the expressiveness of the powerset benchmark. This contrasts with the early smooth adoption of powerset by other information systems communities to, for example, formalize relations. One way of understanding the varying rates of adoption is recogniz-ing that the different communities have different historical baggage. Many conceptual modeling communities emerged from work done on database design, and this creates hur-dles to the adoption of the high level of expressiveness of powersets. Another relevant factor is that these communities also often feel, particularly in the case of domain modeling, a responsibility to explain the semantics of whatever formal structures they adopt. This paper aims to make sense of the formalization of the classiﬁcation pattern in ISE and surveys its history through the literature, starting from the relevant theoretical works of the mathematical literature and gradu-ally shifting focus to the ISE literature. The literature survey follows the evolution of ISE’s understanding of how to for-malize the classiﬁcation pattern. The various proposals are assessed using the classical example of classiﬁcation; the Linnaean taxonomy formalized using powersets as a bench-mark for formal expressiveness. The broad conclusion of the survey is that (1) the ISE community is currently in the early stages of the process of understanding how to formalize the classiﬁcation pattern, particularly in the requirements for expressiveness exempliﬁed by powersets, and (2) that there is an opportunity to intervene and speed up the process of adoption by clarifying this expressiveness. Given the central place that the classiﬁcation pattern has in domain model-ing, this intervention has the potential to lead to signiﬁcant improvements.",
        "keywords": [
            "Classiﬁcation system",
            "Classiﬁcation",
            "Powerset",
            "Powertype",
            "Set theory"
        ],
        "authors": [
            "Chris Partridge",
            "Sergio de Cesare",
            "Andrew Mitchell",
            "James Odell"
        ],
        "file_path": "data/sosym-all/s10270-016-0521-5.pdf"
    },
    {
        "title": "From low-level programming to full-ﬂedged industrial model-based development: the story of the Rubus Component Model",
        "submission-date": "2023/05",
        "publication-date": "2023/05",
        "abstract": "Developing distributed real-time systems is a complex task that has historically entailed specialized handcraft. In this paper,\nwe propose a retrospective on the (r)evolutionary changes that led to the transition from low-level programming to industrial\nfull-ﬂedged model-based development embodied by the Rubus Component Model and its tool-ecosystem. We focus on the\nneeds, challenges, and solutions of a 15-year-long evolution journey of a software development approach that has gone from\nlow-level and manual programming to a highly automated environment offering modeling, analysis, and development of\nvehicular software systems with multi-criticality for deployment on single- and multi-core platforms.",
        "keywords": [
            "Component model",
            "Model-based development",
            "Vehicular embedded systems real",
            "time systems"
        ],
        "authors": [
            "Alessio Bucaioni",
            "Federico Ciccozzi",
            "Amleto Di Salle",
            "Mikael Sjödin"
        ],
        "file_path": "data/sosym-all/s10270-023-01107-3.pdf"
    },
    {
        "title": "Scientiﬁc workﬂow execution in the cloud using a dynamic runtime model",
        "submission-date": "2021/12",
        "publication-date": "2023/06",
        "abstract": "To explain speciﬁc phenomena, scientists perform a sequence of tasks, e.g., to gather, analyze and interpret data, forming a scientiﬁc workﬂow. Depending on the complexity of the workﬂow, scientists require access to various kinds of tools, applications and infrastructures for individual tasks. Current approaches are often limited to managing these resources at design time, requiring the scientist to preemptively set up applications essential for their workﬂow. Therefore, a dynamic provisioning and conﬁguration of computing resources are required that fulﬁlls these needs at runtime. In this paper, we present a dynamic runtime model that couples workﬂow tasks with their individual applications and infrastructure requirements. This runtime model is used as a knowledge base by a model-driven workﬂow execution engine orchestrating the sequence of tasks and their infrastructure. We exhibit that the simplicity of the runtime model supports the creation of highly tailored infrastructures, the integration of self-developed applications, as well as a human-in-the-loop allowing scientists to monitor and interact with the workﬂow at runtime. To tackle the heterogeneity of cloud provider interfaces, we implement the workﬂow runtime model by extending the Open Cloud Computing Interface cloud standard, which provides an extensible data model as well as a uniform interface to manage cloud resources. We demonstrate the applicability of our approach using three case studies and discuss the beneﬁts of the runtime model from a user and system perspective.",
        "keywords": [
            "Runtime model",
            "Workﬂow",
            "Cloud",
            "OCCI"
        ],
        "authors": [
            "Johannes Erbel",
            "Jens Grabowski"
        ],
        "file_path": "data/sosym-all/s10270-023-01112-6.pdf"
    },
    {
        "title": "Evaluating formal model veriﬁcation tools in an industrial context: the case of a smart device life cycle management system",
        "submission-date": "2023/05",
        "publication-date": "2024/08",
        "abstract": "The formal veriﬁcation of the properties of semi-formal models can make it easier to ensure their security and safety. However,\nthis task is generally cumbersome for non-specialists in formal veriﬁcation, particularly in an industrial context. This paper\nintroduces an evaluation of four formal veriﬁcation tools on an industrial case, called a Life Cycle Management System\n(LCMS). This LCMS makes it possible to deploy Product-Service Systems (PSSs) to customers using Systems-on-Chip\n(SoC). A PSS is a business model in which products and services are tightly connected and whose objective is to optimize the\nuse of products, with a positive environmental impact. A SoC can embed hardware security; however, a LCMS must be secure\nfrom end to end, which requires a veriﬁcation not only of the used protocol (in this case, a blockchain-based protocol), but also\nof the whole architecture. For that purpose, semi-formal UML models of a LCMS were ﬁrst speciﬁed and designed with their\nassociated properties, then improved in order to be formally veriﬁable. Despite being more complex, they remain capable of\nbeing processed by dedicated tools. In this paper, Verifpal and ProVerif, two formal cryptographic protocol veriﬁers, are used\nand evaluated for the cryptographic protocol and AnimUML (developed by one of the authors) and HugoRT, two veriﬁcation\ntools for behavior and UML for the architectural model are evaluated. These tools are assessed and compared according to\ntheir coverage of properties and state spaces, limitations, and usability for non-specialists. Some limitations of the approach\nitself are also provided.",
        "keywords": [
            "UML veriﬁcation",
            "Cryptographic protocols",
            "Formal veriﬁcation tools",
            "Formally veriﬁable models",
            "Life cycle\nmanagement systems",
            "Smart devices"
        ],
        "authors": [
            "Maxime Méré",
            "Frédéric Jouault",
            "Loïc Pallardy",
            "Richard Perdriau"
        ],
        "file_path": "data/sosym-all/s10270-024-01201-0.pdf"
    },
    {
        "title": "Model clone detection for rule-based model transformation languages",
        "submission-date": "2016/11",
        "publication-date": "2017/10",
        "abstract": "Cloning is a convenient mechanism to enable reuse across and within software artifacts. On the downside, it is also a practice related to severe long-term maintainability impediments, thus generating a need to identify clones in affected artifacts. A large variety of clone detection techniques have been proposed for programming and modeling languages; yet no specific ones have emerged for model transformation languages. In this paper, we explore clone detection for rule-based model transformation languages, including graph-based ones, such as Henshin, and hybrid ones, such as ATL. We introduce use cases for such techniques in the context of constructive and analytical quality assurance, and a set of key requirements we derived from these use cases. To address these requirements, we describe our customization of existing model clone detection techniques: We consider eScan, an a-priori-based technique, ConQAT, a heuristic technique, and a hybrid technique based on a combination of eScan and ConQAT. We compare these techniques in a comprehensive experimental evaluation, based on three realistic Henshin rule sets, and a comprehensive body of examples from the ATL transformation zoo. Our results indicate that our customization of ConQAT enables the efficient detection of the considered clones, without sacrificing accuracy. With our contributions, we present the first evidence on the usefulness of model clone detection for the quality assurance of model transformations and pave the way for future research efforts at the intersection of model clone detection and model transformation.",
        "keywords": [
            "Quality assurance",
            "Model clone detection",
            "Model transformation",
            "ATL",
            "Henshin"
        ],
        "authors": [
            "Daniel Strüber",
            "Vlad Acretoaie",
            "Jennifer Plöger"
        ],
        "file_path": "data/sosym-all/s10270-017-0625-6.pdf"
    },
    {
        "title": "Refactoring object constraint language speciﬁcations",
        "submission-date": "2005/02",
        "publication-date": "2006/07",
        "abstract": "The object constraint language (OCL) plays an important role in the elaboration of precise models. Although OCL was designed to be both formal and simple, OCL speciﬁcations may be difﬁcult to understand and evolve, particularly those containing complex or duplicated expressions. In this paper, we discuss how refactoringtechniques canbeappliedinorder toimprove the understandability and maintainability of OCL spec-iﬁcations. In particular, we present several potentially bad constructions often found in OCL speciﬁcations and a collection of refactorings that can be applied to replace such constructions by better ones. We also brieﬂy dis-cuss how refactorings can be automated and how model regression testing can be used to increase our conﬁdence that the semantics of an OCL speciﬁcation has been pre-served after manually performed refactorings.",
        "keywords": [],
        "authors": [
            "Alexandre Correa",
            "Cláudia Werner"
        ],
        "file_path": "data/sosym-all/s10270-006-0023-y.pdf"
    },
    {
        "title": "Lessons learned from developing mbeddr: a case study in language engineering with MPS",
        "submission-date": "2016/07",
        "publication-date": "2017/01",
        "abstract": "Language workbenches are touted as a promising technology to engineer languages for use in a wide range of domains, from programming to science to business. However, not many real-world case studies exist that evaluate the suitability of language workbench technology for this task. This paper contains such a case study. In particular, we evaluate the development of mbeddr, a collection of integrated languages and language extensions built with the Jetbrains MPS language workbench. mbeddr consists of 81 languages, with their IDE support, 34 of them C extensions. The mbeddr languages use a wide variety of notations—textual, tabular, symbolic and graphical—and the C extensions are modular; new extensions can be added without changing the existing implementation of C. mbeddr’s development has spanned 10 person-years sofar, andthetool is usedinpracticeandcontinues to be developed. This makes mbeddr a meaningful case study of non-trivial size and complexity. The evaluation is centered around ﬁve research questions: language modularity, notational freedom and projectional editing, mechanisms for managing complexity, performance and scalability issues and the consequences for the development process. We draw generally positive conclusions; language engineering with MPS is ready for real-world use. However, we also identify a number of areas for improvement in the state of the art in language engineering in general, and in MPS in particular.",
        "keywords": [
            "Language engineering",
            "Language extension",
            "Language workbenches",
            "Domain-speciﬁc language",
            "Case study",
            "Languages",
            "Experimentation"
        ],
        "authors": [
            "Markus Voelter",
            "Bernd Kolb",
            "Tamás Szabó",
            "Daniel Ratiu",
            "Arie van Deursen"
        ],
        "file_path": "data/sosym-all/s10270-016-0575-4.pdf"
    },
    {
        "title": "Matching events and activities by integrating behavioral aspects and label analysis",
        "submission-date": "2015/10",
        "publication-date": "2017/05",
        "abstract": "Nowadays, business processes are increasingly supported by IT services that produce massive amounts of event data during the execution of a process. These event data can be used to analyze the process using process mining techniques to discover the real process, measure conformance to a given process model, or to enhance existing models with performance information. Mapping the produced events to activities of a given process model is essential for conformance checking, annotation and understanding of process mining results. In order to accomplish this mapping with low manual effort, we developed a semi-automatic approach that maps events to activities using insights from behavioral analysis and label analysis. The approach extracts Declare constraints from both the log and the model to build matching constraints to efficiently reduce the number of possible mappings. These mappings are further reduced using techniques from natural language processing, which allow for a matching based on labels and external knowledge sources. The evaluation with synthetic and real-life data demonstrates the effectiveness of the approach and its robustness toward non-conforming execution logs.",
        "keywords": [
            "Process mining",
            "Event mapping",
            "Business process intelligence",
            "Constraint satisfaction",
            "Declare",
            "Natural language processing"
        ],
        "authors": [
            "Thomas Baier",
            "Claudio Di Ciccio",
            "Jan Mendling",
            "Mathias Weske"
        ],
        "file_path": "data/sosym-all/s10270-017-0603-z.pdf"
    },
    {
        "title": "Information science and the logic of models",
        "submission-date": "2009/03",
        "publication-date": "2009/05",
        "abstract": "Various ways exist in which a branch of science may be understood... In fact, the reality underlying the application of the systems of information science can only be approached at any point in time via models, which means that in respect of adequacy and intent, the modes of the statements made by information science are in most cases a qualiﬁed network of judgements about models.",
        "keywords": [],
        "authors": [
            "Bernd Mahr"
        ],
        "file_path": "data/sosym-all/s10270-009-0119-2.pdf"
    },
    {
        "title": "From network trafﬁc data to business activities: a conceptualization and a recognition approach",
        "submission-date": "2022/03",
        "publication-date": "2023/11",
        "abstract": "Event logs are the main source for business process mining techniques. However, readily available logs are produced only by part of the existing systems, which may not always be part of an investigated environment. Furthermore, logs that are created by a given information system may reﬂect only parts of the full process, while other parts may span additional systems. We suggest that data generated by communication network trafﬁc that is associated with business processes can ﬁll this gap, both in availability and in span. However, trafﬁc data are technically oriented and noisy, and there is a huge conceptual gap between these data and business meaningful event logs. Considering the above, we set the following aims. First, to assess whether the gap between technical-level trafﬁc data and conceptual-level business activities can be bridged. Once this is established, to automatically recognize business activities within network trafﬁc data, considering that these data hold interleaving activities that are performed in parallel. To address the first aim, we develop a conceptual model of trafﬁc behavior that corresponds to a business activity. We use simulated trafﬁc data annotated by the originating activity and perform an iterative process of abstracting and ﬁltering the data, along with the application of process discovery. As a result, we obtained distinct process models for speciﬁc activity types and a generic higher-level model of trafﬁc behavior in a business activity. To address the second aim, relying on the insights gained from the conceptual models, we propose a method utilizing sequence learning to identify activity types, and their boundaries (start and end) within network trafﬁc data. Evaluation shows that the proposed approach has a high precision and recall in classifying packets by activities, even while these activities are performed in parallel to each other and their data are interleaved.",
        "keywords": [
            "Activity recognition",
            "Event abstraction",
            "Sequence models",
            "Process mining",
            "Interleaved data",
            "Network trafﬁc"
        ],
        "authors": [
            "Moshe Hadad",
            "Gal Engelberg",
            "Pnina Soffer"
        ],
        "file_path": "data/sosym-all/s10270-023-01135-z.pdf"
    },
    {
        "title": "Guest editorial for the special section on MODELS 2016",
        "submission-date": "2019/04",
        "publication-date": "2019/05",
        "abstract": "The MODELS conference series continues to be the premier venue for model-based software and systems engineering covering all aspects of modeling, from theory to languages and methods to tools and applications. This special section presents the ﬁve articles that resulted from an invitation to authors of the best papers at MODELS 2016, each having undergone a full SoSyM review cycle and substantial revision.",
        "keywords": [],
        "authors": [
            "Jörg Kienzle",
            "Alexander Pretschner"
        ],
        "file_path": "data/sosym-all/s10270-019-00733-0.pdf"
    },
    {
        "title": "Promoting traits into model-driven development",
        "submission-date": "2015/02",
        "publication-date": "2015/11",
        "abstract": "Traits, as sets of behaviors, can provide a good mechanism for reusability. However, they are limited in important ways and are not present in widely used programming and modeling languages and hence are not readily available for use by mainstream developers. In this paper, we add UML associations and other modeling concepts to traits and apply them to Java and C++ through model-driven development. We also extend traits with required interfaces so dependencies at the semantics level become part of their usage, rather than simple syntactic capture. All this is accomplished in Umple, a textual modeling language based upon UML that allows adding programming constructs to the model. We applied the work to two case studies. The results show that we can promote traits to the modeling level along with the improvement in ﬂexibility and reusability.",
        "keywords": [
            "Reusability",
            "Traits",
            "Modeling",
            "Umple",
            "UML"
        ],
        "authors": [
            "Vahdat Abdelzad",
            "Timothy C. Lethbridge"
        ],
        "file_path": "data/sosym-all/s10270-015-0505-x.pdf"
    },
    {
        "title": "Student experience with software modeling tools",
        "submission-date": "2018/01",
        "publication-date": "2019/01",
        "abstract": "Modeling is a key concept in software engineering education, since students need to learn it in order to be able to produce large-\nscale and reliable software. Quality tools are needed to support modeling in education, but existing tools vary considerably\nboth in their features and in their strengths and weaknesses. The objective of the research presented in this paper was to\nhelp professors and students choose tools by determining which strengths and weaknesses matter most to students, which\ntools exhibit which of these strengths and weaknesses, and how difﬁcult to use are various tools for students. To achieve this\nobjective, we conducted a survey of the use of modeling tools among students in software engineering courses from Brazil,\nCanada, USA, Spain, Denmark, UK and China. We report the results regarding the 31 UML tools that 117 participants have\nused, focusing on the nine tools that the students have used most heavily. Common beneﬁts quoted by students in choosing\na tool include simplicity of installing and learning, being free, supporting the most important notations and providing code\ngeneration. The most cited complaints about tools include lack of feedback, being slow to use, difﬁculty drawing the diagrams,\nnot interacting well with other tools and being complex to use. This research also compares the results with the ﬁndings of\nanother survey conducted among professors who taught modeling. The results should beneﬁt tool developers by suggesting\nways they could improve their tools. The results should also help inform the selection of tools by educators and students.",
        "keywords": [
            "Software modeling tools",
            "Software engineering education",
            "Survey"
        ],
        "authors": [
            "Luciane T. W. Agner",
            "Timothy C. Lethbridge",
            "Inali W. Soares"
        ],
        "file_path": "data/sosym-all/s10270-018-00709-6.pdf"
    },
    {
        "title": "Translation of ATL to AGT and application to a code generator for Simulink",
        "submission-date": "2016/06",
        "publication-date": "2017/07",
        "abstract": "Analysing and reasoning on model transformations has become very relevant for various applications such as ensuring the correctness of transformations. ATL is a model transformation language with rich semantics and a focus on usability, making its analysis not straightforward. Conversely, algebraic graph transformation (AGT) is an approach with strong theoretical foundations allowing for formal analyses that would be valuable in the context of ATL. In this paper, we propose a translation of ATL to the AGT framework in the objective of bringing theoretical analyses of AGT to ATL transformations. We show that this transformation supports a sufﬁcient subset of ATL to be used on an industrial application example: QGen, a qualiﬁable Simulink® to source code generator developed at AdaCore. In addition to this example, we validate our proposal by translating a set of feature-rich ATL transformations to the Henshin AGT framework. We execute the ATL and AGT versions on the same set of models and verify that the result is the same.",
        "keywords": [
            "ATL",
            "Henshin",
            "Algebraicgraphtransformation",
            "OCL",
            "Nested graph conditions",
            "Analysis of model\ntransformations"
        ],
        "authors": [
            "Elie Richa",
            "Etienne Borde",
            "Laurent Pautet"
        ],
        "file_path": "data/sosym-all/s10270-017-0607-8.pdf"
    },
    {
        "title": "Real-time collaborative multi-level modeling by conflict-free replicated data types",
        "submission-date": "2022/04",
        "publication-date": "2022/11",
        "abstract": "The need for real-time collaborative solutions in model-driven engineering has been increasing over the past years. Conflict-free replicated data types (CRDT) provide scalable and robust replication mechanisms that align well with the requirements of real-time collaborative environments. In this paper, we propose a real-time collaborative multi-level modeling framework to support advanced modeling scenarios, built on a collection of custom CRDT, specifically tailored for the needs of modeling environments. We demonstrate the benefits of the framework through an illustrative modeling case and compare it with other state-of-the-art modeling frameworks.",
        "keywords": [
            "Collaborative modeling",
            "Real-time collaboration",
            "Multi-level modeling",
            "Conflict-free replicated data types",
            "Model-driven engineering"
        ],
        "authors": [
            "Istvan David\nEugene Syriani"
        ],
        "file_path": "data/sosym-all/s10270-022-01054-5.pdf"
    },
    {
        "title": "A model-driven approach to machine learning and software modeling for the IoT",
        "submission-date": "2021/03",
        "publication-date": "2022/01",
        "abstract": "Models are used in both Software Engineering (SE) and Artiﬁcial Intelligence (AI). SE models may specify the architecture at different levels of abstraction and for addressing different concerns at various stages of the software development life-cycle, from early conceptualization and design, to veriﬁcation, implementation, testing and evolution. However, AI models may provide smart capabilities, such as prediction and decision-making support. For instance, in Machine Learning (ML), which is currently the most popular sub-discipline of AI, mathematical models may learn useful patterns in the observed data and can become capable of making predictions. The goal of this work is to create synergy by bringing models in the said communities together and proposing a holistic approach to model-driven software development for intelligent systems that require ML. We illustrate how software models can become capable of creating and dealing with ML models in a seamless manner. The main focus is on the domain of the Internet of Things (IoT), where both ML and model-driven SE play a key role. In the context of the need to take a Cyber-Physical System-of-Systems perspective of the targeted architecture, an integrated design environment for both SE and ML sub-systems would best support the optimization and overall efﬁciency of the implementation of the resulting system. In particular, we implement the proposed approach, called ML-Quadrat, based on ThingML, and validate it using a case study from the IoT domain, as well as through an empirical user evaluation. It transpires that the proposed approach is not only feasible, but may also contribute to the performance leap of software development for smart Cyber-Physical Systems (CPS) which are connected to the IoT, as well as an enhanced user experience of the practitioners who use the proposed modeling solution.",
        "keywords": [
            "Model-driven software engineering",
            "Domain-speciﬁc modeling",
            "Analytics modeling",
            "Machine learning",
            "Internet of things",
            "Cyber-physical systems"
        ],
        "authors": [
            "Armin Moin",
            "Moharram Challenger",
            "Atta Badii",
            "Stephan Günnemann"
        ],
        "file_path": "data/sosym-all/s10270-021-00967-x.pdf"
    },
    {
        "title": "Online adaptation for autonomous unmanned systems driven by requirements satisfaction model",
        "submission-date": "2021/03",
        "publication-date": "2022/02",
        "abstract": "Autonomous unmanned systems (AUSs) emerge to replace human operators for achieving better safety, efﬁciency, and effectiveness in harsh and difﬁcult missions. They usually run in a highly open and dynamic operating environment, in which some unexpected situations may occur, leading to violations of predeﬁned requirements. In order to maintain stable performance, the AUS control software needs to predict in advance whether the requirements will be violated and then make adaptations tomaximizerequirements satisfaction. Wepropose Captain, amodel-drivenandcontrol-basedonlineadaptation approach, for the AUS control software. At the modeling phase, apart from the system behavior model and the operating environment model, we construct a requirements satisfaction model. At runtime, based on the requirements satisfaction model, Captain ﬁrst predicts whether the requirements will be violated in the upcoming situation; then identiﬁes the unsatisﬁable requirements that need to be accommodated; and ﬁnally, ﬁnds an optimal adaptation for the upcoming situation. We evaluate Captain in both simulated scenarios and the real world. For the former, we use two cases of UAV Delivery and UUV Ocean Surveillance, whose results demonstrate the Captain’s robustness, scalability, and real-time performance. For the latter, we have successfully implemented Captain in the DJI Matrice 100 UAV with real-world workloads.",
        "keywords": [
            "Requirements satisfaction model",
            "Runtime adaptation",
            "Models@runtime",
            "Autonomous unmanned systems"
        ],
        "authors": [
            "Yixing Luo",
            "Yuan Zhou",
            "Haiyan Zhao",
            "Zhi Jin",
            "Tianwei Zhang",
            "Yang Liu",
            "Danny Barthaud",
            "Yijun Yu"
        ],
        "file_path": "data/sosym-all/s10270-022-00981-7.pdf"
    },
    {
        "title": "Using empirical studies to mitigate symbol overload in iStar extensions",
        "submission-date": "2018/10",
        "publication-date": "2019/12",
        "abstract": "Modelling languages are frequently extended to include new constructs to be used together with the original syntax. New \nconstructs may be proposed by adding textual information, such as UML stereotypes, or by creating new graphical represen-\ntations. Thus, these new symbols need to be expressive and proposed in a careful way to increase the extension’s adoption. \nA method to create symbols for the original constructs of a modelling language was proposed and has been used to create \nthe symbols when a new modelling language is designed. We argue this method can be used to recommend new symbols for \nthe extension’s constructs. However, it is necessary to make some adjustments since the new symbols will be used with the \nexisting constructs of the modelling language original syntax. In this paper, we analyse the usage of this adapted method to \npropose symbols to mitigate the occurrence of overloaded symbols in the existing iStar extensions. We analysed the existing \niStar extensions in an SLR and identified the occurrence of symbol overload among the existing constructs. We identified a \nset of fifteen overloaded symbols in existing iStar extensions. We used these concepts with symbol overload in a multi-stage \nexperiment that involved users in the visual notation design process. The study involved 262 participants, and its results \nrevealed that most of the new graphical representations were better than those proposed by the extensions, with regard to \nsemantic transparency. Thus, the new representations can be used to mitigate this kind of conflict in iStar extensions. Our \nresults suggest that next extension efforts should consider user-generated notation design techniques in order to increase the \nsemantic transparency.",
        "keywords": [
            "Model-based engineering",
            "Semiotic clarity principle",
            "Symbol overload",
            "Experiment",
            "Modelling language \nextensions",
            "iStar"
        ],
        "authors": [
            "Enyo Gonçalves",
            "Camilo Almendra",
            "Miguel Goulão",
            "João Araújo",
            "Jaelson Castro"
        ],
        "file_path": "data/sosym-all/s10270-019-00770-9.pdf"
    },
    {
        "title": "Supporting method engineering with a low-code approach: the LOMET tool",
        "submission-date": "2023/11",
        "publication-date": "2024/09",
        "abstract": "Method engineering emerged in the 1990s as a discipline aiming to design, construct, and adapt methods, techniques, and tools for the development of information systems. By executing a method step by step, users can follow a well-deﬁned process to achieve the intended results for which the method was created. To create methods in a more guided and systematic manner, a framework of methods can serve as a template. This allows individuals to leverage the expertise of method engineers who have consolidated their best practices within these frameworks. However, the creation and adoption of a method can be challenging in the absence of tools to support these activities. Additionally, method engineers may lack the programming skills required to implement such tools. In this context, we extend an approach inspired by the low-code paradigm for method engineering. By integrating construction rules for guidance (called here protocols), the goal of this approach is to assist method engineers in creating new methods or adapting existing frameworks. It automatically provides tool support, enabling method experts to effectively execute the method. This paper builds upon previous work and presents the approach through a proof-of-concept implementation, LOMET. We present a second version of LOMET, which has been reﬁned based on feedback received during an empirical evaluation conducted through semi-structured interviews.",
        "keywords": [
            "Method engineering",
            "Low-code",
            "Framework of methods",
            "Method execution"
        ],
        "authors": [
            "Raquel Araújo de Oliveira",
            "Mario Cortes-Cornax",
            "Agnès Front"
        ],
        "file_path": "data/sosym-all/s10270-024-01203-y.pdf"
    },
    {
        "title": "Theme section on models and evolution",
        "submission-date": "2025/02",
        "publication-date": "2025/04",
        "abstract": "This theme section falls in the context of the Models and Evolution international workshop. It addresses novel theories, techniques, and tools to support evolution through and in the context of modeling. The section presents papers on topics such as Rubus Component Model evolution, process model repair, legacy system modernization, and adaptive caching for operation-based versioning of models.",
        "keywords": [],
        "authors": [
            "Dalila Tamzalit",
            "Ludovico Iovino"
        ],
        "file_path": "data/sosym-all/s10270-025-01284-3.pdf"
    },
    {
        "title": "The 2013 “State of the Journal” report",
        "submission-date": "2014/01",
        "publication-date": "2014/01",
        "abstract": "SoSyM continues to experience an increasing number of submissions. In 2013, 245 manuscripts were submitted, of which 67% were reviewed for regular issues, while the other 33% were submitted for special or theme sections or industry voice. The average number of days from initial submission to a final decision (that is, a final accept or reject) was 213 days. It should be noted that this time includes the time authors spend on making major and subsequent minor revisions. The number of months to online publication of an accepted paper is between three to four weeks. We will continue to work on improving the review turnaround time.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Geri Georg",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-014-0396-2.pdf"
    },
    {
        "title": "A component-based end-to-end simulation of the Linux ﬁle system",
        "submission-date": "2011/06",
        "publication-date": "2012/06",
        "abstract": "The Linux ﬁle system is designed with components utilizing a layered architecture. The upper components hide details of the lower components, and each layer presents uniﬁed and simple interfaces to the layers above and below. This design helps Linux to be ﬂexible as well as to provide support for multiple types of storage devices. In this paper, this component architecture is used to develop a realistic simulation without having to model lower level details of the hardware layer or particular storage devices. A detailed simulation-based performance model of the Linux ext3 ﬁle system has been developed using Colored Petri Nets. The extensive validation study using the model obtains results that are close to the expected behavior of the real ﬁle system. The model demonstrates that ﬁle system parameters have a significant impact on the I/O performance.",
        "keywords": [
            "Colored Petri Net",
            "File system modeling",
            "File system simulation",
            "Petri Net",
            "Linux ﬁle system",
            "L2 cache model"
        ],
        "authors": [
            "Hai Nguyen",
            "Amy Apon"
        ],
        "file_path": "data/sosym-all/s10270-012-0253-0.pdf"
    },
    {
        "title": "Change-driven model transformations",
        "submission-date": "2010/03",
        "publication-date": "2011/03",
        "abstract": "In this paper, we investigate change-driven model transformations, a novel class of transformations, which are directly triggered by complex model changes carried out by arbitrary transactions on the model (e.g. editing opera-tion, transformation, etc). After a classiﬁcation of relevant change scenarios, we identify challenges for change-driven transformations. As the main technical contribution of the current paper, we deﬁne an expressive, high-level language for specifying change-driven transformations as an exten-sion of graph patterns and graph transformation rules. This language generalizes previous results on live model trans-formations by offering trigger events for arbitrarily complex model changes, and dedicated reactions for speciﬁc kinds of changes, making this way the concept of change to be a ﬁrst-class citizen of the transformation language. We dis-cuss how the underlying transformation engine needs to be adapted in order to use the same language uniformly for dif-ferent change scenarios. The technicalities of our approach will be discussed on a (1) model synchronization case study with non-materialized target models and (2) a case study on detecting the violation of evolutionary (temporal) constraints in the security requirements engineering domain.",
        "keywords": [
            "Incremental model transformation",
            "Change models",
            "Change-driven transformations"
        ],
        "authors": [
            "Gábor Bergmann",
            "István Ráth",
            "Gergely Varró",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-011-0197-9.pdf"
    },
    {
        "title": "Synchronization of abstract and concrete syntax in domain-speciﬁc modeling languages",
        "submission-date": "2009/01",
        "publication-date": "2009/08",
        "abstract": "Moderndomain-speciﬁcmodeling(DSM)frame-works provide reﬁned techniques for developing new languages based on the clear separation of conceptual elements of the language (called abstract syntax) and their graphical visual representation (called concrete syntax). This separation is usually achieved by recording traceability informa-tion between the abstract and concrete syntax using mapping models. However, state-of-the-art DSM frameworks impose severe restrictions on traceability links between elements of the abstract syntax and the concrete syntax. In the cur-rent paper, we propose a mapping model which allows to deﬁne arbitrarily complex mappings between elements of the abstract and concrete syntax. Moreover, we demonstrate how live model transformations can complement mapping mod-els in providing bidirectional synchronization and implicit traceability between models of the abstract and the concrete syntax. In addition, we introduce a novel architecture for DSM environments which enables these concepts, and pro-vide an overview of the tool support.",
        "keywords": [
            "Domain-speciﬁc modeling languages",
            "Model synchronization",
            "Live model transformations",
            "Traceability"
        ],
        "authors": [
            "István Ráth",
            "András Ökrös",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-009-0122-7.pdf"
    },
    {
        "title": "Cost-sensitive precomputation of real-time-aware reconﬁguration strategies based on stochastic priced timed games",
        "submission-date": "2023/05",
        "publication-date": "2024/08",
        "abstract": "In many recent application domains, software systems must repeatedly reconﬁgure themselves at runtime to satisfy changing contextual requirements. To decide which next conﬁguration is presumably best suited is a very challenging task as it involves not only functional requirements but also non-functional properties (NFP). NFP include multiple, potentially contradicting, criteria like real-time constraints and cost measures like energy consumption. Effectiveness of context-aware reconﬁguration decisions further depends on mostly uncertain future contexts which makes greedy one-step decision heuristics potentially misleading. Moreover, the computational runtime overhead for reconﬁguration planning should not nullify the beneﬁts. Nevertheless, entirely pre-planning reconﬁguration decisions during design time is also not feasible due to missing knowledge about runtime contexts. In this article, we propose a model-based technique for precomputing context-aware reconﬁguration decisions under partially uncertain real-time constraints and cost measures. We employ a game-theoretic approach based on stochastic priced timed game automata as reconﬁguration model. This formal model allows us to automatically synthesize winning strategies for the ﬁrst player (the system) which efﬁciently delivers presumably best-ﬁtting reconﬁguration decisions as reactions to moves of the second player (the context) at runtime. Our tool implementation copes with the high computational complexity of strategy synthesis by utilizing the statistical model checker Uppaal Stratego to approximate near-optimal solutions. We applied our tool to a real-world example consisting of a reconﬁgurable robot support system for the construction of aircraft fuselages. Our evaluation results show that Uppaal Stratego is indeed able to precompute effective reconﬁguration strategies within a reasonable amount of time.",
        "keywords": [
            "Stochastic priced timed games",
            "Proactive self-adaptation",
            "Strategy synthesis",
            "Statistical model checking"
        ],
        "authors": [
            "Hendrik Göttmann",
            "Birte Caesar",
            "Lasse Beers",
            "Malte Lochau",
            "Andy Schürr",
            "Alexander Fay"
        ],
        "file_path": "data/sosym-all/s10270-024-01195-9.pdf"
    },
    {
        "title": "Runtime translation of OCL-like statements on Simulink models: Expanding domains and optimising queries",
        "submission-date": "2020/05",
        "publication-date": "2021/08",
        "abstract": "Open-source model management frameworks such as OCL and ATL tend to focus on manipulating models built atop the Eclipse Modelling Framework (EMF), a de facto standard for domain speciﬁc modelling. MATLAB Simulink is a widely used proprietary modelling framework for dynamic systems that is built atop an entirely different technical stack to EMF. To leverage the facilities of open-source model management frameworks with Simulink models, these can be transformed into an EMF-compatible representation. Downsides of this approach include the synchronisation of the native Simulink model and its EMF representation as they evolve; the completeness of the EMF representation, and the transformation cost which can be crippling for large Simulink models. We propose an alternative approach to bridge Simulink models with open-source model management frameworks that uses an “on-the-ﬂy” translation of model management constructs into MATLAB statements. Our approach does not require an EMF representation and can mitigate the cost of the upfront transformation on large models. To evaluate both approaches we measure the performance of a model validation process with Epsilon (a model management framework) on a sample of large Simulink models available on GitHub. Our previous results suggest that, with our approach, the total validation time can be reduced by up to 80%. In this paper, we expand our approach to support the management of Simulink requirements and dictionaries, and we improve the approach to perform queries on collections of model elements more efﬁciently. We demonstrate the use of the Simulink requirements and dictionaries with a case study and we evaluate the optimisations on collection queries with an experiment that compares the performance of a set of queries on models with different sizes. Our results suggest an improvement by up to 99% on some queries.",
        "keywords": [
            "Model driven engineering",
            "Interoperability",
            "Epsilon",
            "MATLAB Simulink",
            "Query optimisation",
            "Eclipse Modelling Framework"
        ],
        "authors": [
            "Beatriz A. Sanchez",
            "Athanasios Zolotas",
            "Horacio Hoyos Rodriguez",
            "Dimitris Kolovos",
            "Richard F. Paige",
            "Justin C. Cooper",
            "Jason Hampson"
        ],
        "file_path": "data/sosym-all/s10270-021-00910-0.pdf"
    },
    {
        "title": "Realizing Model Transformation Chain interoperability",
        "submission-date": "2009/11",
        "publication-date": "2010/10",
        "abstract": "A single Model Transformation Chain (MTC) takes a high-level input model rooted in the problem domain and through one or more transformation steps produces a low-level output model rooted in the solution domain. To build a single “almighty” MTC that is in charge of every design, implementation and speciﬁc platform concern is a complex task. Instead, we can use several smaller MTCs that are easier to develop and maintain, because each MTC is independently developed focusing on a speciﬁc concern. However, the MTCs must interoperate to produce complete applications; this inherently creates dependencies between them, because each MTC generates a part of the ﬁnal low-level model. In this paper, we propose an external and explicit mechanism to track dependencies between the MTCs (i.e., the MTCs are oblivious to the mechanism), which is used to automatically derive correspondence relationships between the ﬁnal models generated by each MTC. The contribution of our mechanism is the reduction of complexity of building interoperable MTCs because the derived correspondences are resolved after the transformations execution, in the solu-tion domain where the semantics of every concept is well-deﬁned. The resolution process consists of (1) checking the consistency between the models, (2) producing communica-tion bridges or (3) guiding the composition of the models. This paper presents three case studies to illustrate the deriva-tion and resolution of correspondence relationships through the MTCs.",
        "keywords": [
            "Software Engineering",
            "Model-driven Engineering",
            "Model Transformation Chains",
            "Interoperability"
        ],
        "authors": [
            "Andrés Yie",
            "Rubby Casallas",
            "Dirk Deridder",
            "Dennis Wagelaar"
        ],
        "file_path": "data/sosym-all/s10270-010-0179-3.pdf"
    },
    {
        "title": "Reference architectures modelling and compliance checking",
        "submission-date": "2021/08",
        "publication-date": "2022/08",
        "abstract": "Reference architectures (RAs) are successfully used to represent families of concrete software architectures in several domains such as automotive, banking, and the Internet of Things. RAs inspire architects when designing concrete architectures, and they help to guarantee compliance with architectural decisions, regulatory requirements, as well as architectural qualities. Despite their importance, reference architectures still suffer from a number of open technical issues, including (i) the lack of a common interpretation, a precise notation for their representation and documentation, and (ii) the lack of conformance mechanisms for checking the compliance of concrete architectures to their related reference architecture, architectural decisions, regulatory requirements, etc. This paper addresses these two issues by introducing a model-driven approach that leverages (i) a domain-independent metamodel for the representation of reference architectures and (ii) the combination of model transformation and weaving techniques for the automatic conformance checking of concrete architectures. We evaluate the applicability, effectiveness, and generalizability of our approach using illustrative examples from the web browsers and automotive domains, including an assessment from an independent practitioner.",
        "keywords": [
            "Model-driven Engineering",
            "Software architecture",
            "Reference architecture"
        ],
        "authors": [
            "Alessio Bucaioni",
            "Amleto Di Salle",
            "Ludovico Iovino",
            "Ivano Malavolta",
            "Patrizio Pelliccione"
        ],
        "file_path": "data/sosym-all/s10270-022-01022-z.pdf"
    },
    {
        "title": "Analysing the Linux kernel feature model changes using FMDiff",
        "submission-date": "2014/10",
        "publication-date": "2015/05",
        "abstract": "Evolving a large scale, highly variable system is a challenging task. For such a system, evolution operations often require to update consistently both their implementation and its feature model. In this context, the evolution of the feature model closely follows the evolution of the system. The purpose of this work is to show that ﬁne-grained feature changes can be used to guide the evolution of the highly variable system. In this paper, we present an approach to obtain ﬁne-grained feature model changes with its supporting tool “FMDiff”. Our approach is tailored for Kconﬁg-based variability models and proposes a feature change classiﬁcation detailing changes in features, their attributes and attribute values. We apply our approach to the Linux kernel feature model, extracting feature changes occurring in sixteen ofﬁcial releases. In contrast to previous studies, we found that feature modiﬁcations are responsible for most of the changes. Then, by taking advantage of the multi-platform aspect of the Linux kernel, we observe the effects of a feature change across the different architecture-speciﬁc feature models of the kernel. We found that between 10 and 50% of feature changes impact all the architecture-speciﬁc feature models, offering a new perspective on studies of the evolution of the Linux feature model and development practices of its developers.",
        "keywords": [
            "Software product line",
            "Feature model",
            "Evolution"
        ],
        "authors": [
            "Nicolas Dintzner",
            "Arie van Deursen",
            "Martin Pinzger"
        ],
        "file_path": "data/sosym-all/s10270-015-0472-2.pdf"
    },
    {
        "title": "Improving the definition of general constraints in UML",
        "submission-date": "2007/03",
        "publication-date": "2008/01",
        "abstract": "An important aspect in the speciﬁcation of conceptual schemas is the definition of general constraints that cannot be expressed by the predeﬁned constructs provided by conceptual modeling languages. This is generally achieved by using general-purpose languages like OCL. In this paper we propose a new approach that facilitates the definition of such general constraints in UML. More precisely, we deﬁne a proﬁle that extends the set of predeﬁned UML constraints by adding certain types of constraints that are commonly used in conceptual schemas. We also show how our proposal facili-tates reasoning about the constraints and their automatic code generation, study the application of our ideas to the speciﬁ-cation of two real-life applications, and present a prototype tool implementation.",
        "keywords": [
            "Conceptual modeling",
            "Integrity constraints",
            "UML proﬁle"
        ],
        "authors": [
            "Dolors Costal",
            "Cristina Gómez",
            "Anna Queralt",
            "Ruth Raventós",
            "Ernest Teniente"
        ],
        "file_path": "data/sosym-all/s10270-007-0078-4.pdf"
    },
    {
        "title": "Using contexts to extract models from code",
        "submission-date": "2014/06",
        "publication-date": "2015/05",
        "abstract": "Behaviour models facilitate the understanding and analysis of software systems by providing an abstract view of their behaviours and also by enabling the use of validation and veriﬁcation techniques to detect errors. However, depending on the size and complexity of these systems, constructing models may not be a trivial task, even for experienced developers. Model extraction techniques can automatically obtain models from existing code, thus reducing the effort and expertise required of engineers and helping avoid errors often present in manually constructed models. Existing approaches for model extraction often fail to produce faithful models, either because they only consider static information, which may include infeasible behaviours, or because they are based only on dynamic information, thus relying on observed executions, which usually results in incomplete models. This paper describes a model extraction approach based on the concept of contexts, which are abstractions of concrete states of a program, combining static and dynamic information. Contexts merge some of the advantages of using either type of information and, by their combination, can overcome some of their problems. The approach is partially implemented by a tool called LTS Extractor, which translates information collected from execution traces produced by instrumented Java code to labelled transition systems (LTS), which can be analysed in an existing veriﬁcation tool. Results from case studies are presented and discussed, showing that, considering a certain level of abstraction and a set of execution traces, the produced models are correct descriptions of the programs from which they were extracted. Thus, they can be used for a variety of analyses, such as program understanding, validation, veriﬁcation, and evolution.",
        "keywords": [
            "Behaviour models",
            "Model extraction",
            "Model analysis"
        ],
        "authors": [
            "Lucio Mauro Duarte",
            "Jeff Kramer",
            "Sebastian Uchitel"
        ],
        "file_path": "data/sosym-all/s10270-015-0466-0.pdf"
    },
    {
        "title": "Proactive modeling: a new model intelligence technique",
        "submission-date": "2013/08",
        "publication-date": "2015/04",
        "abstract": "This article discusses a model intelligence tech-nique called proactive modeling. The goal of proactive modeling is to reduce the amount of manual modeling required when using a graphical DSML and to assist in step-by-step creation of a model. Proactive modeling accomplishes this goal by examining the metamodels syntax and constraints, automatically executing model modiﬁcations, and prompting the modeler for assistance when more than one valid model modiﬁcation exists, but none are neces-sary. We have integrated proactive modeling into the generic modeling environment (GME) as a generic add-on that can operate on any domain-speciﬁc modeling language imple-mented in GME. Lastly, results from applying proactive modeling to several DSMLs in GME show that it can reduce modeling effort.",
        "keywords": [
            "Proactive modeling",
            "Model intelligence",
            "Domain-speciﬁc modeling language",
            "Model-driven engineering"
        ],
        "authors": [
            "Tanumoy Pati",
            "Sowmya Kolli",
            "James H. Hill"
        ],
        "file_path": "data/sosym-all/s10270-015-0465-1.pdf"
    },
    {
        "title": "What makes a good modeling research contribution?",
        "submission-date": "2023/11",
        "publication-date": "2024/04",
        "abstract": "The modeling ﬁeld is rapidly evolving and expanding to address new research topics and to connect with new disciplines. As such, what constituted a good modeling research contribution ten years ago may not be the same today. We try to distill some insights of what we (and the community we aim to represent) consider today as key elements of a good research paper in the ﬁeld of software and systems modeling. Such insights—which will need to evolve and adapt with time—will be useful not just for authors of new papers, but also for reviewers and editors.",
        "keywords": [
            "Modeling",
            "Science",
            "Relevance",
            "Writing",
            "Community"
        ],
        "authors": [
            "Richard F. Paige",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-024-01177-x.pdf"
    },
    {
        "title": "From model transformation to incremental bidirectional model synchronization",
        "submission-date": "2007/03",
        "publication-date": "2008/03",
        "abstract": "The model-driven software development paradigm requires that appropriate model transformations are applicable in different stages of the development process. The transformations have to consistently propagate changes between the different involved models and thus ensure a proper model synchronization. However, most approaches today do not fully support the requirements for model synchronization and focus only on classical one-way batch-oriented transformations. In this paper, we present our approach for an incremental model transformation which supports model synchronization. Our approach employs the visual, formal, and bidirectional transformation technique of triple graph grammars. Using this declarative speciﬁcation formalism, we focus on the efﬁcient execution of the transformation rules and how to achieve an incremental model transformation for synchronization purposes. We present an evaluation of our approach and demonstrate that due to the speedup for the incremental processing in the average case even larger models can be tackled.",
        "keywords": [
            "Model transformation",
            "Incremental model synchronization",
            "Triple graph grammars"
        ],
        "authors": [
            "Holger Giese",
            "Robert Wagner"
        ],
        "file_path": "data/sosym-all/s10270-008-0089-9.pdf"
    },
    {
        "title": "Introduction to the theme issue on models for quality of software architecture",
        "submission-date": "2013/08",
        "publication-date": "2014/01",
        "abstract": "High quality of system architecture is essential for the long-term success of a software product. Software engineers need to design and implement secure, maintainable, usable, efficient, and reliable software systems. They have to design architectures that can fulfill quality requirements, judge quality tradeoffs, and ensure that the implementation adheres to the architecture. In case of long-living software systems, they have to monitor, evaluate, and improve quality continuously throughout the entire software lifecycle. Models play a key role in supporting software engineers to master these tasks. The goal of this special theme issue is to emphasize the deep relationship between system modeling and the quality of system architecture. For example, how can model-driven techniques for system development be used to assess and improve the quality of system architecture? What are the connections between software modeling tools and quality assessment tools and environments? Papers in this theme issue are a step toward bridging the gap between theory and practice of architecture quality and system modeling.",
        "keywords": [],
        "authors": [
            "Dorina C. Petriu",
            "Jens Happe"
        ],
        "file_path": "data/sosym-all/s10270-013-0373-1.pdf"
    },
    {
        "title": "Coordinating large distributed relational process structures",
        "submission-date": "2019/11",
        "publication-date": "2020/11",
        "abstract": "Representing a business process as a collaboration of interacting processes has become feasible with the emergence of\ndata-centric business process management paradigms. Usually, these interacting processes have relations and, thereby, form\na complex relational process structure. The interactions of processes within this relational process structure need to be\ncoordinated to arrive at a meaningful overall business goal. However, relational process structures may become arbitrarily\nlarge. With the use of cloud technology, they may additionally be distributed over multiple nodes, allowing for scalability.\nCoordination processes have been proposed to coordinate relational process structures, where processes may have one-to-many and\nmany-to-many relations at run-time. This paper shows how multiple coordination processes can be used in a decentralized fashion to more efﬁciently coordinate large, distributed process structures. The main challenge of using multiple coordination processes is to effectively realize the coordination responsibility of each coordination process. Key components of the solution are the subsidiary principle and the hierarchy of the relational process structure. Finally, an implementation of the coordination process concept based on microservices was developed, which allows for fast and concurrent enactment of multiple, decentralized coordination processes in large, distributed process structures.",
        "keywords": [
            "Process interactions",
            "Relational process structure",
            "Coordination process",
            "Distributed process execution",
            "BPM in the cloud"
        ],
        "authors": [
            "Sebastian Steinau",
            "Kevin Andrews",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-020-00835-0.pdf"
    },
    {
        "title": "Lifting transformational models of product lines: a case study",
        "submission-date": "2008/11",
        "publication-date": "2009/10",
        "abstract": "Model driven engineering (MDE) of software product lines (SPLs) merges two increasing important paradigms that synthesize programs by transformation. MDE creates programs by transforming models, and SPLs elaborate programs by applying transformations called features. In this paper, we present the design and implementation of a transformational model of a product line of scalar vector graphics and JavaScript applications. We explain how we simpliﬁed our implementation by lifting selected features and their compositions from our original product line (whose implementations were complex) to features and their compositions of another product line (whose speciﬁcations were simple). We used operators to map higher-level features and their compositions to their lower-level counterparts. Doing so exposed commuting relationships among feature compositions in both product lines that helped validate our model and implementation.",
        "keywords": [
            "Transformation reuse",
            "Code generation",
            "Model composition",
            "High-level transformations",
            "Features",
            "Product-lines",
            "Model driven engineering"
        ],
        "authors": [
            "Greg Freeman",
            "Don Batory",
            "Greg Lavender",
            "Jacob Neal Sarvela"
        ],
        "file_path": "data/sosym-all/s10270-009-0131-6.pdf"
    },
    {
        "title": "How low-code platforms support digital twins of processes",
        "submission-date": "2025/05",
        "publication-date": "2025/07",
        "abstract": "Digital Twin of Processes, also deﬁned as process digital twins (PDTs), are emerging as a feasible solution for modeling, monitoring, and optimizing business processes by providing real-time, data-driven insights into operational workﬂows. However, designing, developing, and maintaining PDTs can be complex and resource-intensive, often requiring highly specialized expertise in software engineering and domain-speciﬁc processes. This paper proposes insights and guidelines into using low-code development platforms (LCDPs) to simplify and expedite the modeling and deployment of PDTs, leveraging intuitive, visual development environments and pre-built components. We identiﬁed 11 core characteristics that deﬁne PDTs and assessed the potential of LCDPs to support their design, development, and execution. The applicability of this framework is demonstrated through three case studies of littering management, order management, and guest invitations, where we evaluate how well LCDPs address the key requirements of PDT implementation. Our results indicate that while LCDPs offer signiﬁcant advantages in terms of ease of adoption and cost efﬁciency, several challenges remain, particularly around scalability and process performance. At the same time, we propose lessons learned from these experiences that could help address these challenges in future implementations.",
        "keywords": [
            "Process digital twin",
            "Low-code development platform",
            "Workﬂow management system",
            "Workﬂow modeling"
        ],
        "authors": [
            "Arianna Fedeli",
            "Amleto Di Salle",
            "Daniela Micucci",
            "Luciana Rebelo",
            "Maria Teresa Rossi",
            "Leonardo Mariani",
            "Ludovico Iovino"
        ],
        "file_path": "data/sosym-all/s10270-025-01310-4.pdf"
    },
    {
        "title": "On the uniﬁcation power of models",
        "submission-date": "2003/11",
        "publication-date": "2005/05",
        "abstract": "In November 2000, the OMG made public the MDATMinitiative, a particular variant of a new global trend called MDE (Model Driven Engineering). The basic ideas of MDA are germane to many other approaches such as generative programming, domain speciﬁc languages, model-integrated computing, generic model management, software factories, etc. MDA may be deﬁned as the realization of MDE principles around a set of OMG standards like MOF, XMI, OCL, UML, CWM, SPEM, etc. MDE is presently making several promises about the potential beneﬁts that could be reaped from a move from code-centric to model-based practices. When we observe these claims, we may wonder when they may be satisﬁed: on the short, medium or long term or even never perhaps for some of them. This paper tries to propose a vision of the development of MDE based on some lessons learnt in the past 30 years in the development of object technology. The main message is that a basic principle (“Everything is an object”) was most helpful in driving the technology in the direction of simplicity, generality and power of integration. Similarly in MDE, the basic principle that “Everything is a model” has many interesting properties, among others the capacity to generate a realistic research agenda. We postulate here that two core relations (representation and conformance) are associated to this principle, as inheritance and instantia-tion were associated to the object uniﬁcation principle in the class-based languages of the 80’s. We suggest that this may be most useful in understanding many questions about MDE in general and the MDA approach in particular. We provide some illustrative examples. The personal position taken in this paper would be useful if it could generate a critical debate on the research directions in MDE.",
        "keywords": [
            "MDE",
            "MDA",
            "Models",
            "Metamodels"
        ],
        "authors": [
            "Jean B´ezivin"
        ],
        "file_path": "data/sosym-all/s10270-005-0079-0.pdf"
    },
    {
        "title": "Scalable process discovery and conformance checking",
        "submission-date": "2015/10",
        "publication-date": "2016/07",
        "abstract": "Considerable amounts of data, including process events, are collected and stored by organisations nowadays. Discovering a process model from such event data and ver-iﬁcation of the quality of discovered models are important steps in process mining. Many discovery techniques have been proposed, but none of them combines scalability with strong quality guarantees. We would like such techniques to handle billions of events or thousands of activities, to produce sound models (without deadlocks and other anomalies), and to guarantee that the underlying process can be rediscov-ered when sufﬁcient information is available. In this paper, we introduce a framework for process discovery that ensures these properties while passing over the log only once and introduce three algorithms using the framework. To measure the quality of discovered models for such large logs, we introduce a model–model and model–log comparison framework that applies a divide-and-conquer strategy to measure recall, ﬁtness, and precision. We experimentally show that these discovery and measuring techniques sacriﬁce little compared to other algorithms, while gaining the ability to cope with event logs of 100,000,000 traces and processes of 10,000 activities on a standard computer.",
        "keywords": [
            "Big data",
            "Scalable process mining",
            "Block-structured process discovery",
            "Directly-follows graphs",
            "Algorithm evaluation",
            "Rediscoverability",
            "Conformance checking"
        ],
        "authors": [
            "Sander J. J. Leemans",
            "Dirk Fahland",
            "Wil M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-016-0545-x.pdf"
    },
    {
        "title": "Linking physicality and computation",
        "submission-date": "2016/02",
        "publication-date": "2017/12",
        "abstract": "Cyber-physical systems have developed into a very active research ﬁeld, with a broad range of challenges and research directions going from requirements, to implementation and simulation, as well as validation and veriﬁcation to guarantee essential properties. In this survey paper, we focus exclusively on the following fundamental issue: how to link physicality and computation, continuous time-space dynamics with discrete untimed ones? We consider that cyber-physical system design ﬂow involves the following three main steps: (1) cyber-physical systems modeling; (2) discretization for executability; and (3) simulation and implementation. We review—and strive to provide insight into possible approaches for addressing—the key issues, for each of these three steps.",
        "keywords": [
            "Cyber-physical systems design",
            "Structural equational modeling",
            "Modelica",
            "Linear graphs",
            "Bond graphs",
            "Idealization",
            "Abstraction",
            "Hybrid dataﬂow networks",
            "Discretization",
            "Language embedding"
        ],
        "authors": [
            "Simon Bliudze",
            "Sébastien Furic",
            "Joseph Sifakis",
            "Antoine Viel"
        ],
        "file_path": "data/sosym-all/s10270-017-0642-5.pdf"
    },
    {
        "title": "MDA Tool Components: a proposal for packaging know-how in model driven development",
        "submission-date": "2006/04",
        "publication-date": "2007/05",
        "abstract": "As the Model Driven Development (MDD) and Product Line Engineering (PLE) appear as major trends for reducing software development complexity and costs, an important missing stone becomes more visible: there is no standard and reusable assets for packaging the know-how and artifacts required when applying these approaches. To overcome this limit, we introduce in this paper the notion of MDA Tool Component, i.e., a packaging unit for encapsulating business know-how and required resources in order to support speciﬁc modeling activities on a certain kind of model. The aim of this work is to provide a standard way for representing this know-how packaging unit. This is done by introducing a two-layer MOF-compliant metamodel. Whilst the ﬁrst layer focuses on the deﬁnition of the structure and contents of the MDA Tool Component, the second layer introduces a language independent way for describing its behavior. An OMG RFP (Request For Proposal) has been issued in order to standardize this approach.",
        "keywords": [
            "MDD",
            "Packaging of know-how",
            "MDA Tool Component",
            "Reusability"
        ],
        "authors": [
            "Reda Bendraou",
            "Philippe Desfray",
            "Marie-Pierre Gervais",
            "Alexis Muller"
        ],
        "file_path": "data/sosym-all/s10270-007-0058-8.pdf"
    },
    {
        "title": "AuRUS: explaining the validation of UML/OCL conceptual schemas",
        "submission-date": "2011/12",
        "publication-date": "2013/06",
        "abstract": "The validation and the veriﬁcation of concep-tual schemas have attracted a lot of interest during the last years, and several tools have been developed to automate this process as much as possible. This is achieved, in general, by assessing whether the schema satisﬁes different kinds of desirable properties which ensure that the schema is correct. In this paper we describe AuRUS, a tool we have developed to analyze UML/OCL conceptual schemas and to explain their (in)correctness. When a property is satisﬁed, AuRUS provides a sample instantiation of the schema showing a particular situation where the property holds. When it is not, AuRUS provides an explanation for such unsatisﬁability, i.e., a set of integrity constraints which is in contradiction with the property.",
        "keywords": [
            "Validation",
            "Conceptual modeling",
            "UML",
            "OCL",
            "Automated reasoning",
            "Explanation"
        ],
        "authors": [
            "Guillem Rull",
            "Carles Farré",
            "Anna Queralt",
            "Ernest Teniente",
            "Toni Urpí"
        ],
        "file_path": "data/sosym-all/s10270-013-0350-8.pdf"
    },
    {
        "title": "From process mining to augmented process execution",
        "submission-date": "2023/07",
        "publication-date": "2023/11",
        "abstract": "Business process management (BPM) is a well-established discipline comprising a set of principles, methods, techniques, and tools to continuously improve the performance of business processes. Traditionally, most BPM decisions and activities are undertaken by business stakeholders based on manual data collection and analysis techniques. This is time-consuming and potentially leads to suboptimal decisions, as only a restricted subset of data and options are considered. Over the past decades, a rich set of data-driven techniques has emerged to support and automate various activities and decisions across the BPM lifecycle, particularly within the process mining ﬁeld. More recently, the uptake of artiﬁcial intelligence (AI) methods for BPM has led to a range of approaches for proactive business process monitoring. Given their common data requirements and overlapping goals, process mining and AI-driven approaches to business process optimization are converging. This convergence is leading to a promising emerging concept, which we call (AI-)augmented process execution: a collection of data analytics and artiﬁcial intelligence methods for continuous and automated improvement and adaptation of business processes. This article gives an outline of research at the intersection between process mining and AI-driven process optimization, classiﬁes the researched techniques based on their scope and objectives, and positions augmented process execution as an additional layer on top of this stack.",
        "keywords": [
            "Business process management",
            "Predictive analytics",
            "Prescriptive analytics",
            "Autonomous systems"
        ],
        "authors": [
            "David Chapela-Campa",
            "Marlon Dumas"
        ],
        "file_path": "data/sosym-all/s10270-023-01132-2.pdf"
    },
    {
        "title": "SoSyM reﬂections of 2016: a journal status report",
        "submission-date": "2016/00",
        "publication-date": "2017/01",
        "abstract": "The past year has been an exciting time for SoSyM with the celebration of our 15th Anniversary, the addi- tion of new Editors, and further collaboration with the MODELS conference (e.g., SoSyM awards at MODELS and the SoSyM Journal-ﬁrst arrangement). The ﬁrst issue of this new year’s volume summarizes the status of SoSyM in terms of recent statistics and milestone events over the past year.",
        "keywords": [],
        "authors": [
            "Geri Georg",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-017-0582-0.pdf"
    },
    {
        "title": "An elucidation of blended modeling from an industrial perspective",
        "submission-date": "2024/11",
        "publication-date": "2024/12",
        "abstract": "Model-Driven Engineering (MDE) has been widely adopted across various industrial sectors due to its ability to manage the complexity of modern engineering products. However, traditional modeling languages and tools are often limited to a single, specific concrete syntax, which poses challenges for the diverse stakeholders involved in the modeling process.. To address these limitations, the emerging field of blended modeling introduces the use of multiple concrete syntaxes, and in some cases, even multiple abstract syntaxes, for representing the same information. In this expert perspective, we present generalized, technology-agnostic concepts developed within a European research and development project focused on blended modeling. Specifically, we contribute a standardized terminology and ontology for blended modeling, along with a methodology for creating blended modeling environments. These concepts were developed through collaboration between academic and industrial partners, who aligned on the motivations and benefits of this approach. The insights gained from this project are not only relevant to blended MDE but also can be applied to traditional MDE practices.",
        "keywords": [
            "Model-Driven Engineering",
            "Blended modeling",
            "Terminologies",
            "Ontologies",
            "Methodologies"
        ],
        "authors": [
            "Jörg Holtmann",
            "Federico Ciccozzi",
            "Wim Bast",
            "Joost van Pinxten"
        ],
        "file_path": "data/sosym-all/s10270-024-01246-1.pdf"
    },
    {
        "title": "On the interpretation of binary associations in the Uniﬁed Modelling Language",
        "submission-date": "2002/02",
        "publication-date": "2002/09",
        "abstract": "Binary associations between classiﬁers are among the most fundamental of UML concepts. However, there is considerable room for disagreement concerning what an association is, semantically; it turns out that at least two diﬀerent notions are called Associations. This confusion of concepts gives rise to unnecessary compli-cation and ambiguity in the language, which have impli-cations for the modeller because they can result in seri-ous misunderstandings of static structure diagrams; sim-ilarly, they have implications for tool developers. In this paper we explore these issues, suggest improvements and clariﬁcations, and demonstrate side-beneﬁts that would accrue.",
        "keywords": [
            "UML",
            "Association",
            "Link"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-002-0002-x.pdf"
    },
    {
        "title": "Formal semantics of static and temporal state-oriented OCL constraints",
        "submission-date": "2003/02",
        "publication-date": "2003/07",
        "abstract": "The textual Object Constraint Language (OCL) is primarily intended to specify restrictions over UML class diagrams, in particular class invariants, operation pre-, and postconditions. Based on several improvements in the deﬁnition of the language concepts in last years, a proposal for a new version of OCL has recently been published [43]. That document provides an extensive OCL semantic description that constitutes a tight integration into UML. However, OCL still lacks a semantic integration of UML Statecharts, although it can already be used to refer to states in OCL expressions. This article presents an approach that closes this gap and introduces a formal semantics for such integration through a mathematical model. It also presents the deﬁn-ition of a temporal OCL extension by means of a UML Proﬁle based on the metamodel of the latest OCL proposal. Our OCL extension enables modelers to specify behavioral state-oriented real-time constraints. It provides an intuitive understanding and readability at application level since common OCL syntax and concepts are pre-served. A well-deﬁned formal semantics is given through the mapping of temporal OCL expressions to temporal logics formulae.",
        "keywords": [
            "Object Constraint Language",
            "UML Statecharts",
            "UML Proﬁle",
            "Real-time constraints",
            "Temporal logics"
        ],
        "authors": [
            "Stephan Flake",
            "Wolfgang Mueller"
        ],
        "file_path": "data/sosym-all/s10270-003-0026-x.pdf"
    },
    {
        "title": "Model checking multi-level and recursive nets",
        "submission-date": "2015/03",
        "publication-date": "2016/01",
        "abstract": "With the increasing complexity of the problems and systems arising nowadays, the use of multi-level models is becoming more frequent in practice. However, there are still few reports in the literature concerning methods for analyzing such models without ﬂattening the multi-level structure. For instance, several variants of multi-level Petri nets have been applied for modeling interaction protocols and mobility in multi-agent systems and coordination of cross-organizational workﬂows. But there are few automated tools for analyzing the behavior of these nets. In this paper we explain how to detect faults in models based on a representative class of multi-level nets: the nested Petri nets. We translate a nested net into a veriﬁable model that preserves its modular structure, a PROMELA program. This allows the use of SPIN model checker to verify properties related to termination, boundedness and reachability.",
        "keywords": [
            "Multi-level modeling",
            "Nested Petri nets",
            "Model checking",
            "SPIN"
        ],
        "authors": [
            "Mirtha Lina Fernández Venero",
            "Flávio Soares Corrêa da Silva"
        ],
        "file_path": "data/sosym-all/s10270-015-0509-6.pdf"
    },
    {
        "title": "Bidirectional model transformations in QVT: semantic issues and open questions",
        "submission-date": "2008/04",
        "publication-date": "2008/12",
        "abstract": "We consider the OMG’s queries, views and transformations standard as applied to the speciﬁcation of bidirectional transformations between models. We discuss what is meant by bidirectional transformations, and the model-driven development scenarios in which they are needed. We analyse the fundamental requirements on tools which support such transformations, and discuss some semantic issues which arise. In particular, we show that any transformation language sufﬁcient to the needs of model-driven development would have to be able to express non-bijective transformations. We argue that a considerable amount of basic research is needed before suitable tools will be fully realisable, and suggest directions for this future research.",
        "keywords": [
            "Bidirectional model transformation",
            "QVT",
            "Model-driven development",
            "Semantics"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-008-0109-9.pdf"
    },
    {
        "title": "Feature-based classification of bidirectional transformation approaches",
        "submission-date": "2012/07",
        "publication-date": "2015/01",
        "abstract": "Bidirectional model transformation is a key technology in model-driven engineering (MDE), when two models that can change over time have to be kept constantly consistent with each other. While several model transformation tools include at least a partial support to bidirectionality, it is not clear how these bidirectional capabilities relate to each other and to similar classical problems in computer science, from the view update problem in databases to bidirectional graph transformations. This paper tries to clarify and visualize the space of design choices for bidirectional transformations from an MDE point of view, in the form of a feature model. The selected list of existing approaches are characterized by mapping them to the feature model. Then, the feature model is used to highlight some unexplored research lines in bidirectional transformations.",
        "keywords": [
            "Bidirectional transformation",
            "Feature model",
            "Domain analysis"
        ],
        "authors": [
            "Soichiro Hidaka",
            "Massimo Tisi",
            "Jordi Cabot",
            "Zhenjiang Hu"
        ],
        "file_path": "data/sosym-all/s10270-014-0450-0.pdf"
    },
    {
        "title": "Multi-perspective enterprise modeling: foundational concepts, prospects and future research challenges",
        "submission-date": "2011/11",
        "publication-date": "2012/08",
        "abstract": "The paper presents a method for multi-perspective enterprise modeling (MEMO) and a corresponding (meta-) modeling environment. An extensive analysis of requirements for enterprise modeling serves to motivate and assess the method. The method is based on an elaborate conception of multi-perspective enterprise models and on an extensible language architecture. The language architecture is comprised of a meta modeling language and an extensible set of integrated domain-speciﬁc modeling languages (DSML). The DSML are supplemented with process models and with guidelines for their reﬂective use. The corresponding modeling environment integrates editors for various DSML into multi-language model editors. It includes a meta model editor which enables the convenient use, development and extension of the set of supported DSML and supports the generation of respective graphical model edi-tors. Thus, it also serves as a foundation for method engineering. MEMO covers both software engineering as well as social, managerial and economic aspects of the ﬁrm. The presentation of MEMO is supplemented with a comparative overview of other approaches to enterprise modeling. The paper concludes bys summarizing fundamental technical, epistemological and political challenges for enterprise modeling research and discusses potential paths for future research.",
        "keywords": [
            "Enterprise modeling",
            "Domain-speciﬁc modeling language (DSML)",
            "Method engineering",
            "Modeling tool",
            "Reference model"
        ],
        "authors": [
            "Ulrich Frank"
        ],
        "file_path": "data/sosym-all/s10270-012-0273-9.pdf"
    },
    {
        "title": "Promoting social diversity for the automated learning of complex MDE artifacts",
        "submission-date": "2021/04",
        "publication-date": "2022/01",
        "abstract": "Software modeling activities typically involve a tedious and time-consuming effort by specially trained personnel. This lack of automation hampers the adoption of model-driven engineering (MDE). Nevertheless, in the recent years, much research work has been dedicated to learn executable MDE artifacts instead of writing them manually. In this context, mono- and multi-objective genetic programming (GP) has proven being an efﬁcient and reliable method to derive automation knowledge by using, as training data, a set of examples representing the expected behavior of an artifact. Generally, conformance to the training example set is the main objective to lead the learning process. Yet, single ﬁtness peak, or local optima deadlock, a common challenge in GP, hinders the application of GP to MDE. In this paper, we propose a strategy to promote populations’ social diversity during the GP learning process. We evaluate our approach with an empirical study featuring the case of learning well-formedness rules in MDE with a multi-objective genetic programming algorithm. Our evaluation shows that integration of social diversity leads to more efﬁcient search, faster convergence, and more generalizable results. Moreover, when the social diversity is used as crowding distance, this convergence is uniform through a hundred of runs despite the probabilistic nature of GP. It also shows that genotypic diversity strategies cannot achieve comparable results.",
        "keywords": [
            "Genetic programming",
            "Model-driven engineering",
            "Social diversity"
        ],
        "authors": [
            "Edouard R. Batot",
            "Houari Sahraoui"
        ],
        "file_path": "data/sosym-all/s10270-021-00969-9.pdf"
    },
    {
        "title": "Cross-platform edge deployment of machine learning models: a model-driven approach",
        "submission-date": "2024/07",
        "publication-date": "2025/01",
        "abstract": "Deploying machine learning (ML) models on edge devices presents unique challenges, arising from the different environments used for developing ML models and those required for their deployment, leading to a gray area of competence and expertise between ML engineers and application developers. In this paper, we explore the use of model-driven engineering to simplify the deployment of ML models on edge devices, speciﬁcally smartphones. We present a DSL for the speciﬁcation of the ML serving pipelines (pre- and postprocessing of data before and after inference), together with a model interpretation approach that allows to make changes to the pipeline during runtime, thus removing the need to re-release an application upon changes to a pipeline. We followed a design science approach, in which we elicited requirements through an initial artifact study and interviews with engineers at an industrial partner. This was followed by the design and implementation of a lightweight, JSON-based domain-speciﬁc language designed to describe ML serving pipelines, along with an accompanying Flutter library to execute the pipelines during runtime. A preliminary evaluation with four developers shows the potential of this approach to increase development speed, decrease the amount of code required to make changes to an ML serving pipeline, and make less-experienced engineers more conﬁdent contributing to the domain.",
        "keywords": [
            "Model-driven engineering",
            "MDE4AI",
            "AI engineering",
            "Mobile applications"
        ],
        "authors": [
            "Albin Karlsson Landgren",
            "Philip Perhult Johnsen",
            "Daniel Strüber"
        ],
        "file_path": "data/sosym-all/s10270-025-01273-6.pdf"
    },
    {
        "title": "Modeling event-based communication in component-based software architectures for performance predictions",
        "submission-date": "2012/02",
        "publication-date": "2013/03",
        "abstract": "Event-based communication is used in different\ndomains including telecommunications, transportation, and\nbusiness information systems to build scalable distributed\nsystems. Such systems typically have stringent requirements\nfor performance and scalability as they provide business and\nmission critical services. While the use of event-based com-\nmunication enables loosely-coupled interactions between\ncomponents and leads to improved system scalability, it\nmakes it much harder for developers to estimate the sys-\ntem’s behavior and performance under load due to the decou-\npling of components and control ﬂow. In this paper, we\npresent our approach enabling the modeling and performance\nprediction of event-based systems at the architecture level.\nApplying a model-to-model transformation, our approach\nintegrates platform-speciﬁc performance inﬂuences of the\nunderlying middleware while enabling the use of differ-\nent existing analytical and simulation-based prediction tech-\nniques. In summary, the contributions of this paper are:\n(1) the development of a meta-model for event-based com-\nmunication at the architecture level, (2) a platform aware\nmodel-to-model transformation, and (3) a detailed evaluation\nof the applicability of our approach based on two represen-\ntative real-world case studies. The results demonstrate the\neffectiveness, practicability and accuracy of the proposed\nmodeling and prediction approach.",
        "keywords": [
            "Event-based",
            "Performance model",
            "Performance evaluation",
            "Software architecture",
            "Component-based"
        ],
        "authors": [
            "Christoph Rathfelder",
            "Benjamin Klatt",
            "Kai Sachs",
            "Samuel Kounev"
        ],
        "file_path": "data/sosym-all/s10270-013-0316-x.pdf"
    },
    {
        "title": "A modelling and simulation based process for dependable systems design",
        "submission-date": "2006/02",
        "publication-date": "2007/04",
        "abstract": "Complex real-time system design needs to address dependability requirements, such as safety, reliability, and security. We introduce a modelling and simulation based approach which allows for the analysis and prediction of dependability constraints. Dependability can be improved by making use of fault tolerance techniques. The de-facto example, in the real-time system literature, of a pump control system in a mining environment is used to demonstrate our model-based approach. In particular, the system is modelled using the Discrete EVent system Speciﬁcation (DEVS) formalism, and then extended to incorporate fault tolerance mechanisms. The modularity of the DEVS formalism facilitates this extension. The simulation demonstrates that the employed fault tolerance techniques are effective. That is, the system performs satisfactorily despite the presence of faults. This approach also makes it possible to make an informed choice between different fault tolerance techniques. Performance metrics are used to measure the reliability and safety of the system, and to evaluate the dependability achieved by the design. In our model-based development process, modelling, simulation and eventual deployment of the system are seamlessly integrated.",
        "keywords": [],
        "authors": [
            "Miriam Zia",
            "Sadaf Mustaﬁz",
            "Hans Vangheluwe",
            "Jörg Kienzle"
        ],
        "file_path": "data/sosym-all/s10270-007-0050-3.pdf"
    },
    {
        "title": "A model-driven approach for vulnerability evaluation of modern physical protection systems",
        "submission-date": "2015/06",
        "publication-date": "2016/12",
        "abstract": "Modern physical protection systems integrate a number of security systems (including procedures, equipments, and personnel) into a single interface to ensure an adequate level of protection of people and critical assets against malevolent human actions. Due to the critical functions of a protection system, the quantitative evaluation of its effectiveness is an important issue that still raises several challenges.Inthispaperweproposeamodel-drivenapproach to support the design and the evaluation of physical protection systems based on (a) UML models representing threats, protection facilities, assets, and relationships among them, and (b) the automatic construction of a Bayesian Network model to estimate the vulnerability of different system configurations. Hence, the proposed approach is useful both in the context of vulnerability assessment and in designing new security systems as it enables what-if and cost–benefit analyses. A real-world case study is further illustrated in order to validate and demonstrate the potentiality of the approach. Specifically, two attack scenarios are considered against the depot of a mass transit transportation system in Milan, Italy.",
        "keywords": [
            "Physical security",
            "Vulnerability",
            "CIP_VAM UML profile",
            "Bayesian Network",
            "Model transformation",
            "Railway infrastructure system"
        ],
        "authors": [
            "Annarita Drago",
            "Stefano Marrone",
            "Nicola Mazzocca",
            "Roberto Nardone",
            "Annarita Tedesco",
            "Valeria Vittorini"
        ],
        "file_path": "data/sosym-all/s10270-016-0572-7.pdf"
    },
    {
        "title": "A search-based approach for detecting circular dependency bad smell in goal-oriented models",
        "submission-date": "2020/10",
        "publication-date": "2022/01",
        "abstract": "Goal-oriented models are gaining signiﬁcant attention from researchers and practitioners in various domains, especially in software requirements engineering. Similar to other software engineering models, goal models are subject to bad practices (i.e., bad smells). Detecting and rectifying these bad smells would improve the quality of these models. In this paper, we formally deﬁne the circular dependency bad smell and then develop an approach based on the simulated annealing (SA) search-based algorithm to detect its instances. Furthermore, we propose two mechanisms (namely, pruning and pairing) to improve the effectiveness of the proposed approach. We empirically evaluate three algorithm combinations, i.e., (1) the base SA search algorithm, (2) the base SA search algorithm augmented with pruning mechanism, and (3) the base SA search algorithm augmented with pruning and pairing mechanisms, using several case studies. Results show that simulated annealing augmented with pruning and pairing is the most effective approach, while the simulated annealing augmented with pruning mechanism is more effective than the base SA search algorithm. We also found that the proposed pruning and pairing mechanisms provide a signiﬁcant improvement in the detection of circular dependency bad smell, in terms of computation time and accuracy.",
        "keywords": [
            "Circular dependency",
            "Model-driven engineering",
            "Requirements",
            "Simulated annealing",
            "GRL"
        ],
        "authors": [
            "Mawal A. Mohammed",
            "Mohammad Alshayeb",
            "Jameleddine Hassine"
        ],
        "file_path": "data/sosym-all/s10270-021-00965-z.pdf"
    },
    {
        "title": "Model-driven engineering of middleware-based ubiquitous services",
        "submission-date": "2011/10",
        "publication-date": "2013/04",
        "abstract": "Supporting the execution of service-oriented applications over ubiquitous networks specifically calls for a service-oriented middleware (SOM), which effectively enables ubiquitous networking while benefiting from the diversity and richness of the networking infrastructure. However, developing ubiquitous applications that exploit the specific features offered by a SOM might be a time-consuming task, which demands a deep knowledge spanning from the application domain concepts down to the underlying middleware technicalities. In this paper, first we present the model-driven development process underpinning ubiSOAP, a SOM for the ubiquitous networking domain. Then, based on the domain concepts defined by the conceptual model of ubiSOAP, its architecture and its technicalities, we propose a domain-specific environment, called ubiDSE, that aids the development of applications that exploits the ubiSOAP features, from design to implementation. ubiDSE allows developers to focus on the main behavior of the modeled systems, rather than on complex details inherent to ubiquitous environments. As part of ubiDSE, specific tools are provided to automatically generate skeleton code for service-oriented applications to be executed on ubiSOAP-enabled devices, hence facilitating the exploitation of ubiSOAP by developers.",
        "keywords": [
            "Service-oriented development",
            "Model-driven service engineering",
            "Service-oriented middleware",
            "Ubiquitous computing"
        ],
        "authors": [
            "Marco Autili",
            "Mauro Caporuscio",
            "Valérie Issarny",
            "Luca Berardinelli"
        ],
        "file_path": "data/sosym-all/s10270-013-0344-6.pdf"
    },
    {
        "title": "Correction to: Enforcing ﬁne-grained access control for secure collaborative modelling using bidirectional transformations",
        "submission-date": "2017/11",
        "publication-date": "2018/01",
        "abstract": "The article “Enforcing ﬁne-grained access control for secure collaborativemodellingusingbidirectionaltransformations”, written by Csaba Debreceni, Gábor Bergmann, István Ráth, Dániel Varró, was originally published electronically on the publisher’s internet portal (https://link.springer.com/journal/10270) on [11/21/2017 6:24:42 AM] without open access.",
        "keywords": [],
        "authors": [
            "Csaba Debreceni",
            "Gábor Bergmann",
            "István Ráth",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-017-0650-5.pdf"
    },
    {
        "title": "Approaches to modeling business processes: a critical analysis of BPMN, workﬂow patterns and YAWL",
        "submission-date": "2011/08",
        "publication-date": "2011/09",
        "abstract": "We investigate three approaches describing mod-\nmodels of business processes: the OMG standard BPMN in its\nrecent version 2.0, the workﬂow patterns of the Workﬂow\nPattern Initiative and their reference implementation YAWL.\nWe show how the three approaches fail to provide practitio-\nners with a suitable means precisely and faithfully to capture\nbusiness scenarios and to analyze, communicate and manage\nthe resulting models. On the positive side, we distill from the\ndiscussion six criteria which can help to recognize practical\nand reliable tool-supported business process description and\nmodeling systems.",
        "keywords": [
            "Business process modeling",
            "BPMN",
            "Workﬂow patterns",
            "YAWL"
        ],
        "authors": [
            "Egon Börger"
        ],
        "file_path": "data/sosym-all/s10270-011-0214-z.pdf"
    },
    {
        "title": "Spectra: a speciﬁcation language for reactive systems",
        "submission-date": "2020/02",
        "publication-date": "2021/04",
        "abstract": "We introduce Spectra, a new speciﬁcation language for reactive systems, speciﬁcally tailored for the context of reactive synthesis. The meaning of Spectra is deﬁned by a translation to a kernel language. Spectra comes with the Spectra Tools, a set of analyses, including a synthesizer to obtain a correct-by-construction implementation, several means for executing the resulting controller, and additional analyses aimed at helping engineers write higher-quality speciﬁcations. We present the language in detail and give an overview of its tool set. Together with the language and its tool set, we present four collections of many, non-trivial, large speciﬁcations, written by undergraduate computer science students for the development of autonomous Lego robots and additional example reactive systems. The collected speciﬁcations can serve as benchmarks for future studies on reactive synthesis. We present the speciﬁcations, with observations and lessons learned about the potential use of reactive synthesis by software engineers.",
        "keywords": [
            "Reactive synthesis",
            "GR(1)",
            "Speciﬁcation language"
        ],
        "authors": [
            "Shahar Maoz",
            "Jan Oliver Ringert"
        ],
        "file_path": "data/sosym-all/s10270-021-00868-z.pdf"
    },
    {
        "title": "Integrating smart contracts into the modeling paradigm to harness the potential of models",
        "submission-date": "2024/03",
        "publication-date": "2025/01",
        "abstract": "Despite the increasing interest in blockchain and smart contracts, their inherent complexity has impeded widespread adoption. In order to mitigate this issue, this work introduces SmaC, a model-based framework for the development of smart contracts in Solidity that enables the treatment of contracts as models, opening up new possibilities for their enhancement and maintenance. A key beneﬁt of SmaC is its ability to impose a development pattern, which contributes to improved code quality and reduced vulnerabilities. The framework’s effectiveness is evaluated through several case studies, showing how model-driven engineering can mitigate contracts inherent complexity and promote better collaboration between developers and domain experts. As this work will demonstrate, when smart contracts are treated as models, a vast array of possibilities unfolds.",
        "keywords": [
            "Model-driven engineering",
            "Domain-speciﬁc languages",
            "Smart contracts"
        ],
        "authors": [
            "Cristian Gómez-Macías",
            "Francisco Javier Pérez-Blanco",
            "David Granada",
            "Juan Manuel Vara"
        ],
        "file_path": "data/sosym-all/s10270-024-01260-3.pdf"
    },
    {
        "title": "Redescription mining-based business process deviance analysis",
        "submission-date": "2023/01",
        "publication-date": "2024/11",
        "abstract": "Business processes often deviate from their expected or desired behavior. Such deviations can be either positive or negative, depending on whether or not they lead to better process performance. Deviance mining addresses the problem of identifying such deviations and explaining why a process deviates. In this paper, we propose a novel approach to identify and explain the causes of deviant process executions based on the technique of redescription mining, which extracts knowledge in the form of logical rules. By analyzing, comparing, and ﬁltering these rules, the reasons for the deviant behaviors of a business process are identiﬁed both in general and for particular process instances. Afterward, the results of this analysis are transformed into a concise and well-readable natural language text that can be used by business analysts and process owners to optimize processes in a reasoned manner. We evaluate our approach from different angles using four process models and provide some advice for further optimization.",
        "keywords": [
            "Deviance mining",
            "Redescription mining",
            "Process mining",
            "Natural language generation"
        ],
        "authors": [
            "Engjëll Ahmeti",
            "Martin Käppel",
            "Stefan Jablonski"
        ],
        "file_path": "data/sosym-all/s10270-024-01231-8.pdf"
    },
    {
        "title": "Test model coverage analysis under uncertainty: extended version",
        "submission-date": "2020/02",
        "publication-date": "2021/02",
        "abstract": "In model-based testing, we may have to deal with a non-deterministic model, e.g. because abstraction was applied, or because the software under test itself is non-deterministic. The same test case may then trigger multiple possible execution paths, depending on some internal decisions made by the software. Consequently, performing precise test analyses, e.g. to calculate the test coverage, are not possible.. This can be mitigated if developers can annotate the model with estimated probabilities for taking each transition. A probabilistic model checking algorithm can subsequently be used to do simple probabilistic coverage analysis. However, in practice developers often want to know what the achieved aggregate coverage is, which unfortunately cannot be re-expressed as a standard model checking problem. This paper presents an extension to allow efﬁcient calculation of probabilistic aggregate coverage, and also of probabilistic aggregate coverage in combination with k-wise coverage.",
        "keywords": [
            "Probabilistic model based testing",
            "Probabilistic test coverage",
            "Testing non-deterministic systems"
        ],
        "authors": [
            "I. S. W. B. Prasetya\nRick Klomp"
        ],
        "file_path": "data/sosym-all/s10270-020-00848-9.pdf"
    },
    {
        "title": "RoboChart: modelling and veriﬁcation of the functional behaviour of robotic applications",
        "submission-date": "2018/06",
        "publication-date": "2019/01",
        "abstract": "Robots are becoming ubiquitous: from vacuum cleaners to driverless cars, there is a wide variety of applications, many with potential safety hazards. The work presented in this paper proposes a set of constructs suitable for both modelling robotic applications and supporting veriﬁcation via model checking and theorem proving. Our goal is to support roboticists in writing models and applying modern veriﬁcation techniques using a language familiar to them. To that end, we present RoboChart, a domain-speciﬁc modelling language based on UML, but with a restricted set of constructs to enable a simpliﬁed semantics and automated reasoning. We present the RoboChart metamodel, its well-formedness rules, and its process-algebraic semantics. We discuss veriﬁcation based on these foundations using an implementation of RoboChart and its semantics as a set of Eclipse plug-ins called RoboTool.",
        "keywords": [
            "State machines",
            "Formal semantics",
            "Process algebra",
            "CSP",
            "Model checking",
            "Timed properties",
            "Domain-speciﬁc language for robotics"
        ],
        "authors": [
            "Alvaro Miyazawa",
            "Pedro Ribeiro",
            "Wei Li",
            "Ana Cavalcanti",
            "Jon Timmis",
            "Jim Woodcock"
        ],
        "file_path": "data/sosym-all/s10270-018-00710-z.pdf"
    },
    {
        "title": "Model transformations and tool integration",
        "submission-date": "2003/12",
        "publication-date": "2004/11",
        "abstract": "Model transformations are increasingly recog-\nnised as being of signiﬁcant importance to many areas of\nsoftware development and integration. Recent attention\non model transformations has particularly focused on the\nOMG’s Queries/Views/Transformations (QVT) Request\nfor Proposals (RFP). In this paper I motivate the need for\ndedicated approaches to model transformations, particu-\nlarly for the data involved in tool integration, outline the\nchallenges involved, and then present a number of tech-\nologies and techniques which allow the construction of\nﬂexible, powerful and practical model transformations.",
        "keywords": [
            "Modelling",
            "Transformations",
            "Model transformations",
            "QVT",
            "Tool integration"
        ],
        "authors": [
            "Laurence Tratt"
        ],
        "file_path": "data/sosym-all/s10270-004-0070-1.pdf"
    },
    {
        "title": "What will it take? A view on adoption of model-based methods in practice",
        "submission-date": "2011/12",
        "publication-date": "2012/08",
        "abstract": "Model-basedengineering(MBE)hasbeentouted\nas a new and substantively different approach to software\ndevelopment, characterized by higher levels of abstraction\nand automation compared to traditional methods. Despite\nthe availability of published veriﬁable evidence that it can\nsigniﬁcantly boost both developer productivity and product\nquality in industrial projects, adoption of this approach has\nbeen surprisingly slow. In this article, we review the causes\nbehind this, both technical and non-technical, and outline\nwhat needs to happen for MBE to become a reliable main-\nstream approach to software development.",
        "keywords": [
            "Model-based engineering"
        ],
        "authors": [
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-012-0261-0.pdf"
    },
    {
        "title": "An interactive tool for UML class model evolution in database applications",
        "submission-date": "2012/08",
        "publication-date": "2013/09",
        "abstract": "In the context of model-driven development of\ndatabase applications with UML, the (usually relational)\ndatabase schema is obtained automatically from the appli-\ncation’s structural (class) UML model. Changes in require-\nments often lead to modiﬁcations of the application’s struc-\ntural model. Such changes, in turn, have to be propagated\nto the underlying database schema. Very often, especially\nwhen the system is in production with a large volume of\nusers’ live data, the data is considered to be valuable enough\nto be preserved through these changes. This paper describes\nan approach to cope with the problem of model evolution\nwith the ultimate requirement to preserve the data stored in\nthe database. The algorithm interactively determines differ-\nences between structural UML models before and after the\nchanges and resolves those differences into transformations\nin the relational database domain.",
        "keywords": [
            "Model evolution",
            "Schema evolution",
            "Model differencing",
            "Schema matching",
            "Object-relational mapping"
        ],
        "authors": [
            "Vukasin Milovanovic",
            "Dragan Milicev"
        ],
        "file_path": "data/sosym-all/s10270-013-0378-9.pdf"
    },
    {
        "title": "Improving active participation during enterprise operations modeling with an extended story-card-method and participative modeling software",
        "submission-date": "2022/06",
        "publication-date": "2023/03",
        "abstract": "The COVID-19 pandemic emphasized the need for process automation, using agile software development practices. However,\nwhen agile methods are used in scaled contexts, many software development efforts fail, mainly due to lacking requirements\nengineering practices. When business-oriented software needs to be developed within a scaled context, the story-card method\n(SCM), developed as part of a previous study, assists in structuring emerging software requirements within a taxonomy that\nrepresents enterprise operation. The SCM helps agile team members to develop a common understanding about enterprise\noperation when they construct the enterprise operation taxonomy. Digital participatory enterprise modeling (PEM) may\nincrease collaboration and understanding among team members, especially when team members are geographically dis-\npersed, when they co-model their understanding of enterprise operations. Using design science research to further evolve the\nexisting SCM, we identiﬁed two concerns regarding the existing SCM: (1) The modeling software did not encourage active\nparticipation during modeling, and (2) Low quality of the resulting cooperation structure diagram (CSD) that is used to derive\nan enterprise operation taxonomy, i.e., the need to further extend the existing SCM. As main contribution of this article, we\naddressed previous deﬁciencies of the SCM, developing an extended SCM (eSCM), based on principles and guidelines that\nwould encourage online participation during PEM, also providing a comprehensive case to demonstrate the eSCM. As a\nsecond contribution, we used survey-feedback from research participants, as well as activity tracking to evaluate whether the\nmodeling tool encouraged active PEM. Our third contribution is to evaluate the quality of the resulting CSDs with suggestions\nfor future improvement.",
        "keywords": [
            "Participative enterprise modeling",
            "Participative modeling software",
            "Scaled agile",
            "DEMO"
        ],
        "authors": [
            "Marne De Vries",
            "Petra Opperman"
        ],
        "file_path": "data/sosym-all/s10270-023-01083-8.pdf"
    },
    {
        "title": "Special section on uncertainty, modeling, CPSs and AI: honoring Prof. Antonio Vallecillo",
        "submission-date": "2025/03",
        "publication-date": "2025/04",
        "abstract": "This special section contains high-quality papers in the topics of Prof. Antonio Vallecillo’s research. This set of papers has been compiled in appreciation to Prof. Vallecillo’s career and his contributions to the software engineering community in general and the modeling community in particular. After his recent retirement, this special section opens with a hindsight to Prof. Vallecillo’s fruitful and varied career, where he made contributions in several different dimensions. Then, a set of selected papers in the ﬁelds of uncertainty, modeling, CPSs and AI to honor Prof. Vallecillo’s contributions is presented.",
        "keywords": [
            "Model-based software engineering",
            "Modeling foundations",
            "Uncertainty modeling",
            "CPS",
            "Artiﬁcial intelligence"
        ],
        "authors": [
            "Javier Troya\nAlfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-025-01285-2.pdf"
    },
    {
        "title": "Fast test suite-driven model-based fault localisation with application to pinpointing defects in student programs",
        "submission-date": "2016/06",
        "publication-date": "2017/07",
        "abstract": "Fault localisation, i.e. the identiﬁcation of program locations that cause errors, takes signiﬁcant effort and cost. We describe a fast model-based fault localisation algorithm that, given a test suite, uses symbolic execution methods to fully automatically identify a small subset of program locations where genuine program repairs exist. Our algorithm iterates over failing test cases and collects locations where an assignment change can repair exhibited faulty behaviour. Our main contribution is an improved search through the test suite, reducing the effort for the symbolic execution of the models and leading to speed-ups of more than two orders of magnitude over the previously published implementation by Griesmayer et al. We implemented our algorithm for C programs, using the KLEE symbolic execution engine, and demonstrate its effectiveness on the Siemens TCAS variants. Its performance is in line with recent alternative model-based fault localisation techniques, but narrows the location set further without rejecting any genuine repair locations where faults can be ﬁxed by changing a single assignment. We also show how our tool can be used in an educational context to improve self-guided learning and accelerate assessment. We apply our algorithm to a large selection of actual student coursework submissions, provid-ing precise localisation within a sub-second response time. We show this using small test suites, already provided in the coursework management system, and on expanded test suites, demonstrating the scalability of our approach. We also show compliance with test suites does not reliably grade a class of “almost-correct” submissions, which our tool highlights, as being close to the correct answer. Finally, we show an extension to our tool that extends our fast localisation results to a selection of student submissions that contain two faults.",
        "keywords": [
            "Automated debugging",
            "Model-based fault localisation",
            "Symbolic execution",
            "Automated assessment"
        ],
        "authors": [
            "Geoff Birch",
            "Bernd Fischer",
            "Michael Poppleton"
        ],
        "file_path": "data/sosym-all/s10270-017-0612-y.pdf"
    },
    {
        "title": "A repository for scalable model management",
        "submission-date": "2012/03",
        "publication-date": "2013/03",
        "abstract": "Applying model-driven engineering (MDE) in industrial-scale systems requires managing complex models which may be very large. These models must be persisted in a way that allows their manipulation by client applications without fully loading them. In this paper, we propose Morsa, a model repository that provides scalable manipulation of large models through load on demand and incremental store; model persistence is supported by a NoSQL database. We discuss some load on demand and incremental store algorithms as well as a database design. A prototype that integrates transparently with EMF is presented, and its evaluation demonstrates that it is capable of fully managing large models with a limited amount of memory. Moreover, a set of benchmarks has been executed, exhibiting better performance than the EMF XMI file-based persistence and the most widely used model repository, CDO.",
        "keywords": [
            "MDE",
            "Model persistence",
            "Model repositories",
            "Model scalability",
            "Large models",
            "NoSQL",
            "Document databases"
        ],
        "authors": [
            "Javier Espinazo Pagán",
            "Jesús Sánchez Cuadrado",
            "Jesús García Molina"
        ],
        "file_path": "data/sosym-all/s10270-013-0326-8.pdf"
    },
    {
        "title": "A framework for relating syntactic and semantic model differences",
        "submission-date": "2016/01",
        "publication-date": "2016/08",
        "abstract": "Abstract Model differencing is an important activity in model-based development processes. Differences need to be detected, analyzed, and understood to evolve systems and explore alternatives. Two distinct approaches have been studied in the literature: syntactic differencing, which compares the concrete or abstract syntax of models, and semantic differencing, which compares models in terms of their meaning. Syntactic differencing identifies change operations that transform the syntactical representation of one model to the syntactical representation of the other. However, it does not explain their impact on the meaning of the model. Semantic model differencing is independent of syntactic changes and presents differences as elements in the semantics of one model but not the other. However, it does not reveal the syntactic changes causing these semantic differences. We define Diffuse, a language-independent, abstract framework, which relates syntactic change operations and semantic difference witnesses. We formalize fundamental relations of necessary, exhibiting, and sufficient sets of change operations and analyze their properties. We further demonstrate concrete instances of the Diffuse framework for three different popular modeling languages, namely class diagrams, activity diagrams, and feature models. The Diffuse framework provides a novel foundation for combining syntactic and semantic differencing.",
        "keywords": [
            "Model differencing",
            "Semantics",
            "Model evolution"
        ],
        "authors": [
            "Shahar Maoz",
            "Jan Oliver Ringert"
        ],
        "file_path": "data/sosym-all/s10270-016-0552-y.pdf"
    },
    {
        "title": "How to deﬁne modeling languages?",
        "submission-date": "2022/09",
        "publication-date": "2023/03",
        "abstract": "At the end of September 2022 at RWTH Aachen, Germany, there was a meeting attended by several experts on software language development with a speciﬁc emphasis on modeling. The LangDev meetings are particularly dedicated to the exchange of new ideas and innovations around the deﬁnition and use of Domain-Speciﬁc Languages (DSLs). Developers and users from multiple research and tooling groups attended, including representation from Essential, Freon (formerly ProjectIT), Gemoc, Langium, LinGo, MontiCore, MPS, Rascal, SpooFax, StarLasu, and Stratego. Attendees presented and discussed new trends and developments in their domains. Several practitioners also demonstrated new applications and uses of DSLs in various application domains, ranging from the European Union Digital COVID(-19) Certiﬁcate (the EU Digital Covid Certiﬁcate), to industrial printing systems, and digital twins, to name a few. LangDev 22 presented a very good balance among practitioners and researchers, as well as industrial and academic partners. The meeting welcomed a large number of new and young participants, which is a good sign for the community. Industrial partners included language workbench providers, and language workbench users, with impressive applications ranging from the EU DCC to examples in scientiﬁc computing. As the theme for this editorial, we want to share our impressions from that meeting as well as some general observations.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01098-1.pdf"
    },
    {
        "title": "Coupling solvers with model transformations to generate explorable model sets",
        "submission-date": "2020/04",
        "publication-date": "2021/02",
        "abstract": "Model transformation is an effective technique to produce target models from source models. Most transformation approaches focus on generating a single target model from a given source model. However, there are situations where a collection of possible target models is preferred over a single one. Such situations arise when some choices cannot be encoded in the transformation. Then, search techniques can be used to help ﬁnd a target model having speciﬁc properties. In this paper, we presentanapproachthatcombinesmodeltransformationandconstraintprogrammingtogenerateexplorablesetsofmodels.We extend previous work by adding support for multiple solvers, as well as extending ATL, a declarative transformation language used to write such transformations. We evaluate our approach and language on a task scheduling case study including both scheduling constraints and schedule visualization.",
        "keywords": [
            "Model transformation",
            "Constraint solving",
            "Model set exploration"
        ],
        "authors": [
            "Théo Le Calvar",
            "Fabien Chhel",
            "Frédéric Jouault",
            "Frédéric Saubion"
        ],
        "file_path": "data/sosym-all/s10270-021-00867-0.pdf"
    },
    {
        "title": "Extending interaction overview diagrams with activity diagram constructs",
        "submission-date": "2007/05",
        "publication-date": "2009/02",
        "abstract": "UML2.0 introduced interaction overview diag-rams (IODs) as a way of specifying relationships between UML interactions. IODs are a variant of activity diagrams that show control ﬂow between a set of interactions. The nodes in an IOD are either inline interactions or references to an interaction. A number of recent papers have deﬁned a formal semantics for IODs. These are restricted, however, to interactions that can be speciﬁed using basic sequence diagrams. This excludes the many rich modeling constructs available in activity diagrams such as interruptible regions, activity groups, concurrent node executions, and ﬂow ﬁnal nodes. It is non-trivial to allow such constructs in IODs be-cause their meaning has to be interpreted in the context of interaction sequences rather than activities. In this paper, we consider how some of these activity diagram constructs can be used practically in IODs. We motivate the integration of these constructs into IODs using a NASA air trafﬁc control subsystem and deﬁne a formal semantics for these constructs that builds on an existing semantics definition for IODs.",
        "keywords": [
            "UML",
            "Interactions",
            "Activity diagrams",
            "Formal semantics"
        ],
        "authors": [
            "Jon Whittle"
        ],
        "file_path": "data/sosym-all/s10270-009-0114-7.pdf"
    },
    {
        "title": "Formal translation of YAWL workﬂow models to the Alloy formal speciﬁcations: a testing application",
        "submission-date": "2021/07",
        "publication-date": "2022/11",
        "abstract": "Within microservice architecture-based systems, some microservices are integrated to build the software. The integration of these services may be deﬁned based on a workﬂow model. There are also a variety of different languages available for deﬁning these workﬂow models. BPMN and YAWL are two such options. It is important that testers test the integration of these microservices. This paper proposes the formal method as the solution for integration testing. This method translates the workﬂow model to the Alloy. The algorithm has suggested a translation of workﬂow models to formal speciﬁcations. This speciﬁcation takes into consideration both structural and behavioral aspects. The ﬁrst perspective is about general structures, while the second is about the behavior of the objects in a speciﬁc model. We have proved the correctness of the suggested speciﬁcations. For this purpose, the paper has shown that formal deﬁnitions are sound and are complete with nine theorems for these properties. The translation from YAWL to Alloy is deﬁned based on their BNF grammar. The generated model is an appropriate source for different purposes containing software testing. The suggested method for software testing is model-driven testing. Logical predicates deﬁne the structure of Alloy models. This method uses these logical predicates for generating tests. The test method has used RACC coverage as an example criterion. Alloy Analyzer tests the model by generating test predicates.",
        "keywords": [
            "Integration testing",
            "Alloy",
            "Formal methods",
            "YAWL",
            "Workﬂow models",
            "Microservice architecture"
        ],
        "authors": [
            "Mehran Rivadeh",
            "Seyed-Hassan Mirian-Hosseinabadi"
        ],
        "file_path": "data/sosym-all/s10270-022-01043-8.pdf"
    },
    {
        "title": "Enhancing software model encoding for feature location approaches based on machine learning techniques",
        "submission-date": "2020/02",
        "publication-date": "2021/08",
        "abstract": "Feature location is one of the main activities performed during software evolution. In our previous works, we proposed an approach for feature location in models based on machine learning, providing evidence that machine learning techniques can obtain better results than other retrieval techniques for feature location in models. However, to apply machine learning techniques optimally, the design of an encoding is essential to be able to identify the best realization of a feature. In this work, we present more thorough research about software model encoding for feature location approaches based on machine learning. As part of this study, we have provided two new software model encodings and compared them with the source encoding. The ﬁrst proposed encoding is an extension of the source encoding to take advantage of not only the main concepts and relations of a domain but also the properties of these concepts and relations. The second proposed encoding is inspired by the characteristics used in benchmark datasets for research on Learning to Rank. Afterward, the new encodings are used to compare three different machine learning techniques (RankBoost, Feedforward Neural Network, and Recurrent Neural Network). The study also considers whether a domain-independent encoding such as the ones proposed in this work can outperform an encoding that is speciﬁcally designed to exploit human experience and domain knowledge. Furthermore, the results of the best encoding and the best machine learning technique were compared to two traditional approaches that have been widely applied for feature location as well as for traceability link recovery and bug localization. The evaluation is based on two real-world case studies, one in the railway domain and the other in the induction hob domain. An approach for feature location in models evaluates these case studies with the different encodings and machine learning techniques. The results show that when using the second proposed encoding and RankBoost, the approach outperforms the results of the other encodings and machine learning techniques and the results of the traditional approaches. Speciﬁcally, the approach achieved the best results for all the performance indicators, providing a mean precision value of 90.11%, a recall value of 86.20%, a F-measure value of 87.22%, and a MCC value of 0.87. The statistical analysis of the results shows that this approach signiﬁcantly improves the results and increases the magnitude of the improvement. The promising results of this work can serve as a starting point toward the use of machine learning techniques in other engineering tasks with software models, such as traceability or bug location.",
        "keywords": [
            "Software models",
            "Feature location",
            "Machine learning",
            "Learning to Rank",
            "Neural networks",
            "Encoding"
        ],
        "authors": [
            "Ana C. Marcén",
            "Francisca Pérez",
            "Óscar Pastor",
            "Carlos Cetina"
        ],
        "file_path": "data/sosym-all/s10270-021-00920-y.pdf"
    },
    {
        "title": "Modeling context-aware and intention-aware in-car infotainment systems\nConcepts and modeling processes",
        "submission-date": "2016/02",
        "publication-date": "2016/07",
        "abstract": "It is fundamental to understand users’ intentions\nto support them when operating a computer system with a\ndynamically varying set of functions, e.g., within an in-car\ninfotainment system. The system needs to have sufﬁcient\ninformation about its own and the user’s context to pre-\ndict those intentions. Although the development of current\nin-car infotainment systems is already model-based, explic-\nitly gathering and modeling contextual information and user\nintentions is currently not supported. However, manually\ncreating software that understands the current context and\npredicts user intentions is complex, error-prone and expen-\nsive.Model-baseddevelopmentcanhelpinovercomingthese\nissues. In this paper, we present an approach for modeling\na user’s intention based on Bayesian networks. We sup-\nport developers of in-car infotainment systems by providing\nmeans to model possible user intentions according to the cur-\nrent context. We further allow modeling of user preferences\nand show how the modeled intentions may change during\nrun-time as a result of the user’s behavior. We demonstrate\nfeasibility of our approach using an industrial case study of\nan intention-aware in-car infotainment system. Finally, we\nshow how modeling of contextual information and modeling\nuser intentions can be combined by using model transforma-\ntion.",
        "keywords": [
            "Context-aware",
            "Intention-aware",
            "Modeling",
            "Infotainment"
        ],
        "authors": [
            "Daniel Lüddecke",
            "Christoph Seidl",
            "Jens Schneider",
            "Ina Schaefer"
        ],
        "file_path": "data/sosym-all/s10270-016-0543-z.pdf"
    },
    {
        "title": "Multi-paradigm modelling for cyber–physical systems: a descriptive framework",
        "submission-date": "2020/04",
        "publication-date": "2021/06",
        "abstract": "The complexity of cyber–physical systems (CPSs) is commonly addressed through complex workﬂows, involving models in a plethora of different formalisms, each with their own methods, techniques, and tools. Some workﬂow patterns, combined with particular types of formalisms and operations on models in these formalisms, are used successfully in engineering practice. To identify and reuse them, we refer to these combinations of workﬂow and formalism patterns as modelling paradigms. This paper proposes a unifying (Descriptive) Framework to describe these paradigms, as well as their combinations. This work is set in the context of Multi-Paradigm Modelling (MPM), which is based on the principle to model every part and aspect of a system explicitly, at the most appropriate level(s) of abstraction, using the most appropriate modelling formalism(s) and workﬂows. The purpose of the Descriptive Framework presented in this paper is to serve as a basis to reason about these formalisms, workﬂows, and their combinations. One crucial part of the framework is the ability to capture the structural essence of a paradigm through the concept of a paradigmatic structure. This is illustrated informally by means of two example paradigms commonly used in CPS: Discrete Event Dynamic Systems and Synchronous Data Flow. The presented framework also identiﬁes the need to establish whether a paradigm candidate follows, or qualiﬁes as, a (given) paradigm. To illustrate the ability of the framework to support combining paradigms, the paper shows examples of both workﬂow and formalism combinations. The presented framework is intended as a basis for characterisation and classiﬁcation of paradigms, as a starting point for a rigorous formalisation of the framework (allowing formal analyses), and as a foundation for MPM tool development.",
        "keywords": [
            "Multi-paradigm modelling",
            "Foundations of model-based systems engineering",
            "Cyber",
            "physical systems"
        ],
        "authors": [
            "Moussa Amrani",
            "Dominique Blouin",
            "Robert Heinrich",
            "Arend Rensink",
            "Hans Vangheluwe",
            "Andreas Wortmann"
        ],
        "file_path": "data/sosym-all/s10270-021-00876-z.pdf"
    },
    {
        "title": "Correction to: Low-code development and model-driven engineering: Two sides of the same coin?",
        "submission-date": "2022/08",
        "publication-date": "2022/08",
        "abstract": "Section 3 states that \"Codebots [7] uses UML to specify domain models that are consumed to automatically generate target artefacts, including complete REST APIs, client libraries, Swagger API documentation, and a JSON Schema deﬁnition for each domain object.\" This statement is incorrect as Codebots uses a domain-speciﬁc language, not UML, to specify domain models. Therefore, this sentence should instead read \"Codebots [7] uses a domain-speciﬁc language to specify domain models that are consumed to automatically generate target artefacts, including complete REST APIs, client libraries, Swagger API documentation, and a JSON Schema deﬁnition for each domain object.\"",
        "keywords": [],
        "authors": [
            "Davide Di Ruscio",
            "Dimitris Kolovos",
            "Juan de Lara",
            "Alfonso Pierantonio",
            "Massimo Tisi",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-022-01038-5.pdf"
    },
    {
        "title": "Assessing the impact of meta-model evolution: a measure and its automotive application",
        "submission-date": "2016/05",
        "publication-date": "2017/05",
        "abstract": "Domain-speciﬁc meta-models play an important role in the design of large software systems by deﬁning language for the architectural models. Such common modeling languages are particularly important if multiple actors are involved in the development process as they assure interoperability between modeling tools used by different actors. The main objective of this paper is to facilitate the adoption of new domain-speciﬁc meta-model versions, or a subset of the new architectural features they support, by the architectural modeling tools used by different actors in the development of large software systems. In order to achieve this objective, we developed a simple measure of meta-model evolution (named NoC—Number of Changes) that captures atomic modiﬁcation between different versions of the analyzed meta-model. We evaluated the NoC measure on the evolution of the AUTOSAR meta-model, a domain-speciﬁc meta-model used in the design of automotive system architectures. The evaluation shows that the measure can be used as an indicator of effort needed to update meta-model-based tools to support different actors in modeling new architectural features. Our detailed results show the impact of 14 new AUTOSAR features on the modeling tools used by the main actors in the automotive developmentprocess.Wevalidatedourresultsbyﬁndingasigniﬁcant correlation between the results of the NoC measure and the actual effort needed to support these features in the modeling tools reported by the modeling practitioners from four AUTOSAR tool vendors and the AUTOSAR tooling team at Volvo Cars. Generally, our study shows that quantitative analysis of domain-speciﬁc meta-model evolution using a simple measure such as NoC can be used as an indicator of the required updates in the meta-model-based tools that are needed to support new meta-model versions. However, our study also shows that qualitative analysis that may include an inspection of the actual meta-model changes is needed for more accurate assessment.",
        "keywords": [
            "Domain-speciﬁc meta-models",
            "Modeling tools",
            "Architectural features",
            "Software evolution",
            "Measurement",
            "Automotive software",
            "AUTOSAR"
        ],
        "authors": [
            "Darko Durisic",
            "Miroslaw Staron",
            "Matthias Tichy",
            "Jörgen Hansson"
        ],
        "file_path": "data/sosym-all/s10270-017-0601-1.pdf"
    },
    {
        "title": "The ForeMoSt approach to building valid model-based safety arguments",
        "submission-date": "2022/03",
        "publication-date": "2022/11",
        "abstract": "Safety assurance cases (ACs) are structured arguments designed to comprehensively show that a system is safe. ACs are often model-based, meaning that a model of the system is a primary subject of the argument. ACs use reasoning steps called strategies to decompose high-level claims about system safety into reﬁned subclaims that can be directly supported by evidence. Strategies are often informal and difﬁcult to rigorously evaluate in practice, and consequently, AC arguments often contain reasoning errors. This has led to the deployment of unsafe systems, and caused severe real-world consequences. These errors can be mitigated by formalizing and verifying AC strategies using formal methods; however, these techniques are difﬁcult to use without formal methods expertise. To mitigate potential challenges faced by engineers when developing and interpreting formal ACs, we present ForeMoSt, our tool-supported framework for rigorously validating AC strategies using the Lean theorem prover. The goal of the framework is to straddle the level of abstraction used by the theorem prover and by software engineers. We use case studies from the literature to demonstrate that ForeMoSt is able to (i) augment and validate ACs from the research literature, (ii) support AC development for systems with large models, and (iii) support different model types.",
        "keywords": [
            "Safety",
            "Assurance cases",
            "Strategies",
            "Theorem proving",
            "Lean"
        ],
        "authors": [
            "Torin Viger",
            "Logan Murphy",
            "Alessio Di Sandro",
            "Claudio Menghi",
            "Ramy Shahin",
            "Marsha Chechik"
        ],
        "file_path": "data/sosym-all/s10270-022-01063-4.pdf"
    },
    {
        "title": "FloBP: a model-driven approach for developing and executing IoT-enhanced business processes",
        "submission-date": "2023/06",
        "publication-date": "2024/02",
        "abstract": "The capability to integrate Internet of Things (IoT) technologies into business processes (BPs) has emerged as a transformative paradigm, offering unprecedented opportunities for organisations to enhance their operational efﬁciency and productivity. Interacting with the physical world and leveraging real-world data to make more informed business decisions is of greatest interest, and the idea of IoT-enhanced BPs promises to automate and improve business activities and permit them to adapt to the physical environment of execution. Nonetheless, combining these two domains is challenging, and it requires new modelling methods that do not increase notation complexity and provide independent execution between the process and the underlying device technology. In this work, we propose FloBP, a model-driven engineering approach separating concerns between the IoT and BPs, providing a structured and systematic approach to modelling and executing IoT-enhanced BPs. Applying the separation of concerns through an interdisciplinary team is needed to ensure that the approach covers all necessary process aspects, including technological and modelling ones. The FloBP approach is based on modelling tools and a microservices architecture to deploy BPMN models, and it facilitates integration with the physical world, providing ﬂexibility to support multiple IoT device technologies and their evolution. A smart canteen scenario describes and evaluates the approach’s feasibility and its possible adoption by various stakeholders. The performed evaluation concludes that the application of FloBP facilitates the modelling and development of IoT-enhanced BPs by sharing and reusing knowledge among IoT and BP experts.",
        "keywords": [
            "Internet of Things",
            "Model-driven engineering",
            "Business process",
            "Feature model",
            "IoT-enhanced business process",
            "Microservices"
        ],
        "authors": [
            "Arianna Fedeli",
            "Fabrizio Fornari",
            "Andrea Polini",
            "Barbara Re",
            "Victoria Torres",
            "Pedro Valderas"
        ],
        "file_path": "data/sosym-all/s10270-024-01150-8.pdf"
    },
    {
        "title": "On the impact of size to the understanding of UML diagrams",
        "submission-date": "2015/04",
        "publication-date": "2016/05",
        "abstract": "Background Practical experience suggests that usage and understanding of UML diagrams is greatly affected by the quality of their layout. While existing research failed to provide conclusive and comprehensive evidence in support of this hypothesis, our own previous work provided substantial evidence to this effect, also suggesting diagram size as a relevant factor, for a range of diagram types and layouts.\nAims Since there is no generally accepted precise notion of “diagram size,” we ﬁrst need to operationalize this concept,analyzeitsimpactondiagramunderstanding,andderive practical advice from our ﬁndings.\nMethod Wedeﬁnethreealternative,plausiblemetrics.Since they are all highly correlated on a large sample of UML diagrams, we opt for the simplest one. We use it to re-analyze existing experimental data on diagram understanding.\nResults We ﬁnd a strong negative correlation between diagram size and modeler performance. Our results are statistically highly signiﬁcant and exhibit a very large degree of validity. We utilize these results to derive a recommendation on diagram sizes that are, on average, optimal for model understanding. These recommendations are implemented in a plug-in to a widely used modeling tool, providing continuous feedback about diagram size to modelers.\nConclusions The effect sizes are varying, but generally suggest that the impact of size matches or exceeds that of other factors in diagram understanding. With the guideline and tool, modelers are steered toward avoiding too large diagrams.",
        "keywords": [
            "Diagram understanding",
            "Diagram size metrics",
            "Cognitive load",
            "Experiment",
            "Gestalt principles"
        ],
        "authors": [
            "Harald Störrle"
        ],
        "file_path": "data/sosym-all/s10270-016-0529-x.pdf"
    },
    {
        "title": "Connecting databases with process mining: a meta model and toolset",
        "submission-date": "2016/12",
        "publication-date": "2018/02",
        "abstract": "Process mining techniques require event logs which, in many cases, are obtained from databases. Obtaining these event logs is not a trivial task and requires substantial domain knowledge. In addition, an extracted event log provides only a single view on the database. To change our view, e.g., to focus on another business process and generate another event log, it is necessary to go back to the source of data. This paper proposes a meta model to integrate both process and data perspectives, relating one to the other. It can be used to generate different views from the database at any moment in a highly ﬂexible way. This approach decouples the data extraction from the application of analysis techniques, enabling the application of process mining in different contexts.",
        "keywords": [
            "Process mining",
            "Database",
            "Data schema",
            "Meta model",
            "Event extraction"
        ],
        "authors": [
            "Eduardo González López de Murillas",
            "Hajo A. Reijers",
            "Wil M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-018-0664-7.pdf"
    },
    {
        "title": "Static slicing of Use Case Maps requirements models",
        "submission-date": "2017/07",
        "publication-date": "2018/06",
        "abstract": "Requirements speciﬁcation is a crucial stage in many software development life cycles. As requirements speciﬁcations evolve, they often become more complex. The development of methods to assist the comprehension and maintenance of requirements speciﬁcations has gained much attention in the past 20years. However, there is much room for improvement for model-based speciﬁcations. The Use Case Maps (UCM) language, part of the ITU-T Z.151 User Requirements Notation standard, is a visual modeling notation that aims to describe requirements at a high level of abstraction. A UCM speciﬁcation is used to integrate and capture both functional aspects (based on causal scenarios representing behavior) and architectural aspects (actors and system components responsible for scenario activities). As UCM models evolve and grow, they rapidly become hard to understand and to maintain. In this paper, we propose a static slicing technique to enhance the comprehension of UCM models. The developed slicing approach is implemented within the jUCMNav tool. We validate the proposed approach using a mock system and three publicly available UCM speciﬁcations. The results suggest that, on average, the models can be reduced by about 70% through slicing without losing information required for comprehension and maintenance activities. A small experiment involving 9 participants also suggests that the understandability of UCM speciﬁcations and comprehension speed have both improved substantially by using jUCMNav’s new slicing feature.",
        "keywords": [
            "Requirements speciﬁcation",
            "Use Case Maps",
            "Slicing",
            "User Requirements Notation",
            "Comprehension",
            "Maintenance"
        ],
        "authors": [
            "Taha Binalialhag",
            "Jameleddine Hassine",
            "Daniel Amyot"
        ],
        "file_path": "data/sosym-all/s10270-018-0680-7.pdf"
    },
    {
        "title": "The meaning of multiplicity of n-ary associations in UML",
        "submission-date": "2002/01",
        "publication-date": "2002/12",
        "abstract": "The concept of multiplicity in UML derives from that of cardinality in entity-relationship modeling techniques. The UML documentation deﬁnes this concept but at the same time acknowledges some lack of obvious-ness in the speciﬁcation of multiplicities for n-ary associations. This paper shows an ambiguity in the deﬁnition given by UML documentation and proposes a clariﬁcation to this deﬁnition, as well as the use of outer and inner multiplicities as a simple extension to the current nota-tion to represent other multiplicity constraints, such as participation constraints, that are equally valuable in understanding n-ary associations.",
        "keywords": [
            "UML",
            "multiplicity",
            "cardinality",
            "ternary association",
            "n-ary association",
            "ternary relationship",
            "n-ary relationship",
            "data modeling",
            "entity-relationship modeling"
        ],
        "authors": [
            "Gonzalo G´enova",
            "Juan Llorens",
            "Paloma Mart´ınez"
        ],
        "file_path": "data/sosym-all/s10270-002-0009-3.pdf"
    },
    {
        "title": "Conﬂict management techniques for model merging: a systematic mapping review",
        "submission-date": "2021/09",
        "publication-date": "2022/10",
        "abstract": "Model merging conﬂicts occur when different stakeholders aim to integrate their contradicting changes that are applied con-currently to update software models. We conduct an extensive systematic mapping study on conﬂict management techniques and relevant collaboration attributes to the versioning and merging models from 2001 to the middle of 2021. This study follows the standard guidelines within the software engineering domain. We analyzed a total of 105 articles extracted from an initial pool of more than 1800 articles to infer a taxonomy for conﬂict management techniques. We use this taxonomy to classify existing approaches to understand characteristics, shortcomings, and challenges on conﬂict management techniques in merging models. It also provides a solid foundation for future work in this area. We show that syntactic conﬂicts are the most studied type and that the top three popular conﬂict detection techniques are constraint violation, change overlapping, and pattern matching. We observe the lack of a comprehensive state-of-the-art comparison between academic or industrial tools, as well as the need for real-world case studies. Finally, we show that recent trends have focused on online collaboration, where teams of stakeholders work on large-scale models.",
        "keywords": [
            "Conﬂict management",
            "Model merging conﬂict",
            "Collaborative modeling",
            "Model driven engineering",
            "Taxonomy",
            "Systematic mapping"
        ],
        "authors": [
            "Mohammadreza Sharbaf",
            "Bahman Zamani",
            "Gerson Sunyé"
        ],
        "file_path": "data/sosym-all/s10270-022-01050-9.pdf"
    },
    {
        "title": "Consistency requirements in business process modeling: a thorough overview",
        "submission-date": "2017/02",
        "publication-date": "2017/11",
        "abstract": "The ﬁeld of business process modeling has been beset by inter-model consistency problems which are mainly due to the existence of multiple variants of the same busi-ness process, for instance when models have been produced by different actors, or through the time by a same (or dif-ferent) actor(s), as well as the possibility of its modeling from discrete and complementary perspectives (using dif-ferent lenses). Accordingly, our overall aim in this paper is to provide a thorough overview of consistency requirements in business process modeling, which is strongly needed not only for the sake of a comprehensive investigation of this challenging subject, but also for the sake of empowering sig-niﬁcant contributions to it. In order to do so, we opted for a systematic literature review of consistency among busi-ness process models as starting point and basis to attain the intended overview and to guide our contributions in this ﬁeld.",
        "keywords": [
            "Business process modeling",
            "Consistency requirements",
            "Inter-model consistency",
            "Systematic literature review",
            "Context-awareness",
            "Map formalism"
        ],
        "authors": [
            "Afef Awadid",
            "Selmin Nurcan"
        ],
        "file_path": "data/sosym-all/s10270-017-0629-2.pdf"
    },
    {
        "title": "Using two case studies to explore the applicability of VIATRA for the model-driven engineering of mechatronic production systems",
        "submission-date": "2020/07",
        "publication-date": "2022/02",
        "abstract": "The engineering of mechatronic production systems is complex and requires various disciplines (e.g., systems, mechanical, electrical and software engineers). Model-driven engineering (MDE) supports systems development and the exchange of information based on models and transformations. However, the integration and adoption of different modeling approaches are becoming challenges when it comes to cross-disciplinary work. VIATRA is a long-living enduring and mature modeling framework that offers rich model transformation features to develop MDE applications. This study investigates the extent to which VIATRA can be applied in the engineering of mechatronic production systems. For this purpose, two model transformation case studies are presented: “SysML–AutomationML” and “SysML4Mechatronics–AutomationML.” Both case studies are representative of structural modeling and interdisciplinary data exchange during the development of mechatronic production systems. These case studies are derived from other researchers in the community. A VIATRA software prototype implements these case studies as a batch-oriented transformation and serves as one basis for evaluating VIATRA. To report on our observations and ﬁndings, we built on an evaluation framework from the MDE community. This framework considers 14 different characteristics (e.g., maturity, size, execution time, modularity, learnability), according to the Goal-Question-Metric paradigm. To be able to evaluate our ﬁndings, we compared VIATRA to ATL. We applied all cases to a lab-size mechatronic production system. We found that, with VIATRA, the same functions for model transformation applications can be achieved as with ATL, which is popular for model transformations in both the MDE and the mechatronic production systems commu-nity. VIATRA combines the relational, imperative, and graph-based paradigms and enables the development and execution of model-to-model (M2M) and model-to-text (M2T) transformations. Furthermore, the VIATRA internal DSL is based on Xtend and Java, making VIATRA attractive and intuitive for users with less experience in modeling than in object-oriented programming. Thus, VIATRA leads to an interesting alternative for the model-driven engineering of mechatronic production systems. It has the potential to reduce the complexity during the development of model transformations. To conclude, this paper evaluates the applicability of VIATRA, its strengths and limitations. It provides lessons learned and insights that can stimulate further research in the MDE for mechatronic production systems.",
        "keywords": [
            "Model Transformations",
            "Model-Driven Engineering",
            "VIATRA",
            "Mechatronic Production Systems",
            "Applicability Study"
        ],
        "authors": [
            "Gennadiy Koltun",
            "Mathis Pundel"
        ],
        "file_path": "data/sosym-all/s10270-021-00962-2.pdf"
    },
    {
        "title": "Model-based ideal testing of hardware description language (HDL) programs",
        "submission-date": "2020/07",
        "publication-date": "2021/11",
        "abstract": "An ideal test is supposed to show not only the presence of bugs but also their absence. Based on the Fundamental Test Theory of Goodenough and Gerhart (IEEE Trans Softw Eng SE-1(2):156–173, 1975), this paper proposes an approach to model-based ideal testing of hardware description language (HDL) programs based on their behavioral model. Test sequences are generated from both original (fault-free) and mutant (faulty) models in the sense of positive and negative testing, forming a holistic test view. These test sequences are then executed on original (fault-free) and mutant (faulty) HDL programs, in the sense of mutation testing. Using the techniques known from automata theory, test selection criteria are developed and formally show that they fulﬁll the major requirements of Fundamental Test Theory, that is, reliability and validity. The current paper comprises a preparation step (consisting of the sub-steps model construction, model mutation, model conversion, and test generation) and a composition step (consisting of the sub-steps pre-selection and construction of Ideal test suites). All the steps are supported by a toolchain that is already implemented and is available online. To critically validate the proposed approach, three case studies (a sequence detector, a trafﬁc light controller, and a RISC-V processor) are used and the strengths and weaknesses of the approach are discussed. The proposed approach achieves the highest mutation score in positive and negative testing for all case studies in comparison with two existing methods (regular expression-based test generation and context-based random test generation), using four different techniques.",
        "keywords": [
            "Model-based testing",
            "Ideal testing",
            "Mutation testing",
            "Behavioral model",
            "Hardware description language"
        ],
        "authors": [
            "Onur Kilincceker",
            "Ercument Turk",
            "Fevzi Belli",
            "Moharram Challenger"
        ],
        "file_path": "data/sosym-all/s10270-021-00934-6.pdf"
    },
    {
        "title": "Requirements-driven deployment\nCustomizing the requirements model for the host environment",
        "submission-date": "2011/05",
        "publication-date": "2012/07",
        "abstract": "Deployment is a main development phase which\nconfigures a software to be ready for use in a certain environ-\nment. The ultimate goal of deployment is to enable users to\nachieve their requirements while using the deployed soft-\nware. However, requirements are not uniform and differ\nbetween deployment environments. In one environment, cer-\ntain requirements could be useless or redundant, thereby\nmaking some software functionalities superﬂuous. In another\nenvironment, instead, some requirements could be impossi-\nble to achieve and, thus, additional functionalities would be\nrequired. We advocate that ensuring ﬁtness between require-\nments and the system environment is a basic and critical step\nto achieve a comprehensive deployment process. We pro-\npose a tool-supported modelling and analysis approach to\ntailor a requirements model to each environment in which\nthe system is to be deployed. We study the case of a con-\ntextual goal model, which is a requirements model that cap-\ntures the relationship between the variability of requirements\n(goal variability space) and the varying states of a deploy-\nment environment (context variability space). Our analysis\nrelies on sampling a deployment environment to discover its\ncontext variability space and use it to identify loci in the con-\ntextual goal model where a modiﬁcation has to take place.",
        "keywords": [
            "Requirements engineering",
            "Contextual requirements",
            "Deployment",
            "Context-sensitive systems modelling"
        ],
        "authors": [
            "Raian Ali",
            "Fabiano Dalpiaz",
            "Paolo Giorgini"
        ],
        "file_path": "data/sosym-all/s10270-012-0255-y.pdf"
    },
    {
        "title": "Benchmarking bidirectional transformations: theory, implementation, application, and assessment",
        "submission-date": "2019/03",
        "publication-date": "2019/09",
        "abstract": "Bidirectional transformations (bx) are relevant for a wide range of application domains. While bx problems may be solved with unidirectional languages and tools, maintaining separate implementations of forward and backward synchronizers with mutually consistent behavior can be difﬁcult, laborious, and error-prone. To address the challenges involved in handling bx problems, dedicated languages and tools for bx have been developed. Due to their heterogeneity, however, the numerous and diverse approaches to bx are difﬁcult to compare, with the consequence that fundamental differences and similarities are not yet well understood. This motivates the need for suitable benchmarks that facilitate the comparison of bx approaches. This paper provides a comprehensive treatment of benchmarking bx, covering theory, implementation, application, and assessment. At the level of theory, we introduce a conceptual framework that deﬁnes and classiﬁes architectures of bx tools. At the level of implementation, we describe Benchmarx, an infrastructure for benchmarking bx tools which is based on the conceptual framework. At the level of application, we report on a wide variety of solutions to the well-known Families-to-Persons benchmark, which were developed and compared with the help of Benchmarx. At the level of assessment, we reﬂect on the usefulness of the Benchmarx approach to benchmarking bx, based on the experiences gained from the Families-to-Persons benchmark.",
        "keywords": [
            "Bidirectional transformation",
            "Benchmark",
            "Model synchronization",
            "Framework"
        ],
        "authors": [
            "Anthony Anjorin",
            "Thomas Buchmann",
            "Bernhard Westfechtel",
            "Zinovy Diskin",
            "Hsiang-Shang Ko",
            "Romina Eramo",
            "Georg Hinkel",
            "Leila Samimi-Dehkordi",
            "Albert Zündorf"
        ],
        "file_path": "data/sosym-all/s10270-019-00752-x.pdf"
    },
    {
        "title": "Data warehouse concepts for model artifacts?",
        "submission-date": "2012/04",
        "publication-date": "2012/04",
        "abstract": "Manufacturing industries, in particular avionics and automotive industries, that have embraced model-driven system development approaches, are faced with the challenge of getting development tools that address different, but related aspects of the development process to interoperate. This challenge led to increased interest in and work on tool-integration approaches that enable the creation of “tool chains”. These approaches are based on the assumption that it is best to use tools that are specialized for speciﬁc purposes. An appropriate way to integrate the tools is to provide facilities for transferring the outputs of one tool to another specialized tool. This typically requires transformation of data produced from one tool to a format suitable for input to other tools. As an example, the development of software for a cyber-physical system (CPS) based on a communication bus (e.g., software found in cars) is based on a dominant bus architecture. We have seen in current development approaches any pieceofdata(signal)ﬂowingbetweencontroldevicesthatare connected with a bus needs a unique name (identiﬁer). This enforces that signals be maintained in a centralized catalog to help ensure qualities such as consistency and completeness, even though they never ﬂow on the same bus and do not interfere at all in the CPS and ruins ﬂexible adaption of the CPS.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-012-0244-1.pdf"
    },
    {
        "title": "Efficient construction of family-based behavioral models from adaptively learned models",
        "submission-date": "2023/08",
        "publication-date": "2024/08",
        "abstract": "Family-based behavioral models capture the behavior of a software product line (SPL) in a single model, incorporating the variability among the products. In representing these models, a common technique is to annotate well-known behavioral modeling notations with features, e.g., featured ﬁnite state machine (FFSM) as an extension to the well-known ﬁnite state machine notation. It is not always the case that family-based behavioral models are prepared before developing an SPL, or kept up-to-date during the development and maintenance. Model learning is helpful in such situations. Taking advantage of the commonality among the SPL products, it is possible to reuse the product models in learning the behavior of the entire SPL. In this paper, the process of constructing FFSM models for SPLs is enhanced. Model learning is performed using an adaptive learning algorithm called PL*. Regarding the model learning step, we introduce a new heuristic method for determining the product learning orders with high learning efﬁciency. The proposed heuristic takes into account the complexity of features added by each product and improves the previous heuristics for learning order. To construct the whole family-based behavioral model of an SPL, the behavioral models of individual products are iteratively merged into the whole family-based model. A similarity metric is used to determine which states of the two models are merged with each other. By providing a formalization for the existing FFSMDi f f algorithm for this purpose, we prove that in the FFSM constructed by this algorithm, the choice of the similarity metric does not affect the observable behavior of the constructed FFSM. We study the efﬁciency of three similarity metrics, two of which are local metrics, in the sense that they determine the similarity of two states only in terms of their adjacent transitions. On the other hand, a global similarity metric takes into account not only the adjacent transitions, but also the similarity of their adjacent states. It is shown by experimentation on two case studies that local similarity metrics can result in constructing FFSMs as concise as the FFSM resulting from the global similarity metric. The results also show that local similarity metrics increase the efﬁciency and scalability while maintaining the effectiveness of the FFSM construction.",
        "keywords": [
            "Adaptive model learning",
            "Software product lines",
            "Behavioral model",
            "Featured finite state machine"
        ],
        "authors": [
            "Shaghayegh Tavassoli",
            "Ramtin Khosravi"
        ],
        "file_path": "data/sosym-all/s10270-024-01199-5.pdf"
    },
    {
        "title": "Guest editorial for EMMSAD’2022 special section",
        "submission-date": "2023/09",
        "publication-date": "2023/12",
        "abstract": "The EMMSAD (Exploring Modeling Methods for Systems Analysis and Development) conference series organized 27 events from 1996 to 2022, associated with CAISE (Conference on Advanced Information Systems Engineering). In 2009, EMMSAD became a two-day working conference. Since 2017, EMMSAD best papers are invited to submit extended versions for consideration of their publication in the Journal of Software and Systems Modeling (SoSyM). The main topics of the EMMSAD series have the focus on models and modeling methods for the analysis and development of software information systems of any kind.",
        "keywords": [],
        "authors": [
            "Iris Reinhartz‑Berger",
            "Dominik Bork"
        ],
        "file_path": "data/sosym-all/s10270-023-01130-4.pdf"
    },
    {
        "title": "An experiment in model-driven conceptual database design",
        "submission-date": "2017/09",
        "publication-date": "2018/03",
        "abstract": "The article presents the results of an experiment we conducted with database professionals in order to evaluate an approach to\nautomatic design of the initial conceptual database model based on collaborative business process models. The source business\nprocess model is represented by BPMN, while the target conceptual model is represented by the UML class diagram. The\nresults conﬁrm those already obtained in a case-study-based evaluation, as well as those of an earlier controlled experiment\nconducted with undergraduate students. The evaluation implies that the proposed approach and implemented generator enable\nautomatic generation of the target conceptual model with a high percentage of completeness and precision. The experiment\nalso conﬁrms that the automatically generated model can be efﬁciently used as a starting point for manual design of the target\nmodel, since it signiﬁcantly shortens the estimated efforts and actual time spent to obtain the target model in contrast to the\nmanual design from scratch.",
        "keywords": [
            "BPMN",
            "Collaborative business process model",
            "Conceptual database model",
            "Evaluation",
            "Experiment",
            "Model-driven",
            "UML"
        ],
        "authors": [
            "Drazen Brdjanin",
            "Goran Banjac",
            "Danijela Banjac",
            "Slavko Maric"
        ],
        "file_path": "data/sosym-all/s10270-018-0672-7.pdf"
    },
    {
        "title": "Type inference in ﬂexible model-driven engineering using classiﬁcation algorithms",
        "submission-date": "2016/06",
        "publication-date": "2018/01",
        "abstract": "Flexible or bottom-up model-driven engineering (MDE) is an emerging approach to domain and systems modelling. Domain experts, who have detailed domain knowledge, typically lack the technical expertise to transfer this knowledge using traditional MDE tools. Flexible MDE approaches tackle this challenge by promoting the use of simple drawing tools to increase the involvement of domain experts in the language deﬁnition process. In such approaches, no metamodel is created upfront, but instead the process starts with the deﬁnition of example models that will be used to infer the metamodel. Pre-deﬁned metamodels created by MDE experts may miss important concepts of the domain and thus restrict their expressiveness. However, the lack of a metamodel, that encodes the semantics of conforming models has some drawbacks, among others that of having models with elements that are unintentionally left untyped. In this paper, we propose the use of classiﬁcation algorithms to help with the inference of such untyped elements. We evaluate the proposed approach in a number of random generated example models from various domains. The correct type prediction varies from 23 to 100% depending on the domain, the proportion of elements that were left untyped and the prediction algorithm used.",
        "keywords": [
            "Model-driven engineering",
            "Flexible model-driven engineering",
            "Bottom-up metamodelling",
            "Type inference",
            "Classiﬁcation and regression trees",
            "Random forests"
        ],
        "authors": [
            "Athanasios Zolotas",
            "Nicholas Matragkas",
            "Sam Devlin",
            "Dimitrios S. Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-018-0658-5.pdf"
    },
    {
        "title": "Explicit versus implicit models: What are good languages for modeling?",
        "submission-date": "2022/04",
        "publication-date": "2022/04",
        "abstract": "Although modeling is used in almost all science and engineering disciplines, the explicit deﬁnition of modeling languages is an invention of modern computing technology. It was necessary to deﬁne modeling languages precisely so that computers could process the models described in the languages, as a way to support various analysis and synthesis needs (e.g., model analysis and code generation). However, these were not the ﬁrst languages that were deﬁned pre- cisely. Natural languages, for example English in the Oxford dictionary and German in the Duden, received a rather formal deﬁnition prior to digital automation with computers. The ideas of grammars and related lexicographic deﬁnitions were transferred from the natural language context to the ﬁrst computer languages, namely programming languages and speciﬁcation languages, where it received much more precision. And for diagrammatic modeling languages, the concept of a meta-model was popularized, often as a variant of a class diagram.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-01001-4.pdf"
    },
    {
        "title": "Distributed model validation with Epsilon",
        "submission-date": "2020/03",
        "publication-date": "2021/03",
        "abstract": "Scalable performance is a major challenge with current model management tools. As the size and complexity of models and\nmodel management programs increases and the cost of computing falls, one solution for improving performance of model\nmanagement programs is to perform computations on multiple computers. In this paper, we demonstrate a low-overhead\ndata-parallel approach for distributed model validation in the context of an OCL-like language. Our approach minimises\ncommunication costs by exploiting the deterministic structure of programs and can take advantage of multiple cores on each\n(heterogeneous) machine with highly conﬁgurable computational granularity. Our performance evaluation shows that the\nimplementation is extremely low overhead, achieving a speed up of 24.5× with 26 computers over the sequential case, and\n122× when utilising all six cores on each computer.",
        "keywords": [
            "Model-driven engineering",
            "Model validation",
            "Distributed computing",
            "Model management",
            "Parallelism"
        ],
        "authors": [
            "Sina Madani",
            "Dimitris Kolovos",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-021-00878-x.pdf"
    },
    {
        "title": "“No shit” or “Oh, shit!”: responses to observations on the use of UML in professional practice",
        "submission-date": "2014/08",
        "publication-date": "2014/08",
        "abstract": "This paper follows a paper, “UML in Practice” presentedatICSE2013.Itsummarizesandreﬂectsonthedis-cussion and additional investigation that arose from “UML in Practice.” The paper provides a condensed recap of “UML in Practice” ﬁndings, explains what data were collected from which sources to inform this paper, and describes how the data were analyzed. It reports on the discussion that has arisen, summarizing responses from industry practitioners, academicsteachingsoftwareengineering,andtheUMLcom-munity, and considers how those responses reﬂect on the original observations. The responses to “UML in Practice” divide (crudely) between two perspectives: (1) the observations made are familiar and unsurprizing, and match personal experience (“No shit”); or (2) the observations threaten long-held beliefs about UML use, and in particular about the status of UML as the de facto standard of software engineering, implying a need to change personal practice (“Oh, shit!”).",
        "keywords": [
            "UML",
            "Software development",
            "Software design",
            "Notation",
            "Empirical studies"
        ],
        "authors": [
            "Marian Petre"
        ],
        "file_path": "data/sosym-all/s10270-014-0430-4.pdf"
    },
    {
        "title": "Variability in UML language and semantics",
        "submission-date": "2011/08",
        "publication-date": "2011/08",
        "abstract": "Practitioners, who use UML as a sketching language are generally not too concerned about the precision of their models, but developers who build UML models to rigorously analyze software properties (e.g., to analyze the consistency of design constraints) or that can be mechanically transformed to implementations requiring tools and tool chains that are based on a precisely defined UML semantics (see this issue’s Expert Voice by Manfred Broy and María Victoria Cengarle as well as the regular paper on the many semantics of sequence diagrams by Zoltán Micskei and Hélène Waeselynck).Thisneedmotivatesmuchoftheworkondeﬁning appropriate formal semantics for the UML.\nThere is a significantly large body of work on formalizing the UML—both syntactical appearance, internal representation and semantics (in terms of meaning), and the collective experience suggests that defining appropriate semantics for the UML has both a technical and a strong political/social aspect. This non-technical aspect is concerned primarily with determining what constitutes an “appropriate” language. The problem is that different stakeholders, including UML modelers from different domains, tool vendors with specific ready to use solutions, have varying views of what constitutes an appropriate UML language and its semantics.\nIt is not easily possible to support these sometimes competing views in a single language. This led to the view of UML as a “family of languages” and to the introduction of profile mechanisms and “semantic variation points” that can be used for specializing the syntax and semantics of UML.\nB. Rumpe (B)\nRWTH Aachen, Aachen, Germany\ne-mail: bernhard.rumpe@sosym.org\nR. France (B)\nColorado State University, Fort Collins, CO, USA\ne-mail: france@cs.colostate.edu\nThe UML currently has a wide variety of these semantic variation points indicating points in the language definition that can be tailored to better support the many forms of usage of UML. Although this form of tailoring may be convenient for developers, it makes the development of generic tools and tool chains considerably more complex and makes it almost impossible to provide a well formed, rather complete and precise semantics for the UML as a whole. Furthermore, the UML does currently not provide good mechanisms for introducing and describing variations or selecting concrete sub-variants yet.\nManaging variability within a language, such as the UML can be likened to manage variability of a software product line. Indeed, it is useful to regard the UML as a product line of languages to explore how techniques for managing variability in product lines (e.g., feature diagrams) can be used to explicitly manage variability in UML. We recently invested some efforts in studying this technique and our work suggests that it can very well be used to make the UML, or at least some derivatives of UML, more precise and easier to use.\nIt can also help developers understand similarities and differences across different UML derivatives. One can envisage configuring a UML tool using a configuration that describes a particular UML derivative, the required tool functionality, enhancedanalysisalgorithmsordomain-speciﬁcrestrictions, thedesiredformofcodeandtestgeneration,amongotherfeatures.OnecanalsoenvisagethattheUMLstandarddeﬁnesits semantic variation points explicitly using feature diagrams.\nFrom its many possible forms of uses, it seems clear that the UML will not have a single syntactic form or semantics that adequately serves its community, but understanding and managing variations in the language UML might allow us to cope with this drawback.\nThe time to explore language variability to allow modelers deal with precise and well-assisted language variations.",
        "keywords": [],
        "authors": [
            "Bernhard Rumpe",
            "Robert France"
        ],
        "file_path": "data/sosym-all/s10270-011-0210-3.pdf"
    },
    {
        "title": "Automatic derivation of conceptual database models from differently serialized business process models",
        "submission-date": "2019/09",
        "publication-date": "2020/07",
        "abstract": "The existing tools that aim to derive data models from business process models are typically able to process the source models represented by one single notation and also serialized in one specific way. However, the standards (e.g., BPMN) enable different serialization formats and also provide serialization flexibility, which leads to various implementations of the standard in different modeling tools and results in differently serialized models in practice, which therefore significantly constraints usability of the existing model-driven tools. In this article, we present an approach to automatic derivation of conceptual database models from business process models represented by different notations, with particular focus on differently serialized process models. A deterministic rule-based approach is proposed to overcome the serialization specificities and to enable extraction of characteristic elements from differently serialized process models. Based on the proposed approach, we implemented an online web-based model-driven tool named AMADEOS, which is able to automatically derive conceptual database models from process models represented by different notations and also differently serialized. The experimental results show that the proposed approach and implemented tool enable successful extraction of specific elements from differently serialized process models and enable derivation of the target conceptual database models with very high completeness and precision.",
        "keywords": [
            "AMADEOS",
            "BPMN",
            "Business process model",
            "Conceptual database model",
            "Extractor",
            "Robustness",
            "Serialization",
            "Structural differences"
        ],
        "authors": [
            "Drazen Brdjanin",
            "Stefan Ilic",
            "Goran Banjac",
            "Danijela Banjac",
            "Slavko Maric"
        ],
        "file_path": "data/sosym-all/s10270-020-00808-3.pdf"
    },
    {
        "title": "Conformance checking in UML artifact-centric business process models",
        "submission-date": "2017/01",
        "publication-date": "2018/05",
        "abstract": "Business artifacts have appeared as a new paradigm to capture the information required for the complete execution and\nreasoning of a business process. Likewise, conformance checking is gaining popularity as a crucial technique that enables\nevaluating whether recorded executions of a process match its corresponding model. In this paper, conformance checking\ntechniques are incorporated into a general framework to specify business artifacts. By relying on the expressive power of\nan artifact-centric speciﬁcation, BAUML, which combines UML state and activity diagrams (among others), the problem\nof conformance checking can be mapped into the Petri net formalism and its results be explained in terms of the original\nartifact-centric speciﬁcation. In contrast to most existing approaches, ours incorporates data constraints into the Petri nets, thus\nachieving conformance results which are more precise. We have also implemented a plug-in, within the ProM framework,\nwhich is able to translate a BAUML into a Petri net to perform conformance checking. This shows the feasibility of our\napproach.",
        "keywords": [
            "Conformance checking",
            "Artifact-centric BPM",
            "BAUML framework",
            "Process mining"
        ],
        "authors": [
            "Montserrat Estañol",
            "Jorge Munoz-Gama",
            "Josep Carmona",
            "Ernest Teniente"
        ],
        "file_path": "data/sosym-all/s10270-018-0681-6.pdf"
    },
    {
        "title": "OSTRICH: a rich template language for low-code development (extended version)",
        "submission-date": "2022/03",
        "publication-date": "2022/12",
        "abstract": "Low-code platforms aim at allowing non-experts to develop complex systems and knowledgeable developers to improve their productivity in orders of magnitude. The greater gain comes from using components developed by experts capturing common patterns across all layers of the application, from the user interface to the data layer and integration with external systems. Often, cloning sample code fragments is the only alternative in such scenarios, requiring extensive adaptation to reach the intended use. Such customization activities require deep knowledge outside of the comfort zone of low code. To effectively speed up the reuse, composition, and adaptation of pre-deﬁned components, low-code platforms need to provide safe and easy-to-use language mechanisms. This paper introduces OSTRICH, a strongly typed rich templating language for a low-code platform (OutSystems) that builds on metamodel annotations and allows the correct instantiation of templates. We conservatively extend the existing metamodel and ensure that the resulting code is always well-formed. The results we present include a novel type safety veriﬁcation of template deﬁnitions, and template arguments, providing model consistency across application layers. We implemented this template language in a prototype of the OutSystems platform and ported nine of the top ten most used sample code fragments, thus improving the reuse of professionally designed components.",
        "keywords": [
            "Metamodel templating",
            "Typechecking templates",
            "Parameter constraints",
            "Low-code",
            "Development productivity",
            "Model reuse"
        ],
        "authors": [
            "Hugo Lourenço",
            "Carla Ferreira",
            "João Costa Seco",
            "Joana Parreira"
        ],
        "file_path": "data/sosym-all/s10270-022-01066-1.pdf"
    },
    {
        "title": "Characteristics, potentials, and limitations of open-source Simulink projects for empirical research",
        "submission-date": "2020/09",
        "publication-date": "2021/04",
        "abstract": "Simulink is an example of a successful application of the paradigm of model-based development into industrial practice. Numerous companies create and maintain Simulink projects for modeling software-intensive embedded systems, aiming at early validation and automated code generation. However, Simulink projects are not as easily available as code-based ones, which proﬁt from large publicly accessible open-source repositories, thus curbing empirical research. In this paper, we investigate a set of 1734 freely available Simulink models from 194 projects and analyze their suitability for empirical research. We analyze the projects considering (1) their development context, (2) their complexity in terms of size and organization within projects, and (3) their evolution over time. Our results show that there are both limitations and potentials for empirical research. On the one hand, some application domains dominate the development context, and there is a large number of models that can be considered toy examples of limited practical relevance. These often stem from an academic context, consist of only a few Simulink blocks, and are no longer (or have never been) under active development or maintenance. On the other hand, we found that a subset of the analyzed models is of considerable size and complexity. There are models comprising several thousands of blocks, some of them highly modularized by hierarchically organized Simulink subsystems. Likewise, some of the models expose an active maintenance span of several years, which indicates that they are used as primary development artifacts throughout a project’s lifecycle. According to a discussion of our results with a domain expert, many models can be considered mature enough for quality analysis purposes, and they expose characteristics that can be considered representative for industry-scale models. Thus, we are conﬁdent that a subset of the models is suitable for empirical research. More generally, using a publicly available model corpus or a dedicated subset enables researchers to replicate ﬁndings, publish subsequent studies, and use them for validation purposes. We publish our dataset for the sake of replicating our results and fostering future empirical research.",
        "keywords": [
            "Simulink",
            "Open source",
            "Empirical research",
            "Sample study"
        ],
        "authors": [
            "Alexander Boll",
            "Florian Brokhausen",
            "Tiago Amorim",
            "Timo Kehrer",
            "Andreas Vogelsang"
        ],
        "file_path": "data/sosym-all/s10270-021-00883-0.pdf"
    },
    {
        "title": "Specification of invariability in OCL",
        "submission-date": "2008/05",
        "publication-date": "2011/10",
        "abstract": "The Object Constraint Language (OCL) is a high-level, object-oriented language for contractual system spec-iﬁcations. Despite its expressivity, OCL does not provide primitives for a compact speciﬁcation of invariability. In this paper, problems with invariability speciﬁcation are listed and some weaknesses of existing solutions are pointed out. The question of invariability speciﬁcation is addressed and a simple but expressive extension of OCL is proposed. It allows a view-orientedspeciﬁcationofinvariabilityconstraints,wher-eby we restrict the notion of view to reducts based on order-sorted algebras. The semantics of this extension is deﬁned in terms of standard OCL.",
        "keywords": [
            "OCL",
            "UML",
            "Invariability",
            "Frame problem",
            "Views"
        ],
        "authors": [
            "Piotr Kosiuczenko"
        ],
        "file_path": "data/sosym-all/s10270-011-0215-y.pdf"
    },
    {
        "title": "Applying MDD in the content management system domain\nScenarios, tooling, and a mixed-method empirical assessment",
        "submission-date": "2020/03",
        "publication-date": "2021/02",
        "abstract": "Content management systems (CMSs) such as Joomla and WordPress dominate today’s web. Enabled by standardized\nextensions, administrators can build powerful web applications for diverse customer demands. However, developing CMS\nextensions requires sophisticated technical knowledge, and the complex code structure of an extension gives rise to errors\nduring typical development and migration scenarios. Model-driven development (MDD) seems to be a promising paradigm\nto address these challenges; however, it has not found adoption in the CMS domain yet. Systematic evidence of the beneﬁt of\napplying MDD in this domain could facilitate its adoption; however, an empirical investigation of this beneﬁt is currently lack-\ning. In this paper, we present a mixed-method empirical investigation of applying MDD in the CMS domain, based on an\ninterview suite, a controlled experiment, a ﬁeld experiment, and case studies. During the experiments, we used JooMDD,\nan MDD infrastructure instantiation for CMS extensions. This infrastructure, which is also presented in this work, consists\nof a DSL with model editors, code generators, and reverse engineering facilities. We consider three scenarios of developing\nnew (both independent and dependent) CMS extensions and of migrating existing ones to a new major platform version.\nThe experienced developers in our interviews acknowledge the relevance of these scenarios and report on experiences that\render them suitable candidates for a successful application of MDD. We found a particularly high relevance of the migration\nscenario. Our experiments largely conﬁrm the potentials and limits of MDD as identiﬁed for other domains. In particular,\nwe found a productivity increase up to factor 11.7 and a quality increase up to factor 2.4 during the development of CMS\nextensions. Furthermore, our observations highlight the importance of good tooling that seamlessly integrates with already\nused tool environments and processes.",
        "keywords": [
            "Model-driven development",
            "Content management systems",
            "Empirical assessment"
        ],
        "authors": [
            "Dennis Priefer",
            "Wolf Rost",
            "Daniel Strüber",
            "Gabriele Taentzer",
            "Peter Kneisel"
        ],
        "file_path": "data/sosym-all/s10270-021-00872-3.pdf"
    },
    {
        "title": "The sustainability assessment framework toolkit: a decade of modeling experience",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Software intensive systems play a crucial role in most, if not all, aspects of modern society. As such, both their sustainability and their role in supporting sustainable processes must be realized by design. To this aim, the architecture of software intensive systems should be designed to support sustainability goals; and measured to understand how effectively they do so. In this paper, we present the sustainability assessment framework (SAF) Toolkit—a set of instruments we developed to support software architects and design decision makers in modeling sustainability as a software quality property. The SAF Toolkit is the result of our experience gained in more than a decade of case studies in collaboration with industrial partners. We illustrate the toolkit with examples that come from some of such studies. We extract our lessons learned, our current research, and future plans to extend the SAF Toolkit for further architecture modeling and measurement.",
        "keywords": [
            "Software architecture design",
            "Software sustainability",
            "Quality assessment",
            "Framework"
        ],
        "authors": [
            "Patricia Lago",
            "Nelly Condori Fernandez",
            "Iffat Fatima",
            "Markus Funke",
            "Ivano Malavolta"
        ],
        "file_path": "data/sosym-all/s10270-024-01230-9.pdf"
    },
    {
        "title": "Inter-modelling with patterns",
        "submission-date": "2009/11",
        "publication-date": "2011/02",
        "abstract": "Inter-modelling is the activity of modelling relations between two or more modelling languages. The result of this activity is a model that describes the way in which model instances of these languages can be related. Many tasks in model-driven development can be classified as inter-modelling, for example designing model-to-model transformations, defining model matching and traceability relations, specifying model merging and model weaving, as well as describing mechanisms for inter-model consistency management and model synchronization. This paper presents our approach to inter-modelling in a declarative, relational, visual, and formal style. The approach relies on declarative patterns describing allowed or forbidden relations between two modelling languages. Such specification is then compiled into different operational mechanisms that are tailor-made for concrete inter-modelling scenarios. Up to now, we have used the approach to generate forward and backward transformations from a pattern specification. In this paper we demonstrate that the same specification can be used to derive mechanisms for other inter-modelling tasks, such as model matching and model traceability. In these scenarios the goals are generating the traces between two existing models, checking whether two models are correctly traced, and modifying the traces between two models if they are incorrect.",
        "keywords": [
            "Inter-modelling",
            "Model-to-model transformation",
            "Model matching",
            "Traceability",
            "Graph transformation",
            "Graph constraints"
        ],
        "authors": [
            "Esther Guerra",
            "Juan de Lara",
            "Fernando Orejas"
        ],
        "file_path": "data/sosym-all/s10270-011-0192-1.pdf"
    },
    {
        "title": "A reference requirements set for public service provision enterprise architectures",
        "submission-date": "2011/11",
        "publication-date": "2012/12",
        "abstract": "Electronic Government (eGov) is a political priority worldwide. One of the core objectives of eGov is the online public services provision (PSP). However, many of eGov PSP systems fail in realizing their objectives. Enterprise Architectures (EA) could contribute to overcome some of the relevant obstacles. The objective of this paper is to derive a reference requirements set for eGov PSP that can be used in EA development. Aiming at capitalizing on existing knowledge, we conduct a systematic literature review on eGov PSP systems requirements. This results in identifying a unified requirements set, i.e. 186 requirements, and stakeholders set, i.e. 19 stakeholders, for eGov PSP systems. Based on these findings, we determine 16 overview use cases demonstrating the basic functionality of such systems. Our findings are modeled using ArchiMate 2.0 notation. The identified requirements set can be used by virtually any public organization providing public services for developing its own EA. As a result, it can lead to the reduction of eGov PSP project failures, the decrease of software development costs and the improvement of its effectiveness and quality. Furthermore, it can be used as a basis to develop a complete reference EA for the eGov PSP domain.",
        "keywords": [
            "E-government",
            "Public service provision",
            "Requirements engineering",
            "Enterprise architecture",
            "ArchiMate"
        ],
        "authors": [
            "Efthimios Tambouris",
            "Eleni Kaliva",
            "Michail Liaros",
            "Konstantinos Tarabanis"
        ],
        "file_path": "data/sosym-all/s10270-012-0303-7.pdf"
    },
    {
        "title": "Reference models: how can we leverage them?",
        "submission-date": "2021/10",
        "publication-date": "2021/11",
        "abstract": "This editorial reflects some observations from discussions at the MODELS 2021 conference and its workshops, which were held virtually in Japan during the period of October 10–15, 2021. As the Editors-in-Chief of the journal most associated with the interest of MODELS, we are very much appreciative of all the deep efforts of the organizers and their continued interest in collaborating with SoSyM. A large number (34) of members from the modeling community were involved in the general organization of MODELS 2021. Our deepest gratitude goes to the General Chairs, Zhenjiang Hu (Peking University), Tomoji Kishi (Waseda University), and Naoyasu Ubayashi (Kyushu University), as well as the PC chairs, Shiva Nejati (University of Ottawa) and Daniel Varro (McGill University as well as Budapest University of Tech-nology and Economics). The organization was superb, and the success of the conference suggests that topics related to software and systems modeling continue to grow in interest each year. The diversity of topics around the idea of modeling in development and understanding of software and systems becomes broader with each conference edition. We were also very excited to extend the collaboration with MODELS through the Journal First option (a record 30 SoSyM papers were presented in MODELS sessions) and the continuation of the Most Impactful Papers (MIP) from the past decade (a list of the MIP awards from the current and past years can be found at https://www.sosym.org/awards). During the informal period in one of the sessions, there was a discussion on the definition of “reference models” and what purposes they serve in general use. To prime this discussion, we provide below the definition from Wikipedia: A reference model—in systems, enterprise, and software engineering—is an abstract framework or domain-specific ontology consisting of an interlinked set of clearly defined concepts produced by an expert or body of experts to encourage clear communication. A reference model can represent the component parts of any consistent idea, from business functions to system components, as long as it represents a complete set. This frame of reference can then be used to communicate ideas clearly among members of the same community.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-021-00948-0.pdf"
    },
    {
        "title": "Circular systems engineering",
        "submission-date": "2023/12",
        "publication-date": "2024/02",
        "abstract": "The perception of the value and propriety of modern engineered systems is changing. In addition to their functional and\nextra-functional properties, nowadays’ systems are also evaluated by their sustainability properties. The next generation of\nsystems will be characterized by an overall elevated sustainability—including their post-life, driven by efﬁcient value retention\nmechanisms. Current systems engineering practices fall short of supporting these ambitions and need to be revised appropri-\nate. In this paper, we introduce the concept of circular systems engineering, a novel paradigm for systems sustainability,\nand deﬁne two principles to successfully implement it: end-to-end sustainability and bipartite sustainability. We outline typ-\nical organizational evolution patterns that lead to the implementation and adoption of circularity principles, and outline key\nchallenges and research opportunities.",
        "keywords": [
            "Circular economy",
            "Digital thread",
            "Digital twins",
            "Sustainability",
            "Systems engineering"
        ],
        "authors": [
            "Istvan David",
            "Dominik Bork",
            "Gerti Kappel"
        ],
        "file_path": "data/sosym-all/s10270-024-01154-4.pdf"
    },
    {
        "title": "A systematic approach to constructing families of incremental topology control algorithms using graph transformation",
        "submission-date": "2016/07",
        "publication-date": "2017/03",
        "abstract": "In the communication system domain, constructing and maintaining network topologies via topology control algorithms is an important crosscutting research area. Network topologies are usually modeled using attributed graphs whose nodes and edges represent the network nodes and their interconnecting links. A key requirement of topology control algorithms is to fulﬁll certain consistency and optimization properties to ensure a high quality of service. Still, fewattemptshavebeenmadetoconstructivelyintegratethese properties into the development process of topology control algorithms. Furthermore, even though many topology control algorithms share substantial parts (such as structural patterns or tie-breaking strategies), few works constructively leverage these commonalities and differences of topology control algorithms systematically. In previous work, we addressed the constructive integration of consistency proper- ties into the development process. We outlined a constructive, model-driven methodology for designing individual topol- ogy control algorithms. Valid and high-quality topologies are characterized using declarative graph constraints; topology control algorithms are speciﬁed using programmed graph transformation. We applied a well-known static analysis technique to reﬁne a given topology control algorithm in a way that the resulting algorithm preserves the speciﬁed graph constraints. In this paper, we extend our constructive methodology by generalizing it to support the speciﬁcation of families of topology control algorithms. To show the feasibility of our approach, we reengineering six exist- ing topology control algorithms and develop e-kTC, a novel energy-efﬁcient variant of the topology control algorithm kTC. Finally, we evaluate a subset of the speciﬁed topol- ogy control algorithms using a new tool integration of the graph transformation tool eMoflon and the Simonstra- tor network simulation framework.",
        "keywords": [
            "Graph transformation",
            "Graph constraints",
            "Static analysis",
            "Model-driven engineering",
            "Wireless networks",
            "Network simulation"
        ],
        "authors": [
            "Roland Kluge",
            "Michael Stein",
            "Gergely Varró",
            "Andy Schürr",
            "Matthias Hollick",
            "Max Mühlhäuser"
        ],
        "file_path": "data/sosym-all/s10270-017-0587-8.pdf"
    },
    {
        "title": "Early-stage analysis of cyber-physical production systems through collaborative modelling",
        "submission-date": "2018/09",
        "publication-date": "2019/09",
        "abstract": "This paper demonstrates the flexible methodology of modelling cyber-physical systems (CPSs) using the INTO-CPS tech-nology through co-simulation based on Functional Mock-up Units (FMUs). It explores a novel method with two main co-simulation phases: homogeneous and heterogeneous. In the first phase, high-level, abstract FMUs are produced for all subsystems using a single discrete-event formalism (the VDM-RT language and Overture tool). This approach permits early co-simulation of system-level behaviours and serves as a basis for dialogue between subsystem teams and agreement on interfaces. During the second phase, model refinements of subsystems are gradually introduced, using various simulation tools capable of exporting FMUs. This heterogeneous phase permits high-fidelity models of all subsystems to be produced in appropriate formalisms. This paper describes the use of this methodology to develop a USB stick production line, representing a smart system of systems. The experiments are performed under the assumption that the orders are received in a Gaussian or Uniform distribution. The focus is on the homogeneous co-simulation phase, for which the method demonstrates two important roles: first, the homogeneous phase identifies the right interaction protocols (signals) among the various subsys-tems, and second, the conceptual (system-level) parameters identified before the heterogeneous co-simulation phase reduce the huge size of the design space and create stable constraints, later reflected in the physical implementation.",
        "keywords": [
            "Co-simulation",
            "Cyber-physical production systems",
            "Homogeneous and heterogeneous modelling",
            "Design space exploration"
        ],
        "authors": [
            "Mihai Neghina",
            "Constantin-Bala Zamfirescu",
            "Ken Pierce"
        ],
        "file_path": "data/sosym-all/s10270-019-00753-w.pdf"
    },
    {
        "title": "Advanced and efﬁcient execution trace management for executable domain-speciﬁc modeling languages",
        "submission-date": "2016/06",
        "publication-date": "2017/05",
        "abstract": "Executable Domain-Speciﬁc Modeling Lang-uages (xDSMLs) enable the application of early dynamic veriﬁcation and validation (V&V) techniques for behavioral models. At the core of such techniques, execution traces are used to represent the evolution of models during their execu-tion. In order to construct execution traces for any xDSML, generic trace metamodels can be used. Yet, regarding trace manipulations, generic trace metamodels lack efﬁciency in time because of their sequential structure, efﬁciency in memory because they capture superﬂuous data, and usability because of their conceptual gap with the considered xDSML. Our contribution is a novel generative approach that deﬁnes a multidimensional and domain-speciﬁc trace metamodel enabling the construction and manipulation of execution traces for models conforming to a given xDSML. Efﬁciency in time is improved by providing a variety of navigation paths within traces, while usability and memory are improved by narrowing the scope of trace metamodels to ﬁt the consid-ered xDSML. We evaluated our approach by generating a trace metamodel for fUML and using it for semantic differ-encing, which is an important V&V technique in the realm of model evolution. Results show a signiﬁcant performance improvement and simpliﬁcation of the semantic differencing rules as compared to the usage of a generic trace metamodel.",
        "keywords": [
            "Model execution",
            "Domain-speciﬁc languages",
            "Execution trace"
        ],
        "authors": [
            "Erwan Bousse",
            "Tanja Mayerhofer",
            "Benoit Combemale",
            "Benoit Baudry"
        ],
        "file_path": "data/sosym-all/s10270-017-0598-5.pdf"
    },
    {
        "title": "Reusing metamodels and notation with Diagram Deﬁnition",
        "submission-date": "2014/10",
        "publication-date": "2016/06",
        "abstract": "It is increasingly common for language speciﬁ-cations to describe visual forms (concrete syntax) separately from underlying concepts (abstract syntax). This is typically to enable interchange of visual information between graphi-cal modeling tools, such as positions of nodes and routings of lines. Often overlooked is that separation of visual forms and abstract concepts enables languages to deﬁne multiple visual forms for the same underlying concepts and for the same visual form to be used for similar underlying concepts in different languages (many-to-many relationships between concrete and abstract syntax). Visual forms can be adapted to communities using different notations for the same con-cepts and can be used to integrate communities using the same notation for similar concepts. Models of concrete syn-tax have been available for some time, but are rarely used to capture these many-to-many relationships with abstract syntax. This paper shows how to model these relationships using concrete graphical syntax expressed in the Diagram Deﬁnition standard, examining cases drawn from the Uni-ﬁed Modeling Language and the Business Process Model and Notation. This gives deﬁners of graphical languages a way to specify visual forms for multiple communities.",
        "keywords": [
            "Notation",
            "Metamodel",
            "Diagram Deﬁnition",
            "Syntax"
        ],
        "authors": [
            "Conrad Bock",
            "Maged Elaasar"
        ],
        "file_path": "data/sosym-all/s10270-016-0537-x.pdf"
    },
    {
        "title": "Implementing a graph transformation engine in relational databases",
        "submission-date": "2004/11",
        "publication-date": "2006/06",
        "abstract": "We present a novel approach to implement a graph transformation engine based on standard relational database management systems (RDBMSs). The essence of the approach is to create database views for each rule and to handle pattern matching by inner join operations while handling negative application conditions by left outer join operations. Furthermore, the model manipulation prescribed by the application of a graph transformation rule is also implemented using elementary data manipulation statements (such as insert, delete). As a result, we obtain a robust and fast transformation engine especially suitable for (1) extending modeling tools with an underlying RDBMS repository and (2) embedding model transformations into large distributed applications where models are frequently persisted in a relational database and transaction handling is required to handle large models consistently.",
        "keywords": [
            "Tool support",
            "Graph transformation",
            "Pattern matching",
            "Relational databases"
        ],
        "authors": [
            "Gergely Varró",
            "Katalin Friedl",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-006-0015-y.pdf"
    },
    {
        "title": "Formalizing requirements with object models and temporal constraints",
        "submission-date": "2009/03",
        "publication-date": "2009/08",
        "abstract": "Flaws in requirements often have a negative impact on the subsequent development phases. In this paper, we present a novel approach for the formal representation and validation of requirements, which we used in an industrial project. The formalism allows us to represent and reason about object models and their temporal evolution. The key ingredients are class diagrams to represent classes of objects, their relationships and their attributes, fragments of ﬁrst order logic to constrain the possible conﬁgurations of such objects, and temporal logic operators to deal with the dynamic evolution of the conﬁgurations. The approach to formal validation allows to check whether the requirements are consistent, if they are compatible with some scenarios, and if they guarantee some implicit properties. The validation procedure is based on satisﬁability checking, which is carried out by means of ﬁnite instantiation and model checking techniques.",
        "keywords": [
            "Formal requirement engineering",
            "Temporal logic",
            "Railway domain",
            "European Train Control System (ETCS)"
        ],
        "authors": [
            "Alessandro Cimatti",
            "Marco Roveri",
            "Angelo Susi",
            "Stefano Tonetta"
        ],
        "file_path": "data/sosym-all/s10270-009-0130-7.pdf"
    },
    {
        "title": "Kompren: modeling and generating model slicers",
        "submission-date": "2012/03",
        "publication-date": "2012/11",
        "abstract": "Among model comprehension tools, model slicers are tools that extract a subset of model elements, for a specific purpose. Model slicers provide a mechanism to isolate and focus on parts of the model, thereby improving the overall analysis process. However, existing slicers are dedicated to a specific modeling language. This is an issue when we observe that new domain specific modeling languages, for which we want slicing abilities, are created almost on a daily basis. This paper proposes the Kompren language to model and generate model slicers for any DSL (e.g.modelingforsoftwaredevelopmentorforcivilengineer-ing) and for different purposes (e.g. monitoring and model comprehension). We detail the semantics of the Kompren language and of the model slicer generator. This provides a set of expected properties about the slices that are extracted by the different forms of the slicer. Then we illustrate these different forms of slicers on case studies from various domains.",
        "keywords": [
            "Model slicing",
            "Domain specific language"
        ],
        "authors": [
            "Arnaud Blouin",
            "Benoît Combemale",
            "Benoit Baudry",
            "Olivier Beaudoux"
        ],
        "file_path": "data/sosym-all/s10270-012-0300-x.pdf"
    },
    {
        "title": "CoqTL: a Coq DSL for rule-based model transformation",
        "submission-date": "2018/12",
        "publication-date": "2019/11",
        "abstract": "In model-driven engineering, model transformation (MT) veriﬁcation is essential for reliably producing software artifacts.\nWhile recent advancements have enabled automatic Hoare-style veriﬁcation for non-trivial MTs, there are certain veriﬁcation\ntasks (e.g. induction) that are intrinsically difﬁcult to automate. Existing tools that aim at simplifying the interactive veriﬁcation\nof MTs typically translate the MT speciﬁcation (e.g. in ATL) and properties to prove (e.g. in OCL) into an interactive theorem\nprover. However, since the MT speciﬁcation and proof phases happen in separate languages, the proof developer needs a\ndetailed knowledge of the translation logic. Naturally, any error in the MT translation could cause unsound veriﬁcation, i.e. the\nMT executed in the original environment may have different semantics from the veriﬁed MT. We propose an alternative solution\nby designing and implementing an internal domain-speciﬁc language, namely CoqTL, for the speciﬁcation of declarative MTs\ndirectly in the Coq interactive theorem prover. Expressions in CoqTL are written in Gallina (the speciﬁcation language of Coq),\nincreasing the possibilities of reusing native Coq libraries in the transformation deﬁnition and proof. CoqTL speciﬁcations\ncan be directly executed by our transformation engine encoded in Coq, or a certiﬁed implementation of the transformation\ncan be generated by the native Coq extraction mechanism. We ensure that CoqTL has the same expressive power of Gallina\n(i.e. if a MT can be computed in Gallina, then it can also be represented in CoqTL). In this article, we introduce CoqTL,\nevaluate its practical applicability on a use case, and identify its current limitations.",
        "keywords": [
            "Model-driven engineering",
            "Model transformation",
            "Domain-speciﬁc language",
            "Interactive theorem proving",
            "Coq"
        ],
        "authors": [
            "Zheng Cheng",
            "Massimo Tisi",
            "Rémi Douence"
        ],
        "file_path": "data/sosym-all/s10270-019-00765-6.pdf"
    },
    {
        "title": "Low-code development and model-driven engineering: Two sides of the same coin?",
        "submission-date": "2021/09",
        "publication-date": "2022/01",
        "abstract": "The last few years have witnessed a signiﬁcant growth of so-called low-code development platforms (LCDPs) both in gaining traction on the market and attracting interest from academia. LCDPs are advertised as visual development platforms, typically running on the cloud, reducing the need for manual coding and also targeting non-professional programmers. Since LCDPs share many of the goals and features of model-driven engineering approaches, it is a common point of debate whether low-code is just a new buzzword for model-driven technologies, or whether the two terms refer to genuinely distinct approaches. To contribute to this discussion, in this expert-voice paper, we compare and contrast low-code and model-driven approaches, identifying their differences and commonalities, analysing their strong and weak points, and proposing directions for cross-pollination.",
        "keywords": [
            "Low-code development",
            "No-code development",
            "Model-driven engineering"
        ],
        "authors": [
            "Davide Di Ruscio",
            "Dimitris Kolovos",
            "Juan de Lara",
            "Alfonso Pierantonio",
            "Massimo Tisi",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-021-00970-2.pdf"
    },
    {
        "title": "Editorial to theme section on open environmental software systems modeling",
        "submission-date": "2022/04",
        "publication-date": "2022/08",
        "abstract": "This theme section aims to disseminate the latest research results in the area of open environmental software systems modeling. Software-intensive systems, such as cyber-physical systems and self-adaptive systems, need to be able to constantly operate in open and dynamic environments, which requires them to continuously evolve, handle internal and external uncertainties, and so on. Traditional software engineering methodologies need to be extended for tackling these new challenges, and modeling is a promising technique to do that. This theme section contains two papers that tackle these issues, that were accepted after a thorough peer-reviewing process. Moreover, it contains an “Expert’s Voice” contribution on the uncertainty interaction problem, which is a relevant issue in the systems considered in this theme section.",
        "keywords": [
            "System modeling",
            "Uncertainty",
            "Open environment",
            "Autonomous",
            "Self-adaptation"
        ],
        "authors": [
            "Tao Yue",
            "Paolo Arcaini",
            "Ji Wu",
            "Xiaowei Huang"
        ],
        "file_path": "data/sosym-all/s10270-022-01032-x.pdf"
    },
    {
        "title": "Model-based design: a report from the trenches of the DARPA Urban Challenge",
        "submission-date": "2008/10",
        "publication-date": "2009/03",
        "abstract": "The impact of model-based design on the software engineering community is impressive, and recent research in model transformations, and elegant behavioral speciﬁcations of systems has the potential to revolutionize the way in which systems are designed. Such techniques aim to raise the level of abstraction at which systems are speciﬁed, to remove the burden of producing application-speciﬁc programs with general-purpose programming. For complex real-time systems, however, the impact of model-driven approaches is not nearly so widespread. In this paper, we present a perspective of model-based design researchers who joined with software experts in robotics to enter the DARPA Urban Challenge, and to what extent model-based design techniques were used. Further, we speculate on why, according to our experience and the testimonies of many teams, the full promises of model-based design were not widely realized for the competition. Finally, we present some thoughts for the future of model-based design in complex systems such as these, and what advancements in modeling are needed to motivate small-scale projects to use model-based design in these domains.",
        "keywords": [],
        "authors": [
            "Jonathan Sprinkle",
            "J. Mikael Eklund",
            "Humberto Gonzalez",
            "Esten Ingar Grøtli",
            "Ben Upcroft",
            "Alex Makarenko",
            "Will Uther",
            "Michael Moser",
            "Robert Fitch",
            "Hugh Durrant-Whyte",
            "S. Shankar Sastry"
        ],
        "file_path": "data/sosym-all/s10270-009-0116-5.pdf"
    },
    {
        "title": "On model compatibility with referees and contexts",
        "submission-date": "2011/03",
        "publication-date": "2012/04",
        "abstract": "A model-based engineering discipline presup-\nposes that models are organised by creating relationships\nbetween them. While there has been considerable work on\nunderstanding what it means to instantiate one model from\nanother, little is known about when a model should be con-\nsidered to be a specialisation of another one. This article\nmotivates and discusses ways of deﬁning specialisation rela-\ntionships between models, languages, and transformations\nrespectively. Consideration is given to both structural and\nbehavioural compatibility concerns. Several alternatives of\ndefining a specialisation relationship are considered and dis-\ncussed. The article furthermore discusses the notions of ref-\neree and context in order to validate and define specialisation\nrelationships. The ideas and discussions presented in this arti-\ncle are meant to provide a further stepping stone towards a\nsystematic basis for organising models.",
        "keywords": [
            "Model inheritance",
            "Model compatibility",
            "Language engineering",
            "Model evolution",
            "Subtyping",
            "Refinement",
            "Referee",
            "Context"
        ],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-012-0241-4.pdf"
    },
    {
        "title": "Encoding process discovery problems in SMT",
        "submission-date": "2014/10",
        "publication-date": "2016/06",
        "abstract": "Information systems, which are responsible for\ndriving many processes in our lives (health care, the web,\nmunicipalities, commerce and business, among others), store\ninformation in the form of logs which is often left unused.\nProcess mining, a discipline in between data mining and\nsoftware engineering, proposes tailored algorithms to exploit\nthe information stored in a log, in order to reason about the\nprocesses underlying an information system. A key challenge\nin process mining is discovery: Given a log, derive a formal\nprocess model that can be used afterward for a formal analy-\nsis. In this paper, we provide a general approach based on\nsatisﬁability modulo theories (SMT) as a solution for this\nchallenging problem. By encoding the problem into the log-\nical/arithmetic domains and using modern SMT engines, it\nis shown how two separate families of process models can\nbe discovered. The theory of this paper is accompanied with\na tool, and experimental results witness the signiﬁcance of\nthis novel view of the process discovery problem.",
        "keywords": [
            "Process discovery",
            "SMT application",
            "Causal\nnets",
            "Petri nets"
        ],
        "authors": [
            "Marc Solé\nJosep Carmona"
        ],
        "file_path": "data/sosym-all/s10270-016-0536-y.pdf"
    },
    {
        "title": "Generating domain models from natural language text using NLP: a benchmark dataset and experimental comparison of tools",
        "submission-date": "2023/08",
        "publication-date": "2024/05",
        "abstract": "Software requirements speciﬁcation describes users’ needs and expectations on some target system. Requirements documents are typically represented by unstructured natural language text. Such texts are the basis for the various subsequent activities in software development, such as software analysis and design. As part of software analysis, domain models are made that describe the key concepts and relations between them. Since the analysis process is performed manually by business analysts, it is time-consuming and may introduce mistakes. Recently, researchers have worked toward automating the synthesis of domain models from textual software requirements. Current studies on this topic have limitations in terms of the volume and heterogeneity of experimental datasets. To remedy this, we provide a curated dataset of software requirements to be utilized as a benchmark by algorithms that transform textual requirements documents into domain models. We present a detailed evaluation of two text-to-model approaches: one based on a large-language model (ChatGPT) and one building on grammatical rules (txt2Model). Our evaluation reveals that both tools yield promising results with relatively high F-scores for modeling the classes, attributes, methods, and relationships, with txt2Model performing better than ChatGPT on average. Both tools have relatively lower performance and high variance when it comes to the relation types. We believe our dataset and experimental evaluation pave to way to advance the ﬁeld of automated model generation from requirements.",
        "keywords": [
            "Software functional requirements",
            "Software models",
            "Text-to-model transformation",
            "Benchmark dataset"
        ],
        "authors": [
            "Fatma Bozyigit",
            "Tolgahan Bardakci",
            "Alireza Khalilipour",
            "Moharram Challenger",
            "Guus Ramackers",
            "Önder Babur",
            "Michel R. V. Chaudron"
        ],
        "file_path": "data/sosym-all/s10270-024-01176-y.pdf"
    },
    {
        "title": "Theme issue on Integrated Formal Methods",
        "submission-date": "2015/11",
        "publication-date": "2016/MM",
        "abstract": "This theme issue of the Software and Systems Modeling journal is dedicated to the topic of Integrated Formal Methods. Formal methods allow the modeling and analysis of various aspects of a system. The aim of this theme issue is to provide a resource that describes the state of the art in integrated formal methods and to outline a roadmap that addresses key challenges in this area.",
        "keywords": [],
        "authors": [
            "Einar Broch Johnsen",
            "Luigia Petre"
        ],
        "file_path": "data/sosym-all/s10270-015-0510-0.pdf"
    },
    {
        "title": "UML customization versus domain-speciﬁc languages",
        "submission-date": "2018/06",
        "publication-date": "2018/06",
        "abstract": "We recently collaborated again with 40 developers on a large automotive architecture project. In addition, we discussed some of the characteristics of the project with other modelers and consultants. Several observations emerged that we would like to share.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-018-0685-2.pdf"
    },
    {
        "title": "Dirk Draheim, Gerald Weber: Form-oriented analysis. A new methodology to model form-based applications",
        "submission-date": "2005/05",
        "publication-date": "2005/05",
        "abstract": "In spite of the widespread use of Graphical User Interfaces (GUI), form-based user interfaces still prevail in professional software applications, especially in the business management area. Ordinary internet shopping also makes users navigate through a series of online forms while placing an order and specifying payment details. When working with online forms, users often encounter fatigue and frustration caused by poor design of the form. Form-based interaction is speciﬁc, because it preserves the unfashionable submit-response style interface, revitalized nowadays on the web in countless form-based applications. In principle, they should be operated on a self-service assumption, enabling users to complete their tasks without external help, without frustration and in a minimal number of necessary steps.",
        "keywords": [],
        "authors": [
            "Marcin Sikorski"
        ],
        "file_path": "data/sosym-all/s10270-005-0080-7.pdf"
    },
    {
        "title": "Testing cockpit display systems of aircraft using a model-based approach",
        "submission-date": "2020/02",
        "publication-date": "2021/01",
        "abstract": "Avionics are highly critical systems that require extensive testing to comply with international safety standards. Cockpit display systems (CDS) are a mandatory part of modern cockpits of both manned and unmanned aircraft. The information from various avionics components is displayed on CDS using a variety of ﬂight instruments. An important part of testing avionics systems is to evaluate whether the displayed information on the CDS is correct or not. A common industrial practice is to manually test CDS, which is time-consuming, labor-intensive, and error-prone. In this paper, we propose a model-based approach to automate the CDS testing of aircraft. The proposed approach tests the CDS at two levels: (i) at the system level to verify that the CDS are working correctly and (ii) at system integration level of CDS when these are integrated with various avionics components. As a part of our approach, we develop a UML proﬁle to model various elements of the CDS. The models are then used to support the automated testing process. We evaluate our approach on two industrial case studies, the ﬁrst case study represents a primary ﬂight display (PFD) of an aircraft and the second one is the CDS of the ground control station (GCS-CDS) of an unmanned aerial vehicle. The evaluation results show that three potential faults are identiﬁed in the PFD and four major faults are found in the GCS-CDS.",
        "keywords": [
            "Model-based testing (MBT)",
            "Safety-critical systems",
            "Cockpit display systems (CDS)",
            "Unmanned aerial vehicle (UAV)",
            "Ground control station (GCS)"
        ],
        "authors": [
            "Hassan Sartaj",
            "Muhammad Zohaib Iqbal",
            "Muhammad Uzair Khan"
        ],
        "file_path": "data/sosym-all/s10270-020-00844-z.pdf"
    },
    {
        "title": "Accelerating task completion in mobile ofﬂoading systems through adaptive restart",
        "submission-date": "2015/06",
        "publication-date": "2016/05",
        "abstract": "Mobile application ofﬂoading is an efﬁcient technique to unload the burden of intensive computation from thin clients to powerful servers. In a mobile ofﬂoading system, cloud computing is utilized to complete some heavy tasks which are migrated from resource-constrained mobile devices to the Cloud. To assure system performance, the quality of the wireless network connection plays an important role. In previous work we experimentally explored the impact of packet loss and delay in wireless networks on the completion time of an ofﬂoading task. We investigated a local restart mechanism to mitigate these effects. In the presence of unreliable communication, once the waiting time for the response of a cloud server exceeds a given threshold, exploiting the local resources of a mobile client can accelerate the task completion. In this paper, we upgrade the restart mechanism by allowing several ofﬂoading retries before a job eventually is locally restarted and ﬁnally completed in the client device itself. This is an adaptive restart scheme which aims ﬁrst at completing the job using restart with ofﬂoading. If several successive ofﬂoading attempts fail the job is completed locally. Adaptively selecting the right retry threshold and automatically restarting at the appropriate moment can balance out undesired effects. This paper extends Wang and Wolter (Proceedings of the 6th ACM/SPEC international conference on performance engineering. ACM, pp 3–13, 2015) by adding an adaptive retry scheme, a mathematical derivation of the optimal limit for ofﬂoading attempts so as to minimize the task completion time using a greedy method, and by the results of a practical evaluation study which shows the efﬁciency and beneﬁts of the adaptive restart scheme.",
        "keywords": [
            "Mobile ofﬂoading",
            "Restart",
            "Unreliable network"
        ],
        "authors": [
            "Qiushi Wang",
            "Katinka Wolter"
        ],
        "file_path": "data/sosym-all/s10270-016-0531-3.pdf"
    },
    {
        "title": "Use Case Maps as a property speciﬁcation language",
        "submission-date": "2006/11",
        "publication-date": "2007/12",
        "abstract": "Although a signiﬁcant body of research in the\narea of formal veriﬁcation and model checking tools of soft-\nware and hardware systems exists, the acceptance of these\ntools by industry and end-users is rather limited. Beside the\ntechnical problem of state space explosion, one of the main\nreasons for this limited acceptance is the unfamiliarity of\nusers with the required speciﬁcation notation. Requirements\nhave to be typically expressed as temporal logic formalisms\nand notations. Property speciﬁcation patterns were success-\nfully introduced to bridge this gap between users and model\nchecking tools. They also enable non-experts to write formal\nspeciﬁcations that can be used for automatic model checking.\nIn this paper, we propose an abstract high level pattern-based\napproach to the description of property speciﬁcations based\non Use Case Maps (UCM). We present a set of commonly\nused properties with their speciﬁcations that are described in\nterms of occurrence, ordering and temporal scopes of actions.\nFurthermore, our approach also supports the description of\nproperties with respect to their architectural scope. We pro-\nvide a mapping of our UCM property speciﬁcation patterns in\nterms of CTL, TCTL and Architectural TCTL (ArTCTL), an\nextension to TCTL, introduced in this research that provides\ntemporal logics with architectural scopes. We illustrate the\nuse of our pattern system for requirement speciﬁcations of\nan IP Header compression feature.",
        "keywords": [
            "Formal veriﬁcation",
            "Temporal logic",
            "Property speciﬁcation",
            "Use Case Maps",
            "Temporal and architectural scope"
        ],
        "authors": [
            "Jameleddine Hassine",
            "Juergen Rilling",
            "Rachida Dssouli"
        ],
        "file_path": "data/sosym-all/s10270-007-0076-6.pdf"
    },
    {
        "title": "A manifesto for applicable formal methods",
        "submission-date": "2023/06",
        "publication-date": "2023/08",
        "abstract": "Recently, formal methods have been used in large industrial organisations (including AWS, Facebook/Meta, and Microsoft) and have proved to be an effective part of a software engineering process ﬁnding important bugs. Perhaps because of that, practitioners are interested in using them more often. Nevertheless, formal methods are far less applied than expected, particularly for safety-critical systems where they are strongly recommended and have the most signiﬁcant potential. We hypothesise that formal methods still seem not applicable enough or ready for their intended use in such areas. In critical software engineering, what do we mean when we speak of a formal method? And what does it mean for such a method to be applicable both from a scientiﬁc and practical viewpoint? Based on what the literature tells about the ﬁrst question, with this manifesto, we identify key challenges and lay out a set of guiding principles that, when followed by a formal method, give rise to its mature applicability in a given scope. Rather than exercising criticism of past developments, this manifesto strives to foster increased use of formal methods in any appropriate context to the maximum beneﬁt.",
        "keywords": [
            "Formal methods",
            "Formal veriﬁcation",
            "Software engineering",
            "Tools",
            "Research evaluation",
            "Research transfer"
        ],
        "authors": [
            "Mario Gleirscher",
            "Jaco van de Pol",
            "Jim Woodcock"
        ],
        "file_path": "data/sosym-all/s10270-023-01124-2.pdf"
    },
    {
        "title": "Guest editorial to the special issue on MODELS 2008",
        "submission-date": "2011/05",
        "publication-date": "Not found",
        "abstract": "Welcome to this special issue of “Software and Systems Modeling,” devoted to selected papers of the eleventh ACM/IEEE International Conference on Model-Driven Engineering Languages and Systems (MODELS’08). MODELS is the premier annual conference featuring research results and practical experiences in model-driven engineering of complex, software-intensive systems.",
        "keywords": [],
        "authors": [
            "Krzysztof Czarnecki"
        ],
        "file_path": "data/sosym-all/s10270-011-0202-3.pdf"
    },
    {
        "title": "Meta-environment and executable meta-language using smalltalk: an experience report",
        "submission-date": "2007/03",
        "publication-date": "2008/03",
        "abstract": "Object-oriented modelling languages such as\nEMOF are often used to specify domain speciﬁc meta-\nmodels. However, these modelling languages lack the abil-\nity to describe behavior or operational semantics. Several\napproaches have used a subset of Java mixed with OCL\nas executable meta-languages. In this experience report we\nshow how we use Smalltalk as an executable meta-language\nin the context of the Moose reengineering environment. We\npresent how we implemented EMOF and its behavioral\naspects. Over the last decade we validated this approach\nthrough incrementally building a meta-described reengineer-\ning environment. Such an approach bridges the gap between\na code-oriented view and a meta-model driven one. It avoids\nthe creation of yet another language and reuses the infrastruc-\ntureandrun-timeoftheunderlyingimplementationlanguage.\nIt offers an uniform way of letting developers focus on their\ntasks while at the same time allowing them to meta-describe\ntheir domain model. The advantage of our approach is that\ndevelopers use the same tools and environment they use for\ntheir regular tasks. Still the approach is not Smalltalk speciﬁc\nbut can be applied to language offering an introspective API\nsuch as Ruby, Python, CLOS, Java and C#.",
        "keywords": [
            "Meta behavior description",
            "Reﬂective\nlanguage",
            "Executable modeling language",
            "Smalltalk"
        ],
        "authors": [
            "Stéphane Ducasse",
            "Tudor Girba",
            "Adrian Kuhn",
            "Lukas Renggli"
        ],
        "file_path": "data/sosym-all/s10270-008-0081-4.pdf"
    },
    {
        "title": "Using reactive links to propagate changes across engineering models",
        "submission-date": "2023/05",
        "publication-date": "2024/06",
        "abstract": "Collaborative model-driven development is a de facto practice to create software-intensive systems in several domains (e.g., aerospace, automotive, and robotics). However, when multiple engineers work concurrently, keeping all model artifacts synchronized and consistent is difﬁcult. This is even harder when the engineering process relies on a myriad of tools and domains (e.g., mechanic, electronic, and software). Existing work tries to solve this issue from different perspectives, such as using trace links between different artifacts or computing change propagation paths. However, these solutions mainly provide additional information to engineers, still requiring manual work for propagating changes. Yet, most modeling tools are limited regarding the traceability between different domains, while also lacking the efﬁciency and granularity required during the development of software-intensive systems. Motivated by these limitations, in this work, we present a solution based on what we call “reactive links”, which are highly granular trace links that propagate change between property values across models in different domains, managed in different tools. Differently from traditional “passive links”, reactive links automatically propagate changes when engineers modify models, assuring the synchronization and consistency of the artifacts. The feasibility, performance, and ﬂexibility of our solution were evaluated in three practical scenarios, from two partner organizations. Our solution is able to resolve all cases in which change propagation among models were required. We observed a great improvement of efﬁciency when compared to the same propagation if done manually. The contribution of this work is to enhance the engineering of software-intensive systems by reducing the burden of manually keeping models synchronized and avoiding inconsistencies that potentially can originate from collaborative engineering in a variety of tool from different domains.",
        "keywords": [
            "Change propagation",
            "Model-driven engineering",
            "Heterogeneous models",
            "Collaboration",
            "Multi-domain traceability"
        ],
        "authors": [
            "Cosmina-Cristina Ra¸tiu",
            "Wesley K. G. Assunção",
            "Edvin Herac",
            "Rainer Haas",
            "Christophe Lauwerys",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-024-01186-w.pdf"
    },
    {
        "title": "Report on the State of the SoSyM Journal (2024 summary)",
        "submission-date": "2025/02",
        "publication-date": "2025/02",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Stéphanie Challita",
            "Benoit Combemale",
            "Huseyin Ergin",
            "JeﬀGray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-025-01268-3.pdf"
    },
    {
        "title": "Business processes in the agile organisation: a socio-technical perspective",
        "submission-date": "2014/10",
        "publication-date": "2015/11",
        "abstract": "This paper takes a cross-disciplinary view of the ontology of “business process”: how the concept is treated in the IS research literature and how related concepts (with stronger human behavioural orientation) from organisation and management sciences can potentially inform this IS perspective. In particular, is there room for socio-technical concepts such as technology affordance, derived from the constructivist tradition, in improving our understanding of operational business processes, particularly human-centric business processes? The paper presents a theoretical framework for understanding the role of business processes in organisational agility that distinguishes between the process-as-designed and the process-as-practiced. How this practice aspect of business processes also leads to the improvisation of various information technology enablers, is explored using a socio-technical lens. The posited theoretical framework is illustrated and validated with data drawn from an interpretive empirical case study of a large IT services company. The research suggests that processes within the organisation evolve both by top-down design and by the bottom-up routinisation of practice and that the tension between these is driven by the need for ﬂexibility.",
        "keywords": [
            "Socio-technical systems",
            "Organisational agility",
            "Business process",
            "Technology affordance"
        ],
        "authors": [
            "Charles Crick",
            "Eng K. Chew"
        ],
        "file_path": "data/sosym-all/s10270-015-0506-9.pdf"
    },
    {
        "title": "Guest editorial to the theme issue on domain-speciﬁc modeling in theory and applications",
        "submission-date": "2014/01",
        "publication-date": "2014/01",
        "abstract": "Interest in domain-speciﬁc modeling (DSM) comes from the aspiration to signiﬁcantly improve the productivity and quality of software development by raising the level of abstraction beyond programming. This is done by specifying the solution directly using domain concepts, rather than lower level programminglanguageandspeciﬁcplatformconceptsthatintro-duce layers of accidental complexity. In the past, productivity gains have been sought from new programming languages. Today, domain-speciﬁc modeling languages (DSMLs) provide a solution for continuing to raise the level of abstraction beyond coding, making development faster and easier.",
        "keywords": [],
        "authors": [
            "Juha-Pekka Tolvanen",
            "Matti Rossi",
            "Jeff Gray"
        ],
        "file_path": "data/sosym-all/s10270-013-0319-7.pdf"
    },
    {
        "title": "Specifying dynamic software system architectures",
        "submission-date": "2020/10",
        "publication-date": "2021/03",
        "abstract": "The inexorable penetration of software into practically every facet of modern society calls for sophisticated architectural \nstyles, including ones that can support architectures with dynamically shifting structures, which are required to cope with \nthe dynamics of their applications. With the advent of modern Internet-based systems operating in real time, these types of \nsystems are becoming more widespread. Unfortunately, to date there has been insufficient theoretical work on suitable archi-\ntectural design patterns for such systems. This work describes two such patterns: the dynamic part pattern and the dynamic \nrole pattern, both of which have been proven in earlier-generation dynamic real-time systems. In addition to describing the \nform and semantics of these design patterns, this work proposes a notational form suitable for specifying them in component-\nand-connector-style architecture description languages in a clear and unambiguous manner. The practical application of the \ntwo patterns is illustrated using a running example.",
        "keywords": [
            "Architectural description languages",
            "Dynamic system structure",
            "Architectural design patterns"
        ],
        "authors": [
            "Bran Selić"
        ],
        "file_path": "data/sosym-all/s10270-021-00875-0.pdf"
    },
    {
        "title": "Towards Standardized benchmarks of LLMs in software modeling tasks: a conceptual framework",
        "submission-date": "2024/07",
        "publication-date": "Not found",
        "abstract": "The integration of Large Language Models (LLMs) in software modeling tasks presents both opportunities and challenges. This Expert Voice addresses a signiﬁcant gap in the evaluation of these models, advocating for the need for standardized benchmarking frameworks. Recognizing the potential variability in prompt strategies, LLM outputs, and solution space, we propose a conceptual framework to assess their quality in software model generation. This framework aims to pave the way for standardization of the benchmarking process, ensuring consistent and objective evaluation of LLMs in software modeling. Our conceptual framework is illustrated using UML class diagrams as a running example.",
        "keywords": [
            "Modeling",
            "LLMs",
            "Benchmarking"
        ],
        "authors": [
            "Javier Cámara",
            "Lola Burgueño",
            "Javier Troya"
        ],
        "file_path": "data/sosym-all/s10270-024-01206-9.pdf"
    },
    {
        "title": "Special section on ICMT at STAF 2018",
        "submission-date": "2019/12",
        "publication-date": "2020/01",
        "abstract": "Modelling is a key element in reducing the complexity of software systems during their development and maintenance. Model transformations are essential for elevating models from documentation elements to ﬁrst-class artifacts. Transformations also play a key role in analysing models to reveal conceptual ﬂaws or highlight quality bottlenecks and in integrating heterogeneous tools into uniﬁed tool chains.",
        "keywords": [],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Arend Rensink"
        ],
        "file_path": "data/sosym-all/s10270-020-00775-9.pdf"
    },
    {
        "title": "How do organizational capabilities mature? maturity level characteristics for organizational capabilities",
        "submission-date": "2024/11",
        "publication-date": "2025/06",
        "abstract": "In a dynamic business environment, organizations must continuously evolve to sustain a competitive edge by developing new capabilities while also enhancing the existing ones. Maturity models have become a useful management tool for enhancing organizational capabilities, offering a structured framework to assess and improve capabilities over time. By deﬁning distinct stages of development, maturity models help organizations identify their current state and plan future improvements. However, despite their widespread use in domains like software development and project management, a signiﬁcant theoretical gap remains in the conceptualization of maturity levels, especially those related to organizational capabilities. This paper presents a maturity level characterization model (MLCM) for organizational capabilities, addressing the gap in existing research by providing a model that can be used to develop capability maturity models. Drawing from the Dreyfus model of skill acquisition, this model deﬁnes six stages of maturity, ranging from unaware to expert, and offers a theoretically grounded instantiation template for developing maturity levels for organizational capabilities. The research utilizes design science research methodology, with the model being applied and evaluated in the context of advanced data analytics capabilities. Two evaluation questionnaires among 18 practitioners conﬁrm the model’s validity, relevance, completeness, clarity, and usefulness, providing a solid foundation for developing capability maturity levels. Theoretically, this research advances maturity modeling literature by conceptualizing organizational capability maturity by using the Dreyfus model of skill acquisition as a foundational meta-theory, and combining perspectives from institutional theory. Practically, it provides a useful tool for both researchers and practitioners aiming to evaluate and improve organizational capabilities in various domains.",
        "keywords": [
            "Capability maturity models",
            "Maturity levels",
            "Organizational capability",
            "Dreyfus model"
        ],
        "authors": [
            "Ginger Korsten",
            "Baris Ozkan",
            "Banu Aysolmaz",
            "Daan Mul",
            "Oktay Turetken"
        ],
        "file_path": "data/sosym-all/s10270-025-01309-x.pdf"
    },
    {
        "title": "A metamodeling language supporting subset and union properties",
        "submission-date": "2006/09",
        "publication-date": "2007/06",
        "abstract": "In this article, we describe successive versions of a metamodeling language using a set-theoretic formalization. We focus on language extension mechanisms, particularly on the relatively new subset and union properties of MOF 2.0 and the UML 2.0 Infrastructure. We use Liskov substitutability as the rationale for our formalization. We also show that property redeﬁnitions are not a safe language extension mechanism. Each language version provides new features,andwenotehowsuchfeaturescannotbemixedarbi-trarily. Instead, constraints over the metamodel and model structures must be established. We expect that this article provides a better understanding of the foundations of MOF 2.0, which is necessary to deﬁne new extensions, model transformation languages and tools.",
        "keywords": [
            "Subsets",
            "Unions",
            "Redeﬁnitions",
            "UML",
            "Package merges",
            "Extension mechanisms",
            "Graphs",
            "Graph theory",
            "Software modeling languages",
            "Metamodeling",
            "MOF"
        ],
        "authors": [
            "Marcus Alanen",
            "Ivan Porres"
        ],
        "file_path": "data/sosym-all/s10270-007-0049-9.pdf"
    },
    {
        "title": "Describing and assessing availability requirements in the early stages of system development",
        "submission-date": "2012/06",
        "publication-date": "2013/09",
        "abstract": "Non-functional aspects such as timing constraints, availability, and fault tolerance are critical in the design and implementation of distributed real-time systems. As a result, it is becoming crucial to model and analyze non-functional requirements at the early stages of the software development life cycle. The widespread interest in dependability modeling and analysis techniques at the requirements phase provides the major motivation for this research. This paper presents a novel approach to describe and validate high-level availability requirements using the Use Case Maps (UCM) language of the ITU-T User Requirements Notation standard. The proposed approach relies on a mapping of availability architectural tactics to UCM models. The resulting extensions are described using a metamodel and are implemented within the jUCMNav tool. Early assessment and characterization of the means to achieve availability are then performed using a matrix representation allowing for feature-based availability composition and reasoning. We demonstrate the applicability of our approach through a case study of lawful intercept and ACL-based forwarding features on IP routers.",
        "keywords": [
            "Non-functional requirements",
            "Availability",
            "URN",
            "Use Case Maps",
            "Availability analysis",
            "Architectural tactics"
        ],
        "authors": [
            "Jameleddine Hassine"
        ],
        "file_path": "data/sosym-all/s10270-013-0382-0.pdf"
    },
    {
        "title": "Modeling Paradigms",
        "submission-date": "2005/06",
        "publication-date": "2005/06",
        "abstract": "This issue contains three papers in its regular section and includes a special section that consists of extended versions of the best papers from the “St.Eve” (State versus Event-based Modeling) Workshop. In the state-based modeling paradigm, behavior is conceptualized and described in terms of state changes. Behavior in data intensive applications (e.g., business systems) and in control intensive systems (e.g., embedded controllers) ﬁt well in this paradigm. Event-based modeling is particularly suited to describing systems in terms of interactions across their constituent interfaces in the early development phases (e.g., in requirements and high-level architecture phases). Focusing on interactions across interfaces in the early phases is good practice. It allows a developer to abstract out irrelevant internal details while gaining an early understanding of required interactions and constraints on how parts interact in an application. The focus on understanding interactions across interfaces in the early stages can also lead to early convergence of stable application architectures.\n\nState and event-based modeling paradigms are not the only paradigms that are used when modeling software based systems and their context. For example, neither approach is well-suited to modeling workﬂows. In addition, physical or logical distribution and deployment, and threads of activity cannot be adequately described using events or states. It should not come as a surprise that the development of an application may require the use of multiple modeling paradigms. Integrating multiple modeling paradigms is one of the great challenges of model-drive development. An eﬀective integration must be based on a deep understanding of the relationships among modeling paradigms. Workshops such as “St.Eve” help the community develop such an understanding.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-005-0096-z.pdf"
    },
    {
        "title": "DEVS-based formalism for the modeling of routing processes",
        "submission-date": "2020/05",
        "publication-date": "2021/10",
        "abstract": "The Discrete Event System Speciﬁcation (DEVS) is a modular and hierarchical Modeling and Simulation (M&S) formalism\nbased on systems theory that provides a general methodology for the construction of reusable models. Well-deﬁned M&S\nstructures have a positive impact when building simulation models because they can be applied systematically. However, even\nwhen DEVS can be used to model routing situations, the structures that emerge from this kind of problem are signiﬁcant due\nto the handling of the ﬂow of events. Often, the modeler ends with a lot of simulation models that refer to variants of the\nsame component. The goal of this paper is to analyze the routing process domain from a conceptual modeling perspective\nthrough the use of a new DEVS extension called Routed DEVS (RDEVS). The RDEVS formalism is conceptually deﬁned as\na subclass of DEVS that manages a set of identiﬁed events inside a model network where each node combines a behavioral\ndescription with a routing policy. In particular, we study the modeling effort required to solve the M&S of routing problems\nscenarios employing a comparison between RDEVS modeling solutions and DEVS modeling strategies. Such a comparison\nis based on measures that promote the capture of the behavioral complexity of the ﬁnal models. The results obtained highlight\nthe modeling beneﬁts of the RDEVS formalism as a constructor of routing processes. The proposed solution reduces the\nmodeling effort involved in DEVS by specifying the event routing process directly in the RDEVS models using design\npatterns. The novel contribution is an advance in the understanding of how DEVS as a system modeling formalism supports\nbest practices of software engineering in general and conceptual modeling in particular. The reusability and ﬂexibility of the\nﬁnal simulation models, along with designs with low coupling and high cohesion, are the main beneﬁts of the proposal that\nimprove the M&S task applying a conceptual modeling perspective.",
        "keywords": [
            "Routed Discrete Event System Speciﬁcation",
            "Conceptual modeling",
            "Modeling effort",
            "Routing problem",
            "Design patterns"
        ],
        "authors": [
            "María Julia Blas",
            "Horacio Leone",
            "Silvio Gonnet"
        ],
        "file_path": "data/sosym-all/s10270-021-00928-4.pdf"
    },
    {
        "title": "Scenario-based system design with colored Petri nets: an application to train control systems",
        "submission-date": "2014/06",
        "publication-date": "2016/02",
        "abstract": "For the goal of model-based system software development, this paper exploits the formalism of colored Petri nets (CPNs) to design complex systems based on scenarios. The speciﬁcation of UML sequence diagrams which are easily understood by customers, requirement engineers and software developers are adopted to represent scenarios as speciﬁcation models. A scenario is a partial description of the system behavior, describing how users, system components and the environment interact. Thus scenarios need to be synthesized in order to obtain an overall system behavior. A large number of works (e.g., Whittle and Schumann in Proceedings of the 2000 international conference on soft-ware engineering, pp 314–323, 2000; Elkoutbi and Keller in Application and theory of Petri nets, 2000; Damas et al. in Proceedings of the 14th ACM SIGSOFT international symposium on foundations of software engineering, pp 197–207, 2000; Uchitel et al. in IEEE Trans Softw Eng 29(2):99–115, 2003) have investigated scenario synthesis providing approaches or algorithms. These synthesis approaches and algorithms result in either Petri net models (e.g., Elkoutbi and Keller 2000; Ameedeen and Bordbar in 12th international Communicated by Dr. Kevin Lano. This paper is partly supported by the BJTU Founds for Young Scientists under Grant 2015RC072 and the National Natural Science Foundation of China under Grant 61502029. IEEE enterprise distributed object computing conference (EDOC), pp 213–221, 2008) that are mainly suitable for scenario validation or other forms of behavior models (e.g., labeled transition systems in Damas et al. 2000; Uchitel et al. 2003 and statecharts in Krüger et al. in Distributed and parallel embedded systems, pp 61–71, 1999; Whittle et al. 2000) that may be regarded as design models. Petri nets are well known for describing distributed and concurrent complex systems. Furthermore, numerous techniques, e.g., simulation, testing, state space-based techniques, structural methods and model checking, are currently available for analyzing Petri net models. Therefore, design models in the form of Petri nets, integrating all scenarios into a coherent whole and fitting for further detailed design, are promising. To this end, we present a top-down approach to establish hierarchical CPNs in accordance with specified scenarios (i.e., sequence diagrams). This approach makes use of explicitly labeling component states in the sequence diagrams to correlate scenarios. In addition, the techniques of state space analysis and ASK-CTL model checking are used to verify the correctness and consistency of the CPN model with respect to standard and system-specific properties. As the inspiration of the presented approach derives from the development of train control systems, we present an running example of designing the on-board subsystem of a satellite-based train control system to show the feasibility of our approach.",
        "keywords": [
            "Scenario",
            "System design",
            "Modeling",
            "Veriﬁcation",
            "Colored Petri nets",
            "Train control system"
        ],
        "authors": [
            "Daohua Wu",
            "Eckehard Schnieder"
        ],
        "file_path": "data/sosym-all/s10270-016-0517-1.pdf"
    },
    {
        "title": "An approach for bug localization in models using two levels: model and metamodel",
        "submission-date": "2018/03",
        "publication-date": "2019/03",
        "abstract": "Bug localization is a common task in software engineering, especially when maintaining and evolving software products. This paper introduces a bug localization approach that, in contrast to existing source code approaches, takes advantage of domain information found in the model and the metamodel. Throughout this paper, we present an approach for bug localization in models (BLiM2) that applies the source code ideas for bug localization (textual similarity to the bug description and the Defect Localization Principle) and takes advantage of the domain information from the model and the metamodel. We evaluated our approach in BSH, a real-world industrial case study in the induction hob domain measuring the results in terms of recall, precision, the combination of both the F-measure and the Matthews correlation coefﬁcient. Our study shows that our BLiM2 approach, which combines information from the model and the metamodel for the textual similarity and differentiates between the timespan from the model and metamodel, provides the best results in this work. We also performed a statistical analysis to provide evidence of the signiﬁcance of the results. The values obtained show that there exist signiﬁcant differences in the performance of the best BLiM2 approach with the approach used by our industrial partner. Finally, the effect size statistics reveals that the best BLiM2 approach obtains better results in the 78% of the times in the worst case.",
        "keywords": [
            "Bug localization",
            "Model-driven engineering",
            "Reverse engineering"
        ],
        "authors": [
            "Lorena Arcega",
            "Jaime Font",
            "Øystein Haugen",
            "Carlos Cetina"
        ],
        "file_path": "data/sosym-all/s10270-019-00727-y.pdf"
    },
    {
        "title": "A transformation contract to generate aspects from access control policies",
        "submission-date": "2009/01",
        "publication-date": "2010/03",
        "abstract": "Access control is an important security issue. It has been addressed since the late 1960s in the early time-sharing computer systems. Many access control models have been proposed since than but of particular interest is Ferraiolo and Khun’s role-based access control model (RBAC). It is a simple and yet general model which has been deeply studied and applied both in industry and in academia. A variety of industrial standards have been proposed based on this model. Generating code for an access control policy is an interesting challenge. Understanding access control as a non-functional concern that cross-cuts the functional part of a systemraises difﬁculties quitesuitablefor asolutionbasedon aspect-oriented programming. In this paper, we address the problems of speciﬁcation and validation of code generation for access control policies targeting an aspect-based infra-structure. We propose an MDA approach. The code generator is a transformation from SecureUML, an RBAC-based mod-eling language, to the language Aspects for Access Control (AAC), an aspect-oriented modeling language proposed in this paper. Metamodels are used to represent the languages and to specify the transformation. A metamodel is used to represent the abstract syntax of a language and the constraints that a given instance model of the metamodel must fulﬁll. This paper is dedicated with affection to the memory of Vinicius Braga.",
        "keywords": [],
        "authors": [
            "Christiano Braga"
        ],
        "file_path": "data/sosym-all/s10270-010-0156-x.pdf"
    },
    {
        "title": "Model-integrating development of software systems: a ﬂexible component-based approach",
        "submission-date": "2017/04",
        "publication-date": "2018/06",
        "abstract": "A promising way to develop ﬂexible software systems is to include models that are analyzed, modiﬁed and executed at\nruntimes as an integrated part of the system. Building such model-integrating systems is a challenging task since the respective\nmodeling languages have to be supported comprehensively at runtime, and these systems still need to be developable in\na modular way by composing them from basic building blocks. Model-driven (MDD) and component-based development\n(CBD) are two established orthogonal approaches that can tackle the mentioned challenges. MDD is based on the use of models\nand modeling languages as ﬁrst-class entities to systematically engineer software systems. CBD enables the engineering of\nmodular systems by facilitating a divide-and-conquer approach with reuse. However, combining and aligning the individual\nprinciples from both approaches is an open research problem. In this article, we describe model-integrating development\n(MID), an engineering approach that enables the systematic development of component-based, model-integrating software.\nMID combines principles from MDD and CBD and is based on the central assumption that models and code shall be treated\nequally as ﬁrst-class entities of software throughout its life cycle. In particular, MID leverages the added ﬂexibility that comes\nwith models at runtime, i.e., when models are an integral part of running software. The practicability of the proposed solution\nconcept is rationalized based on a reference implementation that provides the basis for a thoroughly described and critically\ndiscussed feasibility study: a dynamic access control product line. The obtained beneﬁts are presented in a distilled way, and\nfuture research challenges are identiﬁed.",
        "keywords": [
            "Model-integrating development",
            "Model-integrating components",
            "Model-integrating software",
            "Software\ncomponents",
            "Modeling language engineering",
            "Models at runtime",
            "Self-adaptive software"
        ],
        "authors": [
            "Mahdi Derakhshanmanesh",
            "Jürgen Ebert",
            "Marvin Grieger",
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-018-0682-5.pdf"
    },
    {
        "title": "A personal retrospective on language workbenches",
        "submission-date": "2023/01",
        "publication-date": "2023/03",
        "abstract": "Model-driven software engineering and specifically domain-specific languages have contributed to improve the quality of software and the efficiency in the development of software. However, the design and implementation of domain-specific languages requires still an enormous investment. Language workbenches are the most important tools in the field of software language engineering. The introduction of language workbenches has alleviated partly the development effort, but there are still a few major challenges that need to be tackled. This paper presents a personal perspective on the development of tools for language engineering and language workbenches in particular and future challenges to be tackled.",
        "keywords": [
            "Language engineering",
            "DSLs",
            "Programming environment generators",
            "Language workbenches"
        ],
        "authors": [
            "Mark van den Brand"
        ],
        "file_path": "data/sosym-all/s10270-023-01101-9.pdf"
    },
    {
        "title": "Modeling the obsolescence of models",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Programs, like people, get old. The same is true for models, which can become obsolete due to a diversity of factors such as changing requirements, data drift or evolution of the domain itself. Preventing or addressing obsolescence as early as possible helps to reduce the significant costs, risks, and uncertainties incurred by obsolete models and the software system generated from them. Indeed, obsolescence in models can easily propagate to errors in the system resulting in behavioral uncertainty marked by unforeseen, emergent, or unpredictable behavior. Nevertheless, methods and strategies to identify, anticipate, minimize, and manage model obsolescence are presently lacking. This paper presents an innovative approach to tackle model obsolescence. We have designed a domain-specific language (DSL) to specify potential aging and degradation conditions for model elements. Based on the DSL annotations and the history of changes in a model, we can pinpoint those elements that require validation or risk becoming obsolete. Both the DSL and the engine to calculate the obsolescence status of the elements in a model have been released as part of the open-source BESSER modeling platform.",
        "keywords": [
            "Model obsolescence",
            "Model-driven",
            "Domain-specific language"
        ],
        "authors": [
            "Iván Alfonso",
            "Jean-Sébastien Sottet",
            "Pierre Brimont",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-024-01236-3.pdf"
    },
    {
        "title": "A generic framework for representing and analyzing model concurrency",
        "submission-date": "2021/03",
        "publication-date": "2023/01",
        "abstract": "Recent results in language engineering simplify the development of tool-supported executable domain-speciﬁc modeling languages (xDSMLs), including editing (e.g., completion and error checking) and execution analysis tools (e.g., debugging, monitoring and live modeling). However, such frameworks are currently limited to sequential execution traces and cannot handleexecutiontraces resultingfromanexecutionsemantics withaconcurrencymodel supportingparallelismor interleaving. This prevents the development of concurrency analysis tools, like debuggers supporting the exploration of model executions resulting from different interleavings. In this paper, we present a generic framework to integrate execution semantics with either implicit or explicit concurrency models, to explore the possible execution traces of conforming models, and to deﬁne strategies for helping in the exploration of the possible executions. This framework is complemented with a protocol to interact with the resulting executions and hence to build advanced concurrency analysis tools. The approach has been implemented within the GEMOC Studio. We demonstrate how to integrate two representative concurrent meta-programming approaches (MoCCML/Java and Henshin), which use different paradigms and underlying foundations to deﬁne an xDSML’s concurrency model. We also demonstrate the ability to deﬁne an advanced concurrent omniscient debugger with the proposed protocol. The paper, thus, contributes key abstractions and an associated protocol for integrating concurrent meta-programming approaches in a language workbench, and dynamically exploring the possible executions of a model in the modeling workbench.",
        "keywords": [
            "Language engineering",
            "Model execution",
            "Model concurrency",
            "Simulation",
            "Concurrent analyses/debugging"
        ],
        "authors": [
            "Steffen Zschaler",
            "Erwan Bousse",
            "Julien Deantoni",
            "Benoit Combemale"
        ],
        "file_path": "data/sosym-all/s10270-022-01073-2.pdf"
    },
    {
        "title": "Full contract veriﬁcation for ATL using symbolic execution",
        "submission-date": "2016/02",
        "publication-date": "2016/07",
        "abstract": "The Atlas Transformation Language (ATL) is currently one of the most used model transformation languages and has become a de facto standard in model-driven engineering for implementing model transformations. At the same time, it is understood by the community that enhancing methods for exhaustively verifying such transformations allows for a more widespread adoption of model-driven engineering in industry. A variety of proposals for the veriﬁcation of ATL transformations have arisen in the past few years. However, the majority of these techniques are either based on non-exhaustive testing or on proof methods that require human assistance and/or are not complete. In this paper, we describe our method for statically verifying the declarative subset of ATL model transformations. This veriﬁcation is performed by translating the transformation (including features like ﬁlters, OCL expressions, and lazy rules) into our model transformation language DSLTrans. As we handle only the declarative portion of ATL, and DSLTrans is Turing-incomplete, this reduction in expressivity allows us to use a symbolic-execution approach to generate representations of all possible input models to the transformation. We then verify pre-/post-condition contracts on these representations, which in turn veriﬁes the transformation itself. The technique we present in this paper is exhaustive for the subset of declarative ATL model transformations. This means that if the prover indicates a contract holds on a transformation, then the contract’s pre-/post-condition pair will be true for any input model for that transformation. We demonstrate and explore the applicability of our technique by studying several relatively large and complex ATL model transformations, including a model transformation developed in collaboration with our industrial partner. As well, we present our ‘slicing’ technique. This technique selects only those rules in the DSLTrans transformation needed for contract proof, thereby reducing proving time.",
        "keywords": [
            "Model transformation",
            "ATL",
            "Formal veriﬁcation",
            "Symbolic execution",
            "Contracts",
            "Pre-/post-conditions"
        ],
        "authors": [
            "Bentley James Oakes",
            "Javier Troya",
            "Levi Lúcio",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-016-0548-7.pdf"
    },
    {
        "title": "Guest editorial to the special issue on “modelling–foundations and applications”",
        "submission-date": "2015/01",
        "publication-date": "2015/01",
        "abstract": "This paper is a guest editorial for a special issue of the Software and Systems Modelling (SoSyM) journal, stemming from the 2012 European Conference on Modelling Foundations and Applications (ECMFA 2012). It outlines the growth of model-driven engineering (MDE) and the role of ECMFA as a leading European conference in the field. The editorial describes the selection process for papers included in the special issue, which were chosen from those presented at ECMFA 2012 based on reviewer feedback and audience impact.",
        "keywords": [],
        "authors": [
            "Antonio Vallecillo",
            "Juha-Pekka Tolvanen"
        ],
        "file_path": "data/sosym-all/s10270-013-0377-x.pdf"
    },
    {
        "title": "Selecting a process variant modeling approach: guidelines and application",
        "submission-date": "2017/01",
        "publication-date": "2017/12",
        "abstract": "Various modeling approaches have been introduced to manage process diversity in a business context. For practitioners, it is difﬁcult to select an approach suitable for the needs and limitations of their organization due to the limited number of examples and guidelines. In this paper, we report on an action research study to perform a comparative process variant modeling application in a process management consultancy company. This company experienced difﬁculties in maintaining and reusing process deﬁnitions of their customers. We describe how the requirements were determined and led to the selection of two speciﬁc approaches, the Decomposition Driven Method and the Provop approach. We comparatively evaluated the suitability of these approaches to develop variant models for six software project management processes of ﬁve customers. This study contributes to the ﬁeld by presenting an industrial case for process variant modeling, reporting in-depth, real-life applications of two approaches, applying the approaches for hierarchical processes, and presenting guidelines for choosing an approach under comparable conditions.",
        "keywords": [
            "Business process modeling",
            "Process variant modeling",
            "Decomposition driven method",
            "Provop"
        ],
        "authors": [
            "Banu Aysolmaz",
            "Dennis M. M. Schunselaar",
            "Hajo A. Reijers",
            "Ali Yaldiz"
        ],
        "file_path": "data/sosym-all/s10270-017-0648-z.pdf"
    },
    {
        "title": "AI-powered model repair: an experience report—lessons learned, challenges, and opportunities",
        "submission-date": "2021/03",
        "publication-date": "2022/02",
        "abstract": "Artificial intelligence has already proven to be a powerful tool to automate and improve how we deal with software development processes. The application of artificial intelligence to model-driven engineering projects is becoming more and more popular; however, within the model repair field, the use of this technique remains mostly an open challenge. In this paper, we explore some existing approaches in the field of AI-powered model repair. From the existing approaches in this field, we identify a series of challenges which the community needs to overcome. In addition, we present a number of research opportunities by taking inspiration from other fields which have successfully used artificial intelligence, such as code repair. Moreover, we discuss the connection between the existing approaches and the opportunities with the identified challenges. Finally, we present the outcomes of our experience of applying artificial intelligence to model repair.",
        "keywords": [
            "Artificial intelligence",
            "Model repair",
            "Challenges",
            "Opportunities"
        ],
        "authors": [
            "Angela Barriga",
            "Adrian Rutle",
            "Rogardt Heldal"
        ],
        "file_path": "data/sosym-all/s10270-022-00983-5.pdf"
    },
    {
        "title": "An actor-based framework for asynchronous event-based cyber-physical systems",
        "submission-date": "2020/02",
        "publication-date": "2021/04",
        "abstract": "In cyber-physical systems like automotive systems, there are components like sensors, actuators, and controllers that communicate asynchronously with each other. The computational model of actors supports modeling distributed asynchronously communicating systems. We propose the Hybrid Rebeca language to support the modeling of cyber-physical systems. Hybrid Rebeca is an extension of the actor-based language Rebeca. In this extension, physical actors are introduced as new computational entities to encapsulate physical behaviors. To support various means of communication among the entities, the network is explicitly modeled as a separate entity from actors. We develop a tool to derive hybrid automata as the basis for the analysis of Hybrid Rebeca models. We demonstrate the applicability of our approach through a case study in the domain of automotive systems. We use the SpaceEx framework for reachability analysis of the case study. Compared to hybrid automata, our results show that for event-based asynchronous models hybrid Rebeca improves analyzability by reducing the number of real variables, and increases modularity and hence, minimizes the number of changes caused by a modification in the model.",
        "keywords": [
            "Actor model",
            "Cyber-physical systems",
            "Hybrid automata"
        ],
        "authors": [
            "Iman Jahandideh",
            "Fatemeh Ghassemi",
            "Marjan Sirjani"
        ],
        "file_path": "data/sosym-all/s10270-021-00877-y.pdf"
    },
    {
        "title": "Semi-automated metamodel/model co-evolution: a multi-level interactive approach",
        "submission-date": "2021/03",
        "publication-date": "2022/04",
        "abstract": "Metamodels evolve even more frequently than programming languages. This evolution process may result in a large number\nof instance models that are no longer conforming to the revised metamodel. On the one hand, the manual adaptation of\nmodels after the metamodels’ evolution can be tedious, error-prone, and time-consuming. On the other hand, the automated\nco-evolution of metamodels/models is challenging, especially when new semantics is introduced to the metamodels. While\nsome interactive techniques have been proposed, designers still need to explore a large number of possible revised models,\nwhich makes the interaction time-consuming. Existing interactive tools are limited to interactions with the designers to\nevaluate the impact of the co-evolved models on different objectives of the number of inconsistencies, number of changes\nand the deviation from the initial models. However, designers are also interested to check the impact of introduced changes\non the decision space which is composed by model elements. These interactions help designers to understand the differences\nof the co-evolved models solution that have similar objectives value to select the best one based on their preferences. In this\npaper, we propose an interactive approach that enables designers to select their preference simultaneously in the objective\nand decision spaces. Designers may be interested in looking at co-evolution operations that can improve a speciﬁc objective\nsuch as number of non-conformities with the revised metamodel (objective space), but such operations may be related to\ndifferent model locations (decision space). A set of co-evolution solutions is generated at ﬁrst using multi-objective search\nthat suggests edit operations to designers based on three objectives: minimizing the deviation with the initial model, the\nnumber of non-conformities with the revised metamodel and the number of changes. Then, the approach proposes to the\nuser few regions of interest by clustering the set of recommended co-evolution solutions of the multi-objective search. Also,\nanother clustering algorithm is applied within each cluster of the objective space to identify solutions related to different\nmodel element locations. The objective and decision spaces can now be explored more efﬁciently by the designers, who can\nquickly select their preferred cluster and give feedback on a smaller number of solutions by eliminating similar ones. This\nfeedback is then used to guide the search for the next iterations if the user is still not satisﬁed. We evaluated our approach on\na set of metamodel/model co-evolution case studies and compared it to existing fully automated and interactive co-evolution\ntechniques.",
        "keywords": [
            "Metamodel/model co-evolution",
            "Interactive multi-objective search",
            "Search-based software engineering"
        ],
        "authors": [
            "Wael Kessentini",
            "Vahid Alizadeh"
        ],
        "file_path": "data/sosym-all/s10270-022-00978-2.pdf"
    },
    {
        "title": "Consistent change propagation within models",
        "submission-date": "2019/06",
        "publication-date": "2020/08",
        "abstract": "Developers change models with clear intentions—e.g., for refactoring, defects removal, or evolution. However, in doing so,\ndevelopers are often unaware of the consequences of their changes. Changes to one part of a model may affect other parts\nof the same model and/or even other models, possibly created and maintained by other developers. The consequences are\nincomplete changes and with it inconsistencies within or across models. Extensive works exist on detecting and repairing\ninconsistencies. However, the literature tends to focus on inconsistencies as errors in need of repairs rather than on incomplete\nchanges in need of further propagation. Many changes are non-trivial and require a series of coordinated model changes. As\ndevelopers start changing the model, intermittent inconsistencies arise with other parts of the model that developers have not\nyet changed. These inconsistencies are cues for incomplete change propagation. Resolving these inconsistencies should be\ndone in a manner that is consistent with the original changes. We speak of consistent change propagation. This paper leverages\nclassical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only\nsuggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair.\nIn doing so, our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier\nchanges. We argue that consistent change propagation is essential for effective model-driven engineering. Our approach and\nits tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its\nfeasibility and scalability. A comparison with two versioned models shows that our approach identiﬁes actual repair sequences\nthat developers had chosen. Furthermore, an experiment involving 22 participants shows that our change propagation approach\nmeets the workﬂow of how developers handle changes by always computing the sequence of repairs resulting from the change\npropagation.",
        "keywords": [
            "Model-driven engineering",
            "Inconsistency repair",
            "Change propagation",
            "Consistency detection"
        ],
        "authors": [
            "Roland Kretschmer",
            "Djamel Eddine Khelladi",
            "Roberto Erick Lopez-Herrejon",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-020-00823-4.pdf"
    },
    {
        "title": "Research software engineering and the importance of scientific models",
        "submission-date": "2023/07",
        "publication-date": "2023/07",
        "abstract": "Research Software Engineering is the use of Software Engineering practices to support the software needs when conducting research. This is the definition of a new and emerging field of software engineering. Similar to the original term “software engineering,” the creation of the subfield “Research Software Engineering” (RSE) was necessary because there is an ongoing crisis in the development of software that supports research activities.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-023-01119-z.pdf"
    },
    {
        "title": "Resolving model inconsistencies using automated regression planning",
        "submission-date": "2012/10",
        "publication-date": "2013/02",
        "abstract": "One of the main challenges in model-driven software engineering is to automate the resolution of design model inconsistencies. We propose to use the artiﬁcial intelligence technique of automated planning for the purpose of resolving such inconsistencies through the generation of one or more resolution plans. We implemented Badger, a regression planner in Prolog that generates such plans. We assess its scalability on the resolution of different types of structural inconsistencies in UML models using both generated models and reverse-engineered models of varying sizes, the largest ones containing more than 10,000 model elements. We illustrate the metamodel-independence of our approach by applying it to the resolution of code smells in a Java program. We discuss how the user can adapt the order in which resolution plans are presented by modifying the cost function of the planner algorithm.",
        "keywords": [
            "Automated planning",
            "Inconsistency resolution",
            "Software modeling"
        ],
        "authors": [
            "Jorge Pinna Puissant",
            "Ragnhild Van Der Straeten",
            "Tom Mens"
        ],
        "file_path": "data/sosym-all/s10270-013-0317-9.pdf"
    },
    {
        "title": "Improving repair of semantic ATL errors using a social diversity metric",
        "submission-date": "2023/07",
        "publication-date": "2024/04",
        "abstract": "Model transformations play an essential role in the model-driven engineering paradigm. However, writing a correct trans-\nformation requires the user to understand both what the transformation should do and how to enact that change in the\ntransformation. This easily leads to syntactic and semantic errors in transformations which are time-consuming to locate and\nfix. In this article, we extend our evolutionary algorithm (EA) approach to automatically repair transformations containing\nmultiple semantic errors. To prevent the fitness plateaus and the single fitness peak limitations from our previous work, we\ninclude the notion of social diversity as an objective for our EA to promote repair patches tackling errors that are less covered\nby the other patches of the population. We evaluate our approach on four ATL transformations, which have been mutated to\ncontain up to five semantic errors simultaneously. Our evaluation shows that integrating social diversity when searching for\nrepair patches improves the quality of those patches and speeds up the convergence even when up to five semantic errors are\ninvolved.",
        "keywords": [
            "Model-driven engineering",
            "Model transformations",
            "ATL",
            "Evolutionary algorithms",
            "Social diversity"
        ],
        "authors": [
            "Zahra VaraminyBahnemiry",
            "Jessie Galasso",
            "Bentley Oakes",
            "Houari Sahraoui"
        ],
        "file_path": "data/sosym-all/s10270-024-01170-4.pdf"
    },
    {
        "title": "Automated, interactive, and traceable domain modelling empowered by artiﬁcial intelligence",
        "submission-date": "2021/04",
        "publication-date": "2022/01",
        "abstract": "Model-Based Software Engineering provides various modelling formalisms for capturing the structural, behavioral, conﬁg-uration, and intentional aspects of software systems. One of the most widely used kinds of models—domain models—are used during requirements analysis or the early stages of design to capture the domain concepts and relationships in the form of class diagrams. Modellers perform domain modelling to transform the problem descriptions that express informal requirements in natural language to domain models, which are more concise and analyzable. However, this manual practice of domain modelling is laborious and time-consuming. Existing approaches, which aim to assist modellers by automating or semi-automating the construction of domain models from problem descriptions, fail to address three non-trivial aspects of automated domain modelling. First, automatically extracted domain models from existing approaches are not accurate enough to be used directly or with minor modiﬁcations for software development or teaching purposes. Second, existing approaches do not support modeller-system interactions beyond providing recommendations. Finally, existing approaches do not facilitate the modellers to learn the rationale behind the modelling decisions taken by an extractor system. Therefore, in this paper, we extend our previous work to facilitate bot-modeller interactions. We propose an algorithm to discover alterna-tive conﬁgurations during bot-modeller interactions. Our bot uses this algorithm to ﬁnd alternative conﬁgurations and then present these conﬁgurations in the form of suggestions to modellers. Our bot then updates the domain model in response to the acceptance of these suggestions by a modeller. Furthermore, we evaluate the bot for its effectiveness and performance for the test problem descriptions. Our bot achieves median F1 scores of 86%, 91%, and 90% in the Found Conﬁgurations, Offered Suggestions, and Updated Domain Models categories, respectively. We also show that the median time taken by our bot to ﬁnd alternative conﬁgurations is 55.5ms for the problem descriptions which are similar to the test problem descriptions in terms of model size and complexity. Finally, we conduct a pilot user study to assess the beneﬁts and limitations of our bot and present the lessons learned from our study in preparation for a large-scale user study.",
        "keywords": [
            "Domain model",
            "Natural language (NL)",
            "Natural language processing (NLP)",
            "Machine learning (ML)",
            "Neural networks",
            "Descriptive model",
            "Predictive model",
            "Bot",
            "Modeller interactions",
            "Traceability information model",
            "Traceability knowledge graph"
        ],
        "authors": [
            "Rijul Saini",
            "Gunter Mussbacher",
            "Jin L. C. Guo",
            "Jörg Kienzle"
        ],
        "file_path": "data/sosym-all/s10270-021-00942-6.pdf"
    },
    {
        "title": "Quick ﬁxing ATL transformations with speculative analysis",
        "submission-date": "2016/02",
        "publication-date": "2016/07",
        "abstract": "Model transformations are central components of most model-based software projects. While ensuring their correctness is vital to guarantee the quality of the solution, current transformation tools provide limited support to statically detect and ﬁx errors. In this way, the identiﬁcation of errors and their correction are nowadays mostly manual activities which incur in high costs. The aim of this work is to improve this situation. Recently, we developed a static analyser that combines program analysis and constraint solving to identify errors in ATL model transformations. In this paper, we present a novel method and system that uses our analyser to propose suitable quick ﬁxes for ATL transformation errors, notably some non-trivial, transformation-speciﬁc ones. Our approach supports speculative analysis to help developers select the most appropriate ﬁx by creating a dynamic ranking of ﬁxes, reporting on the consequences of applying a quick ﬁx, and providing a pre-visualization of each quick ﬁx application. The approach integrates seamlessly with the ATL editor. Moreover, we provide an evaluation based on existing faulty transformations built by a third party, and on automatically generated transformation mutants, which are then corrected with the quick ﬁxes of our catalogue.",
        "keywords": [
            "Model transformation",
            "ATL",
            "Transformation static analysis",
            "Quick ﬁxes",
            "Speculative analysis"
        ],
        "authors": [
            "Jesús Sánchez Cuadrado",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-016-0541-1.pdf"
    },
    {
        "title": "Bridging the model-to-code abstraction gap with fuzzy logic in model-based regression test selection",
        "submission-date": "2020/09",
        "publication-date": "2021/07",
        "abstract": "Regression test selection (RTS) approaches reduce the cost of regression testing of evolving software systems. Existing RTS approaches based on UML models use behavioral diagrams or a combination of structural and behavioral diagrams. However, in practice, behavioral diagrams are incomplete or not used. In previous work, we proposed a fuzzy logic based RTS approach called FLiRTS that uses UML sequence and activity diagrams. In this work, we introduce FLiRTS2, which drops the need for behavioral diagrams and relies on system models that only use UML class diagrams, which are the most widely used UML diagrams in practice. FLiRTS2 addresses the unavailability of behavioral diagrams by classifying test cases using fuzzy logic after analyzing the information commonly provided in class diagrams. We evaluated FLiRTS2 on UML class diagrams extracted from 3331 revisions of 13 open-source software systems, and compared the results with those of code-based dynamic (Ekstazi) and static (STARTS) RTS approaches. The average test suite reduction using FLiRTS2 was 82.06%. The average safety violations of FLiRTS2 with respect to Ekstazi and STARTS were 18.88% and 16.53%, respectively. FLiRTS2 selected on average about 82% of the test cases that were selected by Ekstazi and STARTS. The average precision violations of FLiRTS2 with respect to Ekstazi and STARTS were 13.27% and 9.01%, respectively. The average mutation score of the full test suites was 18.90%; the standard deviation of the reduced test suites from the average deviation of the mutation score for each subject was 1.78% for FLiRTS2, 1.11% for Ekstazi, and 1.43% for STARTS. Our experiment demonstrated that the performance of FLiRTS2 is close to the state-of-art tools for code-based RTS but requires less information and performs the selection in less time.",
        "keywords": [
            "Class diagram",
            "Fuzzy logic",
            "Regression test selection",
            "UML"
        ],
        "authors": [
            "Walter Cazzola",
            "Sudipto Ghosh",
            "Mohammed Al-Refai",
            "Gabriele Maurina"
        ],
        "file_path": "data/sosym-all/s10270-021-00899-6.pdf"
    },
    {
        "title": "The journal on Software and Systems Modeling Matures",
        "submission-date": "2012/09",
        "publication-date": "2012/09",
        "abstract": "This editorial reflects on the ten-year history of the Journal on Software and Systems Modeling (SoSyM), highlighting its origins, evolution, and contribution to the field of software and system modeling.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-012-0287-3.pdf"
    },
    {
        "title": "A technique for discovering BPMN collaboration diagrams",
        "submission-date": "2023/01",
        "publication-date": "2024/02",
        "abstract": "The process mining domain is actively supported by techniques and tools addressing the discovery of single-participant business processes. In contrast, approaches for discovering collaboration models out of distributed data stored by multiple interacting participants are lacking. In this context, we propose a novel technique for discovering collaboration models from sets of event logs that include data about participants’ interactions. The technique discovers each participant’s process through already available algorithms introduced by the process mining community. Then, it analyzes the logs to extract information on the exchange of messages to automatically combine the discovered processes into a collaboration model representing the distributed system’s behavior and providing analytics on the interactions. The technique has been implemented in a tool evaluated via several experiments on different application domains.",
        "keywords": [
            "BPMN collaborations",
            "Discovery",
            "Messages analysis"
        ],
        "authors": [
            "Flavio Corradini",
            "Sara Pettinari",
            "Barbara Re",
            "Lorenzo Rossi",
            "Francesco Tiezzi"
        ],
        "file_path": "data/sosym-all/s10270-024-01153-5.pdf"
    },
    {
        "title": "A framework for families of domain-speciﬁc modelling languages",
        "submission-date": "2010/11",
        "publication-date": "2012/08",
        "abstract": "Domain-speciﬁc modelling langugages, which\nare tailored to the requirements of their users, can signif-icantly increase the acceptance of formal (or at least semi-formal) modelling in scenarios where informal diagrams and\nnatural language descriptions are predominant today. We\nshow in this article how the Resource Description Frame-\nwork (RDF), which is a standard for the fundamental data\nstructures of the Semantic Web, and algebraic graph trans-\nformations on these data structures can be used to realise\nand modify the abstract syntax of models in such domain-\nspeciﬁc languages. We examine a small domain-speciﬁc\nmodelling language for IT infrastructures—inspired by real-\nworld requirements from a banking environment—as an\napplication scenario. From this scenario, we derive four key\nrequirements for a domain-speciﬁc modelling framework:\n(1) distributed modelling, (2) evolution of language deﬁ-\nnitions, (3) migration of legacy models and (4) integration\nof modelling languages. RDF and transformation rules are\nthen used to provide a solution which meets these require-\nments simultaneously, where all kinds of modiﬁcations—\nfrom simple editing steps via model migration to language\nintegration—are realised in an integrated manner by the sin-\ngle, declarative formalism of algebraic graph transformation.",
        "keywords": [
            "Resource Description Framework",
            "Algebraic\ngraph transformation",
            "Domain-speciﬁc modelling language",
            "Language evolution",
            "Model migration",
            "Language integration",
            "Distributed modelling"
        ],
        "authors": [
            "Benjamin Braatz",
            "Christoph Brandt"
        ],
        "file_path": "data/sosym-all/s10270-012-0271-y.pdf"
    },
    {
        "title": "Participatory modeling from a stakeholder perspective: On the inﬂuence of collaboration and revisions on psychological ownership and perceived model quality",
        "submission-date": "2021/10",
        "publication-date": "2022/08",
        "abstract": "Participatory enterprise modeling is about gathering domain experts and involving them directly in the creation of models, aided by modeling experts. It is meant to increase commitment to and quality of models. This paper presents an exploratory study focusing on the subjective view of the domain experts. We investigated the inﬂuence of direct collaboration versus individual modeling, and the inﬂuence of model revisions by modeling experts on psychological ownership and perceived model quality. We chose process modeling as a particular form of enterprise modeling. Our results give hint that domain experts working individually with a modeling expert perceive model quality as higher than those working collaboratively whereas psychological ownership did not show any difference. Revisions caused changes in the subjects’ assessments only of model quality. Moreover, we will present qualitative results from interviews we led with the participants. They reveal interesting insight on how outcome and perception of the procedure and the method in both settings can be positively inﬂuenced. The interviews also emphasize the special role of the method experts who are sometimes even considered as co-owners of the model.",
        "keywords": [
            "Participatory enterprise modeling",
            "Collaboration",
            "Psychological ownership",
            "Perceived control",
            "Perceived model quality"
        ],
        "authors": [
            "Anne Gutschmidt",
            "Birger Lantow",
            "Ben Hellmanzik",
            "Ben Ramforth",
            "Matteo Wiese",
            "Erko Martins"
        ],
        "file_path": "data/sosym-all/s10270-022-01036-7.pdf"
    },
    {
        "title": "MIKADO: a smart city KPIs assessment modeling framework",
        "submission-date": "2020/10",
        "publication-date": "2021/08",
        "abstract": "Smart decision making plays a central role for smart city governance. It exploits data analytics approaches applied to collected data, for supporting smart cities stakeholders in understanding and effectively managing a smart city. Smart governance is performed through the management of key performance indicators (KPIs), reﬂecting the degree of smartness and sustainability of smart cities. Even though KPIs are gaining relevance, e.g., at European level, the existing tools for their calculation are still limited. They mainly consist in dashboards and online spreadsheets that are rigid, thus making the KPIs evolution and customizationatediousanderror-proneprocess.Inthispaper,weexploitmodel-drivenengineering(MDE)techniques,through metamodel-based domain-speciﬁc languages (DSLs), to build a framework called MIKADO for the automatic assessment of KPIs over smart cities. In particular, the approach provides support for both: (i) domain experts, by the deﬁnition of a textual DSL for an intuitive KPIs modeling process and (ii) smart cities stakeholders, by the deﬁnition of graphical editors for smart cities modeling. Moreover, dynamic dashboards are generated to support an intuitive visualization and interpretation of the KPIs assessed by our KPIs evaluation engine. We provide evaluation results by showing a demonstration case as well as studying the scalability of the KPIs evaluation engine and the general usability of the approach with encouraging results. Moreover, the approach is open and extensible to further manage comparison among smart cities, simulations, and KPIs interrelations.",
        "keywords": [
            "Smart Cities",
            "MDE",
            "KPI",
            "DSL",
            "Smart Governance"
        ],
        "authors": [
            "Martina De Sanctis",
            "Ludovico Iovino",
            "Maria Teresa Rossi",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-021-00907-9.pdf"
    },
    {
        "title": "Reuse in model-to-model transformation languages: are we there yet?",
        "submission-date": "2012/10",
        "publication-date": "2013/05",
        "abstract": "In the area of model-driven engineering, model transformations are proposed as the technique to systematically manipulate models. For increasing development productivity as well as quality of model transformations, reuse mechanisms are indispensable. Although numerous mechanisms have been proposed, no systematic comparison exists, making it unclear, which reuse mechanisms may be best employed in a certain situation. Thus, this paper provides an in-depth comparison of reuse mechanisms in model-to-model transformation languages and categorizes them along their intended scope of application. Finally, current barriers and facilitators to model transformation reuse are discussed.",
        "keywords": [
            "Reuse mechanisms",
            "Model transformations",
            "Survey",
            "Model-driven engineering"
        ],
        "authors": [
            "A. Kusel",
            "J. Schönböck",
            "M. Wimmer",
            "G. Kappel",
            "W. Retschitzegger",
            "W. Schwinger"
        ],
        "file_path": "data/sosym-all/s10270-013-0343-7.pdf"
    },
    {
        "title": "Automatic derivation of BPEL4WS from IDEF0 process models",
        "submission-date": "2005/05",
        "publication-date": "2006/02",
        "abstract": "Integration deﬁnition for function modelling (IDEF0) is one of the most popular notations for modelling business processes. It employs a rather simple and intuitive modelling construct, consisting of boxes representing functions and arrows connecting them signifying ﬂow of information and materials. Web services on the other hand are an emerging technology for implementing distributed systems. Web service orchestration languages, such as Business Process Execution Language for Web Services (BPEL4WS), are the emerging approach for describing processes as networks of coordinated web services. Business processes as captured in IDEF0 models, however, may contain both web services as well as other types of activities which need to be coordinated. By automatically analysing the Extensible Markup Language (XML) deﬁnition of an IDEF0 model, we can identify how web services interact with other activities and at runtime generate code to support the orchestration of web services with the overall business process. The approach proposed is independent of the orchestration language and ensures an implementation independent model for specifying web service orchestrations. This approach also enables the top-down analysis of a business process to its constituent web services and avoids any misalignment problems during design time between the two.",
        "keywords": [
            "IDEF0",
            "UML",
            "Web services",
            "Orchestration",
            "WSDL",
            "BPEL4WS"
        ],
        "authors": [
            "Bill Karakostas",
            "Yannis Zorgios",
            "Charalampos C. Alevizos"
        ],
        "file_path": "data/sosym-all/s10270-006-0003-2.pdf"
    },
    {
        "title": "Modeling difficulties in creating conceptual data models\nMultimodal studies on individual modeling processes",
        "submission-date": "2022/01",
        "publication-date": "2022/10",
        "abstract": "Conceptual modeling is a learning task essential to students of computer science, software engineering and related programs. Construed as a complex task, surprisingly little is known about the actual act of conceptual modeling, and about modeling difficulties learners experience. Combining complementary modes of observation of learners’ modeling processes, we study modeling difficulties encountered while performing a data modeling task. Using the concept of cognitive breakdown, we analyze audiovisual protocols of the individual modelers’ modeling processes, recordings of their interactions with the employed modeling software tool and survey data of modelers about their perception of encountered modeling difficulties. In an exploratory study and a follow-up study, we identify eight types of modeling difficulties related to modeling entity types, generalization hierarchies, relationship types, attributes and cardinalities. The identified types of modeling difficulties contribute to a better and more complete understanding of data modeling processes intended to inform design science research on modeling assistance for data modelers at different stages of their learning and mastering of conceptual data modeling.",
        "keywords": [
            "Conceptual data modeling",
            "Modeling difficulty",
            "Problem solving",
            "Cognitive breakdown",
            "Mixed methods research"
        ],
        "authors": [
            "Kristina Rosenthal\nStefan Strecker\nMonique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-022-01051-8.pdf"
    },
    {
        "title": "Comprehensive analysis of FBD test coverage criteria using mutants",
        "submission-date": "2013/06",
        "publication-date": "2014/07",
        "abstract": "Function block diagram (FBD), a graphical mod-eling language for programmable logic controllers, has been widely used to implement safety critical system software such as nuclear reactor protection systems. With the growing importance of structural testing for FBD models, structural test coverage criteria for FBD models have been proposed and evaluated using mutation analysis in our previous work. We extend the previous work by comprehensively analyz-ing the relationships among fault detection effectiveness, test suite size, and coverage level through several research ques-tions. We generate a large number of test suites achieving an FBD test coverage ranging from 0 to 100%, and we also gen-erate many artiﬁcial faults (i.e. mutants) for the FBD models. Our analysis results show that the fault detection effective-ness of the FBD coverage criteria increases with increasing coverage levels, and the coverage criteria are highly effec-tive at detecting faults in all subject models. Furthermore, the test suites generated with the FBD coverage criteria are more effectiveandefﬁcientthantherandomlygeneratedtestsuites. The FBD coverage criteria are strong at detecting faults in Boolean edges, while relatively weak at detecting wrong con-stants in FBD models. Empirical knowledge regarding our experiments provide the validity of using the FBD coverage criteria, and therefore, of FBD model-based testing.",
        "keywords": [
            "FBD model-based testing",
            "Test coverage criteria",
            "Mutation analysis"
        ],
        "authors": [
            "Donghwan Shin",
            "Eunkyoung Jee",
            "Doo-Hwan Bae"
        ],
        "file_path": "data/sosym-all/s10270-014-0428-y.pdf"
    },
    {
        "title": "Modeling agent organizations using roles",
        "submission-date": "2003/04",
        "publication-date": "2003/06",
        "abstract": "Agent-based and active object systems are no longer contained within the boundaries of a single, small organization. To meet the demands of large-scale software and systems modeling, we need useful analogies for modeling and constructing large-scale systems of autonomous, interactive software entities. In this paper, we employ social and organizational systems theory as a way to guide our understanding of the notion of role and its implications on how agents (and active objects) might behave in group settings.",
        "keywords": [
            "Agent",
            "Role",
            "Active object"
        ],
        "authors": [
            "James J. Odell",
            "H. Van Dyke Parunak",
            "Mitchell Fleischer"
        ],
        "file_path": "data/sosym-all/s10270-003-0017-y.pdf"
    },
    {
        "title": "On model typing",
        "submission-date": "2006/01",
        "publication-date": "2007/01",
        "abstract": "Where object-oriented languages deal with objects as described by classes, model-driven development uses models, as graphs of interconnected objects, described by metamodels. A number of new languages have been and continue to be developed for this model-based paradigm, both for model transformation and for general programming using models. Many of these use single-object approaches to typing, derived from solutions found in object-oriented systems, while others use metamodels as model types, but without a clear notion of polymorphism. Both of these approaches lead to brittle and overly restrictive reuse characteristics. In this paper we propose a simple extension to object-oriented typing to better cater for a model-oriented context, including a simple strategy for typing models as a collection of interconnected objects. We suggest extensions to existing type system formalisms to support these concepts andtheir manipulation. Usingasimpleexampleweshow how this extended approach permits more ﬂexible reuse, while preserving type safety.",
        "keywords": [
            "MDA",
            "MOF",
            "Metamodelling",
            "Type systems",
            "Typing",
            "Model transformation"
        ],
        "authors": [
            "Jim Steel",
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-006-0036-6.pdf"
    },
    {
        "title": "Veriﬁcation, validation, and evaluation of modeling methods: experiences and recommendations",
        "submission-date": "2025/02",
        "publication-date": "Not found",
        "abstract": "Modern modeling methods are developed in various ways, some following method engineering approaches and principles, others in more ad-hoc ways based on experience, observation, or by exploration of innovative ideas. In all cases, the assessment of the method quality and ﬁtness to the different situations in which it has to serve is a challenge that the method engineers have to face. In this paper, we aim to share knowledge concerning the process of development of modeling methods with an emphasis on systematic approach to veriﬁcation, validation, and evaluation (VVE). The goal of the paper is to: (1) clarify the differences and key activities and practices of method VVE, (2) to analyze how these have been applied to several modeling methods, and (3) to capture and share key best practices in this process.",
        "keywords": [
            "Modeling method",
            "Veriﬁcation",
            "Validation",
            "Evaluation",
            "Method engineering",
            "Design science research"
        ],
        "authors": [
            "Jolita Ralyté",
            "Georgios Koutsopoulos",
            "Janis Stirna"
        ],
        "file_path": "data/sosym-all/s10270-025-01304-2.pdf"
    },
    {
        "title": "Improving engineering efﬁciency with PLM/ALM",
        "submission-date": "2012/05",
        "publication-date": "2013/05",
        "abstract": "Rising cost pressure is forcing manufacturers and their suppliers to jointly and consistently master prod-uct development. Our industry case study shows how a leading automotive OEM over time has achieved effective interaction of engineering processes, tools, and people on the basis of product and application life-cycle management (PLM/ALM). Its scope is ﬁrst an introduction to PLM/ALM on the basis of a model-driven engineering (MDE) for one or several products or product families. Second, PLM and ALM need tool support to the degree necessary to ease handling and drive reuse and consistency. Third, introducing MDE needs profound change management. Starting from estab-lishing the relevant engineering processes, we show how they can be effectively automated for best possible usage across the enterprise and even for suppliers. We practically describe how such a profound change process is successfully managed together with impacted engineers and how the concepts can be transferred to other companies. Concrete results for efﬁ-ciency improvement, shorter lead time, and better quality in product development combined with better global engineer-ing underline the business value.",
        "keywords": [
            "Application lifecycle management (ALM)",
            "Product lifecycle management (PLM)",
            "Efﬁciency",
            "Industry\nvoice"
        ],
        "authors": [
            "Christof Ebert"
        ],
        "file_path": "data/sosym-all/s10270-013-0347-3.pdf"
    },
    {
        "title": "Model-based code generation works: But how far does it go?—on the role of the generator",
        "submission-date": "2024/04",
        "publication-date": "2024/04",
        "abstract": "There are many published examples of successful industrial projects that used code generation from abstract models. However, it seems that the actual use of code generation from explicitly defined models written in modeling languages (e.g., UML, SysML, or a domain-specific language) has not been as successful and widespread as possible. Why is that so? Unfortunately, there are too few in-depth examinations of the problems and challenges that actually prohibit the widespread adoption of model-based code generation. It would be very instructive for the SoSyM community to have deeper insights into these challenges so that we could improve the current state-of-the-art.",
        "keywords": [],
        "authors": [
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-024-01172-2.pdf"
    },
    {
        "title": "A transformation-based approach to context-aware modelling",
        "submission-date": "2011/04",
        "publication-date": "2012/03",
        "abstract": "Context-aware computing is a paradigm for governing the numerous mobile devices surrounding us. In this computing paradigm, software applications continuously and dynamically adapt to different “contexts” implying different software configurations of such devices. Unfortunately, modelling a context-aware application (CAA) for all possible contexts is only feasible in the simplest of cases. Hence, tool support verifying certain properties is required. In this article, we introduce the CAA model, in which context adaptations are specified explicitly as model transformations. By mapping this model to graphs and graph transformations, we can exploit graph transformation techniques such as critical pair analysis to find contexts for which the resulting application model is ambiguous. We validate our approach by means of an example of a mobile city guide, demonstrating that we can identify subtle context interactions that might go unnoticed otherwise.",
        "keywords": [
            "Context-aware model",
            "Model transformation",
            "Critical pair analysis",
            "Context adaptation",
            "Context coverage"
        ],
        "authors": [
            "Sylvain Degrandsart",
            "Serge Demeyer",
            "Jan Van den Bergh",
            "Tom Mens"
        ],
        "file_path": "data/sosym-all/s10270-012-0239-y.pdf"
    },
    {
        "title": "Automated anonymity veriﬁcation of the ThreeBallot and VAV voting systems",
        "submission-date": "2013/11",
        "publication-date": "2015/01",
        "abstract": "In recent years, a large number of secure voting protocols have been proposed in the literature. Often these protocols contain ﬂaws, but because they are complex protocols, rigorous formal analysis has proven hard to come by. Rivest’s ThreeBallot and Vote/Anti-Vote/Vote (VAV) voting systems are important because they aim to provide security (voter anonymity and voter veriﬁability) without requiring cryptography. In this paper, we construct CSP models of ThreeBallot and VAV, and use them to produce the ﬁrst automated formal analysis of their anonymity properties. Along the way, we discover that one of the crucial assumptions under which ThreeBallot and VAV (and many other votingsystems) operate—theshort ballot assumption—is highly ambiguous in the literature. We give various plausible precise interpretations and discover that in each case, the interpretation either is unrealistically strong or else fails to ensure anonymity. We give a revised version of the short ballot assumption for ThreeBallot and VAV that is realistic but still provides a guarantee of anonymity.",
        "keywords": [
            "Formal methods",
            "Voting systems",
            "Anonymity",
            "Automatic veriﬁcation",
            "ThreeBallot",
            "VAV"
        ],
        "authors": [
            "Murat Moran",
            "James Heather",
            "Steve Schneider"
        ],
        "file_path": "data/sosym-all/s10270-014-0445-x.pdf"
    },
    {
        "title": "From single-objective to multi-objective reinforcement learning-based model transformation",
        "submission-date": "2024/03",
        "publication-date": "2024/11",
        "abstract": "Model-driven optimization allows to directly apply domain-speciﬁc modeling languages to deﬁne models which are subsequently optimized by applying a predeﬁned set of model transformation rules. Objectives guide the optimization processes which can range from one single objective formulation resulting in one single solution to a set of objectives that necessitates the identiﬁcation of a Pareto-optimal set of solutions. In recent years, a multitude of reinforcement learning approaches has been proposed that support both optimization cases and competitive results for various problem instances have been reported. However, their application to the ﬁeld of model-driven optimization has not gained much attention yet, especially when compared to the extensive application of meta-heuristic search approaches such as genetic algorithms. Thus, there is a lack of knowledge about the applicability and performance of reinforcement learning for model-driven optimization. We therefore present in this paper a general framework for applying reinforcement learning to model-driven optimization problems. In particular, we show how a catalog of different reinforcement learning algorithms can be integrated with existing model-driven optimization approaches that use a transformation rule application encoding. We exemplify this integration by presenting a dedicated reinforcement learning extension for MOMoT. We build on this tool support and investigate several case studies for validating the applicability of reinforcement learning for model-driven optimization and compare the performance against a genetic algorithm. The results show clear advantages of using RL for single-objective problems, especially for cases where the transformation steps are highly dependent on each other. For multi-objective problems, the results are more diverse and case-speciﬁc, which further motivates the usage of model-driven optimization to utilize different approaches to ﬁnd the best solutions.",
        "keywords": [
            "Model-driven optimization",
            "Model transformation",
            "Reinforcement learning",
            "MOMoT"
        ],
        "authors": [
            "Martin Eisenberg",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-024-01233-6.pdf"
    },
    {
        "title": "A case study on consistency management of business and IT process models in banking",
        "submission-date": "2011/11",
        "publication-date": "2013/03",
        "abstract": "Organizations that adopt process modeling often maintain several co-existing models of the same business process. These models target different abstraction levels and stakeholder perspectives. Maintaining consistency among these models has become a major challenge for such organizations. Although several academic works have discussed this challenge, little empirical investigation exists on how people perform process model consistency management in practice. This paper aims to address this lack by presenting an in-depth empirical study of a business-driven engineering process deployed at a large company in the banking sector. We analyzed more than 70 business process models developed by the company, including their change history, with over 1,000 change requests. We also interviewed 9 business and IT practitioners and surveyed 23 such practitioners to understand concrete difﬁculties in consistency management, the rationales for the speciﬁcation-to-implementation reﬁnements found in the models, strategies that the practitioners use to detect and ﬁx inconsistencies, and how tools could help with these tasks. Our contribution is a set of eight empirical ﬁndings, some of which conﬁrm or contradict previous works on process model consistency management found in the literature. The ﬁndings provide empirical evidence of (1) how business process models are created and maintained, including a set of recurrent patterns used to reﬁne business-level process speciﬁcations into IT-level models; (2) what types of inconsistencies occur; how they are introduced; and what problems they cause; and (3) what stakeholders expect from tools to support consistency management.",
        "keywords": [
            "Business processes",
            "Consistency management",
            "Process reﬁnement patterns",
            "Empirical study"
        ],
        "authors": [
            "Moisés Castelo Branco",
            "Yingfei Xiong",
            "Krzysztof Czarnecki",
            "Jochen Küster",
            "Hagen Völzer"
        ],
        "file_path": "data/sosym-all/s10270-013-0318-8.pdf"
    },
    {
        "title": "A model-driven method for describing and predicting the reliability of composite services",
        "submission-date": "2009/05",
        "publication-date": "2010/02",
        "abstract": "Service-oriented computing is the prominent paradigm for viewing business processes as composed of functions provided by modular and standardized services. Web services are the building blocks for the application of service-oriented computing on the Web and provide the necessary support for the consolidation of multiple services into a single composite service corresponding to the overall process. In such a context, service providers are strategically interested in both describing the quality of service (QoS) characteristics of offered services, to better qualify their offer and gain a significant advantage in the global marketplace, and predicting the level of QoS that can be offered to service consumers when building composite web services that make use of services managed by various service providers. This paper illustrates a model-driven method to automatically describe and predict the QoS of composite web services specified by use of business process execution language (BPEL). The paper specifically addresses the reliability characteristic of the QoS. The proposed method is founded on Q-WSDL, a lightweight WSDL extension for the description of the QoS characteristics of a web service, and exploits Q-WSDL to annotate reliability data onto a BPEL-based UML model of the composite service. The UML model is then used to predict and describe the reliability of the composite web service. The proposed method is illustrated by use of an example application that deals with a composite web service for the migration of PSTN telephone numbers.",
        "keywords": [
            "QoS",
            "Service oriented architecture",
            "WSDL",
            "BPEL",
            "UML",
            "Model-driven prediction"
        ],
        "authors": [
            "Paolo Bocciarelli",
            "Andrea D’Ambrogio"
        ],
        "file_path": "data/sosym-all/s10270-010-0150-3.pdf"
    },
    {
        "title": "Design patterns for open tool integration",
        "submission-date": "2004/11",
        "publication-date": "2004/11",
        "abstract": "Design tool integration is a highly relevant\narea of software engineering that can greatly improve the\neﬃciency of development processes. Design patterns have\nbeen widely recognized as important contributors to the\nsuccess of software systems. This paper describes and\ncompares two large-grain, architectural design patterns\nthat solve speciﬁc design tool integration problems. Both\npatterns have been implemented and used in real-life en-\ngineering processes.",
        "keywords": [
            "Design patterns",
            "Software architecture",
            "Tool integration framework",
            "Metamodels",
            "Generative programming"
        ],
        "authors": [
            "Gabor Karsai",
            "Andras Lang",
            "Sandeep Neema"
        ],
        "file_path": "data/sosym-all/s10270-004-0073-y.pdf"
    },
    {
        "title": "Model-driven engineering for mobile robotic systems: a systematic mapping study",
        "submission-date": "2020/06",
        "publication-date": "2021/08",
        "abstract": "Mobile robots operate in various environments (e.g. aquatic, aerial, or terrestrial), they come in many diverse shapes and they are increasingly becoming parts of our lives. The successful engineering of mobile robotics systems demands the interdisciplinary collaboration of experts from different domains, such as mechanical and electrical engineering, artiﬁcial intelligence, and systems engineering. Research and industry have tried to tackle this heterogeneity by proposing a multitude of model-driven solutions to engineer the software of mobile robotics systems. However, there is no systematic study of the state of the art in model-driven engineering (MDE) for mobile robotics systems that could guide research or practitioners in ﬁnding model-driven solutions and tools to efﬁciently engineer mobile robotics systems. The paper is contributing to this direction by providing a map of software engineering research in MDE that investigates (1) which types of robots are supported by existing MDE approaches, (2) the types and characteristics of MRSs that are engineered using MDE approaches, (3) a description of how MDE approaches support the engineering of MRSs, (4) how existing MDE approaches are validated, and (5) how tools support existing MDE approaches. We also provide a replication package to assess, extend, and/or replicate the study. The results of this work and the highlighted challenges can guide researchers and practitioners from robotics and software engineering through the research landscape.",
        "keywords": [
            "Model-driven engineering",
            "Mobile robot systems",
            "Systematic mapping study"
        ],
        "authors": [
            "Giuseppina Lucia Casalaro",
            "Giulio Cattivera",
            "Federico Ciccozzi",
            "Ivano Malavolta",
            "Andreas Wortmann",
            "Patrizio Pelliccione"
        ],
        "file_path": "data/sosym-all/s10270-021-00908-8.pdf"
    },
    {
        "title": "Predictions-on-chip: model-based training and automated deployment of machine learning models at runtime",
        "submission-date": "2020/02",
        "publication-date": "2021/03",
        "abstract": "The design of gas turbines is a challenging area of cyber-physical systems where complex model-based simulations across multiple disciplines (e.g., performance, aerothermal) drive the design process. As a result, a continuously increasing amount of data is derived during system design. Finding new insights in such data by exploiting various machine learning (ML) techniques is a promising industrial trend since better predictions based on real data result in substantial product quality improvements and cost reduction. This paper presents a method that generates data from multi-paradigm simulation tools, develops and trains ML models for prediction, and deploys such prediction models into an active control system operating at runtime with limited computational power. We explore the replacement of existing traditional prediction modules with ML counterparts with different architectures. We validate the effectiveness of various ML models in the context of three (real) gas turbine bearings using over 150,000 data points for training, validation, and testing. We introduce code generation techniques for automated deployment of neural network models to industrial off-the-shelf programmable logic controllers.",
        "keywords": [
            "Prediction-at-runtime",
            "Machine learning",
            "Neural networks",
            "Automated deployment",
            "Code generation",
            "Gas turbine engines"
        ],
        "authors": [
            "Sebastian Pilarski",
            "Martin Staniszewski",
            "Matthew Bryan",
            "Frederic Villeneuve",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-020-00856-9.pdf"
    },
    {
        "title": "Deﬁning and generating multi-level and uncertainty-wise test oracles for cyber-physical systems",
        "submission-date": "2024/03",
        "publication-date": "2025/02",
        "abstract": "Cyber-physical systems (CPSs) blend digital and physical processes. CPS software is the key to realizing their functionalities. This software needs to evolve to deal with different aspects, such as the implementation of new functionalities or bug ﬁxes. Because of this, design–operation methods, colloquially known as “DevOps,” are paramount to be adopted within these systems. During DevOps phases, automating test execution at design time is a key enabler of streamlined software development and software quality improvement. Likewise, monitoring whether a CPS is behaving as expected at operation is similarly important. In DevOps, test oracles play an important role in enabling automated testing, ensuring the reliability of software deployments, providing feedback to developers, etc. However, deﬁning and generating test oracles in the context of DevOps practices in CPSs need to accommodate aspects speciﬁc to CPSs, such as their time-continuous behavior and inherent uncertainties. To this end, in this paper, we propose a domain-speciﬁc language (DSL) to ease the deﬁnition of test oracles and an automated solution for generating a microservice encapsulating the deﬁned test oracles, which is compatible with a DevOps ecosystem for CPSs. We evaluated our DSL with two industrial case study systems and 9 open-source CPSs. Our evaluation results suggest that our DSL can model around 98% of the requirements of these systems through test oracles. Furthermore, it is possible to generate a microservice to be applicable at different test levels within less than 20min, being fast enough to be adopted in practice.",
        "keywords": [
            "Domain-speciﬁc language",
            "Test oracle generation",
            "Cyber-physical systems",
            "Software testing"
        ],
        "authors": [
            "Pablo Valle",
            "Aitor Arrieta",
            "Liping Han",
            "Shaukat Ali",
            "Tao Yue"
        ],
        "file_path": "data/sosym-all/s10270-025-01271-8.pdf"
    },
    {
        "title": "Veriﬁcation of B+ trees by integration of shape analysis and interactive theorem proving",
        "submission-date": "2012/03",
        "publication-date": "2013/03",
        "abstract": "Interactive proofs of correctness of pointer-manipulating programs tend to be difﬁcult. We propose an approach that integrates shape analysis and interactive theorem proving, namely three-valued logic analyzer (TVLA) and KIV. The approach uses shape analysis to automatically discharge proof obligations for various data structure properties, such as “acyclicity”. To this purpose, we deﬁne a mapping between typed algebraic heaps and TVLA. We verify the main operations of B+ trees by decomposing the problem into three layers: The top-level is an interactive proof of the main recursive procedures. The actual modiﬁcations of the data structure are veriﬁed with shape analysis. TVLA itself relies on problem-speciﬁc constraints and lemmas, that were proven in KIV as a foundation for an overall correct analysis.",
        "keywords": [
            "Theorem proving",
            "Shape analysis",
            "B+ trees",
            "Pointer structures"
        ],
        "authors": [
            "Gidon Ernst",
            "Gerhard Schellhorn",
            "Wolfgang Reif"
        ],
        "file_path": "data/sosym-all/s10270-013-0320-1.pdf"
    },
    {
        "title": "Translating between Alloy speciﬁcations and UML class diagrams annotated with OCL",
        "submission-date": "2012/03",
        "publication-date": "2013/06",
        "abstract": "Model-driven engineering (MDE) is a software engineering approach based on model transformations at different abstraction levels. It prescribes the development of software by successively transforming the models from abstract (speciﬁcations) to more concrete ones (code). Alloy is an increasingly popular lightweight formal speciﬁcation language that supports automatic veriﬁcation. Unfortunately, its widespread industrial adoption is hampered by the lack of an ecosystem of MDE tools, namely code generators. This paper presents a model transformation from Alloy to UML class diagrams annotated with OCL (UML+OCL) and shows how an existing transformation from UML+OCL to Alloy can be improved to handle dynamic issues. The proposed bidirectional transformation enables a smooth integration of Alloy in the current MDE contexts, by allowing UML+OCL speciﬁcations to be transformed to Alloy for validation and veriﬁcation, to correct and possibly reﬁne them inside Alloy, and to translate them back to UML+OCL for sharing with stakeholders or to reuse current model-driven architecture tools to reﬁne them toward code.",
        "keywords": [
            "MDE",
            "Alloy",
            "UML",
            "OCL"
        ],
        "authors": [
            "Alcino Cunha",
            "Ana Garis",
            "Daniel Riesco"
        ],
        "file_path": "data/sosym-all/s10270-013-0353-5.pdf"
    },
    {
        "title": "Incorporating message weights in UML-based analysis of behavioral dependencies in distributed systems",
        "submission-date": "2008/04",
        "publication-date": "2008/12",
        "abstract": "Behavioral dependency analysis (BDA) and the visualization of dependency information have been identified as a high priority in industrial software systems (in specific, distributed systems). BDA determines the extent to which the functionality of one system entity (e.g., an object or a node) depends on other entities. Among many uses, a BDA is used to perform risk analysis and assessment, load planning, fault tolerance and redundancy provisions in distributed systems. Traditionally, most BDA techniques are based on source code or execution traces of a system. However, as model-driven development is gaining more popularity, there is a need for model-based BDA techniques. To address this need, we proposed in a previous work a metric, referred to as dependency index (DI), for the BDA of distributed objects and nodes based on UML behavioral models (sequence diagrams). However, in our previous BDA work, for simplicity, it was assumed that all messages are equivalent in terms of the dependencies they entail. However, to perform a more realistic BDA on real-world systems, messages must be weighted, e.g., certain messages may be more critical (or important) than others, and thus entail more intensive dependency. To address the above need, we define in this article a family of new BDA metrics, as extensions to our basic DI metric, based on different weighting mechanisms. Through an example application of the proposed metrics, we show that they can be used to predict more realistic dependency information. Communicated by Prof. Roel J. Wieringa.",
        "keywords": [
            "Behavioral dependency analysis",
            "Message weights",
            "Distributed systems",
            "Metrics",
            "UML",
            "Model-driven development",
            "Dependability"
        ],
        "authors": [
            "Vahid Garousi"
        ],
        "file_path": "data/sosym-all/s10270-008-0111-2.pdf"
    },
    {
        "title": "Human factors in model-driven engineering: future research goals and initiatives for MDE",
        "submission-date": "2024/03",
        "publication-date": "2024/06",
        "abstract": "Software modelling and model-driven engineering (MDE) is traditionally studied from a technical perspective. However, one of the core motivations behind the use of software models is inherently human-centred. Models aim to enable practitioners to communicate about software designs, make software understandable, or make software easier to write through domain-speciﬁc modelling languages. Several recent studies challenge the idea that these aims can always be reached and indicate that human factors play a role in the success of MDE. However, there is an under-representation of research focusing on human factors in modelling. During a GI-Dagstuhl seminar, topics related to human factors in modelling were discussed by 26 expert participants from research and industry. In breakout groups, ﬁve topics were covered in depth, namely modelling human aspects, factors of modeller experience, diversity and inclusion in MDE, collaboration and MDE, and teaching human-aware MDE. We summarise our insights gained during the discussions on the ﬁve topics. We formulate research goals, questions, and propositions that support directing future initiatives towards an MDE community that is aware of and supportive of human factors and values.",
        "keywords": [
            "MDE",
            "Modelling",
            "Human factors",
            "Workshop"
        ],
        "authors": [
            "Grischa Liebel",
            "Jil Klünder",
            "Regina Hebig",
            "Christopher Lazik",
            "Inês Nunes",
            "Isabella Graßl",
            "Jan-Philipp Steghöfer",
            "Joeri Exelmans",
            "Julian Oertel",
            "Kai Marquardt",
            "Katharina Juhnke",
            "Kurt Schneider",
            "Lucas Gren",
            "Lucia Happe",
            "Marc Herrmann",
            "Marvin Wyrich",
            "Matthias Tichy",
            "Miguel Goulão",
            "Rebekka Wohlrab",
            "Reyhaneh Kalantari",
            "Robert Heinrich",
            "Sandra Greiner",
            "Satrio Adi Rukmono",
            "Shalini Chakraborty",
            "Silvia Abrahão",
            "Vasco Amaral"
        ],
        "file_path": "data/sosym-all/s10270-024-01188-8.pdf"
    },
    {
        "title": "Exploring modeling methods for information systems analysis and design: a data-driven retrospective",
        "submission-date": "2025/02",
        "publication-date": "Not found",
        "abstract": "Modeling for information systems (IS) analysis and design offers broad insights into the advances and challenges of enterprise, business process, software, and conceptual modeling. In celebration of its 30th edition, this paper presents a data-driven retrospective analysis of studies published at the Exploring Modeling Methods for Systems Analysis and Development (EMM-SAD) working conference from 2005 to 2024. EMMSAD has long been a key venue for research on Information Systems (IS) Modeling, covering areas such as conceptual modeling, enterprise modeling, and model-driven engineering, as well as the evaluation of modeling techniques and tools. Using machine learning, specifically Dynamic Topic Modeling (DTM) with BERTopic, this study identifies recurring topics, emerging trends, and shifts in research focus within the IS modeling community. The findings highlight key areas of alignment between IS modeling and the broader modeling landscape, providing insights into the field’s evolution and future research opportunities.",
        "keywords": [
            "IS analysis and design",
            "Dynamic Topic Modeling",
            "BERTopic",
            "Data-driven approach",
            "EMMSAD"
        ],
        "authors": [
            "Iris Reinhartz-Berger",
            "Adir Solomon",
            "Jelena Zdravkovic",
            "John Krogstie",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-025-01302-4.pdf"
    },
    {
        "title": "Extracting LPL privacy policy purposes from annotated web service source code",
        "submission-date": "2020/12",
        "publication-date": "2022/04",
        "abstract": "Privacy policies are a mechanism used to inform users of the World Wide Web about the processing of their personal data. Such processing has special requirements, since personal data are regulated by data protection legislation. For example, a consent or another legal basis is typically needed. Privacy policies are documents used, among other things, to inform the data subject about processing of their personal data. These are formally represented by privacy languages. In this paper, we present a technique for constructing Layered Privacy Language policy data from web service code bases. Theoretically, we model the purposes of processing within web services by extending the privacy language with composition. We also present a formal analysis method for generating privacy policy purposes from the source code of web services. Furthermore, as a practical contribution, we present a static analysis tool that implements the theoretical solution. Finally, we report a brief case study for validating the tool",
        "keywords": [
            "Data protection",
            "privacy engineering",
            "privacy language",
            "static analysis",
            "semantic web",
            "GDPR"
        ],
        "authors": [
            "Kalle Hjerppe",
            "Jukka Ruohonen",
            "Ville Leppänen"
        ],
        "file_path": "data/sosym-all/s10270-022-00998-y.pdf"
    },
    {
        "title": "Procedure-modular speciﬁcation and veriﬁcation of temporal safety properties",
        "submission-date": "2012/03",
        "publication-date": "2013/03",
        "abstract": "This paper describes ProMoVer, a tool for fully automated procedure-modular veriﬁcation of Java programs equipped with method-local and global assertions that specify safety properties of sequences of method invocations. Modularity at the procedure-level is a natural instantiation of the modular veriﬁcation paradigm, where correctness of global properties is relativized on the local properties of the methods rather than on their implementations. Here, it is based on the construction of maximal models for a program model that abstracts away from program data. This approach allows global properties to be veriﬁed in the presence of code evolution, multiple method implementations (as arising from software product lines), or even unknown method implementations (as in mobile code for open platforms). ProMoVer automates a typical veriﬁcation scenario for a previously developed tool set for compositional veriﬁcation of control ﬂow safety properties, and provides appropriate pre- and post-processing. Both linear-time temporal logic and ﬁnite automata are supported as formalisms for expressing local and global safety properties, allowing the user to choose a suitable format for the property at hand. Modularity is exploited by a mechanism for proof reuse that detects and minimizes the veriﬁcation tasks resulting from changes in the code and the speciﬁcations. The veriﬁcation task is relatively light-weight due to support for abstraction from private methods and automatic extraction of candidate speciﬁcations from method implementations. We evaluate the tool on a number of applications from the domains of Java Card and web-based application.",
        "keywords": [
            "Temporal logic",
            "Model checking",
            "Maximal models"
        ],
        "authors": [
            "Siavash Soleimanifard",
            "Dilian Gurov",
            "Marieke Huisman"
        ],
        "file_path": "data/sosym-all/s10270-013-0321-0.pdf"
    },
    {
        "title": "Rule-based update transformations and their application to model refactorings",
        "submission-date": "2004/05",
        "publication-date": "2005/09",
        "abstract": "A rule-based update transformation is a model transformation where a single model is transformed in place. A model refactoring is a model transformation that improves the design described in the model. A refactoring should only affect a previously chosen subset of the original model. In this paper, we discuss how to deﬁne and execute model refactorings as rule-based transformations in the context of the UML and MOF standards. We also present an experimental tool to execute this kind of transformation.",
        "keywords": [
            "Model transformation",
            "Model driven engineering"
        ],
        "authors": [
            "Ivan Porres"
        ],
        "file_path": "data/sosym-all/s10270-005-0088-z.pdf"
    },
    {
        "title": "DropsBox: the Dresden Open Software Toolbox Domain-speciﬁc modelling tools beyond metamodels and transformations",
        "submission-date": "2010/11",
        "publication-date": "2012/11",
        "abstract": "The Dresden Open Software Toolbox (DropsBox) is a software modelling toolbox consisting of a set of open source tools developed by the Software Technology Group at TU Dresden. The DropsBox is built on top of the Eclipse Platform and the Eclipse Modeling Framework. The DropsBox contributes to the development and application of domain-speciﬁc language changes (DSLs) in model-driven software development. It can be customised by tool and language developers to support various activities of a DSL’s life cycle ranging from language design to language application and evolution. In this paper, we provide an overview of the DSL life cycle, the DropsBox tools, and their interaction on a common example. Furthermore, we discuss our experiences in developing and integrating tools for DropsBox in an academic environment.",
        "keywords": [
            "Domain-speciﬁc modelling environment",
            "Domain-speciﬁc language",
            "Language life cycle",
            "Modelling tool",
            "MDSD",
            "EMF"
        ],
        "authors": [
            "Uwe Aßmann",
            "Andreas Bartho",
            "Christoff Bürger",
            "Sebastian Cech",
            "Birgit Demuth",
            "Florian Heidenreich",
            "Jendrik Johannes",
            "Sven Karol",
            "Jan Polowinski",
            "Jan Reimann",
            "Julia Schroeter",
            "Mirko Seifert",
            "Michael Thiele",
            "Christian Wende",
            "Claas Wilke"
        ],
        "file_path": "data/sosym-all/s10270-012-0284-6.pdf"
    },
    {
        "title": "Semi-automatic derivation of RESTful choreographies from business process choreographies",
        "submission-date": "2016/12",
        "publication-date": "2018/01",
        "abstract": "Enterprises reach out for collaborations with other organizations in order to offer complex products and services to the market. Such collaboration and coordination between different organizations, for a good share, is facilitated by information technology. The BPMN process choreography is a modeling language for specifying the exchange of information and services between different organizations at the business level. Recently, there is a surging use of the REST architectural style for the provisioning of services on the web, but few systematic engineering approach to design their collaboration. In this paper, we address this gap in a comprehensive way by deﬁning a semi-automatic method for the derivation of RESTful choreographies from process choreographies. The method is based on natural language analysis techniques to derive interactions from the textual information in process choreographies. The proposed method is evaluated in terms of effectiveness resulting in the intervention of a web engineer in only about 10% of all generated RESTful interactions.",
        "keywords": [
            "Business process choreographies",
            "RESTful choreographies",
            "Natural language analysis"
        ],
        "authors": [
            "Adriatik Nikaj",
            "Mathias Weske",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-017-0653-2.pdf"
    },
    {
        "title": "Automatically reasoning about metamodeling",
        "submission-date": "2012/04",
        "publication-date": "2013/02",
        "abstract": "Metamodeling is foundational to many modeling frameworks, and so it is important to formalize and reason about it. Ideally, correctness proofs and test-case generation on the metamodeling framework should be automatic. However, it has yet to be shown that extensive automated reasoning on metamodeling frameworks can be achieved. In this paper, we present one approach to this problem: metamodeling frameworks are speciﬁed modularly using algebraic data types and constraint logic programming (CLP). Proofs and test-case generation are encoded as open world query operations and automatically solved.",
        "keywords": [
            "Metamodeling",
            "Formal speciﬁcations",
            "Automated analysis"
        ],
        "authors": [
            "Ethan K. Jackson",
            "Tihamer Levendovszky",
            "Daniel Balasubramanian"
        ],
        "file_path": "data/sosym-all/s10270-013-0315-y.pdf"
    },
    {
        "title": "A visual language for modeling multiple perspectives of business process compliance rules",
        "submission-date": "2014/10",
        "publication-date": "2016/04",
        "abstract": "A fundamental challenge for enterprises is to ensure compliance of their business processes with imposed compliance rules stemming from various sources, e.g., corporate guidelines, best practices, standards, and laws. In general, a compliance rule may refer to multiple process perspectives including control ﬂow, time, data, resources, and interactions with business partners. On one hand, compliance rules should be comprehensible for domain experts who must deﬁne, verify, and apply them. On the other, these rules should have a precise semantics to avoid ambiguities and enable their automated processing. Providing a visual language is advantageous in this context as it allows hiding formal details and offering an intuitive way of modeling the compliance rules. However, existing visual languages for compliance rule modeling have focused on the control ﬂow perspective so far, but lack proper support for the other process perspectives. To remedy this drawback, this paper introduces the extended Compliance Rule Graph language, which enables the visual modeling of compliance rules with the support of multiple perspectives. Overall, this language will foster the modeling and veriﬁcation of compliance rules in practice.",
        "keywords": [
            "Business process compliance",
            "Extended Compliance Rule Graphs",
            "Business process modeling",
            "Smart processes"
        ],
        "authors": [
            "David Knuplesch",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-016-0526-0.pdf"
    },
    {
        "title": "Formalising privacy regulations with bigraphs",
        "submission-date": "2024/08",
        "publication-date": "2025/04",
        "abstract": "With many governments regulating the handling of user data—the General Data Protection Regulation, the California Consumer Privacy Act, and the Saudi Arabian Personal Data Protection Law—ensuring systems comply with data privacy legislation is of high importance. Checking compliance is a tricky process and often includes many manual elements. We propose that formal methods, that model systems mathematically, can provide strong guarantees to help companies prove their adherence to legislation. To increase usability we advocate a diagrammatic approach, based on bigraphical reactive systems, where privacy experts can explicitly visualise the systems and describe updates, via rewrite rules, that describe system behaviour. The rewrite rules allow ﬂexibility in integrating privacy policies with user-speciﬁed systems. We focus on modelling notions of providing consent, withdrawing consent, purpose limitations, the right to access and sharing data with third parties, and deﬁne privacy properties that we want to prove within the systems. Properties are expressed using the computation tree logic and proved using model checking. To show the generality of the proposed framework, we apply it to two examples: a bank notiﬁcation system, inspired by Monzo’s privacy policy, and a cloud-based home healthcare system based on the Fitbit app’s privacy policy.",
        "keywords": [
            "Privacy regulations",
            "Privacy modelling",
            "Formal modelling",
            "Model checking",
            "Bigraphs"
        ],
        "authors": [
            "Ebtihal Althubiti",
            "Blair Archibald",
            "Michele Sevegnani"
        ],
        "file_path": "data/sosym-all/s10270-025-01293-2.pdf"
    },
    {
        "title": "Automated support for deriving test requirements from UML statecharts",
        "submission-date": "2004/05",
        "publication-date": "2005/09",
        "abstract": "Many statechart-based testing strategies result in specifying a set of paths to be executed through a (flattened) statechart. These techniques can usually be easily automated so that the tester does not have to go through the tedious procedure of deriving paths manually to comply with a coverage criterion. The next step is then to take each test path individually and derive test requirements leading to fully specified test cases. This requires that we determine the system state required for each event/transition that is part of the path to be tested and the input parameter values for all events and actions associated with the transitions. We propose here a methodology towards the automation of this procedure, which is based on a careful normalization and analysis of operation contracts and transition guards written with the Object Constraint Language (OCL). It is illustrated by one case study that exemplifies the steps of our methodology and provides a first evaluation of its applicability.",
        "keywords": [],
        "authors": [
            "L. C. Briand",
            "Y. Labiche",
            "J. Cui"
        ],
        "file_path": "data/sosym-all/s10270-005-0090-5.pdf"
    },
    {
        "title": "Maintaining consistency in networks of models: bidirectional transformations in the large",
        "submission-date": "2018/06",
        "publication-date": "2019/05",
        "abstract": "The model-driven development of systems involves multiple models, metamodels and transformations, and relationships between them. A bidirectional transformation (bx) is usually deﬁned as a means of maintaining consistency between “two (or more)” models. This includes cases where one model may be generated from one or more others, as well as more complex (“symmetric”) cases where models record partially overlapping information. In recent years, binary bx, those relating two models, have been extensively studied. Multiary bx, those relating more than two models, have received less attention. In this paper, we consider how a multiary consistency relation may be deﬁned in terms of binary consistency relations and how consistency restoration may be carried out on a network of models and relationships between them. In particular, we consider the circumstances under which we can prove non-interference between several bidirectional transformations that impact on the same model and how the use of a more reﬁned notion of consistency can help in cases where this is not possible. In the process, we develop an abstract theory of parts of a model that are read or modiﬁed by a bidirectional transformation. We relate the work to megamodelling and discuss further research that is needed.",
        "keywords": [
            "Model-driven development",
            "Bidirectional transformation",
            "Consistency",
            "Megamodel",
            "Model decomposition",
            "Non-interference"
        ],
        "authors": [
            "Perdita Stevens"
        ],
        "file_path": "data/sosym-all/s10270-019-00736-x.pdf"
    },
    {
        "title": "OIL: an industrial case study in language engineering with Spoofax",
        "submission-date": "2023/09",
        "publication-date": "2024/06",
        "abstract": "Domain-speciﬁc languages (DSLs) promise to improve the software engineering process, e.g., by reducing software development and maintenance effort and by improving communication, and are therefore seeing increased use in industry. To support the creation and deployment of DSLs, language workbenches have been developed. However, little is published about the actual added value of a language workbench in an industrial setting, compared to not using a language workbench. In this paper, we evaluate the productivity of using the Spoofax language workbench by comparing two implementations of an industrial DSL, one in Spoofax and one in Python, that already existed before the evaluation. The subject is the Open Interaction Language (OIL): a complex DSL for implementing control software with requirements imposed by its industrial context at Canon Production Printing. Our ﬁndings indicate that it is more productive to implement OIL using Spoofax compared to using Python, especially if editor services are desired. Although Spoofax was sufﬁcient to implement OIL, we find that Spoofax should especially improve on practical aspects to increase its adoptability in industry.",
        "keywords": [
            "Language workbench",
            "Language engineering",
            "Case study"
        ],
        "authors": [
            "Olav Bunte",
            "Jasper Denkers",
            "Louis C. M. van Gool",
            "Jurgen J. Vinju",
            "Eelco Visser",
            "Tim A. C. Willemse",
            "Andy Zaidman"
        ],
        "file_path": "data/sosym-all/s10270-024-01185-x.pdf"
    },
    {
        "title": "Specifying and executing behavioral requirements: the play-in/play-out approach",
        "submission-date": "2002/09",
        "publication-date": "2003/04",
        "abstract": "A powerful methodology for scenario-based speciﬁcation of reactive systems is described, in which the behavior is “played in” directly from the system’s GUI or some abstract version thereof, and can then be “played out”. The approach is supported and illustrated by a tool, which we call the play-engine. As the behav-ior is played in, the play-engine automatically generates a formal version in an extended version of the language of live sequence charts (LSCs). As they are played out, it causes the application to react according to the universal (“must”) parts of the speciﬁcation; the existential (“may”) parts can be monitored to check their successful completion. Play-in is a user-friendly high-level way of specifying behavior and play-out is a rather surprising way of working with a fully operational system directly from its inter-object requirements. The ideas appear to be relevant to many stages of system development, including requirements engineering, speciﬁcation, testing, analysis and implementation.",
        "keywords": [
            "Live sequence charts (LSCs)",
            "Requirements engineering",
            "System modeling and execution",
            "Scenarios",
            "Testing",
            "UML"
        ],
        "authors": [
            "David Harel",
            "Rami Marelly"
        ],
        "file_path": "data/sosym-all/s10270-002-0015-5.pdf"
    },
    {
        "title": "A closer look at activity relationships to improve business process redesign",
        "submission-date": "2023/11",
        "publication-date": "2024/11",
        "abstract": "Business process models serve as visual representations of the structured sequence of activities, or activity relationships, aimed\nat achieving speciﬁc organizational objectives. These models explicitly depict only a portion of the activity relationships,\nleaving other, so-called hidden relationships to be inferred through transitivity and domain knowledge. Because designers\nlack visibility into these hidden relationships, tasks associated with process redesign (BPR) are more challenging. BPR is\na specialized area within business process management focused on modifying process behavior at the model level. While\nbest practices have been introduced, explicit guidance for implementing them is currently lacking. This paper introduces\nan approach to delineate all possible relationships between any two activities within a BPMN diagram by considering a\ncombination of existential and temporal dependencies. Furthermore, the proposed approach facilitates a detailed speciﬁcation\nof change operations related to BPR and their consequential effects on activity relationships. We illustrate this by applying\nBPR strategies such as resequencing and parallelism to the use case of an order management system.",
        "keywords": [
            "Business process modeling",
            "Activity relationships",
            "Existential dependencies",
            "Business process redesign"
        ],
        "authors": [
            "Kerstin Andree",
            "Dorina Bano",
            "Mathias Weske"
        ],
        "file_path": "data/sosym-all/s10270-024-01234-5.pdf"
    },
    {
        "title": "Computing refactorings of state machines",
        "submission-date": "2006/02",
        "publication-date": "2007/01",
        "abstract": "For behavior models expressed in statechart-like formalisms, we show how to compute semantically equivalent yet structurally different models. These refactorings are deﬁned by user-provided logical predicates that partition the system’s state space and that characterize coherent parts – modes or control states – of the behavior. We embed the refactorings into an incremental development process that uses a combination of both tables and graphically represented state machines for describing systems.",
        "keywords": [],
        "authors": [
            "Alexander Pretschner",
            "Wolfgang Prenninger"
        ],
        "file_path": "data/sosym-all/s10270-006-0037-5.pdf"
    },
    {
        "title": "Looking back at UML",
        "submission-date": "2011/10",
        "publication-date": "2012/07",
        "abstract": "This paper sets out in detail the development of the Uniﬁed Modeling Language and its derivatives from its beginning until the present. The paper describes the processes that were used to develop the language, the architecture and intended uses of the language, its strengths and weaknesses, and the steps that are being taken to make it ready for future developments.",
        "keywords": [
            "UML"
        ],
        "authors": [
            "Steve Cook"
        ],
        "file_path": "data/sosym-all/s10270-012-0256-x.pdf"
    },
    {
        "title": "Search-based model transformation by example",
        "submission-date": "2009/06",
        "publication-date": "2010/09",
        "abstract": "Model transformation (MT) has become an important concern in software engineering. In addition to its role in model-driven development, it is useful in many other situations such as measurement, refactoring, and test-case generation. Roughly speaking, MT aims to derive a target model from a source model by following some rules or principles. So far, the contributions in MT have mostly relied on defining languages to express transformation rules. However, the task of defining, expressing, and maintaining these rules can be difficult, especially for proprietary and non-widely used formalisms. In some situations, companies have accumulated examples from past experiences. Our work starts from these observations to view the transformation problem as one to solve with fragmentary knowledge, i.e. with only examples of source-to-target MTs. Our approach has two main advantages: (1) it always proposes a transformation for a source model, even when rule induction is impossible or difficult to achieve; (2) it is independent from the source and target formalisms; aside from the examples, no extra information is needed. In this context, we propose an optimization-based approach that consists of finding in the examples combinations of transformation fragments that best cover the source model. To that end, we use two strategies based on two search-based algorithms: particle swarm optimization and simulated annealing. The results of validating our approach on industrial projects show that the obtained models are accurate.",
        "keywords": [
            "Search-based software engineering",
            "Automated model transformation",
            "Transformation by example"
        ],
        "authors": [
            "Marouane Kessentini",
            "Houari Sahraoui",
            "Mounir Boukadoum",
            "Omar Ben Omar"
        ],
        "file_path": "data/sosym-all/s10270-010-0175-7.pdf"
    },
    {
        "title": "View-based model-driven software development with ModelJoin",
        "submission-date": "2013/06",
        "publication-date": "2014/05",
        "abstract": "Fragmentation of information across instances of different metamodels poses a signiﬁcant problem for software developers and leads to a major increase in effort of transformation development. Moreover, compositions of metamodels tend to be incomplete, imprecise, and erroneous, making it impossible to present it to users or use it directly as input for applications. Customized views satisfy information needs by focusing on a particular concern, and ﬁltering out information that is not relevant to this concern. For a broad establishment of view-based approaches, an automated solution to deal with separate metamodels and the high complexity of model transformations is necessary. In this paper, we present the ModelJoin approach for the rapid creation of views. Using a human-readable textual DSL, developers can deﬁne custom views declaratively without having to write model transformations or deﬁne a bridging metamodel. Instead, a metamodel generator and higher-order transformations create annotated target metamodels and the appropriate transformations on-the-ﬂy. The resulting views, which are based on these metamodels, contain joined instances and can effectively express concerns unforseen during metamodel design. We have applied the ModelJoin approach and validated the textual DSL in a case study using the Palladio Component Model.",
        "keywords": [
            "View-based modeling",
            "Model-driven software development",
            "Model transformation",
            "Model-based query language"
        ],
        "authors": [
            "Erik Burger",
            "Jörg Henss",
            "Martin Küster",
            "Steffen Kruse",
            "Lucia Happe"
        ],
        "file_path": "data/sosym-all/s10270-014-0413-5.pdf"
    },
    {
        "title": "Correct-by-construction synthesis of model transformations using transformation patterns",
        "submission-date": "2011/06",
        "publication-date": "2012/10",
        "abstract": "Model transformations are an essential part\nof model-based development approaches, such as Model-\ndriven Architecture (MDA) and Model-driven Develop-\nment (MDD). Model transformations are used to reﬁne and\nabstract models, to re-express models in a new modelling\nlanguage, and to analyse, refactor, compare and improve\nmodels. Therefore, the correctness of model transformations\nis critically important for successful application of model-\nbased development: software developers should be able to\nrely upon the correct processing of their models by trans-\nformations in the same way that they rely upon compilers\nto produce correct executable versions of their programs.\nIn this paper, we address this problem by deﬁning standard\nstructures for model transformation speciﬁcations and imple-\nmentations, which serve as patterns and strategies for con-\nstructing a wide range of model transformations. These are\nincorporated into a tool-supported process which automati-\ncally synthesises implementations of model transformations\nfrom their speciﬁcations, these implementations are correct-\nby-construction with respect to their speciﬁcations.",
        "keywords": [
            "Model transformation",
            "Patterns",
            "Model-driven\ndevelopment",
            "Veriﬁcation"
        ],
        "authors": [
            "K. Lano",
            "S. Kolahdouz-Rahimi",
            "I. Poernomo",
            "J. Terrell",
            "S. Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-012-0291-7.pdf"
    },
    {
        "title": "A relational approach to deﬁning and implementing transformations between metamodels",
        "submission-date": "2003/02",
        "publication-date": "2003/09",
        "abstract": "The Model-Driven Architecture initiative of the OMG promotes the idea of transformations in the context of mapping from platform independent to platform speciﬁc models. Additionally, the popularity of XML and the wide spread use of XSLT has raised the proﬁle of model transformation as an important technique for computing. In fact, computing may well be moving to a new paradigm in which models are considered ﬁrst class entities and transformations between them are a major function performed on those models. This paper proposes an approach to deﬁning and implementing model transformations which uses metamodelling patterns to capture the essence of mathematical relations. It shows how these patterns can be used to deﬁne the relationship between two diﬀerent metamodels. A goal of the approach is to enable complete speciﬁcations from which tools can be generated. The paper describes implementations of the examples, which have been partially generated from the deﬁnitions using a tool generation tool. A number of issues emerge which need to be solved in order to achieve the stated goal; these are discussed.",
        "keywords": [
            "Model transformation",
            "UML",
            "Model-driven architecture"
        ],
        "authors": [
            "David Akehurst",
            "Stuart Kent",
            "Octavian Patrascoiu"
        ],
        "file_path": "data/sosym-all/s10270-003-0032-z.pdf"
    },
    {
        "title": "More matters on (meta-)modelling: remarks on Thomas Kühne’s “matters\"",
        "submission-date": "2006/02",
        "publication-date": "2006/10",
        "abstract": "This discussion paper refers to the article “Matters on(Meta-)Modeling”of Thomas Kühne. Many of his concepts and proposals are appreciated, as e.g. the clear distinction of type and token models or of prom- inent relationships like instantiation, generalisation or “meta-ness”. However, for some of the presented views and deﬁnitions alternative approaches may be followed. This concerns e.g. the relationship between models and their originals (“systems”) including the distinction of descriptive and prescriptive models, the view on various kinds of instantiation and the way how metamodels are deﬁned. Some of these questions are debated in detail and alternative positions are presented.",
        "keywords": [
            "Modelling",
            "Model and original",
            "Prescriptive/descriptive model",
            "Transformation",
            "Projection",
            "Type model",
            "Token model",
            "Ontological/linguistic instantiation",
            "Metamodel"
        ],
        "authors": [
            "Wolfgang Hesse"
        ],
        "file_path": "data/sosym-all/s10270-006-0033-9.pdf"
    },
    {
        "title": "A ﬁne-grained analysis of the support provided by UML class diagrams and ER diagrams during data model maintenance",
        "submission-date": "2012/03",
        "publication-date": "2013/01",
        "abstract": "This paper presents the results of an empirical study aiming at comparing the support provided by ER and UML class diagrams during maintenance of data models. We performed one controlled experiment and two replications that focused on comprehension activities (the ﬁrst activity in the maintenance process) and another controlled experiment on modiﬁcation activities related to the implementation of given change requests. The results achieved were analyzed at a ﬁne-grained level aiming at comparing the support given by each single building block of the two notations. Such an analysis is used to identify weaknesses (i.e., building blocks not easy to comprehend) in a notation and/or can justify the need of preferring ER or UML for data modeling. The analysis revealed that the UML class diagrams generally provided a better support for both comprehension and modiﬁcation activities performed on data models as compared to ER diagrams. Nevertheless, the former has some weaknesses related to three building blocks, i.e., multi-value attribute, composite attribute, and weak entity. These ﬁndings suggest that an extension of UML class diagrams should be considered to overcome these weaknesses and improve the support provided by UML class diagrams during maintenance of data models.",
        "keywords": [],
        "authors": [
            "Gabriele Bavota",
            "Carmine Gravino",
            "Rocco Oliveto",
            "Andrea De Lucia",
            "Genoveffa Tortora",
            "Marcela Genero",
            "José A. Cruz-Lemus"
        ],
        "file_path": "data/sosym-all/s10270-012-0312-6.pdf"
    },
    {
        "title": "Modelling multi-criticality vehicular software systems: evolution of an industrial component model",
        "submission-date": "2019/01",
        "publication-date": "2020/04",
        "abstract": "Software in modern vehicles consists of multi-criticality functions, where a function can be safety-critical with stringent real-\ntime requirements, less critical from the vehicle operation perspective, but still with real-time requirements, or not critical at\nall. Next-generation autonomous vehicles will require higher computational power to run multi-criticality functions and such\na power can only be provided by parallel computing platforms such as multi-core architectures. However, current model-based\nsoftware development solutions and related modelling languages have not been designed to effectively deal with challenges\nspeciﬁc of multi-core, such as core-interdependency and controlled allocation of software to hardware. In this paper, we report\non the evolution of the Rubus Component Model for the modelling, analysis, and development of vehicular software systems\nwith multi-criticality for deployment on multi-core platforms. Our goal is to provide a lightweight and technology-preserving\ntransition from model-based software development for single-core to multi-core. This is achieved by evolving the Rubus\nComponent Model to capture explicit concepts for multi-core and parallel hardware and for expressing variable criticality of\nsoftware functions. The paper illustrates these contributions through an industrial application in the vehicular domain.",
        "keywords": [
            "Model-based engineering",
            "Metamodelling",
            "Single-core",
            "Multi-core",
            "Multi-criticality",
            "Vehicular embedded systems",
            "Real-time systems"
        ],
        "authors": [
            "Alessio Bucaioni",
            "Saad Mubeen",
            "Federico Ciccozzi",
            "Antonio Cicchetti",
            "Mikael Sjödin"
        ],
        "file_path": "data/sosym-all/s10270-020-00795-5.pdf"
    },
    {
        "title": "EDITORIAL",
        "submission-date": "2004/09",
        "publication-date": "2007/03",
        "abstract": "The field of graph transformations was born in the late 1960s and has been continuously developed and applied in the many areas where the static and the dynamic aspects of models can be naturally described by means of graphical structures and their changes. This special section of the Journal of Software and System Modeling contains three of the papers that were submitted by invitation, and are based on the authors’ contributions to ICGT 2004 and SETra 2004.",
        "keywords": [],
        "authors": [
            "Francesco Parisi-Presicce"
        ],
        "file_path": "data/sosym-all/s10270-007-0052-1.pdf"
    },
    {
        "title": "Semi-automatic service value network modeling approach based on external public data",
        "submission-date": "2021/06",
        "publication-date": "2022/06",
        "abstract": "Various emerging IT technologies are widely used in the service industry. Thus, an increasing number of new service models have also emerged, including the Internet of Services (IoS). The IoS supports network-based service collaboration and transactions among various service participants from different domains and different organizations, and it is expected to deliver the maximum service value to all stakeholders. To describe the cross-domain, cross-organization, and cross-value chain characteristics of the IoS from a value perspective and support subsequent analysis of the value network and optimization of the IoS, this paper proposes a semi-automatic modeling method for a IoS-oriented value network based on external public data. We ﬁrst propose an intelligent domain entity recognition algorithm based on multidimensional web data to help value network modelers realize effective and efﬁcient recognition of service participants. Then, based on external news data, an intelligent domain relationship extraction algorithm that combines the Bert+BiLSTM+CRF model with the LightGBM model is proposed to effectively and efﬁciently identify the value exchange relationships among service participants, thereby forming an IoS-oriented value network model (IVN). Finally, to extend the cross-domain semantics of the IVN and support analysis of the IVN, we present a domain-speciﬁc value chain extraction algorithm based on typical patterns to complete the cross-domain semantic annotation of the IVN. The effectiveness and efﬁciency of the proposed methods and algorithms are validated through experimental analysis and a case study, which can be of great help in IVN modeling.",
        "keywords": [
            "Service value network",
            "Modeling method",
            "Service value",
            "Value network modeling"
        ],
        "authors": [
            "Jingying Wang",
            "Chao Ma",
            "Huixin Xu",
            "Zhiying Tu",
            "Xiaofei Xu",
            "Hanchuan Xu",
            "Zhongjie Wang"
        ],
        "file_path": "data/sosym-all/s10270-022-01014-z.pdf"
    },
    {
        "title": "Survey of reliability and availability prediction methods from the viewpoint of software architecture",
        "submission-date": "2006/01",
        "publication-date": "2007/01",
        "abstract": "Many future software systems will be distributed across a network, extensively providing different kinds of services for their users. These systems must be highly reliable and provide services when required. Reliability and availability must be engineered into software from the onset of its development, and potential problems must be detected in the early stages, when it is easier and less expensive to implement modifications. The software architecture design phase is the first stage of software development in which it is possible to evaluate how well the quality requirements are being met. For this reason, a method is needed for analyzing software architecture with respect to reliability and availability. In this paper, we define a framework for comparing reliability and availability analysis methods from the viewpoint of software architecture. Our contribution is the comparison of the existing analysis methods and techniques that can be used for reliability and availability prediction at the architectural level. The objective is to discover which methods are suitable for the reliability and availability prediction of today’s complex systems, what are the shortcomings of the methods, and which research activities need to be conducted in order to overcome these identified shortcomings. The comparison reveals that none of the existing methods entirely fulfill the requirements that are defined in the framework. The comparison framework also defines the characteristics required of new reliability and availability analysis methods. Additionally, the framework is a valuable tool for selecting the best suitable method for architecture analysis. Furthermore, the framework can be extended and used for other evaluation methods as well.",
        "keywords": [
            "Reliability and availability analysis",
            "Software architecture",
            "Software components"
        ],
        "authors": [
            "Anne Immonen",
            "Eila Niemelä"
        ],
        "file_path": "data/sosym-all/s10270-006-0040-x.pdf"
    },
    {
        "title": "Visual query languages to design complex queries: a systematic literature review",
        "submission-date": "2021/08",
        "publication-date": "2022/12",
        "abstract": "Structured query language (SQL) is a widely used language for accessing both relational and non-relational databases. SQL is the standard form of access in relational databases, while in non-relational databases, SQL is becoming increasingly available and consolidating itself as an access interface for querying data in cluster environments. Despite its declarative syntax, the speciﬁcation of SQL queries is not a trivial task, even for experts, because some queries demand complex constructs (i.e., subqueries, joins, set operations, conditional expressions, grouping restrictions, and recursion). Visual query languages (VQLs) are an alternative to reduce this complexity. However, although several VQLs have been proposed, they are not widely used in practice. By identifying and analyzing the support provided by VQLs that make it possible to design complex SQL queries, this study collected evidence that helps discover the strengths and weaknesses of each VQL, providing useful feedback for other research initiatives that seek to propose improved VQLs. For this purpose, a systematic literature review was carried out. After analyzing 22 relevant studies and performing 462 inspections, this review points to the need for more expressive VQLs, computer-aided software engineering (CASE) tools available to end users, and more rigorous evaluations to investigate the VQL syntax and semantics.",
        "keywords": [
            "Visual query language",
            "Complex query",
            "Conceptual modeling",
            "SQL"
        ],
        "authors": [
            "Edson Silva",
            "Robson Fidalgo",
            "Márcio Ferro",
            "Natália Franco"
        ],
        "file_path": "data/sosym-all/s10270-022-01071-4.pdf"
    },
    {
        "title": "Boosting bug localization in software models of video games with simulations and component-speciﬁc genetic operations",
        "submission-date": "2023/04",
        "publication-date": "2025/01",
        "abstract": "The development and maintenance of video games present unique challenges that differentiate them from Classic Software Engineering (CSE) such as the increased difﬁculty in locating bugs within video games. This distinction has given rise to Game Software Engineering (GSE), a subﬁeld that intersects software engineering and video games. Our work proposes a novel way for bug localization in video games by evolving simulations via an evolutionary algorithm, which helps to explore the large number of possible simulations. Simulations generate data (i.e., traces) from the behavior of non-player characters (NPCs). NPCs are not controlled by the player and are key components of video games. We hypothesize that such traces can be instrumental in locating bugs. Our approach automatically locates potential buggy model elements from traces. Furthermore, we propose a novel way of applying genetic operations to evolve simulations by selectively combining their components, rather than combining all components as a whole. We evaluate our approach in the commercial video game Kromaia, and the results indicate that evolving simulations using our novel component-speciﬁc genetic operations boosts bug localization. Speciﬁcally, our approach improved the F-measure for all bug categories over randomly combining all components, the baseline (which focuses on CSE and utilizes bug reports), and Random Search by 7.93%, 27.17%, and 46.34%, respectively. This work opens a new research direction for further exploration in bug localization within GSE and potentially in CSE as well. Moreover, it encourages other researchers to explore alternative genetic operations rather than selecting them by default.",
        "keywords": [
            "Bug localization",
            "Video games",
            "Search-based software engineering",
            "Model-driven engineering"
        ],
        "authors": [
            "Rodrigo Casamayor",
            "Lorena Arcega",
            "Francisca Pérez",
            "Carlos Cetina"
        ],
        "file_path": "data/sosym-all/s10270-024-01253-2.pdf"
    },
    {
        "title": "Uncertainty-Wise Cyber-Physical System test modeling",
        "submission-date": "2016/07",
        "publication-date": "2017/07",
        "abstract": "It is important that a Cyber-Physical System (CPS) with uncertainty in its behavior caused by its unpredictable operating environment, to ensure its reliable operation. One method to ensure that the CPS will handle such uncertainty during its operation is by testing the CPS with model-based testing (MBT) techniques. However, existing MBT techniques do not explicitly capture uncertainty in test ready models, i.e., capturing the uncertain expected behavior of a CPS in the presence of environment uncertainty. To fill this gap, we present an Uncertainty-Wise test-modeling framework, named as UncerTum, to create test ready models to support MBT of CPSs facing uncertainty. UncerTum relies on the definition of a UML profile [the UML Uncertainty Profile (UUP)] and a set of UML Model Libraries extending the UML profile for Modeling and Analysis of Real-Time and Embedded Systems (MARTE). UncerTum also benefits from the UML Testing Profile V.2 to support standard-based MBT. UncerTum was evaluated with two industrial CPS case studies, one real-world case study, and one open-source CPS case study from the following four perspectives: (1) Completeness and Coverage of the profiles and Model Libraries in terms of concepts defined in their underlying uncertainty conceptual model for CPSs, i.e., U-Model and MARTE, (2) Effort required to model uncertainty with UncerTum, and (3) Correctness of the developed test ready models, which was assessed via model execution. Based on the evaluation, we can conclude that we were successful in modeling all the uncertainties identified in the four case studies, which gives us an indication that UncerTum is sufficiently complete. In terms of modeling effort, we concluded that on average UncerTum requires 18.5% more time to apply stereotypes from UUP on test ready models.",
        "keywords": [
            "Uncertainty",
            "Cyber-Physical System",
            "UML",
            "Model-based testing"
        ],
        "authors": [
            "Man Zhang",
            "Shaukat Ali",
            "Tao Yue",
            "Roland Norgren",
            "Oscar Okariz"
        ],
        "file_path": "data/sosym-all/s10270-017-0609-6.pdf"
    },
    {
        "title": "EB3: an entity-based black-box speciﬁcation method for information systems",
        "submission-date": "2002/05",
        "publication-date": "2003/06",
        "abstract": "This paper describes a formal method for specifying the observable (external) behavior of information systems using a process algebra and input-output traces. Its notation is mainly based on the entity concept, borrowed from the Jackson System Development method, and integrated with the requirements class diagram to represent data structures and associations. The speciﬁcation process promotes modular and incremental description of the behavior of each entity through process abstraction, entity type patterns, and entity attribute function patterns. Valid system input traces result from the composition of entity traces by using parallel composition operations. The association between input traces and outputs through an input-output relation completes the speciﬁcation process.",
        "keywords": [
            "Trace-based speciﬁcations",
            "Black-box speciﬁcations",
            "Process algebra",
            "JSD",
            "Cleanroom",
            "Patterns"
        ],
        "authors": [
            "M. Frappier",
            "R. St-Denis"
        ],
        "file_path": "data/sosym-all/s10270-003-0024-z.pdf"
    },
    {
        "title": "Mining reading patterns from eye-tracking data: method and demonstration",
        "submission-date": "2018/10",
        "publication-date": "2019/10",
        "abstract": "Understanding how developers interact with different software artifacts when performing comprehension tasks has a potential to improve developers’ productivity. In this paper, we propose a method to analyze eye-tracking data using process mining to ﬁnd distinct reading patterns of how developers interacted with the different artifacts. To validate our approach, we conducted an exploratory study using eye-tracking involving 11 participants. We applied our method to investigate how developers interact with different artifacts during domain and code understanding tasks. To contextualize the reading patterns and to better understand the perceived beneﬁts and challenges participants associated with the different artifacts and their choice of reading patterns, we complemented the eye-tracking data with the data obtained from think aloud. The study used behavior-driven development, a development practice that is increasingly used in Agile software development contexts, as a setting. The study shows that our method can be used to explore developers’ behavior at an aggregated level and identify behavioral patterns at varying levels of granularity.",
        "keywords": [
            "Process mining",
            "Eye-tracking",
            "Reading patterns",
            "Source code",
            "Behavior-driven development"
        ],
        "authors": [
            "Constantina Ioannou",
            "Indira Nurdiani",
            "Andrea Burattin",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-019-00759-4.pdf"
    },
    {
        "title": "Formal modeling of biomedical signal acquisition systems: source of evidence for certiﬁcation",
        "submission-date": "2016/06",
        "publication-date": "2017/08",
        "abstract": "Biomedical signal acquisition systems are software-intensive medical systems composed of processors, transducers, ampliﬁers, ﬁlters, and converters. We present in this article a formal modeling methodology of biomedical signal acquisition systems using Colored Petri Nets (CPN) and based on a frequency-domain approach. In the methodology, a reference model represents the main features of these medium risk systems. We argue that this kind of model is useful to assist manufacturers to reduce the number of defects in systems and to generate safety and effectiveness evi- dence throughout certiﬁcation. Therefore, we describe two main contributions in this article. We provide a reference model of biomedical signal acquisition systems and show how manufacturers can generate evidence by means of an electrocardiography (ECG) case study. We carried out the case study by extending the reference model to represent the behavior of an ECG system using a basic cardiac monitor conﬁguration based on the single-lead, heart rate monitor front end (AD8232) and the low power precision analog microcontroller, ARM cortex M3 with dual sigma-delta con- verters (ADUCM360). We veriﬁed the model against safety requirements with the model checking technique (safety evi- dence) and validated it by comparing output signals with a ﬁltered ECG record available on the PHYSIONET ECG-ID database in the frequency and time domains (effectiveness evidence). This methodology enables manufacturers to iden- tify defects in systems earlier in the development process aiming to decrease costs and development time.",
        "keywords": [
            "Biomedical signal acquisition systems",
            "Colored petri nets",
            "Formal methods",
            "Model checking",
            "Frequency-domain approach"
        ],
        "authors": [
            "Alvaro Sobrinho",
            "Leandro Dias da Silva",
            "Angelo Perkusich",
            "Paulo Cunha",
            "Thiago Cordeiro",
            "Antonio Marcus Nogueira Lima"
        ],
        "file_path": "data/sosym-all/s10270-017-0616-7.pdf"
    },
    {
        "title": "Message choreography modeling\nA domain-speciﬁc language for consistent enterprise service integration",
        "submission-date": "2010/12",
        "publication-date": "2012/09",
        "abstract": "Service-based applications are based on modern architectures that require careful design of interfaces and protocols to allow smooth integration of service components. These design artifacts are not only useful for implementation, but could also be used for the derivation of integration tests. In order to be applied in these different activities of the development process, they have to conform to existing requirements and other speciﬁcations at different architectural levels. In addition, their internal consistency has to be ensured. In this paper, we present an approach to service integration based on a domain-speciﬁc language for service choreographies. We ﬁrst explain the motivation for our work by deﬁning the industrial context that led to the deﬁnition of a domain-speciﬁc choreography language, called message choreography modeling (MCM). We then provide syntax and semantics for MCM, together with suitable methods for ensuring its consistency. Finally, we report on our experience in applying the described language in practice.",
        "keywords": [
            "Domain speciﬁc modeling languages",
            "Message choreography models",
            "Service choreography",
            "Enterprise SOA"
        ],
        "authors": [
            "Alin Stefanescu",
            "Sebastian Wieczorek",
            "Matthias Schur"
        ],
        "file_path": "data/sosym-all/s10270-012-0272-x.pdf"
    },
    {
        "title": "Toward a framework for self-adaptive workﬂows in cyber-physical systems",
        "submission-date": "2016/12",
        "publication-date": "2017/11",
        "abstract": "With the establishment of Cyber-physical Systems (CPS) and the Internet of Things, the virtual world of software and services and the physical world of objects and humans move closer together. Despite being a useful means for automation, BPM technologies and workﬂow systems are yet not fully capable of executing processes in CPS. The effects on and possible errors and inconsistencies in the physical world are not considered by “traditional” workﬂow engines. In this work we propose a framework for self-adaptive workﬂows in CPS based on the MAPE-K feedback loop. Within this loop monitoring and analysis of additional sensor and context data is used to check for unanticipated errors in the physical world. Planning and execution of compensation actions restores Cyber-physical Consistency, which leads to an increased resilience of the process execution environment. The framework facilitates the separation of CPS aspects from the “regular” workﬂow views. We show the feasibility of this approach in a smart home scenario and discuss the application of our approach for legacy BPM systems.",
        "keywords": [
            "Workflows for the Internet of Things",
            "Cyber-physical Systems",
            "Self-adaptive Workflows",
            "Real-world processes",
            "Cyber-physical consistency"
        ],
        "authors": [
            "Ronny Seiger",
            "Steffen Huber",
            "Peter Heisig",
            "Uwe Aßmann"
        ],
        "file_path": "data/sosym-all/s10270-017-0639-0.pdf"
    },
    {
        "title": "Evaluating probabilistic models with uncertain model parameters",
        "submission-date": "2012/01",
        "publication-date": "2012/09",
        "abstract": "Probabilistic models are commonly used to evaluate quality attributes, such as reliability, availability, safety and performance of software-intensive systems. The accuracy of the evaluation results depends on a number of system properties which have to be estimated, such as environmental factors or system usage. Researchers have tackled this problem by including uncertainties in the probabilistic models and solving them analytically or with simulations. The input parameters are commonly assumed to be normally distributed. Accordingly, reporting the mean and variances of the resulting attributes is usually considered sufficient. However, many of the uncertain factors do not follow normal distributions,andanalyticalmethodstoderiveobjectiveuncertainties become impractical with increasing complexity of the probabilistic models. In this work, we introduce a simulation-based approach which uses Discrete Time Markov Chains and probabilistic model checking to accommodate a diverse set of parameter range distributions. The number of simulation runs automatically regulates to the desired significance level and reports the desired percentiles of the values which ultimately characterises a specific quality attribute of the system. We include a case study which illustrates the flexibility of this approach using the evaluation of several probabilistic properties.",
        "keywords": [
            "Software architecture evaluation",
            "Parameter uncertainty",
            "Probabilistic quality models",
            "Monte-Carlo simulation"
        ],
        "authors": [
            "Indika Meedeniya",
            "Irene Moser",
            "Aldeida Aleti",
            "Lars Grunske"
        ],
        "file_path": "data/sosym-all/s10270-012-0277-5.pdf"
    },
    {
        "title": "Colouring: execution, debug and analysis of QVT-relations transformations through coloured Petri nets",
        "submission-date": "2010/11",
        "publication-date": "2012/11",
        "abstract": "QVT is the standard language sponsored by the OMG to specify model-to-model transformations. It includes three different languages, being QVT-relations (QVT-R) the one with higher-level of abstraction. Unfortunately, there is scarce tool support for it nowadays, with incompatibilities and disagreements between the few tools implementing it, and lacking support for the analysis and veriﬁcation of transformations. Part of this situation is due to the fact that the standard provides only a semi-formal semantics for QVT-R. In order to alleviate this situation, this paper provides a semantics for QVT-R through its compilation into coloured Petri nets. The theory of coloured Petri nets provides useful techniques to analyse transformations (e.g. detecting relation conﬂicts, or checking whether certain structures are generated or not in the target model) as well as to determine their conﬂuence and termination given a starting model. Our semantics is ﬂexible enough to permit the use of QVT-R speciﬁcations not only for transformation and check-only scenarios, but also for model matching and model comparison, not covered in the original standard. As a proof of concept, we report on the use of CPNTools for the execution, debugging, veriﬁcation and validation of transformations, and on a tool chain (named Colouring) to transform QVT-R speciﬁcations and their input models into the input format of CPNTools, as well as to export and visualize the transformation results back as models.",
        "keywords": [
            "Model-driven engineering",
            "Model-to-model transformations",
            "QVT-relations",
            "Coloured Petri nets",
            "Validation and veriﬁcation"
        ],
        "authors": [
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-012-0292-6.pdf"
    },
    {
        "title": "Localized model transformations for building large-scale transformations",
        "submission-date": "2011/11",
        "publication-date": "2013/09",
        "abstract": "Model-driven engineering (MDE) exploits well-\ndeﬁned, tool-supported modelling languages and operations\napplied to models created using these languages. Model\ntransformation is a critical part of the use of MDE. It has\nbeen argued that transformations must be engineered sys-\ntematically, particularly when the languages to which they\nare applied are large and complicated—e.g., UML 2.x and\nproﬁles such as MARTE—and when the transformation logic\nitself is complex. We present an approach to designing large\nmodel transformations for large languages, based on the\nprinciple of separation of concerns. Speciﬁcally, we deﬁne\na notion of localized transformations that are restricted to\napply to a subset of a modelling language; a composition of\nlocalized transformations is then used to satisfy particular\nMDE objectives, such as the design of very large transfor-\nmations. We illustrate the use of localized transformations\nin a concrete example applied to large transformations for\nsystem-on-chip co-design.",
        "keywords": [
            "Model transformation",
            "Reusable\ntransformation",
            "Transformation chaining"
        ],
        "authors": [
            "Anne Etien",
            "Alexis Muller",
            "Thomas Legrand",
            "Richard F. Paige"
        ],
        "file_path": "data/sosym-all/s10270-013-0379-8.pdf"
    },
    {
        "title": "Supporting domain-speciﬁc model patterns with metamodeling",
        "submission-date": "2007/04",
        "publication-date": "2009/03",
        "abstract": "Metamodeling is a widely applied technique in the ﬁeld of graphical languages to create highly conﬁgurable modeling environments. These environments support the rapid development of domain-speciﬁc modeling languages (DSMLs). Design patterns are efﬁcient solutions for recurring problems. With the proliferation of DSMLs, there is a need for domain-speciﬁc design patterns to offer solutions to problems recurring in different domains. The aim of this paper is to provide theoretical and practical foundations to support domain-speciﬁc model patterns in metamodeling environments. In order to support the treatment of premature model parts, we weaken the instantiation relationship. We provide constructs relaxing the instantiation rules, and we show that these constructs are appropriate and sufﬁcient to express patterns. We provide the necessary modiﬁcations in metamodeling tools for supporting patterns. With the contributed results, a well-founded domain-speciﬁc model pattern support can be realized in metamodeling tools.",
        "keywords": [],
        "authors": [
            "Tihamér Levendovszky",
            "László Lengyel",
            "Tamás Mészáros"
        ],
        "file_path": "data/sosym-all/s10270-009-0118-3.pdf"
    },
    {
        "title": "Automatic data collection for enterprise architecture models",
        "submission-date": "2011/07",
        "publication-date": "2012/06",
        "abstract": "Enterprise Architecture (EA) is an approach used\nto provide decision support based on organization-wide mod-\nmodels is, however, cumbersome as\nmultiple aspects of an organization need to be considered,\nmaking manual efforts time-consuming, and error prone.\nThus, the EA approach would be significantly more prom-\nising if the data used when creating the models could be\ncollected automatically—a topic not yet properly addressed\nby either academia or industry. This paper proposes network\nscanning for automatic data collection and uses an exist-\ning software tool for generating EA models (ArchiMate is\nemployed as an example) based on the IT infrastructure of\nEnterprises. While some manual effort is required to make\nthe models fully useful to many practical scenarios (e.g., to\ndetail the actual services provided by IT components), empir-\nical results show that the methodology is accurate and (in its\ndefault state) require little effort to carry out.",
        "keywords": [
            "Enterprise architecture",
            "Automatic data collection",
            "Network scanning"
        ],
        "authors": [
            "Hannes Holm",
            "Markus Buschle",
            "Robert Lagerström",
            "Mathias Ekstedt"
        ],
        "file_path": "data/sosym-all/s10270-012-0252-1.pdf"
    },
    {
        "title": "Virtual network embedding: ensuring correctness and optimality by construction using model transformation and integer linear programming techniques",
        "submission-date": "2020/03",
        "publication-date": "2021/01",
        "abstract": "Virtualization technology allows service providers to operate data centers in a cost-effective and scalable manner. The data\ncenter network (substrate network) and the applications executed in the data center (virtual networks) are often modeled as\ngraphs. The nodes of the graphs represent (physical or virtual) servers and switches, and the edges represent communication\nlinks. Nodes and links are annotated with the provided and required resources (e.g., memory and bandwidth). The NP-hard\nvirtual network embedding (VNE) problem deals with the embedding of a set of virtual networks to the substrate network\nsuch that (i) all (resource) constraints of the substrate network are fulﬁlled, and (ii) an objective is optimized (e.g., minimizing\nthe communication costs). The existing two-step highly customizable model-driven virtual network embedding (MdVNE)\napproach combines model transformation (MT) and integer linear programming (ILP) techniques to solve the VNE problem\nbased on a declarative speciﬁcation. MdVNE generates element mapping candidates from an MT speciﬁcation and identiﬁes\nan optimal and correct embeddings using an ILP solver. In the past, developers created the MT and ILP speciﬁcations manually\nand needed to ensure carefully that both are compatible and respect the problem description. In this article, we present a novel\nconstruction methodology for synthesizing the MT and ILP speciﬁcation from a given declarative model-based VNE problem\ndescription. This problem description consists of a metamodel for substrate and virtual networks, additional OCL constraints,\nand an objective function that assigns costs to a given model. This methodology ensures that the derived embeddings are\ncorrect w.r.t. the metamodel and the OCL constraints, and optimal w.r.t. the optimization goal. The novel model-based VNE\nspeciﬁcation is applicable to different network domains, environments, and constraints. Thus, the construction methodology\nallows to automate most of the steps to realize a correct and optimal VNE algorithm compared to a hand-crafted VNE\nimplementation. Furthermore, the simulative evaluation conﬁrms that using MT techniques reduces the time for solving the\nVNE problem considerably in comparison with a purely ILP-based approach.",
        "keywords": [
            "Data center",
            "Virtual network embedding",
            "Model-driven development",
            "Integer linear programming",
            "Model\ntransformation",
            "Graph transformation",
            "Triple-graph grammar",
            "Object Constraint Language"
        ],
        "authors": [
            "Stefan Tomaszek",
            "Roland Speith",
            "Andy Schürr"
        ],
        "file_path": "data/sosym-all/s10270-020-00852-z.pdf"
    },
    {
        "title": "Multi-purpose, multi-level feature modeling of large-scale industrial software systems",
        "submission-date": "2016/01",
        "publication-date": "2016/10",
        "abstract": "Feature models are frequently used to capture the knowledge about conﬁgurable software systems and product lines. However, feature modeling of large-scale systems is challenging as models are needed for diverse pur-poses. For instance, feature models can be used to reﬂect the perspectives of product management, technical solution architecture, or product conﬁguration. Furthermore, mod-els are required at different levels of granularity. Although numerous approaches and tools are available, it remains hard to deﬁne the purpose, scope, and granularity of feature mod-els. This paper ﬁrst reports results and experiences of an exploratory case study on developing feature models for two large-scale industrial automation software systems. We report results on the characteristics and modularity of the fea-ture models, including metrics about model dependencies. Based on the ﬁndings from the study, we developed FORCE, a modeling language, and tool environment that extends an existing feature modeling approach to support models for dif-ferent purposes and at multiple levels, including mappings to the code base. We demonstrate the expressiveness and exten-sibility of our approach by applying it to the well-known Pick and Place Unit example and an injection molding sub-system of an industrial product line. We further show how our approach supports consistency between different feature models. Our results and experiences show that consider-ing the purpose and level of features is useful for modeling large-scale systems and that modeling dependencies between feature models is essential for developing a system-wide per-spective.",
        "keywords": [
            "Feature modeling",
            "Large-scale software systems",
            "Case study"
        ],
        "authors": [
            "Daniela Rabiser",
            "Herbert Prähofer",
            "Paul Grünbacher",
            "Michael Petruzelka",
            "Klaus Eder",
            "Florian Angerer",
            "Mario Kromoser",
            "Andreas Grimmer"
        ],
        "file_path": "data/sosym-all/s10270-016-0564-7.pdf"
    },
    {
        "title": "Security modeling for service-oriented systems using security pattern reﬁnement approach",
        "submission-date": "2011/09",
        "publication-date": "2012/08",
        "abstract": "Security is one of the critical aspects of current systems, which are based on loosely coupled and technology-agnostic service-oriented architectures (SOA). Though SOA is the driving force for enterprises to open their ends for global business collaborations, nevertheless it evolves many challenges for modeling and enforcing security. One of the main problems for designing secure systems is the lack of consistent frameworks and methodologies for modeling security concerns. Traditional approaches consider security at the end of system development, which evolves ﬂexible and un-conﬁgurable systems, which are too difﬁcult to maintain and manage. The other major problem with current approaches is that they assume pre-deﬁned and hard-coded security patterns and mechanisms for secure system design. Whereas, the evolving SOA systems require conﬁgurable security to realize different security patterns and secu-rity policies in a variety of business scenarios. To solve these problems, it is necessary to model security concerns from the beginning of system modeling in a platform-independent way. This paper proposes a pattern reﬁnement approach for security modeling to achieve conﬁgurable and declarative security, based on the principles of abstraction, reﬁnement, separation-of-concerns and maintainability to achieve ﬂexibleconﬁgurationsofSOAsecurity.Intheproposedapproach, a Domain Expert deﬁnes abstract policies using common security vocabulary and a Security Expert models security with patterns and reﬁnes them for a target architecture in successive systematic reﬁnements. Furthermore, it facilitates thetransformationofabstractsecuritymodelsintoexecutable security policies for the target platforms.",
        "keywords": [
            "Model-driven security",
            "Security patterns",
            "SOA security",
            "Model transformation"
        ],
        "authors": [
            "Mukhtiar Memon",
            "Gordhan D. Menghwar",
            "Mansoor H. Depar",
            "Akhtar A. Jalbani",
            "Waqar M. Mashwani"
        ],
        "file_path": "data/sosym-all/s10270-012-0268-6.pdf"
    },
    {
        "title": "An architecture framework for enterprise IT service availability analysis",
        "submission-date": "2012/01",
        "publication-date": "2013/01",
        "abstract": "This paper presents an integrated enterprise architecture framework for qualitative and quantitative modeling and assessment of enterprise IT service availability. While most previous work has either focused on formal availability methods such as fault trees or qualitative methods such as maturity models, this framework offers a combination. First, a modeling and assessment framework is described. In addition to metamodel classes, relationships and attributes suitable for availability modeling, the framework also features a formal computational model written in a probabilistic version of the object constraint language. The model is based on 14 systemic factors impacting service availability and also accounts for the structural features of the service architecture. Second, the framework is empirically tested in nine enterprise information system case studies. Based on an initial availability baseline and the annual evolution of the 14 factors of the model, annual availability predictions are made and compared with the actual outcomes as reported in SLA reports and system logs. The practical usefulness of the method is discussed based on the outcomes of a workshop conducted with the participating enterprises, and some directions for future research are offered.",
        "keywords": [
            "Systems availability",
            "Service availability",
            "Downtime",
            "Noisy-OR",
            "System quality analysis",
            "Enterprise Architecture",
            "ArchiMate",
            "Metamodel",
            "OCL"
        ],
        "authors": [
            "Ulrik Franke",
            "Pontus Johnson",
            "Johan König"
        ],
        "file_path": "data/sosym-all/s10270-012-0307-3.pdf"
    },
    {
        "title": "Language-independent look-ahead for checking multi-perspective declarative process models",
        "submission-date": "2019/12",
        "publication-date": "2021/01",
        "abstract": "Declarative process modelling languages focus on describing a process by restrictions over the behaviour, which must be satisﬁedthroughoutthewholeprocessexecution.Hence,theyarewellsuitedformodellingknowledge-intensiveprocesseswith\nmany decision points. However, such models can be hard to read and understand, which affect the modelling and maintenance\nof the process models tremendously as well as their execution. When executing such declarative (multi-perspective) process\nmodels, it may happen that the execution of activities or the change of data values may result in the non-executability of\ncrucial activities. Hence, it would be beneﬁcial to know all consequences of decisions to give recommendations to the process\nparticipants. A look-ahead attempts to predict the effects of executing an activity towards possible consequences within an a\npriori deﬁned time window. The prediction is based on the current state of the process execution, the intended next event and\nthe underlying process model. While execution engines for single-perspective imperative process models already implement\nsuch functionality, execution approaches, for multi-perspective declarative process models that involve constraints on data\nand resources, are less mature. In this paper, we introduce a simulation-based look-ahead approach for multi-perspective\ndeclarative process models. This approach transforms the problem of a context-aware process simulation into a SAT problem,\nby translating a declarative multi-perspective process model and the current state of a process execution into a speciﬁcation of\nthe logic language Alloy. Via a SAT solver, process trajectories are generated that either satisfy or violate this speciﬁcation.\nThe simulated process trajectories are used to derive consequences and effects of certain decisions at any time of process\nexecution. We evaluate our approach by means of three examples and give some advice for further optimizations.",
        "keywords": [
            "Declarative process models",
            "Multi-perspective",
            "Look-ahead",
            "Model checking",
            "Predictive business process monitoring",
            "SAT solving"
        ],
        "authors": [
            "Martin Käppel",
            "Lars Ackermann",
            "Stefan Schönig",
            "Stefan Jablonski"
        ],
        "file_path": "data/sosym-all/s10270-020-00857-8.pdf"
    },
    {
        "title": "Extending organizational capabilities with Open Data to support sustainable and dynamic business ecosystems",
        "submission-date": "2018/10",
        "publication-date": "2019/09",
        "abstract": "Open Data (OD) is data available in a machine-readable format and without restrictions on the permissions for using or distributing it. OD may include textual artifacts, images, maps, video content, and other. The data can be published and maintained by different entities, both public and private. Despite its power to distribute knowledge freely and availability of a large number of datasets, OD initiatives face important challenges related to its widespread take up. More specifically, OD provisioning is based on a unidirectional linking from OD providers to OD users without considering requirements and preferences of the users. The OD users also lack metadata, and they need to develop specific technical solutions for provid-ing a continuous OD flow and processing, which is particularly difficult when real-time OD are to be used. In this paper, we propose solving these challenges by envisioning a business ecosystem for OD. It is network-based, federated, and supports interplay between OD provisioning and knowledge management. As a methodological solution, we have applied the capability-driven development approach, which allows modeling of OD processing ecosystems, facilitates knowledge exchange about OD usage among members of the ecosystem, and supports configuring information systems for OD processing. The proposal is explicated with a theoretical study of its usability for the service of road maintenance in varying conditions.",
        "keywords": [
            "Open Data",
            "Capability",
            "Context",
            "Requirements",
            "CDD"
        ],
        "authors": [
            "Jānis Kampars",
            "Jelena Zdravkovic",
            "Janis Stirna",
            "Jānis Grabis"
        ],
        "file_path": "data/sosym-all/s10270-019-00756-7.pdf"
    },
    {
        "title": "Interactive log-delta analysis using multi-range ﬁltering",
        "submission-date": "2020/11",
        "publication-date": "2021/09",
        "abstract": "Process mining is a family of analytical techniques that extract insights from an event log and present them to an analyst. A key analysis task is to understand the distinctive features of different variants of the process and their impact on process performance. Techniques for log-delta analysis (or variant analysis) put a strong emphasis on automatically extracting explanations for differences between variants. A weakness of them is, however, their limited support for interactively exploring the dividing line between typical and atypical behavior. In this paper, we address this research gap by developing and evaluating an interactive technique for log-delta analysis, which we call InterLog. This technique is developed based on the idea that the analyst can interactively deﬁne ﬁlter ranges and that these ﬁlters are used to partition the log L into sub-logs L1 for the selected cases and L2 for the deselected cases. In this way, the analyst can step-by-step explore the log and manually separate the typical behavior from the atypical. We prototypically implement InterLog and demonstrate its application for a real-world event log. Furthermore, we evaluate it in a preliminary design study with process mining experts for usefulness and ease of use.",
        "keywords": [
            "Process mining",
            "Log-delta analysis",
            "Variant analysis",
            "Multi-range ﬁlter",
            "Event logs",
            "Event sequence data"
        ],
        "authors": [
            "Maxim Vidgof",
            "Djordje Djurica",
            "Saimir Bala",
            "Jan Mendling"
        ],
        "file_path": "data/sosym-all/s10270-021-00902-0.pdf"
    },
    {
        "title": "On the use of domain knowledge for process model repair",
        "submission-date": "2021/12",
        "publication-date": "2022/12",
        "abstract": "Processmodelsareimportantforsupportingorganizationsindocumenting,understandingandmonitoringtheirbusiness.When\nthese process models become outdated, they need to be revised to accurately describe the new status quo of the processes\nin the organization. Process model repair techniques help at automatically revising the existing model from behavior traced\nin event logs. So far, such techniques have focused on identifying which parts of the model to change and how to change\nthem, but they do not use knowledge from practitioners to inform the revision. As a consequence, fragments of the model\nmay change in a way that deﬁes existing regulations or represents outdated information that was wrongly considered from the\nevent log. This paper uses concepts from theory revision to provide formal foundations for process model repair that exploits\ndomain knowledge. Speciﬁcally, it conceptualizes (1) what are unchangeable fragments in the model and (2) the role that\nvarious traces in the event log should play when it comes to model repair. A scenario of use is presented that demonstrates\nthe beneﬁts of this conceptualization. The current state of existing process model repair techniques is compared against the\nproposed concepts. The results show that only two existing techniques partially consider the concepts presented in this paper\nfor model repair.",
        "keywords": [
            "Process model repair",
            "Process mining",
            "Concept drift",
            "Theory revision"
        ],
        "authors": [
            "Kate Revoredo"
        ],
        "file_path": "data/sosym-all/s10270-022-01067-0.pdf"
    },
    {
        "title": "SQME: a framework for modeling and evaluation of software architecture quality attributes",
        "submission-date": "2017/07",
        "publication-date": "2018/05",
        "abstract": "Designing a software architecture that satisﬁes all quality requirements is a difﬁcult task. To determine whether the requirements are achieved, it is necessary to quantitatively evaluate quality attributes on the architecture model. A good evaluation process should have proper answers for these questions: (1) how to feedback the evaluation results to the architecture model (i.e., improve the architecture based on the evaluation results), (2) how to analyze uncertainties in calculations, and (3) how to handle conﬂicts that may exist between the quality preferences of stakeholders. In this paper, we introduce SQME as a framework for automatic evaluation of software architecture models. The framework uses evolutionary algorithms for architecture improvement, evidence theory for uncertainty handling, and EV/TOPSIS for making trade-off decisions. To validate the applicability of the framework, a case study is performed, and a software tool is developed to support the evaluation process.",
        "keywords": [
            "Software architecture",
            "Software quality attributes",
            "Evolutionary algorithms",
            "Evidence theory",
            "EV/TOPSIS"
        ],
        "authors": [
            "Ali Sedaghatbaf",
            "Mohammad Abdollahi Azgomi"
        ],
        "file_path": "data/sosym-all/s10270-018-0684-3.pdf"
    },
    {
        "title": "Models: the fourth dimension of computer science",
        "submission-date": "2021/10",
        "publication-date": "2021/12",
        "abstract": "Models are a universal instrument in science, technology, and daily life. They function as instruments in almost every scenario. Any human activity can be (and is) supported by models, e.g. reason, explain, design, act, predict, explore, communicate, collaborate, interact, orient, direct, guide, socialises, perceive, reﬂect, develop, making sense, teach, learn, imagine, etc. This universal suitability is also the basis for a wide use of models and modelling in Computer Science and Engineering. We claim that models form the fourth dimension in Computer Science. This paper sketches and systematises the main ingredients of the study model and modelling.",
        "keywords": [
            "Model",
            "Study of models and modelling",
            "More"
        ],
        "authors": [
            "Bernhard Thalheim"
        ],
        "file_path": "data/sosym-all/s10270-021-00954-2.pdf"
    },
    {
        "title": "Does aspect-oriented modeling help improve the readability of UML state machines?",
        "submission-date": "2011/11",
        "publication-date": "2012/11",
        "abstract": "Aspect-oriented modeling (AOM) is a relatively recent and very active ﬁeld of research, whose application has, however, been limited in practice. AOM is assumed to yield several potential beneﬁts such as enhanced modular-ization, easier evolution, increased reusability, and improved readability of models, as well as reduced modeling effort. However, credible, solid empirical evidence of such beneﬁts is lacking. We evaluate the “readability” of state machines when modeling crosscutting behavior using AOM and more speciﬁcally AspectSM, a recently published UML pro-ﬁle. This proﬁle extends the UML state machine notation with mechanisms to deﬁne aspects using state machines. Readability is indirectly measured through defect identiﬁ-cation and ﬁxing rates in state machines, and the scores obtained when answering a comprehension questionnaire about the system behavior. With AspectSM, crosscutting behavior is modeled using so-called “aspect state machines”. Their readability is compared with that of system state machines directly modeling crosscutting and standard behav-ior together. An initial controlled experiment and a much larger replication were conducted with trained graduate students, in two different institutions and countries, to achieve the above objective. We use two baselines of comparisons—standard UML state machines without hierarchical features (ﬂat state machines) and standard state machines with hierarchical/concurrent features (hierarchical S. Ali (B) · T. Yue · L. C. Briand Certus Software V&V Center, Simula Research Laboratory, P.O. Box 134, 1325 Lysaker, Norway e-mail: shaukat@simula.no T. Yue e-mail: tao@simula.no L. C. Briand SnT Centre, University of Luxembourg, Luxembourg, Luxembourg e-mail: lionel.briand@uni.lu state machines). The results showed that defect identiﬁcation and ﬁxing rates are signiﬁcantly better with AspectSM than with both ﬂat and hierarchical state machines. How-ever, in terms of comprehension scores and inspection effort, no signiﬁcant difference was observed between any of the approaches. Results of the experiments suggest that one should use, when possible, aspect state machines along with hierarchical and/or concurrent features of UML state machines to model crosscutting behaviors.",
        "keywords": [
            "Aspect-oriented modeling",
            "UML state machines",
            "Controlled experiment",
            "Defect identiﬁcation and ﬁxing",
            "Comprehension"
        ],
        "authors": [
            "Shaukat Ali",
            "Tao Yue",
            "Lionel C. Briand"
        ],
        "file_path": "data/sosym-all/s10270-012-0293-5.pdf"
    },
    {
        "title": "Evaluation of a machine learning classiﬁer for metamodels",
        "submission-date": "2020/05",
        "publication-date": "2021/09",
        "abstract": "Modeling is a ubiquitous activity in the process of software development. In recent years, such an activity has reached a high\ndegree of intricacy, guided by the heterogeneity of the components, data sources, and tasks. The democratized use of models has\nled to the necessity for suitable machinery for mining modeling repositories. Among others, the classiﬁcation of metamodels\nintoindependentcategoriesfacilitatespersonalizedsearchesbyboostingthevisibilityofmetamodels.Nevertheless,themanual\nclassiﬁcation of metamodels is not only a tedious but also an error-prone task. According to our observation, misclassiﬁcation\nis the norm which leads to a reduction in reachability as well as reusability of metamodels. Handling such complexity requires\nsuitable tooling to leverage raw data into practical knowledge that can help modelers with their daily tasks. In our previous\nwork, we proposed AURORA as a machine learning classiﬁer for metamodel repositories. In this paper, we present a thorough\nevaluation of the system by taking into consideration different settings as well as evaluation metrics. More importantly, we\nimprove the original AURORA tool by changing its internal design. Experimental results demonstrate that the proposed\namendment is beneﬁcial to the classiﬁcation of metamodels. We also compared our approach with two baseline algorithms,\nnamely gradient boosted decision tree and support vector machines. Eventually, we see that AURORA outperforms the\nbaselines with respect to various quality metrics.",
        "keywords": [
            "Model-driven engineering",
            "Machine learning",
            "Neural networks",
            "GBDT",
            "SVM"
        ],
        "authors": [
            "Phuong T. Nguyen",
            "Juri Di Rocco",
            "Ludovico Iovino",
            "Davide Di Ruscio",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-021-00913-x.pdf"
    },
    {
        "title": "Standards in software development and modeling",
        "submission-date": "2025/08",
        "publication-date": "2025/08",
        "abstract": "There are important standardization bodies that actually create very good and widely used standards in engineering and development. This is prominent in other engineering domains, but less common in computer science. We may speculate about the reasons, but it may be that computer science is relatively young, and therefore, techniques and methods evolve frequently, and standards may hinder this form of innovation. A second reason may be that in computer science, large companies are developing the de facto standardsthatarenotnecessarilybecomingformalstandards. But in computer science, standards also ensure compatibility, interoperability, reliability, security, reusability, and potentially many other good properties across services, applications, systems, and technologies. And we all know some key categories and examples of relevant standards, such as programming language standards (e.g., ISO/IEC 9899—for C, Java Community Process (JCP) spec—for Java, ECMA-262/ISO/IEC 16262—for JavaScript, HTTP/HTTPS (RFC 9110) protocol—for web communication, and RFC 8259—for JSON). The most relevant standards for Software & Systems Engineering are UML (ﬁrst by the OMG and later by ISO/IEC 19505), IEEE 830 / ISO/IEC/IEEE 29148—for Software Requirements Speciﬁcation, and the newly emerging standards around the digital twin technologies stack that are in discussion by the Digital Twin Consortium (DTC) and the Industrial Digital Twin Association (IDTA).",
        "keywords": [],
        "authors": [
            "Marsha Chechik",
            "Benoit Combemale",
            "JeﬀGray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-025-01312-2.pdf"
    },
    {
        "title": "A compositional semantics of UML-RSDS",
        "submission-date": "2006/06",
        "publication-date": "2007/08",
        "abstract": "This paper provides a semantics for the UML- RSDS (Reactive System Development Support) subset of UML, using the real-time action logic (RAL) formalism. We show how this semantics can be used to resolve some ambiguities and omissions in UML semantics, and to support reasoningaboutspeciﬁcationsusingtheBformalmethodand tools. We use ‘semantic proﬁles’ to provide precise semantics for different semantic variation points of UML. We also show how RAL can be used to give a semantics to notations for real-time speciﬁcation in UML. Unlike other approaches to UML semantics, which concentrate on the class diagram notation, our semantic representation has behaviour as a central element, and can be used to deﬁne semantics for use cases, state machines and interactions, in addition to class diagrams.",
        "keywords": [
            "UML semantics",
            "UML-RSDS",
            "Model transformations"
        ],
        "authors": [
            "K. Lano"
        ],
        "file_path": "data/sosym-all/s10270-007-0064-x.pdf"
    },
    {
        "title": "Guest editorial for the special section on SEFM 2020 and 2021",
        "submission-date": "2024/03",
        "publication-date": "Not found",
        "abstract": "The main objective of the International Conference on Software Engineering and Formal Methods (SEFM) is to bring together practitioners and researchers from academia, industry, and government, to advance the state of the art in formal methods, to help in their large-scale application in the software industry, and to encourage their integration with other practical software engineering methods. This special section consists of a selection of papers presented at SEFM 2020 and 2021, the 18th and 19th editions of SEFM, which have been held virtually during the COVID pandemic.",
        "keywords": [],
        "authors": [
            "Frank S. de Boer",
            "Antonio Cerone"
        ],
        "file_path": "data/sosym-all/s10270-024-01168-y.pdf"
    },
    {
        "title": "Why it is so hard to use models in software development: observations",
        "submission-date": "2013/10",
        "publication-date": "2013/10",
        "abstract": "In a previous editorial, we asked what modeling contributes to the development process. We concluded that models are not so much of value for themselves, but exist to improve certain properties of the product, such as quality or maintainability, or of the process, such as cost-efﬁciency and predictability. In this editorial, we would like to report on speciﬁc ﬁnd-ings that are based on the reported and own experience with some concrete modeling tools and frameworks of different types (without naming them) and draw some conclusions for further tool improvement.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-013-0383-z.pdf"
    },
    {
        "title": "Fabricatable axis: an approach for modelling customized fabrication machines",
        "submission-date": "2021/03",
        "publication-date": "2022/05",
        "abstract": "Digital fabrication tools such as 3D printers, computer-numerically controlled (CNC) milling machines, and laser cutters are becoming increasingly available, ranging from consumer to industrial versions. Recent studies have shown that users, ranging from researchers, to industry professionals, to hobbyists, are interested in modifying and changing the inherit workﬂows these tools provide. As an answer to this, these users are increasingly modifying and customizing their machines by changing the work envelope, adding different end-effectors, and creating their own fabrication workﬂows in software. However, customizing, modifying and creating digital fabrication machines and the workﬂows they provide require extensive knowledge within multiple different engineering domains and is non-trivial. In this article we present a model-driven approach that enables users to expand their digital fabrication scope by providing a high-level tool that facilitates the customization of fabrication tools. We present The Farbicatable Axis, a model that enables users to create customized linear actuators. The model takes high-level input parameters such as length and gearing-parameters, and outputs a CAD model of a linear motion axis consisting of fabricatable parts. We then present how instances of the Fabricatable Axis can be combined and used to design and implement Fabricatable Machines.",
        "keywords": [
            "Model driven engineering",
            "Digital fabrication",
            "Machine building",
            "CNC",
            "CAD/CAM"
        ],
        "authors": [
            "Frikk H. Fossdal",
            "Rogardt Heldal",
            "Jens Dyvik",
            "Adrian Rutle"
        ],
        "file_path": "data/sosym-all/s10270-022-01007-y.pdf"
    },
    {
        "title": "Introduction to the SoSyM theme issue on models and evolution",
        "submission-date": "2013/04",
        "publication-date": "2013/04",
        "abstract": "Software artifacts are subject to many sources of evolutionary pressure, which range from technical changes due to rapidly evolving technology platforms, to modiﬁcations caused by new requirements and insights emerging from the business domain. These modiﬁcations include changes at all levels, from requirements through architecture and design, to source code, documentation and test suites, and might affect any kinds of models. Therefore, adopting models, techniques, and tools for coping with and managing changes that accompany the evolution of software models is an essential discipline of Software Engineering.",
        "keywords": [],
        "authors": [
            "Dalila Tamzalit",
            "Bernhard Schätz",
            "Alfonso Pierantonio",
            "Dirk Deridder"
        ],
        "file_path": "data/sosym-all/s10270-013-0338-4.pdf"
    },
    {
        "title": "How do I find reusable models?",
        "submission-date": "2022/03",
        "publication-date": "2023/04",
        "abstract": "Models play a major role in model-based development and serve as the main artifacts that stakeholders aim to achieve. As it is difficult to develop good-quality models, repositories of models start emerging for reuse purposes. Yet, these repositories face several challenges, such as model representation, scalability, heterogeneity, and how to search for models. In this paper, we aim to address the challenge of querying model repositories by proposing a generic search framework that looks for models that match the intention of the user. The framework is based on a greedy search approach using a similarity function that considers type similarity, structure similarity, and label similarity. We evaluate the framework’s efficiency on different model types: UML class diagrams, Human Know-How, and ME maps. We further compare it with existing alternatives. The evaluation indicates that the framework achieved high performance within a bounded time, and the framework can be adapted to different modeling languages for searching related, reusable models.",
        "keywords": [
            "Model-based development",
            "Search",
            "Greedy algorithm",
            "Similarity",
            "Model repositories"
        ],
        "authors": [
            "Maxim Bragilovski",
            "Roni Stern",
            "Arnon Sturm"
        ],
        "file_path": "data/sosym-all/s10270-023-01103-7.pdf"
    },
    {
        "title": "Feedback on our editorials",
        "submission-date": "2007/05",
        "publication-date": "2007/05",
        "abstract": "In our editorials we give our perspective on the current state of the research and practice with respect to the use of models in software and systems development. In some cases we highlight what we consider to be promising new and emerging research directions, and we sometimes give our perspective on problems arising from immature and incorrect use of models. The editorials are written to stimulate discussion and encourage exploration of new areas of research in modeling software-based systems.",
        "keywords": [],
        "authors": [
            "Robert France",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-007-0059-7.pdf"
    },
    {
        "title": "Effective development of automation systems through domain-speciﬁc modeling in a small enterprise context",
        "submission-date": "2010/11",
        "publication-date": "2012/10",
        "abstract": "High development and maintenance costs and a\nhigh error rate are the major problems in the development of\nautomation systems, which are mainly caused by bad com-\nmunication and inefﬁcient reuse methods. To overcome these\nproblems, we propose a more systematic reuse approach.\nThough systematic reuse approaches such as software prod-\nuct lines are appealing, they tend to involve rather burden-\nsome development and management processes. This paper\nfocuses on small enterprises. Since such companies are often\nunable to perform a “big bang” adoption of the software prod-\nuct line, we suggest an incremental, more lightweight process\nto transition from single-system development to software\nproduct line development. Besides the components of the\ntransition process, this paper discusses tool selection, DSL\ntechnology, stakeholder communication support, and busi-\nness considerations. Although based on problems from the\nautomation system domain, we believe the approach may be\ngeneral enough to be applicable in other domains as well. The\napproach has proven successful in two case studies. First, we\napplied it to a research project for the automation of a logis-\ntics lab model, and in the second case (a real-life industry\ncase), we investigated the approaches suitability for ﬁsh farm\nautomation systems. Several metrics were collected through-\nout the evolution of each case, and this paper presents the\ndata for single system development, clone&own and soft-\nware product line development. The results and observable\neffects are compared, discussed, and ﬁnally summarized in\na list of lessons learned.",
        "keywords": [
            "Domain-speciﬁc modeling",
            "Small enterprise\ncost model",
            "Automation system",
            "Software product line",
            "System development process"
        ],
        "authors": [
            "Andrea Leitner",
            "Christopher Preschern",
            "Christian Kreiner"
        ],
        "file_path": "data/sosym-all/s10270-012-0289-1.pdf"
    },
    {
        "title": "Teaching conceptual modelling in the age of LLMs: shifting from model creation to model evaluation skills",
        "submission-date": "2025/01",
        "publication-date": "Not found",
        "abstract": "When LLMs are used to assist the development of artefact, it may become difﬁcult to distinguish the genuine contribution by the human modeller from the parts that were generated by the LLM. This is not different for conceptual modelling (CM): an LLM may be asked to generate a CM for a given description. To assess a student’s modelling competences, it’s therefore not sufﬁcient to simply assess the model as an output of their work (i.e. the quality of the created model, along different dimensions). This raises two important questions: How should we assess the modelling capabilities of a student, given that a (large) portion of the model may have been generated by an LLM? And: What skills are needed to create a model? Which ones are essentially human? What if part of these are replaced by LLMs? We posit that when teaching CM, instead of focussing on model creation, we rather need to assess a student’s capability of evaluating, reﬁning and improving models according to requirements, technical constraints, etc. This iterative process of evaluating, reﬁning and improving the model is in line with the utility of modelling as instrument of communication instrument, global design and design exploration and constitutes the essence of modelling skill. We thus need to focus on the capability of critically evaluating a model rather than on model creation, and model creation capabilities will naturally follow from these evaluation capabilities. It is our point that this vision will reinforce the beneﬁts of using LLMs for an improved CM teaching practice.",
        "keywords": [
            "Conceptual modelling",
            "Conceptual modelling education",
            "LLMs",
            "AI-assisted modelling"
        ],
        "authors": [
            "Monique Snoeck",
            "Oscar Pastor"
        ],
        "file_path": "data/sosym-all/s10270-025-01307-z.pdf"
    },
    {
        "title": "On submodels and submetamodels with their relation\nA uniform formalization through inclusion properties",
        "submission-date": "2015/07",
        "publication-date": "2016/06",
        "abstract": "Model-driven engineering (MDE) recognized\nsoftware models as ﬁrst-class objects with their own relation-\nships and operations, up to constitute full structured model\nspaces. We focus on inclusion capacities through the con-\ncepts of submodel and submetamodel which contribute a\nlot to the structuring effort. Submodels and submetamod-\nels underlie many MDE practices which require their precise\ncharacterization for plain control. A typical application is\nmodel management as offered by model repositories. On the\nbasis of results on submodel inclusion we stated in a preced-\ning paper, we concentrate on the special form of submodels\nwhich are submetamodels and their speciﬁc role in model\nspace structuring. Pointing out that relating submodels and\nsubmetamodels is two ways, their respective inclusion hierar-\nchies will be systematically characterized and symmetrically\ncomparedunderthelogicalrelationshipsofmetamodelmem-\nbership and model well-formedness. As a major result, it will\nbe shown that submodel well-formedness w.r.t submetamod-\nels closely relates to submodel invariance (a property which\nguarantees transitive structure preservation) applied at both\nlevels. The uniform formalization offers algebraic grounding\nto better comprehension and control of model spaces which\nunderlie MDE activities. At a much more practical level,\nreusable technology which takes advantage of established\nresults will be offered.",
        "keywords": [
            "Submodel",
            "Submetamodel",
            "Model space",
            "Set-theoretic formalization",
            "Model repository"
        ],
        "authors": [
            "Bernard Carré\nGilles Vanwormhoudt\nOlivier Caron"
        ],
        "file_path": "data/sosym-all/s10270-016-0540-2.pdf"
    },
    {
        "title": "Guest editorial for the special section on MODELS 2014",
        "submission-date": "2016/08",
        "publication-date": "2016/09",
        "abstract": "The MODELS conference series is the premier venue for model-based software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. This special section presents the six articles that resulted from an invitation to authors of the best papers at the MODELS 2014 conference to submit revised and extended versions of their papers for publication in SoSyM.",
        "keywords": [],
        "authors": [
            "Juergen Dingel",
            "Wolfram Schulte"
        ],
        "file_path": "data/sosym-all/s10270-016-0561-x.pdf"
    },
    {
        "title": "Unleashing textual descriptions of business processes",
        "submission-date": "2020/06",
        "publication-date": "2021/05",
        "abstract": "Textual descriptions of processes are ubiquitous in organizations, so that documentation of the important processes can be accessible to anyone involved. Unfortunately, the value of this rich data source is hampered by the challenge of analyzing unstructured information. In this paper we propose a framework to overcome the current limitations on dealing with textual descriptions of processes. This framework considers extraction and analysis and connects to process mining via simulation. The framework is grounded in the notion of annotated textual descriptions of processes, which represents a middle-ground between formalization and accessibility, and which accounts for different modeling styles, ranging from purely imperative to purely declarative. The contributions of this paper are implemented in several tools, and case studies are highlighted.",
        "keywords": [
            "Business process management",
            "Natural language processing",
            "Temporal logics",
            "Process mining",
            "Model checking",
            "Simulation"
        ],
        "authors": [
            "Josep Sànchez-Ferreres",
            "Andrea Burattin",
            "Josep Carmona",
            "Marco Montali",
            "Lluís Padró",
            "Luís Quishpi"
        ],
        "file_path": "data/sosym-all/s10270-021-00886-x.pdf"
    },
    {
        "title": "Guest editorial for the special section on the 26th international conference on model-driven engineering languages and systems (MODELS 2023)",
        "submission-date": "2025/08",
        "publication-date": "Not found",
        "abstract": "The MODELS conference series is the premier venue for model-driven software and systems engineering covering all aspects of modeling, from languages and methods to tools and applications. This special section presents the twelve articles that resulted from the subsequent SoSyM reviewing process, which are extended versions of best papers at MODELS 2023.",
        "keywords": [],
        "authors": [
            "Antonio Cicchetti",
            "Thomas Kühne",
            "Alfonso Pierantonio",
            "Gabriele Taentzer"
        ],
        "file_path": "data/sosym-all/s10270-025-01322-0.pdf"
    },
    {
        "title": "Execution trace analysis for a precise understanding of latency violations",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Despite the amount of proposed works for the veriﬁcation of embedded systems, understanding the root cause of violations of requirements in simulation or execution traces is still an open issue, especially when dealing with temporal properties such as latencies. Is the violation due to an unfavorable real-time scheduling, to contentions on buses, to the characteristics of functional algorithms or hardware components? The paper introduces the Precise Latency ANalysis approach (PLAN), a new trace analysis technique whose objective is to classify execution transactions according to their impact on latency. To do so, we rely ﬁrst on a model transformation that builds up a dependency graph from an allocation model, thus including hardware and software aspects of a system model. Then, from this graph and an execution trace, our analysis can highlight how software or hardware elements contributed to the latency violation. The paper ﬁrst formalizes the problem before applying our approach to simulation traces of SysML models. A case study deﬁned in the AQUAS European project illustrates the relevance of our approach. Last, a performance evaluation gives computation times for several models and requirements.",
        "keywords": [
            "Embedded systems",
            "Execution trace analysis",
            "Dependency graph",
            "Model-based systems engineering (MBSE)",
            "Timing analysis",
            "Simulation"
        ],
        "authors": [
            "Maysam Zoor",
            "Ludovic Apvrille",
            "Renaud Pacalet",
            "Sophie Coudert"
        ],
        "file_path": "data/sosym-all/s10270-022-01076-z.pdf"
    },
    {
        "title": "An integrated metamodel-based approach to software model refactoring",
        "submission-date": "2016/06",
        "publication-date": "2017/10",
        "abstract": "Abstract Software refactoring is the process of changing a\nsoftware system in a manner that does not alter its external\nbehavior and yet improving its internal structure. Model-\ndriven architecture and the popularity of the UML enabled\nthe application of refactoring at model level, which was\nearlier applied to software code. In this paper, we propose\na multi-view integrated approach to model-driven refactor-\ning using UML models. We selected a single model from\neach UML view at metamodel level to construct an inte-\ngrated metamodel. We selected class diagram to represent\nthe structural view, sequence diagram to represent the behav-\nioral view and use case diagram to represent the functional\nview. We validated the proposed approach by comparing\nintegrated refactoring approach with refactoring applied to\nmodels individually in terms of quality improvement through\nUML model metrics. Our results indicate that more bad smell\ninstances canbedetectedusingtheintegratedapproachrather\nthan the individual refactoring approach.",
        "keywords": [
            "Refactoring",
            "Metamodel",
            "UML",
            "Model refactoring"
        ],
        "authors": [
            "Mohammed Misbhauddin",
            "Mohammad Alshayeb"
        ],
        "file_path": "data/sosym-all/s10270-017-0628-3.pdf"
    },
    {
        "title": "The wild-west of modeling (Revisited)",
        "submission-date": "2021/10",
        "publication-date": "2021/10",
        "abstract": "Modeling is a creative and intellectually focused activity that has a long history and tradition. Ancient Greek philosophers discussed “what is original” and “how to discern the essential nature of a thing” using models. They endeavored to understand the physical world, the biological world, humanity, human medical conditions, and so on by reducing concrete and specific concepts into simpler analogies and rules. One could even argue that the very first models were cave drawings, which show hunting scenes (such as the one on SoSyM’s cover!) to describe the process of hunting successfully for teaching purposes. Cave drawings therefore have a purpose and fulfill the general criteria for being a model. The actual word “modeling” was used already in the twelfth century in Italy, when 1:10 miniature and wooden buildings from churches were called “models” and were used to represent a newly constructed building for stakeholder and developer discussions (and potentially to help raise the money needed for construction) before actually creating the real physical structure.\nModeling is one of the key supports for scientific and engineering disciplines. However, the use of explicit and precisely defined modeling languages is relatively new and has developed primarily in the context of software support for the modeling activity. Interaction with machines has enforced precise syntactic forms; that is, the machine clearly accepts or rejects a model, before any computation is performed or transformation occurs into some other kind of model, executable program, or test infrastructure. A precise semantics requires definition through the behavior of a machine.",
        "keywords": [],
        "authors": [
            "Jeﬀ Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-021-00932-8.pdf"
    },
    {
        "title": "Editorial to theme section on modeling in low-code development platforms",
        "submission-date": "2022/09",
        "publication-date": "2022/09",
        "abstract": "The growing need for secure, trustworthy, and cost-efﬁcient software, as well as recent developments in cloud computing technologies, and the shortage of highly skilled professional software developers have given rise to a new generation of low-code software development platforms, such as Google AppSheet and Microsoft PowerApps. Low-code platforms enable the development and deployment of fully functional applications using mainly visual abstractions and interfaces and requiring little or no procedural code. This makes them accessible to an increasingly digital-native and tech-savvy workforce who can directly and effectively contribute to the software development process, even if they lack a programming background. At the heart of low-code applications are typically models of the structure, the behavior, and the presentation of the application. In addition, low-code application models need to be edited (using graphical and textual interfaces), validated, version-controlled, and eventually transformed or interpreted to deliver user-facing applications. These activities have been of core interest to the modeling community over the last two decades. However, engineering and employing low-code development platforms still encompasses many research topics, including enabling citizen/end-user software development, realizing recommender systems for low-code platforms, interoperability issues between low-code platforms, and scalability issues in low-code development.",
        "keywords": [],
        "authors": [
            "Davide Di Ruscio",
            "Esther Guerra",
            "Massimo Tisi"
        ],
        "file_path": "data/sosym-all/s10270-022-01045-6.pdf"
    },
    {
        "title": "Guest editorial to the theme section on Multi-Paradigm Modeling for Cyber-Physical Systems",
        "submission-date": "2021/03",
        "publication-date": "2021/04",
        "abstract": "This theme section aims to disseminate the latest research results in the area of Multi-Paradigm Modeling for Cyber-Physical Systems (MPM4CPS). MPM has a long tradition within the Model-Driven Engineering community, e.g., several workshops have been held at the MODELS conference for over more than a decade. The MPM4CPS workshop series is a continuation of the successful MPM workshop series with a stronger focus on CPS as especially these systems pose several new challenges on the engineering process and beyond. This theme section covers papers on the foundations and applications of MPM for CPS. In total, we accepted ﬁve submissions for publication in the theme section after a thorough peer-reviewing process.",
        "keywords": [
            "Multi-paradigm modeling",
            "Model-driven engineering",
            "Systems engineering",
            "Cyber-physical systems"
        ],
        "authors": [
            "Eugene Syriani",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-021-00882-1.pdf"
    },
    {
        "title": "Synchronizing concurrent model updates based on bidirectional transformation",
        "submission-date": "2009/11",
        "publication-date": "2011/01",
        "abstract": "Model-driven software development often involves several related models. When models are updated, the updates need to be propagated across all models to make them consistent. A bidirectional model transformation keeps two models consistent by updating one model in accordance with the other. However, it does not work when the two models are modiﬁed at the same time. In this paper we first examine the requirements for synchronizing concurrent updates. We view a synchronizer for concurrent updates as a function taking the two original models and the two updated models as input, and producing two new models where the updates are synchronized. We argue that the synchronizer should satisfy three properties that we define to ensure a reasonable synchronization behavior. We then propose a new algorithm to wrap any bidirectional transformation into a synchronizer with the help of model difference approaches. We show that synchronizers produced by our algorithm are ensured to satisfy the three properties if the bidirectional transformation satisfies the correctness property and the hippocraticness property. We also show that the history ignorance property contributes to the symmetry of our algorithm. An implementation of our algorithm shows that it worked well in a practical runtime management framework.",
        "keywords": [
            "Model synchronization",
            "Bidirectional transformation",
            "Concurrent updates",
            "Model difference"
        ],
        "authors": [
            "Yingfei Xiong",
            "Hui Song",
            "Zhenjiang Hu",
            "Masato Takeichi"
        ],
        "file_path": "data/sosym-all/s10270-010-0187-3.pdf"
    },
    {
        "title": "Correction to: Evaluation of a machine learning classiﬁer for metamodels",
        "submission-date": "2021/11",
        "publication-date": "2021/11",
        "abstract": "Unfortunately, in the original publication, reference 70 was published wrongly as “Rössler, A. M. S., Günnemann, S.: Thingml+: Augmenting model-driven software engineering for the internet of things with machine learning. In: R. Hebig and T. Berger, editors, Proceedings of Workshops co-located with MODELS 2018, Copenhagen, Denmark, October, 14, 2018, volume 2245 of CEUR Workshop Proceedings, pp 521–523. CEUR-WS.org, (2018).” The original article can be found online at https://doi.org/10.1007/s10270-021-00913-x. The correct reference should be “Moin, A., Rössler, S., Günnemann, S.: Thingml+: Augmenting model-driven soft-ware engineering for the internet of things with machine learning. In: R. Hebig and T. Berger, editors, Proceedings of Workshops co-located with MODELS 2018, Copenhagen, Denmark, October, 14, 2018, volume 2245 of CEUR Work-shop Proceedings, pp 521–523. CEUR-WS.org, (2018).",
        "keywords": [],
        "authors": [
            "Phuong T. Nguyen",
            "Juri Di Rocco",
            "Ludovico Iovino",
            "Davide Di Ruscio",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-021-00944-4.pdf"
    },
    {
        "title": "Guest editorial to the theme issue on non-functional system properties in domain specific modeling languages",
        "submission-date": "2010/08",
        "publication-date": "2010/08",
        "abstract": "Complexity of software system has been recognized as the major cause of difficulties in making software system development an engineering discipline. As identified by F. Brooks (F. Brooks, The Mythical Man-Month, Addison Wesley, 1995), complexity of software system consists of the essential complexity, the complexity of problems that are part of requirements placed upon software systems, and accidental complexity caused by the use of current software engineering methods and technologies. Current trends in the use and development of information technology raise the both dimensions of complexity. Potentials recognized in software systems increase the number of their applications, and thus, causing the number and size of problems to grow. Increasing demands for software systems also push different vendors in producing implementation technologies, which promise to have solutions for all problems and such technologies require significant knowledge from software developers, often not very helpful in solving domain problems. As a solution to constant increase of complexity, model driven engineering (MDE) arises as one of the most prominent approaches. Based on ideas of direct representation, where solutions to problems are specified within the problem domain using domain specific terms and models, and automation, where implementations of software systems are (semi)automatically generated from domain specific models, MDE promotes (software) system development by problem-domain experts (e.g. home automation, insurance, performance, reliability) and not implementation experts.",
        "keywords": [],
        "authors": [
            "Marko Boškovi´c",
            "Dragan Gaševi´c",
            "Claus Pahl",
            "Bernhard Schätz"
        ],
        "file_path": "data/sosym-all/s10270-010-0171-y.pdf"
    },
    {
        "title": "Guiding the evolution of product-line configurations",
        "submission-date": "2020/09",
        "publication-date": "2021/07",
        "abstract": "A product line is an approach for systematically managing configuration options of customizable systems, usually by means of features. Products are generated for configurations consisting of selected features. Product-line evolution can lead to unintended changes to product behavior. We illustrate that updating configurations after product-line evolution requires decisions of both, domain engineers responsible for product-line evolution as well as application engineers responsible for configurations. The challenge is that domain and application engineers might not be able to interact with each other. We propose a formal foundation and a methodology that enables domain engineers to guide application engineers through configuration evolution by sharing knowledge on product-line evolution and by defining automatic update operations for configurations. As an effect, we enable knowledge transfer between those engineers without the need for interactions. We evaluate our methodology on four large-scale industrial product lines. The results of the qualitative evaluation indicate that our method is flexible enough for real-world product-line evolution. The quantitative evaluation indicates that we detect product behavior changes for up to 55.3% of the configurations which would not have been detected using existing methods.",
        "keywords": [
            "Product line",
            "Product-line evolution",
            "Guided feature configuration evolution",
            "Product behavior preservation"
        ],
        "authors": [
            "Michael Nieke",
            "Gabriela Sampaio",
            "Thomas Thüm",
            "Christoph Seidl",
            "Leopoldo Teixeira",
            "Ina Schaefer"
        ],
        "file_path": "data/sosym-all/s10270-021-00906-w.pdf"
    },
    {
        "title": "Modeling in the large: model libraries",
        "submission-date": "2021/05",
        "publication-date": "2021/05",
        "abstract": "Modeling in the Large is a key concept that is vital toward addressing the growing complexity and organizational requirements that are faced by developers when applying modeling techniques to real-world problems. For the most simple products, it is usually not necessary to define and follow a complicated formal development process. Modeling is particularly beneficial if the product is complex, comes in many different variants, or if the product is for a highly regulated domain (e.g., safety and security regulations). In these cases, one model cannot describe the whole product, but many models are needed to define multiple interacting concerns, often requiring several different languages to describe various aspects and viewpoints of the products or parts of the system under development.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-021-00887-w.pdf"
    },
    {
        "title": "A model-based reference architecture for complex assistive systems and its application",
        "submission-date": "2023/03",
        "publication-date": "2024/03",
        "abstract": "Complex assistive systems providing human behavior support independent of the age or abilities of users are broadly used in a variety of domains including automotive, production, aviation, or medicine. Current research lacks a common understanding of which architectural components are needed to create assistive systems that use models at runtime. Existing descriptions of architectural components are focused on particular domains, consider only some parts of an assistive system, or do not consider models at runtime. We have analyzed common functional requirements for such systems to be able to propose a set of reusable components, which have to be considered when creating assistive systems that use models. Such components constitute a reference architecture that we propose within this paper. To validate the proposed architecture, we have expressed the architectures of two assistive systems from different domains, namely assistance for elderly people and assistance for operators in smart manufacturing in terms of compliance with such architecture. The proposed reference architecture will facilitate the creation of future assistive systems.",
        "keywords": [
            "Assistive systems",
            "Context-aware",
            "Reference architecture",
            "Model-based software engineering",
            "Daily activities support",
            "Assistive digital twin"
        ],
        "authors": [
            "Judith Michael",
            "Volodymyr A. Shekhovtsov"
        ],
        "file_path": "data/sosym-all/s10270-024-01157-1.pdf"
    },
    {
        "title": "On theory and management of dependencies between models",
        "submission-date": "2025/06",
        "publication-date": "2025/06",
        "abstract": "Software developers often need to manage dependencies. Unfortunately, software dependencies manifest themselves in various forms, and discussions about dependencies can be challenging due to the very different definitions and relationships that developers may have in mind. To reduce misunderstandings, it may be helpful to categorize the various forms of dependencies.",
        "keywords": [],
        "authors": [
            "Marsha Chechik",
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-025-01301-5.pdf"
    },
    {
        "title": "A framework for FMI-based co-simulation of human–machine interfaces",
        "submission-date": "2018/09",
        "publication-date": "2019/09",
        "abstract": "A framework for co-simulation of human–machine interfaces in Cyber-Physical Systems (CPS) is presented. The framework builds on formal (i.e. mathematical) methods. It aims to support the work of formal methods experts in charge of modelling and analysing safety-critical aspects of user interfaces in CPS. To carry out these modelling and analysis activities, formal methods experts usually need to engage with domain experts that may not fully understand the mathematical details of formal analysis results. The framework presented in this work mitigates this communication barrier by allowing formal methods experts to create interactive prototypes driven by formal models. The prototypes closely resemble the visual appearance of the system being developed. They can be used to discuss details of the formal analysis effort without showing any mathematical detail. An existing prototyping toolkit based on formal methods is used as baseline technology. Novel functionalities are developed for automatic generation of interactive prototypes supporting the Functional Mockup Interface (FMI), a de-facto standard technology for simulation of complex systems. Using the FMI interface, the prototypes can be integrated with simulations of other system components. The architecture of the framework is presented, along with a veriﬁcation of core aspects of its functionalities. A case study based on a medical system is used to demonstrate the capabilities of the framework.",
        "keywords": [
            "User interfaces",
            "Prototyping tools",
            "FMI co-simulation",
            "Model-based design"
        ],
        "authors": [
            "Maurizio Palmieri",
            "Cinzia Bernardeschi",
            "Paolo Masci"
        ],
        "file_path": "data/sosym-all/s10270-019-00754-9.pdf"
    },
    {
        "title": "Editorial for the SoSyM issue 2015/03",
        "submission-date": "2015/06",
        "publication-date": "2015/06",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-015-0478-9.pdf"
    },
    {
        "title": "Correction: Automaton-based comparison of Declare process models",
        "submission-date": "2023/01",
        "publication-date": "2023/01",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Nicolai Schützenmeier",
            "Martin Käppel",
            "Lars Ackermann",
            "Stefan Jablonski",
            "Sebastian Petter"
        ],
        "file_path": "data/sosym-all/s10270-022-01079-w.pdf"
    },
    {
        "title": "From enterprise models to low-code applications: mapping DEMO to Mendix; illustrated in the social housing domain",
        "submission-date": "2023/03",
        "publication-date": "2024/04",
        "abstract": "Due to hyper-competition, technological advancements, regulatory changes, etc, the conditions under which enterprises need to thrive become increasingly turbulent. Consequently, enterprise agility increasingly determines an enterprise’s chances for success. As software development often is a limiting factor in achieving enterprise agility, enterprise agility and software adaptability become increasingly intertwined. As a consequence, decisions that regard ﬂexibility should not be left to software developers alone. By taking a Model-driven Software Development (MDSD) approach, starting from DEMO ontological enterprise models and explicit (enterprise) implementation design decisions, the aim of this research is to bridge the gap from enterprise agility to software adaptability, in such a way that software development is no longer a limiting factor in achieving enterprise agility. Low-code technology is a growing market trend that builds on MDSD concepts and claims to offer a high degree of software adaptability. Therefore, as a ﬁrst step to show the potential beneﬁts to use DEMO ontological enterprise models as a base for MDSD, this research shows the design of a mapping from DEMO models to Mendix for the (automated) creation of a low-code application that also intrinsically accommodates run-time implementation design decisions.",
        "keywords": [
            "Enterprise modeling",
            "Enterprise ontology",
            "DEMO",
            "MDSD",
            "Low-code",
            "Mendix"
        ],
        "authors": [
            "Marien R. Krouwel",
            "Martin Op ’t Land",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-024-01156-2.pdf"
    },
    {
        "title": "Investigating expressiveness and understandability of hierarchy in declarative business process models",
        "submission-date": "2012/09",
        "publication-date": "2013/06",
        "abstract": "Hierarchy has widely been recognized as a viable approach to deal with the complexity of conceptual models. For instance, in declarative business process models, hierarchy is realized by sub-processes. While technical implementations of declarative sub-processes exist, their application, semantics, and the resulting impact on understandability are less understood yet—this research gap is addressed in this work. More specifically, we discuss the semantics and the application of hierarchy and show how sub-processes enhance the expressiveness of declarative modeling languages. Then, we turn to the influence of hierarchy on the understandability of declarative process models. In particular, we present a cognitive-psychology-based framework that allows to assess the impact of hierarchy on the understandability of a declarative process model. To empirically test the proposed framework, a combination of quantitative and qualitative research methods is followed. While statistical tests provide numerical evidence, think-aloud protocols give insights into the reasoning processes taking place when reading declarative process models.",
        "keywords": [
            "Business process management",
            "Declarative business process models",
            "Modularization",
            "Understandability",
            "Cognitive psychology"
        ],
        "authors": [
            "Stefan Zugal",
            "Pnina Soffer",
            "Cornelia Haisjackl",
            "Jakob Pinggera",
            "Manfred Reichert",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-013-0356-2.pdf"
    },
    {
        "title": "Modeling and reasoning about uncertainty in goal models: a decision-theoretic approach",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Goal models have been a popular subject of study by researchers in requirements engineering, due to their ability to capture and analyze alternative solutions through which a software system can achieve business objectives. A plethora of analysis methods for automated identiﬁcation of optimal alternatives have been proposed. However, such methods often assume an idealized reality where all tasks are successfully performed when attempted and all goals are eventually satisﬁed with certainty when pursued according to a solution. In reality, some tasks run the risk of failure while others produce chance outcomes. In this paper, we extend the standard goal modeling language to allow representation and reasoning about both uncertainty and preferential utility in goals. Tasks are extended to allow for probabilistic effects and preferential statements of stakeholders are captured and translated into utilities over possible effects. Moreover, solutions are not mere speciﬁcations (functions, quality constraints, and assumptions), but rather policies, that is sequences of situational action decisions, through which stakeholder goals can be fulﬁlled. An AI reasoning tool is adapted and used for identifying optimal policies with respect to the value they offer to stakeholders measured against their probability of failure. Evaluation of the approach includes a simulation study and scalability experiments to assess the applicability of automated reasoning for larger problems.",
        "keywords": [
            "Goal modeling",
            "Markov decision processes (MDP)",
            "DT-Golog",
            "Golog"
        ],
        "authors": [
            "Sotirios Liaskos",
            "Shakil M. Khan",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-021-00968-w.pdf"
    },
    {
        "title": "Modeling and enforcing secure object ﬂows in process-driven SOAs: an integrated model-driven approach",
        "submission-date": "2011/10",
        "publication-date": "2012/10",
        "abstract": "In this paper, we present an integrated model-driven approach for the speciﬁcation and the enforcement of secure object ﬂows in process-driven service-oriented architectures (SOA). In this context, a secure object ﬂow ensures the conﬁdentiality and the integrity of important objects (such as business contracts or electronic patient records) that are passed between different participants in SOA-based business processes. We specify a formal and generic meta-model for secure object ﬂows that can be used to extend arbitrary process modeling languages. To demonstrate our approach, we present a UML extension for secure object ﬂows. Moreover, we describe how platform-independent models are mapped to platform-speciﬁc software artifacts via automated model transformations. In addition, we give a detailed description of how we integrated our approach with the Eclipse modeling tools.",
        "keywords": [
            "Process modeling",
            "Secure object ﬂows",
            "Security engineering",
            "Service-oriented architecture",
            "Model-driven development",
            "UML",
            "SoaML",
            "Web services"
        ],
        "authors": [
            "Bernhard Hoisl",
            "Stefan Sobernig",
            "Mark Strembeck"
        ],
        "file_path": "data/sosym-all/s10270-012-0263-y.pdf"
    },
    {
        "title": "Extracting ﬁnite state representation of Java programs",
        "submission-date": "2013/05",
        "publication-date": "2014/06",
        "abstract": "We present a static analysis-based technique for\nreverse engineering ﬁnite state machine models from a large\nsubset of sequential Java programs. Our approach enumer-\nates all feasible program paths in a class using symbolic exe-\ncution and records execution summary for each path. Sub-\nsequently, it creates states and transitions by analyzing sym-\nbolic execution summaries. Our approach also detects any\nunhandled exceptions.",
        "keywords": [
            "Software reverse engineering",
            "FSM",
            "System modeling"
        ],
        "authors": [
            "Tamal Sen",
            "Rajib Mall"
        ],
        "file_path": "data/sosym-all/s10270-014-0415-3.pdf"
    },
    {
        "title": "Guest Editorial to the Theme Section on Model-Driven Web Engineering",
        "submission-date": "2013/02",
        "publication-date": "2013/02",
        "abstract": "Model-Driven Engineering (MDE) is becoming a widely accepted paradigm for the design and development of complex distributed applications. MDE advocates the use of models and model transformations as key artefacts in all phases of the software process, from system specification and analysis, to design, development and testing. Each model usually addresses one concern, independently from the rest of the issues involved in the construction of the system. Thus, the basic functionality of the system can be separated from its final implementation, and the business logic can be separated from the underlying platform technology, etc. Transformations between models enable the automated implementation of a system from the different models defined for it. The Web Engineering (WE) community soon discovered that the key aspects of MDE (abstraction through modeling; separation of concerns by multi-viewpoint specification, and software development by model transformation) perfectly matched the principles and practices promoted by WE, and allowed Web application designers to address some of their most critical challenges, for example, the need to cope with the constantly growing complexity and with the new requirements on Web applications, in the face of a rapid evolution of the supporting technologies and platforms. Further architectural concerns, such as adaptation or distribution, need to be modeled and taken into account into new Web systems, beyond the traditional content-navigation-presentation aspects addressed by classical WE proposals. There is also an increasing trend towards the incorporation of emerging technologies like the Semantic Web, especially within the scope of the Web 2.0, its related technologies and richer applications. Furthermore, current Web applications need to interoperate with other external systems, which require their integration with third party Web-services, portals, portlets, and also with legacy systems. Finally, many of the existing WE proposals did not fully exploit all the potential benefits of MDE, such as complete platform independence, model transformation and analysis, and metamodeling.",
        "keywords": [],
        "authors": [
            "Geert-Jan Houben",
            "Nora Koch",
            "Gustavo Rossi",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-011-0196-x.pdf"
    },
    {
        "title": "Using UML/MARTE to support performance tuning and stress testing in real-time systems",
        "submission-date": "2015/07",
        "publication-date": "2017/02",
        "abstract": "Abstract Real-time embedded systems (RTESs) operating\nin safety-critical domains have to satisfy strict performance\nrequirements in terms of task deadlines, response time, and\nCPU usage. Two of the main factors affecting the satisfaction\nof these requirements are the conﬁguration parameters reg-\nulating how the system interacts with hardware devices, and\ntheexternaleventstriggeringthesystemtasks.Inparticular,it\nisnecessarytocarefullytunetheparametersinordertoensure\na satisfactory trade-off between responsiveness and usage of\ncomputational resources, and also to stress test the system\nwith worst-case inputs likely to violate the requirements.\nPerformance tuning and stress testing are usually manual,\ntime-consuming, and error-prone processes, because the sys-\ntem parameters and input values range in a large domain, and\ntheir impact over performance is hard to predict without exe-\ncuting the system. In this paper, we provide an approach,\nbased on UML/MARTE, to support the generation of system\nconﬁgurations predicted to achieve a satisfactory trade-off\nbetween response time and CPU usage, and stress test cases\nthat push the system tasks to violate their deadlines. First,\nwe devise a conceptual model that speciﬁes the abstractions\nrequired for analyzing task deadlines, response time, and\nCPU usage, and provide a mapping between these abstrac-\ntions and UML/MARTE. Then, we prune the UML/MARTE\nmetamodel to only contain a purpose-speciﬁc subset of enti-",
        "keywords": [
            "UML/MARTE",
            "Real-time systems",
            "Safety-critical systems",
            "Performance tuning",
            "Stress testing",
            "Constrained optimization"
        ],
        "authors": [
            "Stefano Di Alesio",
            "Sagar Sen"
        ],
        "file_path": "data/sosym-all/s10270-017-0585-x.pdf"
    },
    {
        "title": "Live modeling in the context of state machine models and code generation",
        "submission-date": "2019/09",
        "publication-date": "2020/11",
        "abstract": "Live modeling has been recognized as an important technique to edit behavioral models while being executed and helps in better understanding the impact of a design choice. In the context of model-driven development, models can be executed by interpretation or by the translation of models into existing programming languages, often by code generation. This work is concerned with the support of live modeling in the context of state machine models when they are executed by code generation. To this end, we propose an approach that is completely independent of any live programming support offered by the target language. This independence is achieved with the help of a model transformation which equips the model with support for features which are required for live modeling. A subsequent code generation then produces a self-reﬂective program that allows changes to the model elements at runtime (through synchronization of design and runtime models). We have applied the approach in the context of UML-RT and created a prototype (Live-UMLRT) that provides a full set of services for live modeling of UML-RT state machines such as re-execution, adding/removing states and transitions, and adding/removing action code. We have evaluated the prototype on several use cases. The evaluation shows that (1) generation of a self-reﬂective and model instrumentation can be carried out with reasonable performance, and (2) our approach can apply model changes to the running execution faster than the standard approach that depends on the live programming support of the target language.",
        "keywords": [
            "Model execution",
            "Live modeling",
            "Model-level debugging",
            "MDD",
            "UML-RT"
        ],
        "authors": [
            "Mojtaba Bagherzadeh",
            "Karim Jahed",
            "Benoit Combemale",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-020-00829-y.pdf"
    },
    {
        "title": "Reﬁnement-based Validation of Event-B Speciﬁcations",
        "submission-date": "2014/11",
        "publication-date": "2016/02",
        "abstract": "Abstract The validation of formal speciﬁcations is a challenging task. It is one of the factors that impede the penetration of formal methods into the common practices of software development. This paper discusses the issue of validating formal models by executing them in the context of Event-B. The most important problem lies in the non-determinism which often prevents purely automatic tools to execute models. In this paper, we ﬁrst present and discuss the techniques we have created to allow the execution of models at all levels of abstraction. These techniques rely on users to overcome the barriers resulting from non-deterministic features by either modifying the model or providing ad hoc implementations. Then, we present our main contribution, the formal deﬁnition of the notion of ﬁdelity, that guarantees that all the observable behaviors of the executable models are indeed speciﬁed by the original (non-deterministic) models. The notion of ﬁdelity can be expressed in terms of proof obligations.",
        "keywords": [
            "Formal methods",
            "Reﬁnement",
            "Model validation",
            "Event-B"
        ],
        "authors": [
            "Atif Mashkoor",
            "Faqing Yang",
            "Jean-Pierre Jacquot"
        ],
        "file_path": "data/sosym-all/s10270-016-0514-4.pdf"
    },
    {
        "title": "How fair are we? From conceptualization to automated assessment of fairness definitions",
        "submission-date": "2023/10",
        "publication-date": "2025/01",
        "abstract": "Fairness is a critical concept in ethics and social domains, but it is also a challenging property to engineer in software systems. With the increasing use of machine learning in software systems, researchers have been developing techniques to assess the fairness of software systems automatically. Nonetheless, many of these techniques rely upon pre-established fairness definitions, metrics, and criteria, which may fail to encompass the wide-ranging needs and preferences of users and stakeholders. To overcome this limitation, we propose a novel approach, called MODNESS, that enables users to customize and define their fairness concepts using a dedicated modeling environment. Our approach guides the user through the definition of new fairness concepts also in emerging domains, and the specification and composition of metrics for its evaluation through a dedicated domain-specific language. Ultimately, MODNESS generates the source code to implement fair assessment based on these custom definitions. In addition, we elucidate the process we followed to collect and analyze relevant literature on fairness assessment in software engineering (SE). We compare MODNESS with the selected approaches and evaluate how they support the distinguishing features identified by our study. Our findings reveal that i) most of the current approaches do not support user-defined fairness concepts; ii) our approach can cover additional application domains not addressed by currently available tools, e.g., mitigating bias in recommender systems for software engineering and Arduino software component recommendations; iii) MODNESS demonstrates the capability to overcome the limitations of the only two other model-driven engineering-based approaches for fairness assessment.",
        "keywords": [
            "Fairness assessment",
            "Model-driven engineering",
            "Bias and Fairness definition"
        ],
        "authors": [
            "Giordano d’ Aloisio",
            "Claudio Di Sipio",
            "Antinisca Di Marco",
            "Davide Di Ruscio"
        ],
        "file_path": "data/sosym-all/s10270-025-01277-2.pdf"
    },
    {
        "title": "Model-based engineering in the embedded systems domain: an industrial survey on the state-of-practice",
        "submission-date": "2015/05",
        "publication-date": "2016/03",
        "abstract": "Model-based engineering (MBE) aims at increasing the effectiveness of engineering by using models as important artifacts in the development process. While empirical studies on the use and the effects of MBE in industry exist, only few of them target the embedded systems domain. We contribute to the body of knowledge with an empirical study on the use and the assessment of MBE in that particular domain. The goal of this study is to assess the current state-of-practice and the challenges the embedded systems domain is facing due to shortcomings with MBE. We collected quantitative data from 113 subjects, mostly professionals working with MBE, using an online survey. The collected data spans different aspects of MBE, such as the used modeling languages, tools, notations, effects of MBE introduction, or shortcomings of MBE. Our main findings are that MBE is used by a majority of all participants in the embedded systems domain, mainly for simulation, code generation, and documentation. Reported positive effects of MBE are higher quality and improved reusability. Main shortcomings are interoperability difficulties between MBE tools, high training effort for developers and usability issues. Our study offers valuable insights into the current industrial practice and can guide future research in the fields of systems modeling and embedded systems.",
        "keywords": [
            "Model-based engineering",
            "Model-driven engineering",
            "Embedded systems",
            "Industry",
            "Modeling",
            "Empirical study",
            "State-of-practice"
        ],
        "authors": [
            "Grischa Liebel",
            "Nadja Marko",
            "Matthias Tichy",
            "Andrea Leitner",
            "Jörgen Hansson"
        ],
        "file_path": "data/sosym-all/s10270-016-0523-3.pdf"
    },
    {
        "title": "Case model landscapes: toward an improved representation of knowledge-intensive processes using the fCM-language",
        "submission-date": "2019/12",
        "publication-date": "2021/04",
        "abstract": "Case Management is a paradigm to support knowledge-intensive processes. The different approaches developed for modeling these types of processes tend to result in scattered information due to the low abstraction level at which the inherently complex processes are represented. Thus, readability and understandability are more challenging than in imperative process models. This paper extends a case modeling language—the fragment-based Case Management (fCM) language—to a so-called fCM landscape (fCML) with the goal of modeling a single knowledge-intensive process from a higher abstraction level. Following the Design Science Research (DSR) methodology, we ﬁrst deﬁne requirements for an fCML, and then review how literature—in the ﬁelds of process overviews and case management—could support them. Design decisions are formalized by specifying a syntax for an fCML and the transformation rules from a given fCM model. The proposal is empirically evaluated via a laboratory experiment. Quantitative results imply that interpreting an fCML requires less effort in terms of time—and is thus more efﬁcient—than interpreting its equivalent fCM case model. Qualitative results provide indications on the factors affecting case model interpretation and guidelines for future work.",
        "keywords": [
            "Case management",
            "Knowledge-intensive process",
            "Process landscape",
            "Process map",
            "Process architecture"
        ],
        "authors": [
            "Fernanda Gonzalez-Lopez",
            "Luise Pufahl",
            "Jorge Munoz-Gama",
            "Valeria Herskovic",
            "Marcos Sepúlveda"
        ],
        "file_path": "data/sosym-all/s10270-021-00885-y.pdf"
    },
    {
        "title": "Recommendations for visual feedback about problems within BPMN process models",
        "submission-date": "2020/10",
        "publication-date": "2022/01",
        "abstract": "Business process modeling is a key task in business process management because, besides representing processes, the process models are used, for example, for communication purposes among stakeholders. When not correctly modeled, process models may diminish businesses’ proﬁtability. In this work, we conducted a survey with 57 participants, where we gathered a list of modelers’ needs regarding the feedback they would like to get about problems in process models. For example, modelers would like to get feedback according to their level of experience and be able to activate/deactivate automatic validation. Then, we built a catalog of required features that represents a set of features that process modeling tools should address regarding feedback about problems in process models. Furthermore, we mapped the identiﬁed modelers’ needs to how a group of process modeling tools provides such kind of feedback and to the solutions found in the literature. Finally, based on the gaps found in the mapping, we provide a set of recommendations for visual feedback about problems in process models, which can guide the development of future process modeling tools. Our work focuses on the Business Process Model and Notation because it is an ISO standard, supported by several process modeling and execution tools.",
        "keywords": [
            "Process modeling",
            "Problems in process models",
            "Survey",
            "Recommendations",
            "Visual feedback"
        ],
        "authors": [
            "Vinicius Stein Dani",
            "Carla Maria Dal Sasso Freitas",
            "Lucinéia Heloisa Thom"
        ],
        "file_path": "data/sosym-all/s10270-021-00972-0.pdf"
    },
    {
        "title": "Systematic approach for constructing an understandable state machine from a contract-based speciﬁcation: controlled experiments",
        "submission-date": "2013/03",
        "publication-date": "2014/11",
        "abstract": "Contract-basedspeciﬁcationsusingOCLorJML\nare employed widely to describe the behaviors of systems.\nHowever, complex behaviors might not be understood using\nthese speciﬁcations because they focus on each individual\nmethod instead of the relationships between them. State\nmachines (SMs) can be used to model the dynamic behavior\nincluding acceptable event sequences. However, the man-\nual construction of SMs is a time-consuming and error-\nprone task. Many studies have aimed to construct SMs from\ncontract-based speciﬁcations. However, existing SM con-\nstruction approaches are not concerned with certain qualities\nof the SMs, such as understandability. In this study, we aimed\nto develop a combined atomic condition-based approach for\nconstructing highly understandable SMs from formal speci-\nﬁcations. We conducted two controlled experiments to evalu-\natetheunderstandabilityoftheSMsconstructed:technology-\noriented and human-oriented evaluations. Two existing SM\nconstruction approaches, i.e., condition-partitioning-based\napproach and experience-based approach, were used as the\ncontrols in the two experiments, for comparison with the pro-\nposed approach. In the technology-oriented experiment, 36\nSMs were constructed from 12 speciﬁcations using the three\napproaches. A paired-samples Wilcoxon’s signed-rank test\nwas used to test the differences in the values of a SM under-\nstandability metric based on cohesion and coupling metrics.\nIn the human-oriented experiment, we used 15 of the 36 SMs\nand the differences between the understandability correct-\nness (the number of correct answers/the number of answered\nquestions) measured by 23 participants were tested using an\nindependent t test. The results of the two experiments showed\nthat the understandability of SMs constructed using the pro-\nposed approach was signiﬁcantly better than that of SMs\nconstructed using the two control approaches (p < 0.05).\nThe proposed approach does not support advanced features\nsuch as the containers of contract-based speciﬁcations and\nthe hierarchy/concurrency of SMs.",
        "keywords": [
            "Systematic construction",
            "Reverse engineering",
            "Design by contract",
            "State machine",
            "Understandability",
            "Technology-oriented controlled experiment",
            "Human-oriented controlled experiment"
        ],
        "authors": [
            "Jung Ho Bae",
            "Heung Seok Chae"
        ],
        "file_path": "data/sosym-all/s10270-014-0440-2.pdf"
    },
    {
        "title": "Questionnaire-based variability modeling for system conﬁguration",
        "submission-date": "2007/06",
        "publication-date": "2008/06",
        "abstract": "Variability management is a recurrent issue in systems engineering. It arises for example in enterprise sys- tems, where modules are conﬁgured and composed to meet the requirements of individual customers based on modiﬁ- cations to a reference model. It also manifests itself in the context of software product families, where variants of a system are built from a common code base. This paper pro- poses an approach to capture system variability based on questionnaire models that include order dependencies and domain constraints. The paper presents analysis techniques to detect circular dependencies and contradictory constraints in questionnaire models, as well as techniques to incrementally prevent invalid conﬁgurations by restricting the space of allowed answers to a question based on previous answers. The approach has been implemented as a toolset and has been used in practice to capture conﬁgurable process models for ﬁlm post-production.",
        "keywords": [
            "Variability modeling",
            "System conﬁguration",
            "Questionnaire",
            "Software product family"
        ],
        "authors": [
            "Marcello La Rosa",
            "Wil M. P. van der Aalst",
            "Marlon Dumas",
            "Arthur H. M. ter Hofstede"
        ],
        "file_path": "data/sosym-all/s10270-008-0090-3.pdf"
    },
    {
        "title": "Domain analysis of dynamic system reconﬁguration",
        "submission-date": "2006/01",
        "publication-date": "2007/01",
        "abstract": "A domain analysis of dynamic system reconﬁguration is presented in this paper. The intent is to provide a comprehensive conceptual framework within which to systematically and consistently address problems and solutions related to dynamically reconﬁgurable systems. The analysis identiﬁes and categorizes the various types of change that may be required, the relationship between those types, and the system integrity characteristics that need to be considered when such changes take place. A system model is employed to describe each change type using examples of global and local properties in the context of a ﬁnancial analysis system. A rigorous formal methodology, based on the Alloy language and tools, is employed to specify precisely and formally the detailed relationships between various parts of the model. Based upon these descriptions, the types of change of dynamic system reconﬁguration are presented as a series of UML class models.",
        "keywords": [
            "Alloy",
            "Component",
            "based systems",
            "Dynamic reconﬁguration",
            "Feature modeling",
            "Model-",
            "driven development",
            "Software evolution",
            "System integrity",
            "UML"
        ],
        "authors": [
            "James D’Arcy Walsh",
            "Francis Bordeleau",
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-006-0038-4.pdf"
    },
    {
        "title": "Guest editorial for the theme section on modeling language engineering",
        "submission-date": "2023/02",
        "publication-date": "2023/03",
        "abstract": "Software-intensive systems are becoming more complex, driven by the need to integrate across multiple aspects. Consequently, the development of such systems requires the integration of different concerns and skills. These concerns are usually covered by different domain-speciﬁc modeling languages, with speciﬁc concepts, technologies, and abstraction levels. This multiplication of languages eases the development related to one speciﬁc concern but raises language and technology integration problems at the different stages of the software life cycle. To support effective language integration, there is a pressing need to reify and classify these relationships, as well as the language interactions that the relationships enable. Similarly, the proliferation of domain-speciﬁc modeling languages increases the need for effective and efﬁcient techniques for engineering languages and their support infrastructures. Hence, software developers are faced both with the challenging task of engineering each separate modeling language and associated technologies and with the task of integrating the different languages from different concern spaces.",
        "keywords": [],
        "authors": [
            "Benoît Combemale",
            "Romina Eramo",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-023-01097-2.pdf"
    },
    {
        "title": "Accidental complexity in multilevel modeling revisited",
        "submission-date": "2020/05",
        "publication-date": "2022/01",
        "abstract": "Multilevel modeling (MLM) conceptualizes software models as layered architectures of sub-models that are inter-related by the instance-of relation. Conceptually, MLM provides beneﬁts in terms of ontological classiﬁcation. Pragmatically, based on arguments in knowledge engineering, MLM meaningfully reduces accidental complexity. In this paper, the problem of accidental complexity in MLM is revisited. The paper focuses on the role of the context of type-instance structures on MLM architectures. We analyze factors of accidental complexity in multilevel models, suggest quantitative metrics for these factors, and show how they can be used for guiding MLM rearchitecture transformations. The relevance of the proposed factors and metrics is shown in an experimental study of type-instance contexts in multiple real-world models.",
        "keywords": [
            "Multilevel modeling",
            "Context",
            "Rearchitecture",
            "Accidental complexity",
            "Quantitative measures",
            "Evaluation criteria"
        ],
        "authors": [
            "Mira Balaban",
            "Igal Khitron",
            "Azzam Maraee"
        ],
        "file_path": "data/sosym-all/s10270-021-00938-2.pdf"
    },
    {
        "title": "Formal methods in the scope of the Software and Systems Modeling journal",
        "submission-date": "2025/04",
        "publication-date": "2025/04",
        "abstract": "Software and Systems Modeling (SoSyM) is a journal dedicated to advancing the field of software and systems modeling by publishing high-quality research that contributes to the theory and practice of modeling in software and systems engineering, which also includes processes executed automatically or involving humans. The journal aims to bridge the gap between academia and industry by fostering discussions on modeling languages, methodologies, tools, and their applications to real-world challenges. SoSyM encourages submissions that present innovative modeling approaches, their precise semantic foundations, empirical evaluations, and applications that have tangible impacts on software and system development processes. Given this mission, the journal welcomes research on formal methods, provided that such work is framed within the context of software and systems modeling.",
        "keywords": [],
        "authors": [
            "Marsha Chechik",
            "Benoit Combemale",
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-025-01287-0.pdf"
    },
    {
        "title": "The Impact of Model Simulation on Semantic Conceptual Model Quality for Novice Modelers",
        "submission-date": "2025/03",
        "publication-date": "Not found",
        "abstract": "Purpose: Conceptual modeling is crucial for information systems development, yet novice modelers often struggle with ensuring a high semantic quality, leading to ﬂawed system designs. This study investigates the impact of Feedback-enabled Interactive Rapid Prototyping (FIRP) simulation on novice modelers’ ability to identify and correct semantic errors in conceptual models.\nMethods: We analyzed data from 39 master-level students who used the MERODE prototyper in a FIRP simulation environment. We examined the relationship between testing effort, measured by event invocations and model coverage, and the likelihood of students identifying seeded semantic mistakes and providing correct solutions, using logistic regression.\nResults: Logistic regression analyses revealed a statistically signiﬁcant positive correlation between testing effort and the detection of speciﬁc behavioral constraint violations that resulted in deadlocks. However, the relationship was not consistent across all mistake types.\nConclusion: FIRP simulation is particularly effective in helping novice modelers identify behavioral constraint violations that disrupt system workﬂows, i.e., those leading to deadlocks. Active model exploration through FIRP enhances error detection, highlighting its potential as a valuable tool for conceptual modeling education. However, the effectiveness varies with the type of semantic error, suggesting the need for tailored simulation strategies.",
        "keywords": [
            "Conceptual Modeling",
            "Model Simulation",
            "Semantic Quality",
            "FIRP",
            "Deadlock Situations"
        ],
        "authors": [
            "Felix Cammaerts",
            "Beatriz Marín",
            "Monique Snoeck"
        ],
        "file_path": "data/sosym-all/s10270-025-01317-x.pdf"
    },
    {
        "title": "Business process modeling language selection for research modelers",
        "submission-date": "2022/09",
        "publication-date": "2023/05",
        "abstract": "Business process modeling is a crucial aspect of domains such as Business Process Management and Software Engineering. The availability of various BPM languages in the market makes it challenging for process modelers to select the best-fit BPM language for a specific process modeling task. A decision model is necessary to systematically capture and make scattered knowledge on BPM languages available for reuse by process modelers and academics. This paper presents a decision model for the BPM language selection problem in research projects. The model contains mappings of 72 BPM features to 23 BPM languages. We validated and refined the decision model through 10 expert interviews with domain experts from various organizations. We evaluated the efficiency, validity, and generality of the decision model by conducting four case studies of academic research projects with their original researchers. The results confirmed that the decision model supports process modelers in the selection process by providing more insights into the decision process. Based on the empirical evidence from the case studies and domain expert feedback, we conclude that having the knowledge readily available in the decision model supports academics in making more informed decisions that align with their preferences and prioritized requirements. Furthermore, the captured knowledge provides a comprehensive overview of BPM languages, features, and quality characteristics that other researchers can employ to tackle future research challenges. Our observations indicate that BPMN is a commonly used modeling language for process modeling. Therefore, it is more sensible for academics to explain why they did not select BPMN than to discuss why they chose it for their research project(s).",
        "keywords": [
            "Business process modeling language selection",
            "Decision model",
            "Multi-criteria decision-making",
            "Decision support system",
            "Case study research"
        ],
        "authors": [
            "Siamak Farshidi",
            "Izaak Beer Kwantes",
            "Slinger Jansen"
        ],
        "file_path": "data/sosym-all/s10270-023-01110-8.pdf"
    },
    {
        "title": "SYMBOLEOPC: checking properties of legal contracts",
        "submission-date": "2023/05",
        "publication-date": "2024/07",
        "abstract": "Legal contracts specify requirements for business transactions. Symboleo was recently proposed as a formal speciﬁcation language for legal contracts. It allows the speciﬁcation of the contractual requirements by specifying the obligations and powers of the parties, as well as specifying the events that can occur in a contract’s lifecycle. With appropriate tool support, Symboleo can allow monitoring the contract lifecycle. However, because of mistakes in contract interpretation or formal speciﬁcation, speciﬁed contracts may violate properties expected by contracting parties. This paper presents SymboleoPC, a tool for analyzing Symboleo contracts using the nuXmv model checker, where properties can be expressed in both Linear Temporal Logic and Computation Tree Logic. The presentation highlights the architecture, implementation, and testing of the tool, as well as a scalability evaluation, based on performance data. The performance of the tool was evaluated with respect to varying numbers of obligations and powers, with varying numbers of inter-dependencies among them, with parameters derived from the analysis of real contracts. These results suggest that SymboleoPC can be usefully applied to the analysis of formal speciﬁcations of contracts with real-life sizes and structures.",
        "keywords": [
            "Legal contracts",
            "Smart contracts",
            "Software requirements speciﬁcations",
            "Formal speciﬁcation languages",
            "Model checking",
            "Performance analysis",
            "SymboleoPC",
            "nuXmv"
        ],
        "authors": [
            "Alireza Parvizimosaed",
            "Marco Roveri",
            "Aidin Rasti",
            "Amal Ahmed Anda",
            "Sofana Alfuhaid",
            "Daniel Amyot",
            "Luigi Logrippo",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-024-01180-2.pdf"
    },
    {
        "title": "Correlating contexts and NFR conﬂicts from event logs",
        "submission-date": "2022/03",
        "publication-date": "2023/02",
        "abstract": "In the design of autonomous systems, it is important to consider the preferences of the interested parties to improve the user experience. These preferences are often associated with the contexts in which each system is likely to operate. The operational behavior of a system must also meet various non-functional requirements (NFRs), which can present different levels of conﬂict depending on the operational context. This work aims to model correlations between the individual contexts and the consequent conﬂicts between NFRs. The proposed approach is based on analyzing the system event logs, tracing them back to the leaf elements at the speciﬁcation level and providing a contextual explanation of the system’s behavior. The traced contexts and NFR conﬂicts are then mined to produce Context-Context and Context-NFR conﬂict sequential rules. The proposed Contextual Explainability (ConE) framework uses BERT-based pre-trained language models and sequential rule mining libraries for deriving the above correlations. Extensive evaluations are performed to compare the existing state-of-the-art approaches. The best-ﬁt solutions are chosen to integrate within the ConE framework. Based on experiments, an accuracy of 80%, a precision of 90%, a recall of 97%, and an F1-score of 88% are recorded for the ConE framework on the sequential rules that were mined.",
        "keywords": [
            "Sequential rule mining",
            "Context correlation",
            "Context-NFR conﬂict correlation",
            "Goal models"
        ],
        "authors": [
            "Mandira Roy\nSouvick Das\nNovarun Deb\nAgostino Cortesi\nRituparna Chaki\nNabendu Chaki"
        ],
        "file_path": "data/sosym-all/s10270-023-01087-4.pdf"
    },
    {
        "title": "Effective product-line testing using similarity-based product prioritization",
        "submission-date": "2015/07",
        "publication-date": "2016/12",
        "abstract": "A software product line comprises a family of software products that share a common set of features. Testing an entire product-line product-by-product is infeasible due to the potentially exponential number of products in the number of features. Accordingly, several sampling approaches have been proposed to select a presumably minimal, yet sufﬁcient number of products to be tested. Since the time budget for testing is limited or even a priori unknown, the order in which products are tested is crucial for effective product-line testing. Prioritizing products is required to increase the probability of detecting faults faster. In this article, we propose similarity-based prioritization, which can be efﬁciently applied on product samples. In our approach, we incrementally select the most diverse product in terms of features to be tested next in order to increase feature interaction coverage as fast as possible during product-by-product testing. We evaluate the gain in the effectiveness of similarity-basedprioritizationonthreeproductlineswithreal faults. Furthermore, we compare similarity-based prioritization to random orders, an interaction-based approach, and the default orders produced by existing sampling algorithms considering feature models of various sizes. The results show that our approach potentially increases effectiveness in terms of fault detection ratio concerning faults within real-world product-line implementations as well as synthetically seeded faults. Moreover, we show that the default orders of recent sampling algorithms already show promising results, which, however, can still be improved in many cases using similarity-based prioritization.",
        "keywords": [
            "Software product lines",
            "Product-line testing",
            "Model-based testing",
            "Combinatorial interaction testing",
            "Test-case prioritization"
        ],
        "authors": [
            "Mustafa Al-Hajjaji",
            "Thomas Thüm",
            "Malte Lochau",
            "Jens Meinicke",
            "Gunter Saake"
        ],
        "file_path": "data/sosym-all/s10270-016-0569-2.pdf"
    },
    {
        "title": "Realizing strategic ﬁt within the business architecture: the design of a Process-Goal Alignment modeling and analysis technique",
        "submission-date": "2015/04",
        "publication-date": "2017/01",
        "abstract": "The realization of strategic ﬁt within the business architecture is an important challenge for organizations. Research in the ﬁeld of enterprise modeling has resulted in the development of a wide range of modeling techniques that provide visual representations to improve the understanding and communication about the business architecture. As these techniques only provide partial solutions for the issue of realizing strategic ﬁt, the Process-Goal Alignment technique is presented in this paper. This technique combines the visual expressiveness of heat mapping techniques with the analytical capabilities of performance measurement and Strategic Management frameworks to provide a comprehensible and well-informed modeling language for the realization of strategic ﬁt within an organization’s business architecture. The paper reports on the design of the proposed technique by means of Action Design Research, which included iterative cycles of building, intervention, and evaluation through case studies. To support the application of the technique, a software tool was developed using the ADOxx meta-modeling platform.",
        "keywords": [
            "Strategic ﬁt",
            "Business architecture",
            "Enterprise modeling",
            "Process-Goal Alignment",
            "Heat map"
        ],
        "authors": [
            "Ben Roelens",
            "Wout Steenacker",
            "Geert Poels"
        ],
        "file_path": "data/sosym-all/s10270-016-0574-5.pdf"
    },
    {
        "title": "Constraint-driven modeling through transformation",
        "submission-date": "2012/10",
        "publication-date": "2013/06",
        "abstract": "In model-driven software engineering, model transformation plays a key role for automatically generating and updating models. Transformation rules define how source model elements are to be transformed into target model elements. However, defining transformation rules is a complex task, especially in situations where semantic differences or incompleteness allow for alternative interpretations or where models change continuously before and after transformation. This paper proposes constraint-driven modeling where transformation is used to generate constraints on the target model rather than the target model itself. We evaluated the approach on three case studies that address the above difficulties and other common transformation issues. We also developed a proof-of-concept implementation that demonstrates its feasibility. The implementation suggests that constraint-driven transformation is an efficient and scalable alternative and/or complement to traditional transformation.",
        "keywords": [
            "Model-driven engineering (MDE)",
            "Model transformation",
            "Ambiguity",
            "Consistency checking",
            "Incremental constraint management"
        ],
        "authors": [
            "Andreas Demuth",
            "Roberto Erick Lopez-Herrejon",
            "Alexander Egyed"
        ],
        "file_path": "data/sosym-all/s10270-013-0363-3.pdf"
    },
    {
        "title": "Exploring inconsistencies between modal transition systems",
        "submission-date": "2009/06",
        "publication-date": "2010/02",
        "abstract": "It is commonplace to have multiple behaviour\nmodels that describe the same system but have been pro-\nduced by different stakeholders or synthesized from different\nsources. Although in practice, such models frequently exhibit\ninconsistencies, there is a lack of tool support for analyz-\ning them. There are two key difﬁculties in explaining why\ntwo behavioural models are inconsistent: (1) explanations\noften require branching structures rather than linear traces,\nor scenarios; and (2) there can be multiple sources of incon-\nsistency and many different ways of explaining each one.\nIn this paper, we present an approach that supports explo-\nration of inconsistencies between modal transition systems,\nan extension to labelled transition systems. We show how\nto produce sound graphical explanations for inconsistencies,\nhow to compactly represent all possible explanations in a\ncomposition of the models being compared, and how mod-\nelers can use this composition to explore the explanations\nencoded therein.",
        "keywords": [
            "Labelled transition systems",
            "Inconsistency identiﬁcation and resolution",
            "μ-Calculus",
            "Distinguishing property",
            "Graphical feedback"
        ],
        "authors": [
            "Mathieu Sassolas",
            "Marsha Chechik",
            "Sebastian Uchitel"
        ],
        "file_path": "data/sosym-all/s10270-010-0148-x.pdf"
    },
    {
        "title": "On the use of models for high-performance scientiﬁc computing applications: an experience report",
        "submission-date": "2015/05",
        "publication-date": "2016/03",
        "abstract": "This paper reports on a four-year project that aims to raise the abstraction level through the use of model-driven engineering (MDE) techniques in the development of scientiﬁc applications relying on high-performance computing. The development and maintenance of high-performance scientiﬁc computing software is reputedly a complex task. This complexity results from the frequent evolutions of supercomputers and the tight coupling between software and hardware aspects. Moreover, current parallel programming approaches result in a mixing of concerns within the source code. Our approach relies on the use of MDE and consists in deﬁning domain-speciﬁc modeling languages targeting various domain experts involved in the development of HPC applications, allowing each of them to handle their dedicated modelinabothuser-friendlyandhardware-independentway. The different concerns are separated thanks to the use of several models as well as several modeling viewpoints on these models. Depending on the targeted execution platforms, these abstract models are translated into executable implementations by means of model transformations. To make all of these effective, we have developed a tool chain that is also presented in this paper. The approach is assessed through a multi-dimensional validation that focuses on its applicability, its expressiveness and its efﬁciency. To capitalize on the gained experience, we analyze some lessons learned during this project.",
        "keywords": [
            "HPC",
            "High-performance calculus",
            "MDE",
            "Model-driven engineering",
            "Architecture",
            "Fortran"
        ],
        "authors": [
            "Ileana Ober",
            "Marc Palyart",
            "Jean-Michel Bruel",
            "David Lugato"
        ],
        "file_path": "data/sosym-all/s10270-016-0518-0.pdf"
    },
    {
        "title": "A systematic approach to generate B preconditions: application to the database domain",
        "submission-date": "2006/08",
        "publication-date": "2008/07",
        "abstract": "Maintaining integrity constraints in information systems is a real issue. In our previous work, we have defined a formal approach that derives B formal specifications from a UML description of the system. Basically, the generated B specification is composed of a set of variables modeling data and a set of operations representing transactions. The integrity constraints are directly specified as B invariant properties. So far, the operations we generate establish only a reduced class of constraints. In this paper, we describe a systematic approach to identify preconditions that take a larger class of invariants into account. The key idea is the definition of rewriting and simplification rules that we apply to the B invariants.",
        "keywords": [
            "Integrity constraints",
            "Formal specification",
            "B operations",
            "Invariant",
            "Precondition"
        ],
        "authors": [
            "Amel Mammar"
        ],
        "file_path": "data/sosym-all/s10270-008-0098-8.pdf"
    },
    {
        "title": "Automated product line test case selection: industrial case study and controlled experiment",
        "submission-date": "2014/08",
        "publication-date": "2015/04",
        "abstract": "Automated test case selection for a new prod-uct in a product line is challenging due to several reasons. First, the variability within the product line needs to be cap-tured in a systematic way; second, the reusable test cases from the repository are required to be identiﬁed for testing a new product. The objective of such automated process is to reduce the overall effort for selection (e.g., selection time), while achieving an acceptable level of the coverage of test-ing functionalities. In this paper, we propose a systematic and automated methodology using a feature model for testing (FM_T) to capture commonalities and variabilities of a prod-uct line and a component family model for testing (CFM_T) to capture the overall structure of test cases in the reposi-tory. With our methodology, a test engineer does not need to manually go through the repository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM_T at a higher level of abstraction for a product and a set of rele-vant test cases will be selected automatically. We evaluated our methodology via three different ways: (1) We applied our methodology to a product line of video conferencing systems called Saturn developed by Cisco, and the results show that our methodology can reduce the selection effort signiﬁcantly; (2) we conducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM_T and CFM_T. The results show that test engineers are positive about adapting our methodology and models (FM_T and CFM_T) in their current practice; (3) we conducted a controlled experiment with 20 graduate students to assess the performance (i.e., cost, effectiveness and efﬁ-ciency) of our automated methodology as compared to the manual approach. The results showed that our methodology is cost-effective as compared to the manual approach, and at the same time, its efﬁciency is not affected by the increased complexity of products.",
        "keywords": [
            "Test case selection",
            "Product line",
            "Feature model",
            "Component family model"
        ],
        "authors": [
            "Shuai Wang",
            "Shaukat Ali",
            "Arnaud Gotlieb",
            "Marius Liaaen"
        ],
        "file_path": "data/sosym-all/s10270-015-0462-4.pdf"
    },
    {
        "title": "Correction: Cost-sensitive precomputation of real-time-aware reconﬁguration strategies based on stochastic priced timed games",
        "submission-date": "2024/08",
        "publication-date": "2025/02",
        "abstract": "The article “Cost-sensitive precomputation of real-time-aware reconﬁguration strategies based on stochastic priced timed games”, written by Hendrik Göttmann, Birte Caesar, Lasse Beers, Malte Lochau, Andy Schürr and Alexander Fay, was originally published electronically on the publisher’s internet portal on 5 August 2024 without open access.",
        "keywords": [],
        "authors": [
            "Hendrik Göttmann",
            "Birte Caesar",
            "Lasse Beers",
            "Malte Lochau",
            "Andy Schürr",
            "Alexander Fay"
        ],
        "file_path": "data/sosym-all/s10270-025-01269-2.pdf"
    },
    {
        "title": "Verifying B proof rules using deep embedding and automated theorem proving",
        "submission-date": "2012/03",
        "publication-date": "2013/06",
        "abstract": "We propose a formal and mechanized framework which consists in verifying proof rules of the B method, which cannot be automatically proved by the elementary prover of Atelier B and using an external automated theorem prover called Zenon. This framework contains in particular a set of tools, named BCARe and developed by Siemens IC-MOL, which relies on a deep embedding of the B theory withinthelogicofthe Coqproofassistant.Thistoolkitallows us to automatically generate the required properties to be checked for a given proof rule. Currently, this tool chain is able to automatically verify a part of the derived rules of the B-Book, as well as some added rules coming from Atelier B and the rule database maintained by Siemens IC-MOL.",
        "keywords": [
            "B Method",
            "Proof rules",
            "Veriﬁcation",
            "Deep embedding",
            "Automated theorem proving",
            "Coq",
            "Zenon"
        ],
        "authors": [
            "Mélanie Jacquel",
            "Karim Berkani",
            "David Delahaye",
            "Catherine Dubois"
        ],
        "file_path": "data/sosym-all/s10270-013-0322-z.pdf"
    },
    {
        "title": "A component framework for system modeling based on high-level replacement systems",
        "submission-date": "2003/01",
        "publication-date": "2004/04",
        "abstract": "The aim of this paper is to present a generic component framework for system modeling that satis-ﬁes main requirements for component-based development in software engineering. In this sense, we have deﬁned a framework that can be used, by providing an adequate instantiation, in connection with a large class of semi-formal and formal modeling techniques. Moreover, the framework is also ﬂexible with respect to the connection of components, providing a compositional semantics of components. This means more precisely that the semantics of a system can be inferred from the semantics of its components. In contrast to other component concepts for data type speciﬁcation techniques, our component framework is based on a generic notion of transformations. In particular, reﬁnements and transformations are used to express intradependencies, between the export interface and the body of a component, and interdependencies, between the import and the export interfaces of diﬀerent components. The generic component framework gener-alizes module concepts for diﬀerent kinds of Petri nets and graph transformation systems proposed in the liter-ature, and seems to be also suitable for visual modeling techniques, including parts of the UML, if these tech-niques provide a suitable reﬁnement or transformation concept. In this paper the generic approach is instanti-ated in two steps. First to high-level replacement systems generalizing the transformation concept of graph trans-formations. In a second step it is further instantiated to low-level and high-level Petri nets. To show applica-bility we present sample components from a case study in the domain of production automation as proposed in a priority program of the German Research Coun-cil (DFG).",
        "keywords": [
            "Components",
            "Formal semantics",
            "Generic framework",
            "Petri nets"
        ],
        "authors": [
            "Hartmut Ehrig",
            "Fernando Orejas",
            "Benjamin Braatz",
            "Markus Klein",
            "Martti Piirainen"
        ],
        "file_path": "data/sosym-all/s10270-003-0043-9.pdf"
    },
    {
        "title": "Models for formal methods and tools: the case of railway systems",
        "submission-date": "2024/05",
        "publication-date": "Not found",
        "abstract": "Formal methods and tools are successfully applied to the development of safety-critical systems for decades now, in particular in the transport domain, without a single technique or tool emerging as the dominant solution for system design. Formal methods are highly recommended by the existing safety standards in the railway industry, but railway engineers typically lack the knowledge to transform their semi-formal models into a formal model, with a precise semantics, that can serve as input to formal methods tools. We share the results of performing empirical studies in the ﬁeld, including usability analyses of formal methods tools involving railway practitioners. We discuss, in particular with respect to railway systems and their modelling, our experiences in applying formal methods and tools to a variety of case studies, for which we interacted with a number of companies from the railway domain. We report on lessons learned from these experiences and provide pointers to steer future research towards facilitating further synergies between researchers and developers of formal methods and tools on the one hand and practitioners from the railway industry on the other.",
        "keywords": [
            "Formal methods",
            "Models",
            "Railway systems"
        ],
        "authors": [
            "M. H. ter Beek"
        ],
        "file_path": "data/sosym-all/s10270-025-01276-3.pdf"
    },
    {
        "title": "Model-driven generative development of measurement software",
        "submission-date": "2009/10",
        "publication-date": "2010/06",
        "abstract": "Metrics offer a practical approach to evaluate properties of domain-speciﬁc models. However, it is costly to develop and maintain measurement software for each domain-speciﬁc modeling language. In this paper, we present a model-driven and generative approach to measuring models. The approach is completely domain-independent and operationalized through a prototype that synthesizes a measurement infrastructure for a domain-speciﬁc modeling language. This model-driven measurement approach is model-driven from two viewpoints: (1) it measures models of a domain-speciﬁc modeling language; (2) it uses models as unique and consistent metric speciﬁcations, with respect to a metric speciﬁcation metamodel which captures all the necessary concepts for model-driven speciﬁcations of metrics. The beneﬁt from applying the approach is evaluated by four case studies. They indicate that this approach significantly eases the measurement activities of model-driven development processes.",
        "keywords": [],
        "authors": [
            "Martin Monperrus",
            "Jean-Marc Jézéquel",
            "Benoit Baudry",
            "Joël Champeau",
            "Brigitte Hoeltzener"
        ],
        "file_path": "data/sosym-all/s10270-010-0165-9.pdf"
    },
    {
        "title": "Early timing analysis based on scenario requirements and platform models",
        "submission-date": "2020/11",
        "publication-date": "2022/04",
        "abstract": "Distributed, software-intensive systems (e.g., in the automotive sector) must fulﬁll communication requirements under hard real-time constraints. The requirements have to be documented and validated carefully using a systematic requirements engineering (RE) approach, for example, by applying scenario-based requirements notations. The resources of the execution platforms and their properties (e.g., CPU frequency or bus throughput) induce effects on the timing behavior, which may lead to violations of the real-time requirements. Nowadays, the platform properties and their induced timing effects are veriﬁed against the real-time requirements by means of timing analysis techniques mostly implemented in commercial-off-the-shelf tools. However, such timing analyses are conducted in late development phases since they rely on artifacts produced during these phases (e.g., the platform-speciﬁc code). In order to enable early timing analyses already during RE, we extend a scenario-based requirements notation with allocation means to platform models and deﬁne operational semantics for the purpose of simulation-based, platform-aware timing analyses. We illustrate and evaluate the approach with an automotive software-intensive system.",
        "keywords": [
            "Scenario-based requirements",
            "Platform modeling",
            "Real-time systems",
            "Timing analysis"
        ],
        "authors": [
            "Jörg Holtmann",
            "Julien Deantoni",
            "Markus Fockel"
        ],
        "file_path": "data/sosym-all/s10270-022-01002-3.pdf"
    },
    {
        "title": "A semi-automated BPMN-based framework for detecting conflicts between security, data-minimization, and fairness requirements",
        "submission-date": "2019/02",
        "publication-date": "2020/02",
        "abstract": "Requirements are inherently prone to conflicts. Security, data-minimization, and fairness requirements are no exception. Importantly, undetected conflicts between such requirements can lead to severe effects, including privacy infringement and legal sanctions. Detecting conflicts between security, data-minimization, and fairness requirements is a challenging task, as such conflicts are context-specific and their detection requires a thorough understanding of the underlying business processes. For example, a process may require anonymous execution of a task that writes data into a secure data storage, where the identity of the writer is needed for the purpose of accountability. Moreover, conflicts not arise from trade-offs between requirements elicited from the stakeholders, but also from misinterpretation of elicited requirements while implementing them in business processes, leading to a non-alignment between the data subjects’ requirements and their specifications. Both types of conflicts are substantial challenges for conflict detection. To address these challenges, we propose a BPMN-based framework that supports: (i) the design of business processes considering security, data-minimization and fairness requirements, (ii) the encoding of such requirements as reusable, domain-specific patterns, (iii) the checking of alignment between the encoded requirements and annotated BPMN models based on these patterns, and (iv) the detection of conflicts between the specified requirements in the BPMN models based on a catalog of domain-independent anti-patterns. The security requirements were reused from SecBPMN2, a security-oriented BPMN 2.0 extension, while the fairness and data-minimization parts are new. For formulating our patterns and anti-patterns, we extended a graphical query language called SecBPMN2-Q. We report on the feasibility and the usability of our approach based on a case study featuring a healthcare management system, and an experimental user study.",
        "keywords": [
            "Conflicts",
            "Requirements engineering",
            "Security",
            "Data minimization",
            "Fairness",
            "BPMN"
        ],
        "authors": [
            "Qusai Ramadan",
            "Daniel Strüber",
            "Mattia Salnitri",
            "Jan Jürjens",
            "Volker Riediger",
            "Steffen Staab"
        ],
        "file_path": "data/sosym-all/s10270-020-00781-x.pdf"
    },
    {
        "title": "Welcome to the second issue, and the ﬁrst special issue of the Software and System Modeling (SoSyM) journal",
        "submission-date": "2001",
        "publication-date": "2002/12",
        "abstract": "This special issue contains extended and improved versions of the best papers presented at the fourth International Conference of the Uniﬁed Modeling Language, << UML >> 2001. The extended forms of the papers published in this issue showcase some of the innovative and high quality work that researchers in the UML community are undertaking. This issue also contains an invited paper by Grady Booch about the future of software engineering and modeling techniques.",
        "keywords": [],
        "authors": [
            "Martin Gogolla",
            "Grady Booch"
        ],
        "file_path": "data/sosym-all/s10270-002-0014-6.pdf"
    },
    {
        "title": "Incremental model transformations with triple graph grammars for multi-version models and multi-version pattern matching",
        "submission-date": "2024/04",
        "publication-date": "Not found",
        "abstract": "Like conventional software projects, projects in model-driven software engineering require adequate management of multiple versions of development artifacts, importantly allowing living with temporary inconsistencies. In previous work, we have introduced multi-version models for model-driven software engineering, which allow checking well-formedness and finding merge conflicts for multiple versions of the same model at once. However, situations where different models are linked via automatic model transformations also have to be handled for multi-version models. In this paper, we propose a technique for jointly handling the transformation of multiple versions of a source model into corresponding versions of a target model. This enables the use of a more compact representation that may afford improved execution time of both the transformation and further analysis. Our approach is based on the well-known formalism of triple graph grammars and the aforementioned encoding of model version histories called multi-version models. In addition to batch transformation of an entire history, the technique covers incremental synchronization of changes in the framework of multi-version models. Our solution is complemented by a dedicated pattern matching technique for multi-version models. We show the correctness of our approach with respect to the standard semantics of triple graph grammars and conduct an empirical evaluation to investigate the performance of our technique regarding execution time and memory consumption. Our results indicate that the proposed solution affords lower memory consumption and may improve execution time for batch transformation of large version histories, but can also come with computational overhead in unfavorable cases.",
        "keywords": [
            "Multi-version models",
            "Triple graph grammars",
            "Incremental model transformation",
            "Graph pattern matching"
        ],
        "authors": [
            "Matthias Barkowsky",
            "Holger Giese"
        ],
        "file_path": "data/sosym-all/s10270-024-01238-1.pdf"
    },
    {
        "title": "Multi-view approaches for software and system modelling: a systematic literature review",
        "submission-date": "2018/02",
        "publication-date": "2019/02",
        "abstract": "Over the years, a number of approaches have been proposed on the description of systems and software in terms of mul- tiple views represented by models. This modelling branch, so-called multi-view software and system modelling, praises a differentiated and complex scientiﬁc body of knowledge. With this study, we aimed at identifying, classifying, and evaluating existing solutions for multi-view modelling of software and systems. To this end, we conducted a systematic literature review of the existing state of the art related to the topic. More speciﬁcally, we selected and analysed 40 research studies among over 8600 entries. We deﬁned a taxonomy for characterising solutions for multi-view modelling and applied it to the selected studies. Lastly, we analysed and discussed the data extracted from the studies. From the analysed data, we made several observations, among which: (i) there is no uniformity nor agreement in the terminology when it comes to multi-view artefact types, (ii) multi-view approaches have not been evaluated in industrial settings and (iii) there is a lack of support for semantic consistency management and the community does not appear to consider this as a priority. The study results provide an exhaustive overview of the state of the art for multi-view software and systems modelling useful for both researchers and practitioners.",
        "keywords": [
            "Model-driven engineering",
            "Multi-view modelling",
            "Viewpoints",
            "Views",
            "Consistency"
        ],
        "authors": [
            "Antonio Cicchetti",
            "Federico Ciccozzi",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-018-00713-w.pdf"
    },
    {
        "title": "Platform independent Web application modeling and development with Netsilon",
        "submission-date": "2004/05",
        "publication-date": "2005/06",
        "abstract": "This paper discusses platform independent Web application modeling and development in the context of model-driven engineering. A specific metamodel (and associated notation) is introduced and motivated for the modeling of dynamic Web specific concerns. Web applications are represented via three independent but related models (business, hypertext and presentation). A kind of action language (based on OCL and Java) is used all over these models to write methods and actions, specify constraints and express conditions. The concepts described in the paper have been implemented in the Netsilon tool and operational model-driven Web information systems have been successfully deployed by translation from abstract models to platform specific models.",
        "keywords": [
            "MDA",
            "PIMs",
            "PSMs",
            "Web application development"
        ],
        "authors": [
            "Pierre-Alain Muller",
            "Philippe Studer",
            "Fr´ed´eric Fondement",
            "Jean Bezivin"
        ],
        "file_path": "data/sosym-all/s10270-005-0091-4.pdf"
    },
    {
        "title": "Validating UML and OCL models in USE by automatic snapshot generation",
        "submission-date": "2004/05",
        "publication-date": "2005/06",
        "abstract": "We study the testing and certiﬁcation of UML and OCL models as supported by the validation tool USE. We extend the available USE features by introducing a language for deﬁning properties of desired snapshots and by showing how such snapshots are generated. Within the approach, it is possible to treat test cases and validation cases. Test cases show that snapshots having desired properties can be constructed. Validation cases show that given properties are consequences of the original UML and OCL model.",
        "keywords": [
            "UML",
            "OCL",
            "Model validation",
            "Model testing",
            "Reasoning about models",
            "Class diagram",
            "Invariant",
            "Pre- and postcondition",
            "Test case",
            "Snapshot"
        ],
        "authors": [
            "Martin Gogolla",
            "Jörn Bohling",
            "Mark Richters"
        ],
        "file_path": "data/sosym-all/s10270-005-0089-y.pdf"
    },
    {
        "title": "Guest editorial to the special section on model transformation",
        "submission-date": "2008/10",
        "publication-date": "2009/11",
        "abstract": "Models have become essential for dealing with the numerous aspects involved in developing and maintaining complex IT systems. Models assist in capturing the relevant aspects of a system from a given perspective and at a precise level of abstraction. Model transformation represents a key activity in model-driven engineering by supporting the definition and implementation of the operations on models, which can provide a chain that enables the automated development of a system from its corresponding models. Furthermore, a model transformation may also be considered a model in its own right, which presents opportunities for higher-order transformations, i.e., transformations that manipulate models representing other model transformations. The objective of this special section is to provide a representative sample of advanced research emerging from the field of model transformation. The selected papers provide an overview of current open issues and identify potential lines for further research.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Alfonso Pierantonio",
            "Antonio Vallecillo"
        ],
        "file_path": "data/sosym-all/s10270-009-0139-y.pdf"
    },
    {
        "title": "Evaluating the appropriateness of the BPMN 2.0 standard for modeling service choreographies: using an extended quality framework",
        "submission-date": "2012/06",
        "publication-date": "2014/03",
        "abstract": "The concept of choreography has emerged over the past years as a foundational concept for capturing and managing collaborative business processes. The latest version of the Business Process Modeling Notation (BPMN 2.0) adopts choreography as a first-class citizen. However, it remains an open question whether BPMN 2.0 is actually appropriate for capturing this concept. In this paper, we extend an existing language evaluation framework in order to evaluate the support for choreographies in BPMN 2.0. The framework provides a means of identifying the strengths and weaknesses of BPMN 2.0 for choreographies. We also give potential solutions that may be taken into account in future extensions or improvements to BPMN 2.0.",
        "keywords": [
            "Service choreographies",
            "Language quality framework",
            "BPMN 2.0",
            "Evaluation"
        ],
        "authors": [
            "Mario Cortes-Cornax",
            "Sophie Dupuy-Chessa",
            "Dominique Rieu",
            "Nadine Mandran"
        ],
        "file_path": "data/sosym-all/s10270-014-0398-0.pdf"
    },
    {
        "title": "Model-driven management of BPMN-based business process families",
        "submission-date": "2020/12",
        "publication-date": "2022/03",
        "abstract": "Business processes can have variants depending on speciﬁc business requirements, which lead to the deﬁnition of a so-called\nbusiness process family. Since conventional business process modeling languages, e.g., the Business Process Model and\nNotation (BPMN), do not explicitly support variants’ speciﬁcation, several proposals have emerged to deal with it. However,\nthey mainly focus on languages’ deﬁnition, while less emphasis is made on providing complete variability management. This\narticle presents a Model-Driven Engineering approach for managing BPMN-based business process families composed of a\nmetamodel for conceptualizing process families, a high-level process for managing them (involving model transformations\nfor the conﬁguration of variants), and tool support for the complete approach. We validated the proposal using a real-world\nexample from a university and an empirical study with real users. Users rated the support tool’s principal functional suitability\nand usability features as very good. Many improvement opportunities were detected, e.g., version control, collaborative work,\nand error reporting. We also provide a literature review and thorough evaluation of BPMN-based business process families’\nproposals using the VIVACE framework.",
        "keywords": [
            "Business process families",
            "Variability",
            "BPMN 2.0",
            "Model-Driven Engineering",
            "VIVACE"
        ],
        "authors": [
            "Andrea Delgado",
            "Daniel Calegari",
            "Félix García",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-022-00985-3.pdf"
    },
    {
        "title": "Dynamic hierarchical mega models: comprehensive traceability and its efﬁcient maintenance",
        "submission-date": "2009/01",
        "publication-date": "2009/12",
        "abstract": "In the world of model-driven engineering (MDE) support for traceability and maintenance of traceability infor-mation is essential. On the one hand, classical traceability approaches for MDE address this need by supporting auto-mated creation of traceability information on the model ele-ment level. On the other hand, global model management approaches manually capture traceability information on the model level. However, there is currently no approach that supports comprehensive traceability, comprising traceabil-ity information on both levels, and efﬁcient maintenance of traceability information, which requires a high-degree of automation and scalability. In this article, we present a com-prehensive traceability approach that combines classical traceability approaches for MDE and global model man-agement in form of dynamic hierarchical mega models. We further integrate efﬁcient maintenance of traceability infor-mation based on top of dynamic hierarchical mega models. The proposed approach is further outlined by using an indus-trial case study and by presenting an implementation of the concepts in form of a prototype.",
        "keywords": [
            "Model-driven engineering",
            "Traceability maintenance",
            "Global model management",
            "Mega model"
        ],
        "authors": [
            "Andreas Seibel",
            "Stefan Neumann",
            "Holger Giese"
        ],
        "file_path": "data/sosym-all/s10270-009-0146-z.pdf"
    },
    {
        "title": "Model-based assurance evidence management for safety–critical systems",
        "submission-date": "2020/11",
        "publication-date": "2022/01",
        "abstract": "Most safety–critical systems are subject to rigorous assurance processes to justify that the systems satisfy given requirements and are dependable. These processes are typically conducted in compliance with standards and require the provision of assurance evidence in the form of system artifacts, such as system speciﬁcations and testing results. The management of assurance evidence is usually a complex process because of the large number of artifacts to deal with, the amount of information to gather about the artifacts, and the need to guarantee evidence quality, among other issues. Our aim is to facilitate assurance evidence management by means of a model-based approach. The approach is based on a metamodel that deﬁnes the information to be collected about evidence artifacts during their lifecycle. A process for assurance evidence management and usage guidance are also presented. The approach has been developed in the scope of several industry-academia projects, implemented in the OpenCert tool, and validated by practitioners in 10 industrial case studies. Based on the results of the validation, we argue that the approach is an effective means for assurance evidence management and that it could improve the state of the practice.",
        "keywords": [
            "Assurance evidence",
            "Safety",
            "critical systems",
            "System assurance",
            "System certiﬁcation",
            "Model-Driven Engineering",
            "OpenCert"
        ],
        "authors": [
            "Jose Luis de la Vara",
            "Arturo S. García",
            "Jorge Valero",
            "Clara Ayora"
        ],
        "file_path": "data/sosym-all/s10270-021-00957-z.pdf"
    },
    {
        "title": "Integration of DFDs into a UML-based model-driven engineering approach",
        "submission-date": "2004/12",
        "publication-date": "2006/06",
        "abstract": "The main aim of this article is to discuss how the functional and the object-oriented views can be inter-played to represent the various modeling perspectives of embedded systems. We discuss whether the object-oriented modeling paradigm, the predominant one to develop software at the present time, is also adequate for modeling embedded software and how it can be used with the functional paradigm. More specifically, we present how the main modeling tool of the traditional structured methods, data ﬂow diagrams, can be integrated in an object-oriented development strategy based on the uniﬁed modeling language. The rationale behind the approach is that both views are important for modeling purposes in embedded systems environments, and thus a combined and integrated model is not only useful, but also fundamental for developing complex systems. The approach was integrated in a model-driven engineering process, where tool support for the models used was provided. In addition, model transformations have been speciﬁed and implemented to automate the process. We exemplify the approach with an IPv6 router case study.",
        "keywords": [
            "MDA",
            "UML",
            "Data-ﬂow Diagram",
            "Model Transformation",
            "Embedded Systems Speciﬁcation",
            "Process Model",
            "Activity Diagram",
            "IPv6 router"
        ],
        "authors": [
            "João M. Fernandes",
            "Johan Lilius",
            "Dragos Truscan"
        ],
        "file_path": "data/sosym-all/s10270-006-0013-0.pdf"
    },
    {
        "title": "Speciﬁcation and automated veriﬁcation of atomic concurrent real-time transactions",
        "submission-date": "2019/09",
        "publication-date": "2020/07",
        "abstract": "Many database management systems (DBMS) need to ensure atomicity and isolation of transactions for logical data consistency, as well as to guarantee temporal correctness of the executed transactions. Since the mechanisms for atomicity and isolation may lead to breaching temporal correctness, trade-offs between these properties are often required during the DBMS design. To be able to address this concern, we have previously proposed the pattern-based UPPCART framework, which models the transactions and the DBMS mechanisms as timed automata, and veriﬁes the trade-offs with provable guarantee. However, the manual construction of UPPCART models can require considerable effort and is prone to errors. In this paper, we advance the formal analysis of atomic concurrent real-time transactions with tool-automated construction of UPPCART models. The latter are generated automatically from our previously proposed UTRAN speciﬁcations, which are high-level UML-based speciﬁcations familiar to designers. To achieve this, we ﬁrst propose formal deﬁnitions for the modeling patterns in UPPCART, as well as for the pattern-based construction of DBMS models, respectively. Based on this, we establish a translational semantics from UTRAN speciﬁcations to UPPCART models, to provide the former with a formal semantics relying on timed automata, and develop a tool that implements the automated transformation. We also extend the expressiveness of UTRAN and UPPCART, to incorporate transaction sequences and their timing properties. We demonstrate the speciﬁcation in UTRAN, automated transformation to UPPCART, and veriﬁcation of the traded-off properties, via an industrial use case.",
        "keywords": [
            "Transaction",
            "Atomicity",
            "Isolation",
            "Temporal correctness",
            "Uniﬁed modeling language",
            "Model checking"
        ],
        "authors": [
            "Simin Cai",
            "Barbara Gallina",
            "Dag Nyström",
            "Cristina Seceleanu"
        ],
        "file_path": "data/sosym-all/s10270-020-00819-0.pdf"
    },
    {
        "title": "Method engineering in information systems analysis and design: a balanced scorecard approach for method improvement",
        "submission-date": "2017/09",
        "publication-date": "2018/08",
        "abstract": "Modeling methods have been proven to provide beneﬁcial instrumental support for different modeling tasks during informa-tion system analysis and design. However, methods are a complex phenomenon that include constructs such as procedural guidelines, concepts to focus on, visual representations and cooperation principles. In general, method development is an expensive task that usually involves many stakeholders and results in various method iterations. Since methods and method development are complex in nature, there is a need for a well-structured and resource-efﬁcient approach for method improve-ment. This paper aims to contribute to the ﬁeld of method improvement by proposing a balanced scorecard-based approach and by reporting on experiences from developing and using it in the context of a method for information demand analysis. The main contributions of the paper are as follows: (1) It provides a description of the process for developing a scorecard for method improvement; (2) it shows how the scorecard as such can be used as a tool for improving a speciﬁc method; and (3) it discusses experiences from applying the scorecard in industrial settings.",
        "keywords": [
            "Method improvement",
            "Balanced scorecard",
            "Method engineering",
            "Information demand analysis method"
        ],
        "authors": [
            "Kurt Sandkuhl",
            "Ulf Seigerroth"
        ],
        "file_path": "data/sosym-all/s10270-018-0692-3.pdf"
    },
    {
        "title": "Lessons learned from building model-driven development tools",
        "submission-date": "2011/11",
        "publication-date": "2012/07",
        "abstract": "Tools to support modelling in system and software engineering are widespread, and have reached a degree of maturity where their use and availability are accepted. Tools to support model-driven development (MDD)—where models are manipulated and managed throughout the system/software engineering lifecycle—have, over the last 10 years, seen much research and development attention. Over the last 10 years, we have had significant experience in the design, development and deployment of MDD tools in practical settings. In this paper, we distill some of the important lessons we have learned in developing and deploying two MDD tools: Epsilon and VIATRA. In doing so, we aim to identify some of the key principles of developing successful MDD tools, as well as some hints of the pitfalls and risks.",
        "keywords": [
            "Model-driven development",
            "Model management",
            "MDD tools",
            "Model tansformation"
        ],
        "authors": [
            "Richard F. Paige",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-012-0257-9.pdf"
    },
    {
        "title": "MDI: a rule-based multi-document and tool integration approach",
        "submission-date": "2004/11",
        "publication-date": "2006/06",
        "abstract": "Nowadays,typicalsoftwareandsystemengineer-ing projects in various industrial sectors (automotive, tele-communication, etc.) involve hundreds of developers using quite a number of different tools. Thus, the data of a pro-ject as a whole is distributed over these tools. Therefore, it is necessary to make the relationships of different tool data repositories visible and keep them consistent with each other. This still is a nightmare due to the lack of domain-speciﬁc adaptable tool and data integration solutions which support maintenance of traceability links, semi-automatic consistency checking as well as incremental update prop-agation. Currently used solutions are usually hand-coded one-way transformations between pairs of tools only. In this article we propose a new rule-based approach that allows for thedeclarativespeciﬁcationofdataintegrationrulesconcern-ing multiple data repositories. Hence, we call our approach “Multi Document Integration”. It generalizes the formalism of triple graph grammars and replaces the underlying data structureofdirectedgraphsbythemoregeneraldatastructure of MOF-compliant meta models. Our integration rule speci-ﬁcations are translated into JMI-compliant Java code which is used for various purposes by a tool integration framework. As a result we give an answer to OMG’s request for proposals for a MOF-compliant “queries, views, and transformation” approach from the “model driven application development” (MDA) ﬁeld.",
        "keywords": [],
        "authors": [
            "Alexander Königs",
            "Andy Schürr"
        ],
        "file_path": "data/sosym-all/s10270-006-0016-x.pdf"
    },
    {
        "title": "Classification and trend analysis of UML books (1997–2009)",
        "submission-date": "2010/04",
        "publication-date": "2011/02",
        "abstract": "Technical books of each subject area denote the levelofmaturityandknowledgedemandinthatarea.Accord-ing to the Google Books database, about 208 Uniﬁed Mod-eling Language (UML) books have been published from itsinception in 1997 until 2009. While various book reviewsare frequently published in various sources (e.g., IEEE Soft-ware Bookshelf), there are no studies to classify UML booksinto meaningful categories. Such a classiﬁcation can helpresearchers in the area to identify trends and also reveal thelevel of activity in each sub-area of UML. The statistical sur-vey reported in this article intends to be a ﬁrst step in classiﬁ-cation and trend analysis of the UML books published from1997 to 2009. The study also sheds light on the quantity ofbooks published in different focus areas (e.g., UML’s coreconcepts, patterns, tool support, Object Constraint Languageand Model-Driven Architecture) and also on different appli-cation domains (e.g., database modeling, web applications,and real-time systems). The trends of book publications ineach sub-area of UML are also used to track the level ofmaturity, to identify possible Hype cycles and also to mea-sure knowledge demand in each area.",
        "keywords": [
            "Survey",
            "Statistical study",
            "Classification",
            "Trend analysis",
            "UML",
            "Books"
        ],
        "authors": [
            "Vahid Garousi"
        ],
        "file_path": "data/sosym-all/s10270-011-0189-9.pdf"
    },
    {
        "title": "Implementation of a continuous delivery pipeline for enterprise architecture model evolution",
        "submission-date": "2019/09",
        "publication-date": "2020/09",
        "abstract": "The discipline of enterprise architecture (EA) is an established approach to model and manage the interaction of business processes and IT in an organization. Thereby, the EA model as a central artifact of EA is subject to a continuous evolution caused by multiple sources of changes. The continuous evolution requires a lot of effort in controlling and managing the evolution of the EA model. This is especially true when merging the induced changes from different sources in the EA model. Additionally, the lack of tool and automation support makes this a very time-consuming and error-prone task. The evolutionary character and the automated quality assessment of artifacts is a well-known challenge in the software development domain as well. To meet these challenges, the discipline of continuous delivery (CD) has emerged to be very useful. The evolution of EA model artifacts shows similarities to the evolution of software artifacts. Therefore, we leveraged practices of CD to practices of EA maintenance. Thus, we created a conceptual framework for automated EA model maintenance. The concepts were realized in a first prototype and were evaluated in a fictitious case study against equivalence classes based on EA model metrics and a set of several requirements for automated EA model maintenance from research. Overall, the concepts prove to be a promising basis for further refinement, implementation, and evaluation in research in an industrial context.",
        "keywords": [
            "Enterprise architecture model evolution",
            "Continuous delivery",
            "Enterprise architecture model maintenance"
        ],
        "authors": [
            "Alex R. Sabau",
            "Simon Hacks",
            "Andreas Steﬀens"
        ],
        "file_path": "data/sosym-all/s10270-020-00828-z.pdf"
    },
    {
        "title": "A local and global tour on MOMoT",
        "submission-date": "2016/11",
        "publication-date": "2017/12",
        "abstract": "Many model transformation scenarios require ﬂexible execution strategies as they should produce models with the highest possible quality. At the same time, transformation problems often span a very large search space with respect to possible transformation results. Recently, different proposals for ﬁnding good transformation results without enumerating the complete search space have been proposed by using meta-heuristic search algorithms. However, determining the impact of the different kinds of search algorithms, such as local search or global search, on the transformation results is still an open research topic. In this paper, we present an extension to MOMoT, which is a search-based model transformation tool, for supporting not only global searchers for model transformation orchestrations, but also local ones. This leads to a model transformation framework that allows as the ﬁrst of its kind multi-objective local and global search. By this, the advantages and disadvantages of global and local search for model transformation orchestration can be evaluated. This is done in a case-study-based evaluation, which compares different performance aspects of the local- and global-search algorithms available in MOMoT. Several interesting conclusions have been drawn from the evaluation: (1) local-search algorithms perform reasonable well with respect to both the search exploration and the execution time for small input models, (2) for bigger input models, their execution time can be similar to those of global-search algorithms, but global-search algorithms tend to outperform local-search algorithms in terms of search exploration, (3) evolutionary algorithms show limitations in situations where single changes of the solution can have a signiﬁcant impact on the solution’s ﬁtness.",
        "keywords": [
            "Model-driven engineering",
            "Model transformation",
            "Search-based software engineering",
            "Local search",
            "Global search"
        ],
        "authors": [
            "Robert Bill",
            "Martin Fleck",
            "Javier Troya",
            "Tanja Mayerhofer",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-017-0644-3.pdf"
    },
    {
        "title": "Guest Editorial to the Special Section on MODELS 2007",
        "submission-date": "2007/09",
        "publication-date": "2009/11",
        "abstract": "This section of “Software & Systems Modeling” contains four selected papers of the tenth MODELS conference. MODELS 2007 was held in Nashville, TN, USA, from September 30 to October 5, 2007. This conference on Model Driven Engineering Languages and Systems has established itself as the key international venue for the presentation of scientific results in the domain of model driven engineering and related topics such as software modeling and model transformation.",
        "keywords": [],
        "authors": [
            "Gregor Engels"
        ],
        "file_path": "data/sosym-all/s10270-009-0140-5.pdf"
    },
    {
        "title": "Securing critical infrastructures with a cybersecurity digital twin",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "With the diffusion of integrated design environments and tools for visual threat modeling for critical infrastructures, the concept of Digital Twin (DT) is gaining momentum in the ﬁeld of cybersecurity. Its main use is for enabling attack simulations and evaluation of countermeasures, without causing outage of the physical system. However, the use of a DT is considered foremost as a facilitator of system operation rather than an integral part of its architecture design. In this work, we introduce a speciﬁc architecture view in the system representation, called Cybersecurity View. From it, we derive a cybersecurity Digital Twin as part of the security-by-design practice for Industrial Automation and Control Systems used in Critical Infrastructures. Not only this digital twin serves the purpose of simulating cyber-attacks and devising countermeasures, but its design and function are also directly tied to the architecture model of the system for which the cybersecurity requirements are posed. Moreover, this holds regardless of whether the model is generated as part of the development cycle or through an empirical observation of the system as-is. With this, we enable the identiﬁcation of adequate cybersecurity measures for the system, while improving the overall system design. To demonstrate the practical usefulness of the proposed methodology, its application is illustrated through two real-world use cases: the Cooperative Intelligent Transport System (C-ITS) and the Road tunnel scenario.",
        "keywords": [
            "Enterprise architecture",
            "Reference architecture",
            "Cybersecurity view",
            "Digital twin",
            "Threat modeling",
            "Critical infrastructure",
            "Transportation"
        ],
        "authors": [
            "Massimiliano Masi",
            "Giovanni Paolo Sellitto",
            "Helder Aranha",
            "Tanja Pavleska"
        ],
        "file_path": "data/sosym-all/s10270-022-01075-0.pdf"
    },
    {
        "title": "Efﬁcient regression testing of distributed real-time reactive systems in the context of model-driven development",
        "submission-date": "2022/03",
        "publication-date": "2023/03",
        "abstract": "Regression testing is indispensable, especially for real-time distributed systems to ensure that existing functionalities are not affected by changes. Despite recent advances, regression testing for distributed systems remains challenging and extremely costly. Existing techniques often require running a failing system several times before detecting a regression. As a result, conventional approaches that use re-execution without considering the inherent non-determinism of distributed systems, and providing no (or low) control over execution are inadequate in many ways. In this paper, we present MRegTest, a replay-based regression testing framework in the context of model-driven development to facilitate deterministic replay of traces for detecting regressions while offering sufﬁcient control for the purpose of testing over the execution of the changed system. The experimental results show that compared to the traditional approaches that annotate traces with timestamps and variable values MRegTest detects almost all regressions while reducing the size of the trace signiﬁcantly and incurring similar runtime overhead.",
        "keywords": [
            "MDD",
            "Distributed systems",
            "Regression testing"
        ],
        "authors": [
            "Majid Babaei",
            "Juergen Dingel"
        ],
        "file_path": "data/sosym-all/s10270-023-01086-5.pdf"
    },
    {
        "title": "RaQuN: a generic and scalable n-way model matching algorithm",
        "submission-date": "2022/03",
        "publication-date": "2022/11",
        "abstract": "Model matching algorithms are used to identify common elements in input models, which is a fundamental precondition for\nmany software engineering tasks, such as merging software variants or views. If there are multiple input models, an n-way\nmatching algorithm that simultaneously processes all models typically produces better results than the sequential application\nof two-way matching algorithms. However, existing algorithms for n-way matching do not scale well, as the computational\neffort grows fast in the number of models and their size. We propose a scalable n-way model matching algorithm, which\nuses multi-dimensional search trees for efﬁciently ﬁnding suitable match candidates through range queries. We implemented\nour generic algorithm named RaQuN (Range Queries on N input models) in Java and empirically evaluate the matching\nquality and runtime performance on several datasets of different origins and model types. Compared to the state of the art,\nour experimental results show a performance improvement by an order of magnitude, while delivering matching results of\nbetter quality.",
        "keywords": [
            "Model-driven engineering",
            "n-Way model matching",
            "Clone-and-own development",
            "Software product lines"
        ],
        "authors": [
            "Alexander Schultheiß\nPaul Maximilian Bittner\nAlexander Boll\nLars Grunske\nThomas Thüm\nTimo Kehrer"
        ],
        "file_path": "data/sosym-all/s10270-022-01062-5.pdf"
    },
    {
        "title": "In memory of Bernhard Schätz, long- time friend and SoSyM editor",
        "submission-date": "2018/01",
        "publication-date": "2018/01",
        "abstract": "This paper is an editorial commemorating Bernhard Schätz, highlighting his contributions to science, research, and the Software and Systems Engineering (SSE) group. It discusses his dedication, skills, and impact on tool development, including AutoFocus, MontiCore, and MontiArc.",
        "keywords": [],
        "authors": [
            "Manfred Broy",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-018-0657-6.pdf"
    },
    {
        "title": "R-IO SUITE: integration of LLM-based AI into a knowledge management and model-driven based platform dedicated to crisis management",
        "submission-date": "2024/03",
        "publication-date": "Not found",
        "abstract": "This article presents how the R-IO SUITE software platform, a decision support system for crisis management entirely based on model-driven engineering principles, considerably beneﬁts from large language model (LLM)-based artiﬁcial intelligence (AI). The different components of the R-IO SUITE platform are used to climb the four abstraction layers: data, informa-tion, decision and action through interpretation (from data to information), exploitation (from information to decision) and implementation (from decision to action). These transitions between layers are supported by a knowledge base embedding knowledge instances structured according to a crisis management metamodel. From a functional perspective, this platform is fully operational, however, to be able to cover any type of crisis situation, the knowledge base should be enriched, ﬁrst, from a “resource perspective” (to embed the various available means to deal with any faced situation), and second, from an “issue perspective” (to understand all risks and damage that can appear on a crisis situation). It is not reasonable to consider creating and maintaining such an exhaustive knowledge base. However, the connection of the R-IO SUITE platform with LLM software such as ChatGPT© makes it possible, by generating appropriate prompts, to update on-the-ﬂy the knowledge base according to the faced context. This article shows how the LLM AI can provide complementary knowledge to formally fulﬁl the knowledge base to make it relevant to the faced crisis situation. This article presents the R-IO SUITE as a LLM-empowered model-driven platform to become an extended crisis management supporting system.",
        "keywords": [
            "Large language model",
            "Artiﬁcial intelligence",
            "Model-driven engineering",
            "Decision support system",
            "Knowledge base",
            "Ontologies",
            "Metamodel",
            "Complex event processing",
            "Business process management"
        ],
        "authors": [
            "Aurélie Congès",
            "Audrey Fertier",
            "Nicolas Salatgé",
            "Sébastien Rebière",
            "Frederick Benaben"
        ],
        "file_path": "data/sosym-all/s10270-024-01237-2.pdf"
    },
    {
        "title": "Model-based resource analysis and synthesis of service-oriented automotive software architectures",
        "submission-date": "2020/03",
        "publication-date": "2021/09",
        "abstract": "Context Automotive software architectures describe distributed functionality by an interaction of software components. One drawback of today’s architectures is their strong integration into the onboard communication network based on predeﬁned dependencies at design time. The idea is to reduce this rigid integration and technological dependencies. To this end, service-oriented architecture offers a suitable methodology since network communication is dynamically established at run-time.\nAim We target to provide a methodology for analysing hardware resources and synthesising automotive service-oriented architectures based on platform-independent service models. Subsequently, we focus on transforming these models into a platform-speciﬁc architecture realisation process following AUTOSAR Adaptive.\nApproach For the platform-independent part, we apply the concepts of design space exploration and simulation to analyse and synthesise deployment conﬁgurations, i.e., mapping services to hardware resources at an early development stage. We reﬁne these conﬁgurations to AUTOSAR Adaptive software architecture models representing the necessary input for a subsequent implementation process for the platform-speciﬁc part.\nResult We present deployment conﬁgurations that are optimal for the usage of a given set of computing resources currently under consideration for our next generation of E/E architecture. We also provide simulation results that demonstrate the ability of these conﬁgurations to meet the run time requirements. Both results helped us to decide whether a particular conﬁguration can be implemented. As a possible software toolchain for this purpose, we ﬁnally provide a prototype.\nConclusion The use of models and their analysis are proper means to get there, but the quality and speed of development must also be considered.",
        "keywords": [
            "Service-oriented architecture",
            "Real-time behaviour",
            "Model-based design",
            "Automotive architectures"
        ],
        "authors": [
            "Stefan Kugele",
            "Philipp Obergfell",
            "Eric Sax"
        ],
        "file_path": "data/sosym-all/s10270-021-00896-9.pdf"
    },
    {
        "title": "Some Basic Tenets of Description",
        "submission-date": "2002/06",
        "publication-date": "2002/09",
        "abstract": "Description – often referred to as modelling – is fundamental to software development. The developer should always be ready to say of each description: what subject it describes; what it says about its subject; and how it fits with other descriptions in the same development. Sometimes a very informal – even a casual – approach to these questions may be adopted. But often a more careful and explicit approach is needed. This short paper lays out some basic tenets of such an approach.",
        "keywords": [
            "Description",
            "Modelling",
            "Model domain",
            "Use cases"
        ],
        "authors": [
            "Michael Jackson"
        ],
        "file_path": "data/sosym-all/s10270-002-0005-7.pdf"
    },
    {
        "title": "Consistent speciﬁcation of interface suites in UML",
        "submission-date": "2002/02",
        "publication-date": "2002/12",
        "abstract": "The paper motivates and describes a model oriented approach for consistent speciﬁcation of interface suites in UML. An interface suite is a coherent collection of interfaces deﬁning interactions that transcend component boundaries. The speciﬁcation of interface suites contains diagrammatic views and documentation, but it is extended with templates for structured speciﬁcations deriving from the ISpec approach. To guarantee that the speciﬁcation views, documentation and templates are consistent, a speciﬁcation model has been constructed. The model contains both structural and behavioural information, represented in the form of sequences of carefully designed tuples. The model provides the underlying structure for the tool supporting the design process. The tool directs the designer to specify all elements of the model in a consistent way. The speciﬁcation is collected both by customized speciﬁcation templates and by diagrams. The documentation and the diagram elements – both derived from the template information – are automatically generated. This prevents errors and provides speciﬁcation consistency.",
        "keywords": [
            "UML",
            "Component speciﬁcation",
            "Consistency of several views",
            "Speciﬁcation model"
        ],
        "authors": [
            "E.E. Roubtsova",
            "L.C.M. van Gool",
            "R. Kuiper",
            "H.B.M. Jonkers"
        ],
        "file_path": "data/sosym-all/s10270-002-0011-9.pdf"
    },
    {
        "title": "Automated generation of consistent, diverse and structurally realistic graph models",
        "submission-date": "2020/01",
        "publication-date": "2021/05",
        "abstract": "In this paper, we present a novel technique to automatically synthesize consistent, diverse and structurally realistic domain-speciﬁc graph models. A graph model is (1) consistent if it is metamodel-compliant and it satisﬁes the well-formedness constraints of the domain; (2) it is diverse if local neighborhoods of nodes are highly different; and (1) it is structurally realistic if a synthetic graph is at a close distance to a representative real model according to various graph metrics used in network science, databases or software engineering. Our approach grows models by model extension operators using a hill-climbing strategy in a way that (A) ensures that there are no constraint violation in the models (for consistency reasons), while (B) more realistic candidates are selected to minimize a target metric value (wrt. the representative real model). We evaluate the effectiveness of the approach for generating realistic models using multiple metrics for guidance heuristics and compared to other model generators in the context of three case studies with a large set of real human models. We also highlight that our technique is able to generate a diverse set of models, which is a requirement in many testing scenarios.",
        "keywords": [
            "Model generation",
            "Domain-speciﬁc languages",
            "Test generation",
            "Graph metrics"
        ],
        "authors": [
            "Oszkár Semeráth",
            "Aren A. Babikian",
            "Boqi Chen",
            "Chuning Li",
            "Kristóf Marussy",
            "Gábor Szárnyas",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-021-00884-z.pdf"
    },
    {
        "title": "On model-based analysis of organizational structures: an assessment of current modeling approaches and application of multi-level modeling in support of design and analysis of organizational structures",
        "submission-date": "2018/09",
        "publication-date": "2019/11",
        "abstract": "Conceptual modeling promises to support various analysis questions on organizational structures, such as allocation of tasks, responsibilities, and authority in an organization. In this paper, we ﬁrst synthesize requirements on an organizational structure analysis from the business scholar literature and assess to what extent and how current modeling languages fulﬁll these. In par-ticular, we ﬁnd limitations in the covered scope as well as the information processing capabilities of the reviewed approaches. Second, as a response to identiﬁed gaps, we propose multi-level modeling and integrated modeling and programming as a way to support design and analysis of organizational structure. We use the structure of universities as a case scenario. This paper is an extension of our earlier work. Firstly, we add an explicit set of requirements derived from business scholar literature. Sec-ondly, we draw a comparison to the abstraction mechanisms used in conventional meta-modeling, as prominently exempliﬁed by UML class diagrams, and we critically discuss multi-level modeling. Finally, we discuss a prototypical implementation of our multi-level model in the XModeler software tool.",
        "keywords": [
            "Organizational structure",
            "Multi-level modeling",
            "Conceptual modeling"
        ],
        "authors": [
            "Sybren de Kinderen",
            "Monika Kaczmarek-Heß"
        ],
        "file_path": "data/sosym-all/s10270-019-00767-4.pdf"
    },
    {
        "title": "Impromptu: a framework for model-driven prompt engineering",
        "submission-date": "2024/04",
        "publication-date": "Not found",
        "abstract": "Generative artiﬁcial intelligence (AI) systems are capable of synthesizing complex artifacts such as text, source code or images according to the instructions provided in a natural language prompt. The quality of the input prompt, in terms of both content and structure, has a large impact on the quality of the output. This has given rise to prompt engineering, the process of designing natural language prompts to best take advantage of the capabilities of generative AI systems. This paper describes Impromptu, a model-driven engineering framework to support the creation, management and reuse of prompts for generative AI. Impromptu offers a domain-speciﬁc language (DSL) to deﬁne multimodal prompts in a modular and tool-independent way. The language offers additional features such as versioning, prompt chaining and multi-language support. Moreover, it provides tool support to adapt prompts for speciﬁc generative AI systems, execute those prompts on a generative AI system and validate the quality of the response that is generated. Impromptu is available as a Langium-based Visual Studio Code plugin.",
        "keywords": [
            "Prompt engineering",
            "Model-driven engineering",
            "Domain-speciﬁc language",
            "Generative AI",
            "Large language models"
        ],
        "authors": [
            "Sergio Morales",
            "Robert Clarisó",
            "Jordi Cabot"
        ],
        "file_path": "data/sosym-all/s10270-024-01235-4.pdf"
    },
    {
        "title": "A framework for the operationalization of monitoring in business intelligence requirements engineering",
        "submission-date": "2013/09",
        "publication-date": "2014/06",
        "abstract": "Business intelligence (BI) is perceived as a critical activity for organizations and is increasingly discussed in requirements engineering (RE). RE can contribute to the successful implementation of BI systems by assisting the identification and analysis of such systems’ requirements and the production of the specification of the system to be. Within RE for BI systems, we focus in this paper on the following questions: (i) how the expectations of a BI system’s stakeholders can be translated into accurate BI requirements, and (ii) how do we operationalize specifically these requirements in a system specification? In response, we define elicitation axes for the documentation of BI-specific requirements, give a list of six BI entities that we argue should be accounted for to operationalize business monitoring, and provide notations for the modeling of these entities. We survey important contributions of BI to define elicitation axes, adapt existing BI notations issued from RE literature, and complement them with new BI-specific notations. Using the i* framework, we illustrate the application of our proposal using a real-world case study.",
        "keywords": [
            "Business intelligence",
            "Requirement",
            "Monitoring",
            "Indicator",
            "Analytic",
            "Field",
            "Schema",
            "Source"
        ],
        "authors": [
            "Corentin Burnay",
            "Ivan J. Jureta",
            "Isabelle Linden",
            "Stéphane Faulkner"
        ],
        "file_path": "data/sosym-all/s10270-014-0417-1.pdf"
    },
    {
        "title": "Evaluating large language models on business process modeling: framework, benchmark, and self-improvement analysis",
        "submission-date": "2024/11",
        "publication-date": "2025/08",
        "abstract": "Large language models (LLMs) are rapidly transforming various ﬁelds, including the ﬁeld of business process management (BPM). LLMs provide new ways for analyzing and improving operational processes. This paper assesses the capabilities of LLMs on business process modeling using a framework for automating this task and a robust evaluation approach. We design a comprehensive benchmark, consisting of 20 diverse business processes, and we demonstrate our evaluation approach by assessing 16 current state-of-the-art LLMs from major AI vendors. Our analysis highlights signiﬁcant performance variations across LLMs and reveals a positive correlation between efﬁcient error handling and the quality of generated models. It also shows consistent performance trends within similar LLM groups. Furthermore, we use our evaluation approach to investigate LLM self-improvement techniques, encompassing self-evaluation, input optimization, and output optimization. Our ﬁndings indicate that output optimization, in particular, offers promising potential for enhancing quality, especially in models with initially lower performance. Our contributions provide insights for leveraging LLMs in BPM, paving the way for more advanced and automated process modeling techniques.",
        "keywords": [
            "Business process modeling",
            "Large language models",
            "Generative AI",
            "Benchmarking",
            "Process mining"
        ],
        "authors": [
            "Humam Kourani",
            "Alessandro Berti",
            "Daniel Schuster",
            "Wil M. P. van der Aalst"
        ],
        "file_path": "data/sosym-all/s10270-025-01318-w.pdf"
    },
    {
        "title": "On the complex nature of MDE evolution and its impact on changeability",
        "submission-date": "2014/08",
        "publication-date": "2015/04",
        "abstract": "In model-driven engineering (MDE), a particular MDE setting of employed languages and automated and manual activities has major impact on productivity. Furthermore, it has been observed that such MDE settings evolve over time. However, currently not much is known about this evolution and its impact on the MDE setting’s maturity, i.e., on changeability and other productivity dimensions. Research so far focuses on evolution of separate building blocks, such as (modeling-) languages, tools, or transformation, only. In this article, we address the lack of knowledge about evolution of MDE settings by investigating case studies from different companies. The ﬁrst results reveal (1) that there is evolution that affects the composition of an MDE setting (structural evolution) and has the potential to strongly impact aspects, such as changeability and (2) that this structural evolution actually occurs in practice. Based on these ﬁrst results, we investigated (3) whether there are cases in practice, where structural evolution already altered the risks of changeability given by the respective MDE setting. Therefore, we search and identify examples for such evolution steps on MDE set-tings from practice and collected six case studies on evolution histories in detail. As a result, we show in this paper that structural evolution (a) is not seldom in practice and (b) sometimes leads to the introduction of changeability risks.",
        "keywords": [
            "Model-driven engineering",
            "Evolution",
            "Empirical research"
        ],
        "authors": [
            "Regina Hebig",
            "Holger Giese"
        ],
        "file_path": "data/sosym-all/s10270-015-0464-2.pdf"
    },
    {
        "title": "UML formal semantics: lessons learned",
        "submission-date": "2011/05",
        "publication-date": "2011/06",
        "abstract": "The article below presents the insights gained during a number of years of research dedicated to the formalisation of the Uniﬁed Modeling Language.",
        "keywords": [
            "Formal semantics",
            "Compositional semantics",
            "Multiple system views",
            "All-encompassing UML semantics",
            "Formal model-driven system development"
        ],
        "authors": [
            "Manfred Broy",
            "María Victoria Cengarle"
        ],
        "file_path": "data/sosym-all/s10270-011-0207-y.pdf"
    },
    {
        "title": "CaRE: a reﬁnement calculus for requirements engineering based on argumentation theory",
        "submission-date": "2020/11",
        "publication-date": "2021/11",
        "abstract": "The Requirements Engineering (RE) process starts with initial requirements elicited from stakeholders—however conﬂicting, unattainable, incomplete and ambiguous—and successively reﬁnes them until a consistent, complete, valid, and unambiguous speciﬁcation is reached. This is achieved by balancing stakeholders’ viewpoints and preferences to reach compromises through negotiation. Several frameworks have been developed to support this process in a structured way, such as KAOS, i*, and RationalGLR. However, none provides the means to model the dialectic negotiation inherent to the RE process, so that the derivation of speciﬁcations from requirements is fully explicit and traceable. To address this gap, we propose CaRE, a reﬁnement calculus for requirements engineering based on argumentation theory. CaRE casts the RE reﬁnement problem as an iterative argument between all relevant stakeholders, who point out defects (ambiguity, incompleteness, etc.) of existing requirements, and then propose suitable reﬁnements to address them, thereby leading to the construction of a reﬁnement graph. This graph is then a conceptual model of the RE process. The semantics of reﬁnement graphs is provided using Argumentation Theory, enabling reasoning over the RE process and the automatic computation of software speciﬁcations. An alternate semantics is also presented based on abduction and using Horn Theory. The application of CaRE is showcased with an extensive example from the railway domain, and a prototype tool for identifying speciﬁcations in a reﬁnement graph is presented.",
        "keywords": [
            "Requirements engineering",
            "Requirements reﬁnement",
            "RE process",
            "RE calculus",
            "Argumentation theory",
            "Formal semantics"
        ],
        "authors": [
            "Yehia Elrakaiby",
            "Alexander Borgida",
            "Alessio Ferrari",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-021-00943-5.pdf"
    },
    {
        "title": "Involving users in the development of a modeling language for customer journeys",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Although numerous methods for handling the technical aspects of developing domain-speciﬁc modeling languages (DSMLs) have been formalized, user needs and usability aspects are often addressed late in the development process and in an ad hoc manner. To this concern, this paper presents the development of the customer journey modeling language (CJML), a DSML for modeling service processes from the end-user’s perspective. Because CJML targets a wide and heterogeneous group of users, its usability can be challenging to plan and assess. This paper describes how an industry-relevant DSML was systematically improved by using a variety of user-centered design techniques in close collaboration with the target group, whose feedback was used to reﬁne and evolve the syntax and semantics of CJML. We also suggest how a service-providing organization may beneﬁt from adopting CJML as a unifying language for documentation purposes, compliance analysis, and service innovation. Finally, we distill what we learned into general lessons and methodological guidelines.",
        "keywords": [
            "Domain-speciﬁc modeling language (DSML)",
            "Customer journey",
            "Conceptual modeling",
            "User involvement"
        ],
        "authors": [
            "Ragnhild Halvorsrud",
            "Odnan Ref Sanchez",
            "Costas Boletsis",
            "Marita Skjuve"
        ],
        "file_path": "data/sosym-all/s10270-023-01081-w.pdf"
    },
    {
        "title": "On the automated translational execution of the action language for foundational UML",
        "submission-date": "2016/03",
        "publication-date": "2016/09",
        "abstract": "To manage the rapidly growing complexity of software development, abstraction and automation have been recognised as powerful means. Among the techniques pushing for them, model-driven engineering has gained increasing attention from industry for, among others, the possibility to automatically generate code from models. To generate fully executable code, models should describe complex behaviours. While pragmatically this is achieved by employing programming languages for deﬁning actions within models, the abstraction gap between modelling and programming languages can undermine consistency between models and code as well as analysability and reusability of models. In light of this, model-aware action languages should be preferred. This is the case of the Action Language for Foundational UML (ALF). In this paper, we provide a solution for the fully automated translational execution of ALF towards C++. Additionally, we give an insight on how to simplify the transition from the use of programming languages for modelling ﬁne-grained behaviours to model-aware action languages in industrial MDE. The solution presented in this paper has been assessed on industrial applications to verify its applicability to complex systems as well as its scalability.",
        "keywords": [
            "Model-driven engineering",
            "Translational execution",
            "Code generation",
            "UML",
            "ALF"
        ],
        "authors": [
            "Federico Ciccozzi"
        ],
        "file_path": "data/sosym-all/s10270-016-0556-7.pdf"
    },
    {
        "title": "Guest editorial to the theme section on AI-enhanced model-driven engineering",
        "submission-date": "2022/02",
        "publication-date": "2022/03",
        "abstract": "This theme section brings together the latest research at the intersection of artiﬁcial intelligence (AI) and model-driven engineering (MDE). Over the past years, we have witnessed a substantial rise of AI successfully applied to different domains, including software development and MDE. Dedicated events at the intersection of AI and MDE have been created, too, such as the MDE Intelligence workshop series co-located with the MODELS conference. This theme section covers research contributions integrating AI components into MDE approaches—increasing the current beneﬁts of MDE processes and tools and pushing the limits of “classic” MDE with the goal to provide software and systems engineers with the right techniques to develop the next generation of highly complex model-based systems—and applications of MDE to the development of AI components. In total, nine submissions were accepted in the theme section after a thorough peer-reviewing process.",
        "keywords": [
            "Artiﬁcial intelligence",
            "Model-driven engineering",
            "Software engineering",
            "Systems engineering"
        ],
        "authors": [
            "Lola Burgueño",
            "Jordi Cabot",
            "Manuel Wimmer",
            "Steﬀen Zschaler"
        ],
        "file_path": "data/sosym-all/s10270-022-00988-0.pdf"
    },
    {
        "title": "Evaluating data-centric process approaches: Does the human factor factor in?",
        "submission-date": "2014/09",
        "publication-date": "2016/02",
        "abstract": "The Business Process Management ﬁeld addresses design, improvement, management, support, and execution of business processes. In doing so, we argue that it focuses more on developing modeling notations and process design approaches than on the needs and preferences of the individual who is modeling (i.e., the user). New data-centric process modeling approaches are taken as a relevant and timely stream of process design approaches to test our argument. First, we provide a review of existing data-centric process approaches, culminating in a theoretical classification framework. Next, we empirically evaluate three specific approaches with regard to the claims they make. We had participants representative of actual users try out these approaches on realistic scenarios via a series of workshops. Participants assessed to what extent quality claims from the literature could be recognized within the workshop sessions. The results of this evaluation substantiate a number of claims behind the approaches, but also identify opportunities to further improve them. Most prominently, we found that the usability aspects of all considered approaches are a source of concern. This leads us to the insight that usability aspects of process design approaches are crucial and, in the perception of groups representative of actual users, leave much to be desired. In that sense, our research can be seen as a wake-up call for process modeling notation designers to consider the usability side—and as such, the interest of the human modeler—more than is currently the case.",
        "keywords": [
            "Process modeling",
            "User",
            "Usability",
            "Data-centric",
            "Evaluation",
            "Review"
        ],
        "authors": [
            "Hajo A. Reijers",
            "Irene Vanderfeesten",
            "Marijn G. A. Plomp",
            "Pieter Van Gorp",
            "Dirk Fahland",
            "Wim L. M. van der Crommert",
            "H. Daniel Diaz Garcia"
        ],
        "file_path": "data/sosym-all/s10270-015-0491-z.pdf"
    },
    {
        "title": "A model-driven approach to automate the propagation of changes among Architecture Description Languages",
        "submission-date": "2009/11",
        "publication-date": "2010/07",
        "abstract": "As it is widely recognized, a universal nota-tion accepted by any software architect cannot exist. This caused a proliferation of architecture description languages (ADLs) each focussing on a speciﬁc application domain, analysis type, or modelling environment, and with its own speciﬁc notations and tools. Therefore, the production of a software architecture description often requires the use of multiple ADLs, each satisfying some stakeholder’s concerns. When dealing with multiple notations, suitable techniques are required in order to keep models in a consistent state. Several solutions have been proposed so far but they lack in convergence and scalability. In this paper, we propose a convergent change propagation approach between multi-ple architectural languages. The approach is generic since it depends neither on the notations to synchronize nor on their corresponding models. It is implemented within the Eclipse modelling framework and we demonstrate its usability and scalability by experimenting it on well known architectural languages.",
        "keywords": [
            "Architectural languages interoperability",
            "Model transformation",
            "Model synchronization",
            "Automation",
            "Metamodelling"
        ],
        "authors": [
            "Romina Eramo",
            "Ivano Malavolta",
            "Henry Muccini",
            "Patrizio Pelliccione",
            "Alfonso Pierantonio"
        ],
        "file_path": "data/sosym-all/s10270-010-0170-z.pdf"
    },
    {
        "title": "Transforming XML schemas into OWL ontologies using formal concept analysis",
        "submission-date": "2016/11",
        "publication-date": "2018/01",
        "abstract": "Ontology Web Language (OWL) is considered as a data representation format exploited by the Extensible Markup Language (XML) format. OWL extends XML by providing properties to further express the semantics of data. To this effect, transforming XML data into OWL proves important and constitutes an added value for indexing XML documents and re-engineering ontologies. In this paper, we propose a formal method to transform XSD schemas into OWL schemas using transformation patterns. To achieve this end, we extend at the beginning, a set of existing transformation patterns to allow the maximum transformation of XSD schema constructions. In addition, a formal method is presented to transform an XSD schema using the extended patterns. This method named PIXCO comprises several processes. The ﬁrst process models both the transformation patterns and all the constructions of XSD schema to be transformed. The patterns are modeled using the context of Formal Concept Analysis. The XSD constructions are modeled using a proposed mathematical model. This modeling will be used in the design of the following process. The second process identiﬁes the most appropriate patterns to transform each construction set of XSD schema. The third process generates for each XSD construction set an OWL model according to the pattern that is identiﬁed. Finally, it creates the OWL ﬁle encompassing the generated OWL models.",
        "keywords": [
            "XML schema",
            "OWL ontology",
            "Formal transformation",
            "XSD formalization",
            "FCA",
            "Transformation patterns"
        ],
        "authors": [
            "Mokhtaria Hacherouf",
            "Saﬁa Nait-Bahloul",
            "Christophe Cruz"
        ],
        "file_path": "data/sosym-all/s10270-017-0651-4.pdf"
    },
    {
        "title": "An example is worth a thousand words: Creating graphical modelling environments by example",
        "submission-date": "2016/11",
        "publication-date": "2017/11",
        "abstract": "Domain-speciﬁc languages (DSLs) are heavily used in model-driven and end-user development approaches. Compared to general-purpose languages, DSLs present numerous beneﬁts like powerful domain-speciﬁc primitives, an intuitive syntax for domain experts, and the possibility of advanced code generation for narrow domains. While a graphical syntax is sometimes desired for a DSL, constructing graphical modelling environments is a costly and highly technical task. This relegates domain experts to a rather passive role in their development and hinders a wider adoption of graphical DSLs. Our aim is achieving a simpler DSL construction process where domain experts can contribute actively. For this purpose, we propose an example-based technique for the automatic generation of modelling environments for graphical DSLs. This way, starting from examples of the DSL likely provided by domain experts using drawing tools like yED, our system synthesizes a graphical modelling environment that mimics the syntax of the provided examples. This includes a meta-model for the abstract syntax of the DSL and a graphical concrete syntax supporting spatial relationships like containment and adjacency. Our system, called metaBUP, is implemented as an Eclipse plug-in. In this paper, we demonstrate its usage on a running example in the home networking domain and evaluate its suitability for the construction of graphical modelling environments by means of a user study.",
        "keywords": [
            "Domain-speciﬁc modelling languages",
            "Graphical modelling environments",
            "Example-based meta-modelling",
            "Flexible modelling"
        ],
        "authors": [
            "Jesús J. López-Fernández",
            "Antonio Garmendia",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-017-0632-7.pdf"
    },
    {
        "title": "Modelling on mobile devices: A systematic mapping study",
        "submission-date": "2021/01",
        "publication-date": "2021/06",
        "abstract": "Modelling is central to many disciplines in engineering and the natural and social sciences. A wide variety of modelling\nlanguages and tools have been proposed along the years, traditionally for static environments such as desktops and laptops.\nHowever, the availability of increasingly powerful mobile devices makes it possible to proﬁt from their embedded sensors\nand components (e.g. camera, microphone, GPS, accelerometer, gyroscope) for modelling. This has promoted a new range\nof modelling tools specially designed for their use in mobility. Such tools open the door to modelling in dynamic scenarios\nthat go beyond the capabilities of traditional desktop tools. For example, modelling in mobility can be useful to design smart\nfactories on-site, or to create models of hiking routes while walking along the routes, among many other scenarios. In this\npaper, we report on a systematic mapping study to identify the state of the art and trends in modelling on mobile devices.\nThe study covers both research papers and modelling apps from the Android and iOS stores. From this analysis, we derive a\nclassiﬁcation for mobile modelling tools along three orthogonal dimensions, discuss current gaps, and propose avenues for\nfurther research.",
        "keywords": [
            "Model-driven engineering",
            "Modelling tools",
            "Mobile devices",
            "Systematic mapping study"
        ],
        "authors": [
            "Léa Brunschwig",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-021-00897-8.pdf"
    },
    {
        "title": "Bridging value modelling to ArchiMate via transaction modelling",
        "submission-date": "2011/11",
        "publication-date": "2012/11",
        "abstract": "The ArchiMate modelling language provides a coherent and a holistic view of an enterprise in terms of its products, services, business processes, actors, business units, software applications and more. Yet, ArchiMate currently lacks (1) expressivity in modelling an enterprise from a value exchange perspective, and (2) rigour and guidelines in modelling business processes that realize the transactions relevant from a value perspective. To address these issues, we show how to connect e3value, a technique for value modelling, to ArchiMate via transaction patterns from the DEMO methodology. Using ontology alignment techniques, we show a transformation between the meta mod-els underlying e3value, DEMO and ArchiMate. Furthermore, we present a step-wise approach that shows how this model transformation is achieved and, in doing so, we also show the relevance of such a transformation. We exemplify the transformation of DEMO and e3value into ArchiMate by means of a case study in the insurance industry. As a proof of concept, we present a software tool supporting our trans-formation approach. Finally, we discuss the functionalities and limitations of our approach; thereby, we analyze its advantages and practical applicability.",
        "keywords": [
            "ArchiMate",
            "e3value",
            "DEMO",
            "Meta model",
            "Model transformation."
        ],
        "authors": [
            "Sybren de Kinderen",
            "Khaled Gaaloul",
            "Henderik A. Proper"
        ],
        "file_path": "data/sosym-all/s10270-012-0299-z.pdf"
    },
    {
        "title": "Modeling languages in Industry 4.0: an extended systematic mapping study",
        "submission-date": "2018/07",
        "publication-date": "2019/09",
        "abstract": "Industry 4.0 integrates cyber-physical systems with the Internet of Things to optimize the complete value-added chain. Successfully applying Industry 4.0 requires the cooperation of various stakeholders from different domains. Domain-speciﬁc modeling languages promise to facilitate their involvement through leveraging (domain-speciﬁc) models to primary develop-ment artifacts. We aim to assess the use of modeling in Industry 4.0 through the lens of modeling languages in a broad sense. Based on an extensive literature review, we updated our systematic mapping study on modeling languages and modeling techniques used in Industry 4.0 (Wortmann et al., Conference on model-driven engineering languages and systems (MOD-ELS’17), IEEE, pp 281–291, 2017) to include publications until February 2018. Overall, the updated study considers 3344 candidate publications that were systematically investigated until 408 relevant publications were identiﬁed. Based on these, we developed an updated map of the research landscape on modeling languages and techniques for Industry 4.0. Research on modeling languages in Industry 4.0 focuses on contributing methods to solve the challenges of digital representation and integration. To this end, languages from systems engineering and knowledge representation are applied most often but rarely combined. There also is a gap between the communities researching and applying modeling languages for Industry 4.0 that originates from different perspectives on modeling and related standards. From the vantage point of modeling, Industry 4.0 is the combination of systems engineering, with cyber-physical systems, and knowledge engineering. Research currently is splintered along topics and communities and accelerating progress demands for multi-disciplinary, integrated research efforts.",
        "keywords": [
            "Industry 4.0",
            "Modeling languages",
            "Smart manufacturing"
        ],
        "authors": [
            "Andreas Wortmann",
            "Olivier Barais",
            "Benoit Combemale",
            "Manuel Wimmer"
        ],
        "file_path": "data/sosym-all/s10270-019-00757-6.pdf"
    },
    {
        "title": "Introducing probabilistic reasoning within Event-B",
        "submission-date": "2017/03",
        "publication-date": "2017/10",
        "abstract": "Event-B is a proof-based formal method used for discrete systems modelling. Several works have previously focused on the extension of Event-B for the description of probabilistic systems. In this paper, we propose an exten-sion of Event-B that allows designing fully probabilistic systems as well as systems containing both probabilistic and non-deterministic choices. Compared to existing approaches which only focus on probabilistic assignments, our approach allows expressing probabilistic choices in all places where non-deterministic choices originally appear in a standard Event-B model: in the choice between enabled events, event parameter values and in probabilistic assignments. Furthermore, we introduce novel and adapted proof obligations for the consistency of such systems and introduce two key aspects to incremental design: probabilisation of existing events and reﬁnement through the addition of new proba-bilistic events. In particular, we provide proof obligations for the almost-certain convergence of a set of new events, which is a required property in order to prove standard reﬁnement in this context. Finally, we propose a fully detailed case study, which we use throughout the paper to illustrate our new con-structions.",
        "keywords": [
            "Event-B",
            "Probabilistic systems",
            "Markov chains"
        ],
        "authors": [
            "Mohamed Amine Aouadhi",
            "Benoît Delahaye",
            "Arnaud Lanoix"
        ],
        "file_path": "data/sosym-all/s10270-017-0626-5.pdf"
    },
    {
        "title": "Supporting multiple perspectives in feature-based conﬁguration",
        "submission-date": "2011/04",
        "publication-date": "2011/11",
        "abstract": "Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.",
        "keywords": [
            "Software product line engineering",
            "Feature diagram",
            "Separation of concerns",
            "Multi-view",
            "Feature-based conﬁguration"
        ],
        "authors": [
            "Arnaud Hubaux",
            "Patrick Heymans",
            "Pierre-Yves Schobbens",
            "Dirk Deridder",
            "Ebrahim Khalil Abbasi"
        ],
        "file_path": "data/sosym-all/s10270-011-0220-1.pdf"
    },
    {
        "title": "Automated generation of smart contract code from legal contract speciﬁcations with SYMBOLEO2SC",
        "submission-date": "2023/05",
        "publication-date": "2024/06",
        "abstract": "Smart contracts (SCs) are software systems that monitor and partially control the execution of legal contracts to ensure compliance with the contracts’ terms and conditions, which essentially are sets of obligations and powers, triggered by events. Such systems often exploit Internet-of-Things technologies to support their monitoring functions and blockchain technology to ensure the integrity of their data. Enterprise-level blockchain platforms (such as Hyperledger Fabric) and public ones (such as Ethereum) are popular choices for SC development. However, usually, legal experts are not able to directly encode contract requirements into SCs. Symboleo is a formal speciﬁcation language for legal contracts that was introduced to address this issue. Symboleo uses an ontology that deﬁnes legal concepts such as parties, obligations, powers, and assets, with semantics expressed with state machines. This paper proposes a tool that automatically translates Symboleo speciﬁcations into smart contract code for Hyperledger Fabric. Towards this end, we have extended the current Symboleo IDE, implemented the ontology and semantics by using the modelling language Umple, and created a reusable library. The resulting Symboleo2SC tool generates Hyperledger Fabric code exploiting this library. This code is a complete translation and does not require further development. Symboleo2SC was evaluated with ﬁve sample contracts. These were converted to SCs for contract monitoring and control purposes. Symboleo2SC helps simplify the SC development process, saves development effort, and helps reduce risks of coding errors.",
        "keywords": [
            "Smart contracts",
            "Code generation",
            "Blockchain",
            "Domain-speciﬁc languages",
            "Legal ontology",
            "Symboleo"
        ],
        "authors": [
            "Aidin Rasti",
            "Amal Ahmed Anda",
            "Sofana Alfuhaid",
            "Alireza Parvizimosaed",
            "Daniel Amyot",
            "Marco Roveri",
            "Luigi Logrippo",
            "John Mylopoulos"
        ],
        "file_path": "data/sosym-all/s10270-024-01187-9.pdf"
    },
    {
        "title": "Matters of (meta-) modeling",
        "submission-date": "2004/11",
        "publication-date": "2006/07",
        "abstract": "With the recent trend to model driven engi-neering a common understanding of basic notions such as “model” and “metamodel” becomes a pivotal issue. Even though these notions have been in widespread use for quite a while, there is still little consensus about when exactly it is appropriate to use them. The aim of this article is to start establishing a consensus about generally acceptable terminology. Its main contributions are the distinction between two fundamentally different kinds of model roles, i.e. “token model” versus “type model” (The terms “type” and “token” have been introduced by C.S. Peirce, 1839–1914.), a formal notion of “meta-ness”, and the consideration of “generalization” as yet another basic relationship between models. In particular, the recognition of the fundamental difference between the above mentioned two kinds of model roles is crucial in order to enable communication among the model driven engineering community that is free of both unnoticed misunderstandings and unnecessary disagreement.",
        "keywords": [
            "Model driven engineering",
            "Modeling",
            "Metamodeling",
            "Token model",
            "Type model"
        ],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-006-0017-9.pdf"
    },
    {
        "title": "Defining business model key performance indicators using intentional linguistic summaries",
        "submission-date": "2020/09",
        "publication-date": "2021/06",
        "abstract": "To sustain competitiveness in contemporary, fast-paced markets, organizations increasingly focus on innovating their business models to enhance current value propositions or to explore novel sources of value creation. However, business model innovation is a complex task, characterized by shifting characteristics in terms of uncertainty, data availability and its impact on decision making. To cope with such challenges, business model evaluation is advocated to make sense of novel business models and to support decision making. Key performance indicators (KPIs) are frequently used in business model evaluation to structure the performance assessment of these models and to evaluate their strategic implications, in turn aiding business model decision making. However, given the shifting characteristics of the innovation process, the application and effectiveness of KPIs depend significantly on how such KPIs are defined. The techniques proposed in the existing literature typically generate or use quantitatively oriented KPIs, which are not well-suited for the early phases of the business model innovation process. Therefore, following a design science research methodology, we have developed a novel method for defining business model KPIs, taking into account the characteristics of the innovation process, offering holistic support toward decision making. Building on theory on linguistic summarization, we use a set of structured templates to define qualitative KPIs that are suitable to support early-phase decision making. In addition, we show how these KPIs can be gradually quantified to support later phases of the innovation process. We have evaluated our method by applying it in two real-life business cases, interviewing 13 industry experts to assess its utility.",
        "keywords": [
            "Business model evaluation",
            "Key performance indicators",
            "Linguistic summarization",
            "Intentional linguistic summaries",
            "Business model innovation"
        ],
        "authors": [
            "Rick Gilsing",
            "Anna Wilbik",
            "Paul Grefen",
            "Oktay Turetken",
            "Baris Ozkan",
            "Onat Ege Adali",
            "Frank Berkers"
        ],
        "file_path": "data/sosym-all/s10270-021-00894-x.pdf"
    },
    {
        "title": "Incremental execution of model-to-text transformations using property access traces",
        "submission-date": "2016/06",
        "publication-date": "2018/03",
        "abstract": "Automatic generation of textual artefacts (including code, documentation, conﬁguration ﬁles, build scripts, etc.) from models\nin a software development process through the application of model-to-text (M2T) transformation is a common MDE activity.\nDespite the importance of M2T transformation, contemporary M2T languages lack support for developing transformations\nthat scale with the size of the input model. As MDE is applied to systems of increasing size and complexity, a lack of scalability\nin M2T transformation languages hinders industrial adoption. In this paper, we propose a form of runtime analysis that can be\nused to identify the impact of source model changes on generated textual artefacts. The structures produced by this runtime\nanalysis, property access traces, can be used to perform efﬁcient source-incremental transformation: our experiments show\nan average reduction of 60% in transformation execution time compared to non-incremental (batch) transformation.",
        "keywords": [
            "Model-driven engineering",
            "Scalability",
            "Model-to-text transformation",
            "Incrementality"
        ],
        "authors": [
            "Babajide Ogunyomi",
            "Louis M. Rose",
            "Dimitrios S. Kolovos"
        ],
        "file_path": "data/sosym-all/s10270-018-0666-5.pdf"
    },
    {
        "title": "The design of a language for model transformations",
        "submission-date": "2004/11",
        "publication-date": "2006/07",
        "abstract": "Model-driven development of software systems envisions transformations applied in various stages of the development process. Similarly, the use of domain-speciﬁc languages also necessitates transformations that map domain-speciﬁc constructs into the constructs of an underlying programming language. Thus, in these cases, the writing of transformation tools becomes a ﬁrst-class activity of the software engineer. This paper introduces a language that was designed to support implementing highly efﬁcient transformation programs that perform model-to-model or model-to-code translations. The language uses the concepts of graph transformations and metamodeling, and is supported by a suite of tools that allow the rapid prototyping and realization of transformation tools.",
        "keywords": [
            "Model transformation",
            "UML",
            "Graph transformation",
            "Graph rewriting",
            "Model driven architecture"
        ],
        "authors": [
            "Aditya Agrawal",
            "Gabor Karsai",
            "Sandeep Neema",
            "Feng Shi",
            "Attila Vizhanyo"
        ],
        "file_path": "data/sosym-all/s10270-006-0027-7.pdf"
    },
    {
        "title": "OCL 1.4/5 vs. 2.0 Expressions",
        "submission-date": "2003/11",
        "publication-date": "2003/11",
        "abstract": "A type inference system and a big-step operational semantics for expressions of the “Object Constraint Language” (OCL), the declarative and navigational constraint language for the “Uniﬁed Modeling Language” (UML), are provided; the account is mainly based on OCL 1.4/5, but also includes the main features of OCL 2.0. The formal systems are parameterised in terms of UML static structures and UML object models, which are treated abstractly. It is proved that the operational semantics satisﬁes a subject reduction property with respect to the type inference system. Proceeding from the operational semantics and providing a denotational semantics, pure OCL 2.0 expressions are shown to exactly represent the primitive recursive functions, whereas pure OCL 1.4/5 expressions are Turing complete.",
        "keywords": [
            "OCL",
            "UML",
            "Formal semantics"
        ],
        "authors": [
            "Mar´ıa Victoria Cengarle",
            "Alexander Knapp"
        ],
        "file_path": "data/sosym-all/s10270-003-0035-9.pdf"
    },
    {
        "title": "Guest editorial to the special section on model transformation",
        "submission-date": "2008/06",
        "publication-date": "2009/01",
        "abstract": "Models play a cornerstone role in Model-Driven Engineering (MDE). The use of models opens up new possibilities for creating, analyzing, and manipulating systems through various types of tools and languages. Each model usually addresses one set of related concerns, and transformations between models provide a chain that enables the automated development of a system from its corresponding models. Model transformation speciﬁcation, implementation, and execution are the major parts of this process. Furthermore, model transformations may also be realized using models, and are, therefore, an integral part of a model-driven approach. Model transformations need specialized support in several aspects in order to realize their full potential for system modelers, transformation developers, and tool vendors. The problem goes beyond having speciﬁc languages to represent model transformations; we also need to understand the key concepts and operators supporting those languages, their semantics and their structuring mechanisms and properties (e.g., modularity, composability, and parameterization). In addition, model transformations can be stored in repositories as reusable assets, where they can be discovered and reused. There is also a need to chain and combine model transformations in order to produce new and more powerful transformations. Moreover, they need to be fully integrated into software development methodologies supported by appropriate tools and environments. The objective of this special section is to provide a representative sample of advanced research emerging from the ﬁeld of model transformation. The selected papers provide an overview of current open issues and identify potential lines for further research.",
        "keywords": [],
        "authors": [
            "Jean Bézivin",
            "Alfonso Pierantonio",
            "Antonio Vallecillo",
            "Jeff Gray"
        ],
        "file_path": "data/sosym-all/s10270-008-0097-9.pdf"
    },
    {
        "title": "An investigation of the relationship between joint visual attention and product quality in collaborative business process modeling: a dual eye-tracking study",
        "submission-date": "2020/11",
        "publication-date": "2022/02",
        "abstract": "Collaborative business process modeling is a collective activity where team members jointly discuss, design, and document business processes. During such activities, team members need to communicate with each other to coordinate the modeling activities, propose and justify changes, and negotiate common terms and deﬁnitions. Throughout this process, stakeholders should be aware of when and what kind of changes have been made by each team member on the shared space so that they can discuss design ideas and build on each other’s work. Joint visual attention has a fundamental role in establishing and maintaining common ground among interlocutors in such cooperative work settings. In addition to this, the co-constructed model’s quality is often considered a key evaluation outcome measure to assess the success of collaboration. However, process and outcome measures of collaboration have been prone to difﬁculties due to challenges in devising measures that can adequately capture the complex dynamics of collaborative work. This study explored the relationship between a popularly used outcome measure in the business process modeling literature and a process measure approximating the level of joint visual attention present among the participants based on the degree of gaze cross-recurrence among the team members over a shared task space. The results suggest that joint visual attention as operationalized in terms of gaze cross-recurrence was a strong predictor of the syntactic, semantic, and pragmatic qualities of collaboratively produced business process models. Moreover, the collaboration process was subjected to qualitative analysis to probe further into the interactional organization of the modeling activity, which identiﬁed communication, coordination, awareness, group decision making, and motivation dimensions as key factors contributing to the quality of collaboration among group members. The results indicated strong relationships between the distribution of quality factors and the degree of gaze cross-recurrence and the ﬁnal models’ syntactic and semantic quality scores. Given the increasing availability of affordable eye trackers and the low resolution, practical nature of the employed analysis methodology, the proposed approach can be fruitfully employed to evaluate team performance and test the effectiveness of software interfaces designed to support collaborative work.",
        "keywords": [
            "Computer-supported collaborative business process modeling",
            "Joint visual attention",
            "Business process model quality",
            "Dual eye tracking",
            "Gaze cross-recurrence"
        ],
        "authors": [
            "Duygu Fındık-Co¸skunçay",
            "Murat Perit Çakır"
        ],
        "file_path": "data/sosym-all/s10270-022-00974-6.pdf"
    },
    {
        "title": "Towards an integrated formal method for veriﬁcation of liveness properties in distributed systems: with application to population protocols",
        "submission-date": "2013/11",
        "publication-date": "2015/12",
        "abstract": "Abstract State-based formal methods [e.g. Event-B/ RODIN (Abrial in Modeling in Event-B—system and soft-ware engineering. Cambridge University Press, Cambridge, 2010; Abrial et al. in Int J Softw Tools Technol Transf (STTT) 12(6):447–466, 2010)] for critical system devel-opment and veriﬁcation are now well established, with track records including tool support and industrial applica-tions. The focus of proof-based veriﬁcation, in particular, is on safety properties. Liveness properties, which guarantee eventual, or converging computations of some requirements, are less well dealt with. Inductive reasoning about liveness is not explicitly supported. Liveness proofs are often com-plex and expensive, requiring high-skill levels on the part of the veriﬁcation engineer. Fairness-based temporal logic approaches have been proposed to address this, e.g. TLA Lamport (ACM Trans Program Lang Syst 16(3):872–923, 1994) and that of Manna and Pnueli (Temporal veriﬁcation of reactive systems—safety. Springer, New York, 1995). We contribute to this technology need by proposing a fairness-based method integrating temporal and ﬁrst-order logic, proof and tools for modelling and veriﬁcation of safety and liveness properties. The method is based on an integration of Event-B and TLA. Building on our previous work (Méry and Poppleton in Integrated formal methods, 10th interna-Communicated by Prof. Einar Broch Johnsen and Luigia Petre.",
        "keywords": [
            "Reﬁnement",
            "Formal method",
            "Distributed sytems",
            "Veriﬁcation",
            "Liveness",
            "Fairness"
        ],
        "authors": [
            "Dominique Méry\nMichael Poppleton"
        ],
        "file_path": "data/sosym-all/s10270-015-0504-y.pdf"
    },
    {
        "title": "Requirement-driven model-based development methodology applied to the design of a real-time MEG data processing unit",
        "submission-date": "2019/05",
        "publication-date": "2020/05",
        "abstract": "The paper describes a multidisciplinary work that uses a model-based systems engineering method for developing real-time magnetoencephalography (MEG) signal processing. We introduce a requirement-driven, model-based development methodology (RDD and MBD) to provide a high-level environment and efficiently handle the complexity of computation and control systems. The proposed development methodology focuses on the use of System Modeling Language to define high-level model-based design descriptions for later implementation in heterogeneous hardware/software systems. The proposed approach was applied to the implementation of a real-time artifact rejection unit in MEG signal processing and demonstrated high efficiency in designing complex high-performance embedded systems. In MEG signal processing, biological artifacts in particular have a signal strength that overtop the signal of interest by orders of magnitude and must be removed from the measurement to achieve high-quality source reconstructions with minimal error contributions. However, many existing brain–computer interface studies overlook real-time artifact removal because of the demanding computational process. In this work, an automated real-time artifact rejection method is introduced, which is based on the recently presented method “ocular and cardiac artifact rejection for real-time analysis in MEG” (OCARTA). The method has been implemented using the RDD and MBD approach and successfully verified on a Virtex-6 field-programmable gate array.",
        "keywords": [
            "MBSE",
            "SysML",
            "Real-time systems",
            "MEG",
            "Artifact rejection",
            "Neurofeedback"
        ],
        "authors": [
            "Tao Chen",
            "Michael Schiek",
            "Jürgen Dammers",
            "N. Jon Shah",
            "Stefan van Waasen"
        ],
        "file_path": "data/sosym-all/s10270-020-00797-3.pdf"
    },
    {
        "title": "MUPPIT: a method for using proper patterns in model transformations",
        "submission-date": "2019/12",
        "publication-date": "2021/01",
        "abstract": "Model transformation plays an important role in developing software systems using the model-driven engineering paradigm. Examples of applications of model transformation include forward engineering, reverse engineering of code into models, and refactoring. Poor-quality model transformation code is costly and hard to maintain. There is a need to develop techniques and tools that can support transformation engineers in designing high-quality model transformations. The goal of this paper is to present a process, called MUPPIT (method for using proper patterns in model transformations), which can be used by transformation engineers to improve the quality of model transformations by detecting anti-patterns in the transformations and automatically applying pattern solutions. MUPPIT consists of four phases: (1) identifying a transformation anti-pattern, (2) proposing a pattern-solution, (3) applying the pattern-solution, and (4) evaluating the transformation model. MUPPIT takes a transformation design model (TDM), which is a representation of the given transformation, to search for the presence of an anti-pattern of interest. If found, MUPPIT proposes a pattern solution from a catalogue of patterns to the transformation engineer. The application of the pattern solution results in the restructuring of the TDM. While MUPPIT, as a process, is independent of any transformation language and transformation engineering framework, we have implemented an instance of it as a tool using transML and MeTAGeM, which support exogenous transformations using rule-based transformation and OCL-based languages such as ATL and ETL. We evaluate MUPPIT through a number of case studies in which we show how MUPPIT can detect four anti-patterns and propose the corresponding pattern solutions. We also evaluate MUPPIT by collecting a number of metrics to assess the quality of the resulting transformations. The results show that MUPPIT optimizes the transformations by improving reusability, modularity, simplicity, and maintainability, as well as decreasing the complexity. MUPPIT can help transformation engineers to produce high-quality transformations using a pattern-based approach. An immediate future direction would be to experiment with more anti-patterns and pattern solutions. Moreover, we need to implement MUPPIT using other transformation engineering frameworks.",
        "keywords": [
            "Transformation pattern",
            "Transformation anti-pattern",
            "Model-driven engineering",
            "Transformation engineering"
        ],
        "authors": [
            "Mahsa Panahandeh",
            "Mohammad Hamdaqa",
            "Bahman Zamani",
            "Abdelwahab Hamou-Lhadj"
        ],
        "file_path": "data/sosym-all/s10270-020-00853-y.pdf"
    },
    {
        "title": "From scenarios to code: An air traﬃc control case study",
        "submission-date": "2003/06",
        "publication-date": "2004/11",
        "abstract": "There has been much recent interest in synthesis algorithms that generate ﬁnite state machines from scenarios of intended system behavior. One of the uses of such algorithms is in the transition from requirements scenarios to design. Despite much theoretical work on the nature of these algorithms, there has been very little work on applying the algorithms to practical applications. In this paper, we apply the Whittle & Schumann synthesis algorithm [32] to a component of an air traﬃc advisory system under development at NASA Ames Research Center. We not only apply the algorithm to generate state machine designs from scenarios but also show how to generate code from the generated state machines using existing commercial code generation tools. The results demonstrate the possibility of generating application code directly from scenarios of system behavior.",
        "keywords": [
            "Code generation",
            "Software modeling",
            "Scenario",
            "State machine",
            "Case study"
        ],
        "authors": [
            "Jon Whittle",
            "Richard Kwan",
            "Jyoti Saboo"
        ],
        "file_path": "data/sosym-all/s10270-004-0067-9.pdf"
    },
    {
        "title": "EDITORIAL",
        "submission-date": "2017/07",
        "publication-date": "2017/08",
        "abstract": "The BPMDS series has produced 11 workshops from 1998 to 2010. Nine of these workshops were held in conjunction with CAiSE conferences. From 2011, BPMDS has become a two-day working conference attached to CAiSE (Conference on Advanced Information Systems Engineering). The topics addressed by the BPMDS series are focused on IT support for business processes. This is one of the keystones of information systems theory. The goals, format, and history of BPMDS can be found on the Web site http://www.bpmds.org/. This special section follows the 16th edition of the BPMDS (business process modeling, development, and support) series, organized in conjunction with CAISE’15, which was held in Stockholm, Sweden, June 2015.",
        "keywords": [],
        "authors": [
            "Selmin Nurcan",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-017-0615-8.pdf"
    },
    {
        "title": "Query-driven soft traceability links for models",
        "submission-date": "2013/06",
        "publication-date": "2014/10",
        "abstract": "Model repositories play a central role in the model driven development of complex software-intensive systems by offering means to persist and manipulate models obtained from heterogeneous languages and tools. Complex models can be assembled by interconnecting model fragments by hard links, i.e., regular references, where the target end points to external resources using storage-speciﬁc identiﬁers. This approach, in certain application scenarios, may prove to be a too rigid and error prone way of interlinking models. As a ﬂexible alternative, we propose to combine derived features with advanced incremental model queries as means for soft interlinking of model elements residing in different model resources. These soft links can be calculated on-demand with graceful handling for temporarily unresolved references. In the background, the links are maintained efﬁciently and ﬂexibly by using incremental model query evaluation. The approach is applicable to modeling environments or even property graphs for representing query results as ﬁrst-class relations, which also allows the chaining of soft links that is useful for modular applications. The approach is evaluated using the Eclipse Modeling Framework (EMF) and EMF- IncQuery in two complex industrial case studies. The ﬁrst case study is motivated by a knowledge management project from the ﬁnancial domain, involving a complex interlinked structure of concept and business process models. The second case study is set in the avionics domain with strict traceability requirements enforced by certiﬁcation standards (DO-178b). It consists of multiple domain models describing the allocation scenario of software functions to hardware components.",
        "keywords": [
            "Soft links",
            "Incremental model queries",
            "Derived features",
            "Traceability"
        ],
        "authors": [
            "Ábel Hegedüs",
            "Ákos Horváth",
            "István Ráth",
            "Rodrigo Rizzi Starr",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-014-0436-y.pdf"
    },
    {
        "title": "Modeling cultures of the embedded software industry: feedback from the field",
        "submission-date": "2019/01",
        "publication-date": "2020/06",
        "abstract": "Engineering of modern embedded systems requires complex technical, managerial and operational processes. To cope with the complexity, modeling is a commonly used approach in the embedded software industry. The modeling approaches in embedded software vary since the characteristics of modeling such as purpose, medium type and life cycle phase differ among systems and industrial sectors. The objective of this paper is to detail the use of a characterization model MAPforES (“Modeling Approach Patterns for Embedded Software”). This paper presents the results of applying MAPforES in multiple case studies. The applications are performed in three sectors of the embedded software industry: defense and aerospace, automotive and transportation, and consumer electronics. A series of both structured and semi-structured interviews with 35 embedded software professionals were conducted as part of the case studies. The characterization model was successfully applied to these cases. The results show that identifying individual patterns provides insight for improving both individual behavior and the behavior of projects and organizations.",
        "keywords": [
            "Software modeling",
            "Embedded software",
            "Modeling patterns and cultures",
            "Characterization model",
            "Case study"
        ],
        "authors": [
            "Deniz Akdur",
            "Bilge Say",
            "Onur Demirörs"
        ],
        "file_path": "data/sosym-all/s10270-020-00810-9.pdf"
    },
    {
        "title": "The 2015 “State of the Journal” report",
        "submission-date": "2016/01",
        "publication-date": "2016/01",
        "abstract": "With the inception of SoSyM in 2001, we will this year celebrate its 15th year anniversary! Over the past 14 volumes, SoSyM has published a total of 512 different articles and editorials. The journal is doing very well and recently received an Impact Factor of 1.408. In 2015, SoSyM published 79 articles and editorials. There were 198 submissions to SoSyM during the 2015 calendar year. The acceptance rate over the past 12months has been 21.8%.",
        "keywords": [],
        "authors": [
            "Geri Georg",
            "Jeff Gray",
            "Bernhard Rumpe",
            "Martin Schindler"
        ],
        "file_path": "data/sosym-all/s10270-016-0515-3.pdf"
    },
    {
        "title": "A logical approach to systems engineering artifacts: semantic relationships and dependencies beyond traceability—from requirements to functional and architectural views",
        "submission-date": "2016/03",
        "publication-date": "2017/09",
        "abstract": "Not only system assurance drives a need for semantically richer relationships across various artifacts, work products, and items of information than are implied in the terms “trace and traceability” as used in current standards and textbooks. This paper deals with the task of work-ing out artifacts in software and system development, their representation, and the analysis and documentation of the relationships between their logical contents—herein referred to as tracing and traceability; this is a richer meaning of traceability than in standards like IEEE STD 830. Among others, key tasks in system development are as follows: capturing, analyzing, and documenting system-level requirements, the step to functional system speciﬁcations, the step to architectures given by the decomposition of systems into subsystems with their connections and behavioral interac-tions. Each of these steps produces artifacts for documenting the development, as a basis for a speciﬁcation and a design rationale, for documentation, for veriﬁcation, and impact analysis of change requests. Crucial questions are how to rep-resent and formalize the content of these artifacts and how to relate their content to support, in particular, system assur-ance. When designing multi-functional systems, key artifacts are system-level requirements, functional speciﬁcations, and architecturesintermsoftheirsubsystemspeciﬁcations.Links and traces between these artifacts are introduced to relate their contents. Traceability has the goal to relate artifacts. It is required for instance in standards for functional system safety such as the ISO 26262. An approach to specifying semantic relationships is shown, such that the activity of cre-ating and using (navigating through) these relationships can be supported with automation.",
        "keywords": [
            "Speciﬁcation",
            "Architecture",
            "Trace",
            "Traceability",
            "Reﬁnement",
            "Semantic dependencies"
        ],
        "authors": [
            "Manfred Broy"
        ],
        "file_path": "data/sosym-all/s10270-017-0619-4.pdf"
    },
    {
        "title": "An approach for reverse engineering of design patterns",
        "submission-date": "2002/12",
        "publication-date": "2004/04",
        "abstract": "For the maintenance of software systems, developers have to completely understand the existing system. The usage of design patterns leads to beneﬁts for new and young developers by enabling them to reuse the knowledge of their experienced colleagues. Design patterns can support a faster and better understanding of software systems. There are diﬀerent approaches for supporting pattern recognition in existing systems by tools. They are evaluated by the Information Retrieval criteria precision and recall. An automated search based on structures has a highly positive inﬂuence on the manual validation of the results by developers. This validation of graphical structures is the most intuitive technique. In this paper a new approach for automated pattern search based on minimal key structures is presented. It is able to detect all patterns described by the GOF [15]. This approach is based on positive and negative search criteria for structures and is prototypically implemented using Rational Rose and Together.",
        "keywords": [
            "Design patterns",
            "Reverse engineering",
            "Pattern recognition"
        ],
        "authors": [
            "Ilka Philippow",
            "Detlef Streitferdt",
            "Matthias Riebisch",
            "Sebastian Naumann"
        ],
        "file_path": "data/sosym-all/s10270-004-0059-9.pdf"
    },
    {
        "title": "Model driven design and aspect weaving",
        "submission-date": "2007/12",
        "publication-date": "2008/02",
        "abstract": "Amodelisasimpliﬁedrepresentationofanaspect\nof the world for a speciﬁc purpose. In complex systems,\nmany aspects are to be handled, from architectural aspects to\ndynamic behavior, functionalities, user-interface, and extra-\nfunctional concerns (such as security, reliability, timeliness,\netc.). For software systems, the design process can then be\ncharacterized as the weaving of all these aspects into a detai-\nled design model. Model Driven Design aims at automating\nthis weaving process, that is automatically deriving software\nsystems from theirs models. This paper explores the rela-\ntionship between modeling and aspect weaving. It points out\nsomeofthechallengesrelatedtosuchautomaticmodelweav-\ning, illustrating them with the example of a weaving process\nfor behavioral models represented as scenarios.",
        "keywords": [],
        "authors": [
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-008-0080-5.pdf"
    },
    {
        "title": "SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines",
        "submission-date": "2016/12",
        "publication-date": "2017/07",
        "abstract": "A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.",
        "keywords": [
            "Software product lines",
            "Search-based software engineering",
            "Multi-objective evolutionary algorithms",
            "Constraint solving",
            "Feature models"
        ],
        "authors": [
            "Jianmei Guo",
            "Jia Hui Liang",
            "Kai Shi",
            "Dingyu Yang",
            "Jingsong Zhang",
            "Krzysztof Czarnecki",
            "Vijay Ganesh",
            "Huiqun Yu"
        ],
        "file_path": "data/sosym-all/s10270-017-0610-0.pdf"
    },
    {
        "title": "Effects of stability on model composition effort: an exploratory study",
        "submission-date": "2011/07",
        "publication-date": "2013/01",
        "abstract": "Model composition plays a central role in many software engineering activities, e.g., evolving design models to add new features. To support these activities, developers usually rely on model composition heuristics. The problem is that the models to-be-composed usually conﬂict with each other in several ways and such composition heuristics might be unable to properly deal with all emerging conﬂicts. Hence, the composed model may bear some syntactic and semantic inconsistencies that should be resolved. As a result, the production of the intended model is an error-prone and effort-consuming task. It is often the case that developers end up examining all parts of the output composed model instead of prioritizing the most critical ones, i.e., those that are likely to be inconsistent with the intended model. Unfortunately, little is known about indicators that help developers (1) to identify which model is more likely to exhibit inconsisten-cies, and (2) to understand which composed models require more effort to be invested. It is often claimed that software systems remaining stable over time tends to have a lower number of defects and require less effort to be ﬁxed than unstable systems. However, little is known about the effects of software stability in the context of model evolution when supported by composition heuristics. This paper, therefore, presents an exploratory study analyzing stability as an indi-cator of inconsistency rate and resolution effort on model composition activities. Our ﬁndings are derived from 180 compositions performed to evolve design models of three software product lines. Our initial results, supported by statistical tests, also indicate which types of changes led to lower inconsistency rate and lower resolution effort.",
        "keywords": [
            "Model composition",
            "Software development effort",
            "Design stability"
        ],
        "authors": [
            "Kleinner Farias",
            "Alessandro Garcia",
            "Carlos Lucena"
        ],
        "file_path": "data/sosym-all/s10270-012-0308-2.pdf"
    },
    {
        "title": "Model-driven analysis and synthesis of textual concrete syntax",
        "submission-date": "2007/03",
        "publication-date": "2008/04",
        "abstract": "Meta-modeling is raising more and more interest in the ﬁeld of language engineering. While this approach is now well understood for deﬁning abstract syntaxes, formally deﬁning textual concrete syntaxes with meta-models is still a challenge. Textual concrete syntaxes are traditionally expressed with rules, conforming to EBNF-like grammars, which can be processed by compiler compilers to generate parsers. Unfortunately, these generated parsers produce concrete syntax trees, leaving a gap with the abstract syntax deﬁned by meta-models, and further ad hoc hand-coding is required. In this paper we propose a new kind of speciﬁcation for concrete syntaxes, which takes advantage of meta-models to generate fully operational tools (such as parsers or text generators). The principle is to map abstract syntaxes to textual concrete syntaxes via bidirectional mapping-models with support for both model-to-text, and text-to-model transformations.",
        "keywords": [
            "MDD",
            "MDE",
            "Language engineering",
            "Meta-modeling"
        ],
        "authors": [
            "Pierre-Alain Muller",
            "Frédéric Fondement",
            "Franck Fleurey",
            "Michel Hassenforder",
            "Rémi Schnekenburger",
            "Sébastien Gérard",
            "Jean-Marc Jézéquel"
        ],
        "file_path": "data/sosym-all/s10270-008-0088-x.pdf"
    },
    {
        "title": "Comparing and classifying model transformation reuse approaches across metamodels",
        "submission-date": "2018/12",
        "publication-date": "2019/11",
        "abstract": "Model transformations are essential elements of model-driven engineering (MDE) solutions, as they enable the automatic manipulation of models. MDE promotes the creation of domain-speciﬁc metamodels, but without proper reuse mechanisms, model transformations need to be developed from scratch for each new metamodel. In this paper, our goal is to understand whether transformation reuse across metamodels is needed by the community, evaluate its current state, identify practical needs and propose promising lines for further research. For this purpose, we ﬁrst report on a survey to understand the reuse approaches used currently in practice and the needs of the community. Then, we propose a classiﬁcation of reuse techniques based on a feature model and compare a sample of speciﬁc approaches—model types, concepts, a-posteriori typing, multilevel modeling, typing requirement models, facet-oriented modeling, mapping operators, constraint-based model types, and design patterns for model transformations—based on this feature model and a common example. We discuss strengths and weaknesses of each approach, provide a reading grid used to compare their features, compare with community needs, identify gaps in current transformation reuse approaches in relation to these needs and propose future research directions.",
        "keywords": [
            "Model transformation",
            "Reuse",
            "Survey",
            "Classiﬁcation",
            "Feature model"
        ],
        "authors": [
            "Jean-Michel Bruel",
            "Benoit Combemale",
            "Esther Guerra",
            "Jean-Marc Jézéquel",
            "Jörg Kienzle",
            "Juan de Lara",
            "Gunter Mussbacher",
            "Eugene Syriani",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-019-00762-9.pdf"
    },
    {
        "title": "Model-driven development platform selection: four industry case studies",
        "submission-date": "2020/04",
        "publication-date": "2021/01",
        "abstract": "Model-driven development platforms shift the focus of software development activity from coding to modeling for enterprises. A signiﬁcant number of such platforms are available in the market. Selecting the best ﬁtting platform is challenging, as domain experts are not typically model-driven deployment platform experts and have limited time for acquiring the needed knowledge. We model the problem as a multi-criteria decision-making problem and capture knowledge systematically about the features and qualities of 30 alternative platforms. Through four industry case studies, we conﬁrm that the model supports decision-makers with the selection problem by reducing the time and cost of the decision-making process and by providing a richer list of options than the enterprises considered initially. We show that having decision knowledge readily available supports decision-makers in making more rational, efﬁcient, and effective decisions. The study’s theoretical contribution is the observation that the decision framework provides a reliable approach for creating decision models in software production.",
        "keywords": [
            "Model-driven development platform",
            "Decision model",
            "Multi-criteria decision-making",
            "Decision support system",
            "Industry case study"
        ],
        "authors": [
            "Siamak Farshidi",
            "Slinger Jansen",
            "Sven Fortuin"
        ],
        "file_path": "data/sosym-all/s10270-020-00855-w.pdf"
    },
    {
        "title": "Querying process models by behavior inclusion",
        "submission-date": "2012/09",
        "publication-date": "2013/12",
        "abstract": "Business processes are vital to managing organizations as they sustain a company’s competitiveness. Consequently, these organizations maintain collections of hundreds or thousands of process models for streamlining working procedures and facilitating process implementation. Yet, the management of large process model collections requires effective searching capabilities. Recent research focused on similarity search of process models, but querying process models is still a largely open topic. This article presents an approach to querying process models that takes a process example as input and discovers all models that allow replaying the behavior of the query. To this end, we provide a notion of behavioral inclusion that is based on trace semantics and abstraction. Additional to deciding a match, a closeness score is provided that describes how well the behavior of the query is represented in the model and can be used for ranking. The article introduces the formal foundations of the approach and shows how they are applied to querying large process model collections. An experimental evaluation has been conducted that confirms the suitability of the solution as well as its applicability and scalability in practice.",
        "keywords": [
            "Process model search",
            "Behavioral querying",
            "Trace inclusion",
            "Process model repositories"
        ],
        "authors": [
            "Matthias Kunze",
            "Matthias Weidlich",
            "Mathias Weske"
        ],
        "file_path": "data/sosym-all/s10270-013-0389-6.pdf"
    },
    {
        "title": "Managing time-awareness in modularized processes",
        "submission-date": "2016/12",
        "publication-date": "2018/02",
        "abstract": "Managing temporal process constraints in a suitable way is crucial for long-running business processes in many application domains. However, proper support of time-aware processes is still missing in contemporary information systems. This paper tackles a particular challenge existing in this context, namely the handling of temporal constraints for modularized processes (i.e., processes comprising subprocesses), which shall enable both the reuse of process knowledge and the modular design of complex processes. In detail, this paper focuses on the representation and support of time-aware modularized processes in process-aware information systems. To this end, we present a sound and complete method to derive the duration restrictions of a time-aware (sub-)process in such a way that its temporal properties are completely speciﬁed. We then show how this characterization of a process can be utilized when reusing it as a subprocess within a modularized process. As a motivating example, we consider a compound process from healthcare. Altogether the proper handling of temporal constraints for modularized processes is crucial for the enhancement of time- and process-aware information systems.",
        "keywords": [
            "Process-aware information system",
            "Temporal constraints",
            "Subprocess",
            "Process modularity",
            "Controllability"
        ],
        "authors": [
            "Roberto Posenato",
            "Andreas Lanz",
            "Carlo Combi",
            "Manfred Reichert"
        ],
        "file_path": "data/sosym-all/s10270-017-0643-4.pdf"
    },
    {
        "title": "A knowledge-based approach for guided development of Infrastructure as Code",
        "submission-date": "2024/01",
        "publication-date": "2025/04",
        "abstract": "Infrastructure as Code (IaC) uses versionable software code to deﬁne, deploy, and conﬁgure physical computational resources, software execution platforms, and applications. As a result, IaC enables the scalable management of complex computing environments while preventing environment drift. IaC frameworks typically offer speciﬁc languages such as the industrial Terraform, Ansible, Chef, or TOSCA—standing for Topology and Orchestration Speciﬁcation for Cloud Applications—the OASIS (Organization for the Advancement of Structured Information Standards) open standard approach to IaC. Developing high-quality IaC for deploying and managing applications demands expertise and knowledge in speciﬁc IaC languages, infrastructure resources, resource providers, quality issues in IaC scripts, and so on. While several model-driven engineering (MDE) approaches have been proposed to simplify IaC development, they cannot capture and use expert knowledge to assist with modeling tasks and MDE processes by providing interactive recommendations. This paper presents a knowledge-based framework for guiding the model-driven development of IaC. We use TOSCA as the target IaC language as it is an open standard. We enable IaC and resource experts to share their IaC and resource-related knowledge with application operational experts to help simplify the development of application deployment models. We use an ontology to record the relevant deployment knowledge and ontology reasoning to implement modeling guidance capabilities such as TOSCA model auto-completion, code smell and error detection, and model element matchmaking. We show the ﬂexibility of our methodology by applying it to three industrial applications, covering cloud, edge, and HPC (High-Performance Computing) domains. Moreover, we also assess the use acceptance of our approach and framework by conducting controlled experiments with expert and non-expert IaC users. The results indicate that our method can simplify IaC development by providing appropriate recommendations.",
        "keywords": [
            "Model-driven engineering",
            "IaC",
            "Recommendation system",
            "TOSCA",
            "Ontology",
            "Knowledge graph",
            "Semantic web"
        ],
        "authors": [
            "Zoe Vasileiou",
            "Indika Kumara",
            "Georgios Meditskos",
            "Kamil Tokmakov",
            "Dragan Radolovi´c",
            "Jesús Gorroñogoitia Cruz",
            "Elisabetta Di Nitto",
            "Damian Andrew Tamburri",
            "Willem-Jan Van Den Heuvel",
            "Stefanos Vrochidis"
        ],
        "file_path": "data/sosym-all/s10270-025-01294-1.pdf"
    },
    {
        "title": "Wodel-Test: a model-based framework for language-independent mutation testing",
        "submission-date": "2019/12",
        "publication-date": "2020/10",
        "abstract": "Mutation testing (MT) targets the assessment of test cases by measuring their efﬁciency to detect faults. This technique involves modifying the program under test to emulate programming faults, and assessing whether the existing test cases detect such mutations. MT has been extensively studied since the 70’s, and many tools have been proposed for widely used languages like C, Java, Fortran, Ada and SQL; and for notations like Petri-nets. However, building MT tools is costly and error-prone, which may prevent their development for new programming and domain-speciﬁc (modelling) languages. In this paper, we propose a framework called Wodel- Test to reduce the effort to create MT tools. For this purpose, it follows a model-driven approach by which MT tools are synthesized from a high-level description. This description makes use of the domain-speciﬁc language Wodel to deﬁne and execute model mutations. Wodel is language-independent, as it allows the creation of mutation operators for any language deﬁned by a meta-model. Starting from the deﬁnition of the mutation operators, Wodel- Test generates a MT environment which parses the program under test into a model, applies the mutation operators, and evaluates the test-suite against the generated mutants, offering a rich collection of MT metrics. We report on an evaluation of the approach based on the creation of MT tools for Java and the Atlas transformation language.",
        "keywords": [
            "Mutation testing",
            "Model mutation",
            "Model-driven engineering",
            "Domain-speciﬁc languages",
            "Java",
            "Model transformation"
        ],
        "authors": [
            "Pablo Gómez-Abajo\nEsther Guerra\nJuan de Lara\nMercedes G. Merayo"
        ],
        "file_path": "data/sosym-all/s10270-020-00827-0.pdf"
    },
    {
        "title": "Detection and quantiﬁcation of ﬂow consistency in business process models",
        "submission-date": "2015/10",
        "publication-date": "2017/01",
        "abstract": "Business process models abstract complex business processes by representing them as graphical models. Their layout, as determined by the modeler, may have an effect when these models are used. However, this effect is currently not fully understood. In order to systematically study this effect, a basic set of measurable key visual features is proposed, depicting the layout properties that are meaningful to the human user. The aim of this research is thus twofold: first, to empirically identify key visual features of business process models which are perceived as meaningful to the user and second, to show how such features can be quantiﬁed into computational metrics, which are applicable to business process models. We focus on one particular feature, consistency of ﬂow direction, and show the challenges that arise when transforming it into a precise metric. We propose three different metrics addressing these challenges, each following a different view of ﬂow consistency. We then report the results of an empirical evaluation, which indicates which metric is more effective in predicting the human perception of this feature. Moreover, two other automatic evaluations describing the performance and the computational capabilities of our metrics are reported as well.",
        "keywords": [
            "Business process modeling",
            "Metrics",
            "Visual layout",
            "Qualitative empirical study",
            "Consistency of ﬂow"
        ],
        "authors": [
            "Andrea Burattin",
            "Vered Bernstein",
            "Manuel Neurauter",
            "Pnina Soffer",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-017-0576-y.pdf"
    },
    {
        "title": "Foundations of information technology based on Bunge’s systemist philosophy of reality",
        "submission-date": "2020/08",
        "publication-date": "2021/01",
        "abstract": "General ontology is a prominent theoretical foundation for information technology analysis, design, and development. Ontology is a branch of philosophy which studies what exists in reality. A widely used ontology in information systems, especially for conceptual modeling, is the BWW (Bunge–Wand–Weber), which is based on ideas of the philosopher and physicist Mario Bunge, as synthesized by Wand and Weber. The ontology was founded on an early subset of Bunge’s philosophy; however, many of Bunge’s ideas have evolved since then. An important question, therefore, is: do the more recent ideas expressed by Bunge call for a new ontology? In this paper, we conduct an analysis of Bunge’s earlier and more recent works to address this question. We present a new ontology based on Bunge’s later and broader works, which we refer to as Bunge’s Systemist Ontology (BSO). We then compare BSO to the constructs of BWW. The comparison reveals both considerable overlap between BSO and BWW, as well as substantial differences. From this comparison and the initial exposition of BSO, we provide suggestions for further ontology studies and identify research questions that could provide a fruitful agenda for future scholarship in conceptual modeling and other areas of information technology.",
        "keywords": [
            "Ontology",
            "Upper-level ontology",
            "General ontology",
            "Mario Bunge",
            "Bunge",
            "Wand",
            "Weber ontology",
            "Bunge’s Systemist Ontology",
            "Conceptual modeling",
            "Software engineering",
            "Database design",
            "IT development",
            "IT design",
            "Real-world domains",
            "Reality",
            "Philosophy"
        ],
        "authors": [
            "Roman Lukyanenko",
            "Veda C. Storey",
            "Oscar Pastor"
        ],
        "file_path": "data/sosym-all/s10270-021-00862-5.pdf"
    },
    {
        "title": "A platform independent model for the electronic marketplace domain",
        "submission-date": "2005/07",
        "publication-date": "2007/04",
        "abstract": "An electronic marketplace supports interactions between multiple users for the exchange of information on products for sale or purchase. The signiﬁcance of electronic marketplaces is apparent from the huge number of websites that currently provide services in almost any area one can think of. However, the absence of clear documentation on the similarities that these sites share restricts the reutilization of software for the development of new electronic marketplaces. To improve this situation, we propose a platform independent model (PIM) for the e-marketplace domain that describes both the structural and behavioral properties of a generic electronic marketplace. Speciﬁc application PIMs aimed at generating different e-marketplaces can be obtained from our generic domain PIM by adapting it to the requirements of each particular application. In this way, reutilization of our domain PIM contributes to a reduction in the cost and time involved in the development of new electronic marketplaces.",
        "keywords": [
            "Platform independent model",
            "Model driven architecture",
            "Uniﬁed modeling language",
            "Domain analysis",
            "Domain model",
            "E-marketplaces",
            "Reference models"
        ],
        "authors": [
            "Anna Queralt",
            "Ernest Teniente"
        ],
        "file_path": "data/sosym-all/s10270-007-0047-y.pdf"
    },
    {
        "title": "Expressing aspectual interactions in design: evaluating three AOM approaches in the slot machine domain",
        "submission-date": "2014/01",
        "publication-date": "2014/12",
        "abstract": "In the context of an industrial project, we evaluated the implementation of the software of a casino slot machine. This software has a significant amount of cross-cutting concerns that depend on and interact with each other as well as with the modular concerns. We therefore wished to express our design using an appropriate aspect-oriented modeling approach. We therefore evaluated three candidate methodologies: Theme/UML, WEAVR, and RAM to establish their suitability. Remarkably, only the last of the three has shown to allow an adequate expression of the interactions, albeit not fully explicit. The first two fall short because half of the interaction types cannot be expressed at all while the other half need to be expressed using a work-around that hides the intention of the design. Neither does RAM allow a fully explicit expression of interactions, but it would be the most adequate approach for the slot machine case.",
        "keywords": [
            "Aspect-oriented modeling",
            "Aspect interactions",
            "Case study"
        ],
        "authors": [
            "Johan Fabry",
            "Arturo Zambrano",
            "Silvia Gordillo"
        ],
        "file_path": "data/sosym-all/s10270-014-0442-0.pdf"
    },
    {
        "title": "Extracting models from source code in software modernization",
        "submission-date": "2011/03",
        "publication-date": "2012/09",
        "abstract": "Model-driven software modernization is a discipline in which model-driven development (MDD) techniques are used in the modernization of legacy systems. When existing software artifacts are evolved, they must be transformed into models to apply MDD techniques such as model transformations. Since most modernization scenarios (e.g., application migration) involve dealing with code in general-purpose programming languages (GPL), the extraction of models from GPL code is an essential task in a model-based modernization process. This activity could be performed by tools to bridge grammarware and MDD technical spaces, which is normally carried out by dedicated parsers. Grammar-to-Model Transformation Language (Gra2MoL) is a domain-speciﬁc language (DSL) tailored to the extraction of models from GPL code. This DSL is actually a text-to-model transformation language which can be applied to any code conforming to a grammar. Gra2MoL aims to reduce the effort needed to implement grammarware-MDD bridges, since building dedicated parsers is a complex and time-consuming task. Like ATL and RubyTL languages, Gra2MoL incorporates the binding concept needed to write mappings between grammar elements and metamodel elements in a simple declarative style. The language also provides a powerful query language which eases the retrieval of scattered information in syntax trees. Moreover, it incorporates extensibility and grammar reuse mechanisms. This paper describes Gra2MoL in detail and includes a case study based on the application of the language in the extraction of models from Delphi code.",
        "keywords": [
            "Model-driven engineering",
            "Model-driven software development",
            "Domain-speciﬁc languages",
            "Software modernization",
            "Model-driven software modernization"
        ],
        "authors": [
            "Javier Luis Cánovas Izquierdo",
            "Jesús García Molina"
        ],
        "file_path": "data/sosym-all/s10270-012-0270-z.pdf"
    },
    {
        "title": "Dinosaur meets Archaeopteryx? or: Is there an alternative for Rational’s Uniﬁed Process?",
        "submission-date": "2003/05",
        "publication-date": "2003/09",
        "abstract": "Since 1999, Rational’s Uniﬁed Process (RUP) is being oﬀered as a guideline for software projects using the Uniﬁed Modeling Language (UML). RUP has been advertised to be iterative, and incremental, use case-driven and architecture-centric. These claims are discussed while RUP core concepts like phase, iteration, discipline (formerly: workﬂow) and milestone are reviewed in more detail. It turns out that the RUP constitutes a considerable step towards a broad dissemination of software process modelling ideas but some of the RUP def-initions and structures lack clear structure and are too complex and overloaded for practical use. Among others, I see the following particular prob-lems: (1) phases do still dominate the process and iteration structure, (2) the term “software architecture” is not clearly deﬁned and its role is still underestimated, (3) RUP “disciplines” are a partly redundant concept complicating the process more than supporting it, (4) powerful and transparent structuring principles like recursion and orthogonality do not get the attention they deserve. As an alternative, our model for Evolutionary, Object-oriented Softwaredevelopment (EOS) is contrasted with the RUP.",
        "keywords": [
            "Software process modeling",
            "Rational Uniﬁed Process (RUP)",
            "Evolutionary software development (EOS model)",
            "Architecture-centric fractal software process",
            "Project management support"
        ],
        "authors": [
            "Wolfgang Hesse"
        ],
        "file_path": "data/sosym-all/s10270-003-0033-y.pdf"
    },
    {
        "title": "Theoretical foundations and implementation of business process diagrams’ complexity management technique based on highlights",
        "submission-date": "2016/11",
        "publication-date": "2017/08",
        "abstract": "The main purpose of business process diagrams is to make the communication between process-related stakeholders more effective. To this end, they need to be simple to read, which is often challenging to achieve. In this manner, the complexity of business process diagrams can negatively affect their correctness and understandability. The goal of this paper was to investigate an approach that makes business process diagrams appear less complex, without changing the corresponding notation. This was done by manipulating one of the properties of the notation’s elements, namely opacity. Firstly, a literature overview was performed in order to obtain the theoretical foundations. Secondly, an exploratory case study was conducted and the results were applied in practice. Finally, the proposed solution was implemented in the form of a prototype software solution. Our analysis demonstrated that the structural complexity of the diagrams decreases when applying the proposed solution.",
        "keywords": [
            "Business process diagram",
            "Complexity",
            "Highlights",
            "Opacity",
            "BPMN"
        ],
        "authors": [
            "Gregor Jošt",
            "Marjan Heriˇcko",
            "Gregor Polanˇciˇc"
        ],
        "file_path": "data/sosym-all/s10270-017-0618-5.pdf"
    },
    {
        "title": "Span(Graph): a canonical feedback algebra of open transition systems",
        "submission-date": "2022/03",
        "publication-date": "2023/03",
        "abstract": "We show that Span(Graph)∗, an algebra for open transition systems introduced by Katis, Sabadini and Walters, satisfies\na universal property. By itself, this is a justiﬁcation of the canonicity of this model of concurrency. However, the universal\nproperty is itself of interest, being a formal demonstration of the relationship between feedback and state. Indeed, feedback\ncategories, also originally proposed by Katis, Sabadini and Walters, are a weakening of traced monoidal categories, with\nvarious applications in computer science. A state bootstrapping technique, which has appeared in several different contexts,\nyields free such categories. We show that Span(Graph)∗arises in this way, being the free feedback category over Span(Set).\nGiven that the latter can be seen as an algebra of predicates, the algebra of open transition systems thus arises—roughly\nspeaking—as the result of bootstrapping state to that algebra. Finally, we generalize feedback categories endowing state\nspaces with extra structure: this extends the framework from mere transition systems to automata with initial and ﬁnal states.",
        "keywords": [],
        "authors": [
            "Elena Di Lavore",
            "Alessandro Gianola",
            "Mario Román",
            "Nicoletta Sabadini",
            "Paweł Soboci´nski"
        ],
        "file_path": "data/sosym-all/s10270-023-01092-7.pdf"
    },
    {
        "title": "Automated testing of metamodels and code co-evolution",
        "submission-date": "2023/08",
        "publication-date": "2024/12",
        "abstract": "Metamodels are cornerstone in MDE. They deﬁne the different domain concepts and the relations between them. A metamodel\nis also used to generate concrete artifacts such as code. Developers then rely on the generated code to build their language\nservices and tooling, e.g., editors, checkers. To check the behavior of their client code, developers write or generate unit\ntests. As metamodels evolve between releases, the generated code is automatically updated. As a consequence, the additional\ndevelopers’ code is impacted and is co-evolved accordingly for each release. However, there is no guarantee that the co-\nevolution of the code is performed correctly. One way to do so is to rerun all the tests after each code co-evolution, which\nis expensive and time-consuming. This paper proposes an automatic solution for tracing impacted tests due to metamodel\nevolution. Thus, we end up matching metamodel changes with impacted code methods and their corresponding tests in both\nthe original and evolved versions of a given project. After that, we map the two versions of the impacted tests and compare\nthem to analyze the behavior of the code before and after its evolution due to the metamodel evolution. In particular, we\nimplemented an Eclipse plug-in that allows tracing, mapping, execution, and reporting back the results to the developers for\neasier in-depth analysis of the effect of metamodel evolutions rather than analyzing the whole test suite. We ﬁrst ran a user\nstudy experiment to gain evidence on the difﬁculty or not of the manual task of tracing impacted tests. We found that manually\ntracing the tests impacted by the evolution of the metamodel is a hard and error-prone task. Not only the participants could\nnot trace all tests, but they even wrongly traced non-impacted tests. We then evaluated our approach on 18 Eclipse projects\nfrom OCL, Modisco, Papyrus, and EMF over several evolved versions of metamodels. For the 14 projects without manual\ntests, we generated a test suite for each release with the state-of-the-art tool EvoSuite. The results show that we successfully\ntraced the impacted tests automatically by selecting 1608 out of 34,612 tests due to 473 metamodel changes. When running\nthe traced tests before and after co-evolution, we observed cases indicating possibly both behaviorally correct and incorrect\ncode co-evolution. Finally, we reached gains representing, on average, a reduction of 88% in the number of tests and 84% in\nthe execution time.",
        "keywords": [
            "Metamodel evolution",
            "Code co-evolution",
            "Unit tests",
            "Testing co-evolution"
        ],
        "authors": [
            "Zohra Kaouter Kebaili",
            "Djamel Eddine Khelladi",
            "Mathieu Acher",
            "Olivier Barais"
        ],
        "file_path": "data/sosym-all/s10270-024-01245-2.pdf"
    },
    {
        "title": "Execution of UML models: a systematic review of research and practice",
        "submission-date": "2017/07",
        "publication-date": "2018/04",
        "abstract": "Several research efforts from different areas have focused on the execution of UML models, resulting in a diverse and complex scientiﬁc body of knowledge. With this work, we aim at identifying, classifying, and evaluating existing solutions for the execution of UML models. We conducted a systematic review in which we selected 63 research studies and 19 tools among over 5400 entries by applying a systematic search and selection process. We deﬁned a classiﬁcation framework for characterizing solutions for UML model execution, and we applied it to the 82 selected entries. Finally, we analyzed and discussed the obtained data. From the analyzed data, we drew the following conclusions: (i) There is a growing scientiﬁc interest on UML model execution; (ii) solutions providing translational execution clearly outnumber interpretive solutions; (iii) model-level debugging is supported in very few cases; (iv) only a few research studies provide evidence of industrial use, with very limited empirical evaluations; (v) the most common limitation deals with coverage of the UML language. Based on these observations, we discuss potential research challenges and implications for the future of UML model execution. Our results provide a concise overview of states of the art and practice for UML model execution intended for use by both researchers and practitioners.",
        "keywords": [
            "UML",
            "Model execution",
            "Code generation",
            "Model compilation",
            "Model interpretation",
            "Systematic review"
        ],
        "authors": [
            "Federico Ciccozzi",
            "Ivano Malavolta",
            "Bran Selic"
        ],
        "file_path": "data/sosym-all/s10270-018-0675-4.pdf"
    },
    {
        "title": "An efﬁcient and scalable search engine for models",
        "submission-date": "2021/02",
        "publication-date": "2021/12",
        "abstract": "Search engines extract data from relevant sources and make them available to users via queries. A search engine typically crawls the web to gather data, analyses and indexes it and provides some query mechanism to obtain ranked results. There exist search engines for websites, images, code, etc., but the speciﬁc properties required to build a search engine for models have not been explored much. In the previous work, we presented MAR, a search engine for models which has been designed to support a query-by-example mechanism with fast response times and improved precision over simple text search engines. The goal of MAR is to assist developers in the task of ﬁnding relevant models. In this paper, we report new developments of MAR which are aimed at making it a useful and stable resource for the community. We present the crawling and analysis architecture with which we have processed about 600,000 models. The indexing process is now incremental and a new index for keyword-based search has been added. We have also added a web user interface intended to facilitate writing queries and exploring the results. Finally, we have evaluated the indexing times, the response time and search precision using different conﬁgurations. MAR has currently indexed over 500,000 valid models of different kinds, including Ecore meta-models, BPMN diagrams, UML models and Petri nets. MAR is available at http://mar-search.org.",
        "keywords": [
            "Model repositories",
            "Search engines",
            "Model-driven engineering"
        ],
        "authors": [
            "José Antonio Hernández López",
            "Jesús Sánchez Cuadrado"
        ],
        "file_path": "data/sosym-all/s10270-021-00960-4.pdf"
    },
    {
        "title": "Models for digitalization",
        "submission-date": "2015/09",
        "publication-date": "2015/09",
        "abstract": "To continue the tradition of SoSyM editorials by high-lighting speciﬁc topics in modeling research, we examine the emerging trend of “digitalization,” which represents the integration of multiple technologies into all aspects of daily life that can be digitized. A few examples of digitalization include smart homes (for entertainment, security, childcare, electrical, and heating), e-healthcare, smart mobility, and smart cities. The widespread impact of digitalization affects everything from personal relationships augmented by social media and their services, to other relationships such as how citizens interact with support services in e-government. Gartner deﬁnes digitalization with a more business-oriented focus: “Digitalization is the use of digital technologies to change a business model and provide new revenue and value-producing opportunities; it is the process of mov-ing to a digital business.” (gartner.com/it-glossary in August 2015). This deﬁnition spans relationships between different businesses, in addition to business and government, and the vital relationship to customers. A goal is to realize digitalizationsuchthatthereisaclearrelationshipbetweentheservices offered by businesses and the actual needs of customers.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-015-0494-9.pdf"
    },
    {
        "title": "Clarifying matters of (meta-) modeling: an author’s reply",
        "submission-date": "2006/09",
        "publication-date": "2006/10",
        "abstract": "Not found",
        "keywords": [],
        "authors": [
            "Thomas Kühne"
        ],
        "file_path": "data/sosym-all/s10270-006-0034-8.pdf"
    },
    {
        "title": "Property-based testing of web services by deriving properties from business-rule models",
        "submission-date": "2016/09",
        "publication-date": "2017/12",
        "abstract": "Property-based testing is well suited for web-service applications, which was already shown in various case studies. For example, it has been demonstrated that JSON schemas can be used to automatically derive test case generators for web forms. In this work, we present a test case generation approach for a rule engine-driven web-service application. Business-rule models serve us as input for property-based testing. We parse these models to automatically derive generators for sequences of web-service requests together with their required form data. Property-based testing is mostly applied in the context of functional programming. Here, we deﬁne our properties in an object-oriented style in C# and its tool FsCheck. We apply our method to the business-rule models of an industrial web-service application in the automotive domain.",
        "keywords": [
            "Model-based testing",
            "Test case generation",
            "Property-based testing",
            "QuickCheck",
            "FsCheck",
            "Web services",
            "Business-rule models"
        ],
        "authors": [
            "Bernhard K. Aichernig",
            "Richard Schumi"
        ],
        "file_path": "data/sosym-all/s10270-017-0647-0.pdf"
    },
    {
        "title": "An architecture for coupled digital twins with semantic lifting",
        "submission-date": "2023/08",
        "publication-date": "2024/11",
        "abstract": "To enable the reuse of Digital Twins, in the form of simulation units or other forms of behavioral models, of single physical components, one must be able to connect and couple them. Current platform and architectures consider mostly monolithic digital twins and offer little support for coupling and checking the consistency of the coupling. The coupling must be internally consistent—satisfy constraints related to their co-simulation—and externally consistent—mirror the structure of the composed physical system. In this paper, we propose an extension to a behavior-extended Digital Twin architecture for individual Digital Twins to include co-simulation scenarios for coupled systems lifted from configuration files, which can be implemented along with a Digital-Twin-as-a-Service platform to make assets reusable in time. To monitor and query these connections, we introduce a semantic lifting service, which interprets the coupled Digital Twins as Knowledge Graphs and enables the use of queries to express internal and external consistency constraints. Two representative case studies for systems with coupled behavior are used for the demonstration of this approach and show that it indeed enables reusability of components and services between different Digital Twins.",
        "keywords": [
            "Digital twin",
            "Knowledge graph",
            "Behavioral model",
            "Co-simulation"
        ],
        "authors": [
            "Santiago Gil",
            "Eduard Kamburjan",
            "Prasad Talasila",
            "Peter Gorm Larsen"
        ],
        "file_path": "data/sosym-all/s10270-024-01221-w.pdf"
    },
    {
        "title": "Reconciling software requirements and architectures with intermediate models",
        "submission-date": "2003/12",
        "publication-date": "2003/12",
        "abstract": "Little guidance and few methods are avail-\nable for the reﬁnement of software requirements into an\narchitecture satisfying those requirements. Part of the\nchallenge stems from the fact that requirements and ar-\nchitectures use diﬀerent terms and concepts to capture\nthe model elements relevant to each. In this paper we will\npresent CBSP, a lightweight approach intended to pro-\n vide a systematic way of reconciling requirements and\n architectures using intermediate models. CBSP lever-\n ages a simple set of architectural concepts (components,\n connectors, overall systems, and their properties) to re-\ncast and reﬁne the requirements into an intermediate\nmodel facilitating their mapping to architectures. Fur-\nthermore, the intermediate CBSP model eases captur-\ning and maintaining arbitrarily complex relationships be-\ntween requirements and architectural model elements, as\nwell as among CBSP model elements. We have applied\nCBSP within the context of diﬀerent requirements and\narchitecture deﬁnition techniques. We leverage that ex-\nperience in this paper to demonstrate the CBSP method\nand tool support using a large-scale example.",
        "keywords": [
            "Requirements elicitation and negotiation",
            "Architecture modeling",
            "Intermediate models",
            "Trace-\nability"
        ],
        "authors": [
            "Paul Gr¨unbacher",
            "Alexander Egyed",
            "Nenad Medvidovic"
        ],
        "file_path": "data/sosym-all/s10270-003-0038-6.pdf"
    },
    {
        "title": "Service feature modeling: modeling and participatory ranking of service design alternatives",
        "submission-date": "2012/08",
        "publication-date": "2014/05",
        "abstract": "The design of software-intensive service systems involves and affects numerous stakeholders including software engineers, legal and business experts as well as a potentially large number of consumers. In consequence, the challenge arises to adequately represent the interests of these groups with respect to service design decisions. Specifically, shared service design artifacts and participatory methods for influencing their development in consensus are required, which are not yet state of the art in software service engineering. To this end, we present service feature modeling. Using a modeling notation based on feature-oriented analysis, our approach can represent and interrelate diverse service design concerns and capture their potential combinations as service design alternatives. We further present a method that allows stakeholders to rank service design alternatives based on their preferences. The ranking can support service engineers in selecting viable alternatives for implementation. To exploit this potential, we have implemented a toolkit to enable both modeling and participative ranking of service design alternatives. It has been used to apply service feature modeling in the context of public service design and evaluate the approach in this context.",
        "keywords": [
            "Software service engineering",
            "Participatory design",
            "Service variation modeling",
            "Multi-criteria feature configuration decisions"
        ],
        "authors": [
            "Erik Wittern",
            "Christian Zirpins"
        ],
        "file_path": "data/sosym-all/s10270-014-0414-4.pdf"
    },
    {
        "title": "Guest editorial to the special section on ECMFA and ICMT at STAF 2016",
        "submission-date": "2018/01",
        "publication-date": "2018/02",
        "abstract": "Model-based engineering (MBE) is an approach to the design, analysis and development of software and systems that relies on exploiting high-level models and computer-based automation to achieve significant boosts in both productivity and quality. Model transformation (MT) is the field where engineers leverage different transformation paradigms to solve complex transformation problems. The 12th European Conference on Modelling Foundations and Applications (ECMFA) and the 9th International Conference on Theory and Practice of Model Transformations (ICMT) were held as part of STAF 2016 in Vienna, Austria, from July 4, 2016, to July 5, 2016.",
        "keywords": [],
        "authors": [
            "Pieter Van Gorp",
            "Andrzej Wąsowski"
        ],
        "file_path": "data/sosym-all/s10270-018-0659-4.pdf"
    },
    {
        "title": "Active model learning of stochastic reactive systems (extended version)",
        "submission-date": "2022/07",
        "publication-date": "2024/03",
        "abstract": "Black-box systems are inherently hard to verify. Many veriﬁcation techniques, like model checking, require formal models as a basis. However, such models often do not exist, or they might be outdated. Active automata learning helps to address this issue by offering to automatically infer formal models from system interactions. Hence, automata learning has been receiving much attention in the veriﬁcation community in recent years. This led to various efﬁciency improvements, paving the way toward industrial applications. Most research, however, has been focusing on deterministic systems. In this article, we present an approach to efﬁciently learn models of stochastic reactive systems. Our approach adapts L∗-based learning for Markov decision processes, which we improve and extend to stochastic Mealy machines. When compared with previous work, our evaluation demonstrates that the proposed optimizations and adaptations to stochastic Mealy machines can reduce learning costs by an order of magnitude while improving the accuracy of learned models.",
        "keywords": [
            "Active automata learning",
            "Model mining",
            "Probabilistic veriﬁcation",
            "Stochastic mealy machines",
            "Markov decision processes"
        ],
        "authors": [
            "Edi Muškardin",
            "Martin Tappler",
            "Bernhard K. Aichernig",
            "Ingo Pill"
        ],
        "file_path": "data/sosym-all/s10270-024-01158-0.pdf"
    },
    {
        "title": "ExpRunA: a domain-speciﬁc approach for technology-oriented experiments",
        "submission-date": "2018/10",
        "publication-date": "2019/08",
        "abstract": "Conducting technology-oriented experiments (i.e., experiments in which treatments are applied to objects by a computer-based tool) without proper tool support is often a time-consuming and highly error-prone task. Although many techniques have been proposed to help conducting controlled experiments, none of them simultaneously addresses (1) the executable speciﬁcation of experiments at a high level of abstraction; (2) automated treatment execution and automated data analysis from the experiment speciﬁcation; and (3) formal guaranties of the correctness of results according to an experiment speciﬁcation for technology-oriented experiments. To address these issues, we provide a Domain-Speciﬁc Modeling approach to create a Web-based tool (ExpRunA) comprising a Domain-Speciﬁc Language named ToExpDSL, execution and analysis script generators, a supporting framework, and a running infrastructure. An experimenter uses ToExpDSL to specify an experiment using experimentation concepts. From this speciﬁcation, applications corresponding to the underlying treatments are executed, execution results are collected and analyzed, and, ﬁnally, the analysis results are presented to the experimenter. We establish the consistency of such results with respect to the experiment speciﬁcation by formalizing and proving key correctness properties of ExpRunA. We empirically evaluated ExpRunA with respect to automation by replicating three already published experiments; we evaluated the level of abstraction by a qualitative assessment. Our empirical evaluation shows that ToExpDSL is expressive enough to specify three technology-oriented experiments and that ExpRunA can be used to enable sound automation of execution and analysis from the speciﬁcation of technology-oriented experiments at a high level of abstraction.",
        "keywords": [
            "Controlled experiments",
            "Technology-oriented experiments",
            "Domain-speciﬁc modeling",
            "Domain-speciﬁc language"
        ],
        "authors": [
            "Eneias Silva",
            "Alessandro Leite",
            "Vander Alves",
            "Sven Apel"
        ],
        "file_path": "data/sosym-all/s10270-019-00749-6.pdf"
    },
    {
        "title": "Machine learning for enterprise modeling assistance: an investigation of the potential and proof of concept",
        "submission-date": "2022/03",
        "publication-date": "2023/01",
        "abstract": "Though modeling tools are developing fast, today, enterprise modeling is still a highly manual task that requires substantial\nhuman effort. Today, human modelers are not only assigned the creative component of the process, but they also need to\nperform routine work related to comparing the being developed model with existing ones. Larger amount of information\navailable today makes it possible for a modeler to analyze more information and existing models when developing own\nmodels. However, it also complicates the process since the modeler is often not able to analyze all of them. In this work, we\ndiscuss the potential of the novel idea of using machine learning methods for enterprise modeling assistance that would beneﬁt\nfrom their ability to discover tacit knowledge/regularities in the available data. Graph neural networks have been chosen as\nthe main technique. The contribution lies in the proposed modeling assistance scenarios as well as carried out evaluation of\nthe potential beneﬁts for the modeler. The presented illustrative case study scenario is aimed to demonstrate the feasibility of\nthe proposed approach. The viability and potential of the idea are proved via experiments.",
        "keywords": [
            "Enterprise modeling",
            "Assisted modeling",
            "Machine learning",
            "Graph neural networks"
        ],
        "authors": [
            "Nikolay Shilov",
            "Walaa Othman",
            "Michael Fellmann",
            "Kurt Sandkuhl"
        ],
        "file_path": "data/sosym-all/s10270-022-01077-y.pdf"
    },
    {
        "title": "Special section of business process modeling, development and support (BPMDS) 2018: new perspectives for business process modeling, development and support",
        "submission-date": "2020/08",
        "publication-date": "2020/09",
        "abstract": "The Business Process Modeling, Development and Support (BPMDS) working conference series serves as a meeting place for researchers and practitioners in the areas of business development and business applications (software) development. By incorporating these multiple views, BPMDS offers a unique community venue that integrates different streams of research on business processes and business information systems, and enables to take in a view on the whole range of BPMDS research and interrelationships among different perspectives. This makes it attractive for authors to publish cutting edge research results at BPMDS. This special section contains a selection of the most inﬂuential contributions from the 2018 edition of the working conference.",
        "keywords": [],
        "authors": [
            "Jens Gulden",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-020-00826-1.pdf"
    },
    {
        "title": "Quo Vadis modeling? Findings of a community survey, an ad-hoc bibliometric analysis, and expert interviews on data, process, and software modeling",
        "submission-date": "2023/07",
        "publication-date": "2023/10",
        "abstract": "Models are the key tools humans use to manage complexity in description, development, and analysis. This applies to all scientific and engineering disciplines and in particular to the development of software and data-intensive systems. However, different methods and terminologies have become established in the individual disciplines, even in the sub-fields of Informatics, which raises the need for a comprehensive and cross-sectional analysis of the past, present, and future of modeling research. This paper aims to shed some light on how different modeling disciplines emerged and what characterizes them with a discussion of the potential toward a common modeling future. It focuses on the areas of software, data, and process modeling and reports on an analysis of the research approaches, goals, and visions pursued in each, as well as the methods used. This analysis is based on the results of a survey conducted in the communities concerned, on a bibliometric study, and on interviews with a prominent representative of each of these communities. The paper discusses the different viewpoints of the communities, their commonalities and differences, and identifies possible starting points for further collaboration. It further discusses current challenges for the communities in general and modeling as a research topic in particular and highlights visions for the future.",
        "keywords": [
            "Research communities",
            "Software engineering",
            "Software modeling",
            "Data modeling",
            "Process modeling",
            "Information systems"
        ],
        "authors": [
            "Judith Michael",
            "Dominik Bork",
            "Manuel Wimmer",
            "Heinrich C. Mayr"
        ],
        "file_path": "data/sosym-all/s10270-023-01128-y.pdf"
    },
    {
        "title": "An approach based on the domain perspective to develop WSAN applications",
        "submission-date": "2014/11",
        "publication-date": "2015/09",
        "abstract": "As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.",
        "keywords": [
            "WSAN applications",
            "Model-driven architecture",
            "Domain-speciﬁc language",
            "UML proﬁle",
            "Architecture",
            "Code generation",
            "Abstraction"
        ],
        "authors": [
            "Taniro Rodrigues",
            "Flávia C. Delicato",
            "Thais Batista",
            "Paulo F. Pires",
            "Luci Pirmez"
        ],
        "file_path": "data/sosym-all/s10270-015-0498-5.pdf"
    },
    {
        "title": "MBIPV: a model-based approach for identifying privacy violations from software requirements",
        "submission-date": "2022/01",
        "publication-date": "2022/12",
        "abstract": "Nowadays, large-scale software systems in many domains, such as smart cities, involve multiple parties whose privacy policies may conﬂict with each other, and thus, data privacy violations may arise even without users being aware of it. In this context, identifying data security requirements and detecting potential privacy violations are crucial. In the area of model-based security requirements analysis, numerous research efforts have been done. However, few existing studies support automatic privacy violation identiﬁcation from software requirements. To ﬁll this gap, this paper presents MBIPV, a Model-Based approach for Identifying Privacy Violations from software requirements. First, this paper identiﬁes six types of privacy violations in software requirements. Second, the MBIPV proﬁle is proposed to support modeling software requirements using UML. Third, the MBIPV prototype tool is developed to generate formal models and corresponding privacy properties automatically. Then, the privacy properties are automatically veriﬁed by model checking. We evaluated the MBIPV method through case studies of four representative software systems from different domains: smart health, smart transportation, smart home, and e-commerce. The results show that MBIPV has high accuracy and efﬁciency in identifying the privacy violations from the software requirements. To the best of our knowledge, MBIPV is the ﬁrst model-based approach that supports the automatic veriﬁcation of privacy properties of UML software requirement models. The source code of the MBIPV tool and the experimental data are available online at https://github.com/YETONG1219/MBIPV.",
        "keywords": [
            "UML",
            "Software requirement modeling",
            "Privacy violation",
            "Formal modeling",
            "Formal veriﬁcation"
        ],
        "authors": [
            "Tong Ye",
            "Yi Zhuang",
            "Gongzhe Qiao"
        ],
        "file_path": "data/sosym-all/s10270-022-01072-3.pdf"
    },
    {
        "title": "Workﬂow patterns put into context",
        "submission-date": "2012/01",
        "publication-date": "2012/02",
        "abstract": "In his paper “Approaches to Modeling Business Processes. A Critical Analysis of BPMN, Workﬂow Pat-terns and YAWL”, Egon Börger criticizes the work of the Workﬂow Patterns Initiative in a rather provocative manner. Although the workﬂow patterns and YAWL are well estab-lished and frequently used, Börger seems to misunderstand the goals and contributions of the Workﬂow Patterns Initia-tive. Therefore, we put the workﬂow patterns and YAWL in their historic context. Moreover, we address some of the criticism of Börger by pointing out the real purpose of the workﬂow patterns and their relationship to formal languages (Petri nets) and real-life WFM/BPM systems.",
        "keywords": [
            "Workﬂow patterns",
            "YAWL",
            "Petri nets",
            "Business process management"
        ],
        "authors": [
            "W. M. P. van der Aalst",
            "A. H. M. ter Hofstede"
        ],
        "file_path": "data/sosym-all/s10270-012-0233-4.pdf"
    },
    {
        "title": "Guest editorial to the special issue on model transformation",
        "submission-date": "2011/08",
        "publication-date": "2011/08",
        "abstract": "Models are widespread in systems engineering, and provide a mechanism for managing and controlling complexity when developing large-scale complex IT systems. Models help engineers capture essential information at a suitable level of abstraction, and reason about that information in precise ways. Models are at the heart of model-driven approaches to systems engineering. A key operation in model-driven engineering is model transformation, which is the practice of deﬁning and implementing operations on models. Model transformations reﬁne and/or evolve a model into a different artefact: a new model (e.g., expressed in a different language), an abstraction of the original model, text (e.g., source code), or some other representation needed for a speciﬁc domain context. Model transformation approaches are becoming mainstream in model-driven engineering: there are now numerous model transformation languages and tools that allow the speciﬁcation, implementation, orchestration and execution of transformations, applied to different languages and on different platforms. The research ﬁeld is extremely active, with substantial research efforts in understanding the advantages and disadvantages of different approaches to model transformation, foundational principles, semantics of transformation languages, and model transformation properties like modularity and composability. There is also substantial interest in treatment of model transformations as reusable assets—models in and of themselves—and on developing rigorous methodologies for the construction of transformations that are ﬁt-for-purpose. Many of these issues are the focus of this special issue. This special issue synthesises some of the advanced, state-of-the-art research in model transformation. The selected papers also provide an overview of current open issues and identify potential lines for further research.",
        "keywords": [],
        "authors": [
            "Richard F. Paige",
            "Jeff Gray"
        ],
        "file_path": "data/sosym-all/s10270-011-0209-9.pdf"
    },
    {
        "title": "Modeling modeling modeling",
        "submission-date": "2010/03",
        "publication-date": "2010/08",
        "abstract": "Model-driven engineering and model-based approaches have permeated all branches of software engineering to the point that it seems that we are using models, as Molière’s Monsieur Jourdain was using prose, without knowing it. At the heart of modeling, there is a relation that we establish to represent something by something else. In this paper we review various definitions of models and relations between them. Then, we deﬁne a canonical set of relations that can be used to express various kinds of representation relations and we propose a graphical concrete syntax to represent these relations. We also define a structural definition for this language in the form of a metamodel and a formal interpretation using Prolog. Hence, this paper is a contribution towards a theory of modeling.",
        "keywords": [
            "Model",
            "Metamodel",
            "Notation",
            "Representation"
        ],
        "authors": [
            "Pierre-Alain Muller",
            "Frédéric Fondement",
            "Benoît Baudry",
            "Benoît Combemale"
        ],
        "file_path": "data/sosym-all/s10270-010-0172-x.pdf"
    },
    {
        "title": "From analytical purposes to data visualizations: a decision process guided by a conceptual framework and eye tracking",
        "submission-date": "2017/10",
        "publication-date": "2019/07",
        "abstract": "Data visualizations are versatile tools for gaining cognitive access to large amounts of data and for making complex relationships in data understandable. This paper proposes a method for assessing data visualizations according to the purposes they fulfill in domain-specific data analysis settings. We introduce a framework that gets configured for a given analysis domain and allows to choose data visualizations in a methodically justified way, based on analysis questions that address different aspects of data to be analyzed. Based on the concepts addressed by the analysis questions, the framework provides systematic guidance for determining which data visualizations are able to serve which conceptual analysis interests. In a second step of the method, we propose to follow a data-driven approach and to experimentally compare alternative data visualizations for a particular analytical purpose. More specifically, we propose to use eye tracking to support justified decisions about which of the data visualizations selected with the help of the framework are most suitable for assessing the analysis domain in a cognitively efficient way. We demonstrate our approach of how to come from analytical purposes to data visualizations using the example domain of Process Modeling Behavior Analysis. The analyses are performed on the background of representative analysis questions from this domain.",
        "keywords": [
            "Data visualization",
            "Process execution data",
            "Process Modeling Behavior Analysis",
            "Eye tracking",
            "Reading patterns",
            "Process mining"
        ],
        "authors": [
            "Jens Gulden",
            "Andrea Burattin",
            "Amine A. Andaloussi",
            "Barbara Weber"
        ],
        "file_path": "data/sosym-all/s10270-019-00742-z.pdf"
    },
    {
        "title": "A functional size measurement method for object-oriented conceptual schemas: design and evaluation issues",
        "submission-date": "2004/10",
        "publication-date": "2005/09",
        "abstract": "Functional Size Measurement (FSM) methods are intended to measure the size of software by quantifying the functional user requirements of the software. The capability to accurately quantify the size of software in an early stage of the development lifecycle is critical to software project managers for evaluating risks, developing project estimates and having early project indicators. In this paper, we present OO-Method Function Points (OOmFP), which is a new FSM method for object-oriented systems that is based on measuring conceptual schemas. OOmFP is presented following the steps of a process model for software measurement. Using this process model, we present the design of the measurement method, its application in a case study, and the analysis of different evaluation types that can be carried out to validate the method and to verify its application and results.",
        "keywords": [
            "Conceptual modeling",
            "Object orientation",
            "Functional size measurement",
            "Measure validation",
            "Measurement veriﬁcation"
        ],
        "authors": [
            "Silvia Abrahão",
            "Geert Poels",
            "Oscar Pastor"
        ],
        "file_path": "data/sosym-all/s10270-005-0098-x.pdf"
    },
    {
        "title": "Paradigm shift in mechanical system design: toward automated and collaborative design with digital twin web",
        "submission-date": "2023/08",
        "publication-date": "2024/10",
        "abstract": "Analyzing multi-vendor mechanical system designs requires a signiﬁcant amount of manual work, resulting in a design paradigm where analysis is conducted only after the design is locked and components are selected. This leads to a suboptimal design with compatibility issues, over-dimensioned components, inferior performance, poor energy efﬁciency, and a lack of collaboration between OEMs (original equipment manufacturers) and system integrators. To overcome these issues, this paper proposes Co-Des (collaborative design) framework for automated and collaborative multi-vendor system design. The framework relies on standardized digital twin documents (DTD) of system designs, components, and analyses. The discov-erability and distribution of these DTDs are enabled with digital twin web (DTW). Co-Des framework allows for ﬁnding suitable components for the design task by automatically running selected analyses employing component digital twins. In addition, OEMs can provide customized components for system integrators using the initial system design deﬁned in the system design DTD. The use of the Co-Des framework was demonstrated with a windmill powertrain design use case, and the applicability of the automated assembly analysis for component selection was veriﬁed with performance measurements. The adoption of the proposed framework will lead to a paradigm shift from manual and siloed work relying on the exchange of PDFs to a more automated and collaborative design of mechanical systems. The adoption rate is deﬁned by the willingness of system integrators to publish their initial system designs and OEMs their components as public digital twins.",
        "keywords": [
            "Collaborative design",
            "Simulation",
            "Digital twin",
            "Digital twin web",
            "Industry 4.0",
            "Open-source"
        ],
        "authors": [
            "Riku Ala-Laurinaho",
            "Juuso Autiosalo",
            "Sampo Laine",
            "Urho Hakonen",
            "Raine Viitala"
        ],
        "file_path": "data/sosym-all/s10270-024-01215-8.pdf"
    },
    {
        "title": "Ontology-based security modeling in ArchiMate",
        "submission-date": "2023/04",
        "publication-date": "2024/02",
        "abstract": "Enterprise Risk Management involves the process of identiﬁcation, evaluation, treatment, and communication regarding risks throughout the enterprise. To support the tasks associated with this process, several frameworks and modeling languages have been proposed, such as the Risk and Security Overlay (RSO) of ArchiMate. An ontological investigation of this artifact would reveal its adequacy, capabilities, and limitations w.r.t. the domain of risk and security. Based on that, a language redesign can be proposed as a reﬁnement. Such analysis and redesign have been executed for the risk elements of the RSO grounded in the Common Ontology of Value and Risk. The next step along this line of research is to address the following research problems: What would be the outcome of an ontological analysis of security-related elements of the RSO? That is, can we identify other semantic deﬁciencies in the RSO through an ontological analysis? Once such an analysis is provided, can we redesign the security elements of the RSO accordingly, in order to produce an improved artifact? Here, with the aid of the Reference Ontology for Security Engineering (ROSE) and the ontological theory of prevention behind it, we address the remaining gap by proceeding with an ontological analysis of the security-related constructs of the RSO. The outcome of this assessment is an ontology-based redesign of the ArchiMate language regarding security modeling. In a nutshell, we report the following contributions: (1) an ontological analysis of the RSO that identiﬁes six limitations concerning security modeling; (2) because of the key role of the notion of prevention in security modeling, the introduction of the ontological theory of prevention in ArchiMate; (3) a well-founded redesign of security elements of ArchiMate; and (4) ontology-based security modeling patterns that are logical consequences of our proposal of redesign due to its underlying ontology of security. As a form of evaluation, we show that our proposal can describe risk treatment options, according to ISO 31000. Finally, besides presenting multiple examples, we proceed with a real-world illustrative application taken from the cybersecurity domain.",
        "keywords": [
            "Security modeling",
            "Ontological analysis",
            "Ontological patterns",
            "Enterprise architecture",
            "ArchiMate",
            "Reference ontology for security engineering",
            "Uniﬁed foundational ontology"
        ],
        "authors": [
            "Ítalo Oliveira",
            "Tiago Prince Sales",
            "João Paulo A. Almeida",
            "Riccardo Baratella",
            "Mattia Fumagalli",
            "Giancarlo Guizzardi"
        ],
        "file_path": "data/sosym-all/s10270-024-01149-1.pdf"
    },
    {
        "title": "On the realizability of collaborative services",
        "submission-date": "2010/04",
        "publication-date": "2011/10",
        "abstract": "This paper considers compositional speciﬁcations of services using UML 2 collaborations, activity and interaction diagrams, and addresses the realizability problem for such speciﬁcations: given a global speciﬁcation, can we construct a set of communicating system components whose joint behavior is precisely the speciﬁed global behavior? We approach the problem by looking at how the sequencing of collaborationsandlocalactionsmaybedescribedusingUML activity diagrams. We identify the realizability problems for each of the sequencing operators, such as strong and weak sequence, choice of alternatives, loops, and concurrency. The nature of these realizability problems and possible solutions are discussed. This brings a new look at already known prob-lems: we show that given some conditions, certain problems can already be detected at an abstract level, without looking at the detailed interactions of the collaborations, provided that we know the components that initiate and terminate the different collaborations.",
        "keywords": [
            "Service composition",
            "Compositional speciﬁcation of collaborations",
            "Realizability of distributed implementations",
            "Distributed system design",
            "Design guidelines",
            "Deriving component behavior from global speciﬁcations",
            "Workﬂow for collaborations",
            "UML activity diagrams",
            "Service oriented architecture"
        ],
        "authors": [
            "Humberto Nicolás Castejón",
            "Gregor von Bochmann",
            "Rolv Bræk"
        ],
        "file_path": "data/sosym-all/s10270-011-0216-x.pdf"
    },
    {
        "title": "A model and workﬂow-driven approach for engineering domain-speciﬁc low-code platforms and applications",
        "submission-date": "2025/02",
        "publication-date": "Not found",
        "abstract": "The need to produce software quicker and in greater quantities continues to grow, while the market for professional program- mers struggles to meet the rising demand. Low-code development platforms (LCDPs) have been proposed as a solution that enables individuals without formal training to develop applications. However, current LCDPs typically rely on proprietary, ﬁxed code generation processes, which limit ﬂexibility, hinder interoperability, and risk vendor lock-in. To address these limitations, this paper introduces Dandelion+, a cloud-based low-code platform designed to create domain-speciﬁc LCDPs and applications within them. Our approach is highly model-driven, from deﬁning the scaffolds of low-code platforms (e.g., models, meta-models, users, and roles) to creating applications. In particular, applications are powered by PlatFlow, a platform-aware graphical workﬂow language. Comprising both model operations and low-code-focused nodes, PlatFlow offers ﬂexible control over the code generation process that drives LCDPs. We conduct two case studies to evaluate our approach. First, we confront Dandelion+ with other low-code platforms to design interactive applications and produce arti- facts. Second, we report on the beneﬁts of Dandelion+ in the industrial context of UGROUND, a Spanish software ﬁrm that uses low code in its projects.",
        "keywords": [
            "Low-code platforms",
            "Model-driven engineering",
            "Workﬂow languages"
        ],
        "authors": [
            "Francisco Martínez-Lasaca",
            "Pablo Díez",
            "Esther Guerra",
            "Juan de Lara"
        ],
        "file_path": "data/sosym-all/s10270-025-01308-y.pdf"
    },
    {
        "title": "Model transformation by example using inductive logic programming",
        "submission-date": "2007/07",
        "publication-date": "2008/08",
        "abstract": "Model transformation by example is a novel approach in model-driven software engineering to derive model transformation rules from an initial prototypical set of interrelated source and target models, which describe critical cases of the model transformation problem in a purely declarative way. In the current paper, we automate this approach using inductive logic programming (Muggleton and Raedt in J Logic Program 19-20:629–679, 1994) which aims at the inductive construction of ﬁrst-order clausal theories from examples and background knowledge.",
        "keywords": [
            "Model transformation",
            "By-example synthesis",
            "Inductive logic programming"
        ],
        "authors": [
            "Zoltán Balogh",
            "Dániel Varró"
        ],
        "file_path": "data/sosym-all/s10270-008-0092-1.pdf"
    },
    {
        "title": "Nivel: a metamodelling language with a formal semantics",
        "submission-date": "2007/04",
        "publication-date": "2008/11",
        "abstract": "Much work has been done to clarify the notion of metamodelling and new ideas, such as strict metamodelling, distinction between ontological and linguistic instantiation, uniﬁed modelling elements and deep instantiation, have been introduced. However, many of these ideas have not yet been fully developed and integrated into modelling languages with (concrete) syntax, rigorous semantics and tool support. Consequently, applying these ideas in practice and reasoning about their meaning is difﬁcult, if not impossible. In this paper, we strive to add semantic rigour and conceptual claritytometamodellingthroughtheintroductionof Nivel,a novel metamodelling language capable of expressing models spanning an arbitrary number of levels. Nivel is based on a core set of conceptual modelling concepts: class, generali-sation, instantiation, attribute, value and association. Nivel adheres to a form of strict metamodelling and supports deep instantiation of classes, associations and attributes. A for-mal semantics is given for Nivel by translation to weight constraint rule language (WCRL), which enables decidable, automated reasoning about Nivel. The modelling facilities of Nivel and the utility of the formalisation are demonstrated in a case study on feature modelling.",
        "keywords": [
            "Conceptual modelling",
            "Metamodelling",
            "Nivel",
            "Weight constraint rules",
            "Formal semantics"
        ],
        "authors": [
            "Timo Asikainen",
            "Tomi Männistö"
        ],
        "file_path": "data/sosym-all/s10270-008-0103-2.pdf"
    },
    {
        "title": "Deﬁnition and validation of model transformations",
        "submission-date": "2004/11",
        "publication-date": "2006/08",
        "abstract": "With model transformations becoming more widely used, there is an increasing need for approaches focussing on a systematic development of model trans-formations. Although a number of approaches for specifyingmodel transformations exist, noneof themfocusses on systematically validating model transformations with respect to termination and conﬂuence. Termination and conﬂuence ensure that a model transformation always produces a unique result. Also called functionality, these properties are important requirements for practical app-lications of model transformations. In this paper, we introduce our approach to model transformation. Using and extending results from the theory of graph trans-formation, we investigate termination and conﬂuence properties of model transformations speciﬁed in our ap-proach. We establish a set of criteria for termination and conﬂuence to be checked at design time by static analysis of the transformation rules and the underlying metamodels. Moreover, the criteria are formulated in such a way that they require less experience with the theory of graph transformation. Our concepts are illus-trated by a running example of a model tranformation from statecharts to the process algebra Communicating Sequential Processes.",
        "keywords": [],
        "authors": [
            "Jochen M. Küster"
        ],
        "file_path": "data/sosym-all/s10270-006-0018-8.pdf"
    },
    {
        "title": "PARMOREL: a framework for customizable model repair",
        "submission-date": "2021/03",
        "publication-date": "2022/05",
        "abstract": "In model-driven software engineering, models are used in all phases of the development process. These models must hold a high quality since the implementation of the systems they represent relies on them. Several existing tools reduce the burden of manually dealing with issues that affect models’ quality, such as syntax errors, model smells, and inadequate structures. However, these tools are often inﬂexible for customization and hard to extend. This paper presents a customizable and extensible model repair framework, PARMOREL, that enables users to deal with different issues in different types of models. The framework uses reinforcement learning to automatically ﬁnd the best sequence of actions for repairing a broken model according to user preferences. As proof of concept, we repair syntactic errors in class diagrams taking into account a model distance metric and quality characteristics. In addition, we restore inter-model consistency between UML class and sequence diagrams while improving the coupling qualities of the sequence diagrams. Furthermore, we evaluate the approach on a large publicly available dataset and a set of real-world inspired models to show that PARMOREL can decide and pick the best solution to solve the issues present in the models to satisfy user preferences.",
        "keywords": [
            "Model repair",
            "Reinforcement learning",
            "Customizable framework"
        ],
        "authors": [
            "Angela Barriga",
            "Rogardt Heldal",
            "Adrian Rutle",
            "Ludovico Iovino"
        ],
        "file_path": "data/sosym-all/s10270-022-01005-0.pdf"
    },
    {
        "title": "Pattern reiﬁcation as the basis for description-driven systems",
        "submission-date": "2003/06",
        "publication-date": "2003/06",
        "abstract": "One of the main factors driving object-oriented software development for information systems is the requirement for systems to be tolerant to change. To address this issue in designing systems, this paper proposes a pattern-based, object-oriented, description-driven system (DDS) architecture as an extension to the standard UML four-layer meta-model. A DDS architecture is proposed in which aspects of both static and dynamic systems behavior can be captured via descriptive models and meta-models. The proposed architecture embodies four main elements – ﬁrstly, the adoption of a multi-layered meta-modeling architecture and reﬂective meta-level architecture, secondly the identiﬁcation of four data modeling relationships that can be made explicit such that they can be modiﬁed dynamically, thirdly the identiﬁcation of ﬁve design patterns which have emerged from practice and have proved essential in providing reusable building blocks for data management, and fourthly the encoding of the structural properties of the ﬁve design patterns by means of one fundamen-tal pattern, the Graph pattern. A practical example of this philosophy, the CRISTAL project, is used to demonstrate the use of description-driven data objects to handle system evolution.",
        "keywords": [
            "Meta-models",
            "System description",
            "UML",
            "Design patterns",
            "Reﬂection"
        ],
        "authors": [
            "Florida Estrella",
            "Zsolt Kovacs",
            "Jean-Marie Le Goﬀ",
            "Richard McClatchey",
            "Tony Solomonides",
            "Norbert Toth"
        ],
        "file_path": "data/sosym-all/s10270-003-0023-0.pdf"
    },
    {
        "title": "On the relationship between models and ontologies",
        "submission-date": "2022/07",
        "publication-date": "2022/07",
        "abstract": "The application of ontologies toward various engineering domains has gained much momentum recently, such that it is beneficial to understand the relationship between ontology building and modeling.",
        "keywords": [],
        "authors": [
            "Jeff Gray",
            "Bernhard Rumpe"
        ],
        "file_path": "data/sosym-all/s10270-022-01021-0.pdf"
    },
    {
        "title": "Software and Systems Modeling (2025) 24:7–8",
        "submission-date": "2024/11",
        "publication-date": "2024/12",
        "abstract": "The Business Process Modeling, Development, and Support (BPMDS) working conference has been held for more than two decades, dealing with, and promoting research on BPMDS and has been a platform for a multitude of inﬂuential research papers. In keeping with its tradition, the working conference covers a broad range of theoretical and application-based research. BPMDS started in 1998 as a recurring workshop. During this period, business process analysis and design were recognized as central issues in the area of information systems engineering. The continued interest in these topics on behalf of the IS engineering community is reﬂected by the success of recent BPMDS events and the emergence of new conferences and workshops devoted to the theme. In 2011, BPMDS became a two-day working conference attached to CAiSE. The goals, format, and history of BPMDS can be found at www.bpmds.org.",
        "keywords": [],
        "authors": [
            "Han van der Aa",
            "Rainer Schmidt"
        ],
        "file_path": "data/sosym-all/s10270-024-01248-z.pdf"
    },
    {
        "title": "Checking security compliance between models and code",
        "submission-date": "2020/09",
        "publication-date": "2022/03",
        "abstract": "It is challenging to verify that the planned security mechanisms are actually implemented in the software. In the context\nof model-based development, the implemented security mechanisms must capture all intended security properties that were\nconsidered in the design models. Assuring this compliance manually is labor intensive and can be error-prone. This work\nintroduces the ﬁrst semi-automatic technique for secure data ﬂow compliance checks between design models and code. We\ndevelop heuristic-based automated mappings between a design-level model (SecDFD, provided by humans) and a code-level\nrepresentation (Program Model, automatically extracted from the implementation) in order to guide users in discovering\ncompliance violations, and hence, potential security ﬂaws in the code. These mappings enable an automated, and project-\nspeciﬁc static analysis of the implementation with respect to the desired security properties of the design model. We developed\ntwo types of security compliance checks and evaluated the entire approach on open source Java projects.",
        "keywords": [
            "Security-by-design",
            "Security compliance",
            "Data ﬂow diagram (DFD)",
            "Static program analysis"
        ],
        "authors": [
            "Katja Tuma",
            "Sven Peldszus",
            "Daniel Strüber",
            "Riccardo Scandariato",
            "Jan Jürjens"
        ],
        "file_path": "data/sosym-all/s10270-022-00991-5.pdf"
    },
    {
        "title": "An empirical study of manual abstraction between class diagrams and code of open-source systems",
        "submission-date": "2024/07",
        "publication-date": "2025/03",
        "abstract": "Models play a crucial role in software design, analysis, and supporting new maintainers. However, over time, the beneﬁts of models can diminish as system implementations evolve without corresponding updates to the original models. Reverse engineering methods and tools can help maintain alignment between models and implementation code. Yet, automatically reverse-engineered models often lack abstraction and contain extensive details that hinder comprehension. Recent advance- ments in AI-based content generation suggest that we may soon see reverse engineering tools capable of human-grade abstraction. To guide the design and validation of such tools, we need a principled understanding of manual abstraction—a topic that has received limited attention in existing literature. In pursuit of this goal, our paper presents a multiple-case study of model-to-code differences, examining nine substantial open-source software projects obtained through repository mining. We manually matched source code from projects comprising 4983 classes, 26k attributes, and 54k operations to 523 model elements (including classes, attributes, operations, and relationships). These mappings precisely capture discrepan- cies between provided class diagram designs and actual implementation code. By analyzing these differences in detail, we derive a taxonomy of difference types and provide a well-organized list of cases corresponding to identiﬁed differences. Our ﬁndings have the potential to contribute to improved reverse engineering methods and tools, propose new mapping rules for model-to-code consistency checks, and offer guidelines to avoid over-abstraction and over-speciﬁcation during the design process.",
        "keywords": [
            "Software design",
            "Modeling",
            "Abstraction"
        ],
        "authors": [
            "Wenli Zhang",
            "Weixing Zhang",
            "Daniel Strüber",
            "Regina Hebig"
        ],
        "file_path": "data/sosym-all/s10270-025-01289-y.pdf"
    },
    {
        "title": "A modular timed graph transformation language for simulation-based design",
        "submission-date": "2009/04",
        "publication-date": "2011/06",
        "abstract": "We introduce the MoTif (Modular Timed graph transformation) language, which allows one to elegantly model complex control structures for programmed graph transformation. These include modular construction, parallel composition, and a temporal dimension in addition to the usual transformation control structures. The ﬁrst part of this contribution formally introduces MoTif and its semantics is based on the Discrete EVent system Speciﬁcation (DEVS) formalism which allows for highly modular, hierarchical modelling of timed, reactive systems. In MoTif, graphs are embedded in events and individual transformation rules are embedded in atomic DEVS models. A side effect of the use of DEVS is the introduction of an explicit notion of time. This allows one to model a time-advance for every rule as well as to interrupt (pre-empt) rule execution. In the second part, we design a case study to show how the explicit notion of time allows for the simulation-based design of reactive systems such as modern computer games. We use the well-known game of PacMan as an example and model its dynamics in MoTif. This also allows the modelling of player behaviour, incorporatingdataabout humanplayers’ behaviour, andreac-tion times. Thus, a model of both player and game is obtained which can be used to evaluate, through simulation, the play-ability of a game design. We propose a playability perfor-mance measure and change the value of some parameters of thePacMangame.Foreachvariantofthegamethusobtained, simulation yields a value for the quality of the game. This allows us to choose an “optimal” (from a playability point of view) game conﬁguration. The user model is subsequently replaced by a visual interface to a real player, and the game model is executed using a real-time DEVS simulator.",
        "keywords": [
            "Simulation",
            "DEVS",
            "Graph transformation"
        ],
        "authors": [
            "Eugene Syriani",
            "Hans Vangheluwe"
        ],
        "file_path": "data/sosym-all/s10270-011-0205-0.pdf"
    }
]